id,doi,publisher,database,url,domain,publication_date,algorithm_type,training_schema,algorithm_goal,architecture,title,abstract
1,10.1016/j.enconman.2021.113856,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85101129959,industry,01/04/2021,supervised,batch,decision making,centralised,the mutual benefits of renewables and carbon capture: achieved by an artificial intelligent scheduling strategy,"Renewable power and carbon capture are key technologies to transfer the power industry into low carbon generation. Renewables have been developed fast, however, the intermittent nature has imposed higher requirement for the flexibility of the power grid. Retrofitting carbon capture technologies to existing fossil-fuel fired power plants is an important solution to avoid the “lock-in” of emissions, but the high operating costs hinders their large scale application. The coexistence of renewable power and carbon capture opens up a new avenue that the deployment of carbon capture can provide additional flexibility for better accommodation of renewable power while excess renewables can be used to reduce the operating costs of carbon capture. To this end, this paper proposes an artificial intelligence based optimal scheduling strategy for the power plant-carbon capture system in the context of renewable power penetration to show that the mutual benefits between carbon capture and renewable power can be achieved when the carbon capture process is made fully adjustable. An artificial intelligent deep belief neural network is used to reflect the complex interactions between carbon, heat and electricity within the power plant carbon capture system. Multiple operating goals are considered in the scheduling such as minimizing the operating costs, renewable power curtailment and carbon emission, and the particle swarm heuristic optimization is employed to find the optimal solution. The impacts of carbon capture constraint mode, carbon emission penalty coefficient, carbon dioxide production constraints and renewable power installed capacity are investigated to provide broader insight on the potential benefit of carbon capture in future low-carbon energy system. A case study using real world data of weather condition and load demand shows that renewable power curtailment can be reduced by 51% with the integration of post-combustion capture systems and 35% of total carbon emission are captured by the use of excess renewable power through optimal scheduling. This paper points out a new way of using artificial intelligent technologies to coordinate the couplings between carbon and electricity for efficient and environmentally friendly operation of future low-carbon energy system."
2,,A multi-FPGA distributed embedded system for the emulation of Multi-Layer CNNs in real time video applications,core,'Institute of Electrical and Electronics Engineers (IEEE)',multimedia,29/03/2010 00:00,supervised,batch,classification,decentralised,https://core.ac.uk/download/60418869.pdf,"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates.This work has been partially supported by the Fundación Séneca de la Región de Murcia through the research projects 08801/PI/08 and 08788/PI/08, and by the Spanish Government through project TIN2008-06893-C03"
3,10.1016/j.softx.2017.09.002,Elsevier,core,,multimedia,,not defined,not defined,not defined,not defined,confocalgn: a minimalistic confocal image generator,"Validating image analysis pipelines and training machine-learning segmentation algorithms require images with known features. Synthetic images can be used for this purpose, with the advantage that large reference sets can be produced easily. It is however essential to obtain images that are as realistic as possible in terms of noise and resolution, which is challenging in the field of microscopy. We describe ConfocalGN, a user-friendly software that can generate synthetic microscopy stacks from a ground truth (i.e. the observed object) specified as a 3D bitmap or a list of fluorophore coordinates. This software can analyze a real microscope image stack to set the noise parameters and directly generate new images of the object with noise characteristics similar to that of the sample image. With a minimal input from the user and a modular architecture, ConfocalGN is easily integrated with existing image analysis solutions. Keywords: Synthetic image, Image analysis, Bioinformatic"
4,http://arxiv.org/abs/2009.10679v1,arxiv,arxiv,http://arxiv.org/abs/2009.10679v1,multimedia,22/09/2020 00:00,supervised,batch,classification,centralised,"an embedded deep learning system for augmented reality in firefighting
  applications","Firefighting is a dynamic activity, in which numerous operations occur
simultaneously. Maintaining situational awareness (i.e., knowledge of current
conditions and activities at the scene) is critical to the accurate
decision-making necessary for the safe and successful navigation of a fire
environment by firefighters. Conversely, the disorientation caused by hazards
such as smoke and extreme heat can lead to injury or even fatality. This
research implements recent advancements in technology such as deep learning,
point cloud and thermal imaging, and augmented reality platforms to improve a
firefighter's situational awareness and scene navigation through improved
interpretation of that scene. We have designed and built a prototype embedded
system that can leverage data streamed from cameras built into a firefighter's
personal protective equipment (PPE) to capture thermal, RGB color, and depth
imagery and then deploy already developed deep learning models to analyze the
input data in real time. The embedded system analyzes and returns the processed
images via wireless streaming, where they can be viewed remotely and relayed
back to the firefighter using an augmented reality platform that visualizes the
results of the analyzed inputs and draws the firefighter's attention to objects
of interest, such as doors and windows otherwise invisible through smoke and
flames."
5,10.1109/itc-egypt52936.2021.9513888,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9513888/,health,15/07/2021 00:00,not defined,not defined,not defined,centralised,a proposed end to end telemedicine system based on embedded system and mobile application using cmos wearable sensors,"Internet of things (IoT) and Embedded systems have extensive applications in healthcare markets. Integration of IoT with healthcare started with wearable smartwatches monitoring some signals and storing this data in the cloud. With 4G/5G and WiFi 6 networks. Healthcare data can be analyzed with Artificial Intelligence providing new era Internet of Medical Things (IoMT) that encompass an array of internet-capable medical devices that are in constant communication with each other or with the cloud; Internet of Healthcare Things (IoHT) that is the digital transformation of the healthcare industry. This article presents an end-to-end architecture with realization of three modules for key IoT aspects for healthcare and telemedicine. Results from a real implementation of application Platform for Data Processing including patient and doctor data base-based web site, MySQL data base, Android based mobile App, and PHP webserver."
6,http://arxiv.org/abs/2101.04930v2,arxiv,arxiv,http://arxiv.org/abs/2101.04930v2,smart cities,13/01/2021 00:00,not defined,not defined,not defined,not defined,"an empirical study on deployment faults of deep learning based mobile
  applications","Deep Learning (DL) is finding its way into a growing number of mobile
software applications. These software applications, named as DL based mobile
applications (abbreviated as mobile DL apps) integrate DL models trained using
large-scale data with DL programs. A DL program encodes the structure of a
desirable DL model and the process by which the model is trained using training
data. Due to the increasing dependency of current mobile apps on DL, software
engineering (SE) for mobile DL apps has become important. However, existing
efforts in SE research community mainly focus on the development of DL models
and extensively analyze faults in DL programs. In contrast, faults related to
the deployment of DL models on mobile devices (named as deployment faults of
mobile DL apps) have not been well studied. Since mobile DL apps have been used
by billions of end users daily for various purposes including for
safety-critical scenarios, characterizing their deployment faults is of
enormous importance. To fill the knowledge gap, this paper presents the first
comprehensive study on the deployment faults of mobile DL apps. We identify 304
real deployment faults from Stack Overflow and GitHub, two commonly used data
sources for studying software faults. Based on the identified faults, we
construct a fine-granularity taxonomy consisting of 23 categories regarding to
fault symptoms and distill common fix strategies for different fault types.
Furthermore, we suggest actionable implications and research avenues that could
further facilitate the deployment of DL models on mobile devices."
7,10.1109/ams.2017.22,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8424312/,robotics,06/12/2017 00:00,not defined,not defined,decision making,decentralised,autonomous rover navigation using gps based path planning,"Nowadays, with the constant evolution of Artificial Intelligence and Machine Learning, robots are getting more perceptive than ever. For this quality they are being used in varying circumstances which humans cannot control. Rovers are special robots, capable of traversing through areas that are too difficult for humans. Even though it is a robust bot, lack of proper intelligence and automation are its basic shortcomings. As the main purpose of a rover is to traverse through areas of extreme difficulties, therefore an intelligent path generation and following system is highly required. Our research work aimed at developing an algorithm for autonomous path generation using GPS (Global Positioning System) based coordinate system and implementation of this algorithm in real life terrain, which in our case is MDRS, Utah, USA. Our prime focus was the development of a robust but easy to implement system. After developing such system, we have been able to successfully traverse our rover through that difficult terrain. It uses GPS coordinates of target points that will be fed into the rover from a control station. The rover capturing its own GPS signal generates a path between the current location and the destination location on its own. It then finds the deviation in its current course of direction and position. And eventually it uses Proportional Integral Derivative control loop feedback mechanism (PID control algorithm) for compensating the error or deviation and thus following that path and reach destination. A low cost on board computer (Raspberry Pi in our case) handles all the calculations during the process and drives the rover fulfilling its task using an microcontroller (Arduino)."
8,10.17863/cam.45198,ASCE,project-academic,project-academic,smart cities,22/10/2019 00:00,not defined,not defined,clustering,not defined,developing a dynamic digital twin at building and city levels a case study of the west cambridge campus," A Digital Twin (DT) refers to a digital replica of physical assets, processes and systems. DTs integrate artificial intelligence, machine learning and data analytics to create living digital simulation models that are able to learn and update from multiple sources, and to represent and predict the current and future conditions of physical counterparts. However, the current activities related to DTs are still at an early stage with respect to buildings and other infrastructure assets from an architectural and engineering/construction point of view. Less attention has been paid to the operation & maintenance (O&M) phase, which is the longest time span in the asset life cycle. A systematic and clear architecture verified with practical use cases for constructing a DT would be the foremost step for effective operation and maintenance of buildings and cities. According to current research about multi-tier architectures, this paper presents a system architecture for DTs which is specifically designed at both the building and city levels. Based on this architecture, a DT demonstrator of the West Cambridge site of the University of Cambridge was developed, which integrates heterogeneous data sources, supports effective data querying and analysing, supports decision-making processes in O&M management, and further bridges the gap between human relationships with buildings/cities. This paper aims at going through the whole process of developing DTs in building and city levels from the technical perspective and sharing lessons learnt and challenges involved in developing DTs in real practices. Through developing this DT demonstrator, the results provide a clear roadmap and present particular DT research efforts for asset management practitioners, policymakers and researchers to promote the implementation and development of DT at the building and city levels."
9,,,core,,multimedia,22/07/2013 00:00,supervised,batch,classification,centralised,"jie ma,","In this paper, an online methodology for the detection of unsafe driving states while driving is presented. The detection is based on the multi-sensor approaches, including gyrometer, accelerometer, radar, video and so on. Various information comes from both the ego vehicle and its surroundings are fused to gain a comprehensive understanding of driving situations. Using subspace modeling techniques, we propose an unsupervised learning algorithm to perform the unsafe states detection. The feature space are decomposed into the normal and anomalous subspace, where the normal space are assumed as the major components of the driving patterns, and significant deviations from the modeled normal subspace are signaled as unsafe states. In addition, the algorithm works in a real-time way incorporating a implementation of sliding window, which enable the method to adapt over time to address changes in the new emerged driving situations. We have implemented our algorithm with a prototype system installed in a transit bus, validations are performed in real driving situations. Our experimental results demonstrate the effectiveness of the approach on forward risk predication. We gain a timely predication while with a low false positive when there occurs conflicts between the ego vehicle and front vehicles"
10,10.1109/lra.2019.2894216,IEEE,project-academic,project-academic,robotics,21/01/2019 00:00,supervised,batch,classification,centralised,vr goggles for robots real to sim domain adaptation for visual control," In this letter, we deal with the None reality gap None from a novel perspective, targeting transferring deep reinforcement learning (DRL) policies learned in simulated environments to the real-world domain for visual control tasks. Instead of adopting the common solutions to the problem by increasing the visual fidelity of synthetic images output from simulators during the training phase, we seek to tackle the problem by translating the real-world image streams back to the synthetic domain during the deployment phase, to None make the robot feel at home . We propose this as a lightweight, flexible, and efficient solution for visual control, as first, no extra transfer steps are required during the expensive training of DRL agents in simulation; second, the trained DRL agents will not be constrained to being deployable in only one specific real-world environment; and third, the policy training and the transfer operations are decoupled, and can be conducted in parallel. Besides this, we propose a simple yet effective None shift loss None that is agnostic to the downstream task, to constrain the consistency between subsequent frames which is important for consistent policy outputs. We validate the None shift loss None for None artistic style transfer for videos None and None domain adaptation , and validate our visual control approach in indoor and outdoor robotics experiments."
11,10.1016/j.eswa.2017.11.011,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85035079318,robotics,15/06/2018,rl,not defined,decision making,not defined,towards a common implementation of reinforcement learning for multiple robotic tasks,"Mobile robots are increasingly being employed for performing complex tasks in dynamic environments. Those tasks can be either explicitly programmed by an engineer or learned by means of some automatic learning method, which improves the adaptability of the robot and reduces the effort of setting it up. In this sense, reinforcement learning (RL) methods are recognized as a promising tool for a machine to learn autonomously how to do tasks that are specified in a relatively simple manner. However, the dependency between these methods and the particular task to learn is a well-known problem that has strongly restricted practical implementations in robotics so far. Breaking this barrier would have a significant impact on these and other intelligent systems; in particular, having a core method that requires little tuning effort for being applicable to diverse tasks would boost their autonomy in learning and self-adaptation capabilities. In this paper we present such a practical core implementation of RL, which enables the learning process for multiple robotic tasks with minimal per-task tuning or none. Based on value iteration methods, we introduce a novel approach for action selection, called Q-biased softmax regression (QBIASSR), that takes advantage of the structure of the state space by attending the physical variables involved (e.g., distances to obstacles, robot pose, etc.), thus experienced sets of states accelerate the decision-making process of unexplored or rarely-explored states. Intensive experiments with both real and simulated robots, carried out with the software framework also introduced here, show that our implementation is able to learn different robotic tasks without tuning the learning method. They also suggest that the combination of true online SARSA(λ) (TOSL) with QBIASSR can outperform the existing RL core algorithms in low-dimensional robotic tasks. All of these are promising results towards the possibility of learning much more complex tasks autonomously by a robotic agent."
12,10.1109/mocast49295.2020.9200283,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9200283/,smart cities,09/09/2020 00:00,supervised,batch,classification,centralised,a cloud based smart recycling bin for waste classification,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. However, most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification. A centralized Information System (IS) collects measurements from smart bins that are deployed all around the city and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low."
13,10.1016/s0925-2312(01)00500-8,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/17544395043,robotics,01/06/2001,supervised,batch,classification,centralised,a biologically inspired visual system for an autonomous robot,"We have implemented an artificial visual system that takes advantage of known properties of biological systems to achieve segmentation and recognition of simple images. The use of biologically plausible mechanisms makes the system inherit a series of features that are present in biological systems, such as flexibility, robustness and adaptability. The implementation of the model on an autonomous robot has proved its reliability and robustness in real environments and shows the relevance of this kind of approach."
14,10.1016/j.microc.2020.105038,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85085341749,health,01/09/2020,not defined,not defined,classification,not defined,a smartphone-based rapid quantitative detection platform for lateral flow strip of human chorionic gonadotropin with optimized image algorithm,"Colloidal gold immunochromatographic test strip has been widely used as a rapid, simple and low-cost correct detection technology. However, its detection is often qualitative or semi-quantitative, which limits its clinical application to some extent. Herein, a portable test strip quantitative detection device based on smartphone to detect human chorionic gonadotropin (HCG) is developed. In experiment, a colloidal gold HCG detection strip based on antigen antibody immune response is constructed, and the quantitative results of three different image processing methods on the same strip detection are compared, including the threshold processing algorithm based on location information, the RGB color component extraction algorithm and the grayscale projection value processing algorithm, the results show that the last algorithm can realize the best recognition of the region of interest of strip. The mobile phone application software (App) based on this design shows that the detection limit of constructed colloidal gold HCG strip is 3 ng/mL with a linear range of 6–300 ng/mL. The detection result of real urine sample is consistent with the spiked concentration (R2 = 0.988), indicating that the concentration of HCG can be accurately measured in urine with this method, presenting the potential for instant diagnosis."
15,10.1109/tamd.2010.2086453,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/5599851/,robotics,01/12/2010 00:00,not defined,not defined,not defined,not defined,multilevel darwinist brain (mdb): artificial evolution in a cognitive architecture for real robots,"The multilevel Darwinist brain (MDB) is a cognitive architecture that follows an evolutionary approach to provide autonomous robots with lifelong adaptation. It has been tested in real robot on-line learning scenarios obtaining successful results that reinforce the evolutionary principles that constitute the main original contribution of the MDB. This preliminary work has lead to a series of improvements in the computational implementation of the architecture so as to achieve realistic operation in real time, which was the biggest problem of the approach due to the high computational cost induced by the evolutionary algorithms that make up the MDB core. The current implementation of the architecture is able to provide an autonomous robot with real time learning capabilities and the capability for continuously adapting to changing circumstances in its world, both internal and external, with minimal intervention of the designer. This paper aims at providing an overview or the architecture and its operation and defining what is required in the path towards a real cognitive robot following a developmental strategy. The design, implementation and basic operation of the MDB cognitive architecture are presented through some successful real robot learning examples to illustrate the validity of this evolutionary approach."
16,10.23919/iccas50221.2020.9268370,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9268370/,robotics,16/10/2020 00:00,supervised,batch,classification,centralised,deep reinforcement learning-based ros-controlled rc car for autonomous path exploration in the unknown environment,"Nowadays, Deep reinforcement learning has become the front runner to solve problems in the field of robot navigation and avoidance. This paper presents a LiDAR-equipped RC car trained in the GAZEBO environment using the deep reinforcement learning method. This paper uses reshaped LiDAR data as the data input of the neural architecture of the training network. This paper also presents a unique way to convert the LiDAR data into a 2D grid map for the input of training neural architecture. It also presents the test result from the training network in different GAZEBO environment. It also shows the development of hardware and software systems of embedded RC car. The hardware system includes-Jetson AGX Xavier, teensyduino and Hokuyo LiDAR; the software system includes-ROS and Arduino C. Finally, this paper presents the test result in the real world using the model generated from training simulation."
17,10.1016/j.neucom.2008.06.019,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/58149470368,robotics,01/01/2009,unsupervised,not defined,decision making,not defined,discretized iso-learning neural network for obstacle avoidance in reactive robot controllers,"Isotropic sequence order learning (ISO-learning) and its variations, input correlation only learning (ICO-learning) and ISO three-factor learning (ISO3-learning) are unsupervised neural algorithms to learn temporal differences. As robotic software operates mainly in discrete time domain, a discretization of ISO-learning is needed to apply classical conditioning to reactive robot controllers.
                  Discretization of ISO-learning is achieved by modifications to original rules: weights sign restriction, to adequate ISO-learning devices outputs to the usually predefined kinds of connections (excitatory/inhibitory) used in neural networks, and decay term in learning rate for weights stabilization. Discrete ISO-learning devices are included into neural networks used to learn simple obstacle avoidance in the reactive control of two real robots."
18,10.1109/access.2020.2970178,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8974224/,health,01/01/2020 00:00,not defined,not defined,not defined,not defined,a novel software engineering approach toward using machine learning for improving the efficiency of health systems,"Recently, machine learning has become a hot research topic. Therefore, this study investigates the interaction between software engineering and machine learning within the context of health systems. We proposed a novel framework for health informatics: the framework and methodology of software engineering for machine learning in health informatics (SEMLHI). The SEMLHI framework includes four modules (software, machine learning, machine learning algorithms, and health informatics data) that organize the tasks in the framework using a SEMLHI methodology, thereby enabling researchers and developers to analyze health informatics software from an engineering perspective and providing developers with a new road map for designing health applications with system functions and software implementations. Our novel approach sheds light on its features and allows users to study and analyze the user requirements and determine both the function of objects related to the system and the machine learning algorithms that must be applied to the dataset. Our dataset used in this research consists of real data and was originally collected from a hospital run by the Palestine government covering the last three years. The SEMLHI methodology includes seven phases: designing, implementing, maintaining and defining workflows; structuring information; ensuring security and privacy; performance testing and evaluation; and releasing the software applications."
19,10.1109/tbme.2007.909506,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/4360143/,health,01/03/2008 00:00,supervised,batch,classification,not defined,a fully automatic cad-ctc system based on curvature analysis for standard and low-dose ct data,"Computed tomography colonography (CTC) is a rapidly evolving noninvasive medical investigation that is viewed by radiologists as a potential screening technique for the detection of colorectal polyps. Due to the technical advances in CT system design, the volume of data required to be processed by radiologists has increased significantly, and as a consequence the manual analysis of this information has become an increasingly time consuming process whose results can be affected by inter- and intrauser variability. The aim of this paper is to detail the implementation of a fully integrated CAD-CTC system that is able to robustly identify the clinically significant polyps in the CT data. The CAD-CTC system described in this paper is a multistage implementation whose main system components are: 1) automatic colon segmentation; 2) candidate surface extraction; 3) feature extraction; and 4) classification. Our CAD-CTC system performs at 100% sensitivity for polyps larger than 10 mm, 92% sensitivity for polyps in the range 5 to 10 mm, and 57.14% sensitivity for polyps smaller than 5 mm with an average of 3.38 false positives per dataset. The developed system has been evaluated on synthetic and real patient CT data acquired with standard and low-dose radiation levels."
20,10.1109/icra.2016.7487617,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7487617/,robotics,21/05/2016 00:00,supervised,not defined,not defined,not defined,decentralized multi-agent exploration with online-learning of gaussian processes,"Exploration is a crucial problem in safety of life applications, such as search and rescue missions. Gaussian processes constitute an interesting underlying data model that leverages the spatial correlations of the process to be explored to reduce the required sampling of data. Furthermore, multi-agent approaches offer well known advantages for exploration. Previous decentralized multi-agent exploration algorithms that use Gaussian processes as underlying data model, have only been validated through simulations. However, the implementation of an exploration algorithm brings difficulties that were not tackle yet. In this work, we propose an exploration algorithm that deals with the following challenges: (i) which information to transmit to achieve multi-agent coordination; (ii) how to implement a light-weight collision avoidance; (iii) how to learn the data's model without prior information. We validate our algorithm with two experiments employing real robots. First, we explore the magnetic field intensity with a ground-based robot. Second, two quadcopters equipped with an ultrasound sensor explore a terrain profile. We show that our algorithm outperforms a meander and a random trajectory, as well as we are able to learn the data's model online while exploring."
21,10.1007/s10664-017-9547-8,Springer US,project-academic,project-academic,industry,01/06/2018 00:00,supervised,batch,not defined,not defined,inference of development activities from interaction with uninstrumented applications," Studying developers’ behavior in software development tasks is crucial for designing effective techniques and tools to support developers’ daily work. In modern software development, developers frequently use different applications including IDEs, Web Browsers, documentation software (such as Office Word, Excel, and PDF applications), and other tools to complete their tasks. This creates significant challenges in collecting and analyzing developers’ behavior data. Researchers usually instrument the software tools to log developers’ behavior for further studies. This is feasible for studies on development activities using specific software tools. However, instrumenting all software tools commonly used in real work settings is difficult and requires significant human effort. Furthermore, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis. This abstraction is often performed manually or based on simple heuristics. In this paper, we propose an approach to address the above two challenges in collecting and analyzing developers’ behavior data. First, we use our ActivitySpace framework to improve the generalizability of the data collection. ActivitySpace uses operating-system level instrumentation to track developer interactions with a wide range of applications in real work settings. Secondly, we use a machine learning approach to reduce the human effort to abstract low-level behavior data. Specifically, considering the sequential nature of the interaction data, we propose a Condition Random Field (CRF) based approach to segment and label the developers’ low-level actions into a set of basic, yet meaningful development activities. To validate the generalizability of the proposed data collection approach, we deploy the ActivitySpace framework in an industry partner’s company and collect the real working data from ten professional developers’ one-week work in three actual software projects. The experiment with the collected data confirms that with initial human-labeled training data, the CRF model can be trained to infer development activities from low-level actions with reasonable accuracy within and across developers and software projects. This suggests that the machine learning approach is promising in reducing the human efforts required for behavior data analysis."
22,10.1109/cbms.2019.00041,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8787393/,multimedia,07/06/2019 00:00,supervised,batch,classification,not defined,action recognition in real homes using low resolution depth video data,"We report work in progress from interdisciplinary research on Assisted Living Technology in smart homes for older adults with mild cognitive impairments or dementia. We present our field trial, the set-up for collecting and storing data from real homes, and preliminary results on action recognition using low resolution depth video cameras. The data have been collected from seven apartments with one resident each over a period of two weeks. We propose a pre-processing of the depth videos by applying an Infinite Response Filter (IIR) for extracting the movements in the frames prior to classification. In this work we classify four actions: TV interaction (turn it on/ off and switch over), standing up, sitting down, and no movement. Our first results indicate that using the IIR filter for movement information extraction improves accuracy and can be an efficient method for recognizing actions. Our current implementation uses a convolutional long short-term memory (ConvLSTM) neural network, and achieved an average peak accuracy of 86%."
23,http://arxiv.org/abs/1805.08692v1,arxiv,arxiv,http://arxiv.org/abs/1805.08692v1,smart cities,04/05/2018 00:00,supervised,batch,classification,centralised,"assessing a mobile-based deep learning model for plant disease
  surveillance","Convolutional neural network models (CNNs) have made major advances in
computer vision tasks in the last five years. Given the challenge in collecting
real world datasets, most studies report performance metrics based on available
research datasets. In scenarios where CNNs are to be deployed on images or
videos from mobile devices, models are presented with new challenges due to
lighting, angle, and camera specifications, which are not accounted for in
research datasets. It is essential for assessment to also be conducted on real
world datasets if such models are to be reliably integrated with products and
services in society. Plant disease datasets can be used to test CNNs in real
time and gain insight into real world performance. We train a CNN object
detection model to identify foliar symptoms of diseases (or lack thereof) in
cassava (Manihot esculenta Crantz). We then deploy the model on a mobile app
and test its performance on mobile images and video of 720 diseased leaflets in
an agricultural field in Tanzania. Within each disease category we test two
levels of severity of symptoms - mild and pronounced, to assess the model
performance for early detection of symptoms. In both severities we see a
decrease in the F-1 score for real world images and video. The F-1 score
dropped by 32% for pronounced symptoms in real world images (the closest data
to the training data) due to a drop in model recall. If the potential of
smartphone CNNs are to be realized our data suggest it is crucial to consider
tuning precision and recall performance in order to achieve the desired
performance in real world settings. In addition, the varied performance related
to different input data (image or video) is an important consideration for the
design of CNNs in real world applications."
24,10.1007/978-3-030-85867-4_4,Springer,springer,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85867-4_4,industry,01/01/2021 00:00,not defined,not defined,not defined,not defined,airpa: an architecture to support the execution and maintenance of ai-powered rpa robots,"Robotic Process Automation (RPA) has quickly evolved from automating simple rule-based tasks. Nowadays, RPA is required to mimic more sophisticated human tasks, thus implying its combination with Artificial Intelligence (AI) technology, i.e., the so-called intelligent RPA. Putting together RPA with AI leads to a challenging scenario since (1) it involves professionals from both fields who typically have different skills and backgrounds, and (2) AI models tend to degrade over time which affects the performance of the overall solution. This paper describes the AIRPA project, which addresses these challenges by proposing a software architecture that enables (1) the abstraction of the robot development from the AI development and (2) the monitor, control, and maintain intelligent RPA developments to ensure its quality and performance over time. The project has been conducted in the Servinform context, a Spanish consultancy firm, and the proposed prototype has been validated with reality settings. The initial experiences yield promising results in reducing AHT (Average Handle Time) in processes where AIRPA deployed cognitive robots, which encourages exploring the support of intelligent RPA development."
25,10.1109/icra40945.2020.9196540,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9196540/,robotics,31/08/2020 00:00,rl,batch,decision making,centralised,meta reinforcement learning for sim-to-real domain adaptation,"Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance."
26,10.1109/jiot.2019.2940131,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8827506/,industry,01/12/2019 00:00,not defined,not defined,not defined,not defined,a two-stage transfer learning-based deep learning approach for production progress prediction in iot-enabled manufacturing,"In make-to-order manufacturing enterprises, accurate production progress (PP) prediction is an important basis for dynamic production process optimization and on-time delivery of orders. The implementation of Internet of Things (IoT) makes it possible to take real-time production state as an important factor affecting PP. In the IoT-enabled workshop, a two-stage transfer learning-based prediction method using both historical production data and real-time state data is proposed to solve the problem of low-prediction accuracy and poor generalization performance caused by insufficient data of target order. The deep autoencoder (DAE) model with transfer learning is designed to extract the generalized features of target order in the first stage, which uses bootstrap sampling to avoid over fitting. The deep belief network (DBN) model with transfer learning is constructed to fit the nonlinear relation for PP prediction in the second stage. A real case from an IoT enabled machining workshop is taken to validate the performance of the proposed method over the other methods such as DBN, deep neural network."
27,http://arxiv.org/abs/1804.09914v1,arxiv,arxiv,http://arxiv.org/abs/1804.09914v1,multimedia,26/04/2018 00:00,supervised,batch,classification,centralised,"itelescope: intelligent video telemetry and classification in real-time
  using software defined networking","Video continues to dominate network traffic, yet operators today have poor
visibility into the number, duration, and resolutions of the video streams
traversing their domain. Current approaches are inaccurate, expensive, or
unscalable, as they rely on statistical sampling, middle-box hardware, or
packet inspection software. We present {\em iTelescope}, the first intelligent,
inexpensive, and scalable SDN-based solution for identifying and classifying
video flows in real-time. Our solution is novel in combining dynamic flow rules
with telemetry and machine learning, and is built on commodity OpenFlow
switches and open-source software. We develop a fully functional system, train
it in the lab using multiple machine learning algorithms, and validate its
performance to show over 95\% accuracy in identifying and classifying video
streams from many providers including Youtube and Netflix. Lastly, we conduct
tests to demonstrate its scalability to tens of thousands of concurrent
streams, and deploy it live on a campus network serving several hundred real
users. Our system gives unprecedented fine-grained real-time visibility of
video streaming performance to operators of enterprise and carrier networks at
very low cost."
28,10.1109/iros40897.2019.8968004,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8968004/,robotics,08/11/2019 00:00,not defined,not defined,not defined,not defined,long range neural navigation policies for the real world,"Learned Neural Network based policies have shown promising results for robot navigation. However, most of these approaches fall short of being used on a real robot due to the extensive simulated training they require. These simulations lack the visuals and dynamics of the real world, which makes it infeasible to deploy on a real robot. We present a novel Neural Net based policy, NavNet, which allows for easy deployment on a real robot. It consists of two sub policies - a high level policy which can understand real images and perform long range planning expressed in high level commands; a low level policy that can translate the long range plan into low level commands on a specific platform in a safe and robust manner. For every new deployment, the high level policy is trained on an easily obtainable scan of the environment modeling its visuals and layout. We detail the design of such an environment and how one can use it for training a final navigation policy. Further, we demonstrate a learned low-level policy. We deploy the model in a large office building and test it extensively, achieving 0.80 success rate over long navigation runs and outperforming SLAM-based models in the same settings."
29,10.1109/icit.2009.4939663,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/4939663/,autonomous vehicle,13/02/2009 00:00,supervised,batch,classification,not defined,real-time neural network based identification of a rotary-wing uav dynamics for autonomous flight,"Real time flight implementation of a neural network based black-box identification (NNID) scheme to a rotary wing unmanned aerial vehicle (RUAV) is presented in this paper. The applicability of NNID scheme for real time identification of longitudinal and lateral dynamics of the RUAV is evaluated in flight. To show the efficacy of the method for real time applications, the identification results and error statistics are provided. The challenges involved in terms of hardware implementation, computational time requirements, and real time coding are investigated and reported. Results indicate that NNID is suitable for modeling the dynamics of the RUAV in real time."
30,10.1109/aero47225.2020.9172439,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9172439/,smart cities,14/03/2020 00:00,supervised,batch,classification,not defined,"smart &amp; integrated management system - smart cities, epidemiological control tool using drones","This paper describes the development of a real application using Drones over urban regions to help the authorities at epidemiological control through a disruptive solutions based on a customizable Smart &amp; Integrated Management System (SIGI), devices and software based on the Enterprise Resource Planning (ERP) concept. Compound by management software, Drones and specific IoT devices, both referred to as sensors, the sensors collect the data of the interest areas in real time, creating a specified database. Based on the data collected from the interest areas, SIGI software has the ability to show real-time situational analysis of these areas and allows that the administrator can optimize resources (material and human) improving the efficiency of resource allocation in these areas. In addition to the development of the management software, the development of sensors to collect the information in the field and update these information to the database of the management software, are considered. The sensors will be recognized as IoT devices for the collection of meteorological data, images and command / control Drones. Initially the system will be customized, using an Artificial Intelligence tool, to collect data and identify the outbreaks of the dengue mosquito, zika and Chikungunya, nominee by risk areas. After the definition of the potential risk areas, in a complementary way, a totally customized Drone will be used to map these areas of interest, generating aerial photographs, identifying and geotagging the potential “targets”, which will allow the agents to identify potential mosquito breeding sites. After the identification of breeding areas, the next step will be the effective combat of the vectors, using the Drones to fly over the areas of interest, where biological defenses will be “dropped” over the targets to combat mosquitoes. Due some Drone flight restrictions over the cities, the whole process will be monitored by a situation room, that will be able to control the Drone remotely, access the air space controller, reads the sensors installed in the city (field), that will measure, for example, rainfall through weather stations installed in risk areas and subsequently processed by Intelligent System Integrated Management (SIGI), which will result to the information public official reflecting the situational analysis of the areas, which will enable a better management of available resources, helping the public agent, preventively in the decision making."
31,10.1109/tmscs.2015.2513741,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7368929/,multimedia,01/12/2015 00:00,supervised,not defined,not defined,not defined,"an ultra-low power, “always-on” camera front-end for posture detection in body worn cameras using restricted boltzman machines","The Internet of Things (IoTs) has triggered rapid advances in sensors, surveillance devices, wearables and body area networks with advanced Human-Computer Interfaces (HCI). One such application area is the adoption of Body Worn Cameras (BWCs) by law enforcement officials. The need to be ‘always-on’ puts heavy constraints on battery usage in these camera front-ends, thus limiting their widespread adoption. Further, the increasing number of such cameras is expected to create a data deluge, which requires large processing, transmission and storage capabilities. Instead of continuously capturing and streaming or storing videos, it is prudent to provide “smartness” to the camera front-end. This requires hardware assisted image recognition and template matching in the front-end, capable of making judicious decisions on when to trigger video capture or streaming. Restricted Boltzmann Machines (RBMs) based neural networks have been shown to provide high accuracy for image recognition and are well suited for low power and re-configurable systems. In this paper we propose an RBM based “always-on’’ camera front-end capable of detecting human posture. Aggressive behavior of the human being in the field of view will be used as a wake-up signal for further data collection and classification. The proposed system has been implemented on a Xilinx Virtex 7 XC7VX485T platform. A minimum dynamic power of 19.18 mW for a target recognition accuracy while maintaining real time constraints has been measured. The hardware-software co-design illustrates the trade-offs in the design with respect to accuracy, resource utilization, processing time and power. The results demonstrate the possibility of a true “always-on” body-worn camera system in the IoT environment."
32,,,core,,robotics,01/01/2009 00:00,not defined,not defined,decision making,not defined,research article bootstrap learning and visual processing management on mobile robots,"which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. A central goal of robotics and AI is to enable a team of robots to operate autonomously in the real world and collaborate with humans over an extended period of time. Though developments in sensor technology have resulted in the deployment of robots in specific applications the ability to accurately sense and interact with the environment is still missing. Key challenges to the widespread deployment of robots include the ability to learn models of environmental features based on sensory inputs, bootstrap off of the learned models to detect and adapt to environmental changes, and autonomously tailor the sensory processing to the task at hand. This paper summarizes a comprehensive effort towards such bootstrap learning, adaptation, and processing management using visual input. We describe probabilistic algorithms that enable a mobile robot to autonomously plan its actions to learn models of color distributions and illuminations. The learned models are used to detect and adapt to illumination changes. Furthermore, we describe a probabilistic sequential decision-making approach that autonomously tailors the visual processing to the task at hand. All algorithms are fully implemented and tested on robot platforms in dynamic environments. 1"
33,10.1016/j.eswa.2012.01.059,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/84858339895,science,01/07/2012,not defined,not defined,decision making,not defined,analyzing the solutions of dea through information visualization and data mining techniques: smartdea framework,"Data envelopment analysis (DEA) has proven to be a useful tool for assessing efficiency or productivity of organizations, which is of vital practical importance in managerial decision making. DEA provides a significant amount of information from which analysts and managers derive insights and guidelines to promote their existing performances. Regarding to this fact, effective and methodologic analysis and interpretation of DEA results are very critical. The main objective of this study is then to develop a general decision support system (DSS) framework to analyze the results of basic DEA models. The paper formally shows how the results of DEA models should be structured so that these solutions can be examined and interpreted by analysts through information visualization and data mining techniques effectively. An innovative and convenient DEA solver, SmartDEA, is designed and developed in accordance with the proposed analysis framework. The developed software provides DEA results which are consistent with the framework and are ready-to-analyze with data mining tools, thanks to their specially designed table-based structures. The developed framework is tested and applied in a real world project for benchmarking the vendors of a leading Turkish automotive company. The results show the effectiveness and the efficacy of the proposed framework."
34,10.1109/access.2018.2873597,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8490234/,health,01/01/2018 00:00,supervised,batch,decision making,not defined,hierarchical semantic mapping using convolutional neural networks for intelligent service robotics,"The introduction of service robots in the public domain has introduced a paradigm shift in how robots are interacting with people, where robots must learn to autonomously interact with the untrained public instead of being directed by trained personnel. As an example, a hospital service robot is told to deliver medicine to Patient Two in Ward Three. Without awareness of what “Patient Two” or “Ward Three” is, a service robot must systematically explore the environment to perform this task, which requires a long time. The implementation of a Semantic Map allows for robots to perceive the environment similar to people by associating semantic information with spatial information found in geometric maps. Currently, many semantic mapping works provide insufficient or incorrect semantic-metric information to allow a service robot to function dynamically in human-centric environments. This paper proposes a semantic map with a hierarchical semantic organization structure based on a hybrid metric-topological map leveraging convolutional neural networks and spatial room segmentation methods. Our results are validated using multiple simulated and real environments on our lab's custom developed mobile service robot and demonstrate an application of semantic maps by providing only vocal commands. We show that this proposed method provides better capabilities in terms of semantic map labeling and retain multiple levels of semantic information."
35,10.3233/thc-140783,'IOS Press',core,,robotics,01/01/2014 00:00,not defined,not defined,not defined,not defined,cardiopulmonary performance testing using a robotics-assisted tilt table: feasibility assessment in able-bodied subjects,"BACKGROUND:

Robotics-assisted tilt table technology was introduced for early rehabilitation of neurological patients. It provides cyclical stepping movement and physiological loading of the legs. The aim of the present study was to assess the feasibility of this type of device for peak cardiopulmonary performance testing using able-bodied subjects.

METHODS:

A robotics-assisted tilt table was augmented with force sensors in the thigh cuffs and a work rate estimation algorithm. A custom visual feedback system was employed to guide the subjects' work rate and to provide real time feedback of actual work rate. Feasibility assessment focused on: (i) implementation (technical feasibility), and (ii) responsiveness (was there a measurable, high-level cardiopulmonary reaction?). For responsiveness testing, each subject carried out an incremental exercise test to the limit of functional capacity with a work rate increment of 5 W/min in female subjects and 8 W/min in males.

RESULTS:

11 able-bodied subjects were included (9 male, 2 female; age 29.6 ± 7.1 years: mean ± SD). Resting oxygen uptake (O_{2}) was 4.6 ± 0.7 mL/min/kg and O_{2}peak was 32.4 ± 5.1 mL/min/kg; this mean O_{2}peak was 81.1% of the predicted peak value for cycle ergometry. Peak heart rate (HRpeak) was 177.5 ± 9.7 beats/min; all subjects reached at least 85% of their predicted HRpeak value. Respiratory exchange ratio (RER) at O_{2}peak was 1.02 ± 0.07. Peak work rate) was 61.3 ± 15.1 W. All subjects reported a Borg CR10 value for exertion and leg fatigue of 7 or more.

CONCLUSIONS:

The robotics-assisted tilt table is deemed feasible for peak cardiopulmonary performance testing: the approach was found to be technically implementable and substantial cardiopulmonary responses were observed. Further testing in neurologically-impaired subjects is warranted"
36,'edp sciences',10.1051/matecconf/201925501003,core,,robotics,01/01/2019 00:00,supervised,not defined,classification,hybrid,automatic image annotation for small and ad hoc intelligent applications using raspberry pi,"The cutting-edge technology Machine Learning (ML) is successfully applied for Business Intelligence. Among the various pre-processing steps of ML, Automatic Image Annotation (also known as automatic image tagging or linguistic indexing) is the process in which a computer system automatically assigns metadata in the form of captioning or keywords to a digital image. Automatic Image Annotation (AIA) methods (which have appeared during the last several years) make a large use of many ML approaches. Clustering and classification methods are most frequently applied to annotate images. In addition, these proposed solutions require a high computational infrastructure. However, certain real-time applications (small and ad-hoc intelligent applications) for example, autonomous small robots, gadgets, drone etc. have limited computational processing capacity. These small and ad-hoc applications demand a more dynamic and portable way to automatically annotate data and then perform ML tasks (Classification, clustering etc.) in real time using limited computational power and hardware resources. Through a comprehensive literature study we found that most image pre-processing algorithms and ML tasks are computationally intensive, and it can be challenging to run them on an embedded platform with acceptable frame rates. However, Raspberry Pi is sufficient for AIA and ML tasks that are relevant to small and ad-hoc intelligent applications. In addition, few critical intelligent applications (which require high computational resources, for example, Deep Learning using huge dataset) are only feasible to run on more powerful hardware resources. In this study, we present the framework of “Automatic Image Annotation for Small and Ad-hoc Intelligent Application using Raspberry Pi” and propose the low-cost infrastructures (single node and multi node using Raspberry Pi) and software module (for Raspberry Pi) to perform AIA and ML tasks in real time for small and ad-hoc intelligent applications. The integration of both AIA and ML tasks in a single software module (with in Raspberry Pi) is challenging. This study will helpful towards the improvement in various practical applications areas relevant to small intelligent autonomous systems"
37,10.1109/iseee48094.2019.9136152,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9136152/,multimedia,20/10/2019 00:00,not defined,not defined,classification,not defined,compact isolated speech recognition on raspberry-pi based on reaction diffusion transform,"A low complexity solution for speech recognition is proposed and its implementation on a resources constrained platform, namely the Raspberry-Pi is evaluated. In order to achieve good performance with limited resources, both the feature extractor and the classifier are specially designed. A special form of feature extractor, called RDT (reaction-diffusion transform) was optimized and evaluated in conjunction with a specially designed ELM (extreme learning machine) classifier. Optimization of parameters led to very good recognition rates (up to 100%) for a small dictionary of isolated sounds representing vocal commands for automotive applications. The real time factor is sub-unitary, ensuring the realization of real time identification using the proposed method."
38,10.1109/tnsm.2021.3085097,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9444314/,multimedia,01/09/2021 00:00,supervised,not defined,classification,not defined,seta++: real-time scalable encrypted traffic analytics in multi-gbps networks,"The security and privacy of the end-users are a few of the most important components of a communication network. Though end-to-end encryption (e.g., TLS/SSL) fulfils this requirement, it makes inspecting network traffic with legacy solutions such as Deep Packet Inspection difficult. Recent Machine Learning techniques have shown outstanding performance in encrypted traffic classification. Nevertheless, such approaches require efficient flow sampling at real enterprise-scale networks due to the sheer volume of transferred data. Through this paper, we propose a holistic architecture to extract flow information of encrypted data at multi Gbps line rate using sampling and sketching mechanisms, enabling network operators to estimate flow size distribution accurately and understand the behavior of VPN-obfuscated traffic. Using over 6000 video traffic traces, under three main evaluation scenarios based on trace duration and starting time point, we show that it is possible to achieve 99% accuracy for service provider classification and over 90% accuracy for content classification for a given service provider in the best case. We also deploy our solution at an operational enterprise-scale network leveraging kernel bypassing to demonstrate its capability to efficiently sample live traffic for analytics."
39,10.1109/tns.2014.2309254,IEEE,project-academic,project-academic,industry,,supervised,batch,classification,not defined,implementation of the disruption predictor apodis in jet s real time network using the marte framework," The evolution in the past years of Machine learning techniques, as well as the technological evolution of computer architectures and operating systems, are enabling new approaches for complex problems in different areas of industry and research, where a classical approach is nonviable due to lack of knowledge of the problem's nature. A typical example of this situation is the prediction of plasma disruptions in Tokamak devices. This paper shows the implementation of a real time disruption predictor. The predictor is based on a support vector machine (SVM). The implementation was done under the MARTe framework on a six core x86 architecture. The system is connected in JET's Real time Data Network (RTDN). Online results show a high degree of successful predictions and a low rate of false alarms thus, confirming its usefulness in a disruption mitigation scheme. The implementation shows a low computational load, which in an immediate future will be exploited to increase the prediction's temporal resolution."
40,http://arxiv.org/abs/1411.3895v1,arxiv,arxiv,http://arxiv.org/abs/1411.3895v1,smart cities,14/11/2014 00:00,not defined,not defined,not defined,not defined,"learning fuzzy controllers in mobile robotics with embedded
  preprocessing","The automatic design of controllers for mobile robots usually requires two
stages. In the first stage,sensorial data are preprocessed or transformed into
high level and meaningful values of variables whichare usually defined from
expert knowledge. In the second stage, a machine learning technique is applied
toobtain a controller that maps these high level variables to the control
commands that are actually sent tothe robot. This paper describes an algorithm
that is able to embed the preprocessing stage into the learningstage in order
to get controllers directly starting from sensorial raw data with no expert
knowledgeinvolved. Due to the high dimensionality of the sensorial data, this
approach uses Quantified Fuzzy Rules(QFRs), that are able to transform
low-level input variables into high-level input variables, reducingthe
dimensionality through summarization. The proposed learning algorithm, called
Iterative QuantifiedFuzzy Rule Learning (IQFRL), is based on genetic
programming. IQFRL is able to learn rules with differentstructures, and can
manage linguistic variables with multiple granularities. The algorithm has been
testedwith the implementation of the wall-following behavior both in several
realistic simulated environmentswith different complexity and on a Pioneer 3-AT
robot in two real environments. Results have beencompared with several
well-known learning algorithms combined with different data
preprocessingtechniques, showing that IQFRL exhibits a better and statistically
significant performance. Moreover,three real world applications for which IQFRL
plays a central role are also presented: path and objecttracking with static
and moving obstacles avoidance."
41,10.1109/robot.2000.844768,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/844768/,multimedia,28/04/2000 00:00,not defined,not defined,not defined,not defined,application of automatic action planning for several work cells to the german ets-vii space robotics experiments,"Experiences in space robotics show, that the user normally has to cope with a huge amount of data. So, only robot and mission specialists are able to control the robot arm directly in teleoperation mode. By means of an intelligent robot control in cooperation with virtual reality methods, it is possible for non-robot specialists to generate tasks for a robot or an automation component intuitively. Furthermore, the intelligent robot control improves the safety of the entire system. The on-ground robot control and command station for the robot arm ERA onboard the satellite ETS-VII builds on a new resource-based action planning approach to manage robot manipulators and other automation components. In the case of ERA, the action planning system also takes care of the ""real"" robot onboard the satellite and the ""virtual"" robot in the simulation system. By means of the simulation system, the user can plan tasks ahead as well as analyze and visualize different strategies. The paper describes the mechanism of resource-based action planning, its application to different work cells, the practical experiences gained from the implementation for the on-ground robot control and command station for the robot arm ERA developed in the GETEX project as well as the services it provides to support VR-based man machine interfaces."
42,10.1109/cnna.2010.5430245,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/5430245/,multimedia,05/02/2010 00:00,not defined,not defined,not defined,decentralised,a multi-fpga distributed embedded system for the emulation of multi-layer cnns in real time video applications,"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates."
43,http://arxiv.org/abs/1802.08960v2,arxiv,arxiv,http://arxiv.org/abs/1802.08960v2,multimedia,25/02/2018 00:00,supervised,not defined,not defined,not defined,"bonnet: an open-source training and deployment framework for semantic
  segmentation in robotics using cnns","The ability to interpret a scene is an important capability for a robot that
is supposed to interact with its environment. The knowledge of what is in front
of the robot is, for example, relevant for navigation, manipulation, or
planning. Semantic segmentation labels each pixel of an image with a class
label and thus provides a detailed semantic annotation of the surroundings to
the robot. Convolutional neural networks (CNNs) are popular methods for
addressing this type of problem. The available software for training and the
integration of CNNs for real robots, however, is quite fragmented and often
difficult to use for non-experts, despite the availability of several
high-quality open-source frameworks for neural network implementation and
training. In this paper, we propose a tool called Bonnet, which addresses this
fragmentation problem by building a higher abstraction that is specific for the
semantic segmentation task. It provides a modular approach to simplify the
training of a semantic segmentation CNN independently of the used dataset and
the intended task. Furthermore, we also address the deployment on a real
robotic platform. Thus, we do not propose a new CNN approach in this paper.
Instead, we provide a stable and easy-to-use tool to make this technology more
approachable in the context of autonomous systems. In this sense, we aim at
closing a gap between computer vision research and its use in robotics
research. We provide an open-source codebase for training and deployment. The
training interface is implemented in Python using TensorFlow and the deployment
interface provides a C++ library that can be easily integrated in an existing
robotics codebase, a ROS node, and two standalone applications for label
prediction in images and videos."
44,,Delft University of Technology,core,,industry,,rl,not defined,not defined,not defined,ananke: a q-learning-based portfolio scheduler for complex industrial workflows: technical report ds-2017-001,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduces operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for datacenters with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Reinforcement learning, based in this work on Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learningbased portfolio scheduler can perform better (5–20%) and cost less (20–35%) than the considered alternatives.Distributed System"
45,10.1109/jsen.2020.3041668,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9274482/,health,01/03/2021 00:00,not defined,not defined,not defined,not defined,design of a pose and force controller for a robotized ultrasonic probe based on neural networks and stochastic gradient approximation,"In medicine and engineering, the implementation of a diagnostic test using an ultrasonic sensor requires suitable contact conditions, and a correct pose to attain the best signal transmission settings. A soft sensor probe provides a good surface adaptation and forces transfer, but it introduces nonlinearities and noisy measurements, making it difficult to control the probe during a real time test by conventional algorithms. In this work, a data driven controller is developed to control force and pose of a soft contact ultrasound sensor. The adaptive controller is based on a fuzzy-rules emulated network structure with the learning algorithm using a stochastic gradient approximation. The proposed control algorithm overcomes the noise environment conditions and nonlinearities of the unknown nonlinear discrete-time system. This was numerically validated and then, experimentally tested with an industrial robotic system using an ultrasonic probe designed in our lab. The results show that the proposed controller performs well under the contact-force regulation and can find the correct contact orientation with a fast convergence."
44,10.1007/978-3-030-77070-9_10,Springer,springer,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-77070-9_10,robotics,01/01/2021 00:00,not defined,not defined,not defined,not defined,smart and intelligent chatbot assistance for future industry 4.0,"Chatbot is an implementation of artificial intelligence (AI) technology that is used to interact with human beings and make them feel like they are talking to the real person, and the chatbot helps them to solve their queries. A chatbot can provide 24 × 7 customer support so that the customer may have a good service experience by any organization. Chatbot helps to resolve the queries and respond to the questions of users. The user is providing the input to the chatbot first, and then, the same input will be processed further; this input can be in the form of text or voice. Therefore, on the basis of the given input and after processing it, the chatbot application will generate the response to the user, and the same response will be the best answer found by the chat application. This response can be in any format like text or a voice output. In this chapter, various approaches of chatbots and how they interact with users are discussed. The proposed approach is also defined using Dialogflow, and it can be accessible through mobile phones, laptops, and portable devices. Chatbots such as Facebook chatbot, WeChat chatbot, Hike chatbot called Natasha, etc. are available in the marker and will respond on the basis of their local databases (DBs). In the proposed method, the focus will be on the scalability, user interactivity, and flexibility of the system, which can be provided by adding both local and Web databases due to which our system will be more fast and accurate. Chatbot uses unification of emerging technologies like machine learning and artificial intelligence. The motive of this chapter is to improve the chatbot system to support and scale businesses and industry domain and maintain relations with customers."
45,10.1016/j.ress.2019.106700,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85073997788,robotics,01/03/2020,not defined,not defined,not defined,not defined,optimizing inspection routes in pipeline networks,"Maintaining an aging network is a challenge for many water utilities due to limited budgets and uncertainty surrounding the physical condition of buried pipeline assets. The deployment of robotic inspections provides high quality data, but these platforms have limited use due to cost and operational constraints. To facilitate cost-efficient inspections, operators need to identify high-risk assets while accounting for the effectiveness of the tools at hand. This paper addresses inspection planning with the goal of finding an optimal route while considering tool limitations. An exact integer programming formulation is presented where only three factors are used to characterize tool constraints. Two classes of solution methods are explored: 1) tree based searches, and 2) integer programming. This paper demonstrates how each method can be used to identify optimal paths within a real water distribution system. Empirical trials suggest that tree-based search methods are the most efficient when the path limit is short, but do not scale well when the path length increases. In contrast, integer-programming methods are more effective for longer path lengths but have scalability issues for large network sizes. Data preprocessing, where the input network size is reduced, can provide large computation time reductions while returning near-optimal solutions."
46,10.1109/bigdata.2018.8621926,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8621926/,science,13/12/2018 00:00,not defined,not defined,not defined,not defined,harnessing the nature of spam in scalable online social spam detection,"Disinformation in social networks has been a worldwide problem. Social users are surrounded by a huge volume of malicious links, biased comments, fake reviews, or fraudulent advertisements, etc. Traditional spam detection approaches propose a variety of statistical feature-based models to filter out social spam from a historical dataset. However, they omit the real word situation of social data, that is, social spam is fast changing with new topics or events. Therefore, traditional approaches cannot effectively achieve online detection of the ""drifting"" social spam with a fixed statistic feature set. In this paper, we present Sifter, a system which can detect online social spam in a scalable manner without the labor-intensive feature engineering. The Sifter system is two-fold: (1) a decentralized DHT-based overlay deployment for harnessing the group characteristics of social spam activities within a specific topic/event; (2) a social spam processing with the support of Recurrent Neural Network (RNN) to get rid of the traditional manual feature engineering. Results show that Sifter achieves graceful spam detection performances with the minimal size of data and good balance in group management."
47,10.1016/j.neucom.2012.04.033,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/84875966713,industry,03/06/2013,not defined,not defined,not defined,not defined,applying soft computing techniques to optimise a dental milling process,"This study presents a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. This novel intelligent procedure is based on the following phases. Firstly, a neural model extracts the internal structure and the relevant features of the data set representing the system. Secondly, the dynamic system performance of different variables is specifically modelled using a supervised neural model and identification techniques. This constitutes the model for the fitness function of the production process, using relevant features of the data set. Finally, a genetic algorithm is used to optimise the machine parameters from a non parametric fitness function. The proposed novel approach was tested under real dental milling processes using a high-precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study."
48,,"Unsupervised Learning for Subterranean Junction Recognition Based on 2D
  Point Cloud",core,,multimedia,07/06/2020 00:00,unsupervised,not defined,clustering,not defined,http://arxiv.org/abs/2006.04225,"This article proposes a novel unsupervised learning framework for detecting
the number of tunnel junctions in subterranean environments based on acquired
2D point clouds. The implementation of the framework provides valuable
information for high level mission planners to navigate an aerial platform in
unknown areas or robot homing missions. The framework utilizes spectral
clustering, which is capable of uncovering hidden structures from connected
data points lying on non-linear manifolds. The spectral clustering algorithm
computes a spectral embedding of the original 2D point cloud by utilizing the
eigen decomposition of a matrix that is derived from the pairwise similarities
of these points. We validate the developed framework using multiple data-sets,
collected from multiple realistic simulations, as well as from real flights in
underground environments, demonstrating the performance and merits of the
proposed methodology"
49,10.1109/lra.2018.2799741,,project-academic,project-academic,robotics,09/09/2017 00:00,supervised,batch,not defined,not defined,how to train a cat learning canonical appearance transformations for direct visual localization under illumination change," Direct visual localization has recently enjoyed a resurgence in popularity with the increasing availability of cheap mobile computing power. The competitive accuracy and robustness of these algorithms compared to state-of-the-art feature-based methods, as well as their natural ability to yield dense maps, makes them an appealing choice for a variety of mobile robotics applications. However, direct methods remain brittle in the face of appearance change due to their underlying assumption of photometric consistency, which is commonly violated in practice. In this paper, we propose to mitigate this problem by training deep convolutional encoder-decoder models to transform images of a scene such that they correspond to a previously-seen canonical appearance. We validate our method in multiple environments and illumination conditions using high-fidelity synthetic RGB-D datasets, and integrate the trained models into a direct visual localization pipeline, yielding improvements in visual odometry (VO) accuracy through time-varying illumination conditions, as well as improved metric relocalization performance under illumination change, where conventional methods normally fail. We further provide a preliminary investigation of transfer learning from synthetic to real environments in a localization context. An open-source implementation of our method using PyTorch is available at this https URL."
50,10.1109/vlsid51830.2021.00035,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9407373/,smart cities,24/02/2021 00:00,supervised,batch,classification,not defined,binary neural network based real time emotion detection on an edge computing device to detect passenger anomaly,"Passenger safety in public transportation especially while riding in the form of shared cabs, and taxis are often ignored, and not much preventive protocols are devised. In the connected mobility world, emotion recognition from facial expressions is a possibility, however a faster processing and edge computing device to derive anomaly state inferences will be apt for further notifying about the safety of the passenger. FPGA implementation is a viable approach to not only implement in the embedded system automotive electronics, but also accelerate the inference results, hence making it as an ideal real time candidate for passenger anomaly state identification. For the same, a real time emotion detection system using facial features was implemented on FPGA. A Binary Neural Network (BNN) feeded by Local Binary Pattern (LBP) output was designed towards the development of an improved and faster emotion recognition system. LBP is configured as a preprocessing step to extract facial features that is passed on to the BNN layer for successful inference. The preprocessing method utilizes Viola-Jones (VJ) algorithm to extract facial data while removing other background information from the image. The LBP-BNN network is modelled using Facial Expression 2013 (FER-2013) data set for training. The custom hardware accelerator or the overlay is synthesized and the designed IP is implemented on FPGA for the inference. Inference is done using the trained model on FPGA to enable faster classified results. Emotion detection using facial expressions is classified to six states namely: angry, disgust, fear, happy, sad, and surprise. The LBP-BNN network is implemented in FPGA, to realize a real time facial emotion recognition by capturing the image of a person from a web camera interfaced to the FPGA acting as edge computing inference device, with acceptable accuracy. The image processing based emotion detection design is highly suitable for other applications including tracking of emotions for movement disorder patients in hospitals."
51,10.1016/j.cie.2019.06.040,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85067600850,science,01/09/2019,not defined,not defined,not defined,centralised,"bernard, an energy intelligent system for raising residential users awareness","Energy efficiency is still a hot topic today. Coming roughly the 25% of the energy consumption in EU from the residential sector, very few cheap and simple tools to promote energy efficiency in home users have been developed. The purpose of this paper is to present Bernard, a concept proof designed for filling this gap. This aims that householders become aware of their energy habits and have useful information that help them to redirect their consumption pattern. To achieve these goals, Bernard offers, through a mobile application, the home energy consumption monitoring in real time, the energy price forecast for the next hour and the appliances which are switched on, among others. Furthermore, it is important to highlight that the system has been designed with the premises of being cheap, non-intrusive, reliable and easily scalable, in order that utilities can gradually deploy and provide it to their customers, gaining at the same time valuable information for decision making and improving its corporate social image. Therefore, the adopted solution is based on a real time streaming data architecture suitable for handling huge volumes of data and applying predictive techniques on a cloud-computing environment. The paper provides a detailed description of the system and experimental results evaluating the performance of the predictive modules built. As case study, REFIT and REDD datasets were used."
52,10.1109/icac.2017.21,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8005354/,health,21/07/2017 00:00,rl,not defined,not defined,not defined,ananke: a q-learning-based portfolio scheduler for complex industrial workflows,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduced operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for data centers with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learning-based portfolio scheduler can perform better and consume fewer resources than state-of-the-art alternatives, in particular for workloads with uniform arrival patterns."
53,10.1109/fdl53530.2021.9568376,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9568376/,industry,10/09/2021 00:00,not defined,not defined,not defined,not defined,a container-based design methodology for robotic applications on kubernetes edge-cloud architectures,"Programming modern Robots' missions and behavior has become a very challenging task. The always increasing level of autonomy of such platforms requires the integration of multi-domain software applications to implement artificial intelligence, cognition, and human-robot/robot-robot interaction applications. In addition, to satisfy both functional and nonfunctional requirements such as reliability and energy efficiency, robotic SW applications have to be properly developed to take advantage of heterogeneous (Edge-Fog-Cloud) architectures. In this context, containerization and orchestration are becoming a standard practice as they allow for better information flow among different network levels as well as increased modularity in the use of software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de-facto development standards (i.e., robotic operating system - ROS - compliancy for robotic applications) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The design methodology allows for (i) integration and verification of multi-domain components since early in the design flow, (ii) task-to-container mapping techniques to guarantee minimum overhead in terms of performance and memory footprint, and (iii) multi-domain verification of functional and non-functional constraints before deployment. We present the results obtained in a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. The source code of the mobile robot is publicly available on GitHub."
54,,USENIX Association,project-academic,project-academic,multimedia,27/03/2017 00:00,not defined,not defined,not defined,centralised,pytheas enabling data driven quality of experience optimization using group based exploration exploitation," Content providers are increasingly using data-driven mechanisms to optimize quality of experience (QoE). Many existing approaches formulate this process as a prediction problem of learning optimal decisions (e.g., server, bitrate, relay) based on observed QoE of recent sessions. While prediction-based mechanisms have shown promising QoE improvements, they are necessarily incomplete as they: (1) suffer from many known biases (e.g., incomplete visibility) and (2) cannot respond to sudden changes (e.g., load changes). Drawing a parallel from machine learning, we argue that data-driven QoE optimization should instead be cast as a real-time exploration and exploitation (E2) process rather than as a prediction problem. Adopting E2 in network applications, however, introduces key architectural (e.g., how to update decisions in real time with fresh data) and algorithmic (e.g., capturing complex interactions between session features vs. QoE) challenges. We present Pytheas, a framework which addresses these challenges using a group-based E2 mechanism. The insight is that application sessions sharing the same features (e.g., IP prefix, location) can be grouped so that we can run E2 algorithms at a per-group granularity. This naturally captures the complex interactions and is amenable to realtime control with fresh measurements. Using an end-to-end implementation and a proof-of-concept deployment in CloudLab, we show that Pytheas improves video QoE over a state-of-the-art prediction-based system by up to 31% on average and 78% on 90th percentile of persession QoE."
55,10.1016/j.future.2020.04.018,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85083299223,science,01/09/2020,not defined,not defined,not defined,centralised,software-defined network for end-to-end networked science at the exascale,"Domain science applications and workflow processes are currently forced to view the network as an opaque infrastructure into which they inject data and hope that it emerges at the destination with an acceptable Quality of Experience. There is little ability for applications to interact with the network to exchange information, negotiate performance parameters, discover expected performance metrics, or receive status/troubleshooting information in real time. The work presented here is motivated by a vision for a new smart network and smart application ecosystem that will provide a more deterministic and interactive environment for domain science workflows. The Software-Defined Network for End-to-end Networked Science at Exascale (SENSE) system includes a model-based architecture, implementation, and deployment which enables automated end-to-end network service instantiation across administrative domains. An intent based interface allows applications to express their high-level service requirements, an intelligent orchestrator and resource control systems allow for custom tailoring of scalability and real-time responsiveness based on individual application and infrastructure operator requirements. This allows the science applications to manage the network as a first-class schedulable resource as is the current practice for instruments, compute, and storage systems. Deployment and experiments on production networks and testbeds have validated SENSE functions and performance. Emulation based testing verified the scalability needed to support research and education infrastructures. Key contributions of this work include an architecture definition, reference implementation, and deployment. This provides the basis for further innovation of smart network services to accelerate scientific discovery in the era of big data, cloud computing, machine learning and artificial intelligence."
56,10.1016/j.knosys.2018.04.015,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85045849458,autonomous vehicle,01/08/2018,not defined,not defined,not defined,not defined,teaching a vehicle to autonomously drift: a data-based approach using neural networks,"This paper presents a novel approach to teach a vehicle how to drift, in a similar manner that professional drivers do. Specifically, a hybrid structure formed by a Model Predictive Controller and feedforward Neural Networks is employed for this purpose. The novelty of this work lies in a) the adoption of a data-based approach to achieve autonomous drifting along a wide range of road radii and body slip angles, and b) in the implementation of a road terrain classifier to adjust the system actuation depending on the current friction characteristics. The presented drift control system is implemented in a multi-actuated ground vehicle equipped with active front steering and in-wheel electric motors and trained to drift by a real test driver using a driver-in-the-loop setup. Its performance is verified in the simulation environment IPG-CarMaker through different open loop and path following drifting manoeuvres."
57,10.1109/ijcnn48605.2020.9207332,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9207332/,robotics,24/07/2020 00:00,not defined,not defined,not defined,not defined,deep reinforcement learning control of hand-eye coordination with a software retina,"Deep Reinforcement Learning (DRL) has gained much attention for solving robotic hand-eye coordination tasks from raw pixel values. Despite promising results, training agents using images is hardware intensive often requiring millions of training steps to converge incurring long training times and increased risk of wear and tear on the robot. To speed up training, images are often cropped and downscaled resulting in a smaller field of view and loss of valuable high-frequency data. In this paper, we propose training the vision system using supervised learning prior to training robotic actuation using Deep Deterministic Policy Gradient (DDPG). The vision system uses a software retina, based on the mammalian retino-cortical transform, to preprocess full-size images to compress image data while preserving the full field of view and high-frequency visual information around the fixation point prior to processing by a Deep Convolutional Neural Network (DCNN) to extract visual state information. Using the vision system to preprocess the environment improves the agent's sample complexity and network update speed leading to significantly faster training with reduced image data loss. Our method is used to train a DRL system to control a real Baxter robot's arm, processing full-size images captured by an in-wrist camera to locate an object on a table and centre the camera over it by actuating the robot arm."
58,10.1109/iceccme52200.2021.9591113,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9591113/,health,08/10/2021 00:00,not defined,not defined,not defined,not defined,cobots for fintech,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested &amp; validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space."
59,10.1109/icra.2016.7487351,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7487351/,robotics,21/05/2016 00:00,supervised,batch,not defined,not defined,object discovery and grasp detection with a shared convolutional neural network,"Grasp an object from a stack of objects in real-time is still a challenge in robotics. This requires the robot to have the ability of both fast object discovery and grasp detection: a target object should be picked out from the stack first and then a proper grasp configuration is applied to grasp the object. In this paper, we propose a shared convolutional neural network (CNN) which can simultaneously implement these two tasks in real-time. The processing speed of the model is about 100 frames per second on a GPU which largely satisfies the requirement. Meanwhile, we also establish a labeled RGBD dataset which contains scenes of stacked objects for robotic grasping. At last, we demonstrate the implementation of our shared CNN model on a real robotic platform and show that the robot can accurately discover a target object from the stack and successfully grasp it."
60,10.1109/tnsm.2019.2929511,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8765778/,multimedia,01/09/2019 00:00,supervised,batch,classification,not defined,itelescope: softwarized network middle-box for real-time video telemetry and classification,"Video continues to dominate network traffic, yet operators today have poor visibility into the number, duration, and resolutions of the video streams traversing their domain. Current monitoring approaches are inaccurate, expensive, or unscalable, as they rely on statistical sampling, middle-box hardware, or packet inspection software. We present iTelescope, the first intelligent, inexpensive, and scalable softwarized network middle-box solution for identifying and classifying video flows in realtime. Our solution is novel in combining dynamic flow rules with telemetry and machine learning, and is built on commodity OpenFlow switches and open-source software. We develop a fully functional system, train it in the lab using multiple machine learning algorithms, and validate its performance to show over 95% accuracy in identifying and classifying video streams from many providers, including YouTube and Netflix. Lastly, we conduct tests to demonstrate its scalability to tens of thousands of concurrent streams, and deploy it live on a campus network serving several hundred real users. Our traffic monitoring system gives unprecedented fine-grained real-time visibility of video streaming performance to operators of enterprise and carrier networks at very low cost."
61,10.1016/j.robot.2021.103891,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85116222985,robotics,01/12/2021,supervised,batch,not defined,not defined,hybrid autonomous controller for bipedal robot balance with deep reinforcement learning and pattern generators[formula presented],"Recovering after an abrupt push is essential for bipedal robots in real-world applications within environments where humans must collaborate closely with robots. There are several balancing algorithms for bipedal robots in the literature, however most of them either rely on hard coding or power-hungry algorithms. We propose a hybrid autonomous controller that hierarchically combines two separate, efficient systems, to address this problem. The lower-level system is a reliable, high-speed, full state controller that was hardcoded on a microcontroller to be power efficient. The higher-level system is a low-speed reinforcement learning controller implemented on a low-power onboard computer. While one controller offers speed, the other provides trainability and adaptability. An efficient control is then formed without sacrificing adaptability to new dynamic environments. Additionally, as the higher-level system is trained via deep reinforcement learning, the robot could learn after deployment, which is ideal for real-world applications. The system’s performance is validated with a real robot recovering after a random push in less than 5 s, with minimal steps from its initial positions. The training was conducted using simulated data."
62,,,project-academic,project-academic,industry,01/01/2006 00:00,supervised,not defined,classification,centralised,a medical claim fraud abuse detection system based on data mining a case study in chile," This paper describes an effective medical claim fraud/abuse detection system based on data mining used by a Chilean private health insurance company. Fraud and abuse in medical claims have become a major concern within health insurance companies in Chile the last years due to the increasing losses in revenues. Processing medical claims is an exhausting manual task carried out by a few medical experts who have the responsibility of approving, modifying or rejecting the subsidies requested within a limited period from their reception. The proposed detection system uses one committee of multilayer perceptron neural networks (MLP) for each one of the entities involved in the fraud/abuse problem: medical claims, affiliates, medical professionals and employers. Results of the fraud detection system show a detection rate of approximately 75 fraudulent and abusive cases per month, making the detection 6.6 months earlier than without the system. The application of data mining to a real industrial problem through the implementation of an automatic fraud detection system changed the original non-standard medical claims checking process to a standardized process helping to fight against new, unusual and known fraudulent/abusive behaviors."
63,10.1109/infcomw.2016.7562053,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7562053/,multimedia,14/04/2016 00:00,not defined,not defined,decision making,centralised,resource provisioning and profit maximization for transcoding in information centric networking,"Adaptive bitrate streaming (ABR) has been widely adopted to support video streaming services over heterogeneous devices and varying network conditions. With ABR, each video content is transcoded into multiple representations in different bitrates and resolutions. However, video transcoding is computing intensive, which requires the transcoding service providers to deploy a large number of servers for transcoding the video contents published by the content producers. As such, a natural question for the transcoding service provider is how to provision the computing resource for transcoding the video contents while maximizing service profit. To address this problem, we design a cloud video transcoding system by taking the advantage of cloud computing technology to elastically allocate computing resource. We propose a method for jointly considering the task scheduling and resource provisioning problem in two timescales, and formulate the service profit maximization as a two-timescale stochastic optimization problem. We derive some approximate policies for the task scheduling and resource provisioning. Based on our proposed methods, we implement our open source cloud video transcoding system Morph and evaluate its performance in a real environment. The experiment results demonstrate that our proposed method can reduce the resource consumption and achieve a higher profit compared with the baseline schemes."
64,,,project-academic,project-academic,robotics,14/03/2021 00:00,not defined,batch,not defined,not defined,success weighted by completion time a dynamics aware evaluation criteria for embodied navigation," We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent's dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exemplifies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an environment containing obstacles. We experiment with deep reinforcement learning and reward shaping to train and compare the navigation performance of agents with different dynamics models. In evaluating these agents, we show that in contrast to SPL, SCT is able to capture the advantages in navigation speed a unicycle model has over a simpler point-turn model of dynamics. Lastly, we show that we can successfully deploy our trained models and algorithms outside of simulation in the real world. We embody our agents in an real robot to navigate an apartment, and show that they can generalize in a zero-shot manner."
65,10.1109/cvpr.2019.00346,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8953386/,multimedia,20/06/2019 00:00,not defined,not defined,not defined,not defined,densefusion: 6d object pose estimation by iterative dense fusion,"A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose."
66,10.1109/iceeccot46775.2019.9114716,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9114716/,multimedia,14/12/2019 00:00,not defined,not defined,not defined,not defined,facial recognition using machine learning algorithms on raspberry pi,"Facial recognition is a non-invasive method of biometric authentication and useful for numerous applications. The real time implementation of the algorithm with adequate accuracy is required, with hardware timing into consideration. This paper deals with the implementation of machine learning algorithm for real time facial image recognition. Two dominant methods out of many facial recognition methods are discussed, simulated and implemented using Raspberry Pi. A rigorous comparative analysis is presented considering various limitations which may be the case required for innumerable application which utilize facial recognition. The drawbacks and different use cases of each method is highlighted. The facial recognition software uses algorithms to compare a digital image captured through a camera, to the stored face print so as to authenticate a person's identity. The Haar-Cascade method was one of the first methods developed for facial recognition. The HOG (Histogram of Oriented Gradients) method has worked very effectively for object recognition and thus suitable for facial recognition also. Both the methods are compared with Eigen feature-based face recognition algorithm. Various important features are experimented like speed of operation, lighting condition, frontal face profile, side profiles, distance of image, size of image etc. The facial recognition model is implemented to detect and recognize faces in real-time by means of Raspberry Pi and Pi camera for the user defined database in addition to the available databases."
67,10.1109/fccm.2017.58,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7966655/,science,02/05/2017 00:00,not defined,not defined,not defined,not defined,accelerating large-scale graph analytics with fpga and hmc,"Graph analytics that explores the relationship among interconnected entities is becoming increasingly important due to its broad applicability from machine learning to social science. However, one major challenge for graph processing systems is the irregular data access pattern of graph computation which can significantly degrade the performance. The algorithms, software, and hardware that have been tailored for mainstream parallel applications are, as a result, generally not effective for massive-scale sparse graphs from the real world due to their complexity and irregularity. To address the performance issues in large-scale graph analytics, we combine the emerging Hybrid Memory Cube (HMC) with a modern FPGA in order to achieve exceptional random access performance without any loss of flexibility or efficiency in computation. In particular, we develop collaborative software/hardware techniques to perform a level-synchronized breadth first search (BFS) on the FPGA-HMC platform. From the software perspective, we develop an architecture-aware graph clustering algorithm that fully exploits the platform's capability to improve data locality and memory access efficiency. For each input graph, this algorithm provides an efficient data layout that allows the FPGA to coalesce memory requests into the largest possible HMC payload requests so that the number of memory requests, which is the primary factor in runtime, can be minimized. From the hardware perspective, we further improve the FPGA-HMC graph processor architecture by adding a merging unit. The merging unit takes the best advantage of the increased data locality resulting from graph clustering. We evaluated the performance of our BFS implementation using the AC-510 development kit from Micron over a set of benchmarks from a wide range of applications. We observed that the combination of the clustering algorithm and the merging hardware achieved 2.8 × average performance improvement compared to the latest FPGA-HMC based graph processing system."
68,http://arxiv.org/abs/2109.07165v1,arxiv,arxiv,http://arxiv.org/abs/2109.07165v1,multimedia,15/09/2021 00:00,supervised,not defined,classification,not defined,3d annotation of arbitrary objects in the wild,"Recent years have produced a variety of learning based methods in the context
of computer vision and robotics. Most of the recently proposed methods are
based on deep learning, which require very large amounts of data compared to
traditional methods. The performance of the deep learning methods are largely
dependent on the data distribution they were trained on, and it is important to
use data from the robot's actual operating domain during training. Therefore,
it is not possible to rely on pre-built, generic datasets when deploying robots
in real environments, creating a need for efficient data collection and
annotation in the specific operating conditions the robots will operate in. The
challenge is then: how do we reduce the cost of obtaining such datasets to a
point where we can easily deploy our robots in new conditions, environments and
to support new sensors? As an answer to this question, we propose a data
annotation pipeline based on SLAM, 3D reconstruction, and 3D-to-2D geometry.
The pipeline allows creating 3D and 2D bounding boxes, along with per-pixel
annotations of arbitrary objects without needing accurate 3D models of the
objects prior to data collection and annotation. Our results showcase almost
90% Intersection-over-Union (IoU) agreement on both semantic segmentation and
2D bounding box detection across a variety of objects and scenes, while
speeding up the annotation process by several orders of magnitude compared to
traditional manual annotation."
69,10.1109/rtsi50628.2021.9597339,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9597339/,industry,09/09/2021 00:00,not defined,not defined,not defined,not defined,towards graph machine learning for smart grid knowledge graphs in industrial scenarios,"Knowledge Graphs (KGs) demonstrated promising application perspective in different scenarios, especially when combined with Graph Machine Learning (GML) techniques able to interpret and infer over facts. Given the natural network structures of Smart Grid equipment and the exponential growth of electric power data, Smart Grid Knowledge Graphs (SGKGs) provides unprecedented opportunities to manage massive power resources and provide intelligent applications. However, a single representation of the SGKGs is never sufficient to properly exploit GML techniques that leverage different aspects of the KG for various objectives. In this work, we provide a methodology to extract various significant views of the SGKG by iteratively applying a series of transformation to the description of the power network in the IEC CIM standard. Our implementation is based on a declarative approach to guarantee easier portability, and we deploy the transformations as a stateless microservice, facilitating modular integration with the rest of the Smart Grid Semantic Platform. Experimental evaluation on two real power distribution networks demonstrates the efficacy of our approach in highlighting important topological information, without discarding precious additional knowledge present in the SGKG."
70,10.1109/aero.2005.1559665,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/1559665/,health,12/03/2005 00:00,not defined,not defined,not defined,not defined,health monitoring: new techniques based on vibrations measurements and identification algorithms,"Purpose of the paper is to present an innovative application inside the nondestructive testing field based on vibrations measurements, developed, at the Department of Aeronautical Engineering of the University of Naples ""Federico II"" (Italy), by the authors during the last three years, and already tested for analysing damages of many structural elements. The aim has been the development of a nondestructive test (NDT) which meet to most of the mandatory requirements for effective health monitoring systems, simultaneously reducing as much as possible the complexity of the data analysis algorithm and of the experimental acquisition instrumentation; these peculiarities may, in fact, not be neglected for an operative implementation of such a system. The proposed new method is based on the acquisition and comparison of frequency response functions (FRFs) of the monitored structure before and after an occurred damage. Structural damages modify the dynamical behaviour of the structure such as mass, stiffened and damping, and consequently the FRFs of the damaged structure in comparison with the FRFs of the sound structure, making possible to identify, to localize and quantify a structural damage. The activities, presented in the paper, mostly focused on a new FRFs processing technique based on the determining of a representative ""damage index"" for identifying and analysing damages both on real scale aeronautical structural components, like large-scale fuselage reinforced panels, and on aeronautical composite panels. Besides it has been carried out a dedicated neural network algorithm aiming at obtaining a ""recognition-based learning""; this kind of learning methodology permits to train the neural network in order to let it recognise only ""positive"" examples discarding as a consequence the ""negative"" ones. Within the structural NDT a ""positive"" example means ""healthy"" state of the analysed structural component and, obviously, a ""negative"" one means a ""damaged"" or perturbed state. With this object in view the neural network has been trained making use of the same FRFs of the healthy structure used for the determining of the damage index, as positive examples. From an architectural point of view magnetostrictive devices have been tested as actuators, and piezoceramic patches as actuators and sensors. Besides it has been used a laser-scanning vibrometer system to validate the behaviour of the piezoceramic patches and define their technical parameters in order to lay the bases for design a light and reliability system. These techniques promise to bring a step forward for the implementation of an automatic ""health monitoring"" system which will be able to identify a structural damage in real time, improving the safety and reducing maintenance costs"
71,10.1109/access.2019.2927461,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8760232/,health,01/01/2019 00:00,not defined,not defined,not defined,not defined,ephort: towards a reference architecture for tele-rehabilitation systems,"In recent years, the software applications for medical assistance, including the tele-rehabilitation, have known a high and a continuous presence in the medical area. The ePHoRt is a Web-based platform for the remote home monitoring rehabilitation exercises in patients after hip replacement surgery. It involves a learning phase and a serious game scheme for the execution and evaluation of the exercises as part of a therapeutic program. Modular software architecture is proposed, under the patient perspective, to be used as a reference model for researchers or professionals who wish to carry out tele-rehabilitation platforms, and to guarantee security, flexibility, and scalability. The architecture incorporates two main components. The first one manages the patient' therapeutic programs taking into account two principles: 1) maintain loose coupling between the layers of the framework and 2) Don't Repeat Yourself (DRY). The second one evaluates the performed exercises in real time considering an independent acquisition mechanism for the patient movements and two artificial algorithms. The first algorithm allows evaluating the quality of the movements, while the second one allows assessing the levels of pain intensity by recognizing the patient' emotions when performing the movements. Details of the components and the meta-model of the architecture are presented and discussed considering their advantages and disadvantages."
72,10.1109/aero50100.2021.9438232,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9438232/,multimedia,13/03/2021 00:00,not defined,not defined,not defined,not defined,a pipeline for vision-based on-orbit proximity operations using deep learning and synthetic imagery,"Deep learning has become the gold standard for image processing over the past decade. Simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. However, two key challenges currently pose a major barrier to the use of deep learning for vision-based on-orbit proximity operations. Firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. Secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. This paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. The core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. The first tool leverages Blender, an open-source 3D graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. The second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. Time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. The presented system has been used in the Texas Spacecraft Laboratory with marked benefits in development speed and quality. Remote development, scalable compute, and automatic organization of data and artifacts have dramatically decreased iteration time while increasing reproducibility and system comprehension. Diverse, high-fidelity synthetic images that more closely replicate the real environment have improved model performance against real-world data. These results demonstrate that the presented pipeline offers tangible benefits to the application of deep learning for vision-based on-orbit proximity operations."
73,10.1109/aero.2018.8396807,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8396807/,autonomous vehicle,10/03/2018 00:00,not defined,not defined,not defined,not defined,learning safe recovery trajectories with deep neural networks for unmanned aerial vehicles,"Unmanned vehicles that use vision sensors for perception to aid autonomous flight are a highly popular area of research. However, these systems are often prone to failures that are often hard to model. Previous work has focused on using deep learning to detect these failures. In this work, we build on these failure detection systems and develop a pipeline that learns to identify the correct trajectory to execute that restores the vision system and the unmanned vehicle to a safe state. The key challenge with using a deep learning pipeline for this problem is the limited amount of training data available from a real world system. Ideally one requires millions of data points to sufficiently train a model from scratch. However, this is not feasible for an unmanned aerial vehicle. The dataset we operate with is limited to 400-500 points. To sufficiently learn from such a small dataset we leverage the idea of transfer learning and non linear dimensionality reduction. We deploy our pipeline on an unmanned aerial vehicle flying autonomously through outdoor clutter (in a GPS denied environment) and show that we are able to achieve long durations of safe autonomous flight."
74,10.1109/tase.2020.3032075,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9246671/,industry,01/10/2021 00:00,not defined,not defined,not defined,not defined,a virtual mechanism approach for exploiting functional redundancy in finishing operations,"We propose a new approach to programming by the demonstration of finishing operations. Such operations can be carried out by industrial robots in multiple ways because an industrial robot is typically functionally redundant with respect to a finishing task. In the proposed system, a human expert demonstrates a finishing operation, and the demonstrated motion is recorded in the Cartesian space. The robot’s kinematic model is augmented with a virtual mechanism, which is defined according to the applied finishing tool. This way, the kinematic model is expanded with additional degrees of freedom that can be exploited to compute the optimal joint space motion of the robot without altering the essential aspects of the Cartesian space task execution as demonstrated by the human expert. Finishing operations, such as polishing and grinding, occur in contact with the treated workpiece. Since information about the contact point position is needed to control the robot during the operation, we have developed a novel approach for accurate estimation of contact points using the measured forces and torques. Finally, we applied iterative learning control to refine the demonstrated operations and compensate for inaccurate calibration and different dynamics of the robot and human demonstrator. The proposed method was verified on real robots and real polishing and grinding tasks. <italic>Note to Practitioners</italic>—This work was motivated by the need for automation of finishing operations, such as polishing and grinding, on contemporary industrial robots. Existing approaches are both too complex and too time-consuming to be applied in flexible and small-scale production, which often requires the frequent deployment of new applications. Our approach is based on programming by demonstration and enables the programming of finishing operations also for users who are not specialists in robot programming. Programming by demonstration is especially useful for teaching finishing operations because it enables the transfer of expert knowledge about finishing skills to robots without providing lengthy task descriptions or manual coding. Besides the human demonstration of the desired operation, the proposed approach also requires the availability of the kinematic model for the machine tool applied to carry out the finishing operation. We provide several practical examples of grinding and polishing tools and how to integrate them into our approach. Another feature of the proposed system is that user demonstrations of finishing operations can be transferred between different combinations of robots and machine tools."
75,10.1109/sysose.2017.7994953,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7994953/,autonomous vehicle,21/06/2017 00:00,not defined,not defined,not defined,not defined,autonomous decision making for a driver-less car,"Autonomous driving has been a hot topic with companies like Google, Uber, and Tesla because of the complexity of the problem, seemingly endless applications, and capital gain. The technology's brain child is DARPA's autonomous urban challenge from over a decade ago. Few companies have had some success in applying algorithms to commercial cars. These algorithms range from classical control approaches to Deep Learning. In this paper, we will use Deep Learning techniques and the Tensor flow framework with the goal of navigating a driverless car through an urban environment. The novelty in this system is the use of Deep Learning vs. traditional methods of real-time autonomous operation as well as the application of the Tensorflow framework itself. This paper provides an implementation of AlexNet's Deep Learning model for identifying driving indicators, how to implement them in a real system, and any unforeseen drawbacks to these techniques and how these are minimized and overcome."
76,10.1109/lars-sbr.2016.49,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7783535/,robotics,12/10/2016 00:00,not defined,not defined,not defined,not defined,integration of people detection and simultaneous localization and mapping systems for an autonomous robotic platform,"This paper presents the implementation of a people detection system for a robotic platform able to perform Simultaneous Localization and Mapping (SLAM), allowing the exploration and navigation of the robot considering people detection interaction. The robotic platform consists of a Pioneer 3DX robot equipped with an RGB-D camera, a Sick Lms200 sensor laser and a computer using the robot operating system ROS. The idea is to integrate the people detection system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. Furthermore, this paper presents an evaluation of two different approaches for the people detection system. The first one uses a manual feature extraction technique, and the other one is based on deep learning methods. The manual feature extraction method in the first approach is based on HOG (Histogram of Oriented Gradients) detectors. The accuracy of the techniques was evaluated using two different libraries. The PCL library (Point Cloud Library) implemented in C ++ and the VLFeat MatLab library with two HOG variants, the original one, and the DPM (Deformable Part Model) variant. The second approaches are based on a Deep Convolutional Neural Network (CNN), and it was implemented using the MatLab MatConvNet library. Tests were made objecting the evaluation of losses and false positives in the people's detection process in both approaches. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment."
77,10.1109/camad.2018.8515001,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8515001/,smart cities,19/09/2018 00:00,not defined,not defined,not defined,not defined,uncertainty management for wearable iot wristband sensors using laplacian-based matrix completion,"Contemporary sensing devices provide reliable mechanisms for continuous process monitoring, accommodating use cases related to mHealth and smart mobility, by generating real-time data streams of numerous physiological and vital parameters. Such data streams can be later utilized by machine learning algorithms and decision support systems to predict critical clinical states and motivate users to adopt behaviours that improve the quality of their life and the society as a whole. However, in many cases, even when deployed over highly sophisticated, cutting-edge network infrastructure and deployment paradigms, data may exhibit missing values and non-uniformities due to various reasons, including device malfunction, deliberate data reduction for efficient processing, or data loss due to sensing and communication failures. This work proposes a novel approach to deal with missing entries in heart rate measurements. Benefiting from the low-rank property of the generated data matrices and the proximity of neighbouring measurements, we provide a novel method that combines classical matrix completion approaches with weighted Laplacian interpolation offering high reconstruction accuracy at fast execution times. Extensive evaluation studies carried out with real measurements show that the proposed methods could be effectively deployed by modern wristband-cloud computing systems increasing the robustness, the reliability and the energy efficiency of these systems."
78,10.1016/j.neunet.2013.04.005,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/84880792738,robotics,01/09/2013,not defined,not defined,not defined,not defined,fpga implementation of a configurable neuromorphic cpg-based locomotion controller,"Neuromorphic engineering is a discipline devoted to the design and development of computational hardware that mimics the characteristics and capabilities of neuro-biological systems. In recent years, neuromorphic hardware systems have been implemented using a hybrid approach incorporating digital hardware so as to provide flexibility and scalability at the cost of power efficiency and some biological realism. This paper proposes an FPGA-based neuromorphic-like embedded system on a chip to generate locomotion patterns of periodic rhythmic movements inspired by Central Pattern Generators (CPGs). The proposed implementation follows a top-down approach where modularity and hierarchy are two desirable features. The locomotion controller is based on CPG models to produce rhythmic locomotion patterns or gaits for legged robots such as quadrupeds and hexapods. The architecture is configurable and scalable for robots with either different morphologies or different degrees of freedom (DOFs). Experiments performed on a real robot are presented and discussed. The obtained results demonstrate that the CPG-based controller provides the necessary flexibility to generate different rhythmic patterns at run-time suitable for adaptable locomotion."
79,10.1016/j.imu.2020.100335,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85084287220,health,01/01/2020,not defined,not defined,not defined,not defined,spark architecture for deep learning-based dose optimization in medical imaging,"Background and objectives
                  Deep Learning (DL) and Machine Learning (ML) have brought several breakthroughs to biomedical image analysis by making available more consistent and robust tools for the identification, classification, reconstruction, denoising, quantification, and segmentation of patterns in biomedical images. Recently, some applications of DL and ML in Computed Tomography (CT) scans for low dose optimization were developed. Nowadays, DL algorithms are used in CT to perform replacement of missing data (processing technique) such as low dose to high dose, sparse view to full view, low resolution to high resolution, and limited angle to full angle. Thus, DL comes with a new vision to process biomedical data imagery from CT scan. It becomes important to develop architectures and/or methods based on DL algorithms for minimizing radiation during a CT scan exam thanks to reconstruction and processing techniques.
               
                  Methods
                  This paper describes DL for CT scan low dose optimization, shows examples described in the literature, briefly discusses new methods used in CT scan image processing, and offers conclusions. We based our study on the literature and proposed a pipeline for low dose CT scan image reconstruction. Our proposed pipeline relies on DL and the Spark Framework using MapReduce programming. We discuss our proposed pipeline with those proposed in the literature to conclude the efficiency and importance.
               
                  Results
                  An architecture for low dose optimization using CT imagery is suggested. We used the Spark Framework to design the architecture. The proposed architecture relies on DL, and permits us to develop efficient and appropriate methods to process dose optimization with CT scan imagery. The real implementation of our pipeline for image denoising shows that we can reduce the radiation dose, and use our proposed pipeline to improve the quality of the captured image.
               
                  Conclusion
                  The proposed architecture based on DL is complete and enables faster processing of biomedical CT imagery as compared with prior methods described in the literature."
80,10.1109/aiam48774.2019.00157,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8950866/,industry,18/10/2019 00:00,not defined,not defined,not defined,not defined,a digital twin-based approach for quality control and optimization of complex product assembly,"To address the problems caused by low ability of quality analysis and decision-making in the process of complex product assembly, in this paper, we proposed a digital twin-based approach for quality control and optimization of complex product assembly, by providing a digital twin system to realize the timely and precisely interactive mapping between the physical world and digital world. Specifically, a quality control and optimization mechanism is presented, which provides the theoretical support to the realization of the digital twin-based approach. A data-driven quality control model is introduced to solve the optimization problem by considering the panoramic assembly quality data. A digital twin system for complex product assembly is elaborated by providing detailed deployment and implementation procedures, which includes (1) building of the digital entity of an assembly line, (2) real-time online sensing in multi-source heterogeneous environment, (3) real-time simulation of equipment and assembly process, (4) realization of the intelligent production scheduling under uncertainty conditions, and (5) dynamical adjustment of the assembly process. Finally, the paper presents the validation results considering the practical applications of the proposed approach in real industrial fields."
81,10.1109/cvpr.2019.00437,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8953663/,multimedia,20/06/2019 00:00,not defined,not defined,not defined,not defined,learning to film from professional human motion videos,"We investigate the problem of 6 degrees of freedom (DOF) camera planning for filming professional human motion videos using a camera drone. Existing methods either plan motions for only a pan-tilt-zoom (PTZ) camera, or adopt ad-hoc solutions without carefully considering the impact of video contents and previous camera motions on the future camera motions. As a result, they can hardly achieve satisfactory results in our drone cinematography task. In this study, we propose a learning-based framework which incorporates the video contents and previous camera motions to predict the future camera motions that enable the capture of professional videos. Specifically, the inputs of our framework are video contents which are represented using subject-related feature based on 2D skeleton and scene-related features extracted from background RGB images, and camera motions which are represented using optical flows. The correlation between the inputs and output future camera motions are learned via a sequence-to-sequence convolutional long short-term memory (Seq2Seq ConvLSTM) network from a large set of video clips. We deploy our approach to a real drone cinematography system by first predicting the future camera motions, and then converting them to the drone's control commands via an odometer. Our experimental results on extensive datasets and showcases exhibit significant improvements in our approach over conventional baselines and our approach can successfully mimic the footage of a professional cameraman."
82,10.1016/j.ifacol.2020.12.1459,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85105082300,autonomous vehicle,01/01/2020,supervised,not defined,not defined,not defined,deep learning based segmentation of fish in noisy forward looking mbes images,"In this work, we investigate a Deep Learning (DL) approach to fish segmentation in a small dataset of noisy low-resolution images generated by a forward-looking multibeam echosounder (MBES). We build on recent advances in DL and Convolutional Neural Networks (CNNs) for semantic segmentation and demonstrate an end-to-end approach for a fish/non-fish probability prediction for all range-azimuth positions projected by an imaging sonar. We use self-collected datasets from the Danish Sound and the Faroe Islands to train and test our model and present techniques to obtain satisfying performance and generalization even with a low-volume dataset. We show that our model proves the desired performance and has learned to harness the importance of semantic context and take this into account to separate noise and non-targets from real targets. Furthermore, we present techniques to deploy models on low-cost embedded platforms to obtain higher performance fit for edge environments – where compute and power are restricted by size/cost – for testing and prototyping."
83,10.1145/3326285.3329051,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9068647/,multimedia,25/06/2019 00:00,not defined,not defined,not defined,not defined,leap: learning-based smart edge with caching and prefetching for adaptive video streaming,"Dynamic Adaptive Streaming over HTTP (DASH) has emerged as a popular approach for video transmission, which brings a potential benefit for the Quality of Experience (QoE) because of its segment-based flexibility. However, the Internet can only provide no guaranteed delivery. The high dynamic of the available bandwidth may cause bitrate switching or video rebuffering, thus inevitably damaging the QoE. Besides, the frequently requested popular videos are transmitted for multiple times and contribute to most of the bandwidth consumption, which causes massive transmission redundancy. Therefore, we propose a Learning-based Edge with cAching and Prefetching (LEAP) to improve the online user QoE of adaptive video streaming. LEAP introduces caching into the edge to reduce the redundant video transmission and employs prefetching to fight against network jitters. Taking the state information of users into account, LEAP intelligently makes the most beneficial decisions of caching and prefetching by a QoE-oriented deep neural network model. To demonstrate the performance of our scheme, we deploy the implemented prototype of LEAP in both the simulated scenario and the real Internet. Compared with all selected schemes, LEAP at least raises average bitrate by 34.4% and reduces video rebuffering by 42.7%, which leads to at least 15.9% improvement in the user QoE in the simulated scenario. The results in the real Internet scenario further confirm the superiority of LEAP."
84,,"Deep Learning-Based Multiple Object Visual Tracking on Embedded System
  for IoT and Mobile Edge Computing Applications",core,,multimedia,31/07/2018 00:00,supervised,batch,classification,not defined,http://arxiv.org/abs/1808.01356,"Compute and memory demands of state-of-the-art deep learning methods are
still a shortcoming that must be addressed to make them useful at IoT
end-nodes. In particular, recent results depict a hopeful prospect for image
processing using Convolutional Neural Netwoks, CNNs, but the gap between
software and hardware implementations is already considerable for IoT and
mobile edge computing applications due to their high power consumption. This
proposal performs low-power and real time deep learning-based multiple object
visual tracking implemented on an NVIDIA Jetson TX2 development kit. It
includes a camera and wireless connection capability and it is battery powered
for mobile and outdoor applications. A collection of representative sequences
captured with the on-board camera, dETRUSC video dataset, is used to exemplify
the performance of the proposed algorithm and to facilitate benchmarking. The
results in terms of power consumption and frame rate demonstrate the
feasibility of deep learning algorithms on embedded platforms although more
effort to joint algorithm and hardware design of CNNs is needed.Comment: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessibl"
85,10.1016/j.jbi.2019.103138,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85062392033,health,01/04/2019,not defined,not defined,not defined,decentralised,distributed learning from multiple ehr databases: contextual embedding models for medical events,"Electronic health record (EHR) data provide promising opportunities to explore personalized treatment regimes and to make clinical predictions. Compared with regular clinical data, EHR data are known for their irregularity and complexity. In addition, analyzing EHR data involves privacy issues and sharing such data is often infeasible among multiple research sites due to regulatory and other hurdles. A recently published work uses contextual embedding models and successfully builds one predictive model for more than seventy common diagnoses. Despite of the high predictive power, the model cannot be generalized to other institutions without sharing data. In this work, a novel method is proposed to learn from multiple databases and build predictive models based on Distributed Noise Contrastive Estimation (Distributed NCE). We use differential privacy to safeguard the intermediary information sharing. The numerical study with a real dataset demonstrates that the proposed method not only can build predictive models in a distributed manner with privacy protection, but also preserve model structure well and achieve comparable prediction accuracy. The proposed methods have been implemented as a stand-alone Python library and the implementation is available on Github (https://github.com/ziyili20/DistributedLearningPredictor) with installation instructions and use-cases."
86,10.1109/isc2.2016.7580798,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7580798/,robotics,15/09/2016 00:00,not defined,not defined,not defined,not defined,smartseal: a ros based home automation framework for heterogeneous devices interconnection in smart buildings,"With this paper we present the SmartSEAL inter-connection system developed for the nationally founded SEAL project. SEAL is a research project aimed at developing Home Automation (HA) solutions for building energy management, user customization and improved safety of its inhabitants. One of the main problems of HA systems is the wide range of communication standards that commercial devices use. Usually this forces the designer to choose devices from a few brands, limiting the scope of the system and its capabilities. In this context, SmartSEAL is a framework that aims to integrate heterogeneous devices, such as sensors and actuators from different vendors, providing networking features, protocols and interfaces that are easy to implement and dynamically configurable. The core of our system is a Robotics middleware called Robot Operating System (ROS). We adapted the ROS features to the HA problem, designing the network and protocol architectures for this particular needs. These software infrastructure allows for complex HA functions that could be realized only levering the services provided by different devices. The system has been tested in our laboratory and installed in two real environments, Palazzo Fogazzaro in Schio and “Le Case” childhood school in Malo. Since one of the aim of the SEAL project is the personalization of the building environment according to the user needs, and the learning of their patterns of behaviour, in the final part of this work we also describe the ongoing design and experiments to provide a Machine Learning based re-identification module implemented with Convolutional Neural Networks (CNNs). The description of the adaptation module complements the description of the SmartSEAL system and helps in understanding how to develop complex HA services through it."
87,10.1016/j.epsr.2011.06.007,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/79960994059,health,01/11/2011,not defined,not defined,not defined,not defined,general asset management model in the context of an electric utility: application to power transformers,"GAMMEU
                        1
                     
                     
                        1
                        GAMMEU: general asset management model for an electric utility.
                      constitutes an integrated approach that covers the different elements related to the asset management of power transformers in the environment of a utility. GAMMEU harmonizes and inter-relates all the relevant subsystems of the asset management that normally are studied as individual entities and not as a system. Concretely, GAMMEU consists of a platform for data integration, an intelligent system for detection and diagnosis of failures, a failure rate estimation model, a module of reliability analysis and an optimisation model for maintenance scheduling. In this work, a brief description of the elements of GAMMEU is presented and the implementation of the intelligent system for detection and diagnosis as well as the failure rate estimation model is exemplified using data of measurements performed in real power transformers. A robust anomaly detection module using prediction models based on artificial intelligence techniques was developed for top oil temperature monitoring and the use of decision trees as classifiers for the assessment of FRA
                        2
                     
                     
                        2
                        Frequency response analysis.
                      measurements is also illustrated. For failure rate estimation, the use of a model based on hidden Markov chains is presented using data of dissolved gas analysis tests. The experience obtained from the implementation of part of the modules of GAMMEU using real data has demonstrated its feasibility."
88,10.1109/icws.2017.76,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8029817/,smart cities,30/06/2017 00:00,not defined,not defined,not defined,not defined,early air pollution forecasting as a service: an ensemble learning approach,"Air quality has become a major global concern for human beings involving all social stratums, for both developing and developed countries. Web service of precise and early air pollution forecasting is of great importance as it allows people to pro-actively take preventative and protective measurements. As an endeavor on the course of machine learning based air quality forecasting, this paper presents an initiative and its technological details in solving this challenging problem. Specifically, this work involves three major highlights regarding with both algorithmic innovation and deployment with its impact: 1) We propose a multi-channel ensemble learning framework, 2) We propose a new supervised feature learning and extraction method, i.e. sufficient statistics feature mapping based on Deep Boltzman Machine, which serves as a building block for our learning system, 3) We target our air pollution prediction method to the city of Beijing, China as it is at the forefront for battling against air pollution, which is embodied as a web service for prediction. Extensive experiments of real time air pollution forecasting on the real-world data demonstrates the effectiveness of the proposed method and value of the deployed web service system."
89,10.1016/b978-0-444-59520-1.50104-4,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/84862870640,science,01/01/2012,not defined,not defined,not defined,not defined,intelligent automation platform for bioprocess development,"High throughput technology has been increasingly adapted for drug screen and bioprocess development, due to the small amount of processing materials and reagents required and parallel experiment execution. It allows a wide design space to be explored in order to discover novel bioprocess solutions. Currently, the high throughput experiments for bioprocess development are implemented in a sequential fashion in which liquid handling system will perform the web lab experiment to prepare the samples; standalone analysis devices detect the data such as protein concentration; and specific software is used to realise the data analysis for process design or further experimentation.
                  The aim of this paper is to show how the efficiency of the high throughput bioprocess development approach can be enhanced by creating an intelligent automation platform that systematically drives liquid handling system, analysis devices and data analysis to perform a closed-loop learning. The first generation prototype has been established which consists of three parts: automated devices, design algorithms and database. In order to prove the concept of prototype, both simulation and real experiments studies have been established. In this case study, the platform is used to investigate the solubility of lysozyme at various ion strengths and pH values. Tecan liquid handling system for experimentation as well as buffer preparation and a plate reader for uv absorption measurement to determine protein concentration were used as the automated devices. The simplex search algorithm and artificial neural network modelling were utilised as design algorithm to iteratively select the experiments to execute and determine the optimal design solution. An entity-relationship database with Tecan system configuration information and experimental data was established. The results demonstrate this integrated approach can implement experiments and data analysis automatically to provide specific bioprocess design solutions in a closed loop strategy at first time. It is a promising approach that may significant increase the level of lab automation to release the engineer from the labour intensive R&D activities and provides the base for sophisticated artificial intelligent learning in the future."
90,10.1109/qrs51102.2020.00018,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9282796/,health,14/12/2020 00:00,not defined,not defined,not defined,not defined,phm technology for memory anomalies in cloud computing for iaas,"The IaaS (Infrastructure as a Service) is one of the most popular services from todays cloud service providers, where the virtual machines (VM) are rented by users who can deploy any program they want in the VMs to make their own websites or use as their remote desktops. However, this poses a major challenge for cloud IaaS providers who cannot control the software programs that users develop, install or download on their rented VMs. Those programs may not be well developed with various bugs or even downloaded/installed together with virus, which often make damages to the VMs or infect the cloud platform. To keep the health of a cloud IaaS platform, it is very important to implement the PHM (Prognostics and Health Management) technology for detecting those software problems and self-healing them in an intelligent and timely way. This paper realized a novel PHM technology inspired by biological autonomic nervous system to deal with the memory anomalies of those programs running on the cloud IaaS platform. We first present an innovative autonomic computing technology called Bionic Autonomic Nervous System (BANS) to endow the cloud system with distinctive capabilities of perception, detection, reflection, and learning. Then, we propose a BANS-based Prognostics and Health Management (BPHM) technology to enable the cloud system self-dealing with various memory anomalies. AI-based failure prognostics, immediate self-healing, self-learning ability and self-improvement functions are implemented. Experimental results illustrate that the designed BPHM can automatically and intelligently deal with complex memory anomalies in a real cloud system for IaaS, to keep the system much more reliable and healthier."
91,https://riunet.upv.es/bitstream/10251/169535/1/sarabia-jacomeusachpalau%20-%20highly-efficient%20fog-based%20deep%20learning%20aal%20fall%20detection%20system.pdf,Highly-efficient fog-based deep learning AAL fall detection system,core,,health,01/09/2020 00:00,not defined,not defined,not defined,hybrid,10.1016/j.iot.2020.100185,"[EN] Falls is one of most concerning accidents in aged population due to its high frequency and serious repercussion; thus, quick assistance is critical to avoid serious health consequences. There are several Ambient Assisted Living (AAL) solutions that rely on the technologies of the Internet of Things (IoT), Cloud Computing and Machine Learning (ML). Recently, Deep Learning (DL) have been included for its high potential to improve accuracy on fall detection. Also, the use of fog devices for the ML inference (detecting falls) spares cloud drawback of high network latency, non-appropriate for delay-sensitive applications such as fall detectors. Though, current fall detection systems lack DL inference on the fog, and there is no evidence of it in real environments, nor documentation regarding the complex challenge of the deployment. Since DL requires considerable resources and fog nodes are resource-limited, a very efficient deployment and resource usage is critical. We present an innovative highly-efficient intelligent system based on a fog-cloud computing architecture to timely detect falls using DL technics deployed on resource-constrained devices (fog nodes). We employ a wearable tri-axial accelerometer to collect patient monitoring data. In the fog, we propose a smart-IoT-Gateway architecture to support the remote deployment and management of DL models. We deploy two DL models (LSTM/GRU) employing virtualization to optimize resources and evaluate their performance and inference time. The results prove the effectiveness of our fall system, that provides a more timely and accurate response than traditional fall detector systems, higher efficiency, 98.75% accuracy, lower delay, and service improvement.This research was supported by the Ecuadorian Government through the Secretary of Higher Education, Science, Technology, and Innovation (SENESCYT) and has received funding from the European Union's Horizon 2020 research and innovation program as part of the ACTIVAGE project under Grant 732679.Sarabia-Jácome, D.; Usach, R.; Palau Salvador, CE.; Esteve Domingo, M. (2020). Highly-efficient fog-based deep learning AAL fall detection system. Internet of Things. 11:1-19. https://doi.org/10.1016/j.iot.2020.100185S1191"
92,10.1109/tcyb.2013.2275291,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/6583249/,robotics,01/10/2013 00:00,not defined,not defined,not defined,not defined,real-time multiple human perception with color-depth cameras on a mobile robot,"The ability to perceive humans is an essential requirement for safe and efficient human-robot interaction. In real-world applications, the need for a robot to interact in real time with multiple humans in a dynamic, 3-D environment presents a significant challenge. The recent availability of commercial color-depth cameras allow for the creation of a system that makes use of the depth dimension, thus enabling a robot to observe its environment and perceive in the 3-D space. Here we present a system for 3-D multiple human perception in real time from a moving robot equipped with a color-depth camera and a consumer-grade computer. Our approach reduces computation time to achieve real-time performance through a unique combination of new ideas and established techniques. We remove the ground and ceiling planes from the 3-D point cloud input to separate candidate point clusters. We introduce the novel information concept, depth of interest, which we use to identify candidates for detection, and that avoids the computationally expensive scanning-window methods of other approaches. We utilize a cascade of detectors to distinguish humans from objects, in which we make intelligent reuse of intermediary features in successive detectors to improve computation. Because of the high computational cost of some methods, we represent our candidate tracking algorithm with a decision directed acyclic graph, which allows us to use the most computationally intense techniques only where necessary. We detail the successful implementation of our novel approach on a mobile robot and examine its performance in scenarios with real-world challenges, including occlusion, robot motion, nonupright humans, humans leaving and reentering the field of view (i.e., the reidentification challenge), human-object and human-human interaction. We conclude with the observation that the incorporation of the depth information, together with the use of modern techniques in new ways, we are able to create an accurate system for real-time 3-D perception of humans by a mobile robot."
93,10.1109/humanoids.2014.7041373,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7041373/,multimedia,20/11/2014 00:00,not defined,not defined,not defined,not defined,footstep planning on uneven terrain with mixed-integer convex optimization,"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1]."
94,10.1016/j.compag.2018.09.037,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85054181612,smart cities,01/11/2018,not defined,not defined,not defined,not defined,a decision support tool to enhance agricultural growth in the mékrou river basin (west africa),"We describe in this paper the implementation of E-Water, an open software Decision Support System (DSS), designed to help local managers assess the Water Energy Food Environment (WEFE) nexus. E-Water aims at providing optimal management solutions to enhance food crop production at river basin level. The DSS was applied in the transboundary Mékrou river basin, shared among Benin, Burkina Faso and Niger. The primary sector for local economy in the region is agriculture, contributing significantly to income generation and job creation. Fostering the productivity of regional agricultural requires the intensification of farming practices, promoting additional inputs (mainly nutrient fertilizers and water irrigation) but, also, a more efficient allocation of cropland.
                  In order to cope with the heterogeneity of data, and the analyses and issues required by the WEFE nexus approach, our DSS integrates the following modules: (1) the EPIC biophysical agricultural model; (2) a simplified regression metamodel, linking crop production with external inputs; (3) a linear programming and a multiobjective genetic algorithm optimization routines for finding efficient agricultural strategies; and (4) a user-friendly interface for input/output analysis and visualization.
                  To test the main features of the DSS, we apply it to various real and hypothetical scenarios in the Mékrou river basin. The results obtained show how food unavailability due to insufficient local production could be reduced by, approximately, one third by enhancing the application and optimal distribution of fertilizers and irrigation. That would also affect the total income of the farming sector, eventually doubling it in the best case scenario. Furthermore, the combination of optimal agricultural strategies and modified optimal cropland allocation across the basin would bring additional moderate increases in food self-sufficiency, and more substantial gains in the total agricultural income.
                  The proposed software framework proves to be effective, enabling decision makers to identify efficient and site-specific agronomic management strategies for nutrients and water. Such practices would augment crop productivity, which, in turn, would allow to cope with increasing future food demands, and find a balanced use of natural resources, also taking other economic sectors—like livestock, urban or energy—into account."
95,10.1109/ijcnn.2008.4633875,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/4633875/,robotics,08/06/2008 00:00,not defined,not defined,not defined,not defined,bio-inspired stochastic chance-constrained multi-robot task allocation using wsn,"The multi-robot task allocation (MRTA) especially in unknown complex environment is one of the fundamental problems, a mostly important object in research of multi-robot. The MRTA problem is initially formulated as a chance-constrained optimization problem. Monte Carlo simulation is used to verify the accuracy of the solution provided by the algorithm. Ant colony optimization (ACO) algorithm based on bionic swarm intelligence was used. A hybrid intelligent algorithm combined Monte Carlo simulation and neural network is used for solving stochastic chance constrained models of MRTA. A practical implementation with real WSN and real mobile robots were carried out. In environment the successful implementation of tasks without collision validates the efficiency, stability and accuracy of the proposed algorithm. The convergence curve shows that as iterative generation grows, the utility increases and finally reaches a stable and optimal value. Results show that using sensor information fusion can greatly improve the efficiency. The algorithm is proved better than tradition algorithms without WSN for MRTA in real time."
96,10.1109/83.791960,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/791960/,industry,01/10/1999 00:00,not defined,not defined,not defined,not defined,real-time dsp implementation for mrf-based video motion detection,"This paper describes the real time implementation of a simple and robust motion detection algorithm based on Markov random field (MRF) modeling, MRF-based algorithms often require a significant amount of computations. The intrinsic parallel property of MRF modeling has led most of implementations toward parallel machines and neural networks, but none of these approaches offers an efficient solution for real-world (i.e., industrial) applications. Here, an alternative implementation for the problem at hand is presented yielding a complete, efficient and autonomous real-time system for motion detection. This system is based on a hybrid architecture, associating pipeline modules with one asynchronous module to perform the whole process, from video acquisition to moving object masks visualization. A board prototype is presented and a processing rate of 15 images/s is achieved, showing the validity of the approach."
97,,'Institute of Electrical and Electronics Engineers (IEEE)',core,10.1109/tro.2021.3084374,multimedia,01/01/2021 00:00,not defined,not defined,not defined,not defined,cat-like jumping and landing of legged robots in low gravity using deep reinforcement learning,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.ISSN:1552-3098ISSN:1042-296XISSN:1941-046"
98,10.1145/3460418.3480415,ACM,project-academic,project-academic,industry,21/09/2021 00:00,not defined,not defined,not defined,not defined,piwims physics informed warehouse inventory monitory via synthetic data generation," State-of-the-art camera-based deep learning methods for inventory monitoring tend to fail to generalize across different domains due to the high variance of scene settings. Large amounts of human labor are required to label and parameterize the models, making a real-world deployment impractical. In a third-party warehouse setting, supervised learning approaches are either too costly and/or inaccurate to deploy due to the need for human labor to address the diverse set of environmental factors (i.e, lighting conditions, product motion, deployment limitations). None We introduce, a realistic synthetic dataset generation technique that combines the physical constraints of the scene in real-world deployments, drastically reducing the need for human labeling. In contrast to other generative techniques, where the generative parameters are learned from a large sample of available data, this compositive approach defines the parameters based on physical characteristics of the particular task, which requires minimal human annotation. We demonstrate performance in a 4-month real operating warehouse deployment and show that with only 32 manually labeled images per object, can achieve an accuracy of up to 87% in inventory tracking, which is a 28% increase when compared to traditional data augmentation techniques and 31% error reduction when compared to the third-party warehouse industry average. Furthermore, we demonstrate the ability of to generalize across different camera angles and positions by achieving an accuracy of 85% in inventory tracking while varying the position and angle of the camera."
99,10.1016/j.eswa.2017.03.002,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85015894497,robotics,01/09/2017,not defined,not defined,not defined,not defined,incremental q-learning strategy for adaptive pid control of mobile robots,"Expert and intelligent systems are being developed to control many technological systems including mobile robots. However, the PID (Proportional-Integral-Derivative) controller is a fast low-level control strategy widely used in many control engineering tasks. Classic control theory has contributed with different tuning methods to obtain the gains of PID controllers for specific operation conditions. Nevertheless, when the system is not fully known and the operative conditions are variable and not previously known, classical techniques are not entirely suitable for the PID tuning. To overcome these drawbacks many adaptive approaches have been arisen, mainly from the field of artificial intelligent. In this work, we propose an incremental Q-learning strategy for adaptive PID control. In order to improve the learning efficiency we define a temporal memory into the learning process. While the memory remains invariant, a non-uniform specialization process is carried out generating new limited subspaces of learning. An implementation on a real mobile robot demonstrates the applicability of the proposed approach for a real-time simultaneous tuning of multiples adaptive PID controllers for a real system operating under variable conditions in a real environment."
100,10.1109/icebe.2011.28,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/6104615/,multimedia,21/10/2011 00:00,not defined,not defined,not defined,not defined,a rfid-based intelligent warehouse management system design and implementation,"A RFID-based intelligent warehouse management system (RFID-IWMS) is proposed in this paper. The RFID-IWMS helps to achieve better inventory control, as well as to improve operation efficiency. To this purpose, it automates the manual warehouse operation and provides tight integration with current warehouse management system (WMS). In this system, RFID tags are embedded in the pallets and shelves. In addition, forklifts are equipped with intelligent terminal as well as RFID reader and antenna to support automatic data scanning and storage location checking. Moreover, a middle layer software component is design to facilitate the communication between WMS, portable terminals and forklift terminals through wireless LAN. Besides, it also supports additional powerful functions such as forklift scheduling, picking sequence management and 3D shelves monitoring. The design of the system makes full use of the existing equipments and facilities and has the feature of low cost and quick inaction. Through real working practice in the distribution center of Bailian Group, the system is proved to be feasible in the aspects of both technology and cost."
101,10.1109/tencon.2019.8929612,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8929612/,health,20/10/2019 00:00,not defined,not defined,not defined,not defined,lung nodule detection from low dose ct scan using optimization on intel xeon and core processors with intel distribution of openvino toolkit,"With the advancement of AI in the field of medical imaging, medical diagnosis is getting faster and viable for medical practitioners especially for cancer diagnosis. Earlier Deep Learning solutions had to be deployed on High Performance Computing devices like GPU for achieving real time performance. But with Optimization on Intel Core and Xeon processors with Intel Distribution of OpenVINO Toolkit (Open Visual Inference and Neural Network Optimization), it is possible to deploy Deep Learning models with accelerated performance, than running Tensorflow / Caffe models on CPU machines. In this paper we describe the proposed work wherein we ported our DetectNet Deep Learning Model with NVIDIA specific custom layer for lung nodule detection trained on LIDC dataset, using Intel Distribution of OpenVINO, and deployed the same in Intel Core/Xeon processors with accelerated performance."
102,10.1109/robot.2004.1308781,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/1308781/,robotics,01/05/2004 00:00,not defined,not defined,not defined,not defined,software approach for the autonomous inspection robot makro,"The sewer inspection robot MAKRO is an autonomous multi-segment robot with worm-like shape driven by wheels. It is currently under development in the project MAKRO-PLUS. The robot has to navigate autonomously within sewer systems. Its first tasks is to take water probes, analyze them onboard, and measure positions of manholes and pipes to detect pollution loaded sewage and to improve current maps of sewer systems. One of the challenging problems is the control software, which should enable the robot to navigate in the sewer system and perform the inspection tasks autonomously, while always taking care of its own safety. Tests in our test environment and in a real sewer system show promising results. This paper focuses on the software approach. To manage the complexity a layered architecture has been chosen, each layer defining a different level of abstraction. After determining the abstraction levels, we use different methods for implementation. For the highest abstraction level a standard AI-planning algorithm is used. For the next level, finite state automata has been chosen. For ""simple"" task implementation we use a modular C++ based method (MCA2), which is also used on the lowest software level."
103,10.1109/icmla.2015.209,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7424444/,smart cities,11/12/2015 00:00,not defined,not defined,not defined,not defined,intelligent bus stop identification using smartphone sensors,"Intelligent transportation systems can be built by developing models that learn from the collected transport data. Data collection and implementation of such systems is often costly, and few countries have support for such systems in their transportation budgets. In places where maintaining currency and accuracy of information is difficult, many problems arise. For instance, in Chennai, India, real time bus transit data is not maintained, there is no proper communication about the bus schedules, bus stops are not regularly updated and inconsistent information about bus stops is observed in the transport authority's website. We are interested in developing models for identifying bus stops from trajectories for situations where accurate and current information is not available and traffic conditions are challenging, such as Chennai, India. We develop a simple yet easily accessible Android mobile application (App) to collect GPS traces of bus routes. We use our App to collect GPS trajectory data from Baltimore, Maryland, a place where there are facilities to access up-to-date information about bus stops. We also collect GPS trajectories from Chennai, India. We then develop a model using machine learning techniques to identify bus stops from the collected trajectories. We experimentally evaluate our model by training it on the Baltimore dataset and testing it on the Chennai dataset, achieving testing accuracy between 85 -- 90%. This is comparable to the accuracy of 95% achieved by both training and testing on the Chennai dataset. This illustrates that our approach is effective in helping maintain an accurate and current transport information system for resource constraint environments."
104,10.1109/isaect50560.2020.9523700,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9523700/,autonomous vehicle,27/11/2020 00:00,supervised,batch,not defined,hybrid,edge-cloud architectures using uavs dedicated to industrial iot monitoring and control applications,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud."
105,10.1109/iria53009.2021.9588707,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9588707/,multimedia,22/09/2021 00:00,not defined,not defined,not defined,not defined,automatic license plate recognition system using ssd,"Automatic License Plate Recognition (ALPR) is a very widely used system in applications such as parking management, theft detection, traffic control and management etc. Most of the existing ALPR systems fail to showcase acceptable performance on real time images/video scenes. This work proposes and demonstrates implementation of a deep learning-based approach to locate license plates of four wheeler vehicles thereby enabling optical character recognition (OCR) to recognize the characters and numbers on the located plates in real time. The proposed system is decomposed into three sub-blocks viz. Vehicle image/video acquisition, License plate localization and OCR. A simple setup using a reasonable resolution webcam has been designed to capture images/videos of vehicles at some entry point. We propose to utilize Single Shot Detector (SSD) based Mobilenet V1 architecture to localize the license plates. The hyper parameters of this architecture are selected with rigorous experimentation so as to avoid over-fitting. We have compared performance of two OCRs viz. Tesseract OCR, Easy OCR and found the superiority of Easy OCR since it utilizes deep learning approach for character recognition. NVIDIA Jetson Nano and Raspberry Pi 3B hardware platforms have been used to implement the entire system. The parameters of these three sub-blocks have been optimized to yield real time performance of ALPR with acceptable accuracy. The proposed and implemented system on Jetson Nano allows processing of videos for ALPR having accuracy more than 95%."
106,http://arxiv.org/abs/1911.01562v1,arxiv,arxiv,http://arxiv.org/abs/1911.01562v1,multimedia,05/11/2019 00:00,supervised,not defined,not defined,not defined,"deepracer: educational autonomous racing platform for experimentation
  with sim2real reinforcement learning","DeepRacer is a platform for end-to-end experimentation with RL and can be
used to systematically investigate the key challenges in developing intelligent
control systems. Using the platform, we demonstrate how a 1/18th scale car can
learn to drive autonomously using RL with a monocular camera. It is trained in
simulation with no additional tuning in physical world and demonstrates: 1)
formulation and solution of a robust reinforcement learning algorithm, 2)
narrowing the reality gap through joint perception and dynamics, 3) distributed
on-demand compute architecture for training optimal policies, and 4) a robust
evaluation method to identify when to stop training. It is the first successful
large-scale deployment of deep reinforcement learning on a robotic control
agent that uses only raw camera images as observations and a model-free
learning method to perform robust path planning. We open source our code and
video demo on GitHub: https://git.io/fjxoJ."
107,10.1109/icecce49384.2020.9179349,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9179349/,smart cities,13/06/2020 00:00,supervised,batch,classification,centralised,a cloud based smart recycling bin for in-house waste classification,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. Most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification for personal in-house usage. A centralized Information System (IS) collects measurements from smart bins that can be deployed virtually anywhere and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low compared to other implementations."
