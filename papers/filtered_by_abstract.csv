id,doi,publisher,database,url,domain,publication_date,algorithm_type,training_schema,algorithm_goal,architecture,title,abstract,status
1,10.1016/j.enconman.2021.113856,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85101129959,industry,01/04/2021,supervised,batch,decision making,centralised,the mutual benefits of renewables and carbon capture: achieved by an artificial intelligent scheduling strategy,"Renewable power and carbon capture are key technologies to transfer the power industry into low carbon generation. Renewables have been developed fast, however, the intermittent nature has imposed higher requirement for the flexibility of the power grid. Retrofitting carbon capture technologies to existing fossil-fuel fired power plants is an important solution to avoid the “lock-in” of emissions, but the high operating costs hinders their large scale application. The coexistence of renewable power and carbon capture opens up a new avenue that the deployment of carbon capture can provide additional flexibility for better accommodation of renewable power while excess renewables can be used to reduce the operating costs of carbon capture. To this end, this paper proposes an artificial intelligence based optimal scheduling strategy for the power plant-carbon capture system in the context of renewable power penetration to show that the mutual benefits between carbon capture and renewable power can be achieved when the carbon capture process is made fully adjustable. An artificial intelligent deep belief neural network is used to reflect the complex interactions between carbon, heat and electricity within the power plant carbon capture system. Multiple operating goals are considered in the scheduling such as minimizing the operating costs, renewable power curtailment and carbon emission, and the particle swarm heuristic optimization is employed to find the optimal solution. The impacts of carbon capture constraint mode, carbon emission penalty coefficient, carbon dioxide production constraints and renewable power installed capacity are investigated to provide broader insight on the potential benefit of carbon capture in future low-carbon energy system. A case study using real world data of weather condition and load demand shows that renewable power curtailment can be reduced by 51% with the integration of post-combustion capture systems and 35% of total carbon emission are captured by the use of excess renewable power through optimal scheduling. This paper points out a new way of using artificial intelligent technologies to coordinate the couplings between carbon and electricity for efficient and environmentally friendly operation of future low-carbon energy system.",experiments
2,,A multi-FPGA distributed embedded system for the emulation of Multi-Layer CNNs in real time video applications,core,'Institute of Electrical and Electronics Engineers (IEEE)',multimedia,29/03/2010 00:00,supervised,batch,classification,decentralised,https://core.ac.uk/download/60418869.pdf,"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates.This work has been partially supported by the Fundación Séneca de la Región de Murcia through the research projects 08801/PI/08 and 08788/PI/08, and by the Spanish Government through project TIN2008-06893-C03",experiments
3,10.1016/j.softx.2017.09.002,Elsevier,core,,multimedia,,not defined,not defined,not defined,not defined,confocalgn: a minimalistic confocal image generator,"Validating image analysis pipelines and training machine-learning segmentation algorithms require images with known features. Synthetic images can be used for this purpose, with the advantage that large reference sets can be produced easily. It is however essential to obtain images that are as realistic as possible in terms of noise and resolution, which is challenging in the field of microscopy. We describe ConfocalGN, a user-friendly software that can generate synthetic microscopy stacks from a ground truth (i.e. the observed object) specified as a 3D bitmap or a list of fluorophore coordinates. This software can analyze a real microscope image stack to set the noise parameters and directly generate new images of the object with noise characteristics similar to that of the sample image. With a minimal input from the user and a modular architecture, ConfocalGN is easily integrated with existing image analysis solutions. Keywords: Synthetic image, Image analysis, Bioinformatic",excluded
4,http://arxiv.org/abs/2009.10679v1,arxiv,arxiv,http://arxiv.org/abs/2009.10679v1,multimedia,22/09/2020 00:00,supervised,batch,classification,centralised,"an embedded deep learning system for augmented reality in firefighting
  applications","Firefighting is a dynamic activity, in which numerous operations occur
simultaneously. Maintaining situational awareness (i.e., knowledge of current
conditions and activities at the scene) is critical to the accurate
decision-making necessary for the safe and successful navigation of a fire
environment by firefighters. Conversely, the disorientation caused by hazards
such as smoke and extreme heat can lead to injury or even fatality. This
research implements recent advancements in technology such as deep learning,
point cloud and thermal imaging, and augmented reality platforms to improve a
firefighter's situational awareness and scene navigation through improved
interpretation of that scene. We have designed and built a prototype embedded
system that can leverage data streamed from cameras built into a firefighter's
personal protective equipment (PPE) to capture thermal, RGB color, and depth
imagery and then deploy already developed deep learning models to analyze the
input data in real time. The embedded system analyzes and returns the processed
images via wireless streaming, where they can be viewed remotely and relayed
back to the firefighter using an augmented reality platform that visualizes the
results of the analyzed inputs and draws the firefighter's attention to objects
of interest, such as doors and windows otherwise invisible through smoke and
flames.",architecture
5,10.1109/itc-egypt52936.2021.9513888,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9513888/,health,15/07/2021 00:00,not defined,not defined,not defined,centralised,a proposed end to end telemedicine system based on embedded system and mobile application using cmos wearable sensors,"Internet of things (IoT) and Embedded systems have extensive applications in healthcare markets. Integration of IoT with healthcare started with wearable smartwatches monitoring some signals and storing this data in the cloud. With 4G/5G and WiFi 6 networks. Healthcare data can be analyzed with Artificial Intelligence providing new era Internet of Medical Things (IoMT) that encompass an array of internet-capable medical devices that are in constant communication with each other or with the cloud; Internet of Healthcare Things (IoHT) that is the digital transformation of the healthcare industry. This article presents an end-to-end architecture with realization of three modules for key IoT aspects for healthcare and telemedicine. Results from a real implementation of application Platform for Data Processing including patient and doctor data base-based web site, MySQL data base, Android based mobile App, and PHP webserver.",architecture
6,http://arxiv.org/abs/2101.04930v2,arxiv,arxiv,http://arxiv.org/abs/2101.04930v2,smart cities,13/01/2021 00:00,not defined,not defined,not defined,not defined,"an empirical study on deployment faults of deep learning based mobile
  applications","Deep Learning (DL) is finding its way into a growing number of mobile
software applications. These software applications, named as DL based mobile
applications (abbreviated as mobile DL apps) integrate DL models trained using
large-scale data with DL programs. A DL program encodes the structure of a
desirable DL model and the process by which the model is trained using training
data. Due to the increasing dependency of current mobile apps on DL, software
engineering (SE) for mobile DL apps has become important. However, existing
efforts in SE research community mainly focus on the development of DL models
and extensively analyze faults in DL programs. In contrast, faults related to
the deployment of DL models on mobile devices (named as deployment faults of
mobile DL apps) have not been well studied. Since mobile DL apps have been used
by billions of end users daily for various purposes including for
safety-critical scenarios, characterizing their deployment faults is of
enormous importance. To fill the knowledge gap, this paper presents the first
comprehensive study on the deployment faults of mobile DL apps. We identify 304
real deployment faults from Stack Overflow and GitHub, two commonly used data
sources for studying software faults. Based on the identified faults, we
construct a fine-granularity taxonomy consisting of 23 categories regarding to
fault symptoms and distill common fix strategies for different fault types.
Furthermore, we suggest actionable implications and research avenues that could
further facilitate the deployment of DL models on mobile devices.",architecture
7,10.1109/ams.2017.22,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8424312/,robotics,06/12/2017 00:00,not defined,not defined,decision making,decentralised,autonomous rover navigation using gps based path planning,"Nowadays, with the constant evolution of Artificial Intelligence and Machine Learning, robots are getting more perceptive than ever. For this quality they are being used in varying circumstances which humans cannot control. Rovers are special robots, capable of traversing through areas that are too difficult for humans. Even though it is a robust bot, lack of proper intelligence and automation are its basic shortcomings. As the main purpose of a rover is to traverse through areas of extreme difficulties, therefore an intelligent path generation and following system is highly required. Our research work aimed at developing an algorithm for autonomous path generation using GPS (Global Positioning System) based coordinate system and implementation of this algorithm in real life terrain, which in our case is MDRS, Utah, USA. Our prime focus was the development of a robust but easy to implement system. After developing such system, we have been able to successfully traverse our rover through that difficult terrain. It uses GPS coordinates of target points that will be fed into the rover from a control station. The rover capturing its own GPS signal generates a path between the current location and the destination location on its own. It then finds the deviation in its current course of direction and position. And eventually it uses Proportional Integral Derivative control loop feedback mechanism (PID control algorithm) for compensating the error or deviation and thus following that path and reach destination. A low cost on board computer (Raspberry Pi in our case) handles all the calculations during the process and drives the rover fulfilling its task using an microcontroller (Arduino).",experiments
8,10.17863/cam.45198,ASCE,project-academic,project-academic,smart cities,22/10/2019 00:00,not defined,not defined,clustering,not defined,developing a dynamic digital twin at building and city levels a case study of the west cambridge campus," A Digital Twin (DT) refers to a digital replica of physical assets, processes and systems. DTs integrate artificial intelligence, machine learning and data analytics to create living digital simulation models that are able to learn and update from multiple sources, and to represent and predict the current and future conditions of physical counterparts. However, the current activities related to DTs are still at an early stage with respect to buildings and other infrastructure assets from an architectural and engineering/construction point of view. Less attention has been paid to the operation & maintenance (O&M) phase, which is the longest time span in the asset life cycle. A systematic and clear architecture verified with practical use cases for constructing a DT would be the foremost step for effective operation and maintenance of buildings and cities. According to current research about multi-tier architectures, this paper presents a system architecture for DTs which is specifically designed at both the building and city levels. Based on this architecture, a DT demonstrator of the West Cambridge site of the University of Cambridge was developed, which integrates heterogeneous data sources, supports effective data querying and analysing, supports decision-making processes in O&M management, and further bridges the gap between human relationships with buildings/cities. This paper aims at going through the whole process of developing DTs in building and city levels from the technical perspective and sharing lessons learnt and challenges involved in developing DTs in real practices. Through developing this DT demonstrator, the results provide a clear roadmap and present particular DT research efforts for asset management practitioners, policymakers and researchers to promote the implementation and development of DT at the building and city levels.",architecture
9,,,core,,multimedia,22/07/2013 00:00,supervised,batch,classification,centralised,"jie ma,","In this paper, an online methodology for the detection of unsafe driving states while driving is presented. The detection is based on the multi-sensor approaches, including gyrometer, accelerometer, radar, video and so on. Various information comes from both the ego vehicle and its surroundings are fused to gain a comprehensive understanding of driving situations. Using subspace modeling techniques, we propose an unsupervised learning algorithm to perform the unsafe states detection. The feature space are decomposed into the normal and anomalous subspace, where the normal space are assumed as the major components of the driving patterns, and significant deviations from the modeled normal subspace are signaled as unsafe states. In addition, the algorithm works in a real-time way incorporating a implementation of sliding window, which enable the method to adapt over time to address changes in the new emerged driving situations. We have implemented our algorithm with a prototype system installed in a transit bus, validations are performed in real driving situations. Our experimental results demonstrate the effectiveness of the approach on forward risk predication. We gain a timely predication while with a low false positive when there occurs conflicts between the ego vehicle and front vehicles",excluded
10,10.1109/lra.2019.2894216,IEEE,project-academic,project-academic,robotics,21/01/2019 00:00,supervised,batch,classification,centralised,vr goggles for robots real to sim domain adaptation for visual control," In this letter, we deal with the None reality gap None from a novel perspective, targeting transferring deep reinforcement learning (DRL) policies learned in simulated environments to the real-world domain for visual control tasks. Instead of adopting the common solutions to the problem by increasing the visual fidelity of synthetic images output from simulators during the training phase, we seek to tackle the problem by translating the real-world image streams back to the synthetic domain during the deployment phase, to None make the robot feel at home . We propose this as a lightweight, flexible, and efficient solution for visual control, as first, no extra transfer steps are required during the expensive training of DRL agents in simulation; second, the trained DRL agents will not be constrained to being deployable in only one specific real-world environment; and third, the policy training and the transfer operations are decoupled, and can be conducted in parallel. Besides this, we propose a simple yet effective None shift loss None that is agnostic to the downstream task, to constrain the consistency between subsequent frames which is important for consistent policy outputs. We validate the None shift loss None for None artistic style transfer for videos None and None domain adaptation , and validate our visual control approach in indoor and outdoor robotics experiments.",excluded
11,10.1016/j.eswa.2017.11.011,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85035079318,robotics,15/06/2018,rl,not defined,decision making,not defined,towards a common implementation of reinforcement learning for multiple robotic tasks,"Mobile robots are increasingly being employed for performing complex tasks in dynamic environments. Those tasks can be either explicitly programmed by an engineer or learned by means of some automatic learning method, which improves the adaptability of the robot and reduces the effort of setting it up. In this sense, reinforcement learning (RL) methods are recognized as a promising tool for a machine to learn autonomously how to do tasks that are specified in a relatively simple manner. However, the dependency between these methods and the particular task to learn is a well-known problem that has strongly restricted practical implementations in robotics so far. Breaking this barrier would have a significant impact on these and other intelligent systems; in particular, having a core method that requires little tuning effort for being applicable to diverse tasks would boost their autonomy in learning and self-adaptation capabilities. In this paper we present such a practical core implementation of RL, which enables the learning process for multiple robotic tasks with minimal per-task tuning or none. Based on value iteration methods, we introduce a novel approach for action selection, called Q-biased softmax regression (QBIASSR), that takes advantage of the structure of the state space by attending the physical variables involved (e.g., distances to obstacles, robot pose, etc.), thus experienced sets of states accelerate the decision-making process of unexplored or rarely-explored states. Intensive experiments with both real and simulated robots, carried out with the software framework also introduced here, show that our implementation is able to learn different robotic tasks without tuning the learning method. They also suggest that the combination of true online SARSA(λ) (TOSL) with QBIASSR can outperform the existing RL core algorithms in low-dimensional robotic tasks. All of these are promising results towards the possibility of learning much more complex tasks autonomously by a robotic agent.",architecture
12,10.1109/mocast49295.2020.9200283,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9200283/,smart cities,09/09/2020 00:00,supervised,batch,classification,centralised,a cloud based smart recycling bin for waste classification,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. However, most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification. A centralized Information System (IS) collects measurements from smart bins that are deployed all around the city and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low.",architecture
13,10.1016/s0925-2312(01)00500-8,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/17544395043,robotics,01/06/2001,supervised,batch,classification,centralised,a biologically inspired visual system for an autonomous robot,"We have implemented an artificial visual system that takes advantage of known properties of biological systems to achieve segmentation and recognition of simple images. The use of biologically plausible mechanisms makes the system inherit a series of features that are present in biological systems, such as flexibility, robustness and adaptability. The implementation of the model on an autonomous robot has proved its reliability and robustness in real environments and shows the relevance of this kind of approach.",excluded
14,10.1016/j.microc.2020.105038,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85085341749,health,01/09/2020,not defined,not defined,classification,not defined,a smartphone-based rapid quantitative detection platform for lateral flow strip of human chorionic gonadotropin with optimized image algorithm,"Colloidal gold immunochromatographic test strip has been widely used as a rapid, simple and low-cost correct detection technology. However, its detection is often qualitative or semi-quantitative, which limits its clinical application to some extent. Herein, a portable test strip quantitative detection device based on smartphone to detect human chorionic gonadotropin (HCG) is developed. In experiment, a colloidal gold HCG detection strip based on antigen antibody immune response is constructed, and the quantitative results of three different image processing methods on the same strip detection are compared, including the threshold processing algorithm based on location information, the RGB color component extraction algorithm and the grayscale projection value processing algorithm, the results show that the last algorithm can realize the best recognition of the region of interest of strip. The mobile phone application software (App) based on this design shows that the detection limit of constructed colloidal gold HCG strip is 3 ng/mL with a linear range of 6–300 ng/mL. The detection result of real urine sample is consistent with the spiked concentration (R2 = 0.988), indicating that the concentration of HCG can be accurately measured in urine with this method, presenting the potential for instant diagnosis.",excluded
15,10.1109/tamd.2010.2086453,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/5599851/,robotics,01/12/2010 00:00,not defined,not defined,not defined,not defined,multilevel darwinist brain (mdb): artificial evolution in a cognitive architecture for real robots,"The multilevel Darwinist brain (MDB) is a cognitive architecture that follows an evolutionary approach to provide autonomous robots with lifelong adaptation. It has been tested in real robot on-line learning scenarios obtaining successful results that reinforce the evolutionary principles that constitute the main original contribution of the MDB. This preliminary work has lead to a series of improvements in the computational implementation of the architecture so as to achieve realistic operation in real time, which was the biggest problem of the approach due to the high computational cost induced by the evolutionary algorithms that make up the MDB core. The current implementation of the architecture is able to provide an autonomous robot with real time learning capabilities and the capability for continuously adapting to changing circumstances in its world, both internal and external, with minimal intervention of the designer. This paper aims at providing an overview or the architecture and its operation and defining what is required in the path towards a real cognitive robot following a developmental strategy. The design, implementation and basic operation of the MDB cognitive architecture are presented through some successful real robot learning examples to illustrate the validity of this evolutionary approach.",architecture
16,10.23919/iccas50221.2020.9268370,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9268370/,robotics,16/10/2020 00:00,supervised,batch,classification,centralised,deep reinforcement learning-based ros-controlled rc car for autonomous path exploration in the unknown environment,"Nowadays, Deep reinforcement learning has become the front runner to solve problems in the field of robot navigation and avoidance. This paper presents a LiDAR-equipped RC car trained in the GAZEBO environment using the deep reinforcement learning method. This paper uses reshaped LiDAR data as the data input of the neural architecture of the training network. This paper also presents a unique way to convert the LiDAR data into a 2D grid map for the input of training neural architecture. It also presents the test result from the training network in different GAZEBO environment. It also shows the development of hardware and software systems of embedded RC car. The hardware system includes-Jetson AGX Xavier, teensyduino and Hokuyo LiDAR; the software system includes-ROS and Arduino C. Finally, this paper presents the test result in the real world using the model generated from training simulation.",experiments
17,10.1016/j.neucom.2008.06.019,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/58149470368,robotics,01/01/2009,unsupervised,not defined,decision making,not defined,discretized iso-learning neural network for obstacle avoidance in reactive robot controllers,"Isotropic sequence order learning (ISO-learning) and its variations, input correlation only learning (ICO-learning) and ISO three-factor learning (ISO3-learning) are unsupervised neural algorithms to learn temporal differences. As robotic software operates mainly in discrete time domain, a discretization of ISO-learning is needed to apply classical conditioning to reactive robot controllers.
                  Discretization of ISO-learning is achieved by modifications to original rules: weights sign restriction, to adequate ISO-learning devices outputs to the usually predefined kinds of connections (excitatory/inhibitory) used in neural networks, and decay term in learning rate for weights stabilization. Discrete ISO-learning devices are included into neural networks used to learn simple obstacle avoidance in the reactive control of two real robots.",excluded
18,10.1109/access.2020.2970178,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8974224/,health,01/01/2020 00:00,not defined,not defined,not defined,not defined,a novel software engineering approach toward using machine learning for improving the efficiency of health systems,"Recently, machine learning has become a hot research topic. Therefore, this study investigates the interaction between software engineering and machine learning within the context of health systems. We proposed a novel framework for health informatics: the framework and methodology of software engineering for machine learning in health informatics (SEMLHI). The SEMLHI framework includes four modules (software, machine learning, machine learning algorithms, and health informatics data) that organize the tasks in the framework using a SEMLHI methodology, thereby enabling researchers and developers to analyze health informatics software from an engineering perspective and providing developers with a new road map for designing health applications with system functions and software implementations. Our novel approach sheds light on its features and allows users to study and analyze the user requirements and determine both the function of objects related to the system and the machine learning algorithms that must be applied to the dataset. Our dataset used in this research consists of real data and was originally collected from a hospital run by the Palestine government covering the last three years. The SEMLHI methodology includes seven phases: designing, implementing, maintaining and defining workflows; structuring information; ensuring security and privacy; performance testing and evaluation; and releasing the software applications.",architecture
19,10.1109/tbme.2007.909506,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/4360143/,health,01/03/2008 00:00,supervised,batch,classification,not defined,a fully automatic cad-ctc system based on curvature analysis for standard and low-dose ct data,"Computed tomography colonography (CTC) is a rapidly evolving noninvasive medical investigation that is viewed by radiologists as a potential screening technique for the detection of colorectal polyps. Due to the technical advances in CT system design, the volume of data required to be processed by radiologists has increased significantly, and as a consequence the manual analysis of this information has become an increasingly time consuming process whose results can be affected by inter- and intrauser variability. The aim of this paper is to detail the implementation of a fully integrated CAD-CTC system that is able to robustly identify the clinically significant polyps in the CT data. The CAD-CTC system described in this paper is a multistage implementation whose main system components are: 1) automatic colon segmentation; 2) candidate surface extraction; 3) feature extraction; and 4) classification. Our CAD-CTC system performs at 100% sensitivity for polyps larger than 10 mm, 92% sensitivity for polyps in the range 5 to 10 mm, and 57.14% sensitivity for polyps smaller than 5 mm with an average of 3.38 false positives per dataset. The developed system has been evaluated on synthetic and real patient CT data acquired with standard and low-dose radiation levels.",excluded
20,10.1109/icra.2016.7487617,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7487617/,robotics,21/05/2016 00:00,supervised,not defined,not defined,not defined,decentralized multi-agent exploration with online-learning of gaussian processes,"Exploration is a crucial problem in safety of life applications, such as search and rescue missions. Gaussian processes constitute an interesting underlying data model that leverages the spatial correlations of the process to be explored to reduce the required sampling of data. Furthermore, multi-agent approaches offer well known advantages for exploration. Previous decentralized multi-agent exploration algorithms that use Gaussian processes as underlying data model, have only been validated through simulations. However, the implementation of an exploration algorithm brings difficulties that were not tackle yet. In this work, we propose an exploration algorithm that deals with the following challenges: (i) which information to transmit to achieve multi-agent coordination; (ii) how to implement a light-weight collision avoidance; (iii) how to learn the data's model without prior information. We validate our algorithm with two experiments employing real robots. First, we explore the magnetic field intensity with a ground-based robot. Second, two quadcopters equipped with an ultrasound sensor explore a terrain profile. We show that our algorithm outperforms a meander and a random trajectory, as well as we are able to learn the data's model online while exploring.",experiments
21,10.1007/s10664-017-9547-8,Springer US,project-academic,project-academic,industry,01/06/2018 00:00,supervised,batch,not defined,not defined,inference of development activities from interaction with uninstrumented applications," Studying developers’ behavior in software development tasks is crucial for designing effective techniques and tools to support developers’ daily work. In modern software development, developers frequently use different applications including IDEs, Web Browsers, documentation software (such as Office Word, Excel, and PDF applications), and other tools to complete their tasks. This creates significant challenges in collecting and analyzing developers’ behavior data. Researchers usually instrument the software tools to log developers’ behavior for further studies. This is feasible for studies on development activities using specific software tools. However, instrumenting all software tools commonly used in real work settings is difficult and requires significant human effort. Furthermore, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis. This abstraction is often performed manually or based on simple heuristics. In this paper, we propose an approach to address the above two challenges in collecting and analyzing developers’ behavior data. First, we use our ActivitySpace framework to improve the generalizability of the data collection. ActivitySpace uses operating-system level instrumentation to track developer interactions with a wide range of applications in real work settings. Secondly, we use a machine learning approach to reduce the human effort to abstract low-level behavior data. Specifically, considering the sequential nature of the interaction data, we propose a Condition Random Field (CRF) based approach to segment and label the developers’ low-level actions into a set of basic, yet meaningful development activities. To validate the generalizability of the proposed data collection approach, we deploy the ActivitySpace framework in an industry partner’s company and collect the real working data from ten professional developers’ one-week work in three actual software projects. The experiment with the collected data confirms that with initial human-labeled training data, the CRF model can be trained to infer development activities from low-level actions with reasonable accuracy within and across developers and software projects. This suggests that the machine learning approach is promising in reducing the human efforts required for behavior data analysis.",excluded
22,10.1109/cbms.2019.00041,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8787393/,multimedia,07/06/2019 00:00,supervised,batch,classification,not defined,action recognition in real homes using low resolution depth video data,"We report work in progress from interdisciplinary research on Assisted Living Technology in smart homes for older adults with mild cognitive impairments or dementia. We present our field trial, the set-up for collecting and storing data from real homes, and preliminary results on action recognition using low resolution depth video cameras. The data have been collected from seven apartments with one resident each over a period of two weeks. We propose a pre-processing of the depth videos by applying an Infinite Response Filter (IIR) for extracting the movements in the frames prior to classification. In this work we classify four actions: TV interaction (turn it on/ off and switch over), standing up, sitting down, and no movement. Our first results indicate that using the IIR filter for movement information extraction improves accuracy and can be an efficient method for recognizing actions. Our current implementation uses a convolutional long short-term memory (ConvLSTM) neural network, and achieved an average peak accuracy of 86%.",architecture
23,http://arxiv.org/abs/1805.08692v1,arxiv,arxiv,http://arxiv.org/abs/1805.08692v1,smart cities,04/05/2018 00:00,supervised,batch,classification,centralised,"assessing a mobile-based deep learning model for plant disease
  surveillance","Convolutional neural network models (CNNs) have made major advances in
computer vision tasks in the last five years. Given the challenge in collecting
real world datasets, most studies report performance metrics based on available
research datasets. In scenarios where CNNs are to be deployed on images or
videos from mobile devices, models are presented with new challenges due to
lighting, angle, and camera specifications, which are not accounted for in
research datasets. It is essential for assessment to also be conducted on real
world datasets if such models are to be reliably integrated with products and
services in society. Plant disease datasets can be used to test CNNs in real
time and gain insight into real world performance. We train a CNN object
detection model to identify foliar symptoms of diseases (or lack thereof) in
cassava (Manihot esculenta Crantz). We then deploy the model on a mobile app
and test its performance on mobile images and video of 720 diseased leaflets in
an agricultural field in Tanzania. Within each disease category we test two
levels of severity of symptoms - mild and pronounced, to assess the model
performance for early detection of symptoms. In both severities we see a
decrease in the F-1 score for real world images and video. The F-1 score
dropped by 32% for pronounced symptoms in real world images (the closest data
to the training data) due to a drop in model recall. If the potential of
smartphone CNNs are to be realized our data suggest it is crucial to consider
tuning precision and recall performance in order to achieve the desired
performance in real world settings. In addition, the varied performance related
to different input data (image or video) is an important consideration for the
design of CNNs in real world applications.",excluded
24,10.1007/978-3-030-85867-4_4,Springer,springer,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85867-4_4,industry,01/01/2021 00:00,not defined,not defined,not defined,not defined,airpa: an architecture to support the execution and maintenance of ai-powered rpa robots,"Robotic Process Automation (RPA) has quickly evolved from automating simple rule-based tasks. Nowadays, RPA is required to mimic more sophisticated human tasks, thus implying its combination with Artificial Intelligence (AI) technology, i.e., the so-called intelligent RPA. Putting together RPA with AI leads to a challenging scenario since (1) it involves professionals from both fields who typically have different skills and backgrounds, and (2) AI models tend to degrade over time which affects the performance of the overall solution. This paper describes the AIRPA project, which addresses these challenges by proposing a software architecture that enables (1) the abstraction of the robot development from the AI development and (2) the monitor, control, and maintain intelligent RPA developments to ensure its quality and performance over time. The project has been conducted in the Servinform context, a Spanish consultancy firm, and the proposed prototype has been validated with reality settings. The initial experiences yield promising results in reducing AHT (Average Handle Time) in processes where AIRPA deployed cognitive robots, which encourages exploring the support of intelligent RPA development.",architecture
25,10.1109/icra40945.2020.9196540,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9196540/,robotics,31/08/2020 00:00,rl,batch,decision making,centralised,meta reinforcement learning for sim-to-real domain adaptation,"Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.",experiments
26,10.1109/jiot.2019.2940131,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8827506/,industry,01/12/2019 00:00,not defined,not defined,not defined,not defined,a two-stage transfer learning-based deep learning approach for production progress prediction in iot-enabled manufacturing,"In make-to-order manufacturing enterprises, accurate production progress (PP) prediction is an important basis for dynamic production process optimization and on-time delivery of orders. The implementation of Internet of Things (IoT) makes it possible to take real-time production state as an important factor affecting PP. In the IoT-enabled workshop, a two-stage transfer learning-based prediction method using both historical production data and real-time state data is proposed to solve the problem of low-prediction accuracy and poor generalization performance caused by insufficient data of target order. The deep autoencoder (DAE) model with transfer learning is designed to extract the generalized features of target order in the first stage, which uses bootstrap sampling to avoid over fitting. The deep belief network (DBN) model with transfer learning is constructed to fit the nonlinear relation for PP prediction in the second stage. A real case from an IoT enabled machining workshop is taken to validate the performance of the proposed method over the other methods such as DBN, deep neural network.",experiments
27,http://arxiv.org/abs/1804.09914v1,arxiv,arxiv,http://arxiv.org/abs/1804.09914v1,multimedia,26/04/2018 00:00,supervised,batch,classification,centralised,"itelescope: intelligent video telemetry and classification in real-time
  using software defined networking","Video continues to dominate network traffic, yet operators today have poor
visibility into the number, duration, and resolutions of the video streams
traversing their domain. Current approaches are inaccurate, expensive, or
unscalable, as they rely on statistical sampling, middle-box hardware, or
packet inspection software. We present {\em iTelescope}, the first intelligent,
inexpensive, and scalable SDN-based solution for identifying and classifying
video flows in real-time. Our solution is novel in combining dynamic flow rules
with telemetry and machine learning, and is built on commodity OpenFlow
switches and open-source software. We develop a fully functional system, train
it in the lab using multiple machine learning algorithms, and validate its
performance to show over 95\% accuracy in identifying and classifying video
streams from many providers including Youtube and Netflix. Lastly, we conduct
tests to demonstrate its scalability to tens of thousands of concurrent
streams, and deploy it live on a campus network serving several hundred real
users. Our system gives unprecedented fine-grained real-time visibility of
video streaming performance to operators of enterprise and carrier networks at
very low cost.",architecture
28,10.1109/iros40897.2019.8968004,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8968004/,robotics,08/11/2019 00:00,not defined,not defined,not defined,not defined,long range neural navigation policies for the real world,"Learned Neural Network based policies have shown promising results for robot navigation. However, most of these approaches fall short of being used on a real robot due to the extensive simulated training they require. These simulations lack the visuals and dynamics of the real world, which makes it infeasible to deploy on a real robot. We present a novel Neural Net based policy, NavNet, which allows for easy deployment on a real robot. It consists of two sub policies - a high level policy which can understand real images and perform long range planning expressed in high level commands; a low level policy that can translate the long range plan into low level commands on a specific platform in a safe and robust manner. For every new deployment, the high level policy is trained on an easily obtainable scan of the environment modeling its visuals and layout. We detail the design of such an environment and how one can use it for training a final navigation policy. Further, we demonstrate a learned low-level policy. We deploy the model in a large office building and test it extensively, achieving 0.80 success rate over long navigation runs and outperforming SLAM-based models in the same settings.",experiments
29,10.1109/icit.2009.4939663,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/4939663/,autonomous vehicle,13/02/2009 00:00,supervised,batch,classification,not defined,real-time neural network based identification of a rotary-wing uav dynamics for autonomous flight,"Real time flight implementation of a neural network based black-box identification (NNID) scheme to a rotary wing unmanned aerial vehicle (RUAV) is presented in this paper. The applicability of NNID scheme for real time identification of longitudinal and lateral dynamics of the RUAV is evaluated in flight. To show the efficacy of the method for real time applications, the identification results and error statistics are provided. The challenges involved in terms of hardware implementation, computational time requirements, and real time coding are investigated and reported. Results indicate that NNID is suitable for modeling the dynamics of the RUAV in real time.",experiments
30,10.1109/aero47225.2020.9172439,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9172439/,smart cities,14/03/2020 00:00,supervised,batch,classification,not defined,"smart &amp; integrated management system - smart cities, epidemiological control tool using drones","This paper describes the development of a real application using Drones over urban regions to help the authorities at epidemiological control through a disruptive solutions based on a customizable Smart &amp; Integrated Management System (SIGI), devices and software based on the Enterprise Resource Planning (ERP) concept. Compound by management software, Drones and specific IoT devices, both referred to as sensors, the sensors collect the data of the interest areas in real time, creating a specified database. Based on the data collected from the interest areas, SIGI software has the ability to show real-time situational analysis of these areas and allows that the administrator can optimize resources (material and human) improving the efficiency of resource allocation in these areas. In addition to the development of the management software, the development of sensors to collect the information in the field and update these information to the database of the management software, are considered. The sensors will be recognized as IoT devices for the collection of meteorological data, images and command / control Drones. Initially the system will be customized, using an Artificial Intelligence tool, to collect data and identify the outbreaks of the dengue mosquito, zika and Chikungunya, nominee by risk areas. After the definition of the potential risk areas, in a complementary way, a totally customized Drone will be used to map these areas of interest, generating aerial photographs, identifying and geotagging the potential “targets”, which will allow the agents to identify potential mosquito breeding sites. After the identification of breeding areas, the next step will be the effective combat of the vectors, using the Drones to fly over the areas of interest, where biological defenses will be “dropped” over the targets to combat mosquitoes. Due some Drone flight restrictions over the cities, the whole process will be monitored by a situation room, that will be able to control the Drone remotely, access the air space controller, reads the sensors installed in the city (field), that will measure, for example, rainfall through weather stations installed in risk areas and subsequently processed by Intelligent System Integrated Management (SIGI), which will result to the information public official reflecting the situational analysis of the areas, which will enable a better management of available resources, helping the public agent, preventively in the decision making.",experiments
31,10.1109/tmscs.2015.2513741,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7368929/,multimedia,01/12/2015 00:00,supervised,not defined,not defined,not defined,"an ultra-low power, “always-on” camera front-end for posture detection in body worn cameras using restricted boltzman machines","The Internet of Things (IoTs) has triggered rapid advances in sensors, surveillance devices, wearables and body area networks with advanced Human-Computer Interfaces (HCI). One such application area is the adoption of Body Worn Cameras (BWCs) by law enforcement officials. The need to be ‘always-on’ puts heavy constraints on battery usage in these camera front-ends, thus limiting their widespread adoption. Further, the increasing number of such cameras is expected to create a data deluge, which requires large processing, transmission and storage capabilities. Instead of continuously capturing and streaming or storing videos, it is prudent to provide “smartness” to the camera front-end. This requires hardware assisted image recognition and template matching in the front-end, capable of making judicious decisions on when to trigger video capture or streaming. Restricted Boltzmann Machines (RBMs) based neural networks have been shown to provide high accuracy for image recognition and are well suited for low power and re-configurable systems. In this paper we propose an RBM based “always-on’’ camera front-end capable of detecting human posture. Aggressive behavior of the human being in the field of view will be used as a wake-up signal for further data collection and classification. The proposed system has been implemented on a Xilinx Virtex 7 XC7VX485T platform. A minimum dynamic power of 19.18 mW for a target recognition accuracy while maintaining real time constraints has been measured. The hardware-software co-design illustrates the trade-offs in the design with respect to accuracy, resource utilization, processing time and power. The results demonstrate the possibility of a true “always-on” body-worn camera system in the IoT environment.",experiments
32,,,core,,robotics,01/01/2009 00:00,not defined,not defined,decision making,not defined,research article bootstrap learning and visual processing management on mobile robots,"which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. A central goal of robotics and AI is to enable a team of robots to operate autonomously in the real world and collaborate with humans over an extended period of time. Though developments in sensor technology have resulted in the deployment of robots in specific applications the ability to accurately sense and interact with the environment is still missing. Key challenges to the widespread deployment of robots include the ability to learn models of environmental features based on sensory inputs, bootstrap off of the learned models to detect and adapt to environmental changes, and autonomously tailor the sensory processing to the task at hand. This paper summarizes a comprehensive effort towards such bootstrap learning, adaptation, and processing management using visual input. We describe probabilistic algorithms that enable a mobile robot to autonomously plan its actions to learn models of color distributions and illuminations. The learned models are used to detect and adapt to illumination changes. Furthermore, we describe a probabilistic sequential decision-making approach that autonomously tailors the visual processing to the task at hand. All algorithms are fully implemented and tested on robot platforms in dynamic environments. 1",experiments
33,10.1016/j.eswa.2012.01.059,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/84858339895,science,01/07/2012,not defined,not defined,decision making,not defined,analyzing the solutions of dea through information visualization and data mining techniques: smartdea framework,"Data envelopment analysis (DEA) has proven to be a useful tool for assessing efficiency or productivity of organizations, which is of vital practical importance in managerial decision making. DEA provides a significant amount of information from which analysts and managers derive insights and guidelines to promote their existing performances. Regarding to this fact, effective and methodologic analysis and interpretation of DEA results are very critical. The main objective of this study is then to develop a general decision support system (DSS) framework to analyze the results of basic DEA models. The paper formally shows how the results of DEA models should be structured so that these solutions can be examined and interpreted by analysts through information visualization and data mining techniques effectively. An innovative and convenient DEA solver, SmartDEA, is designed and developed in accordance with the proposed analysis framework. The developed software provides DEA results which are consistent with the framework and are ready-to-analyze with data mining tools, thanks to their specially designed table-based structures. The developed framework is tested and applied in a real world project for benchmarking the vendors of a leading Turkish automotive company. The results show the effectiveness and the efficacy of the proposed framework.",excluded
34,10.1109/access.2018.2873597,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8490234/,health,01/01/2018 00:00,supervised,batch,decision making,not defined,hierarchical semantic mapping using convolutional neural networks for intelligent service robotics,"The introduction of service robots in the public domain has introduced a paradigm shift in how robots are interacting with people, where robots must learn to autonomously interact with the untrained public instead of being directed by trained personnel. As an example, a hospital service robot is told to deliver medicine to Patient Two in Ward Three. Without awareness of what “Patient Two” or “Ward Three” is, a service robot must systematically explore the environment to perform this task, which requires a long time. The implementation of a Semantic Map allows for robots to perceive the environment similar to people by associating semantic information with spatial information found in geometric maps. Currently, many semantic mapping works provide insufficient or incorrect semantic-metric information to allow a service robot to function dynamically in human-centric environments. This paper proposes a semantic map with a hierarchical semantic organization structure based on a hybrid metric-topological map leveraging convolutional neural networks and spatial room segmentation methods. Our results are validated using multiple simulated and real environments on our lab's custom developed mobile service robot and demonstrate an application of semantic maps by providing only vocal commands. We show that this proposed method provides better capabilities in terms of semantic map labeling and retain multiple levels of semantic information.",experiments
35,10.3233/thc-140783,'IOS Press',core,,robotics,01/01/2014 00:00,not defined,not defined,not defined,not defined,cardiopulmonary performance testing using a robotics-assisted tilt table: feasibility assessment in able-bodied subjects,"BACKGROUND:

Robotics-assisted tilt table technology was introduced for early rehabilitation of neurological patients. It provides cyclical stepping movement and physiological loading of the legs. The aim of the present study was to assess the feasibility of this type of device for peak cardiopulmonary performance testing using able-bodied subjects.

METHODS:

A robotics-assisted tilt table was augmented with force sensors in the thigh cuffs and a work rate estimation algorithm. A custom visual feedback system was employed to guide the subjects' work rate and to provide real time feedback of actual work rate. Feasibility assessment focused on: (i) implementation (technical feasibility), and (ii) responsiveness (was there a measurable, high-level cardiopulmonary reaction?). For responsiveness testing, each subject carried out an incremental exercise test to the limit of functional capacity with a work rate increment of 5 W/min in female subjects and 8 W/min in males.

RESULTS:

11 able-bodied subjects were included (9 male, 2 female; age 29.6 ± 7.1 years: mean ± SD). Resting oxygen uptake (O_{2}) was 4.6 ± 0.7 mL/min/kg and O_{2}peak was 32.4 ± 5.1 mL/min/kg; this mean O_{2}peak was 81.1% of the predicted peak value for cycle ergometry. Peak heart rate (HRpeak) was 177.5 ± 9.7 beats/min; all subjects reached at least 85% of their predicted HRpeak value. Respiratory exchange ratio (RER) at O_{2}peak was 1.02 ± 0.07. Peak work rate) was 61.3 ± 15.1 W. All subjects reported a Borg CR10 value for exertion and leg fatigue of 7 or more.

CONCLUSIONS:

The robotics-assisted tilt table is deemed feasible for peak cardiopulmonary performance testing: the approach was found to be technically implementable and substantial cardiopulmonary responses were observed. Further testing in neurologically-impaired subjects is warranted",excluded
36,'edp sciences',10.1051/matecconf/201925501003,core,,robotics,01/01/2019 00:00,supervised,not defined,classification,hybrid,automatic image annotation for small and ad hoc intelligent applications using raspberry pi,"The cutting-edge technology Machine Learning (ML) is successfully applied for Business Intelligence. Among the various pre-processing steps of ML, Automatic Image Annotation (also known as automatic image tagging or linguistic indexing) is the process in which a computer system automatically assigns metadata in the form of captioning or keywords to a digital image. Automatic Image Annotation (AIA) methods (which have appeared during the last several years) make a large use of many ML approaches. Clustering and classification methods are most frequently applied to annotate images. In addition, these proposed solutions require a high computational infrastructure. However, certain real-time applications (small and ad-hoc intelligent applications) for example, autonomous small robots, gadgets, drone etc. have limited computational processing capacity. These small and ad-hoc applications demand a more dynamic and portable way to automatically annotate data and then perform ML tasks (Classification, clustering etc.) in real time using limited computational power and hardware resources. Through a comprehensive literature study we found that most image pre-processing algorithms and ML tasks are computationally intensive, and it can be challenging to run them on an embedded platform with acceptable frame rates. However, Raspberry Pi is sufficient for AIA and ML tasks that are relevant to small and ad-hoc intelligent applications. In addition, few critical intelligent applications (which require high computational resources, for example, Deep Learning using huge dataset) are only feasible to run on more powerful hardware resources. In this study, we present the framework of “Automatic Image Annotation for Small and Ad-hoc Intelligent Application using Raspberry Pi” and propose the low-cost infrastructures (single node and multi node using Raspberry Pi) and software module (for Raspberry Pi) to perform AIA and ML tasks in real time for small and ad-hoc intelligent applications. The integration of both AIA and ML tasks in a single software module (with in Raspberry Pi) is challenging. This study will helpful towards the improvement in various practical applications areas relevant to small intelligent autonomous systems",excluded
37,10.1109/iseee48094.2019.9136152,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9136152/,multimedia,20/10/2019 00:00,not defined,not defined,classification,not defined,compact isolated speech recognition on raspberry-pi based on reaction diffusion transform,"A low complexity solution for speech recognition is proposed and its implementation on a resources constrained platform, namely the Raspberry-Pi is evaluated. In order to achieve good performance with limited resources, both the feature extractor and the classifier are specially designed. A special form of feature extractor, called RDT (reaction-diffusion transform) was optimized and evaluated in conjunction with a specially designed ELM (extreme learning machine) classifier. Optimization of parameters led to very good recognition rates (up to 100%) for a small dictionary of isolated sounds representing vocal commands for automotive applications. The real time factor is sub-unitary, ensuring the realization of real time identification using the proposed method.",experiments
38,10.1109/tnsm.2021.3085097,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9444314/,multimedia,01/09/2021 00:00,supervised,not defined,classification,not defined,seta++: real-time scalable encrypted traffic analytics in multi-gbps networks,"The security and privacy of the end-users are a few of the most important components of a communication network. Though end-to-end encryption (e.g., TLS/SSL) fulfils this requirement, it makes inspecting network traffic with legacy solutions such as Deep Packet Inspection difficult. Recent Machine Learning techniques have shown outstanding performance in encrypted traffic classification. Nevertheless, such approaches require efficient flow sampling at real enterprise-scale networks due to the sheer volume of transferred data. Through this paper, we propose a holistic architecture to extract flow information of encrypted data at multi Gbps line rate using sampling and sketching mechanisms, enabling network operators to estimate flow size distribution accurately and understand the behavior of VPN-obfuscated traffic. Using over 6000 video traffic traces, under three main evaluation scenarios based on trace duration and starting time point, we show that it is possible to achieve 99% accuracy for service provider classification and over 90% accuracy for content classification for a given service provider in the best case. We also deploy our solution at an operational enterprise-scale network leveraging kernel bypassing to demonstrate its capability to efficiently sample live traffic for analytics.",architecture
39,10.1109/tns.2014.2309254,IEEE,project-academic,project-academic,industry,,supervised,batch,classification,not defined,implementation of the disruption predictor apodis in jet s real time network using the marte framework," The evolution in the past years of Machine learning techniques, as well as the technological evolution of computer architectures and operating systems, are enabling new approaches for complex problems in different areas of industry and research, where a classical approach is nonviable due to lack of knowledge of the problem's nature. A typical example of this situation is the prediction of plasma disruptions in Tokamak devices. This paper shows the implementation of a real time disruption predictor. The predictor is based on a support vector machine (SVM). The implementation was done under the MARTe framework on a six core x86 architecture. The system is connected in JET's Real time Data Network (RTDN). Online results show a high degree of successful predictions and a low rate of false alarms thus, confirming its usefulness in a disruption mitigation scheme. The implementation shows a low computational load, which in an immediate future will be exploited to increase the prediction's temporal resolution.",excluded
40,http://arxiv.org/abs/1411.3895v1,arxiv,arxiv,http://arxiv.org/abs/1411.3895v1,smart cities,14/11/2014 00:00,not defined,not defined,not defined,not defined,"learning fuzzy controllers in mobile robotics with embedded
  preprocessing","The automatic design of controllers for mobile robots usually requires two
stages. In the first stage,sensorial data are preprocessed or transformed into
high level and meaningful values of variables whichare usually defined from
expert knowledge. In the second stage, a machine learning technique is applied
toobtain a controller that maps these high level variables to the control
commands that are actually sent tothe robot. This paper describes an algorithm
that is able to embed the preprocessing stage into the learningstage in order
to get controllers directly starting from sensorial raw data with no expert
knowledgeinvolved. Due to the high dimensionality of the sensorial data, this
approach uses Quantified Fuzzy Rules(QFRs), that are able to transform
low-level input variables into high-level input variables, reducingthe
dimensionality through summarization. The proposed learning algorithm, called
Iterative QuantifiedFuzzy Rule Learning (IQFRL), is based on genetic
programming. IQFRL is able to learn rules with differentstructures, and can
manage linguistic variables with multiple granularities. The algorithm has been
testedwith the implementation of the wall-following behavior both in several
realistic simulated environmentswith different complexity and on a Pioneer 3-AT
robot in two real environments. Results have beencompared with several
well-known learning algorithms combined with different data
preprocessingtechniques, showing that IQFRL exhibits a better and statistically
significant performance. Moreover,three real world applications for which IQFRL
plays a central role are also presented: path and objecttracking with static
and moving obstacles avoidance.",experiments
41,10.1109/robot.2000.844768,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/844768/,multimedia,28/04/2000 00:00,not defined,not defined,not defined,not defined,application of automatic action planning for several work cells to the german ets-vii space robotics experiments,"Experiences in space robotics show, that the user normally has to cope with a huge amount of data. So, only robot and mission specialists are able to control the robot arm directly in teleoperation mode. By means of an intelligent robot control in cooperation with virtual reality methods, it is possible for non-robot specialists to generate tasks for a robot or an automation component intuitively. Furthermore, the intelligent robot control improves the safety of the entire system. The on-ground robot control and command station for the robot arm ERA onboard the satellite ETS-VII builds on a new resource-based action planning approach to manage robot manipulators and other automation components. In the case of ERA, the action planning system also takes care of the ""real"" robot onboard the satellite and the ""virtual"" robot in the simulation system. By means of the simulation system, the user can plan tasks ahead as well as analyze and visualize different strategies. The paper describes the mechanism of resource-based action planning, its application to different work cells, the practical experiences gained from the implementation for the on-ground robot control and command station for the robot arm ERA developed in the GETEX project as well as the services it provides to support VR-based man machine interfaces.",excluded
42,10.1109/cnna.2010.5430245,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/5430245/,multimedia,05/02/2010 00:00,not defined,not defined,not defined,decentralised,a multi-fpga distributed embedded system for the emulation of multi-layer cnns in real time video applications,"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates.",experiments
43,http://arxiv.org/abs/1802.08960v2,arxiv,arxiv,http://arxiv.org/abs/1802.08960v2,multimedia,25/02/2018 00:00,supervised,not defined,not defined,not defined,"bonnet: an open-source training and deployment framework for semantic
  segmentation in robotics using cnns","The ability to interpret a scene is an important capability for a robot that
is supposed to interact with its environment. The knowledge of what is in front
of the robot is, for example, relevant for navigation, manipulation, or
planning. Semantic segmentation labels each pixel of an image with a class
label and thus provides a detailed semantic annotation of the surroundings to
the robot. Convolutional neural networks (CNNs) are popular methods for
addressing this type of problem. The available software for training and the
integration of CNNs for real robots, however, is quite fragmented and often
difficult to use for non-experts, despite the availability of several
high-quality open-source frameworks for neural network implementation and
training. In this paper, we propose a tool called Bonnet, which addresses this
fragmentation problem by building a higher abstraction that is specific for the
semantic segmentation task. It provides a modular approach to simplify the
training of a semantic segmentation CNN independently of the used dataset and
the intended task. Furthermore, we also address the deployment on a real
robotic platform. Thus, we do not propose a new CNN approach in this paper.
Instead, we provide a stable and easy-to-use tool to make this technology more
approachable in the context of autonomous systems. In this sense, we aim at
closing a gap between computer vision research and its use in robotics
research. We provide an open-source codebase for training and deployment. The
training interface is implemented in Python using TensorFlow and the deployment
interface provides a C++ library that can be easily integrated in an existing
robotics codebase, a ROS node, and two standalone applications for label
prediction in images and videos.",experiments
44,,Delft University of Technology,core,,industry,,rl,not defined,not defined,not defined,ananke: a q-learning-based portfolio scheduler for complex industrial workflows: technical report ds-2017-001,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduces operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for datacenters with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Reinforcement learning, based in this work on Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learningbased portfolio scheduler can perform better (5–20%) and cost less (20–35%) than the considered alternatives.Distributed System",excluded
45,10.1109/jsen.2020.3041668,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9274482/,health,01/03/2021 00:00,not defined,not defined,not defined,not defined,design of a pose and force controller for a robotized ultrasonic probe based on neural networks and stochastic gradient approximation,"In medicine and engineering, the implementation of a diagnostic test using an ultrasonic sensor requires suitable contact conditions, and a correct pose to attain the best signal transmission settings. A soft sensor probe provides a good surface adaptation and forces transfer, but it introduces nonlinearities and noisy measurements, making it difficult to control the probe during a real time test by conventional algorithms. In this work, a data driven controller is developed to control force and pose of a soft contact ultrasound sensor. The adaptive controller is based on a fuzzy-rules emulated network structure with the learning algorithm using a stochastic gradient approximation. The proposed control algorithm overcomes the noise environment conditions and nonlinearities of the unknown nonlinear discrete-time system. This was numerically validated and then, experimentally tested with an industrial robotic system using an ultrasonic probe designed in our lab. The results show that the proposed controller performs well under the contact-force regulation and can find the correct contact orientation with a fast convergence.",experiments
46,10.1007/978-3-030-77070-9_10,Springer,springer,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-77070-9_10,robotics,01/01/2021 00:00,not defined,not defined,not defined,not defined,smart and intelligent chatbot assistance for future industry 4.0,"Chatbot is an implementation of artificial intelligence (AI) technology that is used to interact with human beings and make them feel like they are talking to the real person, and the chatbot helps them to solve their queries. A chatbot can provide 24 × 7 customer support so that the customer may have a good service experience by any organization. Chatbot helps to resolve the queries and respond to the questions of users. The user is providing the input to the chatbot first, and then, the same input will be processed further; this input can be in the form of text or voice. Therefore, on the basis of the given input and after processing it, the chatbot application will generate the response to the user, and the same response will be the best answer found by the chat application. This response can be in any format like text or a voice output. In this chapter, various approaches of chatbots and how they interact with users are discussed. The proposed approach is also defined using Dialogflow, and it can be accessible through mobile phones, laptops, and portable devices. Chatbots such as Facebook chatbot, WeChat chatbot, Hike chatbot called Natasha, etc. are available in the marker and will respond on the basis of their local databases (DBs). In the proposed method, the focus will be on the scalability, user interactivity, and flexibility of the system, which can be provided by adding both local and Web databases due to which our system will be more fast and accurate. Chatbot uses unification of emerging technologies like machine learning and artificial intelligence. The motive of this chapter is to improve the chatbot system to support and scale businesses and industry domain and maintain relations with customers.",architecture
47,10.1016/j.ress.2019.106700,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85073997788,robotics,01/03/2020,not defined,not defined,not defined,not defined,optimizing inspection routes in pipeline networks,"Maintaining an aging network is a challenge for many water utilities due to limited budgets and uncertainty surrounding the physical condition of buried pipeline assets. The deployment of robotic inspections provides high quality data, but these platforms have limited use due to cost and operational constraints. To facilitate cost-efficient inspections, operators need to identify high-risk assets while accounting for the effectiveness of the tools at hand. This paper addresses inspection planning with the goal of finding an optimal route while considering tool limitations. An exact integer programming formulation is presented where only three factors are used to characterize tool constraints. Two classes of solution methods are explored: 1) tree based searches, and 2) integer programming. This paper demonstrates how each method can be used to identify optimal paths within a real water distribution system. Empirical trials suggest that tree-based search methods are the most efficient when the path limit is short, but do not scale well when the path length increases. In contrast, integer-programming methods are more effective for longer path lengths but have scalability issues for large network sizes. Data preprocessing, where the input network size is reduced, can provide large computation time reductions while returning near-optimal solutions.",excluded
48,10.1109/bigdata.2018.8621926,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8621926/,science,13/12/2018 00:00,not defined,not defined,not defined,not defined,harnessing the nature of spam in scalable online social spam detection,"Disinformation in social networks has been a worldwide problem. Social users are surrounded by a huge volume of malicious links, biased comments, fake reviews, or fraudulent advertisements, etc. Traditional spam detection approaches propose a variety of statistical feature-based models to filter out social spam from a historical dataset. However, they omit the real word situation of social data, that is, social spam is fast changing with new topics or events. Therefore, traditional approaches cannot effectively achieve online detection of the ""drifting"" social spam with a fixed statistic feature set. In this paper, we present Sifter, a system which can detect online social spam in a scalable manner without the labor-intensive feature engineering. The Sifter system is two-fold: (1) a decentralized DHT-based overlay deployment for harnessing the group characteristics of social spam activities within a specific topic/event; (2) a social spam processing with the support of Recurrent Neural Network (RNN) to get rid of the traditional manual feature engineering. Results show that Sifter achieves graceful spam detection performances with the minimal size of data and good balance in group management.",architecture
49,10.1016/j.neucom.2012.04.033,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/84875966713,industry,03/06/2013,not defined,not defined,not defined,not defined,applying soft computing techniques to optimise a dental milling process,"This study presents a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. This novel intelligent procedure is based on the following phases. Firstly, a neural model extracts the internal structure and the relevant features of the data set representing the system. Secondly, the dynamic system performance of different variables is specifically modelled using a supervised neural model and identification techniques. This constitutes the model for the fitness function of the production process, using relevant features of the data set. Finally, a genetic algorithm is used to optimise the machine parameters from a non parametric fitness function. The proposed novel approach was tested under real dental milling processes using a high-precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study.",excluded
50,,"Unsupervised Learning for Subterranean Junction Recognition Based on 2D
  Point Cloud",core,,multimedia,07/06/2020 00:00,unsupervised,not defined,clustering,not defined,http://arxiv.org/abs/2006.04225,"This article proposes a novel unsupervised learning framework for detecting
the number of tunnel junctions in subterranean environments based on acquired
2D point clouds. The implementation of the framework provides valuable
information for high level mission planners to navigate an aerial platform in
unknown areas or robot homing missions. The framework utilizes spectral
clustering, which is capable of uncovering hidden structures from connected
data points lying on non-linear manifolds. The spectral clustering algorithm
computes a spectral embedding of the original 2D point cloud by utilizing the
eigen decomposition of a matrix that is derived from the pairwise similarities
of these points. We validate the developed framework using multiple data-sets,
collected from multiple realistic simulations, as well as from real flights in
underground environments, demonstrating the performance and merits of the
proposed methodology",experiments
51,10.1109/lra.2018.2799741,,project-academic,project-academic,robotics,09/09/2017 00:00,supervised,batch,not defined,not defined,how to train a cat learning canonical appearance transformations for direct visual localization under illumination change," Direct visual localization has recently enjoyed a resurgence in popularity with the increasing availability of cheap mobile computing power. The competitive accuracy and robustness of these algorithms compared to state-of-the-art feature-based methods, as well as their natural ability to yield dense maps, makes them an appealing choice for a variety of mobile robotics applications. However, direct methods remain brittle in the face of appearance change due to their underlying assumption of photometric consistency, which is commonly violated in practice. In this paper, we propose to mitigate this problem by training deep convolutional encoder-decoder models to transform images of a scene such that they correspond to a previously-seen canonical appearance. We validate our method in multiple environments and illumination conditions using high-fidelity synthetic RGB-D datasets, and integrate the trained models into a direct visual localization pipeline, yielding improvements in visual odometry (VO) accuracy through time-varying illumination conditions, as well as improved metric relocalization performance under illumination change, where conventional methods normally fail. We further provide a preliminary investigation of transfer learning from synthetic to real environments in a localization context. An open-source implementation of our method using PyTorch is available at this https URL.",excluded
52,10.1109/vlsid51830.2021.00035,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9407373/,smart cities,24/02/2021 00:00,supervised,batch,classification,not defined,binary neural network based real time emotion detection on an edge computing device to detect passenger anomaly,"Passenger safety in public transportation especially while riding in the form of shared cabs, and taxis are often ignored, and not much preventive protocols are devised. In the connected mobility world, emotion recognition from facial expressions is a possibility, however a faster processing and edge computing device to derive anomaly state inferences will be apt for further notifying about the safety of the passenger. FPGA implementation is a viable approach to not only implement in the embedded system automotive electronics, but also accelerate the inference results, hence making it as an ideal real time candidate for passenger anomaly state identification. For the same, a real time emotion detection system using facial features was implemented on FPGA. A Binary Neural Network (BNN) feeded by Local Binary Pattern (LBP) output was designed towards the development of an improved and faster emotion recognition system. LBP is configured as a preprocessing step to extract facial features that is passed on to the BNN layer for successful inference. The preprocessing method utilizes Viola-Jones (VJ) algorithm to extract facial data while removing other background information from the image. The LBP-BNN network is modelled using Facial Expression 2013 (FER-2013) data set for training. The custom hardware accelerator or the overlay is synthesized and the designed IP is implemented on FPGA for the inference. Inference is done using the trained model on FPGA to enable faster classified results. Emotion detection using facial expressions is classified to six states namely: angry, disgust, fear, happy, sad, and surprise. The LBP-BNN network is implemented in FPGA, to realize a real time facial emotion recognition by capturing the image of a person from a web camera interfaced to the FPGA acting as edge computing inference device, with acceptable accuracy. The image processing based emotion detection design is highly suitable for other applications including tracking of emotions for movement disorder patients in hospitals.",experiments
53,10.1016/j.cie.2019.06.040,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85067600850,science,01/09/2019,not defined,not defined,not defined,centralised,"bernard, an energy intelligent system for raising residential users awareness","Energy efficiency is still a hot topic today. Coming roughly the 25% of the energy consumption in EU from the residential sector, very few cheap and simple tools to promote energy efficiency in home users have been developed. The purpose of this paper is to present Bernard, a concept proof designed for filling this gap. This aims that householders become aware of their energy habits and have useful information that help them to redirect their consumption pattern. To achieve these goals, Bernard offers, through a mobile application, the home energy consumption monitoring in real time, the energy price forecast for the next hour and the appliances which are switched on, among others. Furthermore, it is important to highlight that the system has been designed with the premises of being cheap, non-intrusive, reliable and easily scalable, in order that utilities can gradually deploy and provide it to their customers, gaining at the same time valuable information for decision making and improving its corporate social image. Therefore, the adopted solution is based on a real time streaming data architecture suitable for handling huge volumes of data and applying predictive techniques on a cloud-computing environment. The paper provides a detailed description of the system and experimental results evaluating the performance of the predictive modules built. As case study, REFIT and REDD datasets were used.",architecture
54,10.1109/icac.2017.21,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8005354/,health,21/07/2017 00:00,rl,not defined,not defined,not defined,ananke: a q-learning-based portfolio scheduler for complex industrial workflows,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduced operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for data centers with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learning-based portfolio scheduler can perform better and consume fewer resources than state-of-the-art alternatives, in particular for workloads with uniform arrival patterns.",architecture
55,10.1109/fdl53530.2021.9568376,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9568376/,industry,10/09/2021 00:00,not defined,not defined,not defined,not defined,a container-based design methodology for robotic applications on kubernetes edge-cloud architectures,"Programming modern Robots' missions and behavior has become a very challenging task. The always increasing level of autonomy of such platforms requires the integration of multi-domain software applications to implement artificial intelligence, cognition, and human-robot/robot-robot interaction applications. In addition, to satisfy both functional and nonfunctional requirements such as reliability and energy efficiency, robotic SW applications have to be properly developed to take advantage of heterogeneous (Edge-Fog-Cloud) architectures. In this context, containerization and orchestration are becoming a standard practice as they allow for better information flow among different network levels as well as increased modularity in the use of software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de-facto development standards (i.e., robotic operating system - ROS - compliancy for robotic applications) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The design methodology allows for (i) integration and verification of multi-domain components since early in the design flow, (ii) task-to-container mapping techniques to guarantee minimum overhead in terms of performance and memory footprint, and (iii) multi-domain verification of functional and non-functional constraints before deployment. We present the results obtained in a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. The source code of the mobile robot is publicly available on GitHub.",architecture
56,,USENIX Association,project-academic,project-academic,multimedia,27/03/2017 00:00,not defined,not defined,not defined,centralised,pytheas enabling data driven quality of experience optimization using group based exploration exploitation," Content providers are increasingly using data-driven mechanisms to optimize quality of experience (QoE). Many existing approaches formulate this process as a prediction problem of learning optimal decisions (e.g., server, bitrate, relay) based on observed QoE of recent sessions. While prediction-based mechanisms have shown promising QoE improvements, they are necessarily incomplete as they: (1) suffer from many known biases (e.g., incomplete visibility) and (2) cannot respond to sudden changes (e.g., load changes). Drawing a parallel from machine learning, we argue that data-driven QoE optimization should instead be cast as a real-time exploration and exploitation (E2) process rather than as a prediction problem. Adopting E2 in network applications, however, introduces key architectural (e.g., how to update decisions in real time with fresh data) and algorithmic (e.g., capturing complex interactions between session features vs. QoE) challenges. We present Pytheas, a framework which addresses these challenges using a group-based E2 mechanism. The insight is that application sessions sharing the same features (e.g., IP prefix, location) can be grouped so that we can run E2 algorithms at a per-group granularity. This naturally captures the complex interactions and is amenable to realtime control with fresh measurements. Using an end-to-end implementation and a proof-of-concept deployment in CloudLab, we show that Pytheas improves video QoE over a state-of-the-art prediction-based system by up to 31% on average and 78% on 90th percentile of persession QoE.",architecture
57,10.1016/j.future.2020.04.018,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85083299223,science,01/09/2020,not defined,not defined,not defined,centralised,software-defined network for end-to-end networked science at the exascale,"Domain science applications and workflow processes are currently forced to view the network as an opaque infrastructure into which they inject data and hope that it emerges at the destination with an acceptable Quality of Experience. There is little ability for applications to interact with the network to exchange information, negotiate performance parameters, discover expected performance metrics, or receive status/troubleshooting information in real time. The work presented here is motivated by a vision for a new smart network and smart application ecosystem that will provide a more deterministic and interactive environment for domain science workflows. The Software-Defined Network for End-to-end Networked Science at Exascale (SENSE) system includes a model-based architecture, implementation, and deployment which enables automated end-to-end network service instantiation across administrative domains. An intent based interface allows applications to express their high-level service requirements, an intelligent orchestrator and resource control systems allow for custom tailoring of scalability and real-time responsiveness based on individual application and infrastructure operator requirements. This allows the science applications to manage the network as a first-class schedulable resource as is the current practice for instruments, compute, and storage systems. Deployment and experiments on production networks and testbeds have validated SENSE functions and performance. Emulation based testing verified the scalability needed to support research and education infrastructures. Key contributions of this work include an architecture definition, reference implementation, and deployment. This provides the basis for further innovation of smart network services to accelerate scientific discovery in the era of big data, cloud computing, machine learning and artificial intelligence.",architecture
58,10.1016/j.knosys.2018.04.015,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85045849458,autonomous vehicle,01/08/2018,not defined,not defined,not defined,not defined,teaching a vehicle to autonomously drift: a data-based approach using neural networks,"This paper presents a novel approach to teach a vehicle how to drift, in a similar manner that professional drivers do. Specifically, a hybrid structure formed by a Model Predictive Controller and feedforward Neural Networks is employed for this purpose. The novelty of this work lies in a) the adoption of a data-based approach to achieve autonomous drifting along a wide range of road radii and body slip angles, and b) in the implementation of a road terrain classifier to adjust the system actuation depending on the current friction characteristics. The presented drift control system is implemented in a multi-actuated ground vehicle equipped with active front steering and in-wheel electric motors and trained to drift by a real test driver using a driver-in-the-loop setup. Its performance is verified in the simulation environment IPG-CarMaker through different open loop and path following drifting manoeuvres.",excluded
59,10.1109/ijcnn48605.2020.9207332,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9207332/,robotics,24/07/2020 00:00,not defined,not defined,not defined,not defined,deep reinforcement learning control of hand-eye coordination with a software retina,"Deep Reinforcement Learning (DRL) has gained much attention for solving robotic hand-eye coordination tasks from raw pixel values. Despite promising results, training agents using images is hardware intensive often requiring millions of training steps to converge incurring long training times and increased risk of wear and tear on the robot. To speed up training, images are often cropped and downscaled resulting in a smaller field of view and loss of valuable high-frequency data. In this paper, we propose training the vision system using supervised learning prior to training robotic actuation using Deep Deterministic Policy Gradient (DDPG). The vision system uses a software retina, based on the mammalian retino-cortical transform, to preprocess full-size images to compress image data while preserving the full field of view and high-frequency visual information around the fixation point prior to processing by a Deep Convolutional Neural Network (DCNN) to extract visual state information. Using the vision system to preprocess the environment improves the agent's sample complexity and network update speed leading to significantly faster training with reduced image data loss. Our method is used to train a DRL system to control a real Baxter robot's arm, processing full-size images captured by an in-wrist camera to locate an object on a table and centre the camera over it by actuating the robot arm.",experiments
60,10.1109/iceccme52200.2021.9591113,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9591113/,health,08/10/2021 00:00,not defined,not defined,not defined,not defined,cobots for fintech,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested &amp; validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space.",architecture
61,10.1109/icra.2016.7487351,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7487351/,robotics,21/05/2016 00:00,supervised,batch,not defined,not defined,object discovery and grasp detection with a shared convolutional neural network,"Grasp an object from a stack of objects in real-time is still a challenge in robotics. This requires the robot to have the ability of both fast object discovery and grasp detection: a target object should be picked out from the stack first and then a proper grasp configuration is applied to grasp the object. In this paper, we propose a shared convolutional neural network (CNN) which can simultaneously implement these two tasks in real-time. The processing speed of the model is about 100 frames per second on a GPU which largely satisfies the requirement. Meanwhile, we also establish a labeled RGBD dataset which contains scenes of stacked objects for robotic grasping. At last, we demonstrate the implementation of our shared CNN model on a real robotic platform and show that the robot can accurately discover a target object from the stack and successfully grasp it.",experiments
62,10.1109/tnsm.2019.2929511,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8765778/,multimedia,01/09/2019 00:00,supervised,batch,classification,not defined,itelescope: softwarized network middle-box for real-time video telemetry and classification,"Video continues to dominate network traffic, yet operators today have poor visibility into the number, duration, and resolutions of the video streams traversing their domain. Current monitoring approaches are inaccurate, expensive, or unscalable, as they rely on statistical sampling, middle-box hardware, or packet inspection software. We present iTelescope, the first intelligent, inexpensive, and scalable softwarized network middle-box solution for identifying and classifying video flows in realtime. Our solution is novel in combining dynamic flow rules with telemetry and machine learning, and is built on commodity OpenFlow switches and open-source software. We develop a fully functional system, train it in the lab using multiple machine learning algorithms, and validate its performance to show over 95% accuracy in identifying and classifying video streams from many providers, including YouTube and Netflix. Lastly, we conduct tests to demonstrate its scalability to tens of thousands of concurrent streams, and deploy it live on a campus network serving several hundred real users. Our traffic monitoring system gives unprecedented fine-grained real-time visibility of video streaming performance to operators of enterprise and carrier networks at very low cost.",architecture
63,10.1016/j.robot.2021.103891,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85116222985,robotics,01/12/2021,supervised,batch,not defined,not defined,hybrid autonomous controller for bipedal robot balance with deep reinforcement learning and pattern generators[formula presented],"Recovering after an abrupt push is essential for bipedal robots in real-world applications within environments where humans must collaborate closely with robots. There are several balancing algorithms for bipedal robots in the literature, however most of them either rely on hard coding or power-hungry algorithms. We propose a hybrid autonomous controller that hierarchically combines two separate, efficient systems, to address this problem. The lower-level system is a reliable, high-speed, full state controller that was hardcoded on a microcontroller to be power efficient. The higher-level system is a low-speed reinforcement learning controller implemented on a low-power onboard computer. While one controller offers speed, the other provides trainability and adaptability. An efficient control is then formed without sacrificing adaptability to new dynamic environments. Additionally, as the higher-level system is trained via deep reinforcement learning, the robot could learn after deployment, which is ideal for real-world applications. The system’s performance is validated with a real robot recovering after a random push in less than 5 s, with minimal steps from its initial positions. The training was conducted using simulated data.",experiments
64,,,project-academic,project-academic,industry,01/01/2006 00:00,supervised,not defined,classification,centralised,a medical claim fraud abuse detection system based on data mining a case study in chile," This paper describes an effective medical claim fraud/abuse detection system based on data mining used by a Chilean private health insurance company. Fraud and abuse in medical claims have become a major concern within health insurance companies in Chile the last years due to the increasing losses in revenues. Processing medical claims is an exhausting manual task carried out by a few medical experts who have the responsibility of approving, modifying or rejecting the subsidies requested within a limited period from their reception. The proposed detection system uses one committee of multilayer perceptron neural networks (MLP) for each one of the entities involved in the fraud/abuse problem: medical claims, affiliates, medical professionals and employers. Results of the fraud detection system show a detection rate of approximately 75 fraudulent and abusive cases per month, making the detection 6.6 months earlier than without the system. The application of data mining to a real industrial problem through the implementation of an automatic fraud detection system changed the original non-standard medical claims checking process to a standardized process helping to fight against new, unusual and known fraudulent/abusive behaviors.",excluded
65,10.1109/infcomw.2016.7562053,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7562053/,multimedia,14/04/2016 00:00,not defined,not defined,decision making,centralised,resource provisioning and profit maximization for transcoding in information centric networking,"Adaptive bitrate streaming (ABR) has been widely adopted to support video streaming services over heterogeneous devices and varying network conditions. With ABR, each video content is transcoded into multiple representations in different bitrates and resolutions. However, video transcoding is computing intensive, which requires the transcoding service providers to deploy a large number of servers for transcoding the video contents published by the content producers. As such, a natural question for the transcoding service provider is how to provision the computing resource for transcoding the video contents while maximizing service profit. To address this problem, we design a cloud video transcoding system by taking the advantage of cloud computing technology to elastically allocate computing resource. We propose a method for jointly considering the task scheduling and resource provisioning problem in two timescales, and formulate the service profit maximization as a two-timescale stochastic optimization problem. We derive some approximate policies for the task scheduling and resource provisioning. Based on our proposed methods, we implement our open source cloud video transcoding system Morph and evaluate its performance in a real environment. The experiment results demonstrate that our proposed method can reduce the resource consumption and achieve a higher profit compared with the baseline schemes.",architecture
66,,,project-academic,project-academic,robotics,14/03/2021 00:00,not defined,batch,not defined,not defined,success weighted by completion time a dynamics aware evaluation criteria for embodied navigation," We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent's dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exemplifies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an environment containing obstacles. We experiment with deep reinforcement learning and reward shaping to train and compare the navigation performance of agents with different dynamics models. In evaluating these agents, we show that in contrast to SPL, SCT is able to capture the advantages in navigation speed a unicycle model has over a simpler point-turn model of dynamics. Lastly, we show that we can successfully deploy our trained models and algorithms outside of simulation in the real world. We embody our agents in an real robot to navigate an apartment, and show that they can generalize in a zero-shot manner.",experiments
67,10.1109/cvpr.2019.00346,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8953386/,multimedia,20/06/2019 00:00,not defined,not defined,not defined,not defined,densefusion: 6d object pose estimation by iterative dense fusion,"A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose.",experiments
68,10.1109/iceeccot46775.2019.9114716,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9114716/,multimedia,14/12/2019 00:00,not defined,not defined,not defined,not defined,facial recognition using machine learning algorithms on raspberry pi,"Facial recognition is a non-invasive method of biometric authentication and useful for numerous applications. The real time implementation of the algorithm with adequate accuracy is required, with hardware timing into consideration. This paper deals with the implementation of machine learning algorithm for real time facial image recognition. Two dominant methods out of many facial recognition methods are discussed, simulated and implemented using Raspberry Pi. A rigorous comparative analysis is presented considering various limitations which may be the case required for innumerable application which utilize facial recognition. The drawbacks and different use cases of each method is highlighted. The facial recognition software uses algorithms to compare a digital image captured through a camera, to the stored face print so as to authenticate a person's identity. The Haar-Cascade method was one of the first methods developed for facial recognition. The HOG (Histogram of Oriented Gradients) method has worked very effectively for object recognition and thus suitable for facial recognition also. Both the methods are compared with Eigen feature-based face recognition algorithm. Various important features are experimented like speed of operation, lighting condition, frontal face profile, side profiles, distance of image, size of image etc. The facial recognition model is implemented to detect and recognize faces in real-time by means of Raspberry Pi and Pi camera for the user defined database in addition to the available databases.",experiments
69,10.1109/fccm.2017.58,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7966655/,science,02/05/2017 00:00,not defined,not defined,not defined,not defined,accelerating large-scale graph analytics with fpga and hmc,"Graph analytics that explores the relationship among interconnected entities is becoming increasingly important due to its broad applicability from machine learning to social science. However, one major challenge for graph processing systems is the irregular data access pattern of graph computation which can significantly degrade the performance. The algorithms, software, and hardware that have been tailored for mainstream parallel applications are, as a result, generally not effective for massive-scale sparse graphs from the real world due to their complexity and irregularity. To address the performance issues in large-scale graph analytics, we combine the emerging Hybrid Memory Cube (HMC) with a modern FPGA in order to achieve exceptional random access performance without any loss of flexibility or efficiency in computation. In particular, we develop collaborative software/hardware techniques to perform a level-synchronized breadth first search (BFS) on the FPGA-HMC platform. From the software perspective, we develop an architecture-aware graph clustering algorithm that fully exploits the platform's capability to improve data locality and memory access efficiency. For each input graph, this algorithm provides an efficient data layout that allows the FPGA to coalesce memory requests into the largest possible HMC payload requests so that the number of memory requests, which is the primary factor in runtime, can be minimized. From the hardware perspective, we further improve the FPGA-HMC graph processor architecture by adding a merging unit. The merging unit takes the best advantage of the increased data locality resulting from graph clustering. We evaluated the performance of our BFS implementation using the AC-510 development kit from Micron over a set of benchmarks from a wide range of applications. We observed that the combination of the clustering algorithm and the merging hardware achieved 2.8 × average performance improvement compared to the latest FPGA-HMC based graph processing system.",excluded
70,http://arxiv.org/abs/2109.07165v1,arxiv,arxiv,http://arxiv.org/abs/2109.07165v1,multimedia,15/09/2021 00:00,supervised,not defined,classification,not defined,3d annotation of arbitrary objects in the wild,"Recent years have produced a variety of learning based methods in the context
of computer vision and robotics. Most of the recently proposed methods are
based on deep learning, which require very large amounts of data compared to
traditional methods. The performance of the deep learning methods are largely
dependent on the data distribution they were trained on, and it is important to
use data from the robot's actual operating domain during training. Therefore,
it is not possible to rely on pre-built, generic datasets when deploying robots
in real environments, creating a need for efficient data collection and
annotation in the specific operating conditions the robots will operate in. The
challenge is then: how do we reduce the cost of obtaining such datasets to a
point where we can easily deploy our robots in new conditions, environments and
to support new sensors? As an answer to this question, we propose a data
annotation pipeline based on SLAM, 3D reconstruction, and 3D-to-2D geometry.
The pipeline allows creating 3D and 2D bounding boxes, along with per-pixel
annotations of arbitrary objects without needing accurate 3D models of the
objects prior to data collection and annotation. Our results showcase almost
90% Intersection-over-Union (IoU) agreement on both semantic segmentation and
2D bounding box detection across a variety of objects and scenes, while
speeding up the annotation process by several orders of magnitude compared to
traditional manual annotation.",experiments
71,10.1109/rtsi50628.2021.9597339,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9597339/,industry,09/09/2021 00:00,not defined,not defined,not defined,not defined,towards graph machine learning for smart grid knowledge graphs in industrial scenarios,"Knowledge Graphs (KGs) demonstrated promising application perspective in different scenarios, especially when combined with Graph Machine Learning (GML) techniques able to interpret and infer over facts. Given the natural network structures of Smart Grid equipment and the exponential growth of electric power data, Smart Grid Knowledge Graphs (SGKGs) provides unprecedented opportunities to manage massive power resources and provide intelligent applications. However, a single representation of the SGKGs is never sufficient to properly exploit GML techniques that leverage different aspects of the KG for various objectives. In this work, we provide a methodology to extract various significant views of the SGKG by iteratively applying a series of transformation to the description of the power network in the IEC CIM standard. Our implementation is based on a declarative approach to guarantee easier portability, and we deploy the transformations as a stateless microservice, facilitating modular integration with the rest of the Smart Grid Semantic Platform. Experimental evaluation on two real power distribution networks demonstrates the efficacy of our approach in highlighting important topological information, without discarding precious additional knowledge present in the SGKG.",excluded
72,10.1109/aero.2005.1559665,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/1559665/,health,12/03/2005 00:00,not defined,not defined,not defined,not defined,health monitoring: new techniques based on vibrations measurements and identification algorithms,"Purpose of the paper is to present an innovative application inside the nondestructive testing field based on vibrations measurements, developed, at the Department of Aeronautical Engineering of the University of Naples ""Federico II"" (Italy), by the authors during the last three years, and already tested for analysing damages of many structural elements. The aim has been the development of a nondestructive test (NDT) which meet to most of the mandatory requirements for effective health monitoring systems, simultaneously reducing as much as possible the complexity of the data analysis algorithm and of the experimental acquisition instrumentation; these peculiarities may, in fact, not be neglected for an operative implementation of such a system. The proposed new method is based on the acquisition and comparison of frequency response functions (FRFs) of the monitored structure before and after an occurred damage. Structural damages modify the dynamical behaviour of the structure such as mass, stiffened and damping, and consequently the FRFs of the damaged structure in comparison with the FRFs of the sound structure, making possible to identify, to localize and quantify a structural damage. The activities, presented in the paper, mostly focused on a new FRFs processing technique based on the determining of a representative ""damage index"" for identifying and analysing damages both on real scale aeronautical structural components, like large-scale fuselage reinforced panels, and on aeronautical composite panels. Besides it has been carried out a dedicated neural network algorithm aiming at obtaining a ""recognition-based learning""; this kind of learning methodology permits to train the neural network in order to let it recognise only ""positive"" examples discarding as a consequence the ""negative"" ones. Within the structural NDT a ""positive"" example means ""healthy"" state of the analysed structural component and, obviously, a ""negative"" one means a ""damaged"" or perturbed state. With this object in view the neural network has been trained making use of the same FRFs of the healthy structure used for the determining of the damage index, as positive examples. From an architectural point of view magnetostrictive devices have been tested as actuators, and piezoceramic patches as actuators and sensors. Besides it has been used a laser-scanning vibrometer system to validate the behaviour of the piezoceramic patches and define their technical parameters in order to lay the bases for design a light and reliability system. These techniques promise to bring a step forward for the implementation of an automatic ""health monitoring"" system which will be able to identify a structural damage in real time, improving the safety and reducing maintenance costs",excluded
73,10.1109/access.2019.2927461,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8760232/,health,01/01/2019 00:00,not defined,not defined,not defined,not defined,ephort: towards a reference architecture for tele-rehabilitation systems,"In recent years, the software applications for medical assistance, including the tele-rehabilitation, have known a high and a continuous presence in the medical area. The ePHoRt is a Web-based platform for the remote home monitoring rehabilitation exercises in patients after hip replacement surgery. It involves a learning phase and a serious game scheme for the execution and evaluation of the exercises as part of a therapeutic program. Modular software architecture is proposed, under the patient perspective, to be used as a reference model for researchers or professionals who wish to carry out tele-rehabilitation platforms, and to guarantee security, flexibility, and scalability. The architecture incorporates two main components. The first one manages the patient' therapeutic programs taking into account two principles: 1) maintain loose coupling between the layers of the framework and 2) Don't Repeat Yourself (DRY). The second one evaluates the performed exercises in real time considering an independent acquisition mechanism for the patient movements and two artificial algorithms. The first algorithm allows evaluating the quality of the movements, while the second one allows assessing the levels of pain intensity by recognizing the patient' emotions when performing the movements. Details of the components and the meta-model of the architecture are presented and discussed considering their advantages and disadvantages.",architecture
74,10.1109/aero50100.2021.9438232,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9438232/,multimedia,13/03/2021 00:00,not defined,not defined,not defined,not defined,a pipeline for vision-based on-orbit proximity operations using deep learning and synthetic imagery,"Deep learning has become the gold standard for image processing over the past decade. Simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. However, two key challenges currently pose a major barrier to the use of deep learning for vision-based on-orbit proximity operations. Firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. Secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. This paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. The core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. The first tool leverages Blender, an open-source 3D graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. The second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. Time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. The presented system has been used in the Texas Spacecraft Laboratory with marked benefits in development speed and quality. Remote development, scalable compute, and automatic organization of data and artifacts have dramatically decreased iteration time while increasing reproducibility and system comprehension. Diverse, high-fidelity synthetic images that more closely replicate the real environment have improved model performance against real-world data. These results demonstrate that the presented pipeline offers tangible benefits to the application of deep learning for vision-based on-orbit proximity operations.",architecture
75,10.1109/aero.2018.8396807,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8396807/,autonomous vehicle,10/03/2018 00:00,not defined,not defined,not defined,not defined,learning safe recovery trajectories with deep neural networks for unmanned aerial vehicles,"Unmanned vehicles that use vision sensors for perception to aid autonomous flight are a highly popular area of research. However, these systems are often prone to failures that are often hard to model. Previous work has focused on using deep learning to detect these failures. In this work, we build on these failure detection systems and develop a pipeline that learns to identify the correct trajectory to execute that restores the vision system and the unmanned vehicle to a safe state. The key challenge with using a deep learning pipeline for this problem is the limited amount of training data available from a real world system. Ideally one requires millions of data points to sufficiently train a model from scratch. However, this is not feasible for an unmanned aerial vehicle. The dataset we operate with is limited to 400-500 points. To sufficiently learn from such a small dataset we leverage the idea of transfer learning and non linear dimensionality reduction. We deploy our pipeline on an unmanned aerial vehicle flying autonomously through outdoor clutter (in a GPS denied environment) and show that we are able to achieve long durations of safe autonomous flight.",excluded
76,10.1109/tase.2020.3032075,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9246671/,industry,01/10/2021 00:00,not defined,not defined,not defined,not defined,a virtual mechanism approach for exploiting functional redundancy in finishing operations,"We propose a new approach to programming by the demonstration of finishing operations. Such operations can be carried out by industrial robots in multiple ways because an industrial robot is typically functionally redundant with respect to a finishing task. In the proposed system, a human expert demonstrates a finishing operation, and the demonstrated motion is recorded in the Cartesian space. The robot’s kinematic model is augmented with a virtual mechanism, which is defined according to the applied finishing tool. This way, the kinematic model is expanded with additional degrees of freedom that can be exploited to compute the optimal joint space motion of the robot without altering the essential aspects of the Cartesian space task execution as demonstrated by the human expert. Finishing operations, such as polishing and grinding, occur in contact with the treated workpiece. Since information about the contact point position is needed to control the robot during the operation, we have developed a novel approach for accurate estimation of contact points using the measured forces and torques. Finally, we applied iterative learning control to refine the demonstrated operations and compensate for inaccurate calibration and different dynamics of the robot and human demonstrator. The proposed method was verified on real robots and real polishing and grinding tasks. <italic>Note to Practitioners</italic>—This work was motivated by the need for automation of finishing operations, such as polishing and grinding, on contemporary industrial robots. Existing approaches are both too complex and too time-consuming to be applied in flexible and small-scale production, which often requires the frequent deployment of new applications. Our approach is based on programming by demonstration and enables the programming of finishing operations also for users who are not specialists in robot programming. Programming by demonstration is especially useful for teaching finishing operations because it enables the transfer of expert knowledge about finishing skills to robots without providing lengthy task descriptions or manual coding. Besides the human demonstration of the desired operation, the proposed approach also requires the availability of the kinematic model for the machine tool applied to carry out the finishing operation. We provide several practical examples of grinding and polishing tools and how to integrate them into our approach. Another feature of the proposed system is that user demonstrations of finishing operations can be transferred between different combinations of robots and machine tools.",experiments
77,10.1109/sysose.2017.7994953,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7994953/,autonomous vehicle,21/06/2017 00:00,not defined,not defined,not defined,not defined,autonomous decision making for a driver-less car,"Autonomous driving has been a hot topic with companies like Google, Uber, and Tesla because of the complexity of the problem, seemingly endless applications, and capital gain. The technology's brain child is DARPA's autonomous urban challenge from over a decade ago. Few companies have had some success in applying algorithms to commercial cars. These algorithms range from classical control approaches to Deep Learning. In this paper, we will use Deep Learning techniques and the Tensor flow framework with the goal of navigating a driverless car through an urban environment. The novelty in this system is the use of Deep Learning vs. traditional methods of real-time autonomous operation as well as the application of the Tensorflow framework itself. This paper provides an implementation of AlexNet's Deep Learning model for identifying driving indicators, how to implement them in a real system, and any unforeseen drawbacks to these techniques and how these are minimized and overcome.",experiments
78,10.1109/lars-sbr.2016.49,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7783535/,robotics,12/10/2016 00:00,not defined,not defined,not defined,not defined,integration of people detection and simultaneous localization and mapping systems for an autonomous robotic platform,"This paper presents the implementation of a people detection system for a robotic platform able to perform Simultaneous Localization and Mapping (SLAM), allowing the exploration and navigation of the robot considering people detection interaction. The robotic platform consists of a Pioneer 3DX robot equipped with an RGB-D camera, a Sick Lms200 sensor laser and a computer using the robot operating system ROS. The idea is to integrate the people detection system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. Furthermore, this paper presents an evaluation of two different approaches for the people detection system. The first one uses a manual feature extraction technique, and the other one is based on deep learning methods. The manual feature extraction method in the first approach is based on HOG (Histogram of Oriented Gradients) detectors. The accuracy of the techniques was evaluated using two different libraries. The PCL library (Point Cloud Library) implemented in C ++ and the VLFeat MatLab library with two HOG variants, the original one, and the DPM (Deformable Part Model) variant. The second approaches are based on a Deep Convolutional Neural Network (CNN), and it was implemented using the MatLab MatConvNet library. Tests were made objecting the evaluation of losses and false positives in the people's detection process in both approaches. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment.",experiments
79,10.1109/camad.2018.8515001,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8515001/,smart cities,19/09/2018 00:00,not defined,not defined,not defined,not defined,uncertainty management for wearable iot wristband sensors using laplacian-based matrix completion,"Contemporary sensing devices provide reliable mechanisms for continuous process monitoring, accommodating use cases related to mHealth and smart mobility, by generating real-time data streams of numerous physiological and vital parameters. Such data streams can be later utilized by machine learning algorithms and decision support systems to predict critical clinical states and motivate users to adopt behaviours that improve the quality of their life and the society as a whole. However, in many cases, even when deployed over highly sophisticated, cutting-edge network infrastructure and deployment paradigms, data may exhibit missing values and non-uniformities due to various reasons, including device malfunction, deliberate data reduction for efficient processing, or data loss due to sensing and communication failures. This work proposes a novel approach to deal with missing entries in heart rate measurements. Benefiting from the low-rank property of the generated data matrices and the proximity of neighbouring measurements, we provide a novel method that combines classical matrix completion approaches with weighted Laplacian interpolation offering high reconstruction accuracy at fast execution times. Extensive evaluation studies carried out with real measurements show that the proposed methods could be effectively deployed by modern wristband-cloud computing systems increasing the robustness, the reliability and the energy efficiency of these systems.",experiments
80,10.1016/j.neunet.2013.04.005,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/84880792738,robotics,01/09/2013,not defined,not defined,not defined,not defined,fpga implementation of a configurable neuromorphic cpg-based locomotion controller,"Neuromorphic engineering is a discipline devoted to the design and development of computational hardware that mimics the characteristics and capabilities of neuro-biological systems. In recent years, neuromorphic hardware systems have been implemented using a hybrid approach incorporating digital hardware so as to provide flexibility and scalability at the cost of power efficiency and some biological realism. This paper proposes an FPGA-based neuromorphic-like embedded system on a chip to generate locomotion patterns of periodic rhythmic movements inspired by Central Pattern Generators (CPGs). The proposed implementation follows a top-down approach where modularity and hierarchy are two desirable features. The locomotion controller is based on CPG models to produce rhythmic locomotion patterns or gaits for legged robots such as quadrupeds and hexapods. The architecture is configurable and scalable for robots with either different morphologies or different degrees of freedom (DOFs). Experiments performed on a real robot are presented and discussed. The obtained results demonstrate that the CPG-based controller provides the necessary flexibility to generate different rhythmic patterns at run-time suitable for adaptable locomotion.",experiments
81,10.1016/j.imu.2020.100335,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85084287220,health,01/01/2020,not defined,not defined,not defined,not defined,spark architecture for deep learning-based dose optimization in medical imaging,"Background and objectives
                  Deep Learning (DL) and Machine Learning (ML) have brought several breakthroughs to biomedical image analysis by making available more consistent and robust tools for the identification, classification, reconstruction, denoising, quantification, and segmentation of patterns in biomedical images. Recently, some applications of DL and ML in Computed Tomography (CT) scans for low dose optimization were developed. Nowadays, DL algorithms are used in CT to perform replacement of missing data (processing technique) such as low dose to high dose, sparse view to full view, low resolution to high resolution, and limited angle to full angle. Thus, DL comes with a new vision to process biomedical data imagery from CT scan. It becomes important to develop architectures and/or methods based on DL algorithms for minimizing radiation during a CT scan exam thanks to reconstruction and processing techniques.
               
                  Methods
                  This paper describes DL for CT scan low dose optimization, shows examples described in the literature, briefly discusses new methods used in CT scan image processing, and offers conclusions. We based our study on the literature and proposed a pipeline for low dose CT scan image reconstruction. Our proposed pipeline relies on DL and the Spark Framework using MapReduce programming. We discuss our proposed pipeline with those proposed in the literature to conclude the efficiency and importance.
               
                  Results
                  An architecture for low dose optimization using CT imagery is suggested. We used the Spark Framework to design the architecture. The proposed architecture relies on DL, and permits us to develop efficient and appropriate methods to process dose optimization with CT scan imagery. The real implementation of our pipeline for image denoising shows that we can reduce the radiation dose, and use our proposed pipeline to improve the quality of the captured image.
               
                  Conclusion
                  The proposed architecture based on DL is complete and enables faster processing of biomedical CT imagery as compared with prior methods described in the literature.",excluded
82,10.1109/aiam48774.2019.00157,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8950866/,industry,18/10/2019 00:00,not defined,not defined,not defined,not defined,a digital twin-based approach for quality control and optimization of complex product assembly,"To address the problems caused by low ability of quality analysis and decision-making in the process of complex product assembly, in this paper, we proposed a digital twin-based approach for quality control and optimization of complex product assembly, by providing a digital twin system to realize the timely and precisely interactive mapping between the physical world and digital world. Specifically, a quality control and optimization mechanism is presented, which provides the theoretical support to the realization of the digital twin-based approach. A data-driven quality control model is introduced to solve the optimization problem by considering the panoramic assembly quality data. A digital twin system for complex product assembly is elaborated by providing detailed deployment and implementation procedures, which includes (1) building of the digital entity of an assembly line, (2) real-time online sensing in multi-source heterogeneous environment, (3) real-time simulation of equipment and assembly process, (4) realization of the intelligent production scheduling under uncertainty conditions, and (5) dynamical adjustment of the assembly process. Finally, the paper presents the validation results considering the practical applications of the proposed approach in real industrial fields.",architecture
83,10.1109/cvpr.2019.00437,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8953663/,multimedia,20/06/2019 00:00,not defined,not defined,not defined,not defined,learning to film from professional human motion videos,"We investigate the problem of 6 degrees of freedom (DOF) camera planning for filming professional human motion videos using a camera drone. Existing methods either plan motions for only a pan-tilt-zoom (PTZ) camera, or adopt ad-hoc solutions without carefully considering the impact of video contents and previous camera motions on the future camera motions. As a result, they can hardly achieve satisfactory results in our drone cinematography task. In this study, we propose a learning-based framework which incorporates the video contents and previous camera motions to predict the future camera motions that enable the capture of professional videos. Specifically, the inputs of our framework are video contents which are represented using subject-related feature based on 2D skeleton and scene-related features extracted from background RGB images, and camera motions which are represented using optical flows. The correlation between the inputs and output future camera motions are learned via a sequence-to-sequence convolutional long short-term memory (Seq2Seq ConvLSTM) network from a large set of video clips. We deploy our approach to a real drone cinematography system by first predicting the future camera motions, and then converting them to the drone's control commands via an odometer. Our experimental results on extensive datasets and showcases exhibit significant improvements in our approach over conventional baselines and our approach can successfully mimic the footage of a professional cameraman.",experiments
84,10.1016/j.ifacol.2020.12.1459,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85105082300,autonomous vehicle,01/01/2020,supervised,not defined,not defined,not defined,deep learning based segmentation of fish in noisy forward looking mbes images,"In this work, we investigate a Deep Learning (DL) approach to fish segmentation in a small dataset of noisy low-resolution images generated by a forward-looking multibeam echosounder (MBES). We build on recent advances in DL and Convolutional Neural Networks (CNNs) for semantic segmentation and demonstrate an end-to-end approach for a fish/non-fish probability prediction for all range-azimuth positions projected by an imaging sonar. We use self-collected datasets from the Danish Sound and the Faroe Islands to train and test our model and present techniques to obtain satisfying performance and generalization even with a low-volume dataset. We show that our model proves the desired performance and has learned to harness the importance of semantic context and take this into account to separate noise and non-targets from real targets. Furthermore, we present techniques to deploy models on low-cost embedded platforms to obtain higher performance fit for edge environments – where compute and power are restricted by size/cost – for testing and prototyping.",excluded
85,10.1145/3326285.3329051,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9068647/,multimedia,25/06/2019 00:00,not defined,not defined,not defined,not defined,leap: learning-based smart edge with caching and prefetching for adaptive video streaming,"Dynamic Adaptive Streaming over HTTP (DASH) has emerged as a popular approach for video transmission, which brings a potential benefit for the Quality of Experience (QoE) because of its segment-based flexibility. However, the Internet can only provide no guaranteed delivery. The high dynamic of the available bandwidth may cause bitrate switching or video rebuffering, thus inevitably damaging the QoE. Besides, the frequently requested popular videos are transmitted for multiple times and contribute to most of the bandwidth consumption, which causes massive transmission redundancy. Therefore, we propose a Learning-based Edge with cAching and Prefetching (LEAP) to improve the online user QoE of adaptive video streaming. LEAP introduces caching into the edge to reduce the redundant video transmission and employs prefetching to fight against network jitters. Taking the state information of users into account, LEAP intelligently makes the most beneficial decisions of caching and prefetching by a QoE-oriented deep neural network model. To demonstrate the performance of our scheme, we deploy the implemented prototype of LEAP in both the simulated scenario and the real Internet. Compared with all selected schemes, LEAP at least raises average bitrate by 34.4% and reduces video rebuffering by 42.7%, which leads to at least 15.9% improvement in the user QoE in the simulated scenario. The results in the real Internet scenario further confirm the superiority of LEAP.",architecture
86,,"Deep Learning-Based Multiple Object Visual Tracking on Embedded System
  for IoT and Mobile Edge Computing Applications",core,,multimedia,31/07/2018 00:00,supervised,batch,classification,not defined,http://arxiv.org/abs/1808.01356,"Compute and memory demands of state-of-the-art deep learning methods are
still a shortcoming that must be addressed to make them useful at IoT
end-nodes. In particular, recent results depict a hopeful prospect for image
processing using Convolutional Neural Netwoks, CNNs, but the gap between
software and hardware implementations is already considerable for IoT and
mobile edge computing applications due to their high power consumption. This
proposal performs low-power and real time deep learning-based multiple object
visual tracking implemented on an NVIDIA Jetson TX2 development kit. It
includes a camera and wireless connection capability and it is battery powered
for mobile and outdoor applications. A collection of representative sequences
captured with the on-board camera, dETRUSC video dataset, is used to exemplify
the performance of the proposed algorithm and to facilitate benchmarking. The
results in terms of power consumption and frame rate demonstrate the
feasibility of deep learning algorithms on embedded platforms although more
effort to joint algorithm and hardware design of CNNs is needed.Comment: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessibl",experiments
87,10.1016/j.jbi.2019.103138,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85062392033,health,01/04/2019,not defined,not defined,not defined,decentralised,distributed learning from multiple ehr databases: contextual embedding models for medical events,"Electronic health record (EHR) data provide promising opportunities to explore personalized treatment regimes and to make clinical predictions. Compared with regular clinical data, EHR data are known for their irregularity and complexity. In addition, analyzing EHR data involves privacy issues and sharing such data is often infeasible among multiple research sites due to regulatory and other hurdles. A recently published work uses contextual embedding models and successfully builds one predictive model for more than seventy common diagnoses. Despite of the high predictive power, the model cannot be generalized to other institutions without sharing data. In this work, a novel method is proposed to learn from multiple databases and build predictive models based on Distributed Noise Contrastive Estimation (Distributed NCE). We use differential privacy to safeguard the intermediary information sharing. The numerical study with a real dataset demonstrates that the proposed method not only can build predictive models in a distributed manner with privacy protection, but also preserve model structure well and achieve comparable prediction accuracy. The proposed methods have been implemented as a stand-alone Python library and the implementation is available on Github (https://github.com/ziyili20/DistributedLearningPredictor) with installation instructions and use-cases.",excluded
88,10.1109/isc2.2016.7580798,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7580798/,robotics,15/09/2016 00:00,not defined,not defined,not defined,not defined,smartseal: a ros based home automation framework for heterogeneous devices interconnection in smart buildings,"With this paper we present the SmartSEAL inter-connection system developed for the nationally founded SEAL project. SEAL is a research project aimed at developing Home Automation (HA) solutions for building energy management, user customization and improved safety of its inhabitants. One of the main problems of HA systems is the wide range of communication standards that commercial devices use. Usually this forces the designer to choose devices from a few brands, limiting the scope of the system and its capabilities. In this context, SmartSEAL is a framework that aims to integrate heterogeneous devices, such as sensors and actuators from different vendors, providing networking features, protocols and interfaces that are easy to implement and dynamically configurable. The core of our system is a Robotics middleware called Robot Operating System (ROS). We adapted the ROS features to the HA problem, designing the network and protocol architectures for this particular needs. These software infrastructure allows for complex HA functions that could be realized only levering the services provided by different devices. The system has been tested in our laboratory and installed in two real environments, Palazzo Fogazzaro in Schio and “Le Case” childhood school in Malo. Since one of the aim of the SEAL project is the personalization of the building environment according to the user needs, and the learning of their patterns of behaviour, in the final part of this work we also describe the ongoing design and experiments to provide a Machine Learning based re-identification module implemented with Convolutional Neural Networks (CNNs). The description of the adaptation module complements the description of the SmartSEAL system and helps in understanding how to develop complex HA services through it.",architecture
89,10.1016/j.epsr.2011.06.007,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/79960994059,health,01/11/2011,not defined,not defined,not defined,not defined,general asset management model in the context of an electric utility: application to power transformers,"GAMMEU
                        1
                     
                     
                        1
                        GAMMEU: general asset management model for an electric utility.
                      constitutes an integrated approach that covers the different elements related to the asset management of power transformers in the environment of a utility. GAMMEU harmonizes and inter-relates all the relevant subsystems of the asset management that normally are studied as individual entities and not as a system. Concretely, GAMMEU consists of a platform for data integration, an intelligent system for detection and diagnosis of failures, a failure rate estimation model, a module of reliability analysis and an optimisation model for maintenance scheduling. In this work, a brief description of the elements of GAMMEU is presented and the implementation of the intelligent system for detection and diagnosis as well as the failure rate estimation model is exemplified using data of measurements performed in real power transformers. A robust anomaly detection module using prediction models based on artificial intelligence techniques was developed for top oil temperature monitoring and the use of decision trees as classifiers for the assessment of FRA
                        2
                     
                     
                        2
                        Frequency response analysis.
                      measurements is also illustrated. For failure rate estimation, the use of a model based on hidden Markov chains is presented using data of dissolved gas analysis tests. The experience obtained from the implementation of part of the modules of GAMMEU using real data has demonstrated its feasibility.",architecture
90,10.1109/icws.2017.76,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8029817/,smart cities,30/06/2017 00:00,not defined,not defined,not defined,not defined,early air pollution forecasting as a service: an ensemble learning approach,"Air quality has become a major global concern for human beings involving all social stratums, for both developing and developed countries. Web service of precise and early air pollution forecasting is of great importance as it allows people to pro-actively take preventative and protective measurements. As an endeavor on the course of machine learning based air quality forecasting, this paper presents an initiative and its technological details in solving this challenging problem. Specifically, this work involves three major highlights regarding with both algorithmic innovation and deployment with its impact: 1) We propose a multi-channel ensemble learning framework, 2) We propose a new supervised feature learning and extraction method, i.e. sufficient statistics feature mapping based on Deep Boltzman Machine, which serves as a building block for our learning system, 3) We target our air pollution prediction method to the city of Beijing, China as it is at the forefront for battling against air pollution, which is embodied as a web service for prediction. Extensive experiments of real time air pollution forecasting on the real-world data demonstrates the effectiveness of the proposed method and value of the deployed web service system.",excluded
91,10.1016/b978-0-444-59520-1.50104-4,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/84862870640,science,01/01/2012,not defined,not defined,not defined,not defined,intelligent automation platform for bioprocess development,"High throughput technology has been increasingly adapted for drug screen and bioprocess development, due to the small amount of processing materials and reagents required and parallel experiment execution. It allows a wide design space to be explored in order to discover novel bioprocess solutions. Currently, the high throughput experiments for bioprocess development are implemented in a sequential fashion in which liquid handling system will perform the web lab experiment to prepare the samples; standalone analysis devices detect the data such as protein concentration; and specific software is used to realise the data analysis for process design or further experimentation.
                  The aim of this paper is to show how the efficiency of the high throughput bioprocess development approach can be enhanced by creating an intelligent automation platform that systematically drives liquid handling system, analysis devices and data analysis to perform a closed-loop learning. The first generation prototype has been established which consists of three parts: automated devices, design algorithms and database. In order to prove the concept of prototype, both simulation and real experiments studies have been established. In this case study, the platform is used to investigate the solubility of lysozyme at various ion strengths and pH values. Tecan liquid handling system for experimentation as well as buffer preparation and a plate reader for uv absorption measurement to determine protein concentration were used as the automated devices. The simplex search algorithm and artificial neural network modelling were utilised as design algorithm to iteratively select the experiments to execute and determine the optimal design solution. An entity-relationship database with Tecan system configuration information and experimental data was established. The results demonstrate this integrated approach can implement experiments and data analysis automatically to provide specific bioprocess design solutions in a closed loop strategy at first time. It is a promising approach that may significant increase the level of lab automation to release the engineer from the labour intensive R&D activities and provides the base for sophisticated artificial intelligent learning in the future.",architecture
92,10.1109/qrs51102.2020.00018,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9282796/,health,14/12/2020 00:00,not defined,not defined,not defined,not defined,phm technology for memory anomalies in cloud computing for iaas,"The IaaS (Infrastructure as a Service) is one of the most popular services from todays cloud service providers, where the virtual machines (VM) are rented by users who can deploy any program they want in the VMs to make their own websites or use as their remote desktops. However, this poses a major challenge for cloud IaaS providers who cannot control the software programs that users develop, install or download on their rented VMs. Those programs may not be well developed with various bugs or even downloaded/installed together with virus, which often make damages to the VMs or infect the cloud platform. To keep the health of a cloud IaaS platform, it is very important to implement the PHM (Prognostics and Health Management) technology for detecting those software problems and self-healing them in an intelligent and timely way. This paper realized a novel PHM technology inspired by biological autonomic nervous system to deal with the memory anomalies of those programs running on the cloud IaaS platform. We first present an innovative autonomic computing technology called Bionic Autonomic Nervous System (BANS) to endow the cloud system with distinctive capabilities of perception, detection, reflection, and learning. Then, we propose a BANS-based Prognostics and Health Management (BPHM) technology to enable the cloud system self-dealing with various memory anomalies. AI-based failure prognostics, immediate self-healing, self-learning ability and self-improvement functions are implemented. Experimental results illustrate that the designed BPHM can automatically and intelligently deal with complex memory anomalies in a real cloud system for IaaS, to keep the system much more reliable and healthier.",architecture
93,https://riunet.upv.es/bitstream/10251/169535/1/sarabia-jacomeusachpalau%20-%20highly-efficient%20fog-based%20deep%20learning%20aal%20fall%20detection%20system.pdf,Highly-efficient fog-based deep learning AAL fall detection system,core,,health,01/09/2020 00:00,not defined,not defined,not defined,hybrid,10.1016/j.iot.2020.100185,"[EN] Falls is one of most concerning accidents in aged population due to its high frequency and serious repercussion; thus, quick assistance is critical to avoid serious health consequences. There are several Ambient Assisted Living (AAL) solutions that rely on the technologies of the Internet of Things (IoT), Cloud Computing and Machine Learning (ML). Recently, Deep Learning (DL) have been included for its high potential to improve accuracy on fall detection. Also, the use of fog devices for the ML inference (detecting falls) spares cloud drawback of high network latency, non-appropriate for delay-sensitive applications such as fall detectors. Though, current fall detection systems lack DL inference on the fog, and there is no evidence of it in real environments, nor documentation regarding the complex challenge of the deployment. Since DL requires considerable resources and fog nodes are resource-limited, a very efficient deployment and resource usage is critical. We present an innovative highly-efficient intelligent system based on a fog-cloud computing architecture to timely detect falls using DL technics deployed on resource-constrained devices (fog nodes). We employ a wearable tri-axial accelerometer to collect patient monitoring data. In the fog, we propose a smart-IoT-Gateway architecture to support the remote deployment and management of DL models. We deploy two DL models (LSTM/GRU) employing virtualization to optimize resources and evaluate their performance and inference time. The results prove the effectiveness of our fall system, that provides a more timely and accurate response than traditional fall detector systems, higher efficiency, 98.75% accuracy, lower delay, and service improvement.This research was supported by the Ecuadorian Government through the Secretary of Higher Education, Science, Technology, and Innovation (SENESCYT) and has received funding from the European Union's Horizon 2020 research and innovation program as part of the ACTIVAGE project under Grant 732679.Sarabia-Jácome, D.; Usach, R.; Palau Salvador, CE.; Esteve Domingo, M. (2020). Highly-efficient fog-based deep learning AAL fall detection system. Internet of Things. 11:1-19. https://doi.org/10.1016/j.iot.2020.100185S1191",architecture
94,10.1109/tcyb.2013.2275291,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/6583249/,robotics,01/10/2013 00:00,not defined,not defined,not defined,not defined,real-time multiple human perception with color-depth cameras on a mobile robot,"The ability to perceive humans is an essential requirement for safe and efficient human-robot interaction. In real-world applications, the need for a robot to interact in real time with multiple humans in a dynamic, 3-D environment presents a significant challenge. The recent availability of commercial color-depth cameras allow for the creation of a system that makes use of the depth dimension, thus enabling a robot to observe its environment and perceive in the 3-D space. Here we present a system for 3-D multiple human perception in real time from a moving robot equipped with a color-depth camera and a consumer-grade computer. Our approach reduces computation time to achieve real-time performance through a unique combination of new ideas and established techniques. We remove the ground and ceiling planes from the 3-D point cloud input to separate candidate point clusters. We introduce the novel information concept, depth of interest, which we use to identify candidates for detection, and that avoids the computationally expensive scanning-window methods of other approaches. We utilize a cascade of detectors to distinguish humans from objects, in which we make intelligent reuse of intermediary features in successive detectors to improve computation. Because of the high computational cost of some methods, we represent our candidate tracking algorithm with a decision directed acyclic graph, which allows us to use the most computationally intense techniques only where necessary. We detail the successful implementation of our novel approach on a mobile robot and examine its performance in scenarios with real-world challenges, including occlusion, robot motion, nonupright humans, humans leaving and reentering the field of view (i.e., the reidentification challenge), human-object and human-human interaction. We conclude with the observation that the incorporation of the depth information, together with the use of modern techniques in new ways, we are able to create an accurate system for real-time 3-D perception of humans by a mobile robot.",experiments
95,10.1109/humanoids.2014.7041373,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7041373/,multimedia,20/11/2014 00:00,not defined,not defined,not defined,not defined,footstep planning on uneven terrain with mixed-integer convex optimization,"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1].",excluded
96,10.1016/j.compag.2018.09.037,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85054181612,smart cities,01/11/2018,not defined,not defined,not defined,not defined,a decision support tool to enhance agricultural growth in the mékrou river basin (west africa),"We describe in this paper the implementation of E-Water, an open software Decision Support System (DSS), designed to help local managers assess the Water Energy Food Environment (WEFE) nexus. E-Water aims at providing optimal management solutions to enhance food crop production at river basin level. The DSS was applied in the transboundary Mékrou river basin, shared among Benin, Burkina Faso and Niger. The primary sector for local economy in the region is agriculture, contributing significantly to income generation and job creation. Fostering the productivity of regional agricultural requires the intensification of farming practices, promoting additional inputs (mainly nutrient fertilizers and water irrigation) but, also, a more efficient allocation of cropland.
                  In order to cope with the heterogeneity of data, and the analyses and issues required by the WEFE nexus approach, our DSS integrates the following modules: (1) the EPIC biophysical agricultural model; (2) a simplified regression metamodel, linking crop production with external inputs; (3) a linear programming and a multiobjective genetic algorithm optimization routines for finding efficient agricultural strategies; and (4) a user-friendly interface for input/output analysis and visualization.
                  To test the main features of the DSS, we apply it to various real and hypothetical scenarios in the Mékrou river basin. The results obtained show how food unavailability due to insufficient local production could be reduced by, approximately, one third by enhancing the application and optimal distribution of fertilizers and irrigation. That would also affect the total income of the farming sector, eventually doubling it in the best case scenario. Furthermore, the combination of optimal agricultural strategies and modified optimal cropland allocation across the basin would bring additional moderate increases in food self-sufficiency, and more substantial gains in the total agricultural income.
                  The proposed software framework proves to be effective, enabling decision makers to identify efficient and site-specific agronomic management strategies for nutrients and water. Such practices would augment crop productivity, which, in turn, would allow to cope with increasing future food demands, and find a balanced use of natural resources, also taking other economic sectors—like livestock, urban or energy—into account.",architecture
97,10.1109/ijcnn.2008.4633875,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/4633875/,robotics,08/06/2008 00:00,not defined,not defined,not defined,not defined,bio-inspired stochastic chance-constrained multi-robot task allocation using wsn,"The multi-robot task allocation (MRTA) especially in unknown complex environment is one of the fundamental problems, a mostly important object in research of multi-robot. The MRTA problem is initially formulated as a chance-constrained optimization problem. Monte Carlo simulation is used to verify the accuracy of the solution provided by the algorithm. Ant colony optimization (ACO) algorithm based on bionic swarm intelligence was used. A hybrid intelligent algorithm combined Monte Carlo simulation and neural network is used for solving stochastic chance constrained models of MRTA. A practical implementation with real WSN and real mobile robots were carried out. In environment the successful implementation of tasks without collision validates the efficiency, stability and accuracy of the proposed algorithm. The convergence curve shows that as iterative generation grows, the utility increases and finally reaches a stable and optimal value. Results show that using sensor information fusion can greatly improve the efficiency. The algorithm is proved better than tradition algorithms without WSN for MRTA in real time.",experiments
98,10.1109/83.791960,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/791960/,industry,01/10/1999 00:00,not defined,not defined,not defined,not defined,real-time dsp implementation for mrf-based video motion detection,"This paper describes the real time implementation of a simple and robust motion detection algorithm based on Markov random field (MRF) modeling, MRF-based algorithms often require a significant amount of computations. The intrinsic parallel property of MRF modeling has led most of implementations toward parallel machines and neural networks, but none of these approaches offers an efficient solution for real-world (i.e., industrial) applications. Here, an alternative implementation for the problem at hand is presented yielding a complete, efficient and autonomous real-time system for motion detection. This system is based on a hybrid architecture, associating pipeline modules with one asynchronous module to perform the whole process, from video acquisition to moving object masks visualization. A board prototype is presented and a processing rate of 15 images/s is achieved, showing the validity of the approach.",excluded
99,,'Institute of Electrical and Electronics Engineers (IEEE)',core,10.1109/tro.2021.3084374,multimedia,01/01/2021 00:00,not defined,not defined,not defined,not defined,cat-like jumping and landing of legged robots in low gravity using deep reinforcement learning,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.ISSN:1552-3098ISSN:1042-296XISSN:1941-046",experiments
100,10.1145/3460418.3480415,ACM,project-academic,project-academic,industry,21/09/2021 00:00,not defined,not defined,not defined,not defined,piwims physics informed warehouse inventory monitory via synthetic data generation," State-of-the-art camera-based deep learning methods for inventory monitoring tend to fail to generalize across different domains due to the high variance of scene settings. Large amounts of human labor are required to label and parameterize the models, making a real-world deployment impractical. In a third-party warehouse setting, supervised learning approaches are either too costly and/or inaccurate to deploy due to the need for human labor to address the diverse set of environmental factors (i.e, lighting conditions, product motion, deployment limitations). None We introduce, a realistic synthetic dataset generation technique that combines the physical constraints of the scene in real-world deployments, drastically reducing the need for human labeling. In contrast to other generative techniques, where the generative parameters are learned from a large sample of available data, this compositive approach defines the parameters based on physical characteristics of the particular task, which requires minimal human annotation. We demonstrate performance in a 4-month real operating warehouse deployment and show that with only 32 manually labeled images per object, can achieve an accuracy of up to 87% in inventory tracking, which is a 28% increase when compared to traditional data augmentation techniques and 31% error reduction when compared to the third-party warehouse industry average. Furthermore, we demonstrate the ability of to generalize across different camera angles and positions by achieving an accuracy of 85% in inventory tracking while varying the position and angle of the camera.",architecture
101,10.1016/j.eswa.2017.03.002,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85015894497,robotics,01/09/2017,not defined,not defined,not defined,not defined,incremental q-learning strategy for adaptive pid control of mobile robots,"Expert and intelligent systems are being developed to control many technological systems including mobile robots. However, the PID (Proportional-Integral-Derivative) controller is a fast low-level control strategy widely used in many control engineering tasks. Classic control theory has contributed with different tuning methods to obtain the gains of PID controllers for specific operation conditions. Nevertheless, when the system is not fully known and the operative conditions are variable and not previously known, classical techniques are not entirely suitable for the PID tuning. To overcome these drawbacks many adaptive approaches have been arisen, mainly from the field of artificial intelligent. In this work, we propose an incremental Q-learning strategy for adaptive PID control. In order to improve the learning efficiency we define a temporal memory into the learning process. While the memory remains invariant, a non-uniform specialization process is carried out generating new limited subspaces of learning. An implementation on a real mobile robot demonstrates the applicability of the proposed approach for a real-time simultaneous tuning of multiples adaptive PID controllers for a real system operating under variable conditions in a real environment.",experiments
102,10.1109/icebe.2011.28,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/6104615/,multimedia,21/10/2011 00:00,not defined,not defined,not defined,not defined,a rfid-based intelligent warehouse management system design and implementation,"A RFID-based intelligent warehouse management system (RFID-IWMS) is proposed in this paper. The RFID-IWMS helps to achieve better inventory control, as well as to improve operation efficiency. To this purpose, it automates the manual warehouse operation and provides tight integration with current warehouse management system (WMS). In this system, RFID tags are embedded in the pallets and shelves. In addition, forklifts are equipped with intelligent terminal as well as RFID reader and antenna to support automatic data scanning and storage location checking. Moreover, a middle layer software component is design to facilitate the communication between WMS, portable terminals and forklift terminals through wireless LAN. Besides, it also supports additional powerful functions such as forklift scheduling, picking sequence management and 3D shelves monitoring. The design of the system makes full use of the existing equipments and facilities and has the feature of low cost and quick inaction. Through real working practice in the distribution center of Bailian Group, the system is proved to be feasible in the aspects of both technology and cost.",excluded
103,10.1109/tencon.2019.8929612,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8929612/,health,20/10/2019 00:00,not defined,not defined,not defined,not defined,lung nodule detection from low dose ct scan using optimization on intel xeon and core processors with intel distribution of openvino toolkit,"With the advancement of AI in the field of medical imaging, medical diagnosis is getting faster and viable for medical practitioners especially for cancer diagnosis. Earlier Deep Learning solutions had to be deployed on High Performance Computing devices like GPU for achieving real time performance. But with Optimization on Intel Core and Xeon processors with Intel Distribution of OpenVINO Toolkit (Open Visual Inference and Neural Network Optimization), it is possible to deploy Deep Learning models with accelerated performance, than running Tensorflow / Caffe models on CPU machines. In this paper we describe the proposed work wherein we ported our DetectNet Deep Learning Model with NVIDIA specific custom layer for lung nodule detection trained on LIDC dataset, using Intel Distribution of OpenVINO, and deployed the same in Intel Core/Xeon processors with accelerated performance.",excluded
104,10.1109/robot.2004.1308781,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/1308781/,robotics,01/05/2004 00:00,not defined,not defined,not defined,not defined,software approach for the autonomous inspection robot makro,"The sewer inspection robot MAKRO is an autonomous multi-segment robot with worm-like shape driven by wheels. It is currently under development in the project MAKRO-PLUS. The robot has to navigate autonomously within sewer systems. Its first tasks is to take water probes, analyze them onboard, and measure positions of manholes and pipes to detect pollution loaded sewage and to improve current maps of sewer systems. One of the challenging problems is the control software, which should enable the robot to navigate in the sewer system and perform the inspection tasks autonomously, while always taking care of its own safety. Tests in our test environment and in a real sewer system show promising results. This paper focuses on the software approach. To manage the complexity a layered architecture has been chosen, each layer defining a different level of abstraction. After determining the abstraction levels, we use different methods for implementation. For the highest abstraction level a standard AI-planning algorithm is used. For the next level, finite state automata has been chosen. For ""simple"" task implementation we use a modular C++ based method (MCA2), which is also used on the lowest software level.",architecture
105,10.1109/icmla.2015.209,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7424444/,smart cities,11/12/2015 00:00,not defined,not defined,not defined,not defined,intelligent bus stop identification using smartphone sensors,"Intelligent transportation systems can be built by developing models that learn from the collected transport data. Data collection and implementation of such systems is often costly, and few countries have support for such systems in their transportation budgets. In places where maintaining currency and accuracy of information is difficult, many problems arise. For instance, in Chennai, India, real time bus transit data is not maintained, there is no proper communication about the bus schedules, bus stops are not regularly updated and inconsistent information about bus stops is observed in the transport authority's website. We are interested in developing models for identifying bus stops from trajectories for situations where accurate and current information is not available and traffic conditions are challenging, such as Chennai, India. We develop a simple yet easily accessible Android mobile application (App) to collect GPS traces of bus routes. We use our App to collect GPS trajectory data from Baltimore, Maryland, a place where there are facilities to access up-to-date information about bus stops. We also collect GPS trajectories from Chennai, India. We then develop a model using machine learning techniques to identify bus stops from the collected trajectories. We experimentally evaluate our model by training it on the Baltimore dataset and testing it on the Chennai dataset, achieving testing accuracy between 85 -- 90%. This is comparable to the accuracy of 95% achieved by both training and testing on the Chennai dataset. This illustrates that our approach is effective in helping maintain an accurate and current transport information system for resource constraint environments.",excluded
106,10.1109/isaect50560.2020.9523700,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9523700/,autonomous vehicle,27/11/2020 00:00,supervised,batch,not defined,hybrid,edge-cloud architectures using uavs dedicated to industrial iot monitoring and control applications,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",architecture
107,10.1109/iria53009.2021.9588707,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9588707/,multimedia,22/09/2021 00:00,not defined,not defined,not defined,not defined,automatic license plate recognition system using ssd,"Automatic License Plate Recognition (ALPR) is a very widely used system in applications such as parking management, theft detection, traffic control and management etc. Most of the existing ALPR systems fail to showcase acceptable performance on real time images/video scenes. This work proposes and demonstrates implementation of a deep learning-based approach to locate license plates of four wheeler vehicles thereby enabling optical character recognition (OCR) to recognize the characters and numbers on the located plates in real time. The proposed system is decomposed into three sub-blocks viz. Vehicle image/video acquisition, License plate localization and OCR. A simple setup using a reasonable resolution webcam has been designed to capture images/videos of vehicles at some entry point. We propose to utilize Single Shot Detector (SSD) based Mobilenet V1 architecture to localize the license plates. The hyper parameters of this architecture are selected with rigorous experimentation so as to avoid over-fitting. We have compared performance of two OCRs viz. Tesseract OCR, Easy OCR and found the superiority of Easy OCR since it utilizes deep learning approach for character recognition. NVIDIA Jetson Nano and Raspberry Pi 3B hardware platforms have been used to implement the entire system. The parameters of these three sub-blocks have been optimized to yield real time performance of ALPR with acceptable accuracy. The proposed and implemented system on Jetson Nano allows processing of videos for ALPR having accuracy more than 95%.",experiments
108,http://arxiv.org/abs/1911.01562v1,arxiv,arxiv,http://arxiv.org/abs/1911.01562v1,multimedia,05/11/2019 00:00,supervised,not defined,not defined,not defined,"deepracer: educational autonomous racing platform for experimentation
  with sim2real reinforcement learning","DeepRacer is a platform for end-to-end experimentation with RL and can be
used to systematically investigate the key challenges in developing intelligent
control systems. Using the platform, we demonstrate how a 1/18th scale car can
learn to drive autonomously using RL with a monocular camera. It is trained in
simulation with no additional tuning in physical world and demonstrates: 1)
formulation and solution of a robust reinforcement learning algorithm, 2)
narrowing the reality gap through joint perception and dynamics, 3) distributed
on-demand compute architecture for training optimal policies, and 4) a robust
evaluation method to identify when to stop training. It is the first successful
large-scale deployment of deep reinforcement learning on a robotic control
agent that uses only raw camera images as observations and a model-free
learning method to perform robust path planning. We open source our code and
video demo on GitHub: https://git.io/fjxoJ.",experiments
109,10.1109/icecce49384.2020.9179349,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9179349/,smart cities,13/06/2020 00:00,supervised,batch,classification,centralised,a cloud based smart recycling bin for in-house waste classification,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. Most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification for personal in-house usage. A centralized Information System (IS) collects measurements from smart bins that can be deployed virtually anywhere and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low compared to other implementations.",experiments
110,http://arxiv.org/abs/1904.02579v2,arxiv,arxiv,http://arxiv.org/abs/1904.02579v2,multimedia,04/04/2019 00:00,supervised,batch,not defined,not defined,"can a robot become a movie director? learning artistic principles for
  aerial cinematography","Aerial filming is constantly gaining importance due to the recent advances in
drone technology. It invites many intriguing, unsolved problems at the
intersection of aesthetical and scientific challenges. In this work, we propose
a deep reinforcement learning agent which supervises motion planning of a
filming drone by making desirable shot mode selections based on aesthetical
values of video shots. Unlike most of the current state-of-the-art approaches
that require explicit guidance by a human expert, our drone learns how to make
favorable viewpoint selections by experience. We propose a learning scheme that
exploits aesthetical features of retrospective shots in order to extract a
desirable policy for better prospective shots. We train our agent in realistic
AirSim simulations using both a hand-crafted reward function as well as reward
from direct human input. We then deploy the same agent on a real DJI M210 drone
in order to test the generalization capability of our approach to real world
conditions. To evaluate the success of our approach in the end, we conduct a
comprehensive user study in which participants rate the shot quality of our
methods. Videos of the system in action can be seen at
https://youtu.be/qmVw6mfyEmw.",experiments
111,10.1016/j.engappai.2018.03.016,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85045475454,smart cities,01/06/2018,not defined,not defined,not defined,not defined,proa: an intelligent multi-criteria personalized route assistant,"Personalization of pedestrian routes becomes a necessity due to the wide variety of user profiles that may differ on preferences or requirements to choose a route. Several software applications offer routes usually based on single criterion like distance or time; however, these criteria do not often fit the pedestrian needs.
                  Here, we will first focus on the Personalized Routes Problem and then we will approach the specific case of designing accessible and green pedestrian routes.
                  The proposal is implemented as a freely available Android application (named as PRoA, by intelligent multi-criteria Personalized Route Assistant), which automatically obtains geographical data and information for the decision criteria from open datasets.
                  The proposal is evaluated using real cases at the city of Granada, Spain.",architecture
112,,,core,,robotics,01/01/2010 00:00,not defined,not defined,not defined,not defined,d.: multilevel darwinist brain (mdb): artificial evolution in a cognitive architecture for real robots,"Abstract—The multilevel Darwinist brain (MDB) is a cognitive architecture that follows an evolutionary approach to provide au-tonomous robots with lifelong adaptation. It has been tested in real robot on-line learning scenarios obtaining successful results that reinforce the evolutionary principles that constitute the main orig-inal contribution of the MDB. This preliminary work has lead to a series of improvements in the computational implementation of the architecture so as to achieve realistic operation in real time, which was the biggest problem of the approach due to the high compu-tational cost induced by the evolutionary algorithms that make up the MDB core. The current implementation of the architecture is able to provide an autonomous robot with real time learning ca-pabilities and the capability for continuously adapting to changing circumstances in its world, both internal and external, with min-imal intervention of the designer. This paper aims at providing an overview or the architecture and its operation and defining what is required in the path towards a real cognitive robot following a developmental strategy. The design, implementation and basic op-eration of the MDB cognitive architecture are presented through some successful real robot learning examples to illustrate the va-lidity of this evolutionary approach. Index Terms—Adaptive systems, artificial neural networks, au-tonomous robotics, cognitive architecture, developmental robotics, evolutionary computation. I",architecture
113,10.1109/access.2020.3022039,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9187238/,health,01/01/2020 00:00,supervised,not defined,not defined,not defined,a lightweight convolutional neural network for real and apparent age estimation in unconstrained face images,"Real and apparent age estimation of human face has attracted increased attention due to its numerous real-world applications. Different intelligent application scenarios can benefit from these computer-based systems that predict the ages of people correctly. Automatic apparent age system is particularly useful in medical diagnosis, facial beauty product development, movie role casting, the effect of plastic surgery, and anti-aging treatment. Predicting the real and apparent age of people has been quite difficult for both machines and humans. More recently, Deep learning with Convolutional Neural Networks (CNNs) methods have been extensively used for these classification task. It has incomparable advantages in extracting discriminative image features from human faces. However, many of the existing CNN-based methods are designed to be deeper and larger with more complex layers that makes it challenging to deploy on mobile devices with resource-constrained features. Therefore, we design a lightweight CNN model of fewer layers to estimate the real and apparent age of individuals from unconstrained real-time face images that can be deployed on mobile devices. The experimental results, when analyzed for classification accuracy on FG-NET, MORPH-II and APPA-REAL, with large-scale face images containing both real and apparent age annotations, show that our model obtains a state-of-the-art performance in both real and apparent age classification when compared to state-of-the-art methods. The new results and model size, therefore, confirm the usefulness of the model on resource-constrained mobile devices.",excluded
114,http://arxiv.org/abs/2008.05255v1,arxiv,arxiv,http://arxiv.org/abs/2008.05255v1,smart cities,12/08/2020 00:00,not defined,not defined,not defined,hybrid,"identity-aware attribute recognition via real-time distributed inference
  in mobile edge clouds","With the development of deep learning technologies, attribute recognition and
person re-identification (re-ID) have attracted extensive attention and
achieved continuous improvement via executing computing-intensive deep neural
networks in cloud datacenters. However, the datacenter deployment cannot meet
the real-time requirement of attribute recognition and person re-ID, due to the
prohibitive delay of backhaul networks and large data transmissions from
cameras to datacenters. A feasible solution thus is to employ mobile edge
clouds (MEC) within the proximity of cameras and enable distributed inference.
In this paper, we design novel models for pedestrian attribute recognition with
re-ID in an MEC-enabled camera monitoring system. We also investigate the
problem of distributed inference in the MEC-enabled camera network. To this
end, we first propose a novel inference framework with a set of distributed
modules, by jointly considering the attribute recognition and person re-ID. We
then devise a learning-based algorithm for the distributions of the modules of
the proposed distributed inference framework, considering the dynamic
MEC-enabled camera network with uncertainties. We finally evaluate the
performance of the proposed algorithm by both simulations with real datasets
and system implementation in a real testbed. Evaluation results show that the
performance of the proposed algorithm with distributed inference framework is
promising, by reaching the accuracies of attribute recognition and person
identification up to 92.9% and 96.6% respectively, and significantly reducing
the inference delay by at least 40.6% compared with existing methods.",architecture
115,10.1016/j.knosys.2016.07.022,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/84979704090,industry,15/10/2016,not defined,not defined,not defined,not defined,software test quality rating: a paradigm shift in swarm computing for software certification,"Recently, software quality issues have emerged to be recognized as a fundamental point as we actualize an extensive growth of organizations involved in software industries. Still, these organizations cannot ensure the quality of their products; therefore abandoning customers in uncertainties. Software certification is the branch of quality by means that quality requires to be measured prior to certification admitting process. However, creating an official certification model is difficult due to the deficiency of data in the domain of software engineering. This research participates in solving the problem of assessing software quality by introducing a model that handles a fuzzy inference engine to mix both of the processes–driven and application-driven quality assurance procedures. The fundamental purpose of the suggested model is to enhance the compactness and the interpretability of the system's fuzzy rules via engaging an ant colony optimization algorithm (ACO), which attempts to discover a good rule description by a set of compound rules initially represented with traditional single rules. The proposed model is a fitting one that can be seen as practicing certification models that have already been created from software quality domain data and modifying them to a context-specific data. The model has been tested by a case study and the results have confirmed feasibility and practicality of the model in a real environment.",excluded
116,10.1016/j.adhoc.2019.102047,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85076174369,industry,01/03/2020,not defined,not defined,not defined,not defined,an intelligent edge-iot platform for monitoring livestock and crops in a dairy farming scenario,"Today’s globalized and highly competitive world market has broadened the spectrum of requirements in all the sectors of the agri-food industry. This paper focuses on the dairy industry, on its need to adapt to the current market by becoming more resource efficient, environment-friendly, transparent and secure. The Internet of Things (IoT), Edge Computing (EC) and Distributed Ledger Technologies (DLT) are all crucial to the achievement of those improvements because they allow to digitize all parts of the value chain, providing detailed information to the consumer on the final product and ensuring its safety and quality. In Smart Farming environments, IoT and DLT enable resource monitoring and traceability in the value chain, allowing producers to optimize processes, provide the origin of the produce and guarantee its quality to consumers. In comparison to a centralized cloud, EC manages the Big Data generated by IoT devices by processing them at the network edge, allowing for the implementation of services with shorter response times, and a higher Quality of Service (QoS) and security. This work presents a platform oriented to the application of IoT, Edge Computing, Artificial Intelligence and Blockchain techniques in Smart Farming environments, by means of the novel Global Edge Computing Architecture, and designed to monitor the state of dairy cattle and feed grain in real time, as well as ensure the traceability and sustainability of the different processes involved in production. The platform is deployed and tested in a real scenario on a dairy farm, demonstrating that the implementation of EC contributes to a reduction in data traffic and an improvement in the reliability in communications between the IoT-Edge layers and the Cloud.",architecture
117,10.1109/icfec51620.2021.00018,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9458893/,multimedia,13/05/2021 00:00,not defined,not defined,not defined,not defined,a privacy preserving system for ai-assisted video analytics,"The emerging Edge computing paradigm facilitates the deployment of distributed AI-applications and hardware, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits for parties in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, the widespread deployment of such mechanism in public areas are a growing cause of privacy and security concerns. Data protection strategies need to be appropriately designed and correctly implemented in order to mitigate the associated risks. Most existing approaches focus on privacy and security related operations of the video stream itself or protecting its transmission. In this paper, we propose a privacy preserving system for AI-assisted video analytics, that extracts relevant information from video data and governs the secure access to that information. The system ensures that applications leveraging extracted data have no access to the video stream. An attribute-based authorization scheme allows applications to only query a predefined subset of extracted data. We demonstrate the feasibility of our approach by evaluating an application motivated by the recent COVID-19 pandemic, deployed on typical edge computing infrastructure.",architecture
118,http://arxiv.org/abs/2001.09938v1,arxiv,arxiv,http://arxiv.org/abs/2001.09938v1,science,22/10/2019 00:00,not defined,not defined,not defined,not defined,"autonomous discovery of battery electrolytes with robotic
  experimentation and machine-learning","Innovations in batteries take years to formulate and commercialize, requiring
extensive experimentation during the design and optimization phases. We
approached the design and selection of a battery electrolyte through a
black-box optimization algorithm directly integrated into a robotic test-stand.
We report here the discovery of a novel battery electrolyte by this experiment
completely guided by the machine-learning software without human intervention.
Motivated by the recent trend toward super-concentrated aqueous electrolytes
for high-performance batteries, we utilize Dragonfly - a Bayesian
machine-learning software package - to search mixtures of commonly used lithium
and sodium salts for super-concentrated aqueous electrolytes with wide
electrochemical stability windows. Dragonfly autonomously managed the robotic
test-stand, recommending electrolyte designs to test and receiving experimental
feedback in real time. In 40 hours of continuous experimentation over a
four-dimensional design space with millions of potential candidates, Dragonfly
discovered a novel, mixed-anion aqueous sodium electrolyte with a wider
electrochemical stability window than state-of-the-art sodium electrolyte. A
human-guided design process may have missed this optimal electrolyte. This
result demonstrates the possibility of integrating robotics with
machine-learning to rapidly and autonomously discover novel battery materials.",excluded
119,,"Unmasking Communication Partners: A Low-Cost AI Solution for Digitally
  Removing Head-Mounted Displays in VR-Based Telepresence",core,,multimedia,06/11/2020 00:00,not defined,not defined,not defined,not defined,http://arxiv.org/abs/2011.03630,"Face-to-face conversation in Virtual Reality (VR) is a challenge when
participants wear head-mounted displays (HMD). A significant portion of a
participant's face is hidden and facial expressions are difficult to perceive.
Past research has shown that high-fidelity face reconstruction with personal
avatars in VR is possible under laboratory conditions with high-cost hardware.
In this paper, we propose one of the first low-cost systems for this task which
uses only open source, free software and affordable hardware. Our approach is
to track the user's face underneath the HMD utilizing a Convolutional Neural
Network (CNN) and generate corresponding expressions with Generative
Adversarial Networks (GAN) for producing RGBD images of the person's face. We
use commodity hardware with low-cost extensions such as 3D-printed mounts and
miniature cameras. Our approach learns end-to-end without manual intervention,
runs in real time, and can be trained and executed on an ordinary gaming
computer. We report evaluation results showing that our low-cost system does
not achieve the same fidelity of research prototypes using high-end hardware
and closed source software, but it is capable of creating individual facial
avatars with person-specific characteristics in movements and expressions.Comment: 9 pages, IEEE 3rd International Conference on Artificial Intelligence
  & Virtual Realit",experiments
120,http://arxiv.org/abs/1808.01356v1,arxiv,arxiv,http://arxiv.org/abs/1808.01356v1,smart cities,31/07/2018 00:00,supervised,not defined,not defined,not defined,"deep learning-based multiple object visual tracking on embedded system
  for iot and mobile edge computing applications","Compute and memory demands of state-of-the-art deep learning methods are
still a shortcoming that must be addressed to make them useful at IoT
end-nodes. In particular, recent results depict a hopeful prospect for image
processing using Convolutional Neural Netwoks, CNNs, but the gap between
software and hardware implementations is already considerable for IoT and
mobile edge computing applications due to their high power consumption. This
proposal performs low-power and real time deep learning-based multiple object
visual tracking implemented on an NVIDIA Jetson TX2 development kit. It
includes a camera and wireless connection capability and it is battery powered
for mobile and outdoor applications. A collection of representative sequences
captured with the on-board camera, dETRUSC video dataset, is used to exemplify
the performance of the proposed algorithm and to facilitate benchmarking. The
results in terms of power consumption and frame rate demonstrate the
feasibility of deep learning algorithms on embedded platforms although more
effort to joint algorithm and hardware design of CNNs is needed.",experiments
121,10.1109/infocom41043.2020.9155467,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9155467/,multimedia,09/07/2020 00:00,rl,not defined,not defined,not defined,rldish: edge-assisted qoe optimization of http live streaming with reinforcement learning,"Recent years have seen a rapidly increasing traffic demand for HTTP-based high-quality live video streaming. The surging traffic demand, as well as the real-time property of live videos, make it challenging for content delivery networks (CDNs) to guarantee the Quality-of-Experiences (QoE) of viewers. The initial video segment (IVS) of live streaming plays an important role in the QoE of live viewers, particularly when users require fast join time and smooth view experience. State-of-the-art research on this regard estimates network throughput for each viewer and thus may incur a large overhead that offsets the benefit. To tackle the problem, we propose Rldish, a scheme deployed at the edge CDN server, to dynamically select a suitable IVS for new live viewers based on Reinforcement Learning (RL). Rldish is transparent to both the client and the streaming server. It collects the real-time QoE observations from the edge without any client-side assistance, then uses these QoE observations as real-time rewards in RL. We deploy Rldish as a virtualized network function (VNF) in a real HTTP cache server, and evaluate its performance using streaming servers distributed over the world. Our experiments show that Rldish improves the state- of-the-art IVS selection scheme w.r.t. the average QoE of live viewers by up to 22%.",excluded
122,http://arxiv.org/abs/2011.03630v1,arxiv,arxiv,http://arxiv.org/abs/2011.03630v1,multimedia,06/11/2020 00:00,not defined,not defined,not defined,not defined,"unmasking communication partners: a low-cost ai solution for digitally
  removing head-mounted displays in vr-based telepresence","Face-to-face conversation in Virtual Reality (VR) is a challenge when
participants wear head-mounted displays (HMD). A significant portion of a
participant's face is hidden and facial expressions are difficult to perceive.
Past research has shown that high-fidelity face reconstruction with personal
avatars in VR is possible under laboratory conditions with high-cost hardware.
In this paper, we propose one of the first low-cost systems for this task which
uses only open source, free software and affordable hardware. Our approach is
to track the user's face underneath the HMD utilizing a Convolutional Neural
Network (CNN) and generate corresponding expressions with Generative
Adversarial Networks (GAN) for producing RGBD images of the person's face. We
use commodity hardware with low-cost extensions such as 3D-printed mounts and
miniature cameras. Our approach learns end-to-end without manual intervention,
runs in real time, and can be trained and executed on an ordinary gaming
computer. We report evaluation results showing that our low-cost system does
not achieve the same fidelity of research prototypes using high-end hardware
and closed source software, but it is capable of creating individual facial
avatars with person-specific characteristics in movements and expressions.",experiments
123,http://arxiv.org/abs/1907.07210v1,arxiv,arxiv,http://arxiv.org/abs/1907.07210v1,smart cities,16/07/2019 00:00,not defined,not defined,not defined,not defined,real-time vision-based depth reconstruction with nvidia jetson,"Vision-based depth reconstruction is a challenging problem extensively
studied in computer vision but still lacking universal solution. Reconstructing
depth from single image is particularly valuable to mobile robotics as it can
be embedded to the modern vision-based simultaneous localization and mapping
(vSLAM) methods providing them with the metric information needed to construct
accurate maps in real scale. Typically, depth reconstruction is done nowadays
via fully-convolutional neural networks (FCNNs). In this work we experiment
with several FCNN architectures and introduce a few enhancements aimed at
increasing both the effectiveness and the efficiency of the inference. We
experimentally determine the solution that provides the best
performance/accuracy tradeoff and is able to run on NVidia Jetson with the
framerates exceeding 16FPS for 320 x 240 input. We also evaluate the suggested
models by conducting monocular vSLAM of unknown indoor environment on NVidia
Jetson TX2 in real-time. Open-source implementation of the models and the
inference node for Robot Operating System (ROS) are available at
https://github.com/CnnDepth/tx2_fcnn_node.",experiments
124,10.1109/isda.2014.7066281,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7066281/,industry,30/11/2014 00:00,not defined,not defined,not defined,not defined,ros-based remote controlled robotic arm workcell,"This paper describes robotic workplace that is being developed at our faculty. It consists of industrial arm Mitsubishi Melfa RV-6SL, gripper Schunk and six Axis cameras. The purpose of this workplace is to serve students for testing algorithms from artificial intelligence and computer vision. It also gives them the opportunity to work with real industrial manipulator and allow them to test things like kinematics and dynamics of the arm. The arm is inaccessible for students so it can be operated only remotely. The software needed for remote controlling and programming is described in this paper.",excluded
125,10.26083/tuprints-00017606,,core,,multimedia,01/01/2021 00:00,not defined,not defined,not defined,not defined,automation for camera-only 6d object detection,"Today a widespread deployment of Augmented Reality (AR) systems is only possible by means of computer vision frameworks like ARKit and ARCore, which abstract from specific devices, yet restrict the set of devices to the respective vendor.
This thesis therefore investigates how to allow deploying AR systems to any device with an attached camera.

One crucial part of an AR system is the detection of arbitrary objects in the camera frame and naturally accompanying the estimation of their 6D-pose.
This increases the degree of scene understanding that AR applications require for placing augmentations in the real world. Currently, this is limited by a coarse segmentation of the scene into planes as provided by the aforementioned frameworks.
Being able to reliably detect individual objects, allows attaching specific augmentations as required by e.g. AR maintenance applications.
For this, we employ convolutional neural networks (CNNs) to estimate the 6D-pose of all visible objects from a single RGB image.
Here, the addressed challenge is the automated training of the respective CNN models, given only the CAD geometry of the target object.
First, we look at reconstructing the missing surface data in real-time before we turn to the more general problem of bridging the domain gap between the non-photorealistic representation and the real world appearance.
To this end, we build upon generative adversarial network (GAN) models to formulate the domain gap as an unsupervised learning problem.
Our evaluation shows an improvement in model performance, while providing a simplified handling compared to alternative solutions.

Furthermore, the calibration data of the used camera must be known for precise pose estimation. This data, again, is only available for the restricted set of devices, that the proprietary frameworks support.
To lift this restriction, we propose a web-based camera calibration service that not only aggregates calibration data, but also guides users in the calibration of new cameras.
Here, we first present a novel calibration-pose selection framework that reduces the number of required calibration images by 30% compared to existing solutions, while ensuring a repeatable and reliable calibration outcome.
Then, we present an evaluation of different user-guidance strategies, which allows choosing a setting suitable for most users.
This enables even novice users to perform a precise camera calibration in about 2 minutes.
Finally, we propose an efficient client-server architecture to deploy the aforementioned guidance on the web, making it available to the widest possible range of devices.
This service is not restricted to AR systems, but allows the general deployment of computer vision algorithms on the web that rely on camera calibration data, which was previously not possible.

These elements combined, allow a semi-automatic deployment of AR systems with any camera to detect any object",excluded
126,10.1109/saci.2007.375494,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/4262496/,robotics,18/05/2007 00:00,not defined,not defined,not defined,not defined,fpga parallel implementation of cmac type neural network with on chip learning,"The hardware implementation of neural networks is a new step in the evolution and use of neural networks in practical applications. The CMAC cerebellar model articulation controller is intended especially for hardware implementation, and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA's has several benefits, with emphasis on parallelism and the real time capabilities. This paper discusses the hardware implementation of the CMAC type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor.",excluded
127,10.1109/compsac48688.2020.0-168,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9202848/,health,17/07/2020 00:00,supervised,batch,not defined,not defined,an early warning system for hemodialysis complications utilizing transfer learning from hd iot dataset,"According to the 2018 annual report of US Department of Kidney Data System (USRDS), Taiwan's dialysis rate and prevalence rate are the highest in the world due to population aging, diabetes and progresses in cardiovascular care. With the rise of artificial intelligence deep learning in recent years, various analytical software resources have gradually become easier to obtain. At the same time, wearable cyber physical sensors are becoming more and more popular. Measurements on vital signs such as heartbeat, electrocardiogram, and blood oxygenation blood pressure values are ubiquitous. We propose an integrated system that combines dialysis big data deep learning analysis with cross platform physiological sensing. We specifically tackle the early warning of dialysis discomfort such as hypotension, hypertension, cramps, etc., this requires a large amount of data collection, related training, data sources including dialysis treatment process and home physiological data. Although the Dialysis machine is able to produce huge amount of IoT data, the usable data for early warning system training is not as huge due to the limited physician labors devoted for labeling questionable samples. This generally leads to low accuracy for regular CNN training methods. We enhance the AI training performance via a transfer learning technique. The AI training accuracy reaches the value of 99% with the help of transfer learning, while that of an original CNN process on the HD data bears a low 60% accuracy. Given the high prediction accuracy of our AI engine, we are able to integrate the real time measurements from Dialysis machine with wearable devices such as ECG sensors and wrist health watches, and make precision prediction of incoming discomfort during the HD treatments. The ECG signal of the same group patients are also analyzed with the same technique. The same accuracy enhancement are also observed.",architecture
128,https://core.ac.uk/download/211997756.pdf,Crowdbreaks: Tracking Health Trends Using Public Social Media Data and Crowdsourcing,core,,health,26/04/2019 00:00,not defined,not defined,not defined,not defined,10.3389/fpubh.2019.00081,"In the past decade, tracking health trends using social media data has shown great promise, due to a powerful combination of massive adoption of social media around the world, and increasingly potent hardware and software that enables us to work with these new big data streams. At the same time, many challenging problems have been identified. First, there is often a mismatch between how rapidly online data can change, and how rapidly algorithms are updated, which means that there is limited reusability for algorithms trained on past data as their performance decreases over time. Second, much of the work is focusing on specific issues during a specific past period in time, even though public health institutions would need flexible tools to assess multiple evolving situations in real time. Third, most tools providing such capabilities are proprietary systems with little algorithmic or data transparency, and thus little buy-in from the global public health and research community. Here, we introduce Crowdbreaks, an open platform which allows tracking of health trends by making use of continuous crowdsourced labeling of public social media content. The system is built in a way which automatizes the typical workflow from data collection, filtering, labeling and training of machine learning classifiers and therefore can greatly accelerate the research process in the public health domain. This work describes the technical aspects of the platform, thereby covering the functionalities at its current state and exploring its future use cases and extensions",architecture
129,10.1109/ijcnn.2015.7280807,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7280807/,robotics,17/07/2015 00:00,not defined,not defined,not defined,not defined,applying the canonical distributed embodied evolution algorithm in a collective indoor navigation task,"The automatic design of control systems for multi-robot teams that operate in real time is not affordable with traditional evolutionary algorithms mainly due to the huge computational requirements they imply. Embodied Evolution (EE) is an evolutionary paradigm that aims to address this problem through the embodiment of the individuals that make up the population in the physical robots. The interest for this type of evolutionary approach has been increasing steadily, leading to different algorithms and variations adapted to solve very specific practical cases. In a previous work, the authors started the implementation of a standard canonical EE algorithm that captures the more general principles of this paradigm and that can be applied to any distributed optimization problem. This canonical algorithm has been characterized already over a set of theoretical fitness landscapes corresponding to representative examples of the basic casuistry found in collective tasks. The current paper goes one step ahead in this research line, and the canonical algorithm is applied here in a collective navigation task in which a fleet of Micro Aerial Vehicles (MAVs) has to gather red rocks in an indoor scenario. The objective is to confirm that the characterization conclusions are generalizable to a practical case and to show that the canonical algorithm can be configured to operate as a specific algorithm easily.",experiments
130,10.1109/cbs.2018.8612261,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8612261/,multimedia,27/10/2018 00:00,not defined,not defined,not defined,not defined,semg-based torque estimation using time-delay ann for control of an upper-limb rehabilitation robot,"Robotic-assisted rehabilitation of the upper limb following neurological injury can achieve best possible functional recovery when patients are engaged in the therapy. However, implementation of active training is still difficult as it's challenging to detect human motion intention online and impose corresponding robot control. This paper introduces a novel upper-limb rehabilitation robot, and proposes a sEMG-driven (sEMG: surface Electromyography) torque estimation model based on artificial neural networks (ANN). The robot has three DOFs, of which the first two DOFs adopt a planar parallel structure, and the wrist module has an exoskeleton form. In this study, we design an impedance controller and an admittance controller for the first two DOFs and the wrist module, respectively. Specifically, for the first two DOFs, the assistance/resistance force at the end-effector was controlled according to its motions and desired interaction impedance; for the wrist module, an sEMG armband was used to collect 8 channels of sEMG signals from the forearm muscles, and a time-delay ANN model was developed to estimate the wrist pronation/supination torque, based on which the wrist rotation was controlled according to the human motion intention. To overcome the overfitting problem, besides the experimental samples of wrist rotation, both resting and co-contraction samples were collected for training. Finally, combining with the design of a virtual reality game and force fields, the proposed methods were implemented and tested experimentally on the upper-limb rehabilitation robot.",experiments
131,10.28925/2663-4023.2020.8.97112,Borys Grinchenko Kyiv University,project-academic,project-academic,smart cities,25/06/2020 00:00,supervised,batch,classification,not defined,application of the convolutional neural networks for the security of the object recognition in a video stream," The article is devoted to analyzing methods for recognizing images and finding them in the video stream. The evolution of the structure of convolutional neural networks used in the field of computer video flow diagnostics is analyzed. The performance of video flow diagnostics algorithms and car license plate recognition has been evaluated. The technique of recognizing the license plates of cars in the video stream of transport neural networks is described. The study focuses on the creation of a combined system that combines artificial intelligence and computer vision based on fuzzy logic. To solve the problem of license plate image recognition in the video stream of the transport system, a method of image recognition in a continuous video stream with its implementation based on the composition of traditional image processing methods and neural networks with convolutional and periodic layers is proposed. The structure and peculiarities of functioning of the intelligent distributed system of urban transport safety, which feature is the use of mobile devices connected to a single network, are described.
A practical implementation of a software application for recognizing car license plates by mobile devices on the Android operating system platform has been proposed and implemented. Various real-time vehicle license plate recognition scenarios have been developed and stored in a database for further analysis and use. The proposed application uses two different specialized neural networks: one for detecting objects in the video stream, the other for recognizing text from the selected image. Testing and analysis of software applications on the Android operating system platform for license plate recognition in real time confirmed the functionality of the proposed mathematical software and can be used to securely analyze the license plates of cars in the scanned video stream by comparing with license plates in the existing database. The authors have implemented the operation of the method of convolutional neural networks detection and recognition of license plates, personnel and critical situations in the video stream from cameras of mobile devices in real time. The possibility of its application in the field of safe identification of car license plates has been demonstrated.",excluded
132,,Predicting large scale fine grain energy consumption,core,https://core.ac.uk/download/pdf/234915328.pdf,science,01/01/2017 00:00,supervised,batch,not defined,not defined,10.1016/j.egypro.2017.03.271,"Today a large volume of energy-related data have been continuously collected. Extracting actionable knowledge from such data is a multi-step process that opens up a variety of interesting and novel research issues across two domains: energy and computer science. The computer science aim is to provide energy scientists with cutting-edge and scalable engines to effectively support them in their daily research activities. This paper presents SPEC, a scalable and distributed predictor of fine grain energy consumption in buildings. SPEC exploits a data stream methodology analysis over a sliding time window to train a prediction model tailored to each building. The building model is then exploited to predict the upcoming energy consumption at a time instant in the near future. SPEC currently integrates the artificial neural networks technique and the random forest regression algorithm. The SPEC methodology exploits the computational advantages of distributed computing frameworks as the current implementation runs on Spark. As a case study, real data of thermal energy consumption collected in a major city have been exploited to preliminarily assess the SPEC accuracy. The initial results are promising and represent a first step towards predicting fine grain energy consumption over a sliding time window",excluded
133,10.1016/j.neunet.2016.08.003,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/84984998255,robotics,01/11/2016,not defined,not defined,not defined,not defined,implementation of imitation learning using natural learner central pattern generator neural networks,"In this paper a new design of neural networks is introduced, which is able to generate oscillatory patterns. The fundamental building block of the neural network is O-neurons that can generate an oscillation in its transfer functions. Since the natural policy gradient learning has been used in training a central pattern generator paradigm, it is called Natural Learner CPG Neural Networks (NLCPGNN). O-neurons are connected and coupled to each other in order to shape a network and their unknown parameters are found by a natural policy gradient learning algorithm. The main contribution of this paper is design of this learning algorithm which is able to simultaneously search for the weights and topology of the network. This system is capable to obtain any complex motion and rhythmic trajectory via first layer and learn rhythmic trajectories in the second layer and converge towards all these movements. Moreover this two layers system is able to provide various features of a learner model for instance resistance against perturbations, modulation of trajectories amplitude and frequency. Simulation of the learning system in the robot simulator (WEBOTS) that is linked with MATLAB software has been done. Implementation on a real NAO robot demonstrates that the robot has learned desired motion with high accuracy. These results show proposed system produces high convergence rate and low test errors.",experiments
134,,Lunds universitet/Institutionen för elektro- och informationsteknik,core,,autonomous vehicle,01/01/2020 00:00,not defined,not defined,not defined,not defined,implementation of a deep learning inference accelerator on the fpga.,"Today, Artificial Intelligence is one of the most important technologies, ubiquitous in our daily lives. Deep Neural Networks (DNN's) have come up as state of art for various machine intelligence applications such as object detection, image classification, face recognition and performs myriad of activities with exceptional prediction accuracy. AI in this contemporary world is moving towards embedded platforms for inference on the edge. This is essential to avoid latency, enhance data security and realize real-time performance. However, these DNN algorithms are computational and memory intensive. Consequently, exploiting immense energy, compute resources and memory-bandwidth making it difficult to be deployed in embedded devices. To solve this problem and realize an on-device AI acceleration, dedicated energy-efficient hardware accelerators are paramount. This thesis involves the implementation of such a dedicated deep learning accelerator on the FPGA. The NVIDIA's Deep Learning Accelerator (NVDLA), is encompassed in this research to explore SoC designs for integrated inference acceleration. NVDLA, an open-source architecture, standardizes deep learning inference acceleration on hardware. It optimizes inference acceleration all across the full stack from application through hardware to achieve energy efficiency synergy with the demanding throughput requirements. Therefore, the following thesis probes into the NVDLA framework to perceive the consistent workflow across the whole hardware-software programming hierarchies. Besides, the hardware design parameters, optimization features and system configurations of the NVDLA systems are analyzed for efficient implementations. Also, a comparative study of the diverse NVDLA SoC implementations (nv\_small and nv\_medium) with respect to performance metrics such as power, area, and throughput are discussed. Our approach engages prototyping of Nvidia’s Deep Learning Accelerator on a Zynq Ultrascale+ ZCU104 FPGA to examine its system functionality. The Hardware design of the system is carried out using Xilinx's Vivado Design Suite 2018.3 in Verilog. While the on-device software runs Linux kernel 4.14 on Zynq MPSoC. Thus, the software ecosystem is built with PetaLinux tools from Xilinx. The entire system architecture is validated using the pre-built regression tests that verify individual CNN layers. Besides these NVDLA hardware design also runs pre-compiled AlexNet as a benchmark for performance evaluation and comparisonToday, Artificial Intelligence is at the edge. This edge or endpoint device is becoming more sophisticated with the evolution of Internet of Things (IoT) and 5G. For instance, these devices are employed in different applications such as autonomous cars, drones, and other IoT gadgets. At present, a self-driving car is a data center on wheels, a drone is a data center on wings as well as robots are data centers with arms and legs. All these mechanisms collect vast real-world information that demands to be processed in real-time. Here in these applications, there is no time to send data to the cloud for processing and wait for action. As the decision making needs to be instantaneous. There is a shift in transforming the processing to the edge devices. The edge acceleration brings computation and data storage closer to the device. With the evolution of specialized hardware’s providing increased computational capabilities, the AI models are processed on the edge. As a result, the overall system latency gets reduced, the bandwidth costs for data transfers are lowered and the data processing is done locally enhances privacy concerns. For example, autonomous cars require a spontaneous reaction (in seconds) to avoid potential hazards on the road. Consider the situation where a self-driving car is collecting real word information like images, videos, in this case, assume it’s sensing for a stop sign. If the system sends the specific image information to the cloud for processing and waits for a decision to stop. By that response time, the autonomous vehicle could have already blown through the stop sign running over several people. Therefore, it is paramount to process the data in real-time which could be accomplished using dedicated hardware for processing locally. This thesis primarily explores those hardware architectures for efficient processing of AI algorithms and their corresponding software execution environment setup. The particular thesis was carried out as a joint collaboration between Ericsson and Lund University. Here Nvidia’s Deep Learning Accelerator architecture is engaged as a target to comprehend the complete system incorporating a hardware-software co-design. The particular architecture is an essential characteristic of NVIDIA’s Xavier Drive chip which is utilized in their autonomous drive platforms. This thesis is addressed to a variety of audiences who are passionate about Deep Learning, Computer Architecture, and System-on-Chip Design. The thesis illustrates a comprehensive implementation of an AI accelerator to envision AI processing on the edge",excluded
135,10.1109/icra.2019.8793510,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8793510/,robotics,24/05/2019 00:00,not defined,not defined,not defined,not defined,bonnet: an open-source training and deployment framework for semantic segmentation in robotics using cnns,"The ability to interpret a scene is an important capability for a robot that is supposed to interact with its environment. The knowledge of what is in front of the robot is, for example, relevant for navigation, manipulation, or planning. Semantic segmentation labels each pixel of an image with a class label and thus provides a detailed semantic annotation of the surroundings to the robot. Convolutional neural networks (CNNs) are popular methods for addressing this type of problem. The available software for training and the integration of CNNs for real robots, however, is quite fragmented and often difficult to use for non-experts, despite the availability of several high-quality open-source frameworks for neural network implementation and training. In this paper, we propose a tool called Bonnet, which addresses this fragmentation problem by building a higher abstraction that is specific for the semantic segmentation task. It provides a modular approach to simplify the training of a semantic segmentation CNN independently of the used dataset and the intended task. Furthermore, we also address the deployment on a real robotic platform. Thus, we do not propose a new CNN approach in this paper. Instead, we provide a stable and easy-to-use tool to make this technology more approachable in the context of autonomous systems. In this sense, we aim at closing a gap between computer vision research and its use in robotics research. We provide an open-source codebase for training and deployment. The training interface is implemented in Python using TensorFlow and the deployment interface provides C++ library that can be easily integrated in an existing robotics codebase, a ROS node, and two standalone applications for label prediction in images and videos.",architecture
136,10.1109/cdc.2006.377499,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/4178112/,robotics,15/12/2006 00:00,not defined,not defined,not defined,not defined,path generation using matrix representations of previous robot state data,"Humans learn by repetition and using past experiences. It is possible for robots to act in a similar fashion. By representing past path traversal experiences with matrices, a new path can be generated without relying on calculations of complex dynamics or control laws. This paper presents one approach for allowing robots to use past experience to generate new paths and control actions. This approach relies on using several matrices to associate each new input value with previous robot states. An example is provided and analyzed which shows a successful simulated implementation of this approach. In addition a real world test of the approach was conducted which demonstrates that the implementation not only generates new paths, but does so fast enough to be feasible for real time systems",experiments
137,10.1109/icdmw.2019.00123,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8955523/,health,11/11/2019 00:00,not defined,not defined,not defined,not defined,implementation of mobile-based real-time heart rate variability detection for personalized healthcare,"The ubiquity of wearable devices together with areas like internet of things, big data and machine learning have promoted the development of solutions for personalized healthcare that use digital sensors. However, there is a lack of an implemented framework that is technically feasible, easily scalable and that provides meaningful variables to be used in applications for translational medicine. This paper describes the implementation and early evaluation of a physiological sensing tool that collects and processes photoplethysmography data from a wearable smartwatch to calculate heart rate variability in real-time. A technical open-source framework is outlined, involving mobile devices for collection of heart rate data, feature extraction and execution of data mining or machine learning algorithms that ultimately deliver mobile health interventions tailored to the users. Eleven volunteers participated in the empirical evaluation that was carried out using an existing mobile virtual reality application for mental health and under controlled slow-paced breathing exercises. The results validated the feasibility of implementation of the proposed framework in the stages of signal acquisition and real-time calculation of heart rate variability (HRV). The analysis of data regarding packet loss, peak detection and overall system performance provided considerations to enhance the real-time calculation of HRV features. Further studies are planned to validate all the stages of the proposed framework.",architecture
138,http://arxiv.org/abs/1702.06329v1,arxiv,arxiv,http://arxiv.org/abs/1702.06329v1,smart cities,21/02/2017 00:00,rl,not defined,not defined,not defined,"towards a common implementation of reinforcement learning for multiple
  robotic tasks","Mobile robots are increasingly being employed for performing complex tasks in
dynamic environments. Reinforcement learning (RL) methods are recognized to be
promising for specifying such tasks in a relatively simple manner. However, the
strong dependency between the learning method and the task to learn is a
well-known problem that restricts practical implementations of RL in robotics,
often requiring major modifications of parameters and adding other techniques
for each particular task. In this paper we present a practical core
implementation of RL which enables the learning process for multiple robotic
tasks with minimal per-task tuning or none. Based on value iteration methods,
this implementation includes a novel approach for action selection, called
Q-biased softmax regression (QBIASSR), which avoids poor performance of the
learning process when the robot reaches new unexplored states. Our approach
takes advantage of the structure of the state space by attending the physical
variables involved (e.g., distances to obstacles, X,Y,{\theta} pose, etc.),
thus experienced sets of states may favor the decision-making process of
unexplored or rarely-explored states. This improvement has a relevant role in
reducing the tuning of the algorithm for particular tasks. Experiments with
real and simulated robots, performed with the software framework also
introduced here, show that our implementation is effectively able to learn
different robotic tasks without tuning the learning method. Results also
suggest that the combination of true online SARSA({\lambda}) with QBIASSR can
outperform the existing RL core algorithms in low-dimensional robotic tasks.",architecture
139,,,core,,robotics,01/01/2000 00:00,not defined,not defined,not defined,not defined,natural landmark recognition using neural networks for autonomous vacuuming robots,"Two types of neural networks were trained and tested on a real robot for a natural landmark recognition task. The neural networks investigated were the multilayer perceptron (MLP) and learning vector quantisation (LVQ). The intended application is for autonomous vacuuming robots in completely unknown indoor environments, using a novel topological world model and region filling algorithm. A topological world model based on natural landmarks is built incrementally while the robot systematically cleans the environment. The implementation of this world model depends on robust and accurate recognition of natural landmarks. Both types of neural network were found to be able to successfully recognise the natural landmarks selected",excluded
140,10.23919/iconac.2019.8895095,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8895095/,industry,07/09/2019 00:00,rl,not defined,not defined,not defined,ant colony optimization algorithm for industrial robot programming in a digital twin,"Advanced manufacturing that is adaptable to constantly changing product designs often requires dynamic changes on the factory floor to enable manufacture. The integration of robotic manufacture with machine learning approaches offers the possibility to enable such dynamic changes on the factory floor. While ensuring safety and the possibility of losses of components and waste of material are against their usage. Furthermore, developments in design of virtual environments makes it possible to perform simulations in a virtual environment, to enable human-in-the-loop production of parts correctly the first time like never before. Such powerful simulation and control software provides the means to design a digital twin of manufacturing environment in which trials are completed at almost at no cost. In this paper, ant colony optimization is used to program an industrial robot to avoid obstacles and find its way to pick and place objects during an assembly task in an environment containing obstacles that must be avoided. The optimization is completed in a digital twin environment first and movements transferred to the real robot after human inspection. It is shown that the proposed methodology can find the optimal solution, in addition to avoiding collisions, for an assembly task with minimum human intervention.",experiments
141,10.1016/j.asoc.2011.05.011,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/80053571498,industry,01/12/2011,supervised,batch,classification,centralised,credit risk evaluation using neural networks: emotional versus conventional models,"Credit scoring and evaluation is one of the key analytical techniques in credit risk evaluation which has been an active research area in financial risk management. Artificial neural networks (NNs) have been considered to be accurate tools for credit analysis among others in the credit industry. Lately, emotional neural networks (EmNNs) have been suggested and applied successfully for pattern recognition. In this paper we investigate the efficiency of EmNNs and compare their performance to conventional NNs when applied to credit risk evaluation. In total 12 neural networks; based equally on emotional and conventional neural models; are arbitrated under three learning schemes to classify whether a credit application is approved or declined. The learning schemes differ in the ratio of training-to-validation data used during training and testing the neural networks. The emotional and conventional neural models are trained using real world credit application cases from the Australian credit approval datasets which has 690 cases; each case with 14 numerical attributes; based on which an application is accepted or rejected. The performance of the 12 neural networks will be evaluated using certain criteria. Experimental results suggest that both emotional and conventional neural models can be used effectively for credit risk evaluations, however the emotional models outperform their conventional counterparts in decision making speed and accuracy, thus, making them ideal for implementation in fast automatic processing of credit applications.",excluded
142,10.1109/conielecomp.2014.6808580,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/6808580/,multimedia,28/02/2014 00:00,not defined,not defined,not defined,not defined,implementation of an embedded system on a ts7800 board for robot control,"Growing Functional Modules (GFM) learning based controllers need to be experimented on real robots. In 2009, looking to develop a flexible and generic embedded interface for such robots, we decided to use a TS-7800 single board computer (SBC) with a Debian Linux operating system. Despite the many advantages of this board, implementing the embedded system has been a complex task. This paper describes the implementation of protocols through the TS-7800 different ports (RS232, TCP/IP, USB, analog and digital pins) as well as the connection of external boards (TS-ADC24, TS-DIO64, SSC-32 and LCD display). This implementation was required to connect a large range of actuators, sensors and other peripherals. Furthermore, the architecture of the embedded system is exposed in detail, including topics such as the XML configuration file that specifies the peripherals connected to the SBC, the concept of virtual sensors, the implementation of parallelism and the embedded system interface launcher. Technical aspects such as the optimization of video capture and processing are detailed because their execution required specific compilers versions, EABI emulation and extra libraries (openCV libjpg and libpngand libv4l). The final embedded system was implemented in a humanoid robot and connected to the GFM controller in charge of developing its equilibrium subsystem.",excluded
143,10.1109/icmla51294.2020.00193,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9356175/,multimedia,17/12/2020 00:00,not defined,not defined,not defined,not defined,an embedded deep learning system for augmented reality in firefighting applications,"Firefighting is a dynamic activity, in which numerous operations occur simultaneously. Maintaining situational awareness (i.e., knowledge of current conditions and activities at the scene) is critical to the accurate decision-making necessary for the safe and successful navigation of a fire environment by firefighters. Conversely, the disorientation caused by hazards such as smoke and extreme heat can lead to injury or even fatality. This research implements recent advancements in technology such as deep learning, point cloud and thermal imaging, and augmented reality platforms to improve a firefighter's situational awareness and scene navigation through improved interpretation of that scene. We have designed and built a prototype embedded system that can leverage data streamed from cameras built into a firefighter's personal protective equipment (PPE) to capture thermal, RGB color, and depth imagery and then deploy already developed deep learning models to analyze the input data in real time. The embedded system analyzes and returns the processed images via wireless streaming, where they can be viewed remotely and relayed back to the firefighter using an augmented reality platform that visualizes the results of the analyzed inputs and draws the firefighter's attention to objects of interest, such as doors and windows otherwise invisible through smoke and flames.",experiments
144,10.1007/978-3-030-28925-6_1,Springer,springer,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-28925-6_1,smart cities,01/01/2020 00:00,not defined,not defined,not defined,not defined,cityflow: supporting spatial-temporal edge computing for urban machine learning applications,"A growing trend in smart cities is the use of machine learning techniques to gather city data, formulate learning tasks and models, and use these to develop solutions to city problems. However, although these processes are sufficient for theoretical experiments, they often fail when they meet the reality of city data and processes, which by their very nature are highly distributed, heterogeneous, and exhibit high degrees of spatial and temporal variance. In order to address those problems, we have designed and implemented an integrated development environment called CityFlow that supports developing machine learning applications. With CityFlow, we can develop, deploy, and maintain machine learning applications easily by using an intuitive data flow model. To verify our approach, we conducted two case studies: deploying a road damage detection application to help monitor transport infrastructure and an automatic labeling application in support of a participatory sensing application. These applications show both the generic applicability of our approach, and its ease of use; both critical if we wish to deploy sophisticated ML based applications to smart cities.",architecture
145,10.1109/osscom.2016.7863679,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7863679/,science,03/12/2016 00:00,not defined,not defined,not defined,not defined,on tackling social engineering web phishing attacks utilizing software defined networks (sdn) approach,"Web phishing attacks are one of the challenging security threats. Phishing depends on humans' behavior but not protocols and devices vulnerabilities. In this work, software defined networking (SDN) will be tailored to tackle phishing attacks. In SDN, network devices forward received packets to a central point `controller' that makes decision on behalf of them. This approach allows more control and management over network devices and protocol. In this work, we propose a neural network based phishing prevention algorithm (PPA) that is implemented utilizing Ryu, an open source, SDN controller. The PPA algorithm has been tested in a home network that is constructed with HP2920-24G switch. Moreover, a phished version of Facebook, Yahoo and Hotmail login pages have been written and hosted on three different free hosting domains. PPA has detected all of the phished versions and allowed the access to real version of these services.",excluded
146,10.1109/icaiic.2019.8669047,IEEE,project-academic,project-academic,autonomous vehicle,01/02/2019 00:00,not defined,not defined,not defined,not defined,a complete multi cpu fpga based design and prototyping methodology for autonomous vehicles multiple object detection and recognition case study," Embedded smart systems are Hardware/Software (HW/SW) architectures integrated in new autonomous vehicles in order to increase their smartness. A key example of such applications are camera-based automatic parking systems. In this paper we introduce a fast prototyping perspective within a complete design methodology for these embedded smart systems. One of our main objective being to reduce development and prototyping time, compared to usual simulation approaches. Based on our previous work [1], a supervised machine learning approach, we propose a HW/SW algorithm implementation for objects detection and recognition around autonomous vehicles. We validate our real-time approach via a quick prototype on the top of a Multi-CPU/FPGA platform (ZYNQ). The main contribution of this current work is the definition of a complete design methodology for smart embedded vehicle applications which defines four main parts: specification & native software, hardware acceleration, machine learning software, and the real embedded system prototype. Toward a full automation of our methodology, several steps are already automated and presented in this work. Our hardware acceleration of point cloud-based data processing tasks is 300 times faster than a pure software implementation.",architecture
147,http://arxiv.org/abs/2109.13602v1,arxiv,arxiv,http://arxiv.org/abs/2109.13602v1,industry,28/09/2021 00:00,not defined,not defined,not defined,not defined,"safetynet: safe planning for real-world self-driving vehicles using
  machine-learned policies","In this paper we present the first safe system for full control of
self-driving vehicles trained from human demonstrations and deployed in
challenging, real-world, urban environments. Current industry-standard
solutions use rule-based systems for planning. Although they perform reasonably
well in common scenarios, the engineering complexity renders this approach
incompatible with human-level performance. On the other hand, the performance
of machine-learned (ML) planning solutions can be improved by simply adding
more exemplar data. However, ML methods cannot offer safety guarantees and
sometimes behave unpredictably. To combat this, our approach uses a simple yet
effective rule-based fallback layer that performs sanity checks on an ML
planner's decisions (e.g. avoiding collision, assuring physical feasibility).
This allows us to leverage ML to handle complex situations while still assuring
the safety, reducing ML planner-only collisions by 95%. We train our ML planner
on 300 hours of expert driving demonstrations using imitation learning and
deploy it along with the fallback layer in downtown San Francisco, where it
takes complete control of a real vehicle and navigates a wide variety of
challenging urban driving scenarios.",experiments
148,10.1016/j.robot.2018.02.010,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85044145526,robotics,01/06/2018,not defined,not defined,not defined,not defined,visual attention and object naming in humanoid robots using a bio-inspired spiking neural network,"Recent advances in behavioural and computational neuroscience, cognitive robotics, and in the hardware implementation of large-scale neural networks, provide the opportunity for an accelerated understanding of brain functions and for the design of interactive robotic systems based on brain-inspired control systems. This is especially the case in the domain of action and language learning, given the significant scientific and technological developments in this field. In this work we describe how a neuroanatomically grounded spiking neural network for visual attention has been extended with a word learning capability and integrated with the iCub humanoid robot to demonstrate attention-led object naming. Experiments were carried out with both a simulated and a real iCub robot platform with successful results. The iCub robot is capable of associating a label to an object with a ‘preferred’ orientation when visual and word stimuli are presented concurrently in the scene, as well as attending to said object, thus naming it. After learning is complete, the name of the object can be recalled successfully when only the visual input is present, even when the object has been moved from its original position or when other objects are present as distractors.",experiments
149,10.1016/j.jmsy.2021.04.005,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85106283308,industry,01/07/2021,not defined,not defined,not defined,not defined,learningadd: machine learning based acoustic defect detection in factory automation,"Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061 s in 99 % probability.",architecture
150,10.1007/s44196-021-00040-x,Springer,springer,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s44196-021-00040-x,health,20/11/2021 00:00,not defined,not defined,not defined,not defined,edge computing using embedded webserver with mobile device for diagnosis and prediction of metastasis in histopathological images,"Diagnosis of different breast cancer stages using histopathology whole slide images is the gold standard in grading the tissue metastasis. Traditional diagnosis involves labor intensive procedures and is prone to human errors. Computer aided diagnosis assists medical experts as a second opinion tool in early detection which prevents further proliferation. Computing facilities have emerged to an extent where algorithms can attain near human accuracy in prediction of diseases, offering better treatment to curb further proliferation. The work introduced in the paper provides an interface in mobile platform, which enables the user to input histopathology image and obtain the prediction results with its class probability through embedded web-server. The trained deep convolutional neural networks model is deployed into a microcomputer-based embedded system after hyper-parameter tuning, offering congruent performance. The implementation results show that the embedded platform with custom-trained CNN model is suitable for medical image classification, as it takes less execution time and mean prediction time. It is also noticed that customized CNN classifier model outperforms pre-trained models when used in embedded platforms for prediction and classification of histopathology images. This work also emphasizes the relevance of portable and flexible embedded device in real time clinical applications.",architecture
151,http://arxiv.org/abs/2109.14549v1,arxiv,arxiv,http://arxiv.org/abs/2109.14549v1,multimedia,29/09/2021 00:00,not defined,not defined,not defined,not defined,"vision-guided quadrupedal locomotion in the wild with multi-modal delay
  randomization","Developing robust vision-guided controllers for quadrupedal robots in complex
environments, with various obstacles, dynamical surroundings and uneven
terrains, is very challenging. While Reinforcement Learning (RL) provides a
promising paradigm for agile locomotion skills with vision inputs in
simulation, it is still very challenging to deploy the RL policy in the real
world. Our key insight is that aside from the discrepancy in the domain gap, in
visual appearance between the simulation and the real world, the latency from
the control pipeline is also a major cause of difficulty. In this paper, we
propose Multi-Modal Delay Randomization (MMDR) to address this issue when
training RL agents. Specifically, we simulate the latency of real hardware by
using past observations, sampled with randomized periods, for both
proprioception and vision. We train the RL policy for end-to-end control in a
physical simulator without any predefined controller or reference motion, and
directly deploy it on the real A1 quadruped robot running in the wild. We
evaluate our method in different outdoor environments with complex terrains and
obstacles. We demonstrate the robot can smoothly maneuver at a high speed,
avoid the obstacles, and show significant improvement over the baselines. Our
project page with videos is at https://mehooz.github.io/mmdr-wild/.",experiments
152,10.5120/20328-2507,,core,,health,21/11/2015 00:00,not defined,not defined,not defined,not defined,software design framework for healthcare systems,"Healthcare is costing lot of money and resources all over the world in all countries; it consumes almost 30-40 percent of their budget in the healthcare industry. The only solution is to make this health care industry automated and online. In this paper we are proposing a software design framework for health care systems which will be based on agent technologies. The proposed system is based on Artificial Intelligence techniques that can support the user for selection and making choices. The proposed software framework suggested that Multi-Agent Systems (MAS) is the most suitable technique for designing such systems. MAS collaborate intelligently for solving this complex problem. Patients can be supported remotely using the proposed framework so that it can reduce the patient load on hospitals, the proposed software framework operate on real time framework. Computing can help in improving the communication process between follow-up doctors and nurses with patients by making appointments the process easier, according to patient preference with a reminder on necessary actions such as taking scheduled prescribed medicine, engaging in exercises, avoiding some kinds of food and harmful habits such as smoking before and after patient",excluded
153,10.1016/j.advwatres.2009.01.001,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/62349136438,smart cities,01/04/2009,not defined,not defined,not defined,not defined,pumping optimization of coastal aquifers based on evolutionary algorithms and surrogate modular neural network models,"Pumping optimization of coastal aquifers involves complex numerical models. In problems with many decision variables, the computational burden for reaching the optimal solution can be excessive. Artificial Neural Networks (ANN) are flexible function approximators and have been used as surrogate models of complex numerical models in groundwater optimization. However, this approach is not practical in cases where the number of decision variables is large, because the required neural network structure can be very complex and difficult to train. The present study develops an optimization method based on modular neural networks, in which several small subnetwork modules, trained using a fast adaptive procedure, cooperate to solve a complex pumping optimization problem with many decision variables. The method utilizes the fact that salinity distribution in the aquifer, depends more on pumping from nearby wells rather than from distant ones. Each subnetwork predicts salinity in only one monitoring well, and is controlled by relatively few pumping wells falling within certain control distance from the monitoring well. While the initial control area is radial, its shape is adaptively improved using a Hermite interpolation procedure. The modular neural subnetworks are trained adaptively during optimization, and it is possible to retrain only the ones not performing well. As optimization progresses, the subnetworks are adapted to maximize performance near the current search space of the optimization algorithm. The modular neural subnetwork models are combined with an efficient optimization algorithm and are applied to a real coastal aquifer in the Greek island of Santorini. The numerical code SEAWAT was selected for solving the partial differential equations of flow and density dependent transport. The decision variables correspond to pumping rates from 34 wells. The modular subnetwork implementation resulted in significant reduction in CPU time and identified an even better solution than the original numerical model.",excluded
154,http://arxiv.org/abs/1805.00361v1,arxiv,arxiv,http://arxiv.org/abs/1805.00361v1,smart cities,30/04/2018 00:00,not defined,not defined,not defined,not defined,"ultra power-efficient cnn domain specific accelerator with 9.3tops/watt
  for mobile and embedded applications","Computer vision performances have been significantly improved in recent years
by Convolutional Neural Networks(CNN). Currently, applications using CNN
algorithms are deployed mainly on general purpose hardwares, such as CPUs, GPUs
or FPGAs. However, power consumption, speed, accuracy, memory footprint, and
die size should all be taken into consideration for mobile and embedded
applications. Domain Specific Architecture (DSA) for CNN is the efficient and
practical solution for CNN deployment and implementation. We designed and
produced a 28nm Two-Dimensional CNN-DSA accelerator with an ultra
power-efficient performance of 9.3TOPS/Watt and with all processing done in the
internal memory instead of outside DRAM. It classifies 224x224 RGB image inputs
at more than 140fps with peak power consumption at less than 300mW and an
accuracy comparable to the VGG benchmark. The CNN-DSA accelerator is
reconfigurable to support CNN model coefficients of various layer sizes and
layer types, including convolution, depth-wise convolution, short-cut
connections, max pooling, and ReLU. Furthermore, in order to better support
real-world deployment for various application scenarios, especially with
low-end mobile and embedded platforms and MCUs (Microcontroller Units), we also
designed algorithms to fully utilize the CNN-DSA accelerator efficiently by
reducing the dependency on external accelerator computation resources,
including implementation of Fully-Connected (FC) layers within the accelerator
and compression of extracted features from the CNN-DSA accelerator. Live demos
with our CNN-DSA accelerator on mobile and embedded systems show its
capabilities to be widely and practically applied in the real world.",experiments
155,10.1109/ro-man50785.2021.9515431,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9515431/,industry,12/08/2021 00:00,not defined,not defined,not defined,not defined,simplifying the a.i. planning modeling for human-robot collaboration,"For an effective deployment in manufacturing, Collaborative Robots should be capable of adapting their behavior to the state of the environment and to keep the user safe and engaged during the interaction. Artificial Intelligence (AI) enables robots to autonomously operate understanding the environment, planning their tasks and acting to achieve some given goals. However, the effective deployment of AI technologies in real industrial environments is not straightforward. There is a need for engineering tools facilitating communication and interaction between AI engineers and Domain experts. This paper proposes a novel software tool, called TENANT (Tool fostEriNg Ai plaNning in roboTics) whose aim is to facilitate the use of AI planning technologies by providing domain experts like e.g., production engineers, with a graphical software framework to synthesize AI planning models abstracting from syntactic features of the underlying planning formalism.",excluded
156,10.1117/12.2540782,International Society for Optics and Photonics,project-academic,project-academic,autonomous vehicle,18/12/2019 00:00,not defined,not defined,not defined,not defined,hardware and software complex for monitoring oil pollution of sea aquatories," A program for recognizing oil spill on the sea surface, based on an artificial intelligence element, was developed and tested on the example of real oil pollution in Peter the Great Bay for use on an unmanned aerial vehicle. The feature of the spectra of broadband radiation ascending from the sea surface is analyzed. It is concluded that the method of recording the spectra of the ascending radiation can be used to detect heavy oil fractions on the sea surface. A software algorithm for the formation of datasets of spectra of induced fluorescence of sea water containing various dissolved grades of petroleum products has been developed and tested. A machine learning procedure has been carried out to create a program element for classifying the type of oil hydrocarbons dissolved in seawater.",excluded
157,,,project-academic,project-academic,health,19/11/2019 00:00,not defined,not defined,not defined,not defined,the human body is a black box supporting clinical decision making with deep learning," Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to accuracy, fairness, accountability, and transparency that come from actual, situated use. Serious questions remain under examined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing on model interpretability to ensure a fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.",excluded
158,,"Can a Robot Become a Movie Director? Learning Artistic Principles for
  Aerial Cinematography",core,,multimedia,15/10/2019 00:00,not defined,not defined,not defined,not defined,http://arxiv.org/abs/1904.02579,"Aerial filming is constantly gaining importance due to the recent advances in
drone technology. It invites many intriguing, unsolved problems at the
intersection of aesthetical and scientific challenges. In this work, we propose
a deep reinforcement learning agent which supervises motion planning of a
filming drone by making desirable shot mode selections based on aesthetical
values of video shots. Unlike most of the current state-of-the-art approaches
that require explicit guidance by a human expert, our drone learns how to make
favorable viewpoint selections by experience. We propose a learning scheme that
exploits aesthetical features of retrospective shots in order to extract a
desirable policy for better prospective shots. We train our agent in realistic
AirSim simulations using both a hand-crafted reward function as well as reward
from direct human input. We then deploy the same agent on a real DJI M210 drone
in order to test the generalization capability of our approach to real world
conditions. To evaluate the success of our approach in the end, we conduct a
comprehensive user study in which participants rate the shot quality of our
methods. Videos of the system in action can be seen at
https://youtu.be/qmVw6mfyEmw",experiments
159,10.1109/rweek.2018.8473535,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8473535/,health,23/08/2018 00:00,not defined,not defined,not defined,not defined,framework for data driven health monitoring of cyber-physical systems,"Modern infrastructure is heavily reliant on systems with interconnected computational and physical resources, named Cyber-Physical Systems (CPSs). Hence, building resilient CPSs is a prime need and continuous monitoring of the CPS operational health is essential for improving resilience. This paper presents a framework for calculating and monitoring of health in CPSs using data driven techniques. The main advantages of this data driven methodology is that the ability of leveraging heterogeneous data streams that are available from the CPSs and the ability of performing the monitoring with minimal a priori domain knowledge. The main objective of the framework is to warn the operators of any degradation in cyber, physical or overall health of the CPS. The framework consists of four components: 1) Data acquisition and feature extraction, 2) state identification and real time state estimation, 3) cyber-physical health calculation and 4) operator warning generation. Further, this paper presents an initial implementation of the first three phases of the framework on a CPS testbed involving a Microgrid simulation and a cyber-network which connects the grid with its controller. The feature extraction method and the use of unsupervised learning algorithms are discussed. Experimental results are presented for the first two phases and the results showed that the data reflected different operating states and visualization techniques can be used to extract the relationships in data features.",architecture
160,10.1016/j.chemolab.2021.104314,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85105360467,industry,15/06/2021,not defined,not defined,not defined,not defined,a scalable approach for the efficient segmentation of hyperspectral images,"The number of applications of hyperspectral imaging (HSI) is steadily increasing, as technology evolves and cameras become more affordable. However, the volume of data in a hyperspectral image is large (order of Gigabytes) and standard off-the-shelf algorithms for multi-channel image analysis cannot be readily applied, due to the prohibitive computational time and large memory requirements. Therefore, new scalable approaches are required to perform hyperspectral image analysis. In this article we address an efficient methodology for conducting Unsupervised Image Segmentation – one of the basic and most fundamental image analysis operations. In the methodology proposed, unsupervised segmentation is conducted after transforming the spectral and spatial dimensions of the raw hyperspectral image into a more compact representation using multivariate and multiresolution techniques. The clusters identified in the compact image representation are then used to train a discriminative classifier. The classifier is then adapted and transferred for application to the raw image, where it will efficiently label all the original pixels. With the proposed methodology, the computational expensive operations (unsupervised clustering and classifier learning) are minimized, whereas the efficient implementation of the classifier guarantees the analysis at the native resolution. The effectiveness of the proposed methodology was tested on a real case study considering an industrial hyperspectral image capturing the reflectance spectrum for several objects made of different unknown materials. A significant reduction in the computational cost was achieved without compromising the quality of the unsupervised segmentation, demonstrating the potential of the proposed approach.",experiments
161,proceedings of machine learning research,,core,,robotics,01/01/2018 00:00,not defined,not defined,not defined,not defined,learning deployable navigation policies at kilometer scale from a single traversal,"Model-free reinforcement learning has recently been shown to be effective at learning navigation policies from complex image input. However, these algorithms tend to require large amounts of interaction with the environment, which can be prohibitively costly to obtain on robots in the real world. We present an approach for efficiently learning goal-directed navigation policies on a mobile robot, from only a single coverage traversal of recorded data. The navigation agent learns an effective policy over a diverse action space in a large heterogeneous environment consisting of more than 2km of travel, through buildings and outdoor regions that collectively exhibit large variations in visual appearance, self-similarity, and connectivity. We compare pretrained visual encoders that enable precomputation of visual embeddings to achieve a throughput of tens of thousands of transitions per second at training time on a commodity desktop computer, allowing agents to learn from millions of trajectories of experience in a matter of hours. We propose multi- ple forms of computationally efficient stochastic augmentation to enable the learned policy to generalise beyond these precomputed embeddings, and demonstrate successful deployment of the learned policy on the real robot without fine tuning, despite environmental appearance differences at test time. The dataset and code required to reproduce these results and apply the technique to other datasets and robots is made publicly available at rl-navigation.github.io/deployable ",excluded
162,http://arxiv.org/abs/2105.05873v1,arxiv,arxiv,http://arxiv.org/abs/2105.05873v1,multimedia,12/05/2021 00:00,not defined,not defined,not defined,not defined,out of the box: embodied navigation in the real world,"The research field of Embodied AI has witnessed substantial progress in
visual navigation and exploration thanks to powerful simulating platforms and
the availability of 3D data of indoor and photorealistic environments. These
two factors have opened the doors to a new generation of intelligent agents
capable of achieving nearly perfect PointGoal Navigation. However, such
architectures are commonly trained with millions, if not billions, of frames
and tested in simulation. Together with great enthusiasm, these results yield a
question: how many researchers will effectively benefit from these advances? In
this work, we detail how to transfer the knowledge acquired in simulation into
the real world. To that end, we describe the architectural discrepancies that
damage the Sim2Real adaptation ability of models trained on the Habitat
simulator and propose a novel solution tailored towards the deployment in
real-world scenarios. We then deploy our models on a LoCoBot, a Low-Cost Robot
equipped with a single Intel RealSense camera. Different from previous work,
our testing scene is unavailable to the agent in simulation. The environment is
also inaccessible to the agent beforehand, so it cannot count on scene-specific
semantic priors. In this way, we reproduce a setting in which a research group
(potentially from other fields) needs to employ the agent visual navigation
capabilities as-a-Service. Our experiments indicate that it is possible to
achieve satisfying results when deploying the obtained model in the real world.
Our code and models are available at https://github.com/aimagelab/LoCoNav.",experiments
163,10.1109/tase.2014.2377791,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7015601/,robotics,01/04/2015 00:00,not defined,not defined,not defined,not defined,roboearth semantic mapping: a cloud enabled knowledge-based approach,"The vision of the RoboEarth project is to design a knowledge-based system to provide web and cloud services that can transform a simple robot into an intelligent one. In this work, we describe the RoboEarth semantic mapping system. The semantic map is composed of: 1) an ontology to code the concepts and relations in maps and objects and 2) a SLAM map providing the scene geometry and the object locations with respect to the robot. We propose to ground the terminological knowledge in the robot perceptions by means of the SLAM map of objects. RoboEarth boosts mapping by providing: 1) a subdatabase of object models relevant for the task at hand, obtained by semantic reasoning, which improves recognition by reducing computation and the false positive rate; 2) the sharing of semantic maps between robots; and 3) software as a service to externalize in the cloud the more intensive mapping computations, while meeting the mandatory hard real time constraints of the robot. To demonstrate the RoboEarth cloud mapping system, we investigate two action recipes that embody semantic map building in a simple mobile robot. The first recipe enables semantic map building for a novel environment while exploiting available prior information about the environment. The second recipe searches for a novel object, with the efficiency boosted thanks to the reasoning on a semantically annotated map. Our experimental results demonstrate that, by using RoboEarth cloud services, a simple robot can reliably and efficiently build the semantic maps needed to perform its quotidian tasks. In addition, we show the synergetic relation of the SLAM map of objects that grounds the terminological knowledge coded in the ontology.",architecture
164,http://arxiv.org/abs/1801.05086v1,arxiv,arxiv,http://arxiv.org/abs/1801.05086v1,autonomous vehicle,16/01/2018 00:00,not defined,not defined,not defined,not defined,autonomous uav navigation using reinforcement learning,"Unmanned aerial vehicles (UAV) are commonly used for missions in unknown
environments, where an exact mathematical model of the environment may not be
available. This paper provides a framework for using reinforcement learning to
allow the UAV to navigate successfully in such environments. We conducted our
simulation and real implementation to show how the UAVs can successfully learn
to navigate through an unknown environment. Technical aspects regarding to
applying reinforcement learning algorithm to a UAV system and UAV flight
control were also addressed. This will enable continuing research using a UAV
with learning capabilities in more important applications, such as wildfire
monitoring, or search and rescue missions.",experiments
165,http://arxiv.org/abs/1411.6326v1,arxiv,arxiv,http://arxiv.org/abs/1411.6326v1,autonomous vehicle,24/11/2014 00:00,not defined,not defined,not defined,not defined,vision and learning for deliberative monocular cluttered flight,"Cameras provide a rich source of information while being passive, cheap and
lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work
we present the first implementation of receding horizon control, which is
widely used in ground vehicles, with monocular vision as the only sensing mode
for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a
number of contributions: novel coupling of perception and control via relevant
and diverse, multiple interpretations of the scene around the robot, leveraging
recent advances in machine learning to showcase anytime budgeted cost-sensitive
feature selection, and fast non-linear regression for monocular depth
prediction. We empirically demonstrate the efficacy of our novel pipeline via
real world experiments of more than 2 kms through dense trees with a quadrotor
built from off-the-shelf parts. Moreover our pipeline is designed to combine
information from other modalities like stereo and lidar as well if available.",experiments
166,http://arxiv.org/abs/2007.07122v2,arxiv,arxiv,http://arxiv.org/abs/2007.07122v2,smart cities,14/07/2020 00:00,not defined,not defined,not defined,not defined,"energy-efficient resource management for federated edge learning with
  cpu-gpu heterogeneous computing","Edge machine learning involves the deployment of learning algorithms at the
network edge to leverage massive distributed data and computation resources to
train artificial intelligence (AI) models. Among others, the framework of
federated edge learning (FEEL) is popular for its data-privacy preservation.
FEEL coordinates global model training at an edge server and local model
training at edge devices that are connected by wireless links. This work
contributes to the energy-efficient implementation of FEEL in wireless networks
by designing joint computation-and-communication resource management
($\text{C}^2$RM). The design targets the state-of-the-art heterogeneous mobile
architecture where parallel computing using both a CPU and a GPU, called
heterogeneous computing, can significantly improve both the performance and
energy efficiency. To minimize the sum energy consumption of devices, we
propose a novel $\text{C}^2$RM framework featuring multi-dimensional control
including bandwidth allocation, CPU-GPU workload partitioning and speed scaling
at each device, and $\text{C}^2$ time division for each link. The key component
of the framework is a set of equilibriums in energy rates with respect to
different control variables that are proved to exist among devices or between
processing units at each device. The results are applied to designing efficient
algorithms for computing the optimal $\text{C}^2$RM policies faster than the
standard optimization tools. Based on the equilibriums, we further design
energy-efficient schemes for device scheduling and greedy spectrum sharing that
scavenges ""spectrum holes"" resulting from heterogeneous $\text{C}^2$ time
divisions among devices. Using a real dataset, experiments are conducted to
demonstrate the effectiveness of $\text{C}^2$RM on improving the energy
efficiency of a FEEL system.",excluded
167,10.1109/aiiot52608.2021.9454183,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9454183/,robotics,13/05/2021 00:00,not defined,not defined,not defined,not defined,image classification with knowledge-based systems on the edge for real-time danger avoidance in robots,"Mobile robots are increasingly common in society and are increasingly being used for complex and high-stakes tasks such as search and rescue. The growing requirements for these robots demonstrate a need for systems which can review and react in real time to environmental hazards, which will allow robots to handle environments that are both dynamic and dangerous. We propose and test a system which allows mobile robots to reclassify environmental objects during operation in conjunction with an edge system. We train an image classification model with 99 percent accuracy and deploy it in conjunction with an edge server and JSON-based ruleset to allow robots to react to and avoid hazards.",architecture
168,10.1109/iros40897.2019.8967592,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8967592/,multimedia,08/11/2019 00:00,not defined,not defined,not defined,not defined,can a robot become a movie director? learning artistic principles for aerial cinematography,"Aerial filming is constantly gaining importance due to the recent advances in drone technology. It invites many intriguing, unsolved problems at the intersection of aesthetical and scientific challenges. In this work, we propose a deep reinforcement learning agent which supervises motion planning of a filming drone by making desirable shot mode selections based on aesthetical values of video shots. Unlike most of the current state-of-the-art approaches that require explicit guidance by a human expert, our drone learns how to make favorable viewpoint selections by experience. We propose a learning scheme that exploits aesthetical features of retrospective shots in order to extract a desirable policy for better prospective shots. We train our agent in realistic AirSim simulations using both a hand-crafted reward function as well as reward from direct human input. We then deploy the same agent on a real DJI M210 drone in order to test the generalization capability of our approach to real world conditions. To evaluate the success of our approach in the end, we conduct a comprehensive user study in which participants rate the shot quality of our methods. Videos of the system in action can be seen at https://youtu.be/qmVw6mfyEmw.",experiments
169,10.1109/icitr51448.2020.9310890,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9310890/,science,04/12/2020 00:00,not defined,not defined,not defined,not defined,hybrid approach and architecture to detect fake news on twitter in real-time using neural networks,"Fake news has been a key issue since the dawn of social media. Currently, we are at a stage where it is merely impossible to differentiate between real and fake news. This directly and indirectly affects people's decision patterns and makes us question the credibility of the news shared via social media platforms. Twitter is one of the leading social networks in the world by active users. There has been an exponential spread of fake news on Twitter in the recent past. In this paper, we will discuss the implementation of a browser extension which will identify fake news on Twitter using deep learning models with a focus on real-world applicability, architectural stability and scalability of such a solution. Experimental results show that the proposed browser extension has an accuracy of 86% accuracy in fake news detection. To the best of our knowledge, our work is the first of its kind to detect fake news on Twitter real-time using a hybrid approach and evaluate using real users.",architecture
170,10.1109/tsmc.2020.2967936,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8989970/,robotics,01/12/2021 00:00,not defined,not defined,not defined,not defined,deep q-learning with q-matrix transfer learning for novel fire evacuation environment,"Deep reinforcement learning (RL) is achieving significant success in various applications like control, robotics, games, resource management, and scheduling. However, the important problem of emergency evacuation, which clearly could benefit from RL, has been largely unaddressed. Indeed, emergency evacuation is a complex task that is difficult to solve with RL. An emergency situation is highly dynamic, with a lot of changing variables and complex constraints that make it challenging to solve. Also, there is no standard benchmark environment available that can be used to train RL agents for evacuation. A realistic environment can be complex to design. In this article, we propose the first fire evacuation environment to train RL agents for evacuation planning. The environment is modeled as a graph capturing the building structure. It consists of realistic features like fire spread, uncertainty, and bottlenecks. The implementation of our environment is in the OpenAI gym format, to facilitate future research. We also propose a new RL approach that entails pretraining the network weights of a DQN-based agent [DQN/Double-DQN (DDQN)/Dueling-DQN] to incorporate information on the shortest path to the exit. We achieved this by using tabular <inline-formula> <tex-math notation=""LaTeX"">$Q$ </tex-math></inline-formula>-learning to learn the shortest path on the building model’s graph. This information is transferred to the network by deliberately overfitting it on the <inline-formula> <tex-math notation=""LaTeX"">$Q$ </tex-math></inline-formula>-matrix. Then, the pretrained DQN model is trained on the fire evacuation environment to generate the optimal evacuation path under time varying conditions due to fire spread, bottlenecks, and uncertainty. We perform comparisons of the proposed approach with state-of-the-art RL algorithms like DQN, DDQN, Dueling-DQN, PPO, VPG, state-action-reward-state-action (SARSA), actor–critic method, and ACKTR. The results show that our method is able to outperform state-of-the-art models by a huge margin including the original DQN-based models. Finally, our model is tested on a large and complex real building consisting of 91 rooms, with the possibility to move to any other room, hence giving 8281 actions. In order to reduce the action space, we propose a strategy that involves one step simulation. That is, an action importance vector is added to the final output of the pretrained DQN and acts like an attention mechanism. Using this strategy, the action space is reduced by 90.1%. In this manner, the model is able to deal with large action spaces. Hence, our model achieves near optimal performance on the real world emergency environment.",experiments
171,10.1109/wf-iot.2019.8767231,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8767231/,health,18/04/2019 00:00,not defined,not defined,not defined,not defined,efficient deployment of predictive analytics in edge gateways: fall detection scenario,"Ambient Assisted Living (AAL) represents the most promising Internet of Things (IoT) application due to its relevance in the elders healthcare and improvement of their quality of life. Recently, the AAL IoT ecosystem has been enriched with promising technologies such as edge computing, which has demonstrated to be the best approach to overcome the demanding requirements of AAL and healthcare services by providing a reduction of the amount of data to transfer to the cloud, an improvement of the response time, and quality of experience. Also, the deployment of Artificial Intelligence (AI) technologies at the edge provides intelligence to improve the decision making timely. However, this approach has been scarcely studied in AAL scenarios and the few proposals based on deploying machine learning models at the edge lack efficiency, security, mechanisms of resource management, service management, and deployment, as well as a real and experimental AAL scenario. For these reasons, this paper proposes an innovative edge gateway architecture to support the deployment of deep learning (DL) models in AAL and healthcare scenarios efficiently. To do so, we have added a predictive analytics module to deploy the models. Since AI technologies demand more resources, a container-based virtualization technology is employed on the edge gateway to manage the limited resources, and provide security and lifecycle services management. The edge gateway performance was evaluated deploying a DL-based fall detection application on it. As a result, our approach improves the inference time compared to that based on the cloud in 34 seconds and to similar approaches in 8 seconds.",architecture
172,10.3390/s20174836,Multidisciplinary Digital Publishing Institute,project-academic,project-academic,robotics,27/08/2020 00:00,not defined,not defined,not defined,not defined,distributed non communicating multi robot collision avoidance via map based deep reinforcement learning," It is challenging to avoid obstacles safely and efficiently for multiple robots of different shapes in distributed and communication-free scenarios, where robots do not communicate with each other and only sense other robots’ positions and obstacles around them. Most existing multi-robot collision avoidance systems either require communication between robots or require expensive movement data of other robots, like velocities, accelerations and paths. In this paper, we propose a map-based deep reinforcement learning approach for multi-robot collision avoidance in a distributed and communication-free environment. We use the egocentric local grid map of a robot to represent the environmental information around it including its shape and observable appearances of other robots and obstacles, which can be easily generated by using multiple sensors or sensor fusion. Then we apply the distributed proximal policy optimization (DPPO) algorithm to train a convolutional neural network that directly maps three frames of egocentric local grid maps and the robot’s relative local goal positions into low-level robot control commands. Compared to other methods, the map-based approach is more robust to noisy sensor data, does not require robots’ movement data and considers sizes and shapes of related robots, which make it to be more efficient and easier to be deployed to real robots. We first train the neural network in a specified simulator of multiple mobile robots using DPPO, where a multi-stage curriculum learning strategy for multiple scenarios is used to improve the performance. Then we deploy the trained model to real robots to perform collision avoidance in their navigation without tedious parameter tuning. We evaluate the approach with multiple scenarios both in the simulator and on four differential-drive mobile robots in the real world. Both qualitative and quantitative experiments show that our approach is efficient and outperforms existing DRL-based approaches in many indicators. We also conduct ablation studies showing the positive effects of using egocentric grid maps and multi-stage curriculum learning.",experiments
173,,,core,,robotics,01/01/2005 00:00,not defined,not defined,not defined,not defined,"an architecture for behaviorbased reinforcement learning,” adaptive behavior","This paper introduces an integration of reinforcement learning and behavior-based control designed to produce real-time learning in situated agents. The model layers a distributed and asynchronous reinforcement learning algorithm over a learned topological map and standard behavioral substrate to create a reinforcement learning complex. The topological map creates a small and task-relevant state space that aims to make learning feasible, while the distributed and asynchronous nature of the model make it compatible with behavior-based design principles. We present the design, implementation and results of an experiment that requires a mobile robot to perform puck foraging in three artificial arenas using the new model, a random decision making model, and a layered standard reinforcement learning model. The results show that our model is able to learn rapidly on a real robot in a real environment, learning and adapting to change more quickly than both alternative models. We show that the robot is able to make the best choices it can given its drives and experiences using only local decisions and therefore displays planning behavior without the use of classical planning techniques. ",excluded
174,10.1109/rose52553.2021.00011,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9474550/,robotics,02/06/2021 00:00,not defined,not defined,not defined,not defined,a modeling tool for reconfigurable skills in ros,"Known attempts to build autonomous robots rely on complex control architectures, often implemented with the Robot Operating System platform (ROS). The implementation of adaptable architectures is very often ad hoc, quickly gets cumbersome and expensive. Reusable solutions that support complex, runtime reasoning for robot adaptation have been seen in the adoption of ontologies. While the usage of ontologies significantly increases system reuse and maintainability, it requires additional effort from the application developers to translate requirements into formal rules that can be used by an ontological reasoner. In this paper, we present a design tool that facilitates the specification of reconfigurable robot skills. Based on the specified skills, we generate corresponding runtime models for self-adaptation that can be directly deployed to a running robot that uses a reasoning approach based on ontologies. We demonstrate the applicability of the tool in a real robot performing a patrolling mission at a university campus.",architecture
175,http://arxiv.org/abs/2103.08022v1,arxiv,arxiv,http://arxiv.org/abs/2103.08022v1,smart cities,14/03/2021 00:00,not defined,not defined,not defined,not defined,"success weighted by completion time: a dynamics-aware evaluation
  criteria for embodied navigation","We present Success weighted by Completion Time (SCT), a new metric for
evaluating navigation performance for mobile robots. Several related works on
navigation have used Success weighted by Path Length (SPL) as the primary
method of evaluating the path an agent makes to a goal location, but SPL is
limited in its ability to properly evaluate agents with complex dynamics. In
contrast, SCT explicitly takes the agent's dynamics model into consideration,
and aims to accurately capture how well the agent has approximated the fastest
navigation behavior afforded by its dynamics. While several embodied navigation
works use point-turn dynamics, we focus on unicycle-cart dynamics for our
agent, which better exemplifies the dynamics model of popular mobile robotics
platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present
RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest
collision-free path and completion time from a starting pose to a goal location
in an environment containing obstacles. We experiment with deep reinforcement
learning and reward shaping to train and compare the navigation performance of
agents with different dynamics models. In evaluating these agents, we show that
in contrast to SPL, SCT is able to capture the advantages in navigation speed a
unicycle model has over a simpler point-turn model of dynamics. Lastly, we show
that we can successfully deploy our trained models and algorithms outside of
simulation in the real world. We embody our agents in an real robot to navigate
an apartment, and show that they can generalize in a zero-shot manner.",experiments
176,10.1109/aivr.2018.00018,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8613637/,multimedia,12/12/2018 00:00,not defined,not defined,not defined,not defined,a compensation method of two-stage image generation for human-ai collaborated in-situ fashion design in augmented reality environment,"In this paper, we consider a human-AI collaboration task, fashion design, in augmented reality environment. In particular, we propose a compensation method of two-stage image generation neural network for generating fashion design with progressive users' inputs. Our work is based on a recent proposed deep learning model, pix2pix, that can successfully transform an image from one domain into another domain, such as from line drawings to color images. However, the pix2pix model relies on the condition that input images should come from the same distribution, which is usually hard for applying it to real human computer interaction tasks, where the input from users differs from individual to individual. To address the problem, we propose a compensation method of two-stage image generation. In the first stage, we ask users to indicate their design preference with an easy task, such as tuning clothing landmarks, and use the input to generate a compensation input. With the compensation input, in the second stage, we then concatenate it with the real sketch from users to generate a perceptual better result. In addition, to deploy the two-stage image generation neural network in augmented reality environment, we designed and implemented a mobile application where users can create fashion design referring to real world human models. With the augmented 2D screen and instant feedback from our system, users can design clothing by seamlessly mixing the real and virtual environment. Through an online experiment with 46 participants and an offline use case study, we showcase the capability and usability of our system. Finally, we discuss the limitations of our system and further works on human-AI collaborated design.",experiments
177,,Software and System Health Management for Autonomous Robotics Missions,core,,health,04/09/2012 00:00,not defined,not defined,not defined,not defined,10.1184/r1/6710654.v1,"Advanced autonomous robotics space missions rely heavily on the flawless interaction of complex hardware, multiple sensors, and a mission-critical software system.  This software system consists of an operating system, device drivers, controllers, and executives; recently highly complex AI-based autonomy software have also been introduced. Prior to launch, this software has to undergo rigorous verification and validation (V&V).  Nevertheless, dormant software bugs, failing sensors, unexpected hardware-software interactions, and unanticipated environmental conditions—likely on a space exploration mission—can cause major software faults that can endanger the entire mission.

Our Integrated Software Health Management (ISWHM) system continuously monitors the hardware sensors and the software in real-time. The ISWHM uses Bayesian networks, compiled to arithmetic circuits, to model software and hardware interactions. Advanced reasoning algorithms using arithmetic circuits not only enable the ISWHM to handle large, hierarchical models that are necessary in the realm of complex autonomous systems, but also enable efficient execution on small embedded processors. The latter capability is of extreme importance for small (mobile) autonomous units with limited computational power and low telemetry bandwidth.  In this paper, we discuss the requirements of ISWHM.  As our initial demonstration platform, we use a primitive Lego rover. A Lego 
Mindstorms microcontroller is used to implement a highly simplified autonomous rover driving system, running on the OSEK real-time operating system. We demonstrate that our ISWHM, running on this small embedded microcontroller, can perform fault detection as well as on-board reasoning for advanced diagnosis and root-cause detection in real time",architecture
178,,,project-academic,project-academic,autonomous vehicle,06/03/2020 00:00,not defined,not defined,not defined,not defined,practical reinforcement learning for mpc learning from sparse objectives in under an hour on a real robot," Model Predictive Control (MPC) is a powerful control technique that handles constraints, takes the system's dynamics into account, and optimizes for a given cost function. In practice, however, it often requires an expert to craft and tune this cost function and find trade-offs between different state penalties to satisfy simple high level objectives. In this paper, we use Reinforcement Learning and in particular value learning to approximate the value function given only high level objectives, which can be sparse and binary. Building upon previous works, we present improvements that allowed us to successfully deploy the method on a real world unmanned ground vehicle. Our experiments show that our method can learn the cost function from scratch and without human intervention, while reaching a performance level similar to that of an expert-tuned MPC. We perform a quantitative comparison of these methods with standard MPC approaches both in simulation and on the real robot.",excluded
179,10.3389/fpubh.2019.00081,Frontiers Media SA,project-academic,project-academic,health,12/04/2019 00:00,not defined,not defined,not defined,not defined,crowdbreaks tracking health trends using public social media data and crowdsourcing," In the past decade, tracking health trends using social media data has shown great promise, due to a powerful combination of massive adoption of social media around the world, and increasingly potent hardware and software that enables us to work with these new big data streams. At the same time, many challenging problems have been identified. First, there is often a mismatch between how rapidly online data can change, and how rapidly algorithms are updated, which means that there is limited reusability for algorithms trained on past data as their performance decreases over time. Second, much of the work is focusing on specific issues during a specific past period in time, even though public health institutions would need flexible tools to assess multiple evolving situations in real time. Third, most tools providing such capabilities are proprietary systems with little algorithmic or data transparency, and thus little buy-in from the global public health and research community. Here, we introduce Crowdbreaks, an open platform which allows tracking of health trends by making use of continuous crowdsourced labelling of public social media content. The system is built in a way which automatizes the typical workflow from data collection, filtering, labelling and training of machine learning classifiers and therefore can greatly accelerate the research process in the public health domain. This work describes the technical aspects of the platform, thereby covering the functionalities at its current state and exploring its future use cases and extensions.",architecture
180,10.1016/j.iot.2020.100185,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85086362688,health,01/09/2020,not defined,not defined,not defined,not defined,highly-efficient fog-based deep learning aal fall detection system,"Falls is one of most concerning accidents in aged population due to its high frequency and serious repercussion; thus, quick assistance is critical to avoid serious health consequences. There are several Ambient Assisted Living (AAL) solutions that rely on the technologies of the Internet of Things (IoT), Cloud Computing and Machine Learning (ML). Recently, Deep Learning (DL) have been included for its high potential to improve accuracy on fall detection. Also, the use of fog devices for the ML inference (detecting falls) spares cloud drawback of high network latency, non-appropriate for delay-sensitive applications such as fall detectors. Though, current fall detection systems lack DL inference on the fog, and there is no evidence of it in real environments, nor documentation regarding the complex challenge of the deployment. Since DL requires considerable resources and fog nodes are resource-limited, a very efficient deployment and resource usage is critical. We present an innovative highly-efficient intelligent system based on a fog-cloud computing architecture to timely detect falls using DL technics deployed on resource-constrained devices (fog nodes). We employ a wearable tri-axial accelerometer to collect patient monitoring data. In the fog, we propose a smart-IoT-Gateway architecture to support the remote deployment and management of DL models. We deploy two DL models (LSTM/GRU) employing virtualization to optimize resources and evaluate their performance and inference time. The results prove the effectiveness of our fall system, that provides a more timely and accurate response than traditional fall detector systems, higher efficiency, 98.75% accuracy, lower delay, and service improvement.",architecture
181,http://arxiv.org/abs/1908.04172v2,arxiv,arxiv,http://arxiv.org/abs/1908.04172v2,science,12/08/2019 00:00,not defined,not defined,not defined,not defined,"ngraph-he2: a high-throughput framework for neural network inference on
  encrypted data","In previous work, Boemer et al. introduced nGraph-HE, an extension to the
Intel nGraph deep learning (DL) compiler, that enables data scientists to
deploy models with popular frameworks such as TensorFlow and PyTorch with
minimal code changes. However, the class of supported models was limited to
relatively shallow networks with polynomial activations. Here, we introduce
nGraph-HE2, which extends nGraph-HE to enable privacy-preserving inference on
standard, pre-trained models using their native activation functions and number
fields (typically real numbers). The proposed framework leverages the CKKS
scheme, whose support for real numbers is friendly to data science, and a
client-aided model using a two-party approach to compute activation functions.
  We first present CKKS-specific optimizations, enabling a 3x-88x runtime
speedup for scalar encoding, and doubling the throughput through a novel use of
CKKS plaintext packing into complex numbers. Second, we optimize
ciphertext-plaintext addition and multiplication, yielding 2.6x-4.2x runtime
speedup. Third, we exploit two graph-level optimizations: lazy rescaling and
depth-aware encoding, which allow us to significantly improve performance.
  Together, these optimizations enable state-of-the-art throughput of 1,998
images/s on the CryptoNets network. Using the client-aided model, we also
present homomorphic evaluation of (to our knowledge) the largest network to
date, namely, pre-trained MobileNetV2 models on the ImageNet dataset, with
60.4\percent/82.7\percent\ top-1/top-5 accuracy and an amortized runtime of 381
ms/image.",excluded
182,,,core,,robotics,02/06/2008 00:00,not defined,not defined,not defined,not defined,francisco cervantes,"As autonomous robots become more complex in their behavior, more sophisticated software architectures are required to support the ever more sophisticated robotics software. These software architectures must support complex behaviors involving adaptation and learning, implemented, in particular, by neural networks. We present in this paper a neural based schema [2] software architecture for the development and execution of autonomous robots in both simulated and real worlds. This architecture has been developed in the context of adaptive robotic agents, ecological robots [6], cooperating and competing with each other in adapting to their environment. The architecture is the result of integrating a number of development an",excluded
183,http://arxiv.org/abs/1910.05765v1,arxiv,arxiv,http://arxiv.org/abs/1910.05765v1,smart cities,13/10/2019 00:00,not defined,not defined,not defined,not defined,"real-time and embedded deep learning on fpga for rf signal
  classification","We designed and implemented a deep learning based RF signal classifier on the
Field Programmable Gate Array (FPGA) of an embedded software-defined radio
platform, DeepRadio, that classifies the signals received through the RF front
end to different modulation types in real time and with low power. This
classifier implementation successfully captures complex characteristics of
wireless signals to serve critical applications in wireless security and
communications systems such as identifying spoofing signals in signal
authentication systems, detecting target emitters and jammers in electronic
warfare (EW) applications, discriminating primary and secondary users in
cognitive radio networks, interference hunting, and adaptive modulation.
Empowered by low-power and low-latency embedded computing, the deep neural
network runs directly on the FPGA fabric of DeepRadio, while maintaining
classifier accuracy close to the software performance. We evaluated the
performance when another SDR (USRP) transmits signals with different modulation
types at different power levels and DeepRadio receives the signals and
classifies them in real time on its FPGA. A smartphone with a mobile app is
connected to DeepRadio to initiate the experiment and visualize the
classification results. With real radio transmissions over the air, we show
that the classifier implemented on DeepRadio achieves high accuracy with low
latency (microsecond per sample) and low energy consumption (microJoule per
sample), and this performance is not matched by other embedded platforms such
as embedded graphics processing unit (GPU).",experiments
184,,,core,,robotics,01/01/1992 00:00,not defined,not defined,not defined,not defined,artificial intelligence for monitoring and diagnosis of robotic spacecraft,"In this thesis the application of artificial intelligence to monitoring and diagnosis of robotic spacecraft is discussed. Several software prototype systems were developed to serve as testbeds for the research and to evaluate the effectiveness of the approach against real problems and current techniques used in NASA\u27s planetary exploration program.  Software prototypes were used to investigate the verification of robot plan execution. New artificial intelligence algorithms for monitoring and diagnosis of robot systems were designed, programmed, and tested. These included plan analysis for monitoring, sensor planning, generation of expected sensor values, and diagnosis of execution failures caused by hardware, environmental or plan anomalies. Testing was performed on a laboratory telerobotic hardware testbed for satellite servicing and on a mobile planetary rover robot operating in natural terrain.  Artificial intelligence algorithms, software prototypes, and more advanced, operationally capable systems for monitoring ground support systems and actual spacecraft in flight were designed, programmed, and tested. A ground support system that served as one test domain was the mirror cooling circuit of the 25-foot Space Simulator at the Jet Propulsion Laboratory (JPL) in Pasadena, California. A prototype monitoring system for this device based on a theory of ""predictive monitoring"" was developed and tested. Mission operations for the Voyager II spacecraft served as another test domain for an intelligent spacecraft health-monitoring and diagnosis system. This system was successfully tested in support of telecommunications operations during Voyager II\u27s encounter with the planet Neptune in 1989. This was the one of the first artificial intelligence systems to be used in planetary spacecraft operations at NASA/JPL. Subsequently, this system was adapted and tested in support of operations of the Magellan spacecraft telecommunications subsystem and the Galileo spacecraft power and pyro subsystem.  Some of the specific artificial intelligence algorithms that were developed for monitoring and diagnosis included the use of heuristic and causal model-based reasoning techniques for predictive generation of sensor values, sensor selection planning, dynamic alarm limit checking, hierarchical procedure specialists for fault diagnosis, and integration of Al with conventional systems in full-scale monitoring and diagnosis applications.  In support of this overall program of research, novel software engineering tools for artificial intelligence research and application development were also developed and will be discussed in the thesis.  The application of artificial intelligence techniques to the monitoring and diagnosis of robotic space systems was shown to be very effective with specific benefits in the areas of systems autonomy, spacecraft safety, ground operations productivity and automation. As a result of this work in part, artificial intelligence is now considered by senior mission designers to be an enabling technology for on-board automation of planetary rovers and for automation in mission operations at the Jet Propulsion Laboratory",excluded
185,georgia institute of technology,,core,https://core.ac.uk/download/304994832.pdf,robotics,22/05/2019 00:00,not defined,not defined,not defined,not defined,balancing generality and specialization for machine learning in the post-isa era,"A growing number of commercial and enterprise systems are increasingly relying on compute-intensive machine learning algorithms. While the demand for these apaplications is growing, the performance benefits from general-purpose platforms is diminishing. This challenge has coincided with the explosion of data where the rate of data generation has reached an overwhelming level that is beyond the capabilities of current computing systems. Therefore, applications such as machine learning and robotics can benefit from hardware acceleration. Traditionally, to accelerate a set of workloads, we pro- file the code optimized for CPUs and offload the hot functions on hardware compute units designed specially for that particular function, hence providing higher performance and energy efficiency. Instead in this work, we take a revolutionary approach where we delve into the algorithmic properties of applications to define domain-generic hardware acceleration solutions. We leverage the property that a wide range of machine learning algorithms can be modeled as stochastic optimization problems. Using this insight we devise compute stacks for hardware acceleration that are built independent of the CPU. These stacks expose a high-level mathematical programming interface and automatically generate accelerators for users who have limited knowledge about hardware design, but can benefit from large performance and efficiency gains for their programs. 
Keeping these ambitious goals in mind, our work (1) strikes a balance between generality and specialization by breaking the long-held traditional abstraction of the Instruction Set Architecture (ISA) in favor of a more algorithm-centric approach; (2) develops hard- ware acceleration frameworks by co-designing a language, compiler, runtime system, and hardware to provide high performance and efficiency, in addition to flexibility and programmability; (3) segregates algorithmic specification from implementation to shield the programmer from continual hardware/software modifications while allowing them to benefit from the emerging heterogeneity of modern compute platforms; and (4) develops real cross-stack prototypes to evaluate these innovative solutions in a real-world setting and make them open-source to maximize community engagement and industry impact. Our work TABLA (http://act-lab.org/artifacts/tabla/) is public, and defines the very first open-source hardware platform for machine learning and artificial intelligence.Ph.D",excluded
186,,,project-academic,project-academic,autonomous vehicle,28/09/2021 00:00,not defined,not defined,not defined,not defined,safetynet safe planning for real world self driving vehicles using machine learned policies," In this paper we present the first safe system for full control of self-driving vehicles trained from human demonstrations and deployed in challenging, real-world, urban environments. Current industry-standard solutions use rule-based systems for planning. Although they perform reasonably well in common scenarios, the engineering complexity renders this approach incompatible with human-level performance. On the other hand, the performance of machine-learned (ML) planning solutions can be improved by simply adding more exemplar data. However, ML methods cannot offer safety guarantees and sometimes behave unpredictably. To combat this, our approach uses a simple yet effective rule-based fallback layer that performs sanity checks on an ML planner's decisions (e.g. avoiding collision, assuring physical feasibility). This allows us to leverage ML to handle complex situations while still assuring the safety, reducing ML planner-only collisions by 95%. We train our ML planner on 300 hours of expert driving demonstrations using imitation learning and deploy it along with the fallback layer in downtown San Francisco, where it takes complete control of a real vehicle and navigates a wide variety of challenging urban driving scenarios.",experiments
187,10.1109/icra40945.2020.9197465,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9197465/,multimedia,31/08/2020 00:00,not defined,not defined,dimensionality reduction,not defined,deepracer: autonomous racing platform for experimentation with sim2real reinforcement learning,"DeepRacer is a platform for end-to-end experimentation with RL and can be used to systematically investigate the key challenges in developing intelligent control systems. Using the platform, we demonstrate how a 1/18th scale car can learn to drive autonomously using RL with a monocular camera. It is trained in simulation with no additional tuning in the physical world and demonstrates: 1) formulation and solution of a robust reinforcement learning algorithm, 2) narrowing the reality gap through joint perception and dynamics, 3) distributed on-demand compute architecture for training optimal policies, and 4) a robust evaluation method to identify when to stop training. It is the first successful large-scale deployment of deep reinforcement learning on a robotic control agent that uses only raw camera images as observations and a model-free learning method to perform robust path planning. We open source our code and video demo on GitHub<sup>2</sup>.",experiments
188,10.1109/icccn52240.2021.9522281,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9522281/,industry,22/07/2021 00:00,not defined,not defined,not defined,not defined,realization of an intrusion detection use-case in onap with acumos,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms.",architecture
189,10.1109/bds/hpsc/ids18.2018.00045,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8552303/,health,05/05/2018 00:00,not defined,not defined,not defined,not defined,real-time intelligent air quality evaluation on a resource-constrained embedded platform,"Indoor air quality has a major impact on health and comfort of building occupants. Poor air quality may reduce productivity in offices and impair students learning in classes. In order to provide localized air pollution data and tailor it for individual, wearable air quality sensor is a promising solution. Furthermore, crowd sensing has emerged as an Internet-of-Things (IoT) solution, which is economical, scalable and easy to deploy and re-deploy as it uses the power of crowd data collection. The goal of our proposed system is to monitor indoor air quality through a crowd sensing system that will use a set of sensors to measure air quality, monitor the concentration of pollutants continuously, and make recommendations in real time for improved air quality. In this paper artificial network is developed to perform real-time indoor air quality control. Utilizing created neural network embedded into a smart controller comfort level of air quality parameters such as temperature, CO_2 air concentration and humidity could be estimated after every measurement and used for adapting air conditioning systems to adjust air quality. Neural network data preparation and training process are discussed along with deployment of trained network on a smart controller.",excluded
190,10.1109/hpca51647.2021.00016,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9407116/,multimedia,03/03/2021 00:00,not defined,not defined,dimensionality reduction,not defined,heterogeneous dataflow accelerators for multi-dnn workloads,"Emerging AI-enabled applications such as augmented and virtual reality (AR/VR) leverage multiple deep neural network (DNN) models for various sub-tasks such as object detection, image segmentation, eye-tracking, speech recognition, and so on. Because of the diversity of the sub-tasks, the layers within and across the DNN models are highly heterogeneous in operation and shape. Diverse layer operations and shapes are major challenges for a fixed dataflow accelerator (FDA) that employs a fixed dataflow strategy on a single DNN accelerator substrate since each layer prefers different dataflows (computation order and parallelization) and tile sizes. Reconfigurable DNN accelerators (RDAs) have been proposed to adapt their dataflows to diverse layers to address the challenge. However, the dataflow flexibility in RDAs is enabled at the cost of expensive hardware structures (switches, interconnects, controller, etc.) and requires per-layer reconfiguration, which introduces considerable energy costs. Alternatively, this work proposes a new class of accelerators, heterogeneous dataflow accelerators (HDAs), which deploy multiple accelerator substrates (i.e., sub-accelerators), each supporting a different dataflow. HDAs enable coarser-grained dataflow flexibility than RDAs with higher energy efficiency and lower area cost comparable to FDAs. To exploit such benefits, hardware resource partitioning across sub-accelerators and layer execution schedule need to be carefully optimized. Therefore, we also present Herald, a framework for co-optimizing hardware partitioning and layer scheduling. Using Herald on a suite of AR/VR and MLPerf workloads, we identify a promising HDA architecture, Maelstrom, which demonstrates 65.3% lower latency and 5.0% lower energy compared to the best fixed dataflow accelerators and 22.0% lower energy at the cost of 20.7% higher latency compared to a state-of-the-art reconfigurable DNN accelerator (RDA). The results suggest that HDA is an alternative class of Pareto-optimal accelerators to RDA with strength in energy, which can be a better choice than RDAs depending on the use cases.",excluded
191,10.1109/dese51703.2020.9450744,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9450744/,multimedia,17/12/2020 00:00,not defined,not defined,not defined,not defined,machine vision intelligent travel aid for the visually impaired (itavi) in developing countries<sup>*</sup>,"The visually impaired have little or no effective visual sensory input and have to rely on external assistance for navigation. Several electronic travel aids have been developed to aid independent navigation of the visually impaired, however they are not without limitations and dependence on third-parties. This paper describes the design and implementation of an Intelligent Travel Aid for the Visually Impaired, it combines the detection and recognition of objects in real-time with audio feedback to provide aid to the visually impaired users. This assistive device uses machine vision for object recognition detection, a camera for capturing object images respectively and a speaker all collectively form the core of the system. The system notifies users of obstacles and objects via synthesized speech. Using a quantized MobileNet based Single Shot multibox object detection model pre-trained on the Common Objects in Context dataset, the device was able to detect objects/obstacles, as well as determine the relative position and approximate distance. The device, when tested, was found to achieve real time performance of up to 70.56 frames per second for detections. Audio feedback was also achieved using the eSpeak Text to Speech engine to provide real time voice instructions to the user. All algorithms were implemented using Python language. The device is user friendly, allowing the visually impaired to enjoy easier navigation. However, other features such as extra object classes as well as language variety could be added in order to boost the robustness of the device.",experiments
192,10.1109/syscon.2018.8369547,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8369547/,industry,26/04/2018 00:00,not defined,not defined,not defined,not defined,an interactive architecture for industrial scale prediction: industry 4.0 adaptation of machine learning,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",architecture
193,10.1109/ijcnn.2013.6706957,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/6706957/,health,09/08/2013 00:00,not defined,not defined,dimensionality reduction,not defined,optimized neuro genetic fast estimator (ongfe) for efficient distributed intelligence instantiation within embedded systems,"The Optimized Neuro Genetic Fast Estimator (ONGFE) is a software tool that allows for embedding system, subsystem, and component failure detection, identification, and prognostics (FDI&amp;P) capability by using Intelligent Software Elements (ISE) based upon Artificial Neural Networks (ANN). With an Application Programming Interface (API), highly innovative algorithms are compiled for efficient distributed intelligence instantiation within embedded systems. The original design had the purpose of providing a real time kernel to deploy health monitoring functions for Condition Based Maintenance (CBM) and Real Time Monitoring (RTM) systems in a broad variety of applications (such as aerospace, structural, and widely distributed support systems). The ONGFE contains embedded fast and on-line training for designing ANNs to perform several high performance FDI&amp;P functions. A key advantage of this technology is an optimization block based upon pseudogenetic algorithms which compensate for effects due to initial weight values and local minimums without the computational burden of genetic algorithms. The ONGFE also provides a synchronization block for communication with secondary diagnostic modules. The algorithms are designed for a distributed, scalar, and modular deployment. Based on this technology, a scheme for conducting sensor data validation has been embedded in Smart Sensors.",experiments
194,10.1109/lra.2017.2665694,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7847341/,robotics,01/04/2017 00:00,not defined,not defined,not defined,not defined,shakey 2016—how much does it take to redo shakey the robot?,"Shakey the robot was one of the first autonomous robots that showed impressive capabilities of navigation and mobile manipulation. Since then, robotics research has made great progress, showing more and more capable robotic systems for a large variety of application domains and tasks. In this letter, we look back on decades of research by rebuilding Shakey with modern robotics technology in the open-source Shakey 2016 system. Hereby, we demonstrate the impact of research by showing that ideas from the original Shakey are still alive in state-of-the-art systems, while robotics in general has improved to deliver more robust and more capable software and hardware. Our Shakey 2016 system has been implemented on real robots and leverages mostly open-source software. We experimentally evaluate the system in real-world scenarios on a PR2 robot and a Turtlebot-based robot and particularly investigate the development effort. The experiments documented in this letter demonstrate that results from robotics research are readily available for building complex robots such as Shakey within a short amount of time and little effort.",experiments
195,,,core,,industry,01/01/2017 00:00,not defined,not defined,not defined,not defined,bayesian modeling for optimization and control in robotics,"Robotics has the potential to be one of the most revolutionary technologies in human history. The impact of cheap and
potentially limitless manpower could have a profound influence on our everyday life and overall onto our society. As
envisioned by Iain M. Banks, Asimov and many other science fictions writers, the effects of robotics on our society might
lead to the disappearance of physical labor and a generalized increase of the quality of life. However, the large-scale
deployment of robots in our society is still far from reality, except perhaps in a few niche markets such as manufacturing.
One reason for this limited deployment of robots is that, despite the tremendous advances in the capabilities of the
robotic hardware, a similar advance on the control software is still lacking. The use of robots in our everyday life is still
hindered by the necessary complexity to manually design and tune the controllers used to execute tasks. As a result,
the deployment of robots often requires lengthy and extensive validations based on human expert knowledge, which
limit their adaptation capabilities and their widespread diffusion. In the future, in order to truly achieve an ubiquitous
robotization of our society, it is necessary to reduce the complexity of deploying new robots in new environments and
tasks.
The goal of this dissertation is to provide automatic tools based on Machine Learning techniques to simplify and
streamline the design of controllers for new tasks. In particular, we here argue that Bayesian modeling is an important tool
for automatically learning models from raw data and properly capture the uncertainty of the such models. Automatically
learning models however requires the definition of appropriate features used as input for the model. Hence, we present
an approach that extend traditional Gaussian process models by jointly learning an appropriate feature representation
and the subsequent model. By doing so, we can strongly guide the features representation to be useful for the subsequent
prediction task.
A first robotics application where the use of Bayesian modeling is beneficial is the accurate learning of complex dynamics models. For highly non-linear robotic systems, such as in presence of contacts, the use of analytical system
identification techniques can be challenging and time-consuming, or even intractable. We introduce a new approach for
learning inverse dynamics models exploiting artificial tactile sensors. This approach allows to recognize and compensate
for the presence of unknown contacts, without requiring a spatial calibration of the tactile sensors. We demonstrate
on the humanoid robot iCub that our approach outperforms state-of-the-art analytical models, and when employed in
control tasks significantly improves the tracking accuracy.
A second robotics application of Bayesian modeling is automatic black-box optimization of the parameters of a controller. When the dynamics of a system cannot be modeled (either out of complexity or due to the lack of a full state
representation), it is still possible to solve a task by adapting an existing controller. The approach used in this thesis is
Bayesian optimization, which allows to automatically optimize the parameters of the controller for a specific task. We
evaluate and compare the performance of Bayesian optimization on a gait optimization task on the dynamic bipedal
walker Fox. Our experiments highlight the benefit of this approach by reducing the parameters tuning time from weeks
to a single day.
In many robotic application, it is however not possible to always define a single straightforward desired objective.
More often, multiple conflicting objectives are desirable at the same time, and thus the designer needs to take a decision
about the desired trade-off between such objectives (e.g., velocity vs. energy consumption). One framework that is
useful to assist in this decision making is the multi-objective optimization framework, and in particular the definition of
Pareto optimality. We propose a novel framework that leverages the use of Bayesian modeling to improve the quality
of traditional multi-objective optimization approaches, even in low-data regimes. By removing the misleading effects
of stochastic noise, the designer is presented with an accurate and continuous Pareto front from which to choose the
desired trade-off. Additionally, our framework allows the seamless introduction of multiple robustness metrics which can
be considered during the design phase. These contributions allow an unprecedented support to the design process of
complex robotic systems in presence of multiple objective, and in particular with regards to robustness.
The overall work in this thesis successfully demonstrates on real robots that the complexity of deploying robots to solve
new tasks can be greatly reduced trough automatic learning techniques. We believe this is a first step towards a future
where robots can be used outside of closely supervised environments, and where a newly deployed robot could quickly
and automatically adapt to accomplish the desired tasks",excluded
196,10.1007/bfb0095437,,core,,robotics,01/01/1998 00:00,not defined,not defined,not defined,not defined,golex - bridging the gap between logic (golog) and a real robot,". The control of mobile robots acting autonomously in the real world is one of the long-term goals of the field of artificial intelligence. So far the field lacks methods bridging the gap between the sophisticated symbolic techniques to represent and reason about action and more and more reliable low-level robot control and navigation systems. In this paper we present GOLEX, an execution and monitoring system for the logic-based action language GOLOG and the complex and distributed RHINO control software which operates on RWI B21 and B14 mobile robots. GOLEX provides the following features: it maps abstract primitive actions into low-level commands of the robot control system, thus allowing the user to concentrate on the application rather than the inner workings of the robot; it monitors the execution of the primitive GOLOG actions, making it possible to detect simple execution failures and timeouts; and it includes means to deal with sensing and user input and to continue the operati..",excluded
197,10.1016/j.micpro.2019.102960,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85077060597,health,01/03/2020,not defined,not defined,not defined,not defined,a novel hybrid optimized and adaptive reconfigurable framework for the implementation of hybrid bio-inspired classifiers for diagnosis,"Due to recent advances in IoT (Internet of Things) technologies, availability of reliable data and emergence of machine learning, bio-inspired learning and artificial intelligence, has demonstrated its ability to solve the large complex problems which is not possible before. In particular, machine learning and bio-inspired learning algorithms provides the effective solutions in image processing techniques. However, the implementation of the above-mentioned algorithms in the general CPU requires the intensive usage of bandwidth, area and power which makes the CPU unhealthy of usage and implementation. To overcome this problem, ASIC (application specific integrated circuits), GPU (Graphics Processing Unit) &FPGA (Field Programmable gate arrays) have been employed to improve the performance of the hybrid machine learning (ML) classifiers and deep learning algorithms. FPGA has been recently employed for an effective implementation and to achieve the high performance of the learning algorithms. But integrating the complex learning algorithms in FPGA still remains to be real challenge among the researchers. The paper proposes new reconfigurable architectures for bio- inspired classifiers to diagnosis the medical casualties which can be suitable for the tele health care applications. This paper aim is as follows (i) Design and implementation of Parallel Fusion of FSM and Reconfigurable shared Distributed Arithmetic for Bio-Inspired Classifiers (ii) Development of Accelerator Environment to test the performance of proposed architecture (iii) Performance evaluation of proposed architecture in terms of accuracy of detection in compared with MATLAB simulation iv) Implementation of proposed architectures in different ARtix-7 architectures and determination of power, throughput and area . Moreover, the proposed architecture has been tested with the and compared with the other existing architectures.",experiments
198,http://arxiv.org/abs/1912.10609v1,arxiv,arxiv,http://arxiv.org/abs/1912.10609v1,multimedia,23/12/2019 00:00,not defined,not defined,not defined,not defined,one-shot imitation filming of human motion videos,"Imitation learning has been applied to mimic the operation of a human
cameraman in several autonomous cinematography systems. To imitate different
filming styles, existing methods train multiple models, where each model
handles a particular style and requires a significant number of training
samples. As a result, existing methods can hardly generalize to unseen styles.
In this paper, we propose a framework, which can imitate a filming style by
""seeing"" only a single demonstration video of the same style, i.e., one-shot
imitation filming. This is done by two key enabling techniques: 1) feature
extraction of the filming style from the demo video, and 2) filming style
transfer from the demo video to the new situation. We implement the approach
with deep neural network and deploy it to a 6 degrees of freedom (DOF) real
drone cinematography system by first predicting the future camera motions, and
then converting them to the drone's control commands via an odometer. Our
experimental results on extensive datasets and showcases exhibit significant
improvements in our approach over conventional baselines and our approach can
successfully mimic the footage with an unseen style.",experiments
199,10.1109/smartworld.2018.00106,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/8560084/,robotics,12/10/2018 00:00,not defined,not defined,not defined,not defined,real-time data processing architecture for multi-robots based on differential federated learning,"The emergency of ubiquitous intelligence in various things has become the ultimate cornerstone in building a smart interconnection of the physical world and the human world, which also caters to the idea of Internet of Things (IoT). Nowadays, robots as a new type of ubiquitous IoT devices have gained much attention. With the increasing number of distributed multi-robots, such smart environment generates unprecedented amounts of data. Robotic applications are faced with challenges of such big data: the serious real-time assurance and data privacy. Therefore, in order to obtain the big data values via knowledge sharing under the premise of ensuring the real-time data processing and data privacy, we propose a real-time data processing architecture for multi-robots based on the differential federated learning, called RT-robots architecture. A global shared model with differential privacy protection is trained on the cloud iteratively and distributed to multiple edge robots in each round, and the robotic tasks are processed locally in real time. Our implementation and experiments demonstrate that our architecture can be applied on multiple robotic recognition tasks, balance the trade-off between the performance and privacy.",architecture
200,http://arxiv.org/abs/2004.05953v1,arxiv,arxiv,http://arxiv.org/abs/2004.05953v1,science,13/04/2020 00:00,not defined,not defined,not defined,not defined,"software-defined network for end-to-end networked science at the
  exascale","Domain science applications and workflow processes are currently forced to
view the network as an opaque infrastructure into which they inject data and
hope that it emerges at the destination with an acceptable Quality of
Experience. There is little ability for applications to interact with the
network to exchange information, negotiate performance parameters, discover
expected performance metrics, or receive status/troubleshooting information in
real time. The work presented here is motivated by a vision for a new smart
network and smart application ecosystem that will provide a more deterministic
and interactive environment for domain science workflows. The Software-Defined
Network for End-to-end Networked Science at Exascale (SENSE) system includes a
model-based architecture, implementation, and deployment which enables
automated end-to-end network service instantiation across administrative
domains. An intent based interface allows applications to express their
high-level service requirements, an intelligent orchestrator and resource
control systems allow for custom tailoring of scalability and real-time
responsiveness based on individual application and infrastructure operator
requirements. This allows the science applications to manage the network as a
first-class schedulable resource as is the current practice for instruments,
compute, and storage systems. Deployment and experiments on production networks
and testbeds have validated SENSE functions and performance. Emulation based
testing verified the scalability needed to support research and education
infrastructures. Key contributions of this work include an architecture
definition, reference implementation, and deployment. This provides the basis
for further innovation of smart network services to accelerate scientific
discovery in the era of big data, cloud computing, machine learning and
artificial intelligence.",architecture
201,10.23919/date48585.2020.9116560,Offline Model Guard: Secure and Private ML on Mobile Devices,core,'Institute of Electrical and Electronics Engineers (IEEE)',multimedia,05/07/2020 00:00,not defined,not defined,blind source operation,not defined,http://arxiv.org/abs/2007.02351,"Performing machine learning tasks in mobile applications yields a challenging
conflict of interest: highly sensitive client information (e.g., speech data)
should remain private while also the intellectual property of service providers
(e.g., model parameters) must be protected. Cryptographic techniques offer
secure solutions for this, but have an unacceptable overhead and moreover
require frequent network interaction. In this work, we design a practically
efficient hardware-based solution. Specifically, we build Offline Model Guard
(OMG) to enable privacy-preserving machine learning on the predominant mobile
computing platform ARM - even in offline scenarios. By leveraging a trusted
execution environment for strict hardware-enforced isolation from other system
components, OMG guarantees privacy of client data, secrecy of provided models,
and integrity of processing algorithms. Our prototype implementation on an ARM
HiKey 960 development board performs privacy-preserving keyword recognition
using TensorFlow Lite for Microcontrollers in real time.Comment: Original Publication (in the same form): DATE 202",architecture
202,10.1007/s42979-021-00726-1,Nature,springer,https://www.nature.com/articles/s42979-021-00726-1,health,19/06/2021 00:00,not defined,not defined,not defined,not defined,towards regulatory-compliant mlops: oravizio’s journey from a machine learning experiment to a deployed certified medical product,"Agile software development embraces change and manifests working software over comprehensive documentation and responding to change over following a plan. The ability to continuously release software has enabled a development approach where experimental features are put to use, and, if they stand the test of real use, they remain in production. Examples of such features include machine learning (ML) models, which are usually pre-trained, but can still evolve in production. However, many domains require more plan-driven approach to avoid hazard to environment and humans, and to mitigate risks in the process. In this paper, we start by presenting continuous software engineering practices in a regulated context, and then apply the results to the emerging practice of MLOps, or continuous delivery of ML features. Furthermore, as a practical contribution, we present a case study regarding Oravizio, first CE-certified medical software for assessing the risks of joint replacement surgeries. Towards the end of the paper, we also reflect the Oravizio experiences to MLOps in regulatory context.",architecture
203,10.1109/med.2017.7984310,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/7984310/,industry,06/07/2017 00:00,not defined,not defined,not defined,not defined,cloud computing for big data analytics in the process control industry,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",architecture
204,10.1109/tro.2021.3084374,,project-academic,project-academic,robotics,17/06/2021 00:00,not defined,not defined,not defined,not defined,cat like jumping and landing of legged robots in low gravity using deep reinforcement learning," In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we trained a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of three-dimensional (re-)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for two-dimensional micro-gravity experiments. The experimental results demonstrate that repetitive, controlled jumping and landing with natural agility is possible.",experiments
205,10.1109/andescon50619.2020.9272196,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9272196/,health,16/10/2020 00:00,not defined,not defined,not defined,not defined,multipurpose unmanned system: an efficient solution to increase the capabilities of the uavs,"The results of this research propose the implementation of a system that significantly increases the capacity of unmanned vehicles, turning them into multifunctional vehicles. The system has a logistics dispatch module and a video analytics module. The first module allows the delivery of medical, food, smoke, disinfectant, etc. The module is practical, safe and economical, features that denote the feasibility of immediate implementation in unmanned vehicles of any rank and / or classification. Note that the implementation of the dispatch module does not require additional radio frequency systems. The second module includes a video analysis process in real time, an aspect that constitutes a significant contribution to the proposed solution, since it allows obtaining important information during the flight; it also reduces the risk in air operations and simultaneously increases the efficiency of themselves. Note that video analytics optimizes resources and avoids jeopardizing the lives of aircraft pilots and crews who traditionally should carry out these activities. In times of pandemic, this innovation avoids direct contact with an infected population and can guarantee the sanitary conditions required in certain circumstances. The solution increases the capabilities of unmanned vehicles and makes them useful tools in various scenarios, whether caused by natural or man-made disasters. Our proposal is very flexible, reliable, and scalable and can be adapted to various models and makes of unmanned vehicles. The system has been implemented on fixed-wing and rotary-wing unmanned vehicles, showing satisfactory results.",experiments
206,10.1016/j.cogsys.2017.08.002,scopus,sciencedirect,https://api.elsevier.com/content/abstract/scopus_id/85034106426,robotics,01/01/2018,not defined,not defined,not defined,not defined,a computational cognitive framework of spatial memory in brains and robots,"Computational cognitive models of spatial memory often neglect difficulties posed by the real world, such as sensory noise, uncertainty, and high spatial complexity. On the other hand, robotics is unconcerned with understanding biological cognition. Here, we describe a computational framework for robotic architectures aiming to function in realistic environments, as well as to be cognitively plausible.
                  We motivate and describe several mechanisms towards achieving this despite the sensory noise and spatial complexity inherent in the physical world. We tackle error accumulation during path integration by means of Bayesian localization, and loop closing with sequential gradient descent. Finally, we outline a method for structuring spatial representations using metric learning and clustering. Crucially, unlike the algorithms of traditional robotics, we show that these mechanisms can be implemented in neuronal or cognitive models.
                  We briefly outline a concrete implementation of the proposed framework as part of the LIDA cognitive architecture, and argue that this kind of probabilistic framework is well-suited for use in cognitive robotic architectures aiming to combine spatial functionality and psychological plausibility.",architecture
207,10.1109/ic4me247184.2019.9036531,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9036531/,multimedia,12/07/2019 00:00,not defined,not defined,not defined,not defined,object detection based security system using machine learning algorthim and raspberry pi,"Conventional security systems that use surveillance cameras to monitor the property lacks the ability to notify the security administrator in the event of trespassing. A security camera when used along with a digital video recorder (DVR) is only effective as a source to gather evidence unless the video feed is constantly being monitored by a dedicated personnel. This paper discusses the implementation of a cost effective, intelligent security system that overcomes drawbacks of conventional security cameras by utilizing a machine learning and Viola-Jones algorithm under image processing literature to identify trespassers and multiple object detection in real time. The paper presents the design and implementation details of the intelligent object detection based security system in two different computing environment, MATLAB and Python respectively using Raspberry Pi 3 B single board computer. The security system is capable of alerting the security administrator through email via internet while activating an alarm locally.",experiments
208,,,core,,robotics,17/02/2010 00:00,not defined,not defined,not defined,not defined,using rfid and a low cost robot to evolve foraging behavior,"The process of developing genetic algorithms, genetic programs or training neural networks is a time consuming task. When the target device is an autonomous mobile robot, this development is often performed using software simulation. Software simulations are a cost effective tool and provide researchers with the ability to test out multiple algorithms quickly and efficiently. However, the end result is that the optimized algorithm(s) must be implemented and tested on an actual robot to evaluate performance in the real world. Significant cost can be associated with this final step. In this paper we propose to leverage Radio Frequency Identification (RFID) and a low-cost RFID capable mobile robot with the intent of creating basic foraging behavior. Additionally, we will present experimental results that demonstrate the effectiveness of using Genetic Programming (GP) and a low-cost RFID capable robot t",experiments
209,10.1109/isie45063.2020.9152441,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9152441/,industry,19/06/2020 00:00,not defined,not defined,not defined,not defined,deployment of a smart and predictive maintenance system in an industrial case study,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions.",architecture
210,http://arxiv.org/abs/2106.09357,'Institute of Electrical and Electronics Engineers (IEEE)',core,10.1109/TRO.2021.3084374,multimedia,17/06/2021 00:00,not defined,not defined,not defined,not defined,"cat-like jumping and landing of legged robots in low-gravity using deep
  reinforcement learning","In this article, we show that learned policies can be applied to solve legged
locomotion control tasks with extensive flight phases, such as those
encountered in space exploration. Using an off-the-shelf deep reinforcement
learning algorithm, we trained a neural network to control a jumping quadruped
robot while solely using its limbs for attitude control. We present tasks of
increasing complexity leading to a combination of three-dimensional
(re-)orientation and landing locomotion behaviors of a quadruped robot
traversing simulated low-gravity celestial bodies. We show that our approach
easily generalizes across these tasks and successfully trains policies for each
case. Using sim-to-real transfer, we deploy trained policies in the real world
on the SpaceBok robot placed on an experimental testbed designed for
two-dimensional micro-gravity experiments. The experimental results demonstrate
that repetitive, controlled jumping and landing with natural agility is
possible.Comment: Published in IEEE Transactions on Robotics:
  https://ieeexplore.ieee.org/document/9453856 Video:
  https://youtu.be/KQhlZa42fe",experiments
211,10.1109/robot.1986.1087518,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/1087518/,autonomous vehicle,10/04/1986 00:00,not defined,not defined,not defined,not defined,architecture and early experience with planning for the alv,"This paper describes the software architecture and the initial algorithms that have proved to be effective for a real time robot planning system. The architecture is designed to incorporate planning technology from research on artificial intelligence while at the same time supporting the high performance decision making needed to control a fast-moving autonomous vehicle. The symbolic representation of the vehicle's plan is a key element in this architecture. Our initial algorithms use an especially efficient version of dynamic programming to find the best routes. The route is then translated into a symbolic plan. Replanning happens at several levels with the cost of replanning proportionate to the scope of the changes. This software is currently running in an environment which simulates the vehicle and perception systems, but it will be transferred to the DARPA Autonomous Land Vehicle built by Martin Marietta Denver Aerospace [Lowrie 86].",excluded
