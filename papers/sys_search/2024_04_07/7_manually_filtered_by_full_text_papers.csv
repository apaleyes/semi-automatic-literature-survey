id,status,doi,publisher,database,query_name,query_value,url,publication_date,title,abstract,semantic_score
1,included,http://arxiv.org/abs/2307.02867v1,arxiv,arxiv,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),http://arxiv.org/abs/2307.02867v1,2023-07-06 00:00:00,"towards a safe mlops process for the continuous development and safety
  assurance of ml-based systems in the railway domain","Traditional automation technologies alone are not sufficient to enable
driverless operation of trains (called Grade of Automation (GoA) 4) on
non-restricted infrastructure. The required perception tasks are nowadays
realized using Machine Learning (ML) and thus need to be developed and deployed
reliably and efficiently. One important aspect to achieve this is to use an
MLOps process for tackling improved reproducibility, traceability,
collaboration, and continuous adaptation of a driverless operation to changing
conditions. MLOps mixes ML application development and operation (Ops) and
enables high frequency software releases and continuous innovation based on the
feedback from operations. In this paper, we outline a safe MLOps process for
the continuous development and safety assurance of ML-based systems in the
railway domain. It integrates system engineering, safety assurance, and the ML
life-cycle in a comprehensive workflow. We present the individual stages of the
process and their interactions. Moreover, we describe relevant challenges to
automate the different stages of the safe MLOps process.",0.8090653699987075
2,included,6fd6d54bc19a7daf293f1b929ec3a749a86c6479,AI and Ethics,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/6fd6d54bc19a7daf293f1b929ec3a749a86c6479,2023.0,how to design an ai ethics board,"The development and deployment of artificial intelligence (AI) systems poses significant risks to society. To reduce these risks to an acceptable level, AI companies need an effective risk management process and sound risk governance. In this paper, we explore a particular way in which AI companies can improve their risk governance: by setting up an AI ethics board. We identify five key design choices: (1) What responsibilities should the board have? (2) What should its legal structure be? (3) Who should sit on the board? (4) How should it make decisions? (5) And what resources does it need? We break each of these questions down into more specific sub-questions, list options, and discuss how different design choices affect the board’s ability to reduce societal risks from AI. Several failures have shown that designing an AI ethics board can be challenging. This paper provides a toolbox that can help AI companies to overcome these challenges.",0.8003887919818654
3,included,a3715592161c25474e5394f023660c127da448ed,LegalAIIA@ICAIL,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/a3715592161c25474e5394f023660c127da448ed,2023.0,strengthening the ai operating environment: distributed competence as a means to risk mitigation,"In the rapidly evolving discourse on artificial intelligence (AI), the familiar refrain of “maximizing potential while mitigating risks” has become somewhat of a ubiquitous mantra, emphasizing the need for an effective risk mitigation framework. This paper briefly examines the current state of AI-enabled applications and discusses the various risk containment strategies being implemented. Initial efforts focused on establishing high-level principles for responsible AI use. More recent strategies have sought to operationalize these principles through normative instruments, such as industry best practices and legal statutes, that govern AI applications and their creators. While valuable, such a top-down approach is not sufficiently effective; a complementary, bottom-up approach focused on strengthening the environment in which AI is deployed is also necessary. The paperanalyzestwospecificinitiativesaimedatenhancingthehumancomponentofAIdeployment(creating abetter-informed public through AI benchmarks, creating a better-equipped public with resources for local validation) and offers insights on how this environment-focused track can contribute to risk containment. Furthermore, we suggest additional steps for leveraging this approach in tandem with top-down strategies to cultivate a more robust risk mitigation framework.",0.815056125907337
4,included,http://arxiv.org/abs/2001.09734v1,arxiv,arxiv,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),http://arxiv.org/abs/2001.09734v1,2020-01-27 00:00:00,"one explanation does not fit all: the promise of interactive
  explanations for machine learning transparency","The need for transparency of predictive systems based on Machine Learning
algorithms arises as a consequence of their ever-increasing proliferation in
the industry. Whenever black-box algorithmic predictions influence human
affairs, the inner workings of these algorithms should be scrutinised and their
decisions explained to the relevant stakeholders, including the system
engineers, the system's operators and the individuals whose case is being
decided. While a variety of interpretability and explainability methods is
available, none of them is a panacea that can satisfy all diverse expectations
and competing objectives that might be required by the parties involved. We
address this challenge in this paper by discussing the promises of Interactive
Machine Learning for improved transparency of black-box systems using the
example of contrastive explanations -- a state-of-the-art approach to
Interpretable Machine Learning.
  Specifically, we show how to personalise counterfactual explanations by
interactively adjusting their conditional statements and extract additional
explanations by asking follow-up ""What if?"" questions. Our experience in
building, deploying and presenting this type of system allowed us to list
desired properties as well as potential limitations, which can be used to guide
the development of interactive explainers. While customising the medium of
interaction, i.e., the user interface comprising of various communication
channels, may give an impression of personalisation, we argue that adjusting
the explanation itself and its content is more important. To this end,
properties such as breadth, scope, context, purpose and target of the
explanation have to be considered, in addition to explicitly informing the
explainee about its limitations and caveats...",0.8003887919818654
5,included,http://arxiv.org/abs/2205.01070v1,arxiv,arxiv,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),http://arxiv.org/abs/2205.01070v1,2022-04-20 00:00:00,five ps: leverage zones towards responsible ai,"There is a growing debate amongst academics and practitioners on whether
interventions made, thus far, towards Responsible AI would have been enough to
engage with root causes of AI problems. Failure to effect meaningful changes in
this system could see these initiatives to not reach their potential and lead
to the concept becoming another buzzword for companies to use in their
marketing campaigns. We propose that there is an opportunity to improve the
extent to which interventions are understood to be effective in their
contribution to the change required for Responsible AI. Using the notions of
leverage zones adapted from the 'Systems Thinking' literature, we suggest a
novel approach to evaluate the effectiveness of interventions, to focus on
those that may bring about the real change that is needed. In this paper we
argue that insights from using this perspective demonstrate that the majority
of current initiatives taken by various actors in the field, focus on low-order
interventions, such as short-term fixes, tweaking algorithms and updating
parameters, absent from higher-order interventions, such as redefining the
system's foundational structures that govern those parameters, or challenging
the underlying purpose upon which those structures are built and developed in
the first place(high-leverage). This paper presents a conceptual framework
called the Five Ps to identify interventions towards Responsible AI and
provides a scaffold for transdisciplinary question asking to improve outcomes
towards Responsible AI.",0.8183542332228493
