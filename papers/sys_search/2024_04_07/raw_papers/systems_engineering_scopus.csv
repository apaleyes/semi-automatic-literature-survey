id,pii,url,type,publication,publisher,publication_date,database,query_name,query_value,title,abstract
10.1007/s11063-024-11574-4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188464418&origin=inward,Article,SCOPUS_ID:85188464418,scopus,2024-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),neural data augmentation for legal overruling task: small deep learning models vs. large language models,"
AbstractView references

Deep learning models produce impressive results in any natural language processing applications when given a better learning strategy and trained with large labeled datasets. However, the annotation of massive training data is far too expensive, especially in the legal domain, due to the need for trained legal professionals. Data augmentation solves the problem of learning without labeled big data. In this paper, we employ pre-trained language models and prompt engineering to generate large-scale pseudo-labeled data for the legal overruling task using 100 data samples. We train small recurrent and convolutional deep-learning models using this data and fine-tune a few other transformer models. We then evaluate the effectiveness of the models, both with and without data augmentation, using the benchmark dataset and analyze the results. We also test the performance of these models with the state-of-the-art GPT-3 model under few-shot setting. Our experimental findings demonstrate that data augmentation results in better model performance in the legal overruling task than models trained without augmentation. Furthermore, our best-performing deep learning model trained on augmented data outperforms the few-shot GPT-3 by 18% in the F1-score. Additionally, our results highlight that the small neural networks trained with augmented data achieve outcomes comparable to those of other large language models. © The Author(s) 2024.
"
10.1093/jcde/qwae017,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188009973&origin=inward,Article,SCOPUS_ID:85188009973,scopus,2024-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative artificial intelligence and building design: early photorealistic render visualization of façades using local identity-trained models,"
AbstractView references

This paper elucidates an approach that utilizes generative artificial intelligence (AI) to develop alternative architectural design options based on local identity. The advancement of AI technologies has increasingly piqued the interest of the architecture, engineering, construction, and facility management industry. Notably, the topic of ""visualization""has gained prominence as a means for enhancing communication related to a project, especially in the early phases of design. This study aims to enhance the ease of obtaining design images during initial phases of design by drawing from multiple texts and images. It develops an additional training model to generate various design alternatives that resonate with the identity of the locale through the application of generative AI to the façade design of buildings. The identity of a locality in cities and regions is the capacity for the cities and regions to be identified and recognized as a specific area. Among the various visual elements of urban and regional landscapes, the front face of buildings may play a significant role in people's aesthetic perception and overall impression of the local environment. The research proposes an approach that transcends the conventional employment of three-dimensional modeling and rendering tools by readily deriving design alternatives that consider this local identity in commercial building remodeling. This approach allows for financial and temporal efficiency in the design communication phase of the initial architectural design process. The implementation and utilization of the proposed approach's supplementary training model in this study proceeds as follows: (i) image data are collected from the target area using open-source street-view resources and preprocessed for conversion to a trainable format; (ii) textual data are prepared for pairing with preprocessed image data; (iii) additional training and outcome testing are performed using varied text prompts and images; and (iv) the ability to generate building façade images that reflect the identity of the collected locale by using the additional trained model is determined, as evidenced by the findings of the proposed application method study. This enables the generation of design alternatives that integrate regional styles and diverse design requirements for buildings. The training model implemented in this study can be leveraged through weight adjustments and prompt engineering to generate a greater number of design reference images, among other diverse approaches. © 2024 The Author(s). Published by Oxford University Press on behalf of the Society for Computational Design and Engineering.
"
10.1002/emp2.13133,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187896656&origin=inward,Article,SCOPUS_ID:85187896656,scopus,2024-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated heart score determination via chatgpt: honing a framework for iterative prompt development,"
AbstractView references

Objectives: This study presents a design framework to enhance the accuracy by which large language models (LLMs), like ChatGPT can extract insights from clinical notes. We highlight this framework via prompt refinement for the automated determination of HEART (History, ECG, Age, Risk factors, Troponin risk algorithm) scores in chest pain evaluation. Methods: We developed a pipeline for LLM prompt testing, employing stochastic repeat testing and quantifying response errors relative to physician assessment. We evaluated the pipeline for automated HEART score determination across a limited set of 24 synthetic clinical notes representing four simulated patients. To assess whether iterative prompt design could improve the LLMs’ ability to extract complex clinical concepts and apply rule-based logic to translate them to HEART subscores, we monitored diagnostic performance during prompt iteration. Results: Validation included three iterative rounds of prompt improvement for three HEART subscores with 25 repeat trials totaling 1200 queries each for GPT-3.5 and GPT-4. For both LLM models, from initial to final prompt design, there was a decrease in the rate of responses with erroneous, non-numerical subscore answers. Accuracy of numerical responses for HEART subscores (discrete 0–2 point scale) improved for GPT-4 from the initial to final prompt iteration, decreasing from a mean error of 0.16–0.10 (95% confidence interval: 0.07–0.14) points. Conclusion: We established a framework for iterative prompt design in the clinical space. Although the results indicate potential for integrating LLMs in structured clinical note analysis, translation to real, large-scale clinical data with appropriate data privacy safeguards is needed. © 2024 The Authors. Journal of the American College of Emergency Physicians Open published by Wiley Periodicals LLC on behalf of American College of Emergency Physicians.
"
10.1007/s10462-024-10720-7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187531043&origin=inward,Article,SCOPUS_ID:85187531043,scopus,2024-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),unraveling the mysteries of ai chatbots,"
AbstractView references

This primer provides an overview of the rapidly evolving field of generative artificial intelligence, specifically focusing on large language models like ChatGPT (OpenAI) and Bard (Google). Large language models have demonstrated unprecedented capabilities in responding to natural language prompts. The aim of this primer is to demystify the underlying theory and architecture of large language models, providing intuitive explanations for a broader audience. Learners seeking to gain insight into the technical underpinnings of large language models must sift through rapidly growing and fragmented literature on the topic. This primer brings all the main concepts into a single digestible document. Topics covered include text tokenization, vocabulary construction, token embedding, context embedding with attention mechanisms, artificial neural networks, and objective functions in model training. The primer also explores state-of-the-art methods in training large language models to generalize on specific applications and to align with human intentions. Finally, an introduction to the concept of prompt engineering highlights the importance of effective human-machine interaction through natural language in harnessing the full potential of artificial intelligence chatbots. This comprehensive yet accessible primer will benefit students and researchers seeking foundational knowledge and a deeper understanding of the inner workings of existing and emerging artificial intelligence models. The author hopes that the primer will encourage further responsible innovation and informed discussions about these increasingly powerful tools. © The Author(s) 2024.
"
10.1093/ejo/cjae011,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187120819&origin=inward,Article,SCOPUS_ID:85187120819,scopus,2024-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),enhancing systematic reviews in orthodontics: a comparative examination of gpt-3.5 and gpt-4 for generating pico-based queries with tailored prompts and configurations,"
AbstractView references

Objectives: The rapid advancement of Large Language Models (LLMs) has prompted an exploration of their efficacy in generating PICO-based (Patient, Intervention, Comparison, Outcome) queries, especially in the field of orthodontics. This study aimed to assess the usability of Large Language Models (LLMs), in aiding systematic review processes, with a specific focus on comparing the performance of ChatGPT 3.5 and ChatGPT 4 using a specialized prompt tailored for orthodontics. Materials/Methods: Five databases were perused to curate a sample of 77 systematic reviews and meta-analyses published between 2016 and 2021. Utilizing prompt engineering techniques, the LLMs were directed to formulate PICO questions, Boolean queries, and relevant ∝. The outputs were subsequently evaluated for accuracy and consistency by independent researchers using three-point and six-point Likert scales. Furthermore, the PICO records of 41 studies, which were compatible with the PROSPERO records, were compared with the responses provided by the models. Results: ChatGPT 3.5 and 4 showcased a consistent ability to craft PICO-based queries. Statistically significant differences in accuracy were observed in specific categories, with GPT-4 often outperforming GPT-3.5. Limitations: The study’s test set might not encapsulate the full range of LLM application scenarios. Emphasis on specific question types may also not reflect the complete capabilities of the models. Conclusions/Implications: Both ChatGPT 3.5 and 4 can be pivotal tools for generating PICO-driven queries in orthodontics when optimally configured. However, the precision required in medical research necessitates a judicious and critical evaluation of LLM-generated outputs, advocating for a circumspect integration into scientific investigations. © The Author(s) 2024. Published by Oxford University Press on behalf of the European Orthodontic Society. All rights reserved.
"
10.1016/j.yjoc.2023.100071,S2713374523000304,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179426552&origin=inward,Article,SCOPUS_ID:85179426552,scopus,2024-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai vs humans in the aut: simulations to llms,"This paper reviews studies of proposed creative machines applied to a prototypical creative task, i.e., the Alternative Uses Task (AUT). Although one system (OROC) did simulate some aspects of human strategies for the AUT, most recent attempts have not been simulation-oriented, but rather have used Large Language Model (LLM) systems such as GPT-3 which embody extremely large connectionist networks trained on huge volumes of textual data. Studies reviewed here indicate that LLM based systems are performing on the AUT at near or somewhat above human levels in terms of scores on originality and usefulness. Moreover, similar patterns are found in the data of humans and LLM models in the AUT, such as output order effects and a negative association between originality and value or utility. However, it is concluded that GPT-3 and similar systems, despite generating novel and useful responses, do not display creativity as they lack agency and are purely algorithmic. LLM studies so far in this area have largely been exploratory and future studies should guard against possible training data contamination."
10.1145/3636555.3636910,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187550642&origin=inward,Conference Paper,SCOPUS_ID:85187550642,scopus,2024-03-18,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompt-based and fine-tuned gpt models for context-dependent and -independent deductive coding in social annotation,"
AbstractView references

GPT has demonstrated impressive capabilities in executing various natural language processing (NLP) and reasoning tasks, showcasing its potential for deductive coding in social annotations. This research explored the effectiveness of prompt engineering and fine-tuning approaches of GPT for deductive coding of context-dependent and context-independent dimensions. Coding context-dependent dimensions (i.e., Theorizing, Integration, Reflection) requires a contextualized understanding that connects the target comment with reading materials and previous comments, whereas coding context-independent dimensions (i.e., Appraisal, Questioning, Social, Curiosity, Surprise) relies more on the comment itself. Utilizing strategies such as prompt decomposition, multi-prompt learning, and a codebook-centered approach, we found that prompt engineering can achieve fair to substantial agreement with expert-labeled data across various coding dimensions. These results affirm GPT's potential for effective application in real-world coding tasks. Compared to context-independent coding, context-dependent dimensions had lower agreement with expert-labeled data. To enhance accuracy, GPT models were fine-tuned using 102 pieces of expert-labeled data, with an additional 102 cases used for validation. The fine-tuned models demonstrated substantial agreement with ground truth in context-independent dimensions and elevated the inter-rater reliability of context-dependent categories to moderate levels. This approach represents a promising path for significantly reducing human labor and time, especially with large unstructured datasets, without sacrificing the accuracy and reliability of deductive coding tasks in social annotation. The study marks a step toward optimizing and streamlining coding processes in social annotation. Our findings suggest the promise of using GPT to analyze qualitative data and provide detailed, immediate feedback for students to elicit deepening inquiries. © 2024 Owner/Author.
"
10.1145/3636555.3636882,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187550433&origin=inward,Conference Paper,SCOPUS_ID:85187550433,scopus,2024-03-18,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),kattis vs chatgpt: assessment and evaluation of programming tasks in the age of artificial intelligence,"
AbstractView references

AI-powered education technologies can support students and teachers in computer science education. However, with the recent developments in generative AI, and especially the increasingly emerging popularity of ChatGPT, the effectiveness of using large language models for solving programming tasks has been underexplored. The present study examines ChatGPT's ability to generate code solutions at different difficulty levels for introductory programming courses. We conducted an experiment where ChatGPT was tested on 127 randomly selected programming problems provided by Kattis, an automatic software grading tool for computer science programs, often used in higher education. The results showed that ChatGPT independently could solve 19 out of 127 programming tasks generated and assessed by Kattis. Further, ChatGPT was found to be able to generate accurate code solutions for simple problems but encountered difficulties with more complex programming tasks. The results contribute to the ongoing debate on the utility of AI-powered tools in programming education. © 2024 Owner/Author.
"
10.1002/advs.202306724,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180443194&origin=inward,Article,SCOPUS_ID:85180443194,scopus,2024-03-13,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),bioinspiredllm: conversational large language model for the mechanics of biological and bio-inspired materials,"
AbstractView references

The study of biological materials and bio-inspired materials science is well established; however, surprisingly little knowledge is systematically translated to engineering solutions. To accelerate discovery and guide insights, an open-source autoregressive transformer large language model (LLM), BioinspiredLLM, is reported. The model is finetuned with a corpus of over a thousand peer-reviewed articles in the field of structural biological and bio-inspired materials and can be prompted to recall information, assist with research tasks, and function as an engine for creativity. The model has proven that it is able to accurately recall information about biological materials and is further strengthened with enhanced reasoning ability, as well as with Retrieval-Augmented Generation (RAG) to incorporate new data during generation that can also help to traceback sources, update the knowledge base, and connect knowledge domains. BioinspiredLLM also has shown to develop sound hypotheses regarding biological materials design and remarkably so for materials that have never been explicitly studied before. Lastly, the model shows impressive promise in collaborating with other generative artificial intelligence models in a workflow that can reshape the traditional materials design process. This collaborative generative artificial intelligence method can stimulate and enhance bio-inspired materials design workflows. Biological materials are at a critical intersection of multiple scientific fields and models like BioinspiredLLM help to connect knowledge domains. © 2023 The Authors. Advanced Science published by Wiley-VCH GmbH.
"
10.1145/3627508.3638344,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188714194&origin=inward,Conference Paper,SCOPUS_ID:85188714194,scopus,2024-03-10,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),task supportive and personalized human-large language model interaction: a user study,"
AbstractView references

Large language model (LLM) applications, such as ChatGPT, are a powerful tool for online information-seeking (IS) and problem-solving tasks. However, users still face challenges initializing and refining prompts, and their cognitive barriers and biased perceptions further impede task completion. These issues reflect broader challenges identified within the fields of IS and interactive information retrieval (IIR). To address these, our approach integrates task context and user perceptions into human-ChatGPT interactions through prompt engineering. We developed a ChatGPT-like platform integrated with supportive functions, including perception articulation, prompt suggestion, and conversation explanation. Our findings of a user study demonstrate that the supportive functions help users manage expectations, reduce cognitive loads, better refine prompts, and increase user engagement. This research enhances our comprehension of designing proactive and user-centric systems with LLMs. It offers insights into evaluating human-LLM interactions and emphasizes potential challenges for under served users. © 2024 Owner/Author.
"
10.1145/3627508.3638293,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188697324&origin=inward,Conference Paper,SCOPUS_ID:85188697324,scopus,2024-03-10,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),jaybot - aiding university students and admission with an llm-based chatbot,"
AbstractView references

This demo paper presents JayBot, an LLM-based chatbot system aimed at enhancing the user experience of prospective and current students, faculty, and staff at a UK university. The objective of JayBot is to provide information to users on general enquiries regarding course modules, duration, fees, entry requirements, lecturers, internship, career paths, course employability and other related aspects. Leveraging the use cases of generative artificial intelligence (AI), the chatbot application was built using OpenAI's advanced large language model (GPT-3.5 turbo); to tackle issues such as hallucination as well as focus and timeliness of results, an embedding transformer model has been combined with a vector database and vector search. Prompt engineering techniques were employed to enhance the chatbot's response abilities. Preliminary user studies indicate JayBot's effectiveness and efficiency. The demo will showcase JayBot in a university admission use case and discuss further application scenarios. © 2024 Owner/Author.
"
10.1145/3627508.3638315,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188663766&origin=inward,Conference Paper,SCOPUS_ID:85188663766,scopus,2024-03-10,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),identifying textual disinformation using large language models,"
AbstractView references

The spread of disinformation is becoming a more acute challenge in modern society. The rise of AI technologies is providing it with an additional boost, making disinformation creation and propagation available to almost anyone. This change in the disinformation landscape needs to be responded to by the improvement of debunking and detection techniques. The plain fact-checking solutions might not be sufficient, since disinformative articles often consist of manipulated versions of correct information. Large Language Models (LLM) can be employed to identify emotions, stances, and motives behind the text. This research aims to find the way those abilities of LLM can be used for disinformation detection with an accuracy comparable to the debunking expert. Additionally, the question of multilingual detection with LLM models will be addressed, since different languages might require different approaches in LLM training and tuning. Based on these results, a semi-automated disinformation labeling system is to be built. © 2024 Owner/Author.
"
10.3390/info15030162,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188881309&origin=inward,Article,SCOPUS_ID:85188881309,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using generative ai to improve the performance and interpretability of rule-based diagnosis of type 2 diabetes mellitus,"
AbstractView references

Introduction: Type 2 diabetes mellitus is a major global health concern, but interpreting machine learning models for diagnosis remains challenging. This study investigates combining association rule mining with advanced natural language processing to improve both diagnostic accuracy and interpretability. This novel approach has not been explored before in using pretrained transformers for diabetes classification on tabular data. Methods: The study used the Pima Indians Diabetes dataset to investigate Type 2 diabetes mellitus. Python and Jupyter Notebook were employed for analysis, with the NiaARM framework for association rule mining. LightGBM and the dalex package were used for performance comparison and feature importance analysis, respectively. SHAP was used for local interpretability. OpenAI GPT version 3.5 was utilized for outcome prediction and interpretation. The source code is available on GitHub. Results: NiaARM generated 350 rules to predict diabetes. LightGBM performed better than the GPT-based model. A comparison of GPT and NiaARM rules showed disparities, prompting a similarity score analysis. LightGBM’s decision making leaned heavily on glucose, age, and BMI, as highlighted in feature importance rankings. Beeswarm plots demonstrated how feature values correlate with their influence on diagnosis outcomes. Discussion: Combining association rule mining with GPT for Type 2 diabetes mellitus classification yields limited effectiveness. Enhancements like preprocessing and hyperparameter tuning are required. Interpretation challenges and GPT’s dependency on provided rules indicate the necessity for prompt engineering and similarity score methods. Variations in feature importance rankings underscore the complexity of T2DM. Concerns regarding GPT’s reliability emphasize the importance of iterative approaches for improving prediction accuracy. © 2024 by the authors.
"
10.1002/aaai.12163,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188287637&origin=inward,Article,SCOPUS_ID:85188287637,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),institute for foundations of machine learning (ifml): advancing ai systems that will transform our world,"
AbstractView references

The Institute for Foundations of Machine Learning (IFML) focuses on core foundational tools to power the next generation of machine learning models. Its research underpins the algorithms and data sets that make generative artificial intelligence (AI) more accurate and reliable. Headquartered at The University of Texas at Austin, IFML researchers collaborate across an ecosystem that spans University of Washington, Stanford, UCLA, Microsoft Research, the Santa Fe Institute, and Wichita State University. Over the past year, we have witnessed incredible breakthroughs in AI on topics that are at the heart of IFML's agenda, such as foundation models, LLMs, fine-tuning, and diffusion with game-changing applications influencing almost every area of science and technology. In this article, we seek to highlight seek to highlight the application of foundational machine learning research on key use-inspired topics: Fairness in Imaging with Deep Learning: designing the correct metrics and algorithms to make deep networks less biased. Deep proteins: using foundational machine learning techniques to advance protein engineering and launch a biomanufacturing revolution. Sounds and Space for Audio-Visual Learning: building agents capable of audio-visual navigation in complex 3D environments via new data augmentations. Improving Speed and Robustness of Magnetic Resonance Imaging: using deep learning algorithms to develop fast and robust MRI methods for clinical diagnostic imaging. IFML is also responding to explosive industry demand for an AI-capable workforce. We have launched an accessible, affordable, and scalable new degree program—the MSAI—that looks to wholly reshape the AI/ML workforce pipeline. © 2024 UT Austin. AI Magazine published by John Wiley & Sons Ltd on behalf of Association for the Advancement of Artificial Intelligence.
"
10.1115/1.4064364,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187198367&origin=inward,Article,SCOPUS_ID:85187198367,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),assessing learning of computer programing skills in the age of generative artificial intelligence,"
AbstractView references

Generative artificial intelligence (AI) tools such as CHATGPT, BARD, and CLAUDE have recently become a concern in the delivery of engineering education. For courses focused on computer coding, such tools are capable for creating working computer code across a range of computer languages and computing platforms. In a course for mechanical engineers focused on Cþþ coding for the ARDUINO microcontroller and coding engineering problems in MATLAB, a new approach to assessment of programing homework assignments was developed. This assessment moved the focus of assigned points from the correctness of the code to the effort and understanding of the code demonstrated by the student during in-person grading. Students who participated fully in in-person grading did significantly better on a midterm exam. Relative to a previous semester, where grading was focused on correct code, students had a slightly higher average midterm exam score. This approach appears to be effective in supporting computational learning in the face of evolving tools that could be used to circumvent learning. Future work should examine how to also encourage responsible use of generative AI in computational learning. Copyright © 2024 by ASME.
"
10.1109/MS.2023.3339408,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187131916&origin=inward,Article,SCOPUS_ID:85187131916,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"testing, debugging, and log analysis with modern ai tools","
AbstractView references

This edition of the Practitioners Digest covers recent papers employing generative artificial intelligence in support of testing, debugging, and log analysis that were presented at the 38th IEEE/ACM International Conference on Automated Software Engineering (ASE 2023) and the 16th IEEE International Conference on Software, Testing, Verification and Validation (ICST 2023). Feedback or suggestions are welcome. In addition, if you try or adopt any of the practices included in the column, please send us and the authors of the paper(s) a note about your experiences. © 1984-2012 IEEE.
"
10.1007/s12665-024-11485-6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187115126&origin=inward,Article,SCOPUS_ID:85187115126,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rock slope stability assessment based on the critical failure state curve for the generalized hoek‒brown criterion,"
AbstractView references

The strength reduction method (SRM) based on the generalized Hoek‒Brown (GHB) criterion has become an important and popular topic to analyse the stability of rock slopes. Various reduction strategies have been proposed and applied by the civil and mining engineering community. This paper proposed a new SRM for rock slopes with the GHB criterion based on the critical failure state curve (CFSC). The existence of the CFSC has been proven by theoretical analysis, and the explicit expression of the CFSCs for different parameters mi and slope angles β, considering the influence of disturbance factor D, has been obtained by curve fitting based on a great deal of simulation data. The new SRM provides a graphic method to determine the parameters at the critical failure state from the initial state by reducing the compressive strength of intact rock σci and the parameter combination sα with the same ratio and proposes a definition of the factor of safety (FOS) based on the parameters of the two states. This method was applied to nine slope examples to verify its validity and accuracy. The relative errors between the critical state parameters obtained from the graphic method and that from the simulation analysis are less than 10%, which proves the accuracy of the CFSCs. The FOSs obtained by the proposed definition are compared with those obtained by the Bishop simplified method and the local linearization method (LLM), and the results are very close. The relative error is less than ± 5% compared with the LLM, and the stability state predicted is perfectly accurate. However, the calculation procedure is largely simplified, and the calculation speed is largely improved. A practical case of an open pit limestone slope with multiple steps was detailed analysed by the proposed SRM based on CFSC. The FOS results comparison with other existing method has demonstrated its feasibility and reliability in engineering application. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
"
10.1115/1.4064564,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185890133&origin=inward,Article,SCOPUS_ID:85185890133,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a study on generative design reasoning and students' divergent and convergent thinking,"
AbstractView references

Computer-aided design (CAD) is a standard design tool used in engineering practice and by students. CAD has become increasingly analytic and inventive in incorporating artificial intelligence (AI) approaches to design, e.g., generative design (GD), to help expand designers' divergent thinking. However, generative design technologies are relatively new, we know little about generative design thinking in students. This research aims to advance our understanding of the relationship between aspects of generative design thinking and traditional design thinking. This study was set in an introductory graphics and design course where student designers used FUSION 360 to optimize a bicycle wheel frame. We collected the following data from the sample: divergent and convergent psychological tests and an open-ended response to a generative design prompt (called the generative design reasoning elicitation problem). A Spearman's rank correlation showed no statistically significant relationship between generative design reasoning and divergent thinking. However, an analysis of variance found a significant difference in generative design reasoning and convergent thinking between groups with moderate GD reasoning and low GD reasoning. This study shows that new computational tools might present the same challenges to beginning designers as conventional tools. Instructors should be aware of informed design practices and encourage students to grow into informed designers by introducing them to new technology, such as generative design. Copyright © 2024 by ASME.
"
10.1016/j.eml.2024.102131,S2352431624000117,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185307555&origin=inward,Article,SCOPUS_ID:85185307555,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"mechagents: large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge","
                  Solving mechanics problems using numerical methods requires comprehensive intelligent capability of retrieving relevant knowledge and theory, constructing and executing codes, analyzing the results, a task that has thus far mainly been reserved for humans. While emerging AI methods can provide effective approaches to solve end-to-end problems, for instance via the use of deep surrogate models or various data analytics strategies, they often lack physical intuition since knowledge is baked into the parametric complement through training, offering less flexibility when it comes to incorporating mathematical or physical insights. By leveraging diverse capabilities of multiple dynamically interacting large language models (LLMs), we can overcome the limitations of conventional approaches and develop a new class of physics-inspired generative machine learning platform, here referred to as MechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for elasticity problems, via autonomous collaborations. A two-agent team can effectively write, execute and self-correct code, in order to apply finite element methods to solve classical elasticity problems in various flavors (different boundary conditions, domain geometries, meshes, small/finite deformation and linear/hyper-elastic constitutive laws, and others). For more complex tasks, we construct a larger group of agents with enhanced division of labor among planning, formulating, coding, executing and criticizing the process and results. The agents mutually correct each other to improve the overall team-work performance in understanding, formulating and validating the solution. Our framework shows the potential of synergizing the intelligence of language models, the reliability of physics-based modeling, and the dynamic collaborations among diverse agents, opening novel avenues for automation of solving engineering problems.
               "
10.1088/1361-6552/ad1fa2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184569156&origin=inward,Article,SCOPUS_ID:85184569156,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the impact of ai in physics education: a comprehensive review from gcse to university levels,"
AbstractView references

With the rapid evolution of artificial intelligence (AI), its potential implications for higher education have become a focal point of interest. This study delves into the capabilities of AI in physics education and offers actionable AI policy recommendations. Using openAI’s flagship gpt-3.5-turbo large language model (LLM), we assessed its ability to answer 1337 physics exam questions spanning general certificate of secondary education (GCSE), A-Level, and introductory university curricula. We employed various AI prompting techniques: Zero Shot, in context learning, and confirmatory checking, which merges chain of thought reasoning with reflection. The proficiency of gpt-3.5-turbo varied across academic levels: it scored an average of 83.4% on GCSE, 63.8% on A-Level, and 37.4% on university-level questions, with an overall average of 59.9% using the most effective prompting technique. In a separate test, the LLM’s accuracy on 5000 mathematical operations was found to be 45.2%. When evaluated as a marking tool, the LLM’s concordance with human markers averaged at 50.8%, with notable inaccuracies in marking straightforward questions, like multiple-choice. Given these results, our recommendations underscore caution: while current LLMs can consistently perform well on physics questions at earlier educational stages, their efficacy diminishes with advanced content and complex calculations. LLM outputs often showcase novel methods not in the syllabus, excessive verbosity, and miscalculations in basic arithmetic. This suggests that at university, there’s no substantial threat from LLMs for non-invigilated physics questions. However, given the LLMs’ considerable proficiency in writing physics essays and coding abilities, non-invigilated examinations of these skills in physics are highly vulnerable to automated completion by LLMs. This vulnerability also extends to pysics questions pitched at lower academic levels. It is thus recommended that educators be transparent about LLM capabilities with their students, while emphasizing caution against overreliance on their output due to its tendency to sound plausible but be incorrect. © 2024 The Author(s). Published by IOP Publishing Ltd.
"
10.1016/j.iswa.2024.200336,S2667305324000127,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184135405&origin=inward,Article,SCOPUS_ID:85184135405,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),claude 2.0 large language model: tackling a real-world classification problem with a new iterative prompt engineering approach,"In the last year, Large Language Models (LLMs) have transformed the way of tackling problems, opening up new perspectives in various works and research fields, due to their ability to generate and understand human languages. In this regard, the recent release of Claude 2.0 has contributed to the processing of more complex prompts. In this scenario, the goal of this paper is to evaluate the effectiveness of Claude 2.0 in a specific classification task. In particular, we considered the Forest cover-type problem, concerning the prediction of a cover-type value according to the geospatial characterization of target worldwide areas. To this end, we propose a novel iterative prompt template engineering approach, which integrates files by exploiting prompts and evaluates the quality of responses provided by the LLM. Moreover, we conducted several comparative analyses to evaluate the effectiveness of Claude 2.0 with respect to online and batch learning models. The results demonstrated that, although some online and batch models performed better than Claude 2.0, the new iterative prompt engineering approach improved the quality of responses, leading to better performance with increases ranging from 14% to 32% in terms of accuracy, precision, recall, and F1-score."
10.1088/2632-2153/ad1af2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182731251&origin=inward,Article,SCOPUS_ID:85182731251,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),discovering interpretable physical models using symbolic regression and discrete exterior calculus,"
AbstractView references

Computational modeling is a key resource to gather insight into physical systems in modern scientific research and engineering. While access to large amount of data has fueled the use of machine learning to recover physical models from experiments and increase the accuracy of physical simulations, purely data-driven models have limited generalization and interpretability. To overcome these limitations, we propose a framework that combines symbolic regression (SR) and discrete exterior calculus (DEC) for the automated discovery of physical models starting from experimental data. Since these models consist of mathematical expressions, they are interpretable and amenable to analysis, and the use of a natural, general-purpose discrete mathematical language for physics favors generalization with limited input data. Importantly, DEC provides building blocks for the discrete analog of field theories, which are beyond the state-of-the-art applications of SR to physical problems. Further, we show that DEC allows to implement a strongly-typed SR procedure that guarantees the mathematical consistency of the recovered models and reduces the search space of symbolic expressions. Finally, we prove the effectiveness of our methodology by re-discovering three models of continuum physics from synthetic experimental data: Poisson equation, the Euler’s elastica and the equations of linear elasticity. Thanks to their general-purpose nature, the methods developed in this paper may be applied to diverse contexts of physical modeling. © 2024 The Author(s). Published by IOP Publishing Ltd.
"
10.1016/j.rineng.2023.101721,S2590123023008484,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182431483&origin=inward,Article,SCOPUS_ID:85182431483,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),from text to tech: shaping the future of physics-based simulations with ai-driven generative models,"This micro-article introduces a method for integrating Large Language Models with geometry/mesh generation software and multiphysics solvers, aimed at streamlining physics-based simulations. Users provide simulation descriptions in natural language, which the language model processes for geometry/mesh generation and physical model definition. Initial results demonstrate the feasibility of this approach, suggesting a future where non-experts can conduct advanced multiphysics simulations by simply describing their needs in natural language, while the code autonomously handles complex tasks like geometry building, meshing, and setting boundary conditions."
10.1016/j.tws.2023.111475,S0263823123009497,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180410571&origin=inward,Article,SCOPUS_ID:85180410571,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),mechanical properties of additively manufactured lattice structures designed by deep learning,"
                  Lattice structures, characterized by their repetitive lightweight cellular forms, enable more effective load distribution compared to solid bodies. Designing lattice structures with tailored mechanical properties remains challenging due to the numerous design variables and their complex relationship with mechanical performance. This paper presents a novel approach employing a deep learning-based Generative Adversarial Network (GAN) model to address this engineering challenge. With its potential for creativity and innovation, GAN provides design diversity that cannot be achieved with traditional design methods or other generative design models. Distinct from previous studies, the GAN training data set consists of lattice structures with improved mechanical properties obtained using parametric design and simulated annealing method. This data set enables the GAN model to create lattice structures with high strength-to-weight ratio. These lattice designs were fabricated using a commercial Material Jetting Additive Manufacturing (MJ-AM) machine, allowing for the production of complex structures. The mechanical performance of the 3D-printed unit cell samples was evaluated through Finite Element Analysis (FEA), compression, and impact testing. The results reveal that the lattice structures generated using the GAN model demonstrated improved mechanical strength (i.e. up to 108 % and 150 % improved strength and elongation performance, respectively). This study shows AI's potential to widen lattice structure design space and create tailored parts with improved mechanical properties. The research also paves the way for future exploration of deep learning techniques in revolutionizing the design and fabrication of parts with tailored mechanical properties.
               "
10.1016/j.iswa.2023.200308,S2667305323001333,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179389129&origin=inward,Article,SCOPUS_ID:85179389129,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt and finetuned bert: a comparative study for developing intelligent design support systems,"Large Language Models (LLMs), like ChatGPT, have sparked considerable interest among researchers across diverse disciplines owing to their remarkable text processing and generation capabilities. While ChatGPT is typically employed for tasks involving general knowledge, researchers increasingly explore the potential of this LLM-based tool in specific domains to enhance productivity. This study aims to compare the performance of a finetuned BERT model with that of ChatGPT on a domain-specific dataset in the context of developing an intelligent design support system. Through experiments conducted on classification and generation tasks, the knowledge transfer and elicitation abilities of ChatGPT are examined and contrasted with those of the finetuned BERT model. The findings indicate that ChatGPT exhibits comparable performance to the finetuned BERT model in sentence-level classification tasks but struggles with short sequences. However, ChatGPT's classification performance significantly improves when a few-shot setting is applied. Moreover, it can filter out unrelated data and enhance dataset quality by assimilating the underlying domain knowledge. Regarding content generation, ChatGPT with a zero-shot setting produces informative and readable output for domain-specific questions, albeit with an excessive amount of unrelated information, which can burden readers. In conclusion, ChatGPT demonstrates a promising potential for application in facilitating data labeling, knowledge transfer, and knowledge elicitation tasks. With minimal guidance, ChatGPT can substantially enhance the efficiency of domain experts in accomplishing their objectives. The findings suggest a nuanced integration of artificial intelligence (AI) with human expertise, bridging the gap from mere classification models to sophisticated human-analogous text generation systems. This signals a future in AI-augmented engineering design where the robust capabilities of AI technologies integrate with human creativity and innovation, creating a dynamic interactions to redefine how we tackle design challenges."
10.1016/j.jrtpm.2023.100429,S2210970623000616,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179128633&origin=inward,Article,SCOPUS_ID:85179128633,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),railroad accident analysis by machine learning and natural language processing,"
                  The evolving complexities of railroad systems also increase their vulnerability to failure from human error. This study compared the outcomes of two workflows that incorporated 11 different machine learning techniques to identify characteristics of railroad operations that are generally associated with human-caused accidents. The first workflow engineered features from the fixed attribute fields of a large railroad accident database and the second applied natural language processing to extract features from the unstructured accident narratives. Both workflows applied a Shapely game-theoretic model to rank the importance of features based on their marginal contribution towards predicting accident cause. Among several interesting findings, some of the most unexpected were that human-caused accidents are generally not associated with high train speeds nor derailment type accidents, and that shoving cars is riskier than pulling cars. Those, and other findings, from this study can inform management decisions, planning, and policies to minimize the risk of human-caused accidents.
               "
10.1016/j.csi.2023.103797,S0920548923000788,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173831572&origin=inward,Article,SCOPUS_ID:85173831572,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sahand 1.0: a new model for extracting information from source code in object-oriented projects,"
                  Providing models that enable developers, architects, and executives to make intelligent decisions about software projects is imperative. Static analyzer tools can extract the information required by developers from the source code to achieve various goals. There are generally two types of tools for analyzing source codes in the software engineering community: commercial tools and open-source tools. Commercial tools provide a limited set of features, pre-built by the tool developers, for extracting information from source code. Open-source tools, while also having limited features, often have fewer features than commercial tools. Extending open-source tools to support new and unembedded features requires a complete knowledge of the programming language used in the tools as well as the architecture used in the modules of tools. Additionally, tool extension requires a complete knowledge of the Abstract Syntax Tree (AST) structure used in those tools. To overcome existing problems, this paper proposes a new model named Sahand, which utilizes a Relational Database Repository (RDP) to store the source code. To this end, utilizing RDP, we propose a new data model to store the source code of the object-oriented projects aiming to extract the required information from it. This new data model provides an infrastructure for developers to extract information from the source code using only the SQL language or its extensions. This model facilitates the extraction of previously inaccessible information, such as the maximum depth of the inheritance tree in the entire project or the maximum method call sequence in the project. The proposed model has been further investigated using two large-scale projects namely JavaParser and Tomcat, consisting of 1802 and 3566 classes, respectively. All source codes and database model used for Sahand are open-source and can be accessed at https://github.com/gClassAcademy/Sahand.
               "
10.1002/smr.2571,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159192862&origin=inward,Article,SCOPUS_ID:85159192862,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),codebert-attack: adversarial attack against source code deep learning models via pre-trained model,"
AbstractView references

Over the past few years, the software engineering (SE) community has widely employed deep learning (DL) techniques in many source code processing tasks. Similar to other domains like computer vision and natural language processing (NLP), the state-of-the-art DL techniques for source code processing can still suffer from adversarial vulnerability, where minor code perturbations can mislead a DL model's inference. Efficiently detecting such vulnerability to expose the risks at an early stage is an essential step and of great importance for further enhancement. This paper proposes a novel black-box effective and high-quality adversarial attack method, namely CodeBERT-Attack (CBA), based on the powerful large pre-trained model (i.e., CodeBERT) for DL models of source code processing. CBA locates the vulnerable positions through masking and leverages the power of CodeBERT to generate textual preserving perturbations. We turn CodeBERT against DL models and further fine-tuned CodeBERT models for specific downstream tasks, and successfully mislead these victim models to erroneous outputs. In addition, taking the power of CodeBERT, CBA is capable of effectively generating adversarial examples that are less perceptible to programmers. Our in-depth evaluation on two typical source code classification tasks (i.e., functionality classification and code clone detection) against the most widely adopted LSTM and the powerful fine-tuned CodeBERT models demonstrate the advantages of our proposed technique in terms of both effectiveness and efficiency. Furthermore, our results also show (1) that pre-training may help CodeBERT gain resilience against perturbations further, and (2) certain pre-training tasks may be beneficial for adversarial robustness. © 2023 John Wiley & Sons Ltd.
"
10.1109/TAI.2022.3229653,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144785990&origin=inward,Article,SCOPUS_ID:85144785990,scopus,2024-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),toward deep generation of guided wave representations for composite materials,"
AbstractView references

Laminated composite materials are widely used in most fields of engineering. Wave propagation analysis plays an essential role in understanding the short-duration transient response of composite structures. The forward physics-based models are utilized to map from elastic properties space to wave propagation behavior in a laminated composite material. Due to the high-frequency, multimodal, and dispersive nature of the guided waves, the physics-based simulations are computationally demanding. It makes property prediction, generation, and material design problems more challenging. In this work, a forward physics-based simulator, such as the stiffness matrix method is utilized to collect group velocities of guided waves for a set of composite materials. A variational autoencoder (VAE)-based deep generative model is proposed for the generation of new and realistic polar group velocity representations. It is observed that the deep generator is able to reconstruct unseen representations with very low mean square reconstruction error. Global Monte Carlo and directional equally spaced samplers are used to sample the continuous, complete, and organized low-dimensional latent space of VAE. The sampled point is fed into the trained decoder to generate new polar representations. The network has shown exceptional generation capabilities. It is also seen that the latent space forms a conceptual space where different directions and regions show inherent patterns related to the generated representations and their corresponding material properties. Impact Statement-AI-Accelerated property prediction, discovery, and design of materials have emerged as a new research front with many promising features. There are many investigations on different materials, but no emphasis is placed on composite materials. Among many challenges, the unavailability of datasets for composite materials is a significant roadblock. This is because conducting multiple experiments is costly and cumbersome, and performing simulations is time-Taking and demands computational resources. In order to accelerate and scale the prediction, discovery, and design, a deep generation approach is proposed for composite materials. The current research requires limited physical simulations to train a deep generator network.The generator can generate enormous data, eliminating the demerits of both experiments and simulations. The work is novel in terms of the deep generation approach as well as the applications for composite materials. © 2022 IEEE.
"
10.1016/j.knosys.2024.111395,S0950705124000303,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183152181&origin=inward,Article,SCOPUS_ID:85183152181,scopus,2024-02-28,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),knowledge-based dynamic prompt learning for multi-label disease diagnosis,"
                  Pretrained language models (PLMs) have been developed rapidly which establish impressive performance on many open-domain downstream tasks. However, conducting these pretrained models directly without additional network architectures on special domain tasks like multi-label disease diagnosis cannot perform well. Recently, prompt learning has been a new paradigm in PLM field which is more convenient and well-performed than the traditional fine-tuning approach for different domain tasks. However, prompt engineering is challenging because it takes time and experience. In this paper, we propose a new prompt learning method named Knowledge-based Dynamic PrompT (KBDPT) to deal with these problems. Firstly, we import medical knowledge into PLMs by prompt templates which make results of the disease diagnosis more reasonable and qualified. Compared with the fine-tuning approach, this method needs fewer trainable parameters and less training data but achieve better performance. Secondly, unlike most existing pre-defined prompt methods, KBDPT dynamically generates prompts based on personal medical information and a large-scale medical knowledge graph, which can provide more valuable guidance information for disease diagnosis. Lastly, the proposed model also ensembles multiple prompts from all possible diseases to introduce more knowledge and obtain differential diagnosis results. Experiments of multi-label disease diagnosis are conduct on three real-world EMR datasets. Results demonstrate that our model can be used in various pretrained models and outperform both classical deep learning methods and fine-tuning PLMs. The source code of our proposed model has been released at: https://github.com/loxs123/KBDPT.
               "
10.1145/3641399.3641403,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186771563&origin=inward,Conference Paper,SCOPUS_ID:85186771563,scopus,2024-02-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),accelerating software development using generative ai: chatgpt case study,"
AbstractView references

The Software Development Life Cycle (SDLC) comprises multiple phases, each requiring Subject Matter Experts (SMEs) with phase-specific skills. The efficacy and quality of deliverables of each phase are skill dependent. In recent times, Generative AI techniques, including Large-scale Language Models (LLMs) like GPT, have become significant players in software engineering. These models, trained on extensive text data, can offer valuable contributions to software development. Interacting with LLMs involves feeding prompts with the context information and guiding the generation of textual responses. The quality of the response is dependent on the quality of the prompt given. This paper proposes a systematic prompting approach based on meta-model concepts for SDLC phases. The approach is validated using ChatGPT for small but complex business application development. We share the approach and our experience, learnings, benefits obtained, and the challenges encountered while applying the approach using ChatGPT. Our experience indicates that Generative AI techniques, such as ChatGPT, have the potential to reduce the skills barrier and accelerate software development substantially. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
10.1145/3641399.3641434,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186766261&origin=inward,Conference Paper,SCOPUS_ID:85186766261,scopus,2024-02-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),legacy software modernization: a journey from non-ai to generative ai approaches,"
AbstractView references

Dealing with ageing software is a reality of the industry, and even open source software systems. This is a great opportunity for the software engineering researchers to apply the traditional techniques of program analysis to solve problems of refactoring and modernization. The generative AI advancements have opened up a whole new world of possibilities for software engineering tasks such as code generation, code translation, bug fixing among others. Industry is keen on exploring scalable solutions for refactoring, automated testing and now automatic code generation. In this tutorial, we aim to (i) provide a background and overview of legacy software modernization and its importance amidst the emergence of AI-Assisted software and Generative AI (ii) discuss the challenges being faced by industry due to monolithic legacy code and systems (iii) introduce architectural and technological paradigms to modernize this legacy or ageing software (iv) highlight the research and engineering problems that remain to be solved in this space discussing the opportunities for the software engineering research community. © 2024 Copyright held by the owner/author(s).
"
10.1145/3641399.3641437,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186762725&origin=inward,Conference Paper,SCOPUS_ID:85186762725,scopus,2024-02-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),workshop report on generative ai-based software engineering,"
AbstractView references

The co-authors have organized and conducting the Generative AI-based Software Engineering workshop, co-located with the 17th Innovations in Software Engineering Conference (ISEC) at Bangalore, India on 22nd Feb. 2024. This report briefly describes the objectives and brief contents of the workshop, and hoping that the execution of the planned contents during the workshop will meet the set objectives. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
10.1145/3641399.3641436,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186757810&origin=inward,Conference Paper,SCOPUS_ID:85186757810,scopus,2024-02-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a report on the sixth workshop on emerging software engineering education (wesee 2024),"
AbstractView references

Software engineering is rapidly adapting to meet the demands of contemporary customers and the challenges posed by relentless technological advancements. A well-prepared and highly competent workforce is crucial to propel this evolution, making it a pivotal element for the successful future of software engineering. To instill the art and science of software engineering across diverse age groups, innovative teaching methods must be introduced at all levels of education dissemination. Software engineering stands out as one of the most dynamic subjects in computer science curricula, spanning both undergraduate and postgraduate levels, given the continuous emergence of new software development process models, methods, and tools. A comprehensive software engineering course should encompass various processes, methods, and tools necessary to support large-scale software systems’ development, operation, and maintenance. Moreover, these courses should significantly emphasize developing the interpersonal and communication skills essential for a well-rounded software engineer. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185511594&origin=inward,Article,SCOPUS_ID:85185511594,scopus,2024-02-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an end-to-end deep learning approach for an indian english repository,"
AbstractView references

A voice recognition system that immediately translates raw audio waveforms into text without the need for separate components for language and acoustic modelling or manually constructed feature engineering is known as end-to-end deep learning for continuous speech recognition. It learns the whole audio to text mapping using a single deep neural network model. Conventional speech systems rely on intricate processing pipelines, but this method is far simpler. An end-to-end model in voice recognition is a simple single model that operates directly on words, subwords, or characters and may be trained from the ground up. This simplifies decoding by doing away with the requirement for both explicit phone modelling and a pronunciation lexicon. Deepspeech was used to construct and test the model, which was designed for Indian English. Additionally, a comparison is made between the results of the bi-directional RNN-based system and the traditional HMM model. With our method, we can quickly get a large amount of heterogeneous data for training due to a number of unique data synthesis techniques and an extremely effective RNN development system that utilises several GPUs. The connectionist temporal classification (CTC) objective function is used to infer the alignments between speech and label sequences, obviating the requirement for pre-generated frame labels. Experiments demonstrate that the RNN-based model has been observed to have equal word error rates (WERs) while also significantly speeding up the decoding process when compared to traditional hybrid HMM based on Kaldi. © 2024 Little Lion Scientific. All rights reserved.
"
10.1145/3634713.3634732,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184283402&origin=inward,Conference Paper,SCOPUS_ID:85184283402,scopus,2024-02-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a demonstration of end-user code customization using generative ai,"
AbstractView references

Producing a variant of code is highly challenging, particularly for individuals unfamiliar with programming. This demonstration introduces a novel use of generative AI to aid end-users in customizing code. We first describe how generative AI can be used to customize code through prompts and instructions, and further demonstrate its potential in building end-user tools for configuring code. We showcase how to transform an undocumented, technical, low-level TikZ into a user-friendly, configurable, Web-based customization tool written in Python, HTML, CSS, and JavaScript and itself configurable. We discuss how generative AI can support this transformation process and traditional variability engineering tasks, such as identification and implementation of features, synthesis of a template code generator, and development of end-user configurators. We believe it is a first step towards democratizing variability programming, opening a path for end-users to adapt code to their needs. © 2024 ACM.
"
10.1145/3597503.3608137,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184270807&origin=inward,Conference Paper,SCOPUS_ID:85184270807,scopus,2024-02-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompting is all you need: automated android bug replay with large language models,"
AbstractView references

Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using the software. As such, researchers have committed considerable resources toward automating bug replay to expedite the process of software maintenance. Nonetheless, the success of current automated approaches is largely dictated by the characteristics and quality of bug reports, as they are constrained by the limitations of manually-crafted patterns and pre-defined vocabulary lists. Inspired by the success of Large Language Models (LLMs) in natural language understanding, we propose AdbGPT, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort. AdbGPT leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer. Our evaluations demonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3% of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines and ablation studies. We also conduct a small-scale user study to confirm the usefulness of AdbGPT in enhancing developers’ bug replay capabilities. © 2024 IEEE Computer Society. All rights reserved.
"
10.4218/etrij.2023-0357,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186143888&origin=inward,Conference Paper,SCOPUS_ID:85186143888,scopus,2024-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),framework for evaluating code generation ability of large language models,"
AbstractView references

Large language models (LLMs) have revolutionized various applications in natural language processing and exhibited proficiency in generating programming code. We propose a framework for evaluating the code generation ability of LLMs and introduce a new metric, (Formula presented.), which captures the granularity of accuracy according to the pass rate of test cases. The framework is intended to be fully automatic to handle the repetitive work involved in generating prompts, conducting inferences, and executing the generated codes. A preliminary evaluation focusing on the prompt detail, problem publication date, and difficulty level demonstrates the successful integration of our framework with the LeetCode coding platform and highlights the applicability of the (Formula presented.) metric. 1225-6463/$ © 2024 ETRI.
"
10.1016/j.jksuci.2023.101865,S1319157823004196,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185289753&origin=inward,Article,SCOPUS_ID:85185289753,scopus,2024-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),enhancing source code retrieval with joint bi-lstm-gnn architecture: a comparative study with chatgpt-llm,"Retrieving relevant source code from large repositories is a significant and ongoing challenge in the field of software engineering, primarily due to the vast and ever-expanding amount of available code. Existing deep learning methods, although effective to some extent, exhibit limitations in capturing the intricate and complex structural information embedded within source code, which hinders their ability to provide highly accurate retrieval results. This study endeavors to tackle this prominent issue by introducing a novel and innovative approach known as the Joint Bi-directional LSTM and Graph Neural Networks (JBLG) model for source code retrieval. The central aim is to harness the combined strengths and capabilities of Bi-directional Long Short-Term Memory (LSTM) networks and Graph Neural Networks (GNNs) to significantly enhance the model’s capacity to capture and interpret the complex structural characteristics intrinsic to source code. The proposed JBLG model employs a unique fusion of Bi-directional LSTM, which excels in capturing sequential and temporal dependencies within code, and GNN, which is adept at modeling the intricate graph structure of the code. By leveraging this hybrid architecture, the model aims to provide a comprehensive and highly effective solution for source code retrieval tasks. To assess the efficacy of the JBLG model, extensive experiments are conducted, and the model’s performance is evaluated against well-established benchmarks, including LSTM, GNN, and ChatGPT, using two diverse datasets: CodeSearchNet and CosBench datasets. These evaluations span multiple programming languages, ensuring a comprehensive and robust assessment of the model’s capabilities. The experimental results indicate that the JBLG model consistently outperforms its counterparts, including Bi-LSTM, GNN, ChatGPT, and DGMS, across various evaluation metrics. the JBLG model showcases an exceptional ability to handle and extract the intricate structural information inherent in source code, resulting in significantly enhanced retrieval accuracy. The JBLG model emerges as a highly promising solution for real-world source code retrieval applications, with the potential to revolutionize the field. The success of this model underscores the importance of combining deep learning techniques like Bi-directional LSTM and GNNs for tackling complex software engineering challenges. Furthermore, future research directions could involve exploring advanced techniques such as attention mechanisms and extending the model’s applicability to other software engineering tasks like code summarization and code completion. The findings of this study are expected to have a lasting impact on the advancement of source code retrieval methodologies."
10.1016/j.dsx.2024.102946,S1871402124000079,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184325913&origin=inward,Article,SCOPUS_ID:85184325913,scopus,2024-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring the potential of chatgpt in the peer review process: an observational study,"
                  Background
                  Peer review is the established method for evaluating the quality and validity of research manuscripts in scholarly publishing. However, scientific peer review faces challenges as the volume of submitted research has steadily increased in recent years. Time constraints and peer review quality assurance can place burdens on reviewers, potentially discouraging their participation. Some artificial intelligence (AI) tools might assist in relieving these pressures. This study explores the efficiency and effectiveness of one of the artificial intelligence (AI) chatbots, ChatGPT (Generative Pre-trained Transformer), in the peer review process.
               
                  Methods
                  Twenty-one peer-reviewed research articles were anonymised to ensure unbiased evaluation. Each article was reviewed by two humans and by versions 3.5 and 4.0 of ChatGPT. The AI was instructed to provide three positive and three negative comments on the articles and recommend whether they should be accepted or rejected. The human and AI results were compared using a 5-point Likert scale to determine the level of agreement. The correlation between ChatGPT responses and the acceptance or rejection of the papers was also examined.
               
                  Results
                  Subjective review similarity between human reviewers and ChatGPT showed a mean score of 3.6/5 for ChatGPT 3.5 and 3.76/5 for ChatGPT 4.0. The correlation between human and AI review scores was statistically significant for ChatGPT 3.5, but not for ChatGPT 4.0.
               
                  Conclusion
                  ChatGPT can complement human scientific peer review, enhancing efficiency and promptness in the editorial process. However, a fully automated AI review process is currently not advisable, and ChatGPT's role should be regarded as highly constrained for the present and near future.
               "
10.1016/j.jbi.2024.104605,S1532046424000236,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184153295&origin=inward,Article,SCOPUS_ID:85184153295,scopus,2024-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ehr-bert: a bert-based model for effective anomaly detection in electronic health records,"Objective: Physicians and clinicians rely on data contained in electronic health records (EHRs), as recorded by health information technology (HIT), to make informed decisions about their patients. The reliability of HIT systems in this regard is critical to patient safety. Consequently, better tools are needed to monitor the performance of HIT systems for potential hazards that could compromise the collected EHRs, which in turn could affect patient safety. In this paper, we propose a new framework for detecting anomalies in EHRs using sequence of clinical events. This new framework, EHR-Bidirectional Encoder Representations from Transformers (BERT), is motivated by the gaps in the existing deep-learning related methods, including high false negatives, sub-optimal accuracy, higher computational cost, and the risk of information loss. EHR-BERT is an innovative framework rooted in the BERT architecture, meticulously tailored to navigate the hurdles in the contemporary BERT method; thus, enhancing anomaly detection in EHRs for healthcare applications. Methods: The EHR-BERT framework was designed using the Sequential Masked Token Prediction (SMTP) method. This approach treats EHRs as natural language sentences and iteratively masks input tokens during both training and prediction stages. This method facilitates the learning of EHR sequence patterns in both directions for each event and identifies anomalies based on deviations from the normal execution models trained on EHR sequences. Results: Extensive experiments on large EHR datasets across various medical domains demonstrate that EHR-BERT markedly improves upon existing models. It significantly reduces the number of false positives and enhances the detection rate, thus bolstering the reliability of anomaly detection in electronic health records. This improvement is attributed to the model’s ability to minimize information loss and maximize data utilization effectively. Conclusion: EHR-BERT showcases immense potential in decreasing medical errors related to anomalous clinical events, positioning itself as an indispensable asset for enhancing patient safety and the overall standard of healthcare services. The framework effectively overcomes the drawbacks of earlier models, making it a promising solution for healthcare professionals to ensure the reliability and quality of health data."
10.1007/s00146-023-01771-5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178935176&origin=inward,Article,SCOPUS_ID:85178935176,scopus,2024-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards a decolonial i in ai: mapping the pervasive effects of artificial intelligence on the art ecosystem,"
AbstractView references

This paper delves into the intricate relationship between Artificial Intelligence (AI) and the art ecosystem, emphasizing the need for a decolonizing approach in the face of AI's growing influence. It argues that the development of AI is not just a technological leap but also a significant cultural and societal moment, akin to the advent of moving images that Walter Benjamin famously analyzed. The paper examines how AI, particularly in its current oligarchical and corporate-driven form, perpetuates and magnifies the existing social inequalities, thereby necessitating a critical and radical rethinking of its role in society and the arts. At the heart of the discussion is the concept of AI as a broad term encompassing various forms of machine intelligence, from natural language processing to computer vision. The paper criticizes the dominant anthropocentric view of intelligence and creativity, proposing a more inclusive approach that considers the diverse forms of intelligence present in other species and potentially in AI itself. It underscores the role of AI in shaping the art ecosystem, not just in the creative process but also in gatekeeping and decision-making. The paper proposes a framework for decolonizing AI in the art ecosystem, focusing on four key tasks: recognizing access as a form of power, understanding and addressing biases inherent in AI, assessing the impact of AI on marginalized communities, and challenging dominant narratives and epistemologies to create space for alternative voices and perspectives. It emphasizes the need for artists and the art community to engage actively with AI, shaping its development towards more equitable and just outcomes. In conclusion, the paper calls for a radical reimagination of AI's role in society and the arts, advocating for a future where AI is not just about technological advancement but also about fostering a more inclusive, equitable, and creatively diverse world. It invites artists, thinkers, and innovators to join in this journey of reimagining and reshaping the future of AI and the art ecosystem. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2023.
"
10.1016/j.compbiolchem.2023.107977,S1476927123001688,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177853749&origin=inward,Article,SCOPUS_ID:85177853749,scopus,2024-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),named entity recognition of rice genes and phenotypes based on bigru neural networks,"
                  Named Entity Recognition (NER) is a fundamental but crucial task in natural language processing (NLP) and big data analysis, with wide application range. NER for rice genes and phenotypes is a technique to identify genes and phenotypes from a large amount of text. NER for rice genes and phenotypes can facilitate the acquisition of information in the field of crops and provide references for our research on higher quality crops. At the same time, named entity recognition still faces many challenges. In this paper, we propose an improved bidirectional gated recurrent unit neural network (BI-GRU) method, which is used to automatically identify the required entities (i.e. gene names, rice phenotypes) from relevant rice literature and patents. The neural network model is combined with the Softmax function to directly output the probabilities of labels, forming the BI-GRU-SF model. With the ability of deep learning methods, the semantic information in the context can be learned without the need for feature engineering. Finally, we conducted experiments, and the results showed that our proposed model provided better performance compared to other models. All datasets and resource codes of BI-GRU-SF are available at https://github.com/qqeeqq/NER for academic use.
               "
10.1007/s11217-023-09907-2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177023280&origin=inward,Article,SCOPUS_ID:85177023280,scopus,2024-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"artificial intelligence and the aims of education: makers, managers, or inforgs?","
AbstractView references

The recent appearance of generative artificial intelligence (AI) platforms has been seen by many as disruptive for education. In this paper I attempt to locate the source of tension between educational goals and new information technologies including AI. I argue that this tension arises from new conceptions of epistemic agency that are incompatible with educational aims. I describe three competing theories of epistemic agency which I refer to as Makers, Managers, and Inforgs. I contend that educators are correct in maintaining the first of these, which is rooted in the educational theories of Locke and Dewey, as their main educational purpose. Competing theories do not serve the goals of learners, even as they must prepare for life in a very different epistemic environment. © 2023, The Author(s), under exclusive licence to Springer Nature B.V.
"
10.1007/s10207-023-00769-w,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174924556&origin=inward,Conference Paper,SCOPUS_ID:85174924556,scopus,2024-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cyber threat assessment and management for securing healthcare ecosystems using natural language processing,"
AbstractView references

The healthcare sectors have constantly faced significant challenge due to the rapid rise of cyber threats. These threats can pose any potential risk within the system context and disrupt the critical healthcare service delivery. It is therefore necessary for the healthcare organisations to understand and tackle the threats to ensure overall security and resilience. However, threats are continuously evolved and there is large amount of unstructured security-related textual information is available. This makes the threat assessment and management task very challenging. There are a number of existing works that consider Machine Learning models for detection and prediction of cyber attack but they lack of focus on the Natural Language Processing (NLP) to extract the threat information from unstructured security-related text. To this end, this work proposes a novel method to assess and manage threats by adopting natural language processing. The proposed method has been tailored for the healthcare ecosystem and allows to identify and assess the possible threats within healthcare information infrastructure so that appropriate control and mitigation actions can be taken into consideration to tackle the threat. In detail, NLP techniques are used to extract the useful threat information related to specific assets of the healthcare ecosystems from the largely available security-related information on Internet (e.g. cyber security news), to evaluate the level of the identified threats and to select the required mitigation actions. We have performed experiments on real healthcare ecosystems in Fraunhofer Institute for Biomedical Engineering, considering in particular three different healthcare scenarios, namely implantable medical devices, wearables, and biobank, with the purpose of demonstrating the feasibility of our approach, which is able to provide a realistic manner to identify and assess the threats, evaluate the threat level and suggest the required mitigation actions. © 2023, The Author(s).
"
10.1016/j.inffus.2023.102038,S1566253523003548,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173490449&origin=inward,Article,SCOPUS_ID:85173490449,scopus,2024-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),semantic understanding and prompt engineering for large-scale traffic data imputation,"
                  Intelligent Transportation Systems (ITS) face the formidable challenge of large-scale missing data, particularly in the imputation of traffic data. Existing studies have mainly relied on modeling network-level spatiotemporal correlations to address this issue. However, these methods often overlook the rich semantic information (e.g., road infrastructure, sensor location, etc.) inherent in road networks when capturing network-wide spatiotemporal correlations. We address this limitation by presenting the Graph Transformer-based Traffic Data Imputation (GT-TDI) model, which imputes missing values in extensive traffic data by leveraging spatiotemporal semantic understanding of road networks. The proposed model leverages semantic descriptions that capture the spatial and temporal dynamics of traffic across road networks, enhancing its capacity to infer comprehensive spatiotemporal relationships. Moreover, to augment the model’s capabilities, we employ a Large Language Model (LLM) and prompt engineering to enable natural and intuitive interactions with the traffic data imputation system, allowing users to query and request in plain language, without requiring expert knowledge or complex mathematical models. The proposed model, GT-TDI, utilizes Graph Neural Networks (GNN) and Transformer architectures to perform large-scale traffic data imputation using deficient observations, sensor social connectivity, and semantic descriptions as inputs. We evaluate the GT-TDI model on the PeMS freeway dataset and benchmark it against cutting-edge models. The experimental evidence demonstrates that GT-TDI surpasses the cutting-edge approaches in scenarios with intricate patterns and varying rates of missing data.
               "
10.1007/s11263-023-01891-x,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171365193&origin=inward,Article,SCOPUS_ID:85171365193,scopus,2024-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),clip-adapter: better vision-language models with feature adapters,"
AbstractView references

Large-scale contrastive vision-language pretraining has shown significant progress in visual representation learning. Unlike traditional visual systems trained by a fixed set of discrete labels, a new paradigm was introduced in Radford et al. (International conference on machine learning, PMLR, 2021) to directly learn to align images with raw texts in an open-vocabulary setting. On downstream tasks, a carefully chosen text prompt is employed to make zero-shot predictions. To avoid non-trivial prompt engineering, context optimization (Zhou et al. in Int J Comput Vis 130(9):2337–2348, 2022) has been proposed to learn continuous vectors as task-specific prompts with few-shot training examples. In this paper, we show that there is an alternative path to achieve better vision-language models other than prompt tuning. While prompt tuning is for the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with feature adapters on either visual or language branch. Specifically, CLIP-Adapter adopts an additional bottleneck layer to learn new features and performs residual-style feature blending with the original pretrained features. As a consequence, CLIP-Adapter is able to outperform context optimization while maintaining a simple design. Experiments and extensive ablation studies on various visual classification tasks demonstrate the effectiveness of our approach. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
"
10.1109/TETCI.2023.3300176,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166782398&origin=inward,Article,SCOPUS_ID:85166782398,scopus,2024-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),multiplierless implementation of fitz-hugh nagumo (fhn) modeling using cordic approach,"
AbstractView references

The study, simulation, and implementation of neural behavior in the human brain are central goals of neuromorphic engineering. By integrating various scientific fields, we present a hardware solution based on neuronal cell mechanisms that can emulate such a nature-inspired system. This article presents a Fitz-Hugh Nagumo (FHN) neuron implemented using COordinate Rotation DIgital Computer (CORDIC), which accurately reproduces various patterns of the original FHN neuron model. We propose a modification to the original nonlinear term using a CORDIC IP-Core, resulting in high matching accuracy and low computational error. The proposed model is validated through time domain and dynamic analysis, which demonstrates its high accuracy and low error in reproducing all features of the FHN model. For large scale neuron implementations, we present an efficient digital hardware solution based on the resource sharing techniques. The hardware is implemented on Field-Programmable Gate Array (FPGA) using Hardware Description Language (HDL), as a proof of concept. The results from the hardware implementation show that the proposed model uses only 1% of the resources available on a Virtex 4 FPGA board. Additionally, the static timing analysis shows that the circuit can operate at a maximum frequency of 320 MHz. © 2017 IEEE.
"
10.1007/s11042-023-16196-x,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165155893&origin=inward,Article,SCOPUS_ID:85165155893,scopus,2024-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),convolutional long short-term memory-based approach for deepfakes detection from videos,"
AbstractView references

The great development in the area of Artificial Intelligence (AI) has introduced tremendous advancements in information technology. Moreover, the introduction of lightweight machine learning (ML) techniques allows the applications to work with limited storage and processing power. Deepfakes is among the most famous type of such applications of this era which generates a large amount of fake and modified audiovisual data. The creation of such fake data has introduced a serious risk to the security and confidentiality of humans all around the globe. Accurate detection and classification of actual and deepfakes content is a challenging task due to the progression of Generative adversarial networks (GANs) which produce such convincing manipulated content that it’s impossible for people to recognize it through their naked eyes. In this work, we have presented deep learning (DL)-based approach namely the convolutional long short-term memory (C-LSTM) method for deepfakes detection from videos. More specifically, the spatial information from the input sample is calculated by employing various pre-trained models like VGG16, VGG19, ResNet50, XceptionNet, and GoogleNet, DenseNet. Further, we have proposed a novel feature descriptor called the Dense-Swish-Net121. Whereas the Bi-LSTM model is utilized to compute the temporal information. Lastly, the results are predicted based on both the frame level and temporal level information to make the final decision. A detailed comparison of all CNN models with the Bi-LSTM approach is performed and has confirmed through the reported results that the proposed Dense-Swish-Net121 with Bi-LSTM approach performs well for deepfakes detection. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
"
10.1007/s10270-023-01093-6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149997186&origin=inward,Article,SCOPUS_ID:85149997186,scopus,2024-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),simima: a virtual simulink intelligent modeling assistant: simulink intelligent modeling assistance through machine learning and model clones,"
AbstractView references

Intelligent virtual model assistance is a key challenge in cultivating model-driven engineering proliferation and growth. Such assistance will help improve the quality of software models, support education for students learning modeling, and lower the entry barriers to new modelers. We present SimIMA, an intelligent modeling assistant for Simulink, which is an extremely popular modeling language in both industry and academia. SimIMA provides modelers with two different forms of data-driven guidance using a knowledge base of configurable repositories and sources. The first form of guidance, SimGESTION, suggests to modelers single-step operations they can perform on their models as they edit them in their modeling environment. These suggestions are based on the machine learning technique of ensemble learning through association rule mining and frequency classification. The second form of guidance, SimXAMPLE, presents modelers with similar/related Simulink systems for modelers to either insert directly into their environments or to view for inspiration. SimXAMPLE accomplishes this through model clone detection. To validate SimIMA, we conduct experiments using an established, open, and curated large set of Simulink models coming from a variety of application domains. Our results show that both of SimIMA’s forms of guidance are inferring the appropriate model and element suggestions given SimIMA’s knowledge base and that SimIMA is both scalable and efficient. Through our evaluation, SimIMA demonstrates a prediction accuracy of 78.86% for block-level suggestions and 82.04% for full system suggestions. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2023.
"
10.1063/5.0194271,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184593259&origin=inward,Conference Paper,SCOPUS_ID:85184593259,scopus,2024-01-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),application of generative design and additive manufacturing for scaffold fabrication,"
AbstractView references

Scaffold design and fabrication plays an important role in tissue engineering. Conventional manufacturing processes for fabrication of scaffolds have limitations with respect to the controlled architecture and resolution required. The issue can be alleviated by using additive manufacturing (AM) technology. AM is a layer-by-layer fabrication technique which can fabricate complex structures with adequate resolutions. This characteristic of the process is suitable for scaffold fabrication. Moreover, the design aspect of the scaffolds such as pore size, density and structure also significantly influence the performance of the scaffold. Generative design is an AI based approach to design features with improved characteristics. Utilizing this approach to design scaffold is a subject of study which needs to be investigated. The present papers discuss about the various manufacturing techniques available for scaffold fabrication and compares them with the additive manufacturing techniques. Moreover, prospects of scaffold design using generative approach is also discussed along with the desired biocompatible properties required for scaffolding. © 2024 Author(s).
"
10.1145/3636243.3636263,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182943874&origin=inward,Conference Paper,SCOPUS_ID:85182943874,scopus,2024-01-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),more than meets the ai: evaluating the performance of gpt-4 on computer graphics assessment questions,"
AbstractView references

Recent studies have showcased the exceptional performance of LLMs (Large Language Models) on assessment questions across various discipline areas. This can be helpful if used to support the learning process, for example by enabling students to quickly generate and contrast alternative solution approaches. However, concerns about student over-reliance and inappropriate use of LLMs in education are common. Understanding the capabilities of LLMs is essential for instructors to make informed decisions on question choices for learning and assessment tasks. In CS (Computer Science), previous evaluations of LLMs have focused on CS1 and CS2 questions, and little is known about how well LLMs perform for assessment questions in upper-level CS courses such as CG (Computer Graphics), which covers a wide variety of concepts and question types. To address this gap, we compiled a dataset of past assessment questions used in a final-year undergraduate course about introductory CG, and evaluated the performance of GPT-4 on this dataset. We also classified assessment questions and evaluated the performance of GPT-4 for different types of questions. We found that the performance tended to be best for simple mathematical questions, and worst for questions requiring creative thinking, and those with complex descriptions and/or images. We share our benchmark dataset with the community and provide new insights into the capabilities of GPT-4 in the context of CG courses. We highlight opportunities for teaching staff to improve student learning by guiding the use of LLMs for CG questions, and inform decisions around question choices for assessment tasks. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
10.1145/3624724,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184761342&origin=inward,Article,SCOPUS_ID:85184761342,scopus,2024-01-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),talking about large language models,"
AbstractView references

Interacting with a contemporary LLM-based conversational agent can create an illusion of being in the presence of a thinking creature. Yet, in their very nature, such systems are fundamentally not like us. © 2024 Association for Computing Machinery. All rights reserved.
"
10.1136/jnis-2023-021163,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183414995&origin=inward,Article,SCOPUS_ID:85183414995,scopus,2024-01-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),assessing the clinical reasoning of chatgpt for mechanical thrombectomy in patients with stroke,"
AbstractView references

Background Artificial intelligence (AI) has become a promising tool in medicine. ChatGPT, a large language model AI Chatbot, shows promise in supporting clinical practice. We assess the potential of ChatGPT as a clinical reasoning tool for mechanical thrombectomy in patients with stroke. Methods An internal validation of the abilities of ChatGPT was first performed using artificially created patient scenarios before assessment of real patient scenarios from the medical center's stroke database. All patients with large vessel occlusions who underwent mechanical thrombectomy at Tulane Medical Center between January 1, 2022 and December 31, 2022 were included in the study. The performance of ChatGPT in evaluating which patients should undergo mechanical thrombectomy was compared with the decisions made by board-certified stroke neurologists and neurointerventionalists. The interpretation skills, clinical reasoning, and accuracy of ChatGPT were analyzed. Results 102 patients with large vessel occlusions underwent mechanical thrombectomy. ChatGPT agreed with the physician's decision whether or not to pursue thrombectomy in 54.3% of the cases. ChatGPT had mistakes in 8.8% of the cases, consisting of mathematics, logic, and misinterpretation errors. In the internal validation phase, ChatGPT was able to provide nuanced clinical reasoning and was able to perform multi-step thinking, although with an increased rate of making mistakes. Conclusion ChatGPT shows promise in clinical reasoning, including the ability to factor a patient's underlying comorbidities when considering mechanical thrombectomy. However, ChatGPT is prone to errors as well and should not be relied on as a sole decision-making tool in its present form, but it has potential to assist clinicians with more efficient work flow. © Author(s) (or their employer(s)) 2024. No commercial re-use. See rights and permissions. Published by BMJ.
"
10.1145/3633053.3633057,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182731336&origin=inward,Conference Paper,SCOPUS_ID:85182731336,scopus,2024-01-05,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),incorporating generative ai into software development education,"
AbstractView references

This paper explores how Generative AI can be incorporated into software development education. We present examples of formative and summative assessments, which explore various aspects of ChatGPT, including its coding capabilities, its ability to construct arguments as well as ethical issues of using ChatGPT and similar tools in education and the workplace. Our work is inspired by the insights from surveys that show that the learners on our Degree Apprenticeship Programme have a great interest in learning about and exploiting emerging AI technology. Similarly, our industrial partners have a clear interest for their employees to be formally prepared to use GenAI in their software engineering roles. In this vein, it is proposed that embedding the use of GenAI tools in a careful and creative way-by developing assessments which encourage learners to critically evaluate AI output-can be beneficial in helping learners understand the subject material being taught without the risk of the AI tools ""doing the homework"". © 2024 Owner/Author.
"
10.1109/ACCESS.2024.3381619,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85189184702&origin=inward,Article,SCOPUS_ID:85189184702,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an optimized lstm-based augmented language model (flstm-alm) using fox algorithm for automatic essay scoring prediction,"
AbstractView references

The computer-based Automated Essay Scoring (AES) system automatically marks or scores student replies by considering relevant criteria. The methodology, which systematically categorizes writing quality, can increase operational effectiveness in academic and major commercial institutions. To study the projected score, AES relies on extracting numerous aspects from the student’s response, including grammatical and textural information. However, the recovered features may result in dimensionality reduction and a challenging-to-understand feature selection procedure. As the number of parameters rises, the model also demands a large cost for processing and training the data. However, these problems worsen the accuracy of score prediction as a whole and widen the gap between actual and anticipated results. This study suggested the Fox-optimized Long Short-Term Memory-based Augmented Language Model (FLSTM-ALM) as a solution to these problems for giving successful training to text features; the model uses an augmented learning paradigm. The retrieval score was then analyzed and generated using a neural knowledge encoder and retriever. The neural model successfully classifies the output based on this score. The best features are then chosen using the fox optimization algorithm based on the food-searching category. This choice of parameters solves the exploration and optimization issue with document classification. The performance of the optimized AES system was assessed using the two datasets, ASAP and ETS, and it demonstrated a high accuracy of 98.92% and a low error rate of 0.096%. Dimensionality reduction can thus be fixed by optimizing the FLSTM-ALM model with an appropriate meta-heuristic method, such as the FOX algorithm, which raises the predicted accuracy, recall, and f1 score for the AES model. Authors
"
10.1109/ACCESS.2024.3381611,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85189172430&origin=inward,Article,SCOPUS_ID:85189172430,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),privacy and security concerns in generative ai: a comprehensive survey,"
AbstractView references

Generative Artificial Intelligence (GAI) has sparked a transformative wave across various domains, including machine learning, healthcare, business, and entertainment, owing to its remarkable ability to generate lifelike data. This comprehensive survey offers a meticulous examination of the privacy and security challenges inherent to GAI. It provides five pivotal perspectives essential for a comprehensive understanding of these intricacies. The paper encompasses discussions on GAI architectures, diverse generative model types, practical applications, and recent advancements within the field. In addition, it highlights current security strategies and proposes sustainable solutions, emphasizing user, developer, institutional, and policymaker involvement. Authors
"
10.1109/ACCESS.2024.3381535,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85189143270&origin=inward,Article,SCOPUS_ID:85189143270,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),advanced pigmented facial skin analysis using conditional generative adversarial networks,"
AbstractView references

In recent years, artificial intelligence (AI) approaches in computer vision and medical technology have been combined to create various convenient and accurate tools to assist medical treatments. In this work, we propose conditional generative adversarial networks (conditional GANs)-based pigmented facial skin analysis system for melasma diagnosis. In the past, melasma diagnosis was based on subjective diagnoses from doctors, and there were few automatic melasma analysis methods. The proposed system helps to determine the region according to the melasma’s severity. Areas associated with melasma and hemoglobin are detected to determine whether they may require special treatments. Furthermore, the proposed work cooperates with HUANGDERM dermatology to collect a facial skin pigmented dataset. We divide the dataset into 3,000 groups for training datasets and 678 groups for testing. Each group contains four categories of images: standard white light, polarized light, melanin and hemoglobin distribution. As a result, the proposed system successfully generates melasma and hemoglobin images and performs well with respect to subjective and objective evaluations. Authors
"
10.1061/9780784485262.011,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188724470&origin=inward,Conference Paper,SCOPUS_ID:85188724470,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),preliminary study: use of large generative artificial intelligence models in integrated project management,"
AbstractView references

Artificial Intelligence (AI) and Machine Learning (ML) have been embraced techniques in various fields including construction processes and materials, yet rarely applied in Integrated Project Management (IPM). IPM using Earned Value Management (EVM) systems is the grouping of the project management processes to ensure they operate in sync for the success of the project. This paper focuses on exploring the potential use of Open AI's powerful tool, Generative Pre-trained Transformer (ChatGPT) in IPM using earned value management systems. The authors survey industry practitioners to identify the capabilities, limitations, and implications of the use of ChatGPT and similar large generative AI models in these fields, specifically for project management field practitioners. The preliminary survey result shows that there are several considerations and limitations to address when applying it in the field. This paper contributes to researchers and professionals in assisting in the use of such a tool with caution aiming to improve EVM data analysis and the overall EVM profession. © 2024 ASCE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188704935&origin=inward,Conference Paper,SCOPUS_ID:85188704935,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),style vectors for steering generative large language models,"
AbstractView references

This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems. © 2024 Association for Computational Linguistics.
"
10.1007/978-3-031-53227-6_14,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188687773&origin=inward,Conference Paper,SCOPUS_ID:85188687773,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompt patterns for agile software project managers: first results,"
AbstractView references

In the evolving field of Agile Project Management (APM), the role of the project manager is in transition. This paper identifies common ‘pain points’ in APM through a literature review and constructs a theoretical model to address them. The study introduces ‘Prompt Engineering’ as a novel approach to leverage artificial intelligence (AI), specifically ChatGPT, for mitigating these challenges. Empirical research evaluates ChatGPT's capabilities and reliability in managing various project tasks using engineered prompts. The findings suggest that while ChatGPT cannot fully replace human project managers, it excels in assisting, guiding, and automating specific tasks when guided by well-crafted prompts. As an outcome, prompt engineering patterns for project managers is proposed to facilitate the application of AI in agile settings. In this paper, we introduce patterns for requirements management, stakeholder and management teams and role clarification. The paper concludes that ChatGPT's knowledge is generally reliable but emphasizes the need for expert evaluation in critical areas. © The Author(s) 2024.
"
10.1108/MIP-10-2023-0526,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188582458&origin=inward,Article,SCOPUS_ID:85188582458,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt’s applications in marketing: a topic modeling approach,"
AbstractView references

Purpose: ChatGPT is a versatile technology with practical use cases spanning many professional disciplines including marketing. Being a recent innovation, however, there is a lack of academic insight into its tangible applications in the marketing realm. To address this gap, the current study explores ChatGPT’s application in marketing by mining social media data. Additionally, the study employs the stages-of- growth model to assess the current state of ChatGPT’s adoption in marketing organizations. Design/methodology/approach: The study collected tweets related to ChatGPT and marketing using a web-scraping technique (N = 23,757). A topic model was trained on the tweet corpus using latent Dirichlet allocation to delineate ChatGPT’s major areas of applications in marketing. Findings: The topic model produced seven latent topics that encapsulated ChatGPT’s major areas of applications in marketing including content marketing, digital marketing, search engine optimization, customer strategy, B2B marketing and prompt engineering. Further analyses reveal the popularity of and interest in these topics among marketing practitioners. Originality/value: The findings contribute to the literature by offering empirical evidence of ChatGPT’s applications in marketing. They demonstrate the core use cases of ChatGPT in marketing. Further, the study applies the stages-of-growth model to situate ChatGPT’s current state of adoption in marketing organizations and anticipate its future trajectory. © 2024, Emerald Publishing Limited.
"
10.1080/14703297.2024.2316716,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188573532&origin=inward,Article,SCOPUS_ID:85188573532,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),field courses for dummies: to what extent can chatgpt design a higher education field course?,"
AbstractView references

We tested whether ChatGPT can play a role in designing field courses in higher education. In collaboration with ChatGPT, we developed two field courses; the first aimed at creating a completely new field trip, while the second was tailored to fit an existing university module, and then compared to the human module design. From our case studies, several insights emerged. These include the importance of precise prompt engineering and the need to iterate and refine prompts to achieve optimal results. We outlined a workflow for effective prompt engineering, emphasising clear objectives, sequential prompting, and feedback loops. We also identify best practices, including highlighting the importance of collaborating with human expertise, validating AI suggestions, and integrating adaptive management for continual refinement. While ChatGPT is a potent tool with the potential to save a significant amount of time and effort in field course design, human expertise remains indispensable for achieving optimal results. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.
"
10.1007/s00766-024-00416-3,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188521670&origin=inward,Article,SCOPUS_ID:85188521670,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),improving requirements completeness: automated assistance through large language models,"
AbstractView references

Natural language (NL) is arguably the most prevalent medium for expressing systems and software requirements. Detecting incompleteness in NL requirements is a major challenge. One approach to identify incompleteness is to compare requirements with external sources. Given the rise of large language models (LLMs), an interesting question arises: Are LLMs useful external sources of knowledge for detecting potential incompleteness in NL requirements? This article explores this question by utilizing BERT. Specifically, we employ BERT’s masked language model to generate contextualized predictions for filling masked slots in requirements. To simulate incompleteness, we withhold content from the requirements and assess BERT’s ability to predict terminology that is present in the withheld content but absent in the disclosed content. BERT can produce multiple predictions per mask. Our first contribution is determining the optimal number of predictions per mask, striking a balance between effectively identifying omissions in requirements and mitigating noise present in the predictions. Our second contribution involves designing a machine learning-based filter to post-process BERT’s predictions and further reduce noise. We conduct an empirical evaluation using 40 requirements specifications from the PURE dataset. Our findings indicate that: (1) BERT’s predictions effectively highlight terminology that is missing from requirements, (2) BERT outperforms simpler baselines in identifying relevant yet missing terminology, and (3) our filter reduces noise in the predictions, enhancing BERT’s effectiveness for completeness checking of requirements. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.
"
10.1177/00222429231224748,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188503942&origin=inward,Article,SCOPUS_ID:85188503942,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the caring machine: feeling ai for customer care,"
AbstractView references

Customer care is important for its role in relationship building. This role has traditionally been performed by human customer agents; however, the emergence of interactive generative AI (GenAI) shows potential for using AI for customer care in emotionally charged interactions. Bridging practice and the academic literatures in marketing and computer science, this article develops an AI-enabled customer care journey, from accurate emotion recognition to empathetic response, emotional management support, and, finally, the establishment of an emotional connection. Marketing requirements for each of the stages are derived from in-depth interviews with top managers and a survey of chief marketing officers. By juxtaposing these requirements against the current feeling capabilities of GenAI, the authors highlight the technological challenges engineers must tackle. The article concludes with a set of marketing tenets for implementing and researching the caring machine. These include verifying emotion recognition accuracy using marketing emotion theories through multiple emotion signals and methods, utilizing prompt engineering to enhance GenAI’s emotion understanding, employing “response engineering” to personalize emotion management recommendations, and strategically deploying GenAI for emotional connection to simultaneously enhance customer emotional well-being and customer lifetime value. © The Author(s) 2024.
"
10.1109/TLT.2024.3378306,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188460123&origin=inward,Article,SCOPUS_ID:85188460123,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),write-curate-verify: a case study of leveraging generative ai for scenario writing in scenario-based learning,"
AbstractView references

This case study explored the use of generative artificial intelligence (GenAI), specifically ChatGPT, in writing scenarios for scenario-based learning (SBL). Our research addressed three key questions: (a) how do teachers leverage GenAI to write scenarios for SBL purposes? (b) What is the quality of GenAI-generated SBL scenarios and tasks? (c) How does GenAI-supported SBL affect students’ motivation, learning performance, and learning perceptions? A 3-step prompting engineering process (Write the prompts, Curate the output, and Verify the output, WCV) was established during the teacher interaction with GenAI in the scenario writing. Findings revealed that by using the WCV approach, ChatGPT enabled the efficient creation of quality scenarios for SBL purpose in a short timeframe. Moreover, students exhibited increased intrinsic motivation, learning performance and positive attitudes toward GenAI-supported scenarios. We also suggest guidelines for using the WCV prompt engineering process in scenario writing. IEEE
"
10.1007/s40192-024-00344-8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188054796&origin=inward,Article,SCOPUS_ID:85188054796,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),high-throughput extraction of phase–property relationships from literature using natural language processing and large language models,"
AbstractView references

Consolidating published research on aluminum alloys into insights about microstructure–property relationships can simplify and reduce the costs involved in alloy design. One critical design consideration for many heat-treatable alloys deriving superior properties from precipitation are phases as key microstructure constituents because they can have a decisive impact on the engineering properties of alloys. Here, we present a computational framework for high-throughput extraction of phases and their impact on properties from scientific papers. Our framework includes transformer-based and large language models to identify sentences with phase-property information in papers, recognize phase and property entities, and extract phase-property relationships and their “sentiment.” We demonstrate the application of our framework on aluminum alloys, for which we build a database of 7,675 phase–property relationships extracted from a corpus of almost 5000 full-text papers. We comment on the extracted relationships based on common metallurgical knowledge. © The Author(s) 2024.
"
10.1080/23311916.2024.2324614,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188052048&origin=inward,Article,SCOPUS_ID:85188052048,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring the health care system’s representation in the media through hierarchical topic modeling,"
AbstractView references

As a large social structure, the health care system is often reflected in media publications. This creates a significant impact on society’s attitude towards the system and the state in general. In order to predict and correct state policies, media actions, and identify media shortcomings, it is necessary to analyze the image portrayed by the media and the public’s attitude towards it. In this article, we present the results of a multidirectional analysis of a corpus of media publications related to health care. We propose a method for analyzing the information image of health care formed by the mass media based on a topic model of a text corpus. The method evaluates reader interest in various healthcare topics, the dynamics of changes in publication sentiment, and the main information trends. The article presents the results of analyzing a corpus of mass media publications in Kazakhstan from January 2020 to January 2023. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.
"
10.1109/ACCESS.2024.3375882,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187996490&origin=inward,Article,SCOPUS_ID:85187996490,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),devising and detecting phishing emails using large language models,"
AbstractView references

AI programs, built using large language models, make it possible to automatically create phishing emails based on a few data points about a user. The V-Triad is a set of rules for manually designing phishing emails to exploit our cognitive heuristics and biases. In this study, we compare the performance of phishing emails created automatically by GPT-4 and manually using the V-Triad. We also combine GPT-4 with the V-Triad to assess their combined potential. A fourth group, exposed to generic phishing emails, was our control group. We sent emails to 112 participants recruited for the study. The control group emails received a click-through rate between 19-28%, the GPT-generated emails 30-44%, emails generated by the V-Triad 69-79%, and emails generated by GPT and the V-Triad 43-81%. Each participant was asked to explain why they pressed or did not press a link in the email. These answers often contradict each other, highlighting the importance of personal differences. Next, we used four popular large language models (GPT, Claude, PaLM, and LLaMA) to detect the intention of phishing emails and compare the results to human detection. The language models demonstrated a strong ability to detect malicious intent, even in non-obvious phishing emails. They sometimes surpassed human detection, although often being slightly less accurate than humans. Finally, we make an analysis of the economic aspects of AI-enabled phishing attacks, showing how large language models increase the incentives of phishing and spear phishing by reducing their costs. Authors
"
10.1109/TSE.2024.3368208,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187981851&origin=inward,Article,SCOPUS_ID:85187981851,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"software testing with large language models: survey, landscape, and vision","
AbstractView references

Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing. IEEE
"
10.1007/s10270-023-01142-0,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187920481&origin=inward,Article,SCOPUS_ID:85187920481,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"identifying and fixing ambiguities in, and semantically accurate formalisation of, behavioural requirements","
AbstractView references

To correctly formalise requirements expressed in natural language, ambiguities must first be identified and then fixed. This paper focuses on behavioural requirements (i.e. requirements related to dynamic aspects and phenomena). Its first objective is to show, based on a practical, public case study, that the disambiguation process cannot be fully automated: even though natural language processing (NLP) tools and machine learning might help in the identification of ambiguities, fixing them often requires a deep, application-specific understanding of the reasons of being of the system of interest, of the characteristics of its environment, of which trade-offs between conflicting objectives are acceptable, and of what is achievable and what is not; it may also require arduous negotiations between stakeholders. Such an understanding and consensus-making ability is not in the reach of current tools and technologies, and will likely remain so for a long while. Beyond ambiguity, requirements are often marred by various other types of defects that could lead to wholly unacceptable consequences. In particular, operational experience shows that requirements inadequacy (whereby, in some of the situations the system could face, what is required is woefully inappropriate or what is necessary is left unspecified) is a significant cause for systems failing to meet expectations. The second objective of this paper is to propose a semantically accurate behavioural requirements formalisation format enabling tool-supported requirements verification, notably with simulation. Such support is necessary for the engineering of large and complex cyber-physical and socio-technical systems to ensure, first, that the specified requirements indeed reflect the true intentions of their authors and second, that they are adequate for all the situations the system could face. To that end, the paper presents an overview of the BASAALT (Behaviour Analysis and Simulation All Along systems Life Time) systems engineering method, and of FORM-L (FOrmal Requirements Modelling Language), its supporting language, which aims at representing as accurately and completely as possible the semantics expressed in the original, natural language behavioural requirements, and is markedly different from languages intended for software code generation. The paper shows that generally, semantically accurate formalisation is not a simple paraphrasing of the original natural language requirements: additional elements are often needed to fully and explicitly reflect all that is implied in natural language. To provide such complements for the case study presented in the paper, we had to follow different formalisation patterns, i.e. sequences of formalisation steps. For this paper, to avoid being skewed by what a particular automatic tool can and cannot do, BASAALT and FORM-L were applied manually. Still, the lessons learned could be used to specify and develop NLP tools that could assist the disambiguation and formalisation processes. However, more studies are needed to determine whether an exhaustive set of formalisation patterns can be identified to fully automate the formalisation process. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
"
10.1007/978-981-97-0272-5_4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187721269&origin=inward,Conference Paper,SCOPUS_ID:85187721269,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a knowledge base of shale gas play and its application on eur prediction by integrating knowledge graph and automated machine learning techniques,"
AbstractView references

The objective of this study is to analyze dominant controlling factors of the EUR of shale gas wells and then to forecast the EUR precisely by employing knowledge graph and automated machine learning techniques. First, an ontology knowledge representation model and a set of classification system for shale gas production are constructed, which include 13 shale gas objects such as basin, shale gas play, shale gas field, shale gas reservoir, and shale gas well, and their 112 geological, engineering and production parameters, such as mineral brittleness, fracturing section length, sanding intensity, and first-year production, and so on. Subsequently, structured data from existing databases are transformed, and loaded into the knowledge base. Large amount of unstructured data from papers, presentations, professional books are extracted and loaded by using various natural language processing (NLP) tools. The final shale gas knowledge base contains 56 shale gas plays and more than 1,000 shale gas wells worldwide. Based on the shale gas knowledge base, the graph embedding algorithm is used to convert the graph into a vector in order to train the machine learning models. Various automated machine learning frameworks such as TPOT, H2O, Auto-Sklearn, and AutoGluon are implemented and the performances are compared. According to the model with best performance, the main controlling factors of the EUR of shale gas wells are high-quality bed thickness, fracturing section length, and fracturing fluid volume, etc., which are consistent with shale gas production practices. The MSE and MAE of the best model on the testing dataset are 0.06 and 0.19, respectively. The approach of knowledge base construction and application developed in this paper can be extended to the entire life cycle of E&P process, which can make full use of various documents, data and knowledge accumulated in the oil and gas industry to conduct decision support. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.
"
10.1007/978-981-99-7569-3_10,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187647502&origin=inward,Conference Paper,SCOPUS_ID:85187647502,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the value of proactive data for intelligent contracts,"
AbstractView references

Intelligent Contracts (iContracts) is a new branch of research at the intersection of AI and law. It has many challenges, among which including the quality of data used. In our research we focus on generating and including quality Proactive Control Data (PCD) to improve iContracts, which is a novel research scope in literature. Our scope is defined by the main challenge in regards to emerging legal technologies. Currently, the legal system is more reactive than proactive, leading to high consequential legal costs. By shifting the focus to proactiveness, we discuss and improve upon the available methodologies (Bow-Tie Method and Logocratic Method) and technologies (Ontology Engineering, Software Engineering and Large Language Models [LLMs]) to demonstrate a higher degree of proactiveness in iContracts. Our results are threefold. First, we prove that the generation of PCD is possible with the development of a prototype that leverages the foundations of the Bow-Tie Method. Second, we demonstrate that the impact of PCD on contract drafting is significant, as the explicit inclusion of PCD in prompt engineering alters significantly the content of an LLM-drafted contract. Third, we show how the quality of PCD can be assessed and improved upon with the application of the Logocratic Method. The discussion highlights the feasibility of the research with available technologies. Ultimately, the implementation of our research depends on organisational considerations and resource allocation. We conclude that the generation of PCD is feasible, their impact on contract drafting is significant and their quality assessment is both possible and novel. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.
"
10.1117/12.3025216,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187541793&origin=inward,Conference Paper,SCOPUS_ID:85187541793,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),method for extracting power emergency plan information based on llm prompt learning,"
AbstractView references

The Large Language Model (LLM) as a representative of generative artificial intelligence, demonstrates strong capabilities in natural language comprehension, which was recently put into engineering applications in the field of power emergency. The author proposes a method of extracting information from power emergency plans by leveraging its emergent abilities and prompt learning techniques. By this method, custom-defined contents can be extracted from power emergency plans and linked to the corresponding personnel to generate executable task instructions. The results indicated that this method can accurately extract the custom-defined information from power emergency plans and applys to different LLMs. And the stronger the emergent abilities of the LLM, the more accurate the information extraction is. The method is proofed to effectively assist power emergency personnel in making decisions and expected to be used in various practical scenarios. © 2024 SPIE.
"
10.1109/TIFS.2024.3374558,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187407698&origin=inward,Article,SCOPUS_ID:85187407698,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on hardware security bug code fixes by prompting large language models,"
AbstractView references

Novel AI-based code-writing Large Language Models (LLMs) such as OpenAI’s Codex have demonstrated capabilities in many coding-adjacent domains. In this work, we consider how LLMs may be leveraged to automatically repair identified security-relevant bugs present in hardware designs by generating replacement code. We focus on bug repair in code written in Verilog. For this study, we curate a corpus of domain-representative hardware security bugs. We then design and implement a framework to quantitatively evaluate the performance of any LLM tasked with fixing the specified bugs. The framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the LLM. We show that an ensemble of LLMs can repair all fifteen of our benchmarks. This ensemble outperforms a state-of-the-art automated hardware bug repair tool on its own suite of bugs. These results show that LLMs have the ability to repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair tool. IEEE
"
10.1109/TPAMI.2024.3373868,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187387774&origin=inward,Article,SCOPUS_ID:85187387774,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ninerec: a benchmark dataset suite for evaluating transferable recommendation,"
AbstractView references

Large foundational models, through upstream pre-training and downstream fine-tuning, have achieved immense success in the broad AI community due to improved model performance and significant reductions in repetitive engineering. By contrast, the <underline>trans</underline>ferable one-for-all models in the <underline>rec</underline>ommender system field, referred to as TransRec, have made limited progress. The development of TransRec has encountered multiple challenges, among which the lack of large-scale, high-quality transfer learning recommendation dataset and benchmark suites is one of the biggest obstacles. To this end, we introduce NineRec, a TransRec dataset suite that comprises a large-scale source domain recommendation dataset and <underline>nine</underline> diverse target domain recommendation datasets. Each item in NineRec is accompanied by a descriptive text and a high-resolution cover image. Leveraging NineRec, we enable the implementation of TransRec models by learning from raw multimodal features instead of relying solely on pre-extracted off-the-shelf features. Finally, we present robust TransRec benchmark results with several classical network architectures, providing valuable insights into the field. To facilitate further research, we will release our code, datasets, benchmarks, and leaderboards at <uri>https://github.com/anonymous-ninerec/NineRec</uri>. IEEE
"
10.1109/LCOMM.2024.3365158,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187299289&origin=inward,Article,SCOPUS_ID:85187299289,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),enhancing reasoning ability in semantic communication through generative ai-assisted knowledge construction,"
AbstractView references

Semantic communication (SemCom), a pioneering paradigm that places emphasis on conveying the meaning of information, faces challenges in constructing background knowledge to drive precise reasoning of semantic coding models. Fortunately, the recent emergence of Generative Artificial Intelligence (GAI) technology is promising to create high-quality content that can be harnessed to assist knowledge construction in SemCom, enhancing the reasoning ability of semantic coding models. In this letter, we propose a GAI-assisted SemCom framework, named Gen-SC, where sufficient samples for training SemCom transceivers are generated using GAI as per user contextual information. In addition, to guide the GAI model in producing contextually relevant content, a discriminator is incorporated into Gen-SC to measure the disparity between generated samples and actual samples. The simulation results demonstrate that the Gen-SC achieves higher semantic accuracy, especially when the original training samples are insufficient, in contrast to traditional SemCom without knowledge enhancement. IEEE
"
10.1109/CGO57630.2024.10444830,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187240759&origin=inward,Conference Paper,SCOPUS_ID:85187240759,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),askit: unified programming interface for programming with large language models,"
AbstractView references

Large Language Models (LLMs) exhibit a unique phenomenon known as emergent abilities, demonstrating adeptness across numerous tasks, from text summarization to code generation. While these abilities open up novel avenues in software design and crafting, their incorporation presents substantial challenges. Developers face decisions regarding the use of LLMs for directly performing tasks within applications as well as for generating and executing code to accomplish these tasks. Moreover, effective prompt design becomes a critical concern, given the necessity of extracting data from natural language outputs. To address these complexities, this paper introduces AskIt, a domain-specific language (DSL) specifically designed for LLMs. AskIt simplifies LLM integration by providing a unified interface that not only allows for direct task execution using LLMs but also supports the entire cycle of code generation and execution. This dual capability is achieved through (1) type-guided output control, (2) template-based function definitions, and (3) prompt generation for both usage modes. Our evaluations underscore AskIt's effectiveness. Across 50 tasks, AskIt generated concise prompts, achieving a 16.14 % reduction in prompt length compared to benchmarks. Additionally, by enabling a seamless transition between using LLMs directly in applications and for generating code, AskIt achieved significant efficiency improvements, as observed in our GSM8K benchmark experiments. The implementations of AskIt in TypeScript and Python are available at https://github.com/katsumiok/ts-askit and https://github.com/katsumiok/pyaskit, respectively. © 2024 IEEE.
"
10.14569/IJACSA.2024.0150216,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187210202&origin=inward,Article,SCOPUS_ID:85187210202,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),roadmap for generative models redefining learning in egyptian higher education,"
AbstractView references

—Artificial Intelligence (AI) Generative models have become powerful tools in all sciences, research, academia, and businesses. Egyptian Universities need to leverage those models while using them ethically and responsibly to survive in the current global market. This paper explains the evolution of those models, from basic natural language processing by IBM in 1954 to the current powerful revolutionary generative models. The paper presents research that helps us get desired outputs or behaviors from generative models through prompt engineering, chain of thought prompting and ReAct. The paper presents Egypt and Egyptian Universities readiness and steps taken to get advantage of the latest AI technologies. The paper examines the training of those models to identify their advantages and disadvantages for university members focusing on the Egyptian context. The roadmap for Egyptian Universities use of generative models consists of a SWOT analysis; an infographic of policies and guidelines with regard to faculty and students use of generative models at Egyptian Universities promoting academic integrity and innovation, while minimizing the risks associated with this technology; A table of types and severities of penalties for policy violations by students using generative models is specified and finally a framework for nontechnical users of generative models of reusable patterns to get the optimal desired output of the models is developed. © (2024), (Science and Information Organization). All Rights Reserved.
"
10.1109/JBHI.2024.3370680,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186999797&origin=inward,Article,SCOPUS_ID:85186999797,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a transferability-based method for evaluating the protein representation learning,"
AbstractView references

Self-supervised pre-trained language models have recently risen as a powerful approach in learning protein representations, showing exceptional effectiveness in various biological tasks, such as drug discovery. Amidst the evolving trend in protein language model development, there is an observable shift towards employing large-scale multimodal and multitask models. However, the predominant reliance on empirical assessments using specific benchmark datasets for evaluating these models raises concerns about the comprehensiveness and efficiency of current evaluation methods. Addressing this gap, our study introduces a novel quantitative approach for estimating the performance of transferring multi-task pre-trained protein representations to downstream tasks. This transferability-based method is designed to quantify the similarities in latent space distributions between pre-trained features and those fine-tuned for downstream tasks. It encompasses a broad spectrum, covering multiple domains and a variety of heterogeneous tasks. To validate this method, we constructed a diverse set of protein-specific pre-training tasks. The resulting protein representations were then evaluated across several downstream biological tasks. Our experimental results demonstrate a robust correlation between the transferability scores obtained using our method and the actual transfer performance observed. This significant correlation highlights the potential of our method as a more comprehensive and efficient tool for evaluating protein representation learning. Code and data are freely available at <bold><uri>https://github.com/SIAT-code/OTMTD</uri></bold>. IEEE
"
10.1007/s00766-024-00413-6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186591994&origin=inward,Article,SCOPUS_ID:85186591994,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),grlmerger: an automatic approach for integrating grl models,"
AbstractView references

Goal-oriented requirements engineering aims to describe both stakeholders and system goals and their relationships using goal models. Large goal models for complex systems are often constructed from sub-models describing various stakeholders’ views and context-related aspects. These goal-oriented sub-models, also called views, may exhibit overlaps and present discrepancies. Hence, integrating such views is considered a significant barrier to the construction of a unified goal model. Current approaches to merging goal models require intensive human intervention. This paper proposes an approach and a prototype tool, called GRLMerger, to integrate two GRL (Goal-oriented Requirement Language) models into one consolidated model that is correct, complete, and free from any conflict that may arise during the merging process. GRLMerger considers both syntactic and semantic aspects of the GRL models and allows analysts to merge them either interactively or in a fully automated mode. GRLMerger employs natural language processing (NLP) techniques to match intentional elements based on their semantic similarity. The proposed GRLMerger approach and tool have been validated using 12 experimental tasks derived from two case studies, exhibiting very promising performance. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.
"
10.1002/cae.22729,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186584200&origin=inward,Article,SCOPUS_ID:85186584200,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),examples and tutorials on using google colab and gradio to create online interactive student-learning modules,"
AbstractView references

This work provides online learning modules and instructions on how educators can leverage these technologies to help students learn in a personalized online environment. In particular, we focus on Google Colab, and the features provided by the Gradio Python library to provide interactivity within these modules. The contributions of this work include: (1) Development of a teaching framework using Gradio/Colab that offers automated grading and feedback for both educators and students; (2) Design of a versatile proposal, accommodating beginners with a straightforward interface while addressing the needs of advanced learners; (3) Creation of a comprehensive set of examples tailored for teaching digital logic subjects, with adaptability for application in various computer science areas. (4) A classification of these example learning modules in terms of their learning level for the students; (5) A novel client-server approach based on Colab/Gradio, allowing teachers to manage the main notebook efficiently while providing a lightweight and reliable interface for students. The goal of this work is to further expose educators to the remarkable capabilities that cloud computing brings to online supplemental education, noting that large language models such as ChatGPT complement this work, in that chatbots will be able to guide students in these dynamic simulations. © 2024 Wiley Periodicals LLC.
"
10.1109/ICAIC60265.2024.10433803,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186523970&origin=inward,Conference Paper,SCOPUS_ID:85186523970,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"dataagent: evaluating large language models' ability to answer zero-shot, natural language queries","
AbstractView references

Conventional processes for analyzing datasets and extracting meaningful information are often time-consuming and laborious. Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects. To combat this, we evaluated OpenAI's GPT-3.5 as a ""Language Data Scientist""(LDS) that can extrapolate key findings, including correlations and basic information, from a given dataset. The model was tested on a diverse set of benchmark datasets to evaluate its performance across multiple standards, including data science code-generation based tasks involving libraries such as NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in correctly answering a given data science query related to the benchmark dataset. The LDS used various novel prompt engineering techniques to effectively answer a given question, including Chain-of-Thought reinforcement and SayCan prompt engineering. Our findings demonstrate great potential for leveraging Large Language Models for low-level, zero-shot data analysis. © 2024 IEEE.
"
10.1016/j.dim.2024.100066,S2543925124000020,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186173966&origin=inward,Article,SCOPUS_ID:85186173966,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative ai: a systematic review using topic modelling techniques,"Generative artificial intelligence (GAI) is a rapidly growing field with a wide range of applications. In this paper, a thorough examination of the research landscape in GAI is presented, encompassing a comprehensive overview of the prevailing themes and topics within the field. The study analyzes a corpus of 1319 records from Scopus spanning from 1985 to 2023 and comprises journal articles, books, book chapters, conference papers, and selected working papers. The analysis revealed seven distinct clusters of topics in GAI research: image processing and content analysis, content generation, emerging use cases, engineering, cognitive inference and planning, data privacy and security, and Generative Pre-Trained Transformer (GPT) academic applications. The paper discusses the findings of the analysis and identifies some of the key challenges and opportunities in GAI research. The paper concludes by calling for further research in GAI, particularly in the areas of explainability, robustness, cross-modal and multi-modal generation, and interactive co-creation. The paper also highlights the importance of addressing the challenges of data privacy and security in GAI and responsible use of GAI."
10.2196/51391,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186092843&origin=inward,Article,SCOPUS_ID:85186092843,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),learning to make rare and complex diagnoses with generative ai assistance: qualitative study of popular large language models,"
AbstractView references

Background: Patients with rare and complex diseases often experience delayed diagnoses and misdiagnoses because comprehensive knowledge about these diseases is limited to only a few medical experts. In this context, large language models (LLMs) have emerged as powerful knowledge aggregation tools with applications in clinical decision support and education domains. Objective: This study aims to explore the potential of 3 popular LLMs, namely Bard (Google LLC), ChatGPT-3.5 (OpenAI), and GPT-4 (OpenAI), in medical education to enhance the diagnosis of rare and complex diseases while investigating the impact of prompt engineering on their performance. Methods: We conducted experiments on publicly available complex and rare cases to achieve these objectives. We implemented various prompt strategies to evaluate the performance of these models using both open-ended and multiple-choice prompts. In addition, we used a majority voting strategy to leverage diverse reasoning paths within language models, aiming to enhance their reliability. Furthermore, we compared their performance with the performance of human respondents and MedAlpaca, a generative LLM specifically designed for medical tasks. Results: Notably, all LLMs outperformed the average human consensus and MedAlpaca, with a minimum margin of 5% and 13%, respectively, across all 30 cases from the diagnostic case challenge collection. On the frequently misdiagnosed cases category, Bard tied with MedAlpaca but surpassed the human average consensus by 14%, whereas GPT-4 and ChatGPT-3.5 outperformed MedAlpaca and the human respondents on the moderately often misdiagnosed cases category with minimum accuracy scores of 28% and 11%, respectively. The majority voting strategy, particularly with GPT-4, demonstrated the highest overall score across all cases from the diagnostic complex case collection, surpassing that of other LLMs. On the Medical Information Mart for Intensive Care-III data sets, Bard and GPT-4 achieved the highest diagnostic accuracy scores, with multiple-choice prompts scoring 93%, whereas ChatGPT-3.5 and MedAlpaca scored 73% and 47%, respectively. Furthermore, our results demonstrate that there is no one-size-fits-all prompting approach for improving the performance of LLMs and that a single strategy does not universally apply to all LLMs. Conclusions: Our findings shed light on the diagnostic capabilities of LLMs and the challenges associated with identifying an optimal prompting strategy that aligns with each language model's characteristics and specific task requirements. The significance of prompt engineering is highlighted, providing valuable insights for researchers and practitioners who use these language models for medical training. Furthermore, this study represents a crucial step toward understanding how LLMs can enhance diagnostic reasoning in rare and complex medical cases, paving the way for developing effective educational tools and accurate diagnostic aids to improve patient care and outcomes. © Tassallah Abdullahi, Ritambhara Singh, Carsten Eickhoff.
"
10.1109/TIV.2024.3365997,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186065598&origin=inward,Article,SCOPUS_ID:85186065598,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"smart mining with autonomous driving in industry 5.0: architectures, platforms, operating systems, foundation models, and applications","
AbstractView references

The increasing importance of mineral resources in contemporary society is becoming more prominent, playing an indispensable and crucial role in the global economy. These resources not only provide essential raw materials for the global economic system but also play an irreplaceable role in supporting the development of modern industry, technology, and infrastructure. With the rapid development of intelligent technologies such as Industry 5.0 and advanced Large Language Models (LLMs), the mining industry is facing unprecedented opportunities and challenges. The development of smart mines has become a crucial direction for industry progress. This article aims to explore the strategic requirements for the development of smart mines by combining advanced products or technologies such as Chat-GPT (one of the successful applications of LLMs), digital twins, and scenario engineering. We propose a comprehensive architecture consisting of three different levels, the mining industrial Internet of Things (IoT) platform, mining operating systems, and foundation models. The systems and models empower the mining equipment for transportation. The architecture delivers a comprehensive solution that aligns perfectly with the demands of Industry 5.0. The application and validation outcomes of this intelligent solution showcase a noteworthy enhancement in mining efficiency and a reduction in safety risks, thereby laying a sturdy groundwork for the advent of Mining 5.0. IEEE
"
10.1080/17499518.2024.2316882,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185960313&origin=inward,Article,SCOPUS_ID:85185960313,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"future of machine learning in geotechnics (fomlig), 5–6 dec 2023, okayama, japan","
AbstractView references

This report presents the key talking points in the First Workshop on the Future of Machine Learning in Geotechnics (FOMLIG), that include data infrastructure, geotechnical context, computational cost, and human judgment. On the first point, it was argued that further growth in data sharing needs stronger demonstration of value to practice and protection of data privacy. On the second point, significant progress has been made in addressing site specificity (site recognition challenge). On the third point, it is costly to interpret monitoring data in the context of machine learning guided observational method (MLOM) because the 3D domain influencing the geotechnical structure is large, real-time dataset is very large and its attributes are complicated, data fusion remains challenging, and computation speed must support real-time decision making. Real-time machine learning-based decision support is clearly not useful if it is not providing the engineer with sufficient lead time to adjust the construction process. On the fourth point, the capability of generative AIs such as ChatGPT to act as an intelligent companion to an engineer in decision making is exciting. The role of human judgment in human-machine teaming is unclear, but for human-machine teaming to be effective, a deliberate approach is needed to build trust between the human and the AI/robot partner. © 2024 Informa UK Limited, trading as Taylor & Francis Group.
"
10.1007/978-3-031-53308-2_7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185723182&origin=inward,Conference Paper,SCOPUS_ID:85185723182,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),drive-clip: cross-modal contrastive safety-critical driving scenario representation learning and zero-shot driving risk analysis,"
AbstractView references

Driving risk analysis, especially in safety-critical driving scenarios (SCDSs), plays a paramount role in providing subsequent driving assistance to mitigate potential traffic hazards. Previous studies predominantly relied on unimodal event data for risk analysis, overlooking the valuable source of information embedded in text narratives. This limitation hindered the full utilization of available data for representing SCDSs effectively. Capitalizing on the success of large language models in natural language processing, text-based models offer new potential for enhancing the performance of this task. In this paper, we introduce a novel framework named Drive-CLIP, designed for cross-modal contrastive SCDS representation learning, incorporating both text narratives and event data as two modalities. Through cross-modal analysis, Drive-CLIP distills SCDS event embeddings from natural language supervision, enabling text-guided zero-shot driving risk analysis on event data. Experiments conducted on a naturalistic driving dataset demonstrate that Drive-CLIP surpasses the performance of current best-performing methods, underscoring its effectiveness and superiority. Furthermore, we highlight that cross-modal analysis yields advantages over using a single data modality, and the cross-modal contrastive SCDS representation learning remains beneficial even in scenarios with limited data. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-53022-7_5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185711696&origin=inward,Conference Paper,SCOPUS_ID:85185711696,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a qualitative assessment of chatgpt generated code in the computer science curriculum,"
AbstractView references

The emergence of Large Language Models and their deployment in systems such as ChatGPT are poised to have a major impact on STEM education, particularly Computer Science. These generative large language models can produce program code as well as human language output. This has potentially serious implications for computer science programs and pedagogy. This work provides a qualitative assessment sample code generated by ChatGPT, as an example of an LLM explores implications for computing pedagogy.... © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
"
10.1137/22M1518189,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185535997&origin=inward,Article,SCOPUS_ID:85185535997,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),neuraluq: a comprehensive library for uncertainty quantification in neural differential equations and operators,"
AbstractView references

Uncertainty quantification (UQ) in machine learning is currently drawing increasing research interest, driven by the rapid deployment of deep neural networks across different fields, such as computer vision and natural language processing, and by the need for reliable tools in risk-sensitive applications. Recently, various machine learning models have also been developed to tackle problems in the field of scientific computing with applications to computational science and engineering (CSE). Physics-informed neural networks and deep operator networks are two such models for solving partial differential equations (PDEs) and learning operator mappings, respectively. In this regard, a comprehensive study of UQ methods tailored specifically for scientific machine learning (SciML) models has been provided in [A. F. Psaros et al., J. Comput. Phys., 477 (2023), art. 111902]. Nevertheless, and despite their theoretical merit, implementations of these methods are not straightforward, especially in large-scale CSE applications, hindering their broad adoption in both research and industry settings. In this paper, we present an open-source Python library (https://github.com/Crunch-UQ4MI), termed NeuralUQ and accompanied by an educational tutorial, for employing UQ methods for SciML in a convenient and structured manner. The library, designed for both educational and research purposes, supports multiple modern UQ methods and SciML models. It is based on a succinct workflow and facilitates flexible employment and easy extensions by the users. We first present a tutorial of NeuralUQ and subsequently demonstrate its applicability and efficiency in four diverse examples, involving dynamical systems and high-dimensional parametric and time-dependent PDEs. Copyright © by SIAM.
"
10.1108/ITSE-10-2023-0198,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185480824&origin=inward,Article,SCOPUS_ID:85185480824,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"bard, chatgpt and 3dgpt: a scientometric analysis of generative ai tools and assessment of implications for mechanical engineering education","
AbstractView references

Purpose: Following the recent rise in generative artificial intelligence (GenAI) tools, fundamental questions about their wider impacts have started to reverberate around various disciplines. This study aims to track the unfolding landscape of general issues surrounding GenAI tools and to elucidate the specific opportunities and limitations of these tools as part of the technology-assisted enhancement of mechanical engineering education and professional practices. Design/methodology/approach: As part of the investigation, the authors conduct and present a brief scientometric analysis of recently published studies to unravel the emerging trend on the subject matter. Furthermore, experimentation was done with selected GenAI tools (Bard, ChatGPT, DALL.E and 3DGPT) for mechanical engineering-related tasks. Findings: The study identified several pedagogical and professional opportunities and guidelines for deploying GenAI tools in mechanical engineering. Besides, the study highlights some pitfalls of GenAI tools for analytical reasoning tasks (e.g., subtle errors in computation involving unit conversions) and sketching/image generation tasks (e.g., poor demonstration of symmetry). Originality/value: To the best of the authors’ knowledge, this study presents the first thorough assessment of the potential of GenAI from the lens of the mechanical engineering field. Combining scientometric analysis, experimentation and pedagogical insights, the study provides a unique focus on the implications of GenAI tools for material selection/discovery in product design, manufacturing troubleshooting, technical documentation and product positioning, among others. © 2024, Emerald Publishing Limited.
"
10.1080/02188791.2023.2286920,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185468324&origin=inward,Article,SCOPUS_ID:85185468324,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the comparison of general tips for mathematical problem solving generated by generative ai with those generated by human teachers,"
AbstractView references

In designing an intelligent tutoring system, a core area of the application of AI in education, tips from the system or virtual tutors are crucial in helping students solve difficult questions in disciplines like mathematics. Traditionally, the manual design of general tips by teachers is time-consuming and error-prone. Generative AI, like ChatGPT, presents a new channel for designing general tips. This study utilized prompt engineering and Chain of Thought to summarize general tips for given mathematical problems (one geometry problem and one algebra problem) and their solutions. A Turing test was conducted to compare ChatGPT-generated general tips with human-designed ones. Results from 121 human evaluators, each assessing 6 ChatGPT-generated and 6 human-designed general tips for each of two mathematical problems, showed that the average score for ChatGPT-generated tips is less than that of human-designed tips at a statistically significant level (p < 0.05), and Zero-Shot CoT achieved the best score. However, no evaluator could distinguish the tip types exactly. The average precision, recall and F-value of all ChatGPT-generated tips are less than 40%. AI-generated general tips can serve as a valuable reference for teachers to enhance efficiency and students’ mathematical learning. © 2023 National Institute of Education, Singapore.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185405265&origin=inward,Article,SCOPUS_ID:85185405265,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a panel report on higher education in the age of ai from the perspective of academic leaders in the midwest u.s.,"
AbstractView references

This panel report is based on an academic leadership panel at the 18th annual Midwest Association for Information Systems (MWAIS) 2023 conference held in St. Paul, Minnesota. A panel of five university chancellors and presidents was brought together to discuss the current and potential challenges and opportunities of higher education institutions, particularly the comprehensive regional universities in the Midwest U.S. region. The panel was asked to reflect on the challenges and opportunities afforded by the rapid use and deployment of Artificial Intelligence (AI) and how this impacts higher education. Discussion themes and insights are presented along with the future outlook considerations. The panel discussed how universities are preparing themselves for broad challenges and opportunities necessitated by the rapid technological advances in AI along with other social and economic changes-such as declining state support and lower student enrollments, by reexamining the value of college education and reevaluating and repositioning curricula and programs. The discussion underscored that university leaders are placing a strong emphasis on human qualities in the human-AI dynamic. These qualities include adaptive skills for tackling unstructured problems, fostering collaboration, critical thinking, empathy, emotional intelligence, and a focus on ethical and social aspects. They highlight these attributes as vital elements, considering them more important than just the technological components when grappling with challenges posed by AI in human decision-making and higher education. © 2024, Association for Information Systems. All rights reserved.
"
10.1108/ETPC-08-2023-0087,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185324618&origin=inward,Article,SCOPUS_ID:85185324618,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),“weaving tales of resilience”: cyborg composing with ai,"
AbstractView references

Purpose: This paper aims to offer an approach to cyborg composing with artificial intelligence (AI). The author posits that the hybridity of the cyborg, which amalgamates human and artificial elements, invites a cascade of creative and emancipatory possibilities. The author critically examines the biases embedded in AI systems while gesturing toward the generative potential of AI–human entanglements. Drawing on Bakhtinian theories of dialogism, the author contends that crafting found poetry with AI could inspire writers to problematize the ideologies embedded into the corpus while teasing apart its elisions or contradictions, sparking new forms of expression at the interface of the organic and the artificial. Design/methodology/approach: To illustrate this approach to human–AI composing, the author shares a found poem that she wrote using ChatGPT alongside her reflection on the poem. The author reflects on her positionality as well as the positionality of her artificial interlocutor, interrogating the notion of subjectivity in relation to Bakhtinian dialogism and multivocality. Findings: Weaving tales of resilience in harmony or tension with AI could unravel threads of possibility as human writers enrich, deepen or complicate AI-generated texts. By composing with AI, writers can resist closure, infiltrate illusions of objectivity and “speak back” to AI and the dominant voices replicated in its systems. Originality/value: By encouraging students to critically engage with, question and complicate AI-generated texts, one can open avenues for alternative ways of thinking and writing, inspiring students to imagine and compose speculative futures. Ultimately, in animating assemblages of the organic and the artificial, one can invite transformative possibilities of being and becoming. © 2024, Emerald Publishing Limited.
"
10.14569/IJACSA.2024.0150143,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184998761&origin=inward,Article,SCOPUS_ID:85184998761,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),students' perception of chatgpt usage in education,"
AbstractView references

This research article delves into the impact of ChatGPT on education, focusing on the perceptions and usage patterns among high school and university students. The article begins by introducing ChatGPT, emphasizing its rapid user adoption and widespread interest. It explores the application of ChatGPT in various fields, including healthcare, agriculture, and education. A comprehensive survey involving 102 students, both high school and university, is detailed, covering aspects like familiarity with ChatGPT, reasons for usage, self-assessment of its effectiveness, and attitudes toward informing teachers about its use. The findings reveal varied perspectives on the benefits and challenges of incorporating ChatGPT in the learning process. The article concludes by emphasizing the need for careful consideration and integration of AI technologies in education, highlighting the risks of uncritical reliance on such tools and advocating for a balanced approach to foster students' critical thinking and intellectual growth. © (2024), (Science and Information Organization). All Rights Reserved.
"
10.14569/IJACSA.2024.0150168,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184992259&origin=inward,Article,SCOPUS_ID:85184992259,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),practical application of ai and large language models in software engineering education,"
AbstractView references

Subjects with limited application in the software industry like AI have recently received tremendous boon due to the development and raise of publicity of LLMs. LLM-powered software has a wide array of practical applications that must be taught to Software Engineering students, so that they can be relevant in the field. The speed of technological change is extremely fast, and university curriculums must include those changes. Renewing and creating new methodologies and workshops is a difficult task to complete successfully in such a dynamic environment full of cutting-edge technologies. This paper aims to showcase our approach to using LLM-powered software for AI generated images, like Stable diffusion and code generation tools like ChatGPT in workshops for two relevant subjects - Analysis of Software Requirements and Specifications, as well as Artificial Intelligence. A comparison between the different available LLMs that generate images is made, and the choice between them is explained. Student feedback is shown and a general positive and motivational impact is noted during and after the workshop. A brief introduction that covers the subjects where AI is applied is made. The proposed solutions for several uses of AI in the field of higher education, more specifically software engineering, are presented. Several workshops have been made and included in the curriculum. The results of their application have been noted and an analysis is made. More propositions on further development based on the gained experience, feedback and retrieved data are made. Conclusions are made on the application of AI in higher education and different ways to utilize such tools are presented. © (2024), (Science and Information Organization). All Rights Reserved.
"
10.1007/s42489-024-00159-9,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184877372&origin=inward,Article,SCOPUS_ID:85184877372,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative text-to-image diffusion for automated map production based on geosocial media data,"
AbstractView references

The state of generative AI has taken a leap forward with the availability of open source diffusion models. Here, we demonstrate an integrated workflow that uses text-to-image stable diffusion at its core to automatically generate icon maps such as for the area of the Großer Garten, a tourist hotspot in Dresden, Germany. The workflow is based on the aggregation of geosocial media data from Twitter, Flickr, Instagram and iNaturalist. This data are used to create diffusion prompts to account for the collective attribution of meaning and importance by the population in map generation. Specifically, we contribute methods for simplifying the variety of contexts communicated on social media through spatial clustering and semantic filtering for use in prompts, and then demonstrate how this human-contributed baseline data can be used in prompt engineering to automatically generate icon maps. Replacing labels on maps with expressive graphics has the general advantage of reaching a broader audience, such as children and other illiterate groups. For example, the resulting maps can be used to inform tourists of all backgrounds about important activities, points of interest, and landmarks without the need for translation. Several challenges are identified and possible future optimizations are described for different steps of the process. The code and data are fully provided and shared in several Jupyter notebooks, allowing for transparent replication of the workflow and adoption to other domains or datasets. © The Author(s) 2024.
"
10.1109/ACCESS.2024.3363655,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184807754&origin=inward,Article,SCOPUS_ID:85184807754,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the analysis of artificial intelligence digital technology in art education under the internet of things,"
AbstractView references

This study aims to explore the integration of Internet of Things (IoT) technology and artificial intelligence (AI) in art education, assessing its impact on learners' experiences and learning outcomes. The study first proposes a digital teaching system that enables the IoT and Generative Adversarial Networks (GANs) to play a role in art education by monitoring students' creative state in real-time, providing immediate feedback, and facilitating the generation of creative works. The system framework includes sensor nodes, an IoT platform, a GAN model, and a user interface to build a real-time interactive environment. Sensor nodes constantly collect physiological, movement, and environmental data from students, and the GAN model receives student data from the IoT platform, combining creative input from students to generate artwork in real-time. The generated works are transmitted to the discriminator through the IoT platform, which evaluates their quality and provides real-time feedback. Students interact with the system through the user interface, observe the generated artwork, adjust generator parameters, and propose new ideas. These interactions influence further artistic creation. The WikiArt public art creation dataset is selected to establish the experimental foundation, and the experimental evaluation focuses on image generation quality, system performance, and student learning outcomes. It is compared with Deep Convolutional Generative Adversarial Network (DCGAN) and Variational Autoencoder (VAE) models. The research results indicate that the designed IoT and GANs integrated system remarkably outperforms DCGAN and VAE in image generation quality, with an Inception Score of 4.5, which is more diverse and recognizable than other models. Regarding system performance, the IoT and GANs integrated system is significantly ahead in image generation speed and user interaction, with a transmission speed of up to 200 Mbps. Regarding student learning outcomes, the system performs excellently in emotional feedback, learning outcomes, and creative work quality, achieving 80% satisfaction and 90% positive feedback. Overall, the research conclusion clearly points out that the integration of IoT and GANs has a significant and comprehensive effect on improving the quality of art education. This study expands the field of art education by integrating IoT and GANs, enhancing students' creative experiences, and providing innovative methods for art teaching in the digital age. © 2013 IEEE.
"
10.1007/978-3-031-48573-2_53,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184802782&origin=inward,Conference Paper,SCOPUS_ID:85184802782,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt for a flexible higher education: a rapid review of the literature,"
AbstractView references

The OpenAI-developed ChatGPT, which is based on artificial intelligence (AI), has gained widespread acceptance in a number of industries. Education is included. The use of this technology when creating content allows students to learn about concepts and theories. ChatGPT is built on State of the Art (SOA), which incorporates Natural Language Processing (NLP), Deep Learning (DL), and an extrapolation of a family of ML-NLP models known as Large Language Models (LLMs), which mixes NLP and ML. It can be used to automate the grading of tests and assignments, freeing up instructors’ time to focus on training. This technology can be used to tailor education for children, allowing them to concentrate more attentively on the material and engage in critical thinking. Because ChatGPT can translate text between different languages, it’s a great tool for language learning. It might offer lists of vocabulary words and their definitions, giving pupils tools to improve their language skills. One of ChatGPT’s key applications in the class-room is individualized learning. Using ChatGPT in the classroom has the potential to be very beneficial for both teachers and students. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
"
10.1109/ACCESS.2024.3361946,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184797988&origin=inward,Article,SCOPUS_ID:85184797988,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),developing personalized marketing service using generative ai,"
AbstractView references

In today's world, the development of social network services (SNS) like Facebook and Instagram has enabled consumers to acquire information about products through various channels. The acquisition of diverse information has led to a diversification in consumer preferences and requirements. As consumer preferences diversify and online channels expand, there is an increasing need for companies to provide personalized marketing. Among the means of personalized marketing, personalized marketing messages are a key tool that can enhance customer engagement. However, a limitation of personalized marketing message services is the cost issue associated with manually writing individual marketing messages for personalization. To solve this problem, when developing automated technology for personalized marketing messages, there were concerns about the complexity of model development and the quality of messages generated automatically. In this study, we propose the Persuasive Message Intelligence (PMI) service, which utilizes the recently prominent Large Language Model for automated individual personalized marketing messages. PMI generates marketing messages through prompt engineering based on the theory of persuasion in marketing and prior research on AI-generated messages, and validates the elements of prompts through surveys. The trial and error of researchers presented in this study, along with the know-how and rules of prompt engineering, will serve as guidelines for those who wish to develop services through prompts in the future. © 2013 IEEE.
"
10.4108/eetsis.4067,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184776787&origin=inward,Article,SCOPUS_ID:85184776787,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),algorithmic literacy: generative artificial intelligence technologies for data librarians,"
AbstractView references

INTRODUCTION: Artificial intelligence (AI) is a novel type of library technology. AI technologies and the needs of data librarians are hybrid and symbiotic, because academic libraries must insert AI technologies into their information and data services. Library services need AI to interpret the context of big data. OBJECTIVES: In this context, we explore the use of the OpenAI Codex, a deep learning model trained on Python code from repositories, to generate code scripts for data librarians. This investigation examines the practices, models, and methodologies for obtaining code script insights from complex code environments linked to AI GPT technologies. METHODS: The proposed AI-powered method aims to assist data librarians in creating code scripts using Python libraries and plugins such as the integrated development environment PyCharm, with additional support from the Machinet AI and Bito AI plugins. The process involves collaboration between the data librarian and the AI agent, with the librarian providing a natural language description of the programming problem and the OpenAI Codex generating the solution code in Python. RESULTS: Five specific web-scraping problems are presented. The scripts demonstrate how to extract data, calculate metrics, and write the results to files. CONCLUSION: Overall, this study highlights the application of AI in assisting data librarians with code script creation for web scraping tasks. AI may be a valuable resource for data librarians dealing with big data challenges on the Web. The possibility of creating Python code with AI is of great value, as AI technologies can help data librarians work with various types of data sources. The Python code in Data Science web scraping projects uses a machine-learning model that can generate human-like code to help create and improve the library service for extracting data from a web collection. The ability of nonprogramming data librarians to use AI technologies facilitates their interactions with all types and data sources. The Python programming language has artificial intelligence modules, packages, and plugins such as the OpenAI Codex, which serialises automation and navigation in web browsers to simulate human behaviour on pages by entering passwords, selecting captcha options, collecting data, and creating different collections of datasets to be viewed. © 2024 Author et al., licensed to EAI. This is an open access article distributed under the terms of the CC BY-NC-SA 4.0, which permits copying, redistributing, remixing, transformation, and building upon the material in any medium so long as the original work is properly cited. All Rights Reserved.
"
10.1057/s41254-023-00322-5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184437675&origin=inward,Article,SCOPUS_ID:85184437675,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),assessing the risks and opportunities posed by ai-enhanced influence operations on social media,"
AbstractView references

Large language models (LLMs) like GPT-4 have the potential to dramatically change the landscape of influence operations. They can generate persuasive, tailored content at scale, making campaigns using falsified content, such as disinformation and fake accounts, easier to produce. Advances in self-hosted open-source models have meant that adversaries can evade content moderation and security checks built into large commercial models such as those commercialised by Anthropic, Google, and OpenAI. New multi-lingual models make it easier than ever for foreign adversaries to pose as local actors. This article examines the heightened threats posed by synthetic media, as well as the potential that these tools hold for creating effective countermeasures. It begins with assessing the challenges posed by a toxic combination of automated bots, human-controlled troll accounts, and more targeted social engineering operations. However, the second part of the article assesses the potential for these same tools to improve detection. Promising countermeasures include running internal generative models to bolster training data for internal classifiers, detecting statistical anomalies, identifying output from common prompts, and building specialised classifiers optimised for specific monitoring needs. © The Author(s) 2024.
"
10.1007/s10559-024-00652-z,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184410537&origin=inward,Article,SCOPUS_ID:85184410537,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),fundamentals of the integrated use of neural network and ontolinguistic paradigms: a comprehensive approach,"
AbstractView references

The paper presents an integrated approach that combines neural network and ontolinguistic paradigms. The method encompasses methodological underpinnings, information technology, and the MedRehabBot system. Collectively, they embody the core principles of meta-learning and structured prompts, ultimately enhancing the efficiency of information system interaction with Chatbots and information retrieval rooted in ontologies. The method also offers the flexibility to adapt the MedRehabBot system for utilization within different Large Language Model (LLM) systems. © Springer Science+Business Media, LLC, part of Springer Nature 2024.
"
10.1109/TSMC.2024.3349555,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184327362&origin=inward,Article,SCOPUS_ID:85184327362,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative ai empowering parallel manufacturing: building a &amp;#x201c;6s&amp;#x201d; collaborative production ecology for manufacturing 5.0,"
AbstractView references

Since Manufacturing 4.0 faces various challenges, including the risks of data leakage and privacy violation, the struggle to meet the growing demand for personalization, and the limitations in harnessing human creativity, it has become crucial to embark on a transformation toward Manufacturing 5.0. To this end, we propose a DeFACT framework for parallel manufacturing and Manufacturing 5.0, which focuses on safe, efficient and personalized collaborative production. In DeFACT, different enterprises and parallel workers (i.e., digital, robotic and biological workers) are organized, coordinated and scheduled based on decentralized autonomous organizations and operations to promote mutual benefits among members, even in the context of low or zero trust. This contributes to providing customers with higher-quality personalized products and services while ensuring the confidentiality and safeguarding of data. Additionally, various advanced technologies, such as generative artificial intelligence, scenarios engineering, and blockchain, are leveraged to achieve trustworthy and adaptable decision making, user-friendly human–machine interaction, and the federated control and management of parallel workers. Finally, the effectiveness and efficiency of DeFACT are experimentally validated through the design and implementation of three case studies. IEEE
"
10.1007/978-3-031-51060-1_2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184278743&origin=inward,Book Chapter,SCOPUS_ID:85184278743,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),actor-based designs for distributed self-organisation programming,"
AbstractView references

Self-organisation and collective adaptation are highly desired features for several kinds of large-scale distributed systems including robotic swarms, computational ecosystems, wearable collectives, and Internet-of-Things systems. These kinds of distributed processes, addressing functional and non-functional aspects of complex socio-technical systems, can emerge in an engineered/controlled way from (re)active decentralised activity and interaction across all physical and logical system devices. In this work, we study how the Actors programming model can be adopted to support collective self-organising behaviours. Specifically, we analyse the features of the Actors model, such as reactivity, asynchrony, and locality, that are instrumental for implementing the adaptive coordination of large-scale systems, and discuss potential actor-based designs, from simple ad-hoc implementation of algorithms to a full-fledged general toolkit. In particular, the approach is incarnated in the aggregate computing paradigm, which stands as a comprehensive engineering approach for self-organisation. This is based on Akka, and can be fully programmed in the Scala programming language thanks to the ScaFi aggregate computing toolkit. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
"
10.1080/10447318.2024.2307670,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184207934&origin=inward,Article,SCOPUS_ID:85184207934,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),“journey of finding the best query”: understanding the user experience of ai image generation system,"
AbstractView references

With the advancement of AI, even people without professional experience can create artworks using AI-based image generation systems like DALL-E 2. However, little is known about how users interact with these new AI algorithms, much less how AI-infused systems can be designed. We explore the user experience of these new technologies and their potential to foster creativity. A user study was carried out where 13 participants executed tasks of creating artworks using DALL-E 2 alongside in-depth interviews related to their experience. The results showed that users had ambivalent opinions regarding the algorithm’s performance. When users were informed of the system’s capabilities, they subsequently utilized more specific prompts to generate the intended output. Users also optimized their prompts (the queries they entered to create artworks) based on how algorithms worked to achieve their desired outcome. The users wanted a two-way interaction where AI explained the outcome and accepted feedback rather than simply accepting unilateral instructions. We discuss the implications for designing interfaces that maximize creativity while providing comfort for the users. © 2024 Taylor & Francis Group, LLC.
"
10.1057/s41270-023-00284-w,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184202809&origin=inward,Article,SCOPUS_ID:85184202809,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),hey chatgpt: an examination of chatgpt prompts in marketing,"
AbstractView references

Marketing is one of the areas where large language models (LLMs) such as ChatGPT have found practical applications. This study examines marketing prompts—text inputs created by marketers to guide LLMs in generating desired outputs. By combining insights from the marketing literature and the latest research on LLMs, the study develops a conceptual framework around three key features of marketing prompts: prompt domain (the specific marketing actions that the prompts target), prompt appeal (the intended output of the prompts being informative or emotional), and prompt format (the intended output of the prompts being generic or contextual). The study collected hundreds of marketing prompt templates shared on X (formerly Twitter) and analyzed them using a combination of natural language processing techniques and descriptive statistics. The findings indicate that the prompt templates target a wide range of marketing domains—about 16 altogether. Likewise, the findings indicate that most of the marketing prompts are designed to generate informative output (as opposed to emotionally engaging output). Further, the findings indicate that the marketing prompts are designed to generate a balanced mix of generic and contextual output. The study further finds that the use of prompt appeal and prompt format differs by prompt domain. © 2024, The Author(s), under exclusive licence to Springer Nature Limited.
"
10.1007/978-3-031-50396-2_22,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184091925&origin=inward,Conference Paper,SCOPUS_ID:85184091925,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt-healthprompt. harnessing the power of xai in prompt-based healthcare decision support using chatgpt,"
AbstractView references

This study presents an innovative approach to the application of large language models (LLMs) in clinical decision-making, focusing on OpenAI’s ChatGPT. Our approach introduces the use of contextual prompts-strategically designed to include task description, feature description, and crucially, integration of domain knowledge-for high-quality binary classification tasks even in data-scarce scenarios. The novelty of our work lies in the utilization of domain knowledge, obtained from high-performing interpretable ML models, and its seamless incorporation into prompt design. By viewing these ML models as medical experts, we extract key insights on feature importance to aid in decision-making processes. This interplay of domain knowledge and AI holds significant promise in creating a more insightful diagnostic tool. Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI’s ChatGPT with traditional supervised ML models in different data conditions, we aim to provide insights into the effectiveness of prompt engineering strategies under varied data availability. In essence, this paper bridges the gap between AI and healthcare, proposing a novel methodology for LLMs application in clinical decision support systems. It highlights the transformative potential of effective prompt design, domain knowledge integration, and flexible learning approaches in enhancing automated decision-making. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
"
10.1109/ACCESS.2024.3356568,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184003386&origin=inward,Article,SCOPUS_ID:85184003386,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),zddr: a zero-shot defender for adversarial samples detection and restoration,"
AbstractView references

Natural language processing (NLP) models find extensive applications but face vulnerabilities against adversarial inputs. Traditional defenses lean heavily on supervised detection techniques, which makes them vulnerable to issues arising from training data quality, inherent biases, noise, or adversarial inputs. This study observed common compromises in sentence fluency during aggression. On this basis, the Zero Sample Defender (ZDDR) is introduced for adversarial sample detection and recovery without relying on prior knowledge. ZDDR combines the log probability calculated by the model and the syntactic normative score of a large language model (LLM) to detect adversarial examples. Furthermore, using strategic prompts, ZDDR guides LLM in rephrasing adversarial content, maintaining clarity, structure, and meaning, thereby restoring the sentence from the attack. Benchmarking reveals a 9% improvement in area under receiver operating characteristic curve (AUROC) for adversarial detection over existing techniques. Post-restoration, model classification efficacy surges by 45% compared to the offensive inputs, setting new performance standards against other restoration techniques. © 2013 IEEE.
"
10.1148/radiol.231971,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184000797&origin=inward,Article,SCOPUS_ID:85184000797,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai-powered hyperrealism: next step in cinematic rendering?,"
AbstractView references

Background: Recent advancements in artificial intelligence (AI)–powered image generation present opportunities to enhance three-dimensional medical images. Diffusion, an iterative denoising process, represents the standard of many of the current tools used for this purpose. Purpose: To demonstrate the current capabilities of diffusion technology by using Midjourney, version 5.2, a text-to-image generative AI tool, and present a practical guide for its use. Materials and Methods: This exploratory study investigates the principles, parameters, and prompt engineering techniques for generating images focusing on Midjourney from July 27 to August 3, 2023. Step-by-step instructions show the innate capability of this technology in creating realistic medical images. Results: Thirty images were selected, including eye, skin, and vascular aneurysm images. Varying prompt phrasing and weighting techniques allowed for the customization of output image characteristics. Although the details of Midjourney’s model training are confidential, it is estimated that it was trained on at least hundreds of millions of images from the web. Anatomic fidelity was not always maintained because the training data set is not necessarily based on accurate medical images. There are shortcomings in this nascent technology regarding its ability to create entities such as digits of the hand or precise text. Conclusion: AI image generation has the potential to improve three-dimensional medical images for certain applications through added visual detail and appeal but ongoing collaboration is needed between radiologists and AI developers due to the overreliance on art and photography in the training data, which may result in inaccurate anatomic results. Moreover, the evolving landscape of ethical discussions and copyright stipulations warrants close attention. Supplemental material is available for this article.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183887388&origin=inward,Conference Paper,SCOPUS_ID:85183887388,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai assistant to improve experimentation in software startups using large language model and prompt engineering,"
AbstractView references

Software startup is a unique type of company with unique characteristics. On the one hand, they must offer innovative products appealing to customers to generate revenue and survive, but on the other hand, they are limited in resources, time, and experience. During the new product development, it is important to experiment with their original ideas. However, doing a meaningful experiment requires resources and challenges. A study on failed software startups shows that, despite its importance, many software startups skipped or did not experiment with their ideas. The study identifies 25 inhibitors spread in five experimentation stages. In the last few years, Large Language Models (LLMs) have become a popular technology. The advancement of LLM has made it adopted into many parts of the software development cycle. Studies show that LLM also has been used to generate new innovative product ideas and to manage innovation. However, there is no investigation into the possibility of utilizing the power of LLM to help software startups do experimentation. Interactions to an LLM are done through prompts. During the interaction or session, a user will send one or more prompts in a zero-, one-, or few-shots to an LLM agent. Unfortunately, learning and using prompts effectively requires time and resources, things that software startups are scarce with. In this project, we aim to help improve the experimentation process and address the inhibitors by leveraging the power of LLMs. There are five initial research questions and studies planned in the project. In the first step, we will investigate current experimentation practices, challenges, inhibitors, and the strategies used to circumvent them. Secondly, we will investigate how AI has been used in today's experimentation. Then, we will investigate the set of measurements available to measure the success of an experiment. The next step is to investigate how to support experimentation using LLMs followed by a validation sequence. The first form of support is a prompt guidebook to help software startups use an LLM agent to help their experimentation. The second form is an LLM-based assistant tailored specifically to guide the experimentation process. © 2006 Gesellschaft für Informatik, Bonn.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183864164&origin=inward,Conference Paper,SCOPUS_ID:85183864164,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai-driven digital business design assistant: a prototype demo,"
AbstractView references

Constant and rapid changes in the business environment require companies to transform daily from a business and IT perspective. Managers coordinate this transformation via various design activities: business model design, enterprise architecture management, enterprise design, digital service design, business process engineering and information system design. All these activities can be embraced by the term digital business design. Business/enterprise/domain architects, service designers, business development managers, digital transformation leaders, and business analysts are examples of roles involved in digital business design. The problem is that high-quality business design is time-consuming and requires specialised expertise, often resulting in (semi-)intuitive decision-making. Our AI-driven business design assistant addresses this problem by accelerating business design activities, lowering expertise requirements, and still focusing on high-quality resultant business artefacts. The solution enables easy visual design and collaboration (an app for the Miro collaboration platform), providing a shared repository of objects, a library of reference content (best practices), AI services and method guidance. AI services use a large language model (LLM) and include object extraction from corporate or market data, generation suggestions, and library content selection. Business design experts (e.g. consultants, trainers, lecturers) can customise and extend the tool in a no-code way. The current prototype of the solution is customised for the digital service design and rollout task, assisting the following subtasks: value proposition design, value delivery activities design and IT support planning. © 2006 Gesellschaft für Informatik, Bonn.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183862551&origin=inward,Conference Paper,SCOPUS_ID:85183862551,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"diversity, equity and inclusion in the age of generative ai","
AbstractView references

As generative AI services continue to expand across various industries, especially in the field of software engineering, it is critical to comprehend how these services represent different groups of people. Despite software engineers' crucial role in this industry, limited research exists on how generative AI image tools portray software engineers themselves. This study examines the representation of software engineers in generative AI image tools like DALL-E and Canva. Using qualitative analysis, we assess image outputs to identify representational themes. Our results indicate that while these tools can generate varied images, there are noticeable representation gaps, especially regarding gender and age. © 2006 Gesellschaft für Informatik, Bonn.
"
10.1007/s00766-023-00411-0,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183775500&origin=inward,Article,SCOPUS_ID:85183775500,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),advances in automated support for requirements engineering: a systematic literature review,"
AbstractView references

Requirements Engineering (RE) has undergone several transitions over the years, from traditional methods to agile approaches emphasising increased automation. In many software development projects, requirements are expressed in natural language and embedded within large volumes of text documents. At the same time, RE activities aim to define software systems' functionalities and constraints. However, manually executing these tasks is time-consuming and prone to errors. Numerous research efforts have proposed tools and technologies for automating RE activities to address this challenge, which are documented in published works. This review aims to examine empirical evidence on automated RE and analyse its impact on the RE sub-domain and software development. To achieve our goal, we conducted a Systematic Literature Review (SLR) following established guidelines for conducting SLRs. We aimed to identify, aggregate, and analyse papers on automated RE published between 1996 and 2022. We outlined the output of the support tool, the RE phase covered, levels of automation, development approach, and evaluation approaches. We identified 85 papers that discussed automated RE from various perspectives and methodologies. The results of this review demonstrate the significance of automated RE for the software development community, which has the potential to shorten development cycles and reduce associated costs. The support tools primarily assist in generating UML models (44.7%) and other activities such as omission of steps, consistency checking, and requirement validation. The analysis phase of RE is the most widely automated phase, with 49.53% of automated tools developed for this purpose. Natural language processing technologies, particularly POS tagging and Parser, are widely employed in developing these support tools. Controlled experimental methods are the most frequently used (48.2%) for evaluating automated RE tools, while user studies are the least employed evaluation method (8.2%). This paper contributes to the existing body of knowledge by providing an updated overview of the research literature, enabling a better understanding of trends and state-of-the-art practices in automated RE for researchers and practitioners. It also paves the way for future research directions in automated requirements engineering. © 2024, Crown.
"
10.3390/electronics13020320,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183441231&origin=inward,Article,SCOPUS_ID:85183441231,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),augmenting large language models with rules for enhanced domain-specific interactions: the case of medical diagnosis,"
AbstractView references

In this paper, we present a novel Artificial Intelligence (AI) -empowered system that enhances large language models and other machine learning tools with rules to provide primary care diagnostic advice to patients. Specifically, we introduce a novel methodology, represented through a process diagram, which allows the definition of generative AI processes and functions with a focus on the rule-augmented approach. Our methodology separates various components of the generative AI process as blocks that can be used to generate an implementation data flow diagram. Building upon this framework, we utilize the concept of a dialogue process as a theoretical foundation. This is specifically applied to the interactions between a user and an AI-empowered software program, which is called “Med|Primary AI assistant” (Alpha Version at the time of writing), and provides symptom analysis and medical advice in the form of suggested diagnostics. By leveraging current advancements in natural language processing, a novel approach is proposed to define a blueprint of domain-specific knowledge and a context for instantiated advice generation. Our approach not only encompasses the interaction domain, but it also delves into specific content that is relevant to the user, offering a tailored and effective AI–user interaction experience within a medical context. Lastly, using an evaluation process based on rules, defined by context and dialogue theory, we outline an algorithmic approach to measure content and responses. © 2024 by the authors.
"
10.1007/978-3-031-50139-5_8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183413598&origin=inward,Book Chapter,SCOPUS_ID:85183413598,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),opinion of spanish teachers about artificial intelligence and its use in education,"
AbstractView references

Although it has been with us for decades, artificial intelligence (AI) is becoming essential in society. It is beginning to be used in all areas, and education is no stranger. Sound practices are already beginning to be presented and made known in which the benefits of using AI in the classroom are verified, both in students’ learning outcomes and in teachers’ professional development. However, there are also many fears and suspicions caused by the use of this technology, especially when the effects that it may present in the future are unknown. This research tries to find out the opinion of Spanish teachers about AI and its use in the classroom. To this end, a brief ad hoc questionnaire was prepared, validated and publicized through different social networks and educational channels. A total of 599 teachers from all Spanish regions and educational stages answered the questionnaire. From the results, it can be extracted that Spanish teachers need more knowledge about AI and its implementation possibilities in the classroom. They see more possibilities for improving teacher professional development than teaching-learning processes. Moreover, they recognize that they need training in its use. In conclusion, in addition to effective and efficient training, the possible implementation of AI in education also requires a critical reflection on ethics and the pedagogical functionalities of AI. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/s00146-023-01852-5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183358410&origin=inward,Article,SCOPUS_ID:85183358410,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the sociotechnical entanglement of ai and values,"
AbstractView references

Scholarship on embedding values in AI is growing. In what follows, we distinguish two concepts of AI and argue that neither is amenable to values being ‘embedded’. If we think of AI as computational artifacts, then values and AI cannot be added together because they are ontologically distinct. If we think of AI as sociotechnical systems, then components of values and AI are in the same ontologic category—they are both social. However, even here thinking about the relationship as one of ‘embedding’ is a mischaracterization. The relationship between values and AI is best understood as a dimension of the relationship between technology and society, a relationship that can be theorized in multiple ways. The literature in this area is consistent in showing that technology and society are co-productive. Within the co-production framework, the relationship between values and AI is shown to be generative of new meaning. This stands in stark contrast to the framework of ‘embedding’ values which frames values as fixed things that can be inserted into technological artifacts. © 2024, The Author(s).
"
10.3390/buildings14010220,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183190607&origin=inward,Article,SCOPUS_ID:85183190607,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),opportunities and challenges of generative ai in construction industry: focusing on adoption of text-based models,"
AbstractView references

In the last decade, despite rapid advancements in artificial intelligence (AI) transforming many industry practices, construction largely lags in adoption. Recently, the emergence and rapid adoption of advanced large language models (LLMs) like OpenAI’s GPT, Google’s PaLM, and Meta’s Llama have shown great potential and sparked considerable global interest. However, the current surge lacks a study investigating the opportunities and challenges of implementing Generative AI (GenAI) in the construction sector, creating a critical knowledge gap for researchers and practitioners. This underlines the necessity to explore the prospects and complexities of GenAI integration. Bridging this gap is fundamental to optimizing GenAI’s early stage adoption within the construction sector. Given GenAI’s unprecedented capabilities to generate human-like content based on learning from existing content, we reflect on two guiding questions: What will the future bring for GenAI in the construction industry? What are the potential opportunities and challenges in implementing GenAI in the construction industry? This study delves into reflected perception in literature, analyzes the industry perception using programming-based word cloud and frequency analysis, and integrates authors’ opinions to answer these questions. This paper recommends a conceptual GenAI implementation framework, provides practical recommendations, summarizes future research questions, and builds foundational literature to foster subsequent research expansion in GenAI within the construction and its allied architecture and engineering domains. © 2024 by the authors.
"
10.3758/s13428-024-02344-0,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183139045&origin=inward,Article,SCOPUS_ID:85183139045,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),can generative ai infer thinking style from language? evaluating the utility of ai as a psychological text analysis tool,"
AbstractView references

Generative AI, short for Generative Artificial Intelligence, a class of artificial intelligence systems, is not currently the choice technology for text analysis, but prior work suggests it may have some utility to assess dynamics like emotion. The current work builds upon this empirical foundation to consider how analytic thinking scores from a large language model chatbot, ChatGPT, were linked to analytic thinking scores from dictionary-based tools like Linguistic Inquiry and Word Count (LIWC). Using over 16,000 texts from four samples and tested against three prompts and two large language models (GPT-3.5, GPT-4), the evidence suggests there were small associations between ChatGPT and LIWC analytic thinking scores (meta-analytic effect sizes:.058 < rs <.304; ps <.001). When given the formula to calculate the LIWC analytic thinking index, ChatGPT performed incorrect mathematical operations in 22% of the cases, suggesting basic word and number processing may be unreliable with large language models. Researchers should be cautious when using AI for text analysis. © 2024, The Psychonomic Society, Inc.
"
10.1109/TPAMI.2024.3354961,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182926169&origin=inward,Article,SCOPUS_ID:85182926169,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),point-to-pixel prompting for point cloud analysis with pre-trained image models,"
AbstractView references

Nowadays, pre-training big models on large-scale datasets has achieved great success and dominated many downstream tasks in natural language processing and 2D vision, while pre-training in 3D vision is still under development. In this paper, we provide a new perspective of transferring the pre-trained knowledge from 2D domain to 3D domain with <italic>Point-to-Pixel Prompting</italic> in data space and <italic>Pixel-to-Point distillation</italic> in feature space, exploiting shared knowledge in images and point clouds that display the same visual world. Following the principle of prompting engineering, <italic>Point-to-Pixel Prompting</italic> transforms point clouds into colorful images with geometry-preserved projection and geometry-aware coloring. Then the pre-trained image models can be directly implemented for point cloud tasks without structural changes or weight modifications. With projection correspondence in feature space, <italic>Pixel-to-Point distillation</italic> further regards pre-trained image models as the teacher model and distills pre-trained 2D knowledge to student point cloud models, remarkably enhancing inference efficiency and model capacity for point cloud analysis. We conduct extensive experiments in both object classification and scene segmentation under various settings to demonstrate the superiority of our method. In object classification, we reveal the important scale-up trend of Point-to-Pixel Prompting and attain 90.3% accuracy on ScanObjectNN dataset, surpassing previous literature by a large margin. In scene-level semantic segmentation, our method outperforms traditional 3D analysis approaches and shows competitive capacity in dense prediction tasks. Code is available at <uri>https://github.com/wangzy22/P2P</uri>. IEEE
"
10.1007/s00146-023-01840-9,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182713455&origin=inward,Article,SCOPUS_ID:85182713455,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),how will the state think with chatgpt? the challenges of generative artificial intelligence for public administrations,"
AbstractView references

This article explores the challenges surrounding generative artificial intelligence (GenAI) in public administrations and its impact on human‒machine interactions within the public sector. First, it aims to deconstruct the reasons for distrust in GenAI in public administrations. The risks currently linked to GenAI in the public sector are often similar to those of conventional AI. However, while some risks remain pertinent, others are less so because GenAI has limited explainability, which, in return, limits its uses in public administrations. Confidentiality, marking of GenAI outputs and errors are specific matters for which responses should be technical as well as cultural, as they are pushing the boundaries of our instrumental conceptions of machines. Second, this article proposes some paradigm shifts in the perspective of using GenAI in public administrations due to the radical change caused by its language-based nature. GenAI represents a profound break from the “numerical” nature of AI systems implemented in public administrations to date. The transformative impact of GenAI on the intellectual production of the state raises fears of the replacement, or rather enslavement, of civil servants to machines. The article argues for the development of critical thinking as a specific skill for civil servants who have become highly specialized and will have to think with a machine that is eclectic by nature. It anticipates a transformation in the political nature of public administrations, which should lead to more considerations for the strategic stake related to training corpus and for our conceptualization of the neutrality of AI. © 2024, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.
"
10.1177/14749041231221266,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182637673&origin=inward,Article,SCOPUS_ID:85182637673,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"chatgpt and the entangled evolution of society, education, and technology: a systems theory perspective","
AbstractView references

This paper presents a novel contribution to the discourse surrounding Large Language Models (LLMs) like ChatGPT in relation to education and society by using systems theory. We argue that ChatGPT can be understood not just as an ‘artificial’ intelligence but that it is entangled in the evolution of society and therefore education. ChatGPT is a subsystem of the autopoietic system of technology, which in modern society mediates between individual thinking, the physical world, and between thought and society. It is an instrumental tool and a semantic communication medium. With this bimodal framing, we consider ChatGPT and its role in society and education and consider the uses and implications of the technology. In this we respond to the need to introduce a scientific understanding of ChatGPT. We consider its emerging role in promoting educational inclusion, while also reflecting on challenges and limitations. We conclude by identifying the critical multi-dimensional skill sets required for individuals in a ChatGPT-integrated society and calls for strategic educational policies to facilitate this integration responsibly. Overall, this study paves the way for further research by providing a foundational understanding of LLMs through systems theory, thereby informing their ethical and effective incorporation into education. © The Author(s) 2024.
"
10.1007/978-3-031-50974-2_34,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182609064&origin=inward,Conference Paper,SCOPUS_ID:85182609064,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),large language models can accomplish business process management tasks,"
AbstractView references

Business Process Management (BPM) aims to improve organizational activities and their outcomes by managing the underlying processes. To achieve this, it is often necessary to consider information from various sources, including unstructured textual documents. Therefore, researchers have developed several BPM-specific solutions that extract information from textual documents using Natural Language Processing techniques. These solutions are specific to their respective tasks and cannot accomplish multiple process-related problems as a general-purpose instrument. However, in light of the recent emergence of Large Language Models (LLMs) with remarkable reasoning capabilities, such a general-purpose instrument with multiple applications now appears attainable. In this paper, we illustrate how LLMs can accomplish text-related BPM tasks by applying a specific LLM to three exemplary tasks: mining imperative process models from textual descriptions, mining declarative process models from textual descriptions, and assessing the suitability of process tasks from textual descriptions for robotic process automation. We show that, without extensive configuration or prompt engineering, LLMs perform comparably to or better than existing solutions and discuss implications for future BPM research as well as practical usage. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-50974-2_32,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182597010&origin=inward,Conference Paper,SCOPUS_ID:85182597010,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"abstractions, scenarios, and prompt definitions for process mining with llms: a case study","
AbstractView references

Large Language Models (LLMs) are capable of answering questions in natural language for various purposes. With recent advancements (such as GPT-4), LLMs perform at a level comparable to humans for many proficient tasks. The analysis of business processes could benefit from a natural process querying language and using the domain knowledge on which LLMs have been trained. However, it is impossible to provide a complete database or event log as an input prompt due to size constraints. In this paper, we apply LLMs in the context of process mining by i) abstracting the information of standard process mining artifacts and ii) describing the prompting strategies. We implement the proposed abstraction techniques into pm4py, an open-source process mining library. We present a case study using available event logs. Starting from different abstractions and analysis questions, we formulate prompts and evaluate the quality of the answers. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-47721-8_43,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182504520&origin=inward,Conference Paper,SCOPUS_ID:85182504520,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the role of the user in meaningful production with ai,"
AbstractView references

This poster documents the basic points of a research project that investigates the role of the user of AI systems in the production of meaning and content. Whether the user is a student, a researcher or an artist, the output of the human-machine cooperation should be original. As a generative assembly of textual or visual data, the outgoing content looks like the rephrasing of things that have already been said. But how AI generated texts and images surprise us? How is their production been triggered? Beginning form the assumption that the user is a compositor of thoughts and connotations, which, in cooperation with the AI system could lead to the formation of an out of the box thinking, we notice that her practice has similarities to the practices of the gardener, of the interviewer and of the interrogator. But what do these similarities concern? What do a gardener, an interviewer and an interrogator have in common with the user of AI? We’ll attempt to answer these questions with the help AI itself, placing ourselves in the position of the user in question, thus creating a simulation. This is what we like to call a tautological method, since the compositing procedure replicates itself, permitting a first person observation. Whether this approach will be fruitful of not, is something that we are going to discover at the end of this poster. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-44260-5_5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182466841&origin=inward,Book Chapter,SCOPUS_ID:85182466841,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),comparison of error correction and extraction approaches,"
AbstractView references

We compare different approaches for error correction detection and error correction. For the error correction detection task, the inputs are the last two utterances of a user and the output is whether there is an error correction in the last utterance. The error correction task gets the same inputs, but the output is the correction of the second last utterance according to the error correction in the last utterance and the extracted pairs of reparandum and repair entity. There are two advantages when using the compared approaches as utility component for a dialog system. It can be avoided to collect corrections for every new domain, and the extraction of the reparandum and repair pairs offers the possibility to learn from them. As benchmark for our comparison, we use an adapted version of the EPIC-KITCHENS-100 dataset. The best approach, a pipeline approach with a fine-tuned sequence labeling BERT model for error correction detection and a fine-tuned sequence-to-sequence T5 model for error correction, has an accuracy of 96.40% on synthetic validation data and an accuracy of 77.81% on human-created real-world test data. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1080/08963568.2024.2303944,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182454593&origin=inward,Article,SCOPUS_ID:85182454593,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an early or somewhat late chatgpt guide for librarians,"
AbstractView references

With over 180 million users engaging with ChatGPT by late-2023 estimates, the need for educational resources, workshops, and discussions on its use has become imperative. This article explores the evolving role of ChatGPT in academic libraries, highlighting proactive initiatives by university libraries to integrate the technology. Through 25 prompts nested in 10 relevant use cases, the authors underscore the utility of ChatGPT, while cautioning against the tool’s limitations. The article is a call for libraries to stay informed, skill share, and adapt their practices to harness the benefits of AI, while also mitigating potential pitfalls. © 2024 The Author(s). Published with license by Taylor & Francis Group, LLC.
"
10.1109/ACCESS.2024.3350773,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182363977&origin=inward,Article,SCOPUS_ID:85182363977,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a review of computer vision-based monitoring approaches for construction workers' work-related behaviors,"
AbstractView references

Construction workers' behaviors directly affects labor productivity and their own safety, thereby influencing project quality. Recognizing and monitoring the construction-related behaviors is therefore crucial for high-quality management and orderly construction site operation. Recent strides in computer vision technology suggest its potential to replace traditional manual supervision approaches. This paper explores research on monitoring construction workers' behaviors using computer vision. Through bibliometrics and content-based analysis, the authors present the latest research in this area from three perspectives: 'Detection, Localization, and Tracking for Construction Workers,' 'Recognition of Workers' Construction Activities,' and 'Occupational Health and Safety Behavior Monitoring.' In terms of the literature's volume, there has been a notable increase in this field. Notably, the focus on safety-related literature is predominant, underscoring the concern for occupational health. Vision algorithms have witnessed an increase in the utilization of object detection. The ongoing and future research trajectory is anticipated to involve multi-algorithm integration and an emphasis on enhancing robustness. Then the authors summarize the review from engineering impact and technical suitability, and analyze the limitations of current research from the perspectives of technical approaches and application scenarios. Finally, it discusses future research directions in this field together with generative AI models. Furthermore, the authors hope this paper can serves as a valuable reference for both scholars and engineers. © 2013 IEEE.
"
10.1109/TCSVT.2024.3349567,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182346274&origin=inward,Article,SCOPUS_ID:85182346274,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),mtartgpt: a multi-task art generation system with pre-trained transformer,"
AbstractView references

Instruction tuning large language models are making rapid advances in the field of artificial intelligence where GPT-4 models have exhibited impressive multi-modal perception capabilities. Such models have been used as the core assistant for many tasks including art generation. However, high-quality art generation relies heavily on human prompt engineering which is in general uncontrollable. To address these issues, we propose a multi-task AI generated content (AIGC) system for art generation. Specifically, a dense representation manager is designed to process multi-modal user queries and generate dense and applicable prompts to GPT. To enhance artistic sophistication of the whole system, we fine-tune the GPT model by a meticulously collected prompt-art dataset. Furthermore, we introduce artistic benchmarks for evaluating the system based on professional knowledge. Experiments demonstrate the advantages of our proposed MtArtGPT system. IEEE
"
10.1080/13506285.2023.2250518,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182168316&origin=inward,Article,SCOPUS_ID:85182168316,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"purple perils redux: open-ended, ai-resistant reasoning problems for introductory undergraduate sensation and perception instruction","
AbstractView references

In terms of pedagogy, there is a need in introductory sensation and perception courses for practice problems that promote critical thinking and elaborate on course material, like those given in the physical sciences. However, perceptual psychology mostly lacks the mathematical foundations that provide ready material for such elaborative assignments. Here I introduce a series of short assignments dubbed Purple Perils, which are inspired by the perception-related missives made famous by J. J. Gibson. These assignments provide students with open-ended opportunities to think through compelling and relatable problems in perception, often with an applied component. By their nature, such assignments are at present somewhat resistant to large language model AI assistance: probes of ChatGPT suggest that students who illicitly use such systems may not gain the synthesis of ideas that Purple Perils demand, and will therefore be disincentivized to use such illicit aid. This kind of assignment can reinforce learning in topics across sensory modalities, and can be used for exam questions. Purple Perils promote critical thinking and quantitative reasoning, e.g., through graphical literacy; reinforce biological knowledge; encourage classroom debate and discussion; and connect to historical and contemporary debates and findings. Gibson’s own Purple Peril about the El Greco fallacy leads the series of assignments. A protected, crowd-sourced, repository of Purple Perils is also deployed to allow sharing of questions among instructors. © 2024 Informa UK Limited, trading as Taylor & Francis Group.
"
10.1007/978-3-031-48550-3_19,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181984212&origin=inward,Conference Paper,SCOPUS_ID:85181984212,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),turning large language models into ai assistants for startups using prompt patterns,"
AbstractView references

Most startups operate with limited resources and experience. AI technologies enable them to accomplish many tasks under these constraints. The recent advance of large language models (LLMs) offers new opportunities to support startup endeavors. Given the nascent nature of LLMs, how they could be utilized to support startups is yet to be investigated. Since prompt engineering is believed to be at the core of the effective use of LLMs, we aim to understand how to apply prompt engineering to turn LLMs into AI assistants for startups. As the first step, we investigated the application of a set of prompt patterns to ChatGPT, arguably the most widely known LLM currently. The preliminary results show that some patterns are more suitable for brainstorming which is a typical activity conducted by early-stage startups. Prompt-tuned questions may lead to more specific and more detailed responses, but it is not guaranteed. Meantime, human factors play an important role in the effective application of prompt patterns. Large-size and systematic studies are needed to apply the right patterns to different questions, taking into account the differences among startups in terms of their startup knowledge, domain knowledge, and their attitudes and behaviors towards LLMs. © 2024, The Author(s).
"
10.1007/978-3-031-44131-8_6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181981959&origin=inward,Conference Paper,SCOPUS_ID:85181981959,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),artificial intelligence language models: the path to development or regression for education?,"
AbstractView references

Artificial intelligence language models have the potential to transform various aspects of the world, for example in the areas of human communication, industry, and science. However, the evolution of these language models also raises concerns about issues related to privacy, security, ethics and responsibility. In this sense, this exploratory research, supported by a literature review, aims to determine the benefits and risks of using these models in an educational context, namely, the ChatGPT (Generative Pre-trained Transformer). The ChatGPT is an artificial language model developed by OpenAI that can be used to generate natural language responses to a wide variety of questions and tasks. This model is trained on a large set of text data and uses deep learning techniques to generate relevant and contextually appropriate answers. Since this technology was launched very recently, in November 2022, in addition to the literature review, the authors make an evaluation of the technology with support in the interaction with it. The results of the research point to the existence of a set of educational potentialities, but also a range of risks in the use of this technology. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
"
10.1007/978-3-031-48536-7_8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181979460&origin=inward,Conference Paper,SCOPUS_ID:85181979460,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using chatgpt to encourage critical ai literacy skills and for assessment in higher education,"
AbstractView references

Generative AI is about to radically transform the way intellectual and creative work is being done. Since the release of ChatGPT in late 2022, the potential impact of generative AI tools on higher education has been intensively debated. ChatGPT can generate well-formulated human-like text passages and conversations that is often, but not always, of a surprisingly high quality. This paper reports on an early experiment to explore ways in which ChatGPT can be used in the higher education context. The experiment involved a written assignment which required postgraduate Information Systems students to formulate a critique of the outputs of ChatGPT to a specific question in Information Systems project management. The paper investigates the level of criticality that the students demonstrated in working with ChatGPT and assessing the quality of its outputs. It further explores the claim that ChatGPT can be used to generate rubrics and assess students’ assignments by asking ChatGPT to produce a rubric for critical thinking and assess the students’ assignments against the rubric produced. The findings indicate that students perceive the ChatGPT produced responses as generally accurate, although they tend to lack depth, with some key information omitted, produced biased responses and have limitations with academic writing conventions. The rubric that ChatGPT produced for assessing critical thinking is lacking in certain areas and the reliability of using it as an assessment tool is questionable given the inconsistency in the results. Overall, the paper concludes that while ChatGPT and other text generative AI can be useful learning and teaching companions for both students and lectures, human expertise and judgement is needed in working with ChatGPT. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-48550-3_20,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181976399&origin=inward,Conference Paper,SCOPUS_ID:85181976399,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt as a fullstack web developer - early results,"
AbstractView references

The arrival of ChatGPT has caused a lot of turbulence also in the field of software engineering in the past few months. Little is empirically known about the capabilities of ChatGPT to actually implement a complete system rather than a few code snippets. This paper reports the first-hand experiences from a graduate level student project where a real-life software platform for financial sector was implemented from the scratch by using ChatGPT for all possible software engineering tasks. The main conclusions drawn are as follows: 1) these findings demonstrate the potential for ChatGPT to be integrated into the software engineering workflow, 2) it can be used for creating a base for new components and for dividing coding tasks into smaller pieces, and 3) noticeable enhancements in ChatGPT-4, compared to ChatGPT-3.5, indicate superior working memory and the ability to continue incomplete responses, thereby leading to more coherent and less repetitive dialogues. © 2024, The Author(s).
"
10.1109/JIOT.2024.3349381,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181578780&origin=inward,Article,SCOPUS_ID:85181578780,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),vulnerability of machine learning approaches applied in iot-based smart grid: a review,"
AbstractView references

Machine learning (ML) sees an increasing prevalence of being used in the internet-of-things (IoT)-based smart grid. However, the trustworthiness of ML is a severe issue that must be addressed to accommodate the trend of ML-based smart grid applications (MLsgAPPs). The adversarial distortion injected into the power signal will greatly affect the system’s normal control and operation. Therefore, it is imperative to conduct vulnerability assessment for MLsgAPPs applied in the safety-critical power systems. In this paper, we provide a comprehensive review of the recent progress in designing attack and defense methods for MLsgAPPs. Unlike the traditional survey about ML security, this is the first review work about the security of MLsgAPPs that focuses on the characteristics of power systems. We first highlight the specifics for constructing adversarial attacks on MLsgAPPs. Then, the vulnerability of MLsgAPP is analyzed from the perspective of the power system and ML model, respectively. Afterward, a comprehensive survey is conducted to review and compare existing studies about the adversarial attacks on MLsgAPPs in scenarios of generation, transmission, distribution, and consumption, and the countermeasures are reviewed according to the attacks that they defend against. Finally, the future research directions are discussed on the attacker’s and defender’s side, respectively. We also analyze the potential vulnerability of large language model-based (e.g., ChatGPT) smart grid applications. Overall, our purpose is to encourage more researchers to contribute to investigating the adversarial issues of MLsgAPPs. IEEE
"
10.1109/TIV.2024.3349466,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181558346&origin=inward,Article,SCOPUS_ID:85181558346,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),integrating large language models and metaverse in autonomous racing: an education-oriented perspective,"
AbstractView references

This letter is the third report from a series of IEEE TIV's decentralized and hybrid workshops (DHWs) on intelligent vehicles for education (IV4E). Autonomous racing serves as a vital platform for nurturing engineering talents among university students, contributing to the development of skills essential for the intelligent vehicle industry. This letter investigates how recent emerging techniques, such as large language models (LLMs) and the Metaverse, can contribute to organizing IV4E-oriented autonomous racing events. Among these DHWs, scholars from diverse fields have collectively explored the integration of LLMs and the Metaverse into autonomous racing for educational purposes. The discussions emphasize the role of Metaverse in creating dynamic and immersive training virtual reality platforms and the role of LLMs in enhancing race commentary and the spectator experience. Within this context, the Metaverse introduces complex scenarios to the racetrack, maintaining suspense about the winning team until a race's final moment. This dynamic feature excites the race and motivates the participating teams to intensify their competition efforts. LLMs facilitate personalized commentary, inspiring spectators to become future participants in these races. Our DHWs highlighted a future in which technology, autonomy, and education intersect, fostering inclusive, educational, and engaging autonomous racing events. © 2016 IEEE.
"
10.1111/bjet.13425,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181494686&origin=inward,Article,SCOPUS_ID:85181494686,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),investigation of the moderation effect of gender and study level on the acceptance and use of generative ai by higher education students: comparative evidence from poland and egypt,"
AbstractView references

This study delves into the implications of incorporating AI tools, specifically ChatGPT, in higher education contexts. With a primary focus on understanding the acceptance and utilization of ChatGPT among university students, the research utilizes the Unified Theory of Acceptance and Use of Technology (UTAUT) as the guiding framework. The investigation probes into four crucial constructs of UTAUT—performance expectancy, effort expectancy, social influence and facilitating conditions—to understand their impact on the intent and actual use behaviour of students. The study relies on data collected from six universities in two countries and assessed through descriptive statistics and structural equation modelling techniques, and also takes into account participants' gender and study level. The key findings show that performance expectancy, effort expectancy, and social influence significantly influence behavioural intention. Furthermore, behavioural intention, when considered alongside facilitating conditions, influences actual use behaviour. This research also explores the moderating impact of gender and study level on the relationships among these variables. The results not only augment our comprehension of technology acceptance in the context of AI tools but also provide valuable input for formulating strategies that promote effective incorporation of ChatGPT in higher education. The study underscores the need for effective awareness initiatives, bespoke training programmes, and intuitive tool designs to bolster students' perceptions and foster the wider adoption of AI tools in education.Practitioner notesWhat is already known about this topic ChatGPT is a tool that is quickly gaining worldwide recognition. ChatGPT helps with writing essays and solving assignments. ChatGPT raises ethical concerns about authorship, plagiarism and ethics. What this paper adds This study explores students' acceptance of ChatGPT as an aid in their education, which has not been studied previously. We used the extended Unified Technology Acceptance and Use of Technology theory to test what factors mostly influence the use of ChatGPT by students. We conducted a multiple study in Poland and Egypt based on sampling strategy from six universities. Implications for practice and/or policy ChatGPT is a global game changer and should be incorporated into study programmes. The limitations of ChatGPT should be well explained and known since it is prone to making mistakes. Higher education teachers should be aware of ChatGPT's capabilities. © 2024 British Educational Research Association.
"
10.1007/978-3-031-49333-1_2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180778596&origin=inward,Conference Paper,SCOPUS_ID:85180778596,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a formal metamodel for software architectures with composite components,"
AbstractView references

Formal component-based modeling has been shown to be invaluable for verifying the compatibility of specified components, discovering flaws early in design stages, and enabling the reuse of components, across multiple projects and teams. However, complex system specifications are large and difficult to reason with which has limited the adoption of formal approaches. In this paper, we use a formal language to build a metamodel to represent software architectures consisting of composite components. First, we propose a metamodel to describe the high-level concepts of software architectures in a component-port-connector fashion. We focus on providing hierarchical modeling capabilities by considering the construction of composite components from existing ones. Second, using Alloy as a tooled formal language, we formalize the metamodel concepts to build a reusable framework for modeling complex systems consisting of composite component structures that can be automatically constructed and checked for architectural conformance. We use a smart metering system to demonstrate the use our formal metamodel. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-46002-9_23,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180630873&origin=inward,Conference Paper,SCOPUS_ID:85180630873,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"large language model assisted software engineering: prospects, challenges, and a case study","
AbstractView references

Large language models such as OpenAI’s GPT and Google’s Bard offer new opportunities for supporting software engineering processes. Large language model assisted software engineering promises to support developers in a conversational way with expert knowledge over the whole software lifecycle. Current applications range from requirements extraction, ambiguity resolution, code and test case generation, code review and translation to verification and repair of software vulnerabilities. In this paper we present our position on the potential benefits and challenges associated with the adoption of language models in software engineering. In particular, we focus on the possible applications of large language models for requirements engineering, system design, code and test generation, code quality reviews, and software process management. We also give a short review of the state-of-the-art of large language model support for software construction and illustrate our position by a case study on the object-oriented development of a simple “search and rescue” scenario. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-50188-3_7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180629085&origin=inward,Conference Paper,SCOPUS_ID:85180629085,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative ai for healthcare engineering and technology challenges,"
AbstractView references

Healthcare field engineers play a critical role in ensuring the smooth operation and maintenance of medical equipment. However, they face numerous challenges such as adhering to standard operating procedures (SOPs), managing inventory, maintaining equipment quality, and optimizing time allocation. This research paper proposes a novel approach that harnesses the power of generative artificial intelligence (AI) to overcome these challenges. In this study, generative AI algorithms are employed to develop an intelligent system that assists healthcare field engineers in following SOPs accurately while being always compliant. This is aimed to ensure consistent and efficient procedures, leading to improved equipment performance and patient safety. Additionally, the system integrates generative AI techniques to uphold equipment quality. It transforms lengthy equipment manuals into interactive Q&A systems, enabling engineers to focus on their tasks and access key information as needed. This enhances engineer productivity and indirectly contributes to the equipment’s working quality. While from a use case perspective, generative AI seems to effectively solve the problem of manually referring SOPs, compliance manuals and product catalogs. There would be technology challenges (especially around Artificial Intelligence) like data security, geo-political influences on data governance, dependency on specific technology platforms in addition to maintaining such systems over time effectively. In summary, this research introduces an innovative solution to address challenges faced by healthcare field engineers through the application of generative AI. By utilizing machine learning algorithms, the proposed system enhances adherence to standard operating procedures (SOPs), streamlines inventory management, improves equipment quality maintenance, and optimizes time management. The study’s outcomes contribute to the efficient implementation of SOP adherence and process guidelines, while also providing guidelines to tackle long-term challenges related to technology maintenance, ethical compliance of AI systems, mitigation of risks and data governance influenced by the dynamic geopolitical landscape. © 2024, IFIP International Federation for Information Processing.
"
10.1007/978-3-031-46002-9_24,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180624511&origin=inward,Conference Paper,SCOPUS_ID:85180624511,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt in the loop: a natural language extension for domain-specific modeling languages,"
AbstractView references

This paper presents an approach to no-code development based on the interplay of formally defined (graphical) Domain-Specific Languages and informal, intuitive Natural Language which is enriched with contextual information to enable referencing of formally defined entities. The paper focuses on the use and automated integration of these enriched intuitive languages via ChatGPT-based code generation to exploit the best of both language paradigms for domain-specific application development. To compensate for the lack of control over the intuitive languages we apply automated system-level validation via automata learning and subsequent model checking. All this is illustrated using the development of point-and-click adventures as a minimal viable example. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1016/j.asoc.2023.111165,S1568494623011833,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180587044&origin=inward,Article,SCOPUS_ID:85180587044,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),systems engineering issues for industry applications of large language model,"
                  Large language model (LLM) is an important direction in the development of AGI, but its technology is still in rapid change, and its capabilities still have obvious deficiencies and imbalances, with persistent problems such as hallucination, value non-alignment, weak specialization, and black-box effect. In this case, how to apply LLM to different professional fields and develop high-quality AIGC industry applications has become a great challenge for ISVs. Building AIGC industry applications based on LLM is not simply a matter of functional realization. Although researchers and open-source communities have proposed numerous application development frameworks or tool components, there is a lack of overall architecture design for systems engineering and a lack of discussion on theories and methods of LLM application development in large-scale industry domains, such as healthcare, government affairs, finance, and media. This paper analyzes the basic ideas of LLM industry applications development, defines the functional requirements and feature requirements of LLM industry applications, puts forward the concept of Large Language Model Systems Engineering (LLM-SE), and develops an AI assisted clinical risk prediction system for amyloidosis disease based on the architecture of LLM-SE, which adopt knowledge engineering, quality engineering, etc., and verifies the LLM-SE development architecture and methodology.
               "
10.1007/978-3-031-48796-5_10,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180539015&origin=inward,Conference Paper,SCOPUS_ID:85180539015,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),stableyolo: optimizing image generation for large language models,"
AbstractView references

AI-based image generation is bounded by system parameters and the way users define prompts. Both prompt engineering and AI tuning configuration are current open research challenges and they require a significant amount of manual effort to generate good quality images. We tackle this problem by applying evolutionary computation to Stable Diffusion, tuning both prompts and model parameters simultaneously. We guide our search process by using Yolo. Our experiments show that our system, dubbed StableYolo, significantly improves image quality (52% on average compared to the baseline), helps identify relevant words for prompts, reduces the number of GPU inference steps per image (from 100 to 45 on average), and keeps the length of the prompt short (≈ 7 keywords). © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-48796-5_12,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180532579&origin=inward,Conference Paper,SCOPUS_ID:85180532579,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),evaluating explanations for software patches generated by large language models,"
AbstractView references

Large language models (LLMs) have recently been integrated in a variety of applications including software engineering tasks. In this work, we study the use of LLMs to enhance the explainability of software patches. In particular, we evaluate the performance of GPT 3.5 in explaining patches generated by the search-based automated program repair system ARJA-e for 30 bugs from the popular Defects4J benchmark. We also investigate the performance achieved when explaining the corresponding patches written by software developers. We find that on average 84% of the LLM explanations for machine-generated patches were correct and 54% were complete for the studied categories in at least 1 out of 3 runs. Furthermore, we find that the LLM generates more accurate explanations for machine-generated patches than for human-written ones. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/s10489-023-05201-3,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180499028&origin=inward,Article,SCOPUS_ID:85180499028,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),determining the best feature combination through text and probabilistic feature analysis for gpt-2-based mobile app review detection,"
AbstractView references

Mobile apps, used by many people worldwide, have become an essential part of life. Before using a mobile app, users judge the reliability of apps according to their reviews. Therefore, app reviews are essential components of management for companies. Unfortunately, some fake reviewers write negative reviews for competing apps. Moreover, artificial intelligence (AI)-based macro bot programs that generate app reviews have emerged and can create large numbers of reviews with malicious purposes in a short time. One notable AI technology that can generate such reviews is Generative Pre-trained Transformer-2 (GPT-2). The reviews generated by GPT-2 use human-like grammar; therefore, it is difficult to detect them with only text mining techniques, which use tools like part-of-speech (POS) tagging and sentiment scores. Thus, probability-based sampling techniques in GPT-2 must be used. In this study, we identified features to detect reviews generated by GPT-2 and determined the optimal feature combination for improving detection performance. To achieve this, based on the analysis results, we built a training dataset to find the best feature combination for detecting the generated reviews. Various machine learning models were then trained and evaluated using this dataset. As a result, the model that used both text mining and probability-based sampling techniques detected generated reviews more effectively than the model that used only text mining techniques. This model achieved a top classification accuracy of 90% and a macro F1 of 0.90. We expect the results of this study to help app developers maintain a more stable mobile app ecosystem. Graphical abstract: (Figure presented.) © The Author(s) 2023.
"
10.1007/978-3-031-49252-5_14,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180157925&origin=inward,Conference Paper,SCOPUS_ID:85180157925,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards llm-based system migration in language-driven engineering,"
AbstractView references

In this paper we show how our approach of extending Language Driven Engineering (LDE) with natural language-based code generation supports system migration: The characteristic decomposition of LDE into tasks that are solved with dedicated domain-specific languages divides the migration tasks into portions adequate to apply LLM-based code generation. We illustrate this effect by migrating a low-code/no-code generator for point-and-click adventures from JavaScript to TypeScript in a way that maintains an important property: generated web applications can automatically be validated via automata learning and model analysis by design. In particular, this allows to easily test the correctness of migration by learning the difference automaton for the generated products of the source and the target system of the migration. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1109/JBHI.2023.3327951,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179225051&origin=inward,Article,SCOPUS_ID:85179225051,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),genhpf: general healthcare predictive framework for multi-task multi-source learning,"
AbstractView references

Despite the remarkable progress in the development of predictive models for healthcare, applying these algorithms on a large scale has been challenging. Algorithms trained on a particular task, based on specific data formats available in a set of medical records, tend to not generalize well to other tasks or databases in which the data fields may differ. To address this challenge, we propose General Healthcare Predictive Framework (GenHPF), which is applicable to any EHR with minimal preprocessing for multiple prediction tasks. GenHPF resolves heterogeneity in medical codes and schemas by converting EHRs into a hierarchical textual representation while incorporating as many features as possible. To evaluate the efficacy of GenHPF, we conduct multi-task learning experiments with single-source and multi-source settings, on three publicly available EHR datasets with different schemas for 12 clinically meaningful prediction tasks. Our framework significantly outperforms baseline models that utilize domain knowledge in multi-source learning, improving average AUROC by 1.2%P in pooled learning and 2.6%P in transfer learning while also showing comparable results when trained on a single EHR dataset. Furthermore, we demonstrate that self-supervised pretraining using multi-source datasets is effective when combined with GenHPF, resulting in a 0.6%P AUROC improvement compared to models without pretraining. By eliminating the need for preprocessing and feature engineering, we believe that this work offers a solid framework for multi-task and multi-source learning that can be leveraged to speed up the scaling and usage of predictive algorithms in healthcare. © 2013 IEEE.
"
10.1007/978-981-99-8391-9_4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178591574&origin=inward,Conference Paper,SCOPUS_ID:85178591574,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),lateral ai: simulating diversity in virtual communities,"
AbstractView references

In this paper, we present Lateral AI that offers a diverse and multi-dimensional world experience. It makes use of semi-automated prompt engineering on top of GPT3.5. The coupling with named entity recognition and text summarization enables creation of a diversity of AI personas and a multiplicity of requests. The features of Lateral AI, such as creation of custom AI personas, prioritisation of user-embedded knowledge in those personas and follow-up requests, enable users to co-create with AI. Users can contribute certain information and perspectives to the application if a Large Language Model does not have access to it. Lateral AI makes the user an active component of the integrated system rather than a mere AI consumer. We demonstrate use of Lateral AI to generate a range of diverse responses and illustrate the ability of AI to predict beyond its factual knowledge. Lateral AI is a unique and alternative option to other AI models, contributing to the diverse and creative pool of emerging AI technologies and applications. The principles behind Lateral AI can be used to simulate diverse communities in a variety of settings such as online virtual communities and human robotics. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
10.1007/978-981-99-8391-9_6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178574570&origin=inward,Conference Paper,SCOPUS_ID:85178574570,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a prompting framework to enhance language model output,"
AbstractView references

This research investigates the role of prompt engineering in enhancing the performance and generalisation of large-scale language models (LLMs) across a wide range of Natural Language Processing (NLP) tasks. The study introduces a comprehensive framework for prompt engineering, titled the “PERFECT” framework, and evaluates its effectiveness across different tasks and domains. The research findings underscore the pivotal role of advanced prompting techniques in eliciting more nuanced and flexible responses from AI models. The study also explores the future implications of prompt engineering, including the integration of reinforcement learning with human feedback, the emergence of prompt engineering as a new job market, and the rise of context-aware and interactive prompts. The research contributes to a deeper understanding of the principles, mechanisms, and best practices in prompt engineering, with practical implications for improving LLM performance and reducing the barrier to entry for new adoptees through using prompting frameworks. The research aims have been largely achieved, providing a new framework for prompting while also exploring future advancements. However, the study also highlights the need for further exploration of the constraints placed on current prompting techniques, such as token size and context window. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177864887&origin=inward,Article,SCOPUS_ID:85177864887,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),artificial intelligence model for citizen service in mixed-economy companies,"
AbstractView references

In a competitive and constantly evolving world, excellence in customer service is critical to business success; however, the management of PQRS (Petitions, Complaints, Claims, and Suggestions) can be challenging. In this context, data-driven artificial intelligence, artificial intelligence emerges as a powerful transformational tool in this area, using natural language processing algorithms and data analytics, and can provide personalized and rapid responses, thereby improving the customer experience. Similarly, machine learning is critical as it enables automation and improves the quality of responses; furthermore, machine learning systems can analyze large volumes of information, identify patterns and trends, and learn from feedback. These systems can understand the content of requests and generate accurate and relevant responses, adapting to the needs of each user, optimizing customer service, and improving problem resolution. This thesis presents a practical approach for mixed-economy companies interested in optimizing their customer service processes as follows: data-driven artificial intelligence can drive operational efficiency and overall business success; the combination of industrial engineering and artificial intelligence offers opportunities to optimize processes; and simulation and data analytics facilitate decision making, empowering industrial engineers to efficiently optimize processes. © 2024, Ismail Saritas. All rights reserved.
"
10.1007/978-981-99-7587-7_29,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177193239&origin=inward,Conference Paper,SCOPUS_ID:85177193239,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),applications and implication of generative ai in non-stem disciplines in higher education,"
AbstractView references

There has been considerable research on the use of generative artificial intelligence techniques to support teaching and learning in science, technology, engineering, and mathematics (STEM) subjects in higher education. However, few studies have explored the role of such technologies in non-STEM subjects in higher education. This paper reviews the relevant literature on the application of generative AI in higher education and proposes the application and implications of using generative AI tools to support student and instructors work in non-STEM higher education disciplines. An assessment of the role of AI in complex student tasks in non-STEM subjects is provided. Several considerations for the effective use of generative AI in non-STEM higher education are suggested. Faculty and students should focus on: 1) ensuring that ethical and moral implications are addressed; 2) using AI to augment rather than replace human intelligence; 3) using AI as an instructional tool rather than a fully automated system; 4) using AI to improve academic assessment and self-assessment methods; 5) critically reviewing the results of generative AI systems. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
10.1007/978-981-99-7587-7_13,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177179104&origin=inward,Conference Paper,SCOPUS_ID:85177179104,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an assessment of chatgpt on log data,"
AbstractView references

Recent development of large language models (LLMs), such as ChatGPT has been widely applied to a wide range of software engineering tasks. Many papers have reported their analysis on the potential advantages and limitations of ChatGPT for writing code, summarization, text generation, etc. However, the analysis of the current state of ChatGPT for log processing has received little attention. Logs generated by large-scale software systems are complex and hard to understand. Despite their complexity, they provide crucial information for subject matter experts to understand the system status and diagnose problems of the systems. In this paper, we investigate the current capabilities of ChatGPT to perform several interesting tasks on log data, while also trying to identify its main shortcomings. Our findings show that the performance of the current version of ChatGPT for log processing is limited, with a lack of consistency in responses and scalability issues. We also outline our views on how we perceive the role of LLMs in the log processing discipline and possible next steps to improve the current capabilities of ChatGPT and the future LLMs in this area. We believe our work can contribute to future academic research to address the identified issues. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
10.1007/978-981-99-7587-7_3,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177174389&origin=inward,Conference Paper,SCOPUS_ID:85177174389,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),inheritance and revitalization: exploring the synergy between aigc technologies and chinese traditional culture,"
AbstractView references

Diffusion models like Stable Diffusion have made impressive progress in T2I (text-to-image) generation. However, when applied to image generation tasks concerned with Chinese cultural subjects, Stable Diffusion needs to improve the quality of its results. This paper proposes a practical approach to address the challenges of utilizing popular AIGC (AI-Generated Content) technologies and integrating them into a cohesive system, which makes it easier to use Stable Diffusion to create high-quality generated images related to Chinese cultural subjects with direct Chinese prompts. Specifically, with the capabilities of Large Language Models (LLMs), the approach can weave expressive visual descriptions based on initial inputs (scattered words in Chinese, Chinese poems…) and align them in suitable English text prompts for subsequent image generation with Stable Diffusion. Through the parameter-efficient finetuning method called LoRA, Stable Diffusion can effectively learn complex and nuanced concepts of Chinese culture. Additionally, Prompt Engineering plays a role in optimizing inputs, assuring quality and stability, and setting the detailed behavior patterns of LLMs throughout the workflow. This success is attributed to overcoming the constraints of accepting only English prompts and significantly improving the understanding of certain concepts in Chinese culture. The experiments show that our method can produce high-quality images associated with complex and nuanced concepts in Chinese culture by leveraging the fusion of all independent components. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
10.1007/s11528-023-00911-4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176544428&origin=inward,Article,SCOPUS_ID:85176544428,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative artificial intelligence in education and its implications for assessment,"
AbstractView references

The abrupt emergence and rapid advancement of generative artificial intelligence (AI) technologies, transitioning from research labs to potentially all aspects of social life, has brought a profound impact on education, science, arts, journalism, and every facet of human life and communication. The purpose of this paper is to recapitulate the use of AI in education and examine potential opportunities and challenges of employing generative AI for educational assessment, with systems thinking in mind. Following a review of the opportunities and challenges, we discuss key issues and dilemmas associated with using generative AI for assessment and for education in general. We hope that the opportunities, challenges, and issues discussed in this paper could serve as a foundation for educators to harness the power of AI within the digital learning ecosystem. © 2023, Association for Educational Communications & Technology.
"
10.1007/s11528-023-00900-7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176102675&origin=inward,Article,SCOPUS_ID:85176102675,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"the ethical consequences, contestations, and possibilities of designs in educational systems","
AbstractView references

Emerging technologies present new possibilities for schools, but also present ethical issues for designers. Ethical issues arising from the design, accessibility, adoption, and implementation of emerging technologies in schools are intertwined with existing power dynamics, hierarchies, and decision-making norms that perpetuate entrenched systems. Using a framework called the fives spaces for design in education framework as an analytical lens, we explore the ethical implications of two emerging artificial intelligence technologies in education: remote proctoring software and large language models. We find that designers adopting and implementing these emerging technologies must attend to the consequences of past design decisions and recognize that emerging technologies also create places for resistance and contestations. Lastly, by recognizing the wide scope of what can be redesigned, designers can start to see possibilities for redesigning in ways that are inclusive, equitable, and ethically conscious. Ultimately, we hope to begin a critical conversation about the two technologies by thinking about the sites of consequence, contestation, and possibilities in the designed cultures, systems, experiences, processes, and artifacts of schooling. © 2023, Association for Educational Communications & Technology.
"
10.1007/978-981-99-7339-2_50,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176016418&origin=inward,Conference Paper,SCOPUS_ID:85176016418,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),abstractive summarization evaluation for prompt engineering,"
AbstractView references

The task of summarizing large documents for easier and faster readability is widely acknowledged and a standard task in the field of Natural Language Processing. The metrics that are used to measure the working of this task are based on statistical measures such as n-grams and Longest Common Subsequences. Abstractive summarization is a type of automatic text summarization which refers to creating the summary from the main document without entirely copying words from the original document. With the advent of Deep learning architectures, abstract summarization has increased in popularity. The Large language models which are used for abstractive text summarization need proper prompts to generate summary. Engineering proper prompts is important as the quality of the summary generated depends on the prompt. In this paper, an abstractive measure of similarity is proposed where the textual similarity is measured by using Euclidean distance to compare a Principal Component based transformed BERT Embedding vector of the document and the summary. The metric is used to create prompts for Generative Pretrained Transformer and Text to Text Transfer Transformer models which are standard state-of-the-art language models. The summary generated shows significant improvement and the prompts generated by using the abstractive comparison metric are seen to have the perplexity almost the same as the document thus promising better summarization results. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.
"
10.1007/s11528-023-00894-2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176013016&origin=inward,Article,SCOPUS_ID:85176013016,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring artificial intelligence tools and their potential impact to instructional design workflows and organizational systems,"
AbstractView references

This article explores the potential impact of Artificial Intelligence (AI) tools on Instructional Design (ID) workflows and organizations from a systems thinking perspective (Meadows, 2008). We provide an in-depth analysis of how three AI tools, ChatGPT, Midjourney, and Descript, can enhance efficiency in instructional design content creation processes. We also consider, in our analysis, challenges and ethical considerations associated with the integration of AI in instructional design workflows. Our explorations and findings suggest that AI holds significant potential for improving content creation processes for instructional design, allowing instructional designers more time for higher order thinking tasks and ensuring content quality. Considerations of AI’s possibilities and limitations were also identified as crucial to ensure quality of content and ethical use. Furthermore, we conclude that AI prompt engineering is an important skill in AI-assisted ID workflows. This article contributes to the ongoing discourse on the intersection of AI and instructional design practice and suggests possible opportunities for subsequent empirical research. © 2023, Association for Educational Communications & Technology.
"
10.1109/TLT.2023.3324714,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174820021&origin=inward,Article,SCOPUS_ID:85174820021,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),unleashing chatgpt's power: a case study on optimizing information retrieval in flipped classrooms via prompt engineering,"
AbstractView references

This research project investigates the impact of prompt engineering, a key aspect of chat generative pretrained transformer (ChatGPT), on college students' information retrieval in flipped classrooms. In recent years, an increasing number of students have been using AI-based tools, such as ChatGPT rather than traditional research engines to learn and to complete course assignments. Despite this growing trend, previous research has largely overlooked the influence of prompt engineering on students' use of ChatGPT and effective strategies for improving the quality of information retrieval in learning settings. To address this research gap, this study examines the information quality obtained from ChatGPT in a flipped classroom by evaluating its effectiveness in task completion among 26 novice undergraduates from the same major and cohort. The experimental results provide evidence that proficient mastery of prompt engineering improves the quality of information obtained by students using ChatGPT. Consequently, by acquiring proficiency in prompt engineering, students can maximize the positive impact of ChatGPT, obtain high-quality information, and enhance their learning efficiency in flipped classrooms. © 2008-2011 IEEE.
"
10.1007/s11528-023-00896-0,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174411230&origin=inward,Article,SCOPUS_ID:85174411230,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompting change: exploring prompt engineering in large language model ai and its potential to transform education,"
AbstractView references

This paper explores the transformative potential of Large Language Models Artificial Intelligence (LLM AI) in educational contexts, particularly focusing on the innovative practice of prompt engineering. Prompt engineering, characterized by three essential components of content knowledge, critical thinking, and iterative design, emerges as a key mechanism to access the transformative capabilities of LLM AI in the learning process. This paper charts the evolving trajectory of LLM AI as a tool poised to reshape educational practices and assumptions. In particular, this paper breaks down the potential of prompt engineering practices to enhance learning by fostering personalized, engaging, and equitable educational experiences. The paper underscores how the natural language capabilities of LLM AI tools can help students and educators transition from passive recipients to active co-creators of their learning experiences. Critical thinking skills, particularly information literacy, media literacy, and digital citizenship, are identified as crucial for using LLM AI tools effectively and responsibly. Looking forward, the paper advocates for continued research to validate the benefits of prompt engineering practices across diverse learning contexts while simultaneously promoting potential defects, biases, and ethical concerns related to LLM AI use in education. It calls upon practitioners to explore and train educational stakeholders in best practices around prompt engineering for LLM AI, fostering progress towards a more engaging and equitable educational future. © 2023, Association for Educational Communications & Technology.
"
10.1002/aic.18259,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173498639&origin=inward,Article,SCOPUS_ID:85173498639,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),toward automatic generation of control structures for process flow diagrams with large language models,"
AbstractView references

Developing Piping and Instrumentation Diagrams (P&IDs) is a crucial step during process development. We propose a data-driven method for the prediction of control structures. Our methodology is inspired by end-to-end transformer-based human language translation models. We cast the control structure prediction as a translation task where Process Flow Diagrams (PFDs) without control structures are translated to PFDs with control structures. We represent the topology of PFDs as strings using the SFILES 2.0 notation. We pretrain our model using generated PFDs to learn the grammatical structure. Thereafter, the model is fine-tuned leveraging transfer learning on real PFDs. The model achieved a top-5 accuracy of 74.8% on 10,000 generated PFDs and 89.2% on 100,000 generated PFDs. These promising results show great potential for AI-assisted process engineering. The tests on a dataset of 312 real PFDs indicate the need for a larger PFD dataset for industry applications and hybrid artificial intelligence solutions. © 2023 The Authors. AIChE Journal published by Wiley Periodicals LLC on behalf of American Institute of Chemical Engineers.
"
10.1007/s11528-023-00893-3,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171766392&origin=inward,Article,SCOPUS_ID:85171766392,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),are we dupes? limitations of ai systems: what should educators do with them?,"
AbstractView references

Extant chatbots such as ChatGPT and Bard are currently able to converse with humans in natural language, demonstrating impressive linguistic responses. Or so it seems. I critically examine artificial intelligence systems such as these chatbots through examples of dialogue. When taking a systems view of AI, there is a vast and unique human culture in the environmental surroundings of the AI system (its negasystem) that is not accessible to extant AI systems. These generative AI systems, based on large language models, are trained with trillions of signs created by humans in the form of digital text and images as part of their machine learning from which they construct their unique neural networks. However, AI systems do not understand well, if at all, the meanings of those signs that we associate with our human experience of the world and our culture (i.e., in the AI negasystem). Similarly, we humans do not understand well the inner workings of an AI system (its neural network). Teachers and students in education must be very careful and cautious when using such AI systems. Are we dupes? Or not? Without thinking critically and checking facts independently, we can be fooled by responses of those AI systems. © 2023, Association for Educational Communications & Technology.
"
10.1016/j.eswa.2023.121186,S0957417423016883,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169601961&origin=inward,Article,SCOPUS_ID:85169601961,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),can chatgpt provide intelligent diagnoses? a comparative study between predictive models and chatgpt to define a new medical diagnostic bot,"Intelligent diagnosis processes rely on Artificial Intelligence (AI) techniques to provide possible diagnoses by analyzing patient data and medical information. To make accurate and quick diagnoses, it is possible to use AI tools to efficiently analyze huge amounts of data and find patterns that a clinician might miss. In recent years, new large language models (LLMs), such as ChatGPT and Google BARD, have shown remarkable capabilities in several domains, including intelligent diagnostics. This research aims to compare the performances of ChatGPT and traditional machine learning models for making diagnoses of low- and medium- risk diseases only based on their symptoms. On the basis of our study, we defined four research questions: RQ1) What are the benefits and limitations of using ChatGPT in intelligent diagnosis? RQ2) How do traditional machine learning approaches compare to ChatGPT for intelligent diagnosis? RQ3) How does ChatGPT compare with other LLMs and domain-specific natural language processing models in the intelligent diagnosis tasks?, and RQ4) What are the implications of the predictive models and ChatGPT for healthcare, and how can they be used to support people?. To answer these RQs, we first evaluate the performances of different engines of ChatGPT, also introducing a new prompt engineering methodology specifically tailored for achieving accurate diagnostic outcomes. Moreover, we compare these results with those achieved by different predictive models trained for intelligent diagnosis tasks, i.e., Google BARD, and two domain-specific NLP models. Finally, we propose a new interactive bot available for users that relies on the best-performing models evaluated in the previous steps. The experiments have been conducted using two medical datasets for disease prediction consisting of more than 100 symptoms associated with several diagnoses."
10.1080/08982112.2023.2206479,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162001829&origin=inward,Article,SCOPUS_ID:85162001829,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"how generative ai models such as chatgpt can be (mis)used in spc practice, education, and research? an exploratory study","
AbstractView references

Generative Artificial Intelligence (AI) models such as OpenAI’s ChatGPT have the potential to revolutionize Statistical Process Control (SPC) practice, learning, and research. However, these tools are in the early stages of development and can be easily misused or misunderstood. In this paper, we give an overview of the development of Generative AI. Specifically, we explore ChatGPT’s ability to provide code, explain basic concepts, and create knowledge related to SPC practice, learning, and research. By investigating responses to structured prompts, we highlight the benefits and limitations of the results. Our study indicates that the current version of ChatGPT performs well for structured tasks, such as translating code from one language to another and explaining well-known concepts but struggles with more nuanced tasks, such as explaining less widely known terms and creating code from scratch. We find that using new AI tools may help practitioners, educators, and researchers to be more efficient and productive. However, in their current stages of development, some results are misleading and wrong. Overall, the use of generative AI models in SPC must be properly validated and used in conjunction with other methods to ensure accurate results. © 2023 Taylor & Francis Group, LLC.
"
10.1080/10095020.2022.2054732,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128183105&origin=inward,Article,SCOPUS_ID:85128183105,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),integration of 3dgis and bim and its application in visual detection of concealed facilities,"
AbstractView references

The multi-level modeling technology of Building Information Modeling (BIM), combined with Three-dimensional Geographic Information System (3DGIS) macro-scene visualization technology and location information, can realize the transmission of decentralized information from various disciplines to multi-disciplinary collaborative information sharing services. It can be applied independently for the whole life cycle, which plays a positive role in reducing the cost and improving the efficiency of engineering planning, design, construction, operation, and maintenance. In this paper, the data integration and function integration methods of 3DGIS and BIM are designed. In order to avoid the breaking problems caused by attribute information loss and excessive simplification in the process of BIM data integration, the attribute mapping between 3DGIS and BIM based on Industry Foundation Classes (IFC) and City Geography Markup Language (CityGML) and the data simplification method considering the geometric characteristics of BIM are designed. By setting the relevant preconditions and thresholds of patch merging, on the premise of maintaining the structural characteristics of BIM data surface, reduce the amount of model data to improve the efficiency of BIM data loading, rendering and display effect in 3D geospatial scene. Through the data and function integration of 3DGIS and BIM, we can effectively manage the data of large-scale model, and calculate and obtain the geospatial location and direction of key parts of buildings through the coordinate transformation of BIM, which can effectively assist the rapid and accurate positioning of BIM in virtual 3D scene and expand the visualization ability of 3DGIS. By effectively integrating 3DGIS and BIM, this paper gives full play to the spatial management advantages of 3DGIS and the component management advantages of BIM. The rationality and operability of the method are verified by its application in the operation and maintenance management project of concealed facilities in actual buildings. © 2022 Wuhan University. Published by Informa UK Limited, trading as Taylor & Francis Group.
"
10.1109/TEM.2022.3152216,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126318026&origin=inward,Article,SCOPUS_ID:85126318026,scopus,2024-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deep learning for technical document classification,"
AbstractView references

In large technology companies, the requirements for managing and organizing technical documents created by engineers and managers have increased dramatically in recent years, which has led to a higher demand for more scalable, accurate, and automated document classification. Prior studies have only focused on processing text for classification, whereas technical documents often contain multimodal information. To leverage multimodal information for document classification to improve the model performance, this article presents a novel multimodal deep learning architecture, i.e., TechDoc, which utilizes three types of information, including natural language texts and descriptive images within documents and the associations among the documents. The architecture synthesizes the convolutional neural network, recurrent neural network, and graph neural network through an integrated training process. We applied the architecture to a large multimodal technical document database and trained the model for classifying documents based on the hierarchical International Patent Classification system. Our results show that TechDoc presents a greater classification accuracy than the unimodal methods and other state-of-the-art benchmarks. The trained model can potentially be scaled to millions of real-world multimodal technical documents, which is useful for data and knowledge management in large technology companies and organizations. © 1988-2012 IEEE.
"
10.4018/9798369304877.ch011,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183803607&origin=inward,Book Chapter,SCOPUS_ID:85183803607,scopus,2023-12-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring the challenges and applications of generative ai on engineering education in mexico,"
AbstractView references

This chapter explores the potential and challenges of implementing generative artificial intelligence (AI) in engineering education in Mexico. It highlights the relevance of developing broader competences like critical thinking, collaboration, and cognitive processing to prepare students for the Fourth Industrial Revolution. Integrating generative AI and STEM education can foster collaboration, communication, critical thinking, and creativity. Challenges include the gap between demand and supply of skilled engineers and the need to enhance education quality. The chapter proposes personalized learning experiences, enhanced student engagement, workload reduction for teachers, and the use of large language models and simulated environments as key applications of generative AI. Leveraging learning analytics and generative AI can tailor content to students' needs. Ethical considerations and human oversight are crucial for successful integration. © 2024, IGI Global. All rights reserved.
"
10.4018/9798369304877.ch012,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183794411&origin=inward,Book Chapter,SCOPUS_ID:85183794411,scopus,2023-12-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),innovative teaching methodology in higher education with generative ai- engineering education in developing countries,"
AbstractView references

The rapidly evolving landscape of higher education is causing significant changes in the educational system. The needs of the digital world cannot be satisfied by conventional teaching and learning approaches. To improve educational quality, fostering global collaboration and knowledge sharing has become imperative for preparing students for a connected world. Therefore, technological integration in the classroom has become essential. Generative artificial intelligence (AI) has emerged as a powerful tool in this endeavour. In addition to examining the possible advantages and difficulties of incorporating technology in higher education, this study also assesses the extent of technology integration in teaching, referred to as innovative teaching methodologies (ITM). This chapter explores how generative AI can be leveraged to enhance teaching methods. © 2024, IGI Global. All rights reserved.
"
10.1145/3639631.3639663,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185846546&origin=inward,Conference Paper,SCOPUS_ID:85185846546,scopus,2023-12-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a prompt engineering approach for structured data extraction from unstructured text using conversational llms,
10.1145/3632754.3633480,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185405871&origin=inward,Conference Paper,SCOPUS_ID:85185405871,scopus,2023-12-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),efficiency of large language models to scale up ground truth: overview of the irse track at forum for information retrieval 2023,"
AbstractView references

The Software Engineering Information Retrieval (IRSE) track aims to devise solutions for the automated evaluation of code comments within a machine learning framework, with labels generated by both humans and large language models. Within this track, there is a binary classification task: discerning comments as either useful or not useful. The dataset includes 9,048 pairs of code comments and surrounding code snippets drawn from open-source C-based projects on GitHub and an additional dataset generated by teams employing large language models. In total, 17 teams representing various universities and software companies have contributed 56 experiments. These experiments were assessed through quantitative metrics, primarily the F1-Score, and qualitative evaluations based on the features developed, the supervised learning models employed, and their respective hyperparameters. It is worth noting that labels generated by large language models introduce bias into the prediction model but lead to less over-fitted results. © 2023 Owner/Author.
"
10.1016/j.eswa.2023.120982,S0957417423014847,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165534584&origin=inward,Article,SCOPUS_ID:85165534584,scopus,2023-12-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),toward practical and plausible counterfactual explanation through latent adjustment in disentangled space,"
                  Extensive research into eXplainable AI (XAI) has raised interest in generating counterfactual (CF) explanations. In the past, minimizing the perturbation of input was considered a priority aspect of CF for the benefit of user practicality. However, closeness to the CF data manifold, indicating plausibility, is now emerging as another important property of CF. Thus, we propose a novel framework for generating practical and plausible CFs by minimally perturbing the semantic information of inputs in a disentangled latent space of a generative adversarial network (GAN). Considering the possibility of linear change of semantic information in a disentangled latent space, we obtain the desired CFs using proposed algorithms that adjust the input latents and reference CF latents derived using an optimization-based GAN inversion method. The results of qualitative and quantitative experiments on several datasets from different domains demonstrate the superiority and versatility of our framework. In comparative experiments, it not only achieves 1.0 Validity for test samples from all datasets but also achieves the minimum values of 0.07 Dissimilarity, 5.96 Rec. Error, 0.94 IM1, and 0.01 Infer. Time for the MNIST dataset.
               "
10.1145/3633083.3633099,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183328892&origin=inward,Conference Paper,SCOPUS_ID:85183328892,scopus,2023-12-14,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring the research gap: generative ai and learning of python programming among post-primary students,"
AbstractView references

The introduction of Leaving Certificate Computer Science (LCCS) in Ireland in 2018 signifies a notable advancement in post-primary education. Moreover, developments in generative Artificial Intelligence (GAI) in education, are gaining prominence, yet we do not understand its value or how best to implement it in post-primary educational settings. Despite a growing international body of research in this area, my scoping review highlights that many aspects of these topics have yet to be explored, particularly in the context of post-primary students in Ireland. My study will begin to bridge this gap by exploring how a purposeful sample of LCCS post-primary students in Ireland engage with GAI tools, such as ChatGPT, during their initial experiences learning Python programming. These findings, when approached through the lens of Human-Centred Artificial Intelligence (HCAI), can help enhance pedagogical strategies and lead to improved learning experiences for students. © 2023 Owner/Author.
"
10.1108/S1085-462220230000027005,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179157202&origin=inward,Book Chapter,SCOPUS_ID:85179157202,scopus,2023-12-14,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design thinking and cybernetics: the case for generative ai in ais pedagogy,"
AbstractView references

We use design thinking in the context of accounting pedagogy to exploit recent advances in cybernetics in the form of generative artificial intelligence technology. Relying on the intuition that supplementing or augmenting human argumentation (natural intelligence or NI) with parallel AI output can produce better student written assignments, we posit the “augmentation premise,” that is, ((NI + AI) > AI > NI). To test the augmentation premise, we compare student written submissions in an Accounting Information Systems (AIS) course with and without the benefit of parallel generative AI output. We then evaluate how the generative AI output enhances student-crafted revisions to their initial submissions. Using a summative quality improvement index (QII) consisting of quantitative and qualitative assessments, we present preliminary evidence supporting the augmentation premise. The augmentation premise © 2024, Emerald Publishing. All rights reserved.
"
10.1097/GOX.0000000000005471,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180066985&origin=inward,Article,SCOPUS_ID:85180066985,scopus,2023-12-13,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),can ai think like a plastic surgeon? evaluating gpt-4's clinical judgment in reconstructive procedures of the upper extremity,"
AbstractView references

This study delves into the potential application of OpenAI's Generative Pretrained Transformer 4 (GPT-4) in plastic surgery, with a particular focus on procedures involving the hand and arm. GPT-4, a cutting-edge artificial intelligence (AI) model known for its advanced chat interface, was tested on nine surgical scenarios of varying complexity. To optimize the performance of GPT-4, prompt engineering techniques were used to guide the model's responses and improve the relevance and accuracy of its output. A panel of expert plastic surgeons evaluated the responses using a Likert scale to assess the model's performance, based on five distinct criteria. Each criterion was scored on a scale of 1 to 5, with 5 representing the highest possible score. GPT-4 demonstrated a high level of performance, achieving an average score of 4.34 across all cases, consistent across different complexities. The study highlights the ability of GPT-4 to understand and respond to complicated surgical scenarios. However, the study also identifies potential areas for improvement. These include refining the prompts used to elicit responses from the model and providing targeted training with specialized, up-to-date sources. This study demonstrates a new approach to exploring large language models and highlights potential future applications of AI. These could improve patient care, refine surgical outcomes, and even change the way we approach complex clinical scenarios in plastic surgery. However, the intrinsic limitations of AI in its current state, together with the potential ethical considerations and the inherent uncertainty of unanticipated issues, serve to reiterate the indispensable role and unparalleled value of human plastic surgeons. © 2023 Lippincott Williams and Wilkins. All rights reserved.
"
10.1016/j.patter.2023.100860,S2666389923002441,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176223182&origin=inward,Article,SCOPUS_ID:85176223182,scopus,2023-12-08,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),classification of integers based on residue classes via modern deep learning algorithms,"Judging whether an integer can be divided by prime numbers such as 2 or 3 may appear trivial to human beings, but it can be less straightforward for computers. Here, we tested multiple deep learning architectures and feature engineering approaches to classifying integers based on their residues when divided by small prime numbers. We found that the ability of classification critically depends on the feature space. We also evaluated automated machine learning (AutoML) platforms from Amazon, Google, and Microsoft and found that, without appropriately engineered features, they failed on this task. Furthermore, we introduced a method that utilizes linear regression on Fourier series basis vectors and demonstrated its effectiveness. Finally, we evaluated large language models (LLMs) such as GPT-4, GPT-J, LLaMA, and Falcon, and we demonstrated their failures. In conclusion, feature engineering remains an important task to improve performance and increase interpretability of machine learning models, even in the era of AutoML and LLMs."
10.3233/FAIA230964,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181175586&origin=inward,Conference Paper,SCOPUS_ID:85181175586,scopus,2023-12-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),harnessing gpt-3.5-turbo for rhetorical role prediction in legal cases,
10.3233/FAIA230979,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181168104&origin=inward,Conference Paper,SCOPUS_ID:85181168104,scopus,2023-12-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompt engineering and provision of context in domain specific use of gpt,"
AbstractView references

Large Language Models (LLMs) can appear to generate expert advice on legal matters. However, at closer analysis, some of the advice provided has proven unsound or erroneous. We tested LLMs' performance in the procedural and technical area of insolvency law in which our team has relevant expertise. This paper demonstrates that statistically more accurate results to evaluation questions come from a design which adds a curated knowledge base to produce quality responses when querying LLMs. We evaluated our bot head-to-head on an unseen test set of twelve questions about insolvency law against the unmodified versions of gpt-3.5-turbo and gpt-4 with a mark scheme similar to those used in examinations in law schools. On the 'unseen test set', the Insolvency Bot based on gpt-3.5-turbo outper-formed gpt-3.5-turbo (p = 1.8%), and our gpt-4 based bot outperformed unmodified gpt-4 (p = 0.05%). These promising results can be expanded to cross-jurisdictional queries and be further improved by matching on-point legal information to user queries. Overall, they demonstrate the importance of incorporating trusted knowledge sources into traditional LLMs in answering domain-specific queries. © 2023 The Authors.
"
10.1145/3628797.3628947,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180548945&origin=inward,Conference Paper,SCOPUS_ID:85180548945,scopus,2023-12-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an approach to generating api test scripts using gpt,"
AbstractView references

As more software systems publish and use web services or APIs today, automated API testing is an important activity to help effectively ensure the quality of software services before they are released for their usage. Generating test scripts and data is a crucial step to perform API test automation successfully. In this paper, we propose an approach leveraging GPT, a large language model, and API's Swagger specification to automatically generate test scripts and test data for API testing. Our approach also applies GPT's self-refining with the feedback by executing tests on Katalon. We evaluate our proposed approach using a data set of seven APIs consisting of 157 endpoints and 179 operations. The result shows that while our approach generates fewer test scripts and data inputs, it can cover more successful status codes of 2xx than a state-of-the-art tool. This result suggests that leveraging the ability of GPT as a large language model to interpret API's Swagger specification has the potential for improving the efficacy of generating test scripts and data for API testing. © 2023 ACM.
"
10.1145/3587259.3627572,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180376354&origin=inward,Conference Paper,SCOPUS_ID:85180376354,scopus,2023-12-05,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),procedural text mining with large language models,"
AbstractView references

Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training data, a hurdle often encountered in deep learning-based Natural Language Processing techniques for procedure extraction. © 2023 Owner/Author.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181769338&origin=inward,Conference Paper,SCOPUS_ID:85181769338,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the design and use of conversational intelligent tutoring systems and computer simulation for the use of students of technology entrepreneurship,"
AbstractView references

Entrepreneurship is complex and dynamic. It involves continuously pursuing novel or better products and business models amidst constraints, uncertainty, and constant change among ecosystem participants (or ""agents""). Entrepreneurship education, therefore, needs to be non-linear. Yet, traditional teaching methods in entrepreneurship came from business management education practices: lectures, case studies, and group discussions-mostly ineffective because entrepreneurship is more dynamic and non-linear. Recent entrepreneurial experiential learning attempts include starting and running a business and using computer simulations to reduce time and cost. There are opportunities to introduce non-linear and more human-like approaches to the learning interface, and these are some of the aims of intelligent tutor systems (ITS). This study proposes using a conversational ITS (CITS) as the learning experience interface for a technology entrepreneurship program to teach students various concepts. Conversations, through natural language, will take advantage of recent developments in large language models (LLMs) and related conversational agents and Al assistants such as ChatGPT. At the heart of the learning tool is a suite of computer simulation environments specifically for technology entrepreneurship, with the choice of technology entrepreneurship forcing novelty and relative market uncertainty in product offerings. The design and selection of technologies will follow evaluation frameworks on the effectiveness of entrepreneurship teaching simulation environments: fidelity, verification, and validity. The expected output will be a simulation environment resulting from multiple design-build-implement iterations. The CITS and the simulation core engine shall interface with a Learning Management System (LMS). The study will also generate insights after simulation sessions with domain experts and students through educational data mining (EDM) of the resulting logs. © 2023 Asia-Pacific Society for Computers in Education.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181536521&origin=inward,Conference Paper,SCOPUS_ID:85181536521,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exgen: ready-to-use exercise generation in introductory programming courses,"
AbstractView references

In introductory programming courses, students as novice programmers would benefit from doing frequent practices set at a difficulty level and concept suitable for their skills and knowledge. However, setting many good programming exercises for individual learners is very time-consuming for instructors. In this work, we propose an automated exercise generation system, named ExGen, which leverages recent advances in pre-trained large language models (LLMs) to automatically create customized and ready-to-use programming exercises for individual students ondemand. The system integrates seamlessly with Visual Studio Code, a popular development environment for computing students and software engineers. ExGen effectively does the following: 1) maintaining a set of seed exercises in a personalized database stored locally for each student; 2) constructing appropriate prompts from the seed exercises to be sent to a cloud-based LLM deployment for generating candidate exercises; and 3) implementing a novel combination of filtering checks to automatically select only ready-to-use exercises for a student to work on. Extensive evaluation using more than 600 Python exercises demonstrates the effectiveness of ExGen in generating customized, ready-to-use programming exercises for new computing students. © 2023 Asia-Pacific Society for Computers in Education.
"
10.3390/bdcc7040182,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180700331&origin=inward,Article,SCOPUS_ID:85180700331,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),computers’ interpretations of knowledge representation using pre-conceptual schemas: an approach based on the bert and llama 2-chat models,"
AbstractView references

Pre-conceptual schemas are a straightforward way to represent knowledge using controlled language regardless of context. Despite the benefits of using pre-conceptual schemas by humans, they present challenges when interpreted by computers. We propose an approach to making computers able to interpret the basic pre-conceptual schemas made by humans. To do that, the construction of a linguistic corpus is required to work with large language models—LLM. The linguistic corpus was mainly fed using Master’s and doctoral theses from the digital repository of the University of Nariño to produce a training dataset for re-training the BERT model; in addition, we complement this by explaining the elicited sentences in triads from the pre-conceptual schemas using one of the cutting-edge large language models in natural language processing: Llama 2-Chat by Meta AI. The diverse topics covered in these theses allowed us to expand the spectrum of linguistic use in the BERT model and empower the generative capabilities using the fine-tuned Llama 2-Chat model and the proposed solution. As a result, the first version of a computational solution was built to consume the language models based on BERT and Llama 2-Chat and thus automatically interpret pre-conceptual schemas by computers via natural language processing, adding, at the same time, generative capabilities. The validation of the computational solution was performed in two phases: the first one for detecting sentences and interacting with pre-conceptual schemas with students in the Formal Languages and Automata Theory course—the seventh semester of the systems engineering undergraduate program at the University of Nariño’s Tumaco campus. The second phase was for exploring the generative capabilities based on pre-conceptual schemas; this second phase was performed with students in the Object-oriented Design course—the second semester of the systems engineering undergraduate program at the University of Nariño’s Tumaco campus. This validation yielded favorable results in implementing natural language processing using the BERT and Llama 2-Chat models. In this way, some bases were laid for future developments related to this research topic. © 2023 by the authors.
"
10.3390/fi15120375,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180546873&origin=inward,Article,SCOPUS_ID:85180546873,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a structured narrative prompt for prompting narratives from large language models: sentiment assessment of chatgpt-generated narratives and real tweets,"
AbstractView references

Large language models (LLMs) excel in providing natural language responses that sound authoritative, reflect knowledge of the context area, and can present from a range of varied perspectives. Agent-based models and simulations consist of simulated agents that interact within a simulated environment to explore societal, social, and ethical, among other, problems. Simulated agents generate large volumes of data and discerning useful and relevant content is an onerous task. LLMs can help in communicating agents’ perspectives on key life events by providing natural language narratives. However, these narratives should be factual, transparent, and reproducible. Therefore, we present a structured narrative prompt for sending queries to LLMs, we experiment with the narrative generation process using OpenAI’s ChatGPT, and we assess statistically significant differences across 11 Positive and Negative Affect Schedule (PANAS) sentiment levels between the generated narratives and real tweets using chi-squared tests and Fisher’s exact tests. The narrative prompt structure effectively yields narratives with the desired components from ChatGPT. In four out of forty-four categories, ChatGPT generated narratives which have sentiment scores that were not discernibly different, in terms of statistical significance (alpha level (Formula presented.)), from the sentiment expressed in real tweets. Three outcomes are provided: (1) a list of benefits and challenges for LLMs in narrative generation; (2) a structured prompt for requesting narratives of an LLM chatbot based on simulated agents’ information; (3) an assessment of statistical significance in the sentiment prevalence of the generated narratives compared to real tweets. This indicates significant promise in the utilization of LLMs for helping to connect a simulated agent’s experiences with real people. © 2023 by the authors.
"
10.5603/cj.97515,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178886642&origin=inward,Article,SCOPUS_ID:85178886642,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),beyond chatgpt: what does gpt-4 add to healthcare? the dawn of a new era,"
AbstractView references

Over the past few years, artificial intelligence (AI) has significantly improved healthcare. Once the stuff of science fiction, AI is now widely used, even in our daily lives — often without us thinking about it. All healthcare professionals — especially executives and medical doctors — need to understand the capabilities of advanced AI tools and other breakthrough innovations. This understanding will allow them to recognize opportunities and threats emerging technologies can bring to their organizations. We hope to contribute to a meaningful public discussion about the role of this new type of AI and how our approach to healthcare and medicine can best evolve with the rapid development of this technology. Since medicine learns by example, only a few possible uses of AI in medicine are provided, which merely outline the system’s capabilities. Among the examples, it is worth highlighting the roles of AI in medical notes, education, preventive programs, consultation, triage and intervention. It is believed by the authors that large language models such as chat generative pre-trained transformer (ChatGPT) are reaching a level of maturity that will soon impact clinical medicine as a whole and improve the delivery of individualized, compassionate, and scalable healthcare. It is unlikely that AI will replace physicians in the near future. The human aspects of care, including empathy, compassion, critical thinking, and complex decision-making, are invaluable in providing holistic patient care beyond diagnosis and treatment decisions. The GPT-4 has many limitations and cannot replace direct contact between an experienced physician and a patient for even the most seemingly simple consultations, not to mention the ethical and legal aspects of responsibility for diagnosis. © 2023 Via Medica.
"
10.24059/olj.v27i4.4055,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178875529&origin=inward,Article,SCOPUS_ID:85178875529,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deep learning models for analyzing social construction of knowledge online,"
AbstractView references

Gunawardena et al.’s (1997) Interaction Analysis Model (IAM) is one of the most frequently employed frameworks to guide the qualitative analysis of social construction of knowledge online. However, qualitative analysis is time consuming, and precludes immediate feedback to revise online courses while being delivered. To expedite analysis with a large dataset, this study explores how two neural network architectures—a feed-forward network (Doc2Vec) and a large language model transformer (BERT)—could automatically predict phases of knowledge construction using IAM. The methods interrogated the extent to which the artificial neural networks’ predictions of IAM Phases approximated a human coder’s qualitative analysis. Key results indicate an accuracy of 21.55% for Doc2Vec phases I-V, 43% for fine-tuning a pre-trained large language model (LLM), and 52.79% for prompt-engineering an LLM. Future studies for improving accuracy should consider either training the models with larger datasets or focusing on the design of prompts to improve classification accuracy. Grounded on social constructivism and IAM, this study has implications for designing and supporting online collaborative learning where the goal is social construction of knowledge. Moreover, it has teaching implications for guiding the design of AI tools that provide beneficial feedback for both students and course designers. © 2023, The Online Learning Consortium. All rights reserved.
"
10.1007/s11431-023-2496-6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177065366&origin=inward,Article,SCOPUS_ID:85177065366,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),machinery fault diagnostic method based on numerical simulation driving partial transfer learning,"
AbstractView references

Artificial intelligence (AI), which has recently gained popularity, is being extensively employed in modern fault diagnostic research to preserve the reliability and productivity of machines. The effectiveness of AI is influenced by the quality of the labeled training data. However, in engineering scenarios, available data on mechanical equipment are scarce, and collecting massive amounts of well-annotated fault data to train AI models is expensive and difficult. In response to the inadequacy of training samples, a numerical simulation-based partial transfer learning method for machinery fault diagnosis is proposed. First, a suitable simulation model of critical components in a mechanical system is developed using the finite element method (FEM), and numerical simulation is performed to acquire FEM simulation samples containing different fault types. Second, several synthetic simulation samples are generated to form complete source domain training samples using a generative adversarial network. Subsequently, the partial transfer learning network is trained to extract shared fault characteristics between the simulation and measured samples in the case of class imbalance. Finally, the resulting model is used to diagnose unknown samples from real-world mechanical systems in operation. The proposed method is tested on actual fault samples of bearings and gears obtained from a public dataset and experimental test rig available in our laboratory, achieving average classification accuracy of 99.54% and 99.64%, respectively. Comparison investigations reveal that the proposed method has superior classification and generalization ability when detecting faults in real mechanical systems. © 2023, Science China Press.
"
10.1038/s41598-023-46523-z,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176430682&origin=inward,Article,SCOPUS_ID:85176430682,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),repurposing existing skeletal spatial structure (sks) system designs using the field information modeling (fim) framework for generative decision-support in future construction projects,"
AbstractView references

Skeletal spatial structure (SkS) systems are modular systems which have shown promise to support mass customization, and sustainability in construction. SkS have been used extensively in the reconstruction efforts since World War II, particularly to build geometrically flexible and free-form structures. By employing advanced digital engineering and construction practices, the existing SkS designs may be repurposed to generate new optimal designs that satisfy current construction demands of contemporary societies. To this end, this study investigated the application of point cloud processing using the Field Information Modeling (FIM) framework for the digital documentation and generative redesign of existing SkS systems. Three new algorithms were proposed to (i) expand FIM to include generative decision-support; (ii) generate as-built building information modeling (BIM) for SkS; and (iii) modularize SkS designs with repeating patterns for optimal production and supply chain management. These algorithms incorporated a host of new AI-inspired methods, including support vector machine (SVM) for decision support; Bayesian optimization for neighborhood definition; Bayesian Gaussian mixture clustering for modularization; and Monte Carlo stochastic multi-criteria decision making (MCDM) for selection of the top Pareto front solutions obtained by the non-dominant sorting Genetic Algorithm (NSGA II). The algorithms were tested and validated on four real-world point cloud datasets to solve two generative modeling problems, namely, engineering design optimization and facility location optimization. It was observed that the proposed Bayesian neighborhood definition outperformed particle swarm and uniform sampling by 34% and 27%, respectively. The proposed SVM-based linear feature detection outperformed k-means and spectral clustering by 56% and 9%, respectively. Finally, the NSGA II algorithm combined with the stochastic MCDM produced diverse “top four” solutions based on project-specific criteria. The results indicate promise for future utilization of the framework to produce training datasets for generative adversarial networks that generate new designs based only on stakeholder requirements. © 2023, The Author(s).
"
10.1016/j.plas.2023.100101,S2666721523000224,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176233959&origin=inward,Article,SCOPUS_ID:85176233959,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),who is better in project planning?generative artificial intelligence or project managers?,"This paper presents a comparative study of generative artificial intelligence (AI), specifically the GPT-4 model, and a human project manager in the context of a project plan development. The study's objective was to analyze the content and structure of a project plan prepared by this disruptive new technology and its human counterpart, focusing on the digital technology sector. Through a primarily qualitative methodology, the study scrutinizes critical aspects of each part of the project plan, including scope preparation, schedule development, cost estimation, resources evaluation, quality planning, stakeholder mapping, communication planning, and risk analysis. The results indicate unique strengths and weaknesses for both AI-generated and human-generated project plans, revealing them as complementary in the project planning process. It also emphasizes the continued importance of human expertise in refining AI outputs and harnessing the full potential of AI through the process known as prompt engineering. In conclusion, this study illustrates the potential synergy between human experience and AI in project planning, suggesting the careful integration of human and AI capabilities is key to developing robust and trustworthy project plans."
10.1016/j.jbi.2023.104533,S153204642300254X,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175801114&origin=inward,Article,SCOPUS_ID:85175801114,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),leveraging gpt-4 for food effect summarization to enhance product-specific guidance development via iterative prompting,"
                  Food effect summarization from New Drug Application (NDA) is an essential component of product-specific guidance (PSG) development and assessment, which provides the basis of recommendations for fasting and fed bioequivalence studies to guide the pharmaceutical industry for developing generic drug products. However, manual summarization of food effect from extensive drug application review documents is time-consuming. Therefore, there is a need to develop automated methods to generate food effect summary. Recent advances in natural language processing (NLP), particularly large language models (LLMs) such as ChatGPT and GPT-4, have demonstrated great potential in improving the effectiveness of automated text summarization, but its ability with regard to the accuracy in summarizing food effect for PSG assessment remains unclear. In this study, we introduce a simple yet effective approach,iterative prompting, which allows one to interact with ChatGPT or GPT-4 more effectively and efficiently through multi-turn interaction. Specifically, we propose a three-turn iterative prompting approach to food effect summarization in which the keyword-focused and length-controlled prompts are respectively provided in consecutive turns to refine the quality of the generated summary. We conduct a series of extensive evaluations, ranging from automated metrics to FDA professionals and even evaluation by GPT-4, on 100 NDA review documents selected over the past five years. We observe that the summary quality is progressively improved throughout the iterative prompting process. Moreover, we find that GPT-4 performs better than ChatGPT, as evaluated by FDA professionals (43% vs. 12%) and GPT-4 (64% vs. 35%). Importantly, all the FDA professionals unanimously rated that 85% of the summaries generated by GPT-4 are factually consistent with the golden reference summary, a finding further supported by GPT-4 rating of 72% consistency. Taken together, these results strongly suggest a great potential for GPT-4 to draft food effect summaries that could be reviewed by FDA professionals, thereby improving the efficiency of the PSG assessment cycle and promoting generic drug product development.
               "
10.1115/1.4062597,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173951276&origin=inward,Article,SCOPUS_ID:85173951276,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),zero-trust for the system design lifecycle,"
AbstractView references

In an age of worsening global threat landscape and accelerating uncertainty, the design and manufacture of systems must increase resilience and robustness across both the system itself and the entire systems design process. We generally trust our colleagues after initial clearance/background checks; and systems to function as intended and within operating parameters after safety engineering review, verification, validation, and/ or system qualification testing. This approach has led to increased insider threat impacts; thus, we suggest moving to the ""trust, but verify""approach embodied by the Zero-Trust paradigm. Zero-Trust is increasingly adopted for network security but has not seen wide adoption in systems design and operation. Achieving the goal of Zero-Trust throughout the systems lifecycle will help to ensure that no single bad actor-whether human or machine learning/artificial intelligence (ML/AI)-can induce failure anywhere in a system's lifecycle. Additionally, while ML/AI and their associated risks are already entrenched within the operations phase of many systems' lifecycles, ML/AI is gaining traction during the design phase. For example, generative design algorithms are increasingly popular, but there is less understanding of potential risks. Adopting the Zero-Trust philosophy helps ensure robust and resilient design, manufacture, operations, maintenance, upgrade, and disposal of systems. We outline the rewards and challenges of implementing Zero-Trust and propose the framework for Zero-Trust for the system design lifecycle. This article highlights several areas of ongoing research with focus on high priority areas where the community should focus efforts. © 2023 by ASME.
"
10.1016/j.compedu.2023.104898,S0360131523001756,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171619433&origin=inward,Article,SCOPUS_ID:85171619433,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),beyond chatgpt: a conceptual framework and systematic review of speech-recognition chatbots for language learning,"
                  The diversification of chatbot technology, such as the emergence of large language models and their incorporation into various technologies, necessitates a conceptual framework for a comprehensive understanding of different chatbot types and their possibilities for educational use. However, despite the fact that chatbots with different characteristics can provide learners with different interaction experiences, previous research has drawn on a loose conceptualization of chatbots, ignoring the common or unique design features of different chatbots and the educational affordances that are provided accordingly. In response to this concern, this review aims to further our understanding of different types of speech-recognition chatbots for language learning and the affordances provided by the chatbots. Based on an analysis of 37 empirical studies on uses of chatbots ranging from those with predefined dialogue systems to those utilizing artificial intelligence technology, this review proposes a conceptual framework that comprises three key components of a chatbot system: goal-orientation, embodiment, and multimodality. Using this framework as an analytical tool, eight chatbot types are identified and defined. Additionally, a total of 12 affordances are derived from the presence and absence of each component of the framework. Analysis of the studies through the framework also offers specific insights into how future chatbot research and development should be pursued in terms of goal-orientation, embodiment, and multimodality. Finally, we discuss the potential of the framework as a relevant model for understanding chatbots in adjacent disciplines and types other than speech-recognition chatbots, including ChatGPT and other large language models.
               "
10.1016/j.yjoc.2023.100065,S2713374523000249,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169775407&origin=inward,Article,SCOPUS_ID:85169775407,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the originality of machines: ai takes the torrance test,"This exploratory research investigated the creative abilities of OpenAI's large language model, ChatGPT, based on the GPT-4 architecture, as assessed by the Torrance Tests of Creative Thinking. In comparison to human samples and a national percentile from Scholastic Testing Services, ChatGPT's performance was analyzed for fluency, flexibility, and originality. Results indicated that ChatGPT scored within the top 1% for originality and fluency, and showed high scores for flexibility, thus highlighting the current creative abilities of AI and the potential of AI systems to support and augment human creativity in new and meaningful ways. The study encourages additional research to further define, measure, and develop creativity in the era of advanced AI."
10.1016/j.infsof.2023.107302,S0950584923001568,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167993953&origin=inward,Article,SCOPUS_ID:85167993953,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),clebpi: contrastive learning for bug priority inference,"
                  Context:
                  Automated bug priority inference (BPI) can reduce the time overhead of bug triagers for priority assignments, improving the efficiency of software maintenance.
               
                  Objective:
                  There are two orthogonal lines for this task, i.e., traditional machine learning based (TML-based) and neural network based (NN-based) approaches. Although these approaches achieve competitive performance, our observation finds that existing approaches face the following two issues: 1) TML-based approaches require much manual feature engineering and cannot learn the semantic information of bug reports; 2) Both TML-based and NN-based approaches cannot effectively address the label imbalance problem because they are difficult to distinguish the semantic difference between bug reports with different priorities.
               
                  Method:
                  We propose CLeBPI (Contrastive Learning for Bug Priority Inference), which leverages pre-trained language model and contrastive learning to tackle the above-mentioned two issues. Specifically, CLeBPI is first pre-trained on a large-scale bug report corpus in a self-supervised way, thus it can automatically learn contextual representations of bug reports without manual feature engineering. Afterward, it is further pre-trained by a contrastive learning objective, which enables it to distinguish semantic differences between bug reports, learning more precise contextual representations for each bug report. When finishing pre-training, we can connect a classification layer to CLeBPI and fine-tune it for BPI in a supervised way.
               
                  Results:
                  We choose four baseline approaches and conduct comparison experiments on a public dataset. The experimental results show that CLeBPI outperforms all baseline approaches by 23.86%–77.80% in terms of weighted average F1-score, showing its effectiveness.
               
                  Conclusion:
                  This paper propose CLeBPI, a pre-trained model combining contrastive learning that can automatically predict bug priority. Experimental results show that It achieves new result in BPI and can effectively alleviate label imbalance problem.
               "
10.1038/s41746-023-00873-0,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164295991&origin=inward,Article,SCOPUS_ID:85164295991,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the imperative for regulatory oversight of large language models (or generative ai) in healthcare,"
AbstractView references

The rapid advancements in artificial intelligence (AI) have led to the development of sophisticated large language models (LLMs) such as GPT-4 and Bard. The potential implementation of LLMs in healthcare settings has already garnered considerable attention because of their diverse applications that include facilitating clinical documentation, obtaining insurance pre-authorization, summarizing research papers, or working as a chatbot to answer questions for patients about their specific data and concerns. While offering transformative potential, LLMs warrant a very cautious approach since these models are trained differently from AI-based medical technologies that are regulated already, especially within the critical context of caring for patients. The newest version, GPT-4, that was released in March, 2023, brings the potentials of this technology to support multiple medical tasks; and risks from mishandling results it provides to varying reliability to a new level. Besides being an advanced LLM, it will be able to read texts on images and analyze the context of those images. The regulation of GPT-4 and generative AI in medicine and healthcare without damaging their exciting and transformative potential is a timely and critical challenge to ensure safety, maintain ethical standards, and protect patient privacy. We argue that regulatory oversight should assure medical professionals and patients can use LLMs without causing harm or compromising their data or privacy. This paper summarizes our practical recommendations for what we can expect from regulators to bring this vision to reality. © 2023, The Author(s).
"
10.1007/s11704-022-8262-9,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146664699&origin=inward,Article,SCOPUS_ID:85146664699,scopus,2023-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),user story clustering in agile development: a framework and an empirical study,"
AbstractView references

Agile development aims at rapidly developing software while embracing the continuous evolution of user requirements along the whole development process. User stories are the primary means of requirements collection and elicitation in the agile development. A project can involve a large amount of user stories, which should be clustered into different groups based on their functionality’s similarity for systematic requirements analysis, effective mapping to developed features, and efficient maintenance. Nevertheless, the current user story clustering is mainly conducted in a manual manner, which is time-consuming and subjective to human bias. In this paper, we propose a novel approach for clustering the user stories automatically on the basis of natural language processing. Specifically, the sentence patterns of each component in a user story are first analysed and determined such that the critical structure in the representative tasks can be automatically extracted based on the user story meta-model. The similarity of user stories is calculated, which can be used to generate the connected graph as the basis of automatic user story clustering. We evaluate the approach based on thirteen datasets, compared against ten baseline techniques. Experimental results show that our clustering approach has higher accuracy, recall rate and F1-score than these baselines. It is demonstrated that the proposed approach can significantly improve the efficacy of user story clustering and thus enhance the overall performance of agile development. The study also highlights promising research directions for more accurate requirements elicitation. © 2023, Higher Education Press.
"
10.1145/3611643.3613093,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180556133&origin=inward,Conference Paper,SCOPUS_ID:85180556133,scopus,2023-11-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on using information retrieval to recommend machine learning good practices for software engineers,"
AbstractView references

Machine learning (ML) is nowadays widely used for different purposes and with several disciplines. From self-driving cars to automated medical diagnosis, machine learning models extensively support users' daily activities, and software engineering tasks are no exception. Not embracing good ML practices may lead to pitfalls that hinder the performance of an ML system and potentially lead to unexpected results. Despite the existence of documentation and literature about ML best practices, many non-ML experts turn towards gray literature like blogs and Q&A systems when looking for help and guidance when implementing ML systems. To better aid users in distilling relevant knowledge from such sources, we propose a recommender system that recommends ML practices based on the user's context. As a first step in creating a recommender system for machine learning practices, we implemented Idaka. A tool that provides two different approaches for retrieving/generating ML best practices: i) an information retrieval (IR) engine and ii) a large language model. The IR-engine uses BM25 as the algorithm for retrieving the practices, and a large language model, in our case Alpaca. The platform has been designed to allow comparative studies of best practices retrieval tools. Idaka is publicly available at GitHub: https://bit.ly/idaka. Video: https://youtu.be/cEb-AhIPxnM © 2023 ACM.
"
10.1145/3611643.3613892,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180554634&origin=inward,Conference Paper,SCOPUS_ID:85180554634,scopus,2023-11-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),inferfix: end-to-end program repair with llms,"
AbstractView references

Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose : a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs. combines a Retriever - transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator - an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow. © 2023 ACM.
"
10.1145/3605770.3625214,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180011088&origin=inward,Conference Paper,SCOPUS_ID:85180011088,scopus,2023-11-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an empirical study on using large language models to analyze software supply chain security failures,"
AbstractView references

As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing past failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact. GPT 3.5's categorizations had an average accuracy of 68% and Bard's had an accuracy of 58% over these dimensions. We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. Future work can improve LLM performance in this context, and study a broader range of articles and failures. © 2023 Owner/Author.
"
10.1145/3611643.3616323,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174900365&origin=inward,Conference Paper,SCOPUS_ID:85174900365,scopus,2023-11-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),natural language to code: how far are we?,"
AbstractView references

A longstanding dream in software engineering research is to devise effective approaches for automating development tasks based on developers' informally-specified intentions. Such intentions are generally in the form of natural language descriptions. In recent literature, a number of approaches have been proposed to automate tasks such as code search and even code generation based on natural language inputs. While these approaches vary in terms of technical designs, their objective is the same: transforming a developer's intention into source code. The literature, however, lacks a comprehensive understanding towards the effectiveness of existing techniques as well as their complementarity to each other. We propose to fill this gap through a large-scale empirical study where we systematically evaluate natural language to code techniques. Specifically, we consider six state-of-the-art techniques targeting code search, and four targeting code generation. Through extensive evaluations on a dataset of 22K+ natural language queries, our study reveals the following major findings: (1) code search techniques based on model pre-training are so far the most effective while code generation techniques can also provide promising results; (2) complementarity widely exists among the existing techniques; and (3) combining the ten techniques together can enhance the performance for 35% compared with the most effective standalone technique. Finally, we propose a post-processing strategy to automatically integrate different techniques based on their generated code. Experimental results show that our devised strategy is both effective and extensible. © 2023 ACM.
"
10.1145/3626111.3628189,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179852970&origin=inward,Conference Paper,SCOPUS_ID:85179852970,scopus,2023-11-28,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),toward reproducing network research results using large language models,"
AbstractView references

Reproducing research results is important for the networking community. The current best practice typically resorts to: (1) looking for publicly available prototypes; (2) contacting the authors to get a private prototype; or (3) manually implementing a prototype following the description of the publication. However, most published network research does not have public prototypes and private ones are hard to get. As such, most reproducing efforts are spent on manual implementation based on the publications, which is both time and labor consuming and error-prone. In this paper, we boldly propose reproducing network research results using the emerging large language models (LLMs). We first prove its feasibility with a small-scale experiment, in which four students with essential networking knowledge each reproduces a different networking system published in prominent conferences and journals by prompt engineering ChatGPT. We report our observations and lessons and discuss future open research questions of this proposal. © 2023 ACM.
"
10.1145/3626111.3628205,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179849073&origin=inward,Conference Paper,SCOPUS_ID:85179849073,scopus,2023-11-28,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prosper: extracting protocol specifications using large language models,"
AbstractView references

We explore the application of Large Language Models (LLMs) (specifically GPT-3.5-turbo) to extract specifications and automating understanding of networking protocols from Internet Request for Comments (RFC) documents. LLMs have proven successful in specialized domains like medical and legal text understanding, and this work investigates their potential in automatically comprehending RFCs. We develop Artifact Miner, a tool to extract diagram artifacts from RFCs. We then couple extracted artifacts with natural language text to extract protocol automata using GPT-turbo 3.5 (chatGPT) and present our zero-shot and few-shot extraction results. We call this framework for FSM extraction 'PROSPER: Protocol Specification Miner'. We compare PROSPER with existing state-of-the-art techniques for protocol FSM state and transition extraction. Our experiments indicate that employing artifacts along with text for extraction can lead to lower false positives and better accuracy for both extracted states and transitions. Finally, we discuss efficient prompt engineering techniques, the errors we encountered, and pitfalls of using LLMs for knowledge extraction from specialized domains such as RFC documents. © 2023 ACM.
"
10.1145/3635059.3635104,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187553323&origin=inward,Conference Paper,SCOPUS_ID:85187553323,scopus,2023-11-24,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),large language models versus natural language understanding and generation,"
AbstractView references

In recent years, the process humans adopt to learn a foreign language has moved from the strict ""Grammar -Translation""method, which is based mainly on grammar and syntax rules, to more innovative processes, resulting to the more modern ""Communicative approach"". As its name states, this approach focuses on the coherent communication with native speakers and the cultivation of oral skills, without taking into consideration, at least at the first stages, the rules that govern the language. The same trend seems to have been applied to the way machinery can be ""educated""to comprehend and reproduce the unfamiliar, human language. The ""rule based""Natural Language Generation (NLG) and Natural Language Understanding (NLU) algorithms, on one hand, and the ""text based""Large Language Models (LLMs), on the other, are two, analogous to the two human foreign language learning processes, subareas of Natural Language Processing (NLP). This paper presents these two alternative approaches, LLMs (a technology having surfaced as an influential catalyst of NLP, during last years) on the one hand and NLG/NLU on the other, highlighting their applications, their technologies, their capabilities, their differences, their strengths and weaknesses and the challenges they present, contributing to a deeper comprehension of the evolving landscape of Artificial Intelligence and human-computer communication. © 2023 Owner/Author.
"
10.7166/34-3-2944,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176944975&origin=inward,Article,SCOPUS_ID:85176944975,scopus,2023-11-17,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai usefulness in systems modelling and simulation: gpt-4 application,"
AbstractView references

In this study, we investigate the potential of artificial intelligence (AI), specifically Generative Pre-trained Transformer 4 (GPT-4), to accelerate the development of system dynamics simulations within the broader context of systems engineering. The research aims to uncover the opportunities and limitations of leveraging AI to assist humans in constructing and refining system dynamics models. Through a systematic iterative process, GPT-4 was engaged in tasks such as creating, expanding, and stabilising simulations, identifying errors, generating expansion ideas, and converting models to Python code. Our findings reveal that GPT-4, while not flawless, can significantly enhance the modelling process, reduce human error, and expedite learning. This paper critically examines the role of AI in model development, emphasising the continued importance of human expertise in the evaluation and testing of simulations. Ultimately, we argue for a symbiotic relationship between AI and human modellers, harnessing the power of GPT-4 to augment human capabilities and advance the fields of system dynamics and systems engineering. © 2023, Southern African Institute for Industrial Engineering. All rights reserved.
"
10.1016/j.foodchem.2023.136580,S0308814623011986,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162171209&origin=inward,Article,SCOPUS_ID:85162171209,scopus,2023-11-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a sweeter future: using protein language models for exploring sweeter brazzein homologs,"
                  With growing concerns over the health impact of sugar, brazzein offers a viable alternative due to its sweetness, thermostability, and low risk profile. Here, we demonstrated the ability of protein language models to design new brazzein homologs with improved thermostability and potentially higher sweetness, resulting in new diverse optimized amino acid sequences that improve structural and functional features beyond what conventional methods could achieve. This innovative approach resulted in the identification of unexpected mutations, thereby generating new possibilities for protein engineering. To facilitate the characterization of the brazzein mutants, a simplified procedure was developed for expressing and analyzing related proteins. This process involved an efficient purification method using Lactococcus lactis (L. lactis), a generally recognized as safe (GRAS) bacterium, as well as taste receptor assays to evaluate sweetness. The study successfully demonstrated the potential of computational design in producing a more heat-resistant and potentially more palatable brazzein variant, V23.
               "
10.33423/jhetp.v23i17.6543,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176939234&origin=inward,Article,SCOPUS_ID:85176939234,scopus,2023-11-14,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"the human teacher, the ai teacher and the aied-teacher relationship","
AbstractView references

ChatGPT, an Artificial Intelligence (AI) powered chatbot, has caused a stir in the Higher Education landscape, with fears of plagiarism and a disruption of the student-teacher relationship that has formed the bedrock of teaching. ChatGPT-3 and now four have been reported to pass many exams, including medical, law, and engineering. Overwhelming concerns from academics about students using these generative AI tools to work on their assessments is alarming. These AI tools are here to stay. Teachers should not treat AI as ‘the enemy’, and instead find ways to work with it for the betterment of learning outcomes for students. Working with AI can mean transforming teaching and the AIed-teacher relationship, resulting in positive outcomes and learning experiences for teachers and students. © 2023, North American Business Press. All rights reserved.
"
10.1145/3628356.3630117,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180128385&origin=inward,Conference Paper,SCOPUS_ID:85180128385,scopus,2023-11-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sensorloader: bridging the gap in cyber-physical reverse engineering across embedded peripheral devices,"
AbstractView references

Safety-critical cyber-physical systems, such as autonomous vehicles and medical devices, are often driven by notions of state provided by sensor information translated through embedded firmware. This sensor pipeline is often a fragmented supply chain across vendors, and analyzing the associated security properties entails semantic reverse engineering of third-party software, i.e., mapping low-level software representations to cyber-physical models without access to source code. This mapping is a manual, time-consuming, and error-prone process. This paper introduces SensorLoader, a tool designed to automate mapping sensor semantics across all layers of closed-source software representations. SensorLoader exploits open-source knowledge, potentially derived from structured vendor description files or unstructured vendor datasheets, to extract and infer sensor semantics. We leverage large language models to extract sensor semantics from unstructured sources and map the semantics to memory maps and structures used by the Ghidra reverse engineering framework. We formalize the limitations of this automatic extraction and demonstrate how our approach can streamline the reverse engineering process for embedded systems. Preliminary evaluations suggest that SensorLoader can effectively and scalably aid in identifying vulnerabilities and deviations from expected behaviors, offering a more efficient pathway to secure cyber-physical systems. © 2023 Owner/Author(s).
"
10.1145/3624062.3624147,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178143571&origin=inward,Conference Paper,SCOPUS_ID:85178143571,scopus,2023-11-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),centralized provisioning of large language models for a research community,"
AbstractView references

Local support for large language models (LLMs) in a research community can address unique technological and procedural challenges that arise in an academic setting. Platforms providing multi-GPU nodes, typically found in a centralized computational resource, such as a university datacenter, can manage the large memory footprint of the open-source LLMs. Customizations employing peripheral frameworks help extend the capabilities of these models. Further, the local implementation addresses the protection of researcher IP and control of restricted data sources. This report describes recent efforts toward provisioning this popular new tool and provides guidance for recreating our approach at Arizona State University. © 2023 ACM.
"
10.3233/ATDE230608,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184295754&origin=inward,Conference Paper,SCOPUS_ID:85184295754,scopus,2023-11-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"exploring intelligent user interfaces from design students' perspectives on smart home products through peer assessment, focus group and chatgpt","
AbstractView references

In a changing and connected world, people are surrounded by an increasing number of smart devices in a complex system. Intelligent technology has revolutionised the way we interact with these devices, and has resulted in improved user experiences through the integration of physical status and digital applications. However, this transition has also presented new challenges and demands for transdisciplinary adaptation in traditional approaches to design education. Many existing design methods and frameworks have not kept pace with the level of automation now seen in intelligent interactive products, nor have they addressed human-machine interdependence in a system-thinking context. The aim of this study is to gain insights from the younger generation of design students to inform the development of a more suitable design course. Using smart home products as the case scenario, 39 industrial design students evaluated the user experience with the products through hands-on interaction. The individual product reviews of the robot cleaner, smart speaker and smart lightbulb were then analysed and consolidated. Thus, this study contributes to the elucidation of design students' perspectives on intelligent user interfaces. Furthermore, a comparative analysis of user insights was conducted through peer assessments, focus groups and large language models to explore their potential and difference in terms of the design process. Overall, the goal of this analysis is to advance the field of design practice and education. © 2023 The Authors.
"
10.1145/3637907.3637947,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187555234&origin=inward,Conference Paper,SCOPUS_ID:85187555234,scopus,2023-11-03,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research on the educational application of generative artificial intelligence images in the design of semiotics learning models,"
AbstractView references

Generative Artificial Intelligence (AI) has emerged as a novel technology with profound implications for education and deep learning, particularly due to its advancements in image generation. This progress has had a disruptive impact on the design industry, where designers are increasingly embracing generative AI images as innovative tools and techniques to enhance design ideation and creative expression. Integrating generative AI images into design education has become an inevitable trend, as it guides students in developing a deeper understanding of design aesthetics. However, the current application of generative AI images in design education lacks innovative use grounded in design theory. Therefore, the incorporation of generative AI images within the framework of design theory becomes crucial to interpret and refine design processes and design thinking. This research aims to explore the application of generative AI images in design semiotics, utilizing design theory as its foundation. By integrating design processes and incorporating design case studies, the study seeks to analyze how generative AI images can be effectively applied in design semiotics. Ultimately, the research strives to establish a pedagogical approach for the application of generative AI image-based design semiotics. © 2023 ACM.
"
10.1109/MITP.2023.3340529,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184025096&origin=inward,Article,SCOPUS_ID:85184025096,scopus,2023-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),nothing is harder to resist than the temptation of ai,"
AbstractView references

The use of generative AI has become increasingly prevalent in the business world. With the ability to create original content and automate certain tasks, businesses have been quick to adopt this technology. However, as with any emerging technology, there are potential pitfalls to be aware of. This article aims to review the current state of generative AI in business and highlight some of the potential risks associated with its use. Specifically, we examine issues such as pausing giant AI experiments, misinformation, data accuracy, process automation, shift of power, control of civilization, organizational security, and the potential for AI-generated content to deceive individuals. By bringing these concerns to the forefront, we hope to encourage a more thoughtful and cautious approach to the use of generative AI in business. © 1999-2012 IEEE.
"
10.1109/MITP.2023.3333073,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183994097&origin=inward,Article,SCOPUS_ID:85183994097,scopus,2023-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"an executive guide to ai, machine learning, and generative ai - with some help from chatgpt and bard","
AbstractView references

Executives can turn ChatGPT and Bard to help them understand the differences among artificial intelligence (AI), machine learning (ML), and generative AI; how AI/ML/GAI sees ""business""; how companies can hit the ground running with AI; how companies should organize and fund AI initiatives; and what AI leadership looks like. While ChatGPT and Bard are really smart, they lack some knowledge of what it means to live in the trenches. Many of their answers are ""comfortable""lectures but not actionable playbooks for now. © 1999-2012 IEEE.
"
10.1631/FITEE.2300537,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178939500&origin=inward,Article,SCOPUS_ID:85178939500,scopus,2023-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),software development in the age of intelligence: embracing large language models with the right approach,"
AbstractView references

Embracing LLMs is definitely a correct and even necessary direction for software enterprises to improve quality and efficiency. However, achieving systematic and comprehensive intelligent software development still requires careful consideration and there is much fundamental work to do. For enterprises, solidifying the digitization and knowledge accumulation of software development, as well as the fundamental capabilities of software engineering such as requirement analysis, design, and validation, remains crucial and is also a basic condition for achieving higher levels of intelligent development. For academic research, there is still much work to do in the direction of systematic and comprehensive intelligent software development. This also requires us have a deeper understanding of the complexity of software systems and software requirements and design, based on understanding the capabilities of LLMs. © 2023, Zhejiang University Press.
"
10.1515/lex-2023-0013,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178120000&origin=inward,Article,SCOPUS_ID:85178120000,scopus,2023-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),theoretical considerations on ai-based business models for lexicography,"
AbstractView references

AI-generated text production is on the rise Zandan (2020), and AI writers seem to be playing an increasingly important role in marketing, L2 text production, lexicography and language teaching, cf. Simonsen (2020b; 2021a; 2022a; 2022b; Sharples/Pérez Y Pérez 2022; ChatGPT 2023; RYTR 2023; Writewithlaika 2023). In addition to that large national language datasets are being developed in many countries, cf. for example Kirchmeier et al. (2020), and these national word registers are expected to become an important backbone in AI-based lexicographic services. On this background, there seems to be a need for AI-based business models for lexicography. This article draws on a literature review focussing on business models and business-related considerations of relevance for lexicography. The insights from the literature review led to the development of a number of theoretical considerations on AI-based business models for lexicography. The article suggests three AI-based business models for different types of lexicography and demonstrates how these business models could be implemented in three concrete projects. © 2023 Walter de Gruyter GmbH, Berlin/Boston.
"
10.1016/j.compositesb.2023.110993,S1359836823004961,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173803105&origin=inward,Article,SCOPUS_ID:85173803105,scopus,2023-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative ai for performance-based design of engineered cementitious composite,"
                  Engineered cementitious composite (ECC) has been intensively studied due to its excellent tensile performance. However, classical micro-mechanical design theory of ECC is qualitative and fails to give detailed ECC mixtures at specific tensile parameters. This study aims to develop a performance-based mixture design model to generate ECC mixtures using generative AI method. An experimental database consisting of 129 polyethylene fiber reinforced ECC (PE-ECC) records has been built. The database was used to train one invertible neural network model and two artificial neural network models. A series of PE-ECC mixtures were generated by the proposed model based on desired mechanical performance and sustainable requirements. Based on the experimental results, the developed model was proven to compose PE-ECC mixtures that satisfy the target requirements with a maximum deviation of less than 16%. The neural network-based model can be used in various application scenarios (e.g., low-cost ECC and low-carbon ECC), thus promoting the development of ECC materials in the area of research and engineering application.
               "
10.1007/s42979-023-02212-2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173739137&origin=inward,Article,SCOPUS_ID:85173739137,scopus,2023-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),machine learning based approach for user story clustering in agile engineering,"
AbstractView references

In the traditional software development strategy for large systems, it is a very typical and time-consuming process to express the requirements in natural language, where the customer and developer relationship is a key feature to develop quick, better, and high-quality projects. In Agile Software Development (ASD), which is an iterative and incremental model, developers need to understand requirements in the form of user stories before moving to the implementation phase. To understand and implement the user stories at early stage, the cohesiveness between user stories is required to be increased. In this context, the clustering approach of machine learning to organize the clusters of users stories will play a key role. In this study, we implemented K-means and K-medoids clustering algorithms using cosine distance values to create clusters (size 5, 10 and 15) of user stories. The silhouette coefficient value is then used to compare the cohesiveness between user stories. During the analysis of the outcome of the study, it is observed that silhouette coefficient value of K-means algorithm is greater than the K-medoids algorithm for the cluster size 5, 10 and 15, respectively. It indicates that the performance of K-means algorithm is better than K-medoids algorithm for user story clustering in agile engineering. © 2023, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.
"
10.1109/TPWRS.2023.3315543,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171572895&origin=inward,Article,SCOPUS_ID:85171572895,scopus,2023-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on the potential of chatgpt to generate distribution systems for load flow studies using opendss,
10.1016/j.cma.2023.116377,S0045782523005017,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170252670&origin=inward,Article,SCOPUS_ID:85170252670,scopus,2023-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a meshfree large-deformation analysis method for geotechnical engineering based on the rbf field variable mapping technology,"
                  In this paper, a practical meshfree large deformation method (MFLDM) is proposed for numerical analysis in geotechnical engineerings, including: soil foundation, slop, dam, etc. The MFLDM leverages both the flexible nodal distribution in the meshfree method and the high stability in the arbitrary Lagrangian–Eulerian (ALE) framework. In each calculation step, two sets of Gauss points, fixed and moving Gauss points, are generated in the background mesh. In addition, the radial basis function (RBF) is used to map field variables, including stress, stain, and constitutive variables, between the fixed and moving Gauss points to achieve the field variable redistribution during the large-deformation analysis. The proposed MFLDM, which is written in C++ using the object-oriented programming approach, can be completely integrated into the self-development calculating system named GEODYNA and coupled with the finite element method (FEM) at the matrix level, which significantly broadens its practical application. The proposed model is verified by several numerical examples and compared with different constitutive models, including the linear elasticity model, ideal elastic–plastic model, and generalized elastic–plastic model. The comparison results verify the high accuracy, fast convergence, and good robustness of the proposed MFLDM. Finally, the proposed MFLDM is applied to a local large deformation analysis between the cut-off wall and the core wall on a deep overburden.
               "
10.1016/j.autcon.2023.105067,S0926580523003278,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169001595&origin=inward,Article,SCOPUS_ID:85169001595,scopus,2023-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),dynamic prompt-based virtual assistant framework for bim information search,"
                  Efficient information search from building information models (BIMs) requires deep BIM knowledge or extensive engineering efforts for building natural language (NL)-based interfaces. To address this challenge, this paper introduces a dynamic prompt-based virtual assistant framework dubbed “BIMS-GPT” that integrates generative pre-trained transformer (GPT) technologies, supporting NL-based BIM search. To understand users' NL queries, extract relevant information from BIM databases, and deliver NL responses along with 3D visualizations, a dynamic prompt-based process was developed. In a case study, BIMS-GPT's functionality is demonstrated through a virtual assistant prototype for a hospital building. When evaluated with a BIM query dataset, the approach achieves accuracy rates of 99.5% for classifying NL queries with incorporating 2% of the data in prompts. This paper contributes to the advancement of effective and versatile virtual assistants for BIMs in the construction industry as it significantly enhances BIM accessibility while reducing the engineering and training data prerequisites for processing NL queries.
               "
10.1016/j.ijpe.2023.109015,S0925527323002475,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168809450&origin=inward,Article,SCOPUS_ID:85168809450,scopus,2023-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),are both generative ai and chatgpt game changers for 21st-century operations and supply chain excellence?,"
                  The remarkable growth of ChatGPT, a Generative Artificial Intelligence (Gen-AI), has triggered a significant debate in society. It has the potential to radically transform the business landscape, with consequences for operations and supply chain management (O&SCM). However, empirical evidence on Gen-AI's effects in O&SCM remains limited. This study investigates the benefits, challenges, and trends associated with Gen-AI/ChatGPT in O&SCM. We collected data from O&SCM practitioners in the UK (N = 154) and the USA (N = 161). As we used the organizational learning theory for the research, our findings reveal increased efficiency as a significant benefit for both adopters and non-adopters in both countries, while indicating security, risks, and ethical as prominent concerns. In particular, it appeared that the integration of Gen-AI/ChatGPT leads to the enhancement of the overall supply chain performance. Moreover, organizational learning can speed up the results of Gen-AI/ChatGPT in O&SCM. No wonders that adopters express their satisfaction about the post-implementation benefits of the technology, which include reduced perceived challenges for pre-implementation, and greater optimism about future Gen-AI/ChatGPT utilization compared to non-adopters. Adopters also display diverse behavioral patterns toward efficiency, agility, responsiveness, etc. This study provides valuable insights for scholars, practitioners, and policymakers interested in comprehending Gen-AI/ChatGPT's implications in O&SCM for both adopters and non-adopters. Additionally, it underscores the importance of organizational learning processes in facilitating successful Gen-AI/ChatGPT adoption in O&SCM.
               "
10.1002/sys.21688,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85153519422&origin=inward,Article,SCOPUS_ID:85153519422,scopus,2023-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),model-based-systems-engineering for conceptual design: an integrative approach,"
AbstractView references

Conceptual-Design is an early development phase, where innovation and creativeness shape the future system/product. Model-Based-Conceptual-Design (MBCD) attempts to use best-practices of Model-Based-Systems-Engineering (MBSE) to gain the envisioned benefits of model connectivity. Using MBSE supporting tools can transform Conceptual-Design into a digital-engineered process but may impede creativity and innovation. Concurrently, the design domain offers specific methods and tools for innovative Conceptual-Design. In the current study, we explore an existing Conceptual-Design framework and offer MBSE interpretation and tools extensions needed for its digital implementation. Through such exploration we highlight MBCD specific insights and discuss modeling-innovation interrelations. The implementation was accomplished using a domain-specific enabling software package on top of a market-accepted UML/SysML platform, extending the language definitions, where appropriate. The framework guided extensions allow generation of innovative bottom-up alternatives, solution integration, and solutions’ comparison. The use of modeling is shown to offer clearer process definition, specific methods assistance, and alternative ranking—both manually and automatically. Consequently, MBCD is accomplished, which supports innovation, while being digitally connected to full-scale-development models and the organizational assets at large. Through integration into the orderly Systems-Engineering process, traceability is maintained, and repeated iterations are supported, where conceptual decisions may be revisited. Additionally, through the introduction of an assets’ catalog, cross-organizational knowledge sharing is accomplished. The paper presents samples of the extensions, using a simplified example of technology design for Future Firefighting. The value of incorporating Conceptual-Design specific methodology and tools is evaluated through feedback from multiple domain experts. Discussion and future research directions are offered. © 2023 The Authors. Systems Engineering published by Wiley Periodicals LLC.
"
10.21037/jmai-23-71,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178933851&origin=inward,Article,SCOPUS_ID:85178933851,scopus,2023-10-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the impact of prompt engineering in large language model performance: a psychiatric example,"
AbstractView references

Large language models (LLMs) are increasing in prevalence, and the use of these tools in academic medical research is increasing. ChatGPT is one specific example of an LLM. Many research studies utilize ChatGPT to perform a task (take a medical exam, for instance), and the researchers will assess the model’s ability to perform such a task. They often look at the results and determine whether the model is effective at this task or not. However, little attention is paid to the prompts that are input into the model. OpenAI, ChatGPT’s parent company, provides a free course for developers on prompt engineering, which is the process of asking the LLM to perform a task. The purpose of this course is predicated on the fact that the construction of a prompt dictates the performance of the model. Therefore, in medical research studies, we should be paying more attention to prompt construction as a variable in a model’s performance. This investigation utilized ChatGPT 4.0 to answer a common question patients ask in mental healthcare: “How can I feel happier?” The model was given varying levels of prompting to answer this question, and it significantly impacted the responses. Furthermore, specific prompts resulted in the LLM suggesting potentially harmful answers to the proposed question. This highlights the importance of prompt engineering in achieving desired results from LLMs, especially in the realm of mental healthcare where context and subjectivity play larger roles than in most of medicine. © Journal of Medical Artificial Intelligence. All rights reserved.
"
10.1145/3586183.3606756,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178453037&origin=inward,Conference Paper,SCOPUS_ID:85178453037,scopus,2023-10-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sensecape: enabling multilevel exploration and sensemaking with large language models,"
AbstractView references

People are increasingly turning to large language models (LLMs) for complex information tasks like academic research or planning a move to another city. However, while they often require working in a nonlinear manner - e.g., to arrange information spatially to organize and make sense of it, current interfaces for interacting with LLMs are generally linear to support conversational interaction. To address this limitation and explore how we can support LLM-powered exploration and sensemaking, we developed Sensecape, an interactive system designed to support complex information tasks with an LLM by enabling users to (1) manage the complexity of information through multilevel abstraction and (2) switch seamlessly between foraging and sensemaking. Our within-subject user study reveals that Sensecape empowers users to explore more topics and structure their knowledge hierarchically, thanks to the externalization of levels of abstraction. We contribute implications for LLM-based workflows and interfaces for information tasks. © 2023 Owner/Author.
"
10.1145/3586182.3615825,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178317268&origin=inward,Conference Paper,SCOPUS_ID:85178317268,scopus,2023-10-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),bringing context-aware completion suggestions to arbitrary text entry interfaces,"
AbstractView references

Large language models (LLMs) can predict ""obvious""next steps that users will take in text entry fields, especially the tedious components of tasks like software engineering or email composition. These models are not only useful in large, unbroken text fields, however. We present OmniFill, a browser extension that detects text entry fields and offers ""autofill""-style suggestions based on context from the browsing session. The system constructs an LLM prompt that includes three main components: (a) a description of the active tab's text fields and their current values, (b) information from the user's recent web browsing context, and (c) a history, if available, of the user's prior submissions to the web form (alongside those submissions' associated browsing context). Suggestions from the LLM's response are offered to the user to be automatically typed into each corresponding text field. We offer a motivating example of a time-saving interaction and discuss the broader utility of interface-agnostic LLM integrations. © 2023 Owner/Author.
"
10.1145/3606038.3616157,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178270305&origin=inward,Conference Paper,SCOPUS_ID:85178270305,scopus,2023-10-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generating factually consistent sport highlights narrations,"
AbstractView references

Sports highlights are an important form of media for fans worldwide, as they provide short videos that capture key moments from games, often accompanied by the original commentaries of the game's announcers. However, traditional forms of presenting sports highlights have limitations in conveying the complexity and nuance of the game. In recent years, the use of Large Language Models (LLMs) for natural language generation has emerged and is a promising approach for generating narratives that can provide a more compelling and accessible viewing experience. In this paper, we propose an end-to-end solution to enhance the experience of watching sports highlights by automatically generating factually consistent narrations using LLMs and crowd noise extraction. Our solution involves several steps, including extracting the source of information from the live broadcast using a transcription model, prompt engineering, and comparing out-of-the-box models for consistency evaluation. We also propose a new dataset annotated on generated narratives from 143 Premier League plays and fine-tune a Natural Language Inference (NLI) model on it, achieving 92% precision. Furthermore, we extract crowd noise from the original video to create a more immersive and realistic viewing experience for sports fans by adapting speech enhancement SOTA models on a brand new dataset created from 155 Ligue 1 games. © 2023 ACM.
"
10.1145/3586182.3616623,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178265369&origin=inward,Conference Paper,SCOPUS_ID:85178265369,scopus,2023-10-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative facial expressions and eye gaze behavior from prompts for multi-human-robot interaction,"
AbstractView references

Nonverbal cues such as eye gaze and facial expressions play critical roles in conveying intent, regulating conversation, and fostering engagement. A robot's ability to effectively deploy these behaviors can significantly enhance human-robot collaboration. We describe a simple zero-shot learning approach to generate facial expression and gaze shifting behaviors to control a social robot conversing with an individual or group. An initial prompt provides instructions to a pre-trained large language model on how the model can control a robot's facial expression and eye gaze behaviors during a conversation. To demonstrate this, we describe a proof-of-concept implementation using the robot Furhat. This simple and easily customizable approach can be used to improve perception of a robot's social presence in multi-human-robot interactions. © 2023 Owner/Author.
"
10.1145/3586182.3616663,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178263262&origin=inward,Conference Paper,SCOPUS_ID:85178263262,scopus,2023-10-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),itutor: a generative tutorial system for teaching the elders to use smartphone applications,"
AbstractView references

We present iTutor, a generative tutorial system for promoting smartphone use proficiency among elders. iTutor is unique because it can dynamically generate tutorials based on current operation goals and UI context, which we achieved through leveraging prompt engineering to large language models (LLMs). Our evaluations showed potential for this approach, as we yielded 78.6% accuracy in the instruction generation process. We conclude by providing the roadmap for further development. © 2023 Owner/Author.
"
10.1145/3586182.3616660,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178253467&origin=inward,Conference Paper,SCOPUS_ID:85178253467,scopus,2023-10-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chainforge: an open-source visual programming environment for prompt engineering,"
AbstractView references

Prompt engineering for large language models (LLMs) is a critical to effectively leverage their capabilities. However, due to the inherent stochastic and opaque nature of LLMs, prompt engineering is far from an exact science. Crafting prompts that elicit the desired responses still requires a lot of trial and error to gain a nuanced understanding of a model's strengths and limitations for one's specific task context and target application. To support users in sensemaking around the outputs of LLMs, we create ChainForge, an open-source visual programming environment for prompt engineering. ChainForge is publicly available, both on the web (https://chainforge.ai) and as a locally installable Python package hosted on PyPI. We detail some features of ChainForge and how we iterated the design in response to internal and external feedback. © 2023 Owner/Author.
"
10.1145/3586183.3606725,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175688335&origin=inward,Conference Paper,SCOPUS_ID:85175688335,scopus,2023-10-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),promptify: text-to-image generation through interactive prompt exploration with large language models,"
AbstractView references

Text-to-image generative models have demonstrated remarkable capabilities in generating high-quality images based on textual prompts. However, crafting prompts that accurately capture the user's creative intent remains challenging. It often involves laborious trial-and-error procedures to ensure that the model interprets the prompts in alignment with the user's intention. To address these challenges, we present Promptify, an interactive system that supports prompt exploration and refinement for text-to-image generative models. Promptify utilizes a suggestion engine powered by large language models to help users quickly explore and craft diverse prompts. Our interface allows users to organize the generated images flexibly, and based on their preferences, Promptify suggests potential changes to the original prompt. This feedback loop enables users to iteratively refine their prompts and enhance desired features while avoiding unwanted ones. Our user study shows that Promptify effectively facilitates the text-to-image workflow, allowing users to create visually appealing images on their first attempt while requiring significantly less cognitive load than a widely-used baseline tool. © 2023 ACM.
"
10.1016/j.commatsci.2023.112525,S0927025623005190,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172884480&origin=inward,Article,SCOPUS_ID:85172884480,scopus,2023-10-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a scalable crystal representation for reverse engineering of novel inorganic materials using deep generative models,"
                  The efficient search for crystals with targeted properties is a significant challenge in materials discovery. The rapidly growing field of materials informatics has so far primarily focused on the application of AI/ML models to predict the properties of known crystals from their fundamental and derived properties as descriptors. In the last few years, deep learning-based approaches have spawned a slew of innovative data-driven materials research applications. Materials scientists have used these techniques for the reverse engineering of crystal structures for target applications. However, one of the challenges has been the representation of the crystal structures in the machine readable format. Proposed representations in the literature lack in generality and scalability. In this paper, we train a conditional variational autoencoder with a scalable and invertible representation along with the elemental properties of the constituents as descriptors to inverse-design new crystal structures with specified attributes. When targeting formation energy, we show that our model predicts structures that are not in the complete OQMD database. Finally, we use first-principles density functional theory calculations to validate our findings and show that the developed model is able to generate novel crystal structures for targeted property, i.e. formation energy in this case.
               "
10.1145/3618305.3623587,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178368001&origin=inward,Conference Paper,SCOPUS_ID:85178368001,scopus,2023-10-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),large language models for automated program repair,"
AbstractView references

This paper introduces two methods for automated program repair (APR) utilizing pre-Trained language models. The first method demonstrates program repair as a code completion task and is validated on a dataset of Java programs. The second method, Mentat, leverages OCaml's parser and type system as fault localization techniques to generate prompts for GPT-3, producing candidate patches. Evaluation results show promising repair rates, with 27% and 39.2% effectiveness, respectively. For OCaml, a comparative study employing an automated validation strategy is presented in which the technique outperforms other tools. Language models are effective at APR, enhancing bug fixing and freeing developers to focus on other critical aspects of software engineering. © 2023 ACM.
"
10.1145/3583780.3614737,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178152933&origin=inward,Conference Paper,SCOPUS_ID:85178152933,scopus,2023-10-21,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),datadoc analyzer: a tool for analyzing the documentation of scientific datasets,"
AbstractView references

Recent public regulatory initiatives and relevant voices in the ML community have identified the need to document datasets according to several dimensions to ensure the fairness and trustworthiness of machine learning systems. In this sense, the data-sharing practices in the scientific field have been quickly evolving in the last years, with more and more research works publishing technical documentation together with the data for replicability purposes. However, this documentation is written in natural language, and its structure, content focus, and composition vary, making them challenging to analyze. We present DataDoc Analyzer, a tool for analyzing the documentation of scientific datasets by extracting the details of the main dimensions required to analyze the fairness and potential biases. We believe that our tool could help improve the quality of scientific datasets, aid dataset curators during its documentation process, and be a helpful tool for empirical studies on the overall quality of the datasets used in the ML field. The tool implements an ML pipeline that uses Large Language Models at its core for information retrieval. DataDoc is open-source, and a public demo is published online. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
10.1145/3622758.3622882,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174877165&origin=inward,Conference Paper,SCOPUS_ID:85174877165,scopus,2023-10-18,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),will code remain a relevant user interface for end-user programming with generative ai models,"
AbstractView references

The research field of end-user programming has largely been concerned with helping non-experts learn to code sufficiently well in order to achieve their tasks. Generative AI stands to obviate this entirely by allowing users to generate code from naturalistic language prompts. In this essay, we explore the extent to which ""traditional""programming languages remain relevant for non-expert end-user programmers in a world with generative AI. We posit the ""generative shift hypothesis"": That generative AI will create qualitative and quantitative expansions in the traditional scope of end-user programming. We outline some reasons that traditional programming languages may still be relevant and useful for end-user programmers. We speculate whether each of these reasons might be fundamental and enduring, or whether they may disappear with further improvements and innovations in generative AI. Finally, we articulate a set of implications for end-user programming research, including the possibility of needing to revisit many well-established core concepts, such as Ko's learning barriers and Blackwell's attention investment model. © 2023 ACM.
"
10.4018/979-8-3693-1634-4.ch016,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177521600&origin=inward,Book Chapter,SCOPUS_ID:85177521600,scopus,2023-10-16,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),empowering business transformation: the positive impact and ethical considerations of generative ai in software product management - a systematic literature review,
10.1051/e3sconf/202343001148,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175427047&origin=inward,Conference Paper,SCOPUS_ID:85175427047,scopus,2023-10-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a survey (nlp) natural language processing and transactions on (nnl) neural networks and learning systems,"
AbstractView references

Natural Language Processing NLP, or Natural Language Processing, is an area of artificial intelligence (AI) that concentrates on the interaction between computers and human language. Its goal is to develop algorithms and models that enable computers to understand, interpret, and generate human language in a meaningful manner. NLP aims to bridge the gap between human language and computer language, enabling computers to process and comprehend natural language data effectively. This field encompasses a range of tasks, including speech recognition, language translation, sentiment analysis, text summarization, question answering, and many others. These tasks rely on machine learning models that are trained using large amounts of annotated data. media monitoring, information retrieval, customer support chatbots, and many other areas where understanding human language is crucial. As research and development in NLP progress, we can anticipate the emergence of more advanced and sophisticated models that will further enhance the capabilities of language processing and understanding by computers. This advancement holds exciting possibilities for human-computer interaction. NLP research has seamless communication and empowering us to extract valuable knowledge from textual data on a large scale. technique employed in the process of multi-criteria decision-making and prioritizing alternatives. Its purpose is to provide a structured method for assessing and ranking different options in the presence of multiple criteria that may conflict with one another. MOORA is widely utilized across various domains, including operations research, engineering, and management. Alternative: Programming language, Linguistic theory, Hardware platform, Knowledge Representation. © 2023 EDP Sciences. All rights reserved.
"
10.1145/3610099,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174403833&origin=inward,Article,SCOPUS_ID:85174403833,scopus,2023-10-04,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),probing respiratory care with generative deep learning,"
AbstractView references

This paper combines design, machine learning and social computing to explore generative deep learning as both tool and probe for respiratory care. We first present GANspire, a deep learning tool that generates fine-grained breathing waveforms, which we crafted in collaboration with one respiratory physician, attending to joint materialities of human breathing data and deep generative models. We then relate a probe, produced with breathing waveforms generated with GANspire, and led with a group of ten respiratory care experts, responding to its material attributes. Qualitative annotations showed that respiratory care experts interpreted both realistic and ambiguous attributes of breathing waveforms generated with GANspire, according to subjective aspects of physiology, activity and emotion. Semi-structured interviews also revealed experts' broader perceptions, expectations and ethical concerns on AI technology, based on their clinical practice of respiratory care, and reflexive analysis of GANspire. These findings suggest design implications for technological aids in respiratory care, and show how ambiguity of deep generative models can be leveraged as a resource for qualitative inquiry, enabling socio-material research with generative deep learning. Our paper contributes to the CSCW community by broadening how generative deep learning may be approached not only as a tool to design human-computer interactions, but also as a probe to provoke open conversations with communities of practice about their current and speculative uses of AI technology. © 2023 ACM.
"
10.1145/3616961.3616974,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180010359&origin=inward,Conference Paper,SCOPUS_ID:85180010359,scopus,2023-10-03,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"""call me kiran"" chatgpt as a tutoring chatbot in a computer science course","
AbstractView references

Natural language processing has taken enormous steps during the last few years. The development of large language models and generative AI has elevated natural language processing to the level that it can output coherent and contextually relevant text for a given natural language prompt. ChatGPT is one incarnation of these steps, and its use in education is a rather new phenomenon. In this paper, we study students' perception on ChatGPT during a computer science course. On the course, we integrated ChatGPT into Teams private discussion groups. In addition, all the students had freedom to employ ChatGPT and related technologies to help them in their coursework. The results show that the majority of students had at least tested AI-powered chatbots, and that students are using AI-powered chatbots for multiple tasks, e.g., debugging code, tutoring, and enhancing comprehension. The amount of positive implications of using ChatGPT takes over the negative implications, when the implications were considered from an understanding, learning and creativity perspective. Relatively many students reported reliability issues with the outputs and that the iterations with prompts might be necessary for satisfactory outputs. It is important to try to steer the usage of ChatGPT so that it complements students' learning processes, but does not replace it. © 2023 Owner/Author.
"
10.3390/math11204230,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175052522&origin=inward,Article,SCOPUS_ID:85175052522,scopus,2023-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),predictive prompts with joint training of large language models for explainable recommendation,"
AbstractView references

Large language models have recently gained popularity in various applications due to their ability to generate natural text for complex tasks. Recommendation systems, one of the frequently studied research topics, can be further improved using the capabilities of large language models to track and understand user behaviors and preferences. In this research, we aim to build reliable and transparent recommendation system by generating human-readable explanations to help users obtain better insights into the recommended items and gain more trust. We propose a learning scheme to jointly train the rating prediction task and explanation generation task. The rating prediction task learns the predictive representation from the input of user and item vectors. Subsequently, inspired by the recent success of prompt engineering, these predictive representations are served as predictive prompts, which are soft embeddings, to elicit and steer any knowledge behind language models for the explanation generation task. Empirical studies show that the proposed approach achieves competitive results compared with other existing baselines on the public English TripAdvisor dataset of explainable recommendations. © 2023 by the authors.
"
10.1007/s13272-023-00683-w,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173771116&origin=inward,Article,SCOPUS_ID:85173771116,scopus,2023-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),model-based design and multidisciplinary optimization of complex system architectures in the aircraft cabin,"
AbstractView references

The aviation industry is currently facing major challenges due to environmental and socio-economic trends toward sustainable and digitalized aviation. Revolutionary, more powerful and efficient technologies must be rapidly integrated into aircraft, while aircraft manufacturers must demonstrate the required safety. To support the implementation of new concepts, the DLR Institute of System Architectures in Aeronautics is researching methods for end-to-end digitalization from the preliminary design phase to assembly and production. In this context, Model-Based Systems Engineering (MBSE) and Multidisciplinary Design Optimization are important approaches for the development of complex systems. This paper presents a method for the end-to-end use of digital models for multidisciplinary optimization of system architectures. The Systems Modeling Language (SysML) is used to represent the system architecture. The focus is on the cabin and cabin systems, since they are highly coupled to other aircraft systems and have dynamic, customer-specific configuration requirements. The system architecture in SysML is instantiated and configured by the interface to the aircraft fuselage and cabin design parameter sets in the Common Parametric Configuration Schema. The subsequent coupling of the generated system architecture model with the cabin system design model developed in Matlab allows a multidisciplinary optimization of the system properties. A sensitivity analysis is performed using the Passenger Service Unit as an example. The effects of different cabin configurations on the system architecture are investigated and interdisciplinary synergies are identified and analyzed. The results of this analysis are discussed in this paper. © 2023, The Author(s).
"
10.1038/s43588-023-00527-x,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173757934&origin=inward,Article,SCOPUS_ID:85173757934,scopus,2023-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt,"
AbstractView references

We design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI’s generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input–output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics. © 2023, The Author(s).
"
10.1109/TSE.2023.3310874,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170519171&origin=inward,Article,SCOPUS_ID:85170519171,scopus,2023-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"dexbert: effective, task-agnostic and fine-grained representation learning of android bytecode","
AbstractView references

The automation of an increasingly large number of software engineering tasks is becoming possible thanks to Machine Learning (ML). One foundational building block in the application of ML to software artifacts is the representation of these artifacts (e.g., source code or executable code) into a form that is suitable for learning. Traditionally, researchers and practitioners have relied on manually selected features, based on expert knowledge, for the task at hand. Such knowledge is sometimes imprecise and generally incomplete. To overcome this limitation, many studies have leveraged representation learning, delegating to ML itself the job of automatically devising suitable representations and selections of the most relevant features. Yet, in the context of Android problems, existing models are either limited to coarse-grained whole-app level (e.g., apk2vec) or conducted for one specific downstream task (e.g., smali2vec). Thus, the produced representation may turn out to be unsuitable for fine-grained tasks or cannot generalize beyond the task that they have been trained on. Our work is part of a new line of research that investigates effective, task-agnostic, and fine-grained universal representations of bytecode to mitigate both of these two limitations. Such representations aim to capture information relevant to various low-level downstream tasks (e.g., at the class-level). We are inspired by the field of Natural Language Processing, where the problem of universal representation was addressed by building Universal Language Models, such as BERT, whose goal is to capture abstract semantic information about sentences, in a way that is reusable for a variety of tasks. We propose DexBERT, a BERT-like Language Model dedicated to representing chunks of DEX bytecode, the main binary format used in Android applications. We empirically assess whether DexBERT is able to model the DEX language and evaluate the suitability of our model in three distinct class-level software engineering tasks: Malicious Code Localization, Defect Prediction, and Component Type Classification. We also experiment with strategies to deal with the problem of catering to apps having vastly different sizes, and we demonstrate one example of using our technique to investigate what information is relevant to a given task. © 1976-2012 IEEE.
"
10.1007/s10270-023-01084-7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147772739&origin=inward,Article,SCOPUS_ID:85147772739,scopus,2023-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),assessing the usefulness of a visual programming ide for large-scale automation software,"
AbstractView references

Industrial control applications are usually designed by domain experts instead of software engineers. These experts frequently use visual programming languages based on standards such as IEC 61131-3 and IEC 61499. The standards apply model-based engineering concepts to abstract from hardware and low-level communication. Developing industrial control software is challenging due to the fact that control systems are usually unique and need to be maintained for many years. The arising challenges, together with the growing complexity of control software, require very usable model-based development environments for visual programming languages. However, so far only little empirical research exists on the practical usefulness of such environments, i.e., their usability and utility. In this paper, we discuss common control software maintenance tasks and tool capabilities based on existing research and show the realization of these capabilities in the 4diac IDE. We performed a walkthrough of the demonstrated capabilities using the cognitive dimensions of notations framework from the field of human–computer interaction. We then improved the tool and conducted a user study involving ten industrial automation engineers, who used the 4diac IDE in a realistic control software maintenance scenario. Based on lessons learnt from this study, we adapted the 4diac IDE to better handle large graphical models. We evaluated these changes in a reassessment study with automation engineers from seven industrial enterprises. We derive general implications with respect to large-scale applications for developers of IDEs that we deem applicable in the context of (visual) model-based engineering tools. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.1145/3624032.3624035,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175402712&origin=inward,Conference Paper,SCOPUS_ID:85175402712,scopus,2023-09-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an initial investigation of chatgpt unit test generation capability,"
AbstractView references

Context: Software testing ensures software quality, but developers often disregard it. The use of automated testing generation is pursued to reduce the consequences of overlooked test cases in a software project. Problem: In the context of Java programs, several tools can completely automate generating unit test sets. Additionally, studies are conducted to offer evidence regarding the quality of the generated test sets. However, it is worth noting that these tools rely on machine learning and other AI algorithms rather than incorporating the latest advancements in Large Language Models (LLMs). Solution: This work aims to evaluate the quality of Java unit tests generated by an OpenAI LLM algorithm, using metrics like code coverage and mutation test score. Method: For this study, 33 programs used by other researchers in the field of automated test generation were selected. This approach was employed to establish a baseline for comparison purposes. For each program, 33 unit test sets were generated automatically, without human interference, by changing Open AI API parameters. After executing each test set, metrics such as code line coverage, mutation score, and success rate of test execution were collected to evaluate the efficiency and effectiveness of each set. Summary of Results: Our findings revealed that the OpenAI LLM test set demonstrated similar performance across all evaluated aspects compared to traditional automated Java test generation tools used in the previous research. These results are particularly remarkable considering the simplicity of the experiment and the fact that the generated test code did not undergo human analysis. © 2023 ACM.
"
10.1093/postmj/qgad053,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172424430&origin=inward,Article,SCOPUS_ID:85172424430,scopus,2023-09-21,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),artificial intelligence in orthopaedics: can chat generative pre-trained transformer (chatgpt) pass section 1 of the fellowship of the royal college of surgeons (trauma &amp; orthopaedics) examination?,"
AbstractView references

PURPOSE: Chat Generative Pre-trained Transformer (ChatGPT) is a large language artificial intelligence (AI) model which generates contextually relevant text in response to questioning. After ChatGPT successfully passed the United States Medical Licensing Examinations, proponents have argued it should play an increasing role in medical service provision and education. AI in healthcare remains in its infancy, and the reliability of AI systems must be scrutinized. This study assessed whether ChatGPT could pass Section 1 of the Fellowship of the Royal College of Surgeons (FRCS) examination in Trauma and Orthopaedic Surgery. METHODS: The UK and Ireland In-Training Examination (UKITE) was used as a surrogate for the FRCS. Papers 1 and 2 of UKITE 2022 were directly inputted into ChatGPT. All questions were in a single-best-answer format without wording alterations. Imaging was trialled to ensure ChatGPT utilized this information. RESULTS: ChatGPT scored 35.8%: 30% lower than the FRCS pass rate and 8.2% lower than the mean score achieved by human candidates of all training levels. Subspecialty analysis demonstrated ChatGPT scored highest in basic science (53.3%) and lowest in trauma (0%). In 87 questions answered incorrectly, ChatGPT only stated it did not know the answer once and gave incorrect explanatory answers for the remaining questions. CONCLUSION: ChatGPT is currently unable to exert the higher-order judgement and multilogical thinking required to pass the FRCS examination. Further, the current model fails to recognize its own limitations. ChatGPT's deficiencies should be publicized equally as much as its successes to ensure clinicians remain aware of its fallibility. © The Author(s) 2023. Published by Oxford University Press on behalf of Postgraduate Medical Journal. All rights reserved. For permissions, please e-mail: [email protected].
"
10.1145/3570945.3607303,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183587658&origin=inward,Conference Paper,SCOPUS_ID:85183587658,scopus,2023-09-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompting for socially intelligent agents with chatgpt,"
AbstractView references

Socially Intelligent Agents (SIAs) have become increasingly popular in various contexts, including education and entertainment. However, creating complex social scenarios tailored to a designer’s specific goals remains a significant challenge. The authoring burden can be substantial, limiting the potential of SIAs to deliver rich, engaging experiences. In this work, we propose leveraging the extensive knowledge stored within Large Language Models and use theory-driven prompting to extract social practices and identify appropriate social affordances for a scenario description. Our prompting approach aims to guide the system into considering the essential components (beliefs and desires) necessary to produce intentions, actions, and emotions1. Results show that our approach produces large amounts of accurate and new information that can add value to the scenario. However, the process can introduce inaccuracies without human supervision. © 2023 Copyright held by the owner/author(s).
"
10.1017/dsj.2023.25,S2053470123000252,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172899266&origin=inward,Article,SCOPUS_ID:85172899266,scopus,2023-09-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design representation for performance evaluation of 3d shapes in structure-aware generative design,"
AbstractView references

Data-driven generative design (DDGD) methods utilize deep neural networks to create novel designs based on existing data. The structure-aware DDGD method can handle complex geometries and automate the assembly of separate components into systems, showing promise in facilitating creative designs. However, determining the appropriate vectorized design representation (VDR) to evaluate 3D shapes generated from the structure-aware DDGD model remains largely unexplored. To that end, we conducted a comparative analysis of surrogate models' performance in predicting the engineering performance of 3D shapes using VDRs from two sources: the trained latent space of structure-aware DDGD models encoding structural and geometric information and an embedding method encoding only geometric information. We conducted two case studies: one involving 3D car models focusing on drag coefficients and the other involving 3D aircraft models considering both drag and lift coefficients. Our results demonstrate that using latent vectors as VDRs can significantly deteriorate surrogate models' predictions. Moreover, increasing the dimensionality of the VDRs in the embedding method may not necessarily improve the prediction, especially when the VDRs contain more information irrelevant to the engineering performance. Therefore, when selecting VDRs for surrogate modeling, the latent vectors obtained from training structure-aware DDGD models must be used with caution, although they are more accessible once training is complete. The underlying physics associated with the engineering performance should be paid attention. This paper provides empirical evidence for the effectiveness of different types of VDRs of structure-aware DDGD for surrogate modeling, thus facilitating the construction of better surrogate models for AI-generated designs. © The Author(s), 2023. Published by Cambridge University Press.
"
10.1145/3604932,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174561621&origin=inward,Article,SCOPUS_ID:85174561621,scopus,2023-09-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"demystifying graph databases: analysis and taxonomy of data organization, system designs, and graph queries","
AbstractView references

Numerous irregular graph datasets, for example social networks or web graphs, may contain even trillions of edges. Often, their structure changes over time and they have domain-specific rich data associated with vertices and edges. Graph database systems such as Neo4j enable storing, processing, and analyzing such large, evolving, and rich datasets. Due to the sheer size and irregularity of such datasets, these systems face unique design challenges. To facilitate the understanding of this emerging domain, we present the first survey and taxonomy of graph database systems. We focus on identifying and analyzing fundamental categories of these systems (e.g., document stores, tuple stores, native graph database systems, or object-oriented systems), the associated graph models (e.g., Resource Description Framework or Labeled Property Graph), data organization techniques (e.g., storing graph data in indexing structures or dividing data into records), and different aspects of data distribution and query execution (e.g., support for sharding and Atomicity, Consistency, Isolation, Durability). Fifty-one graph database systems are presented and compared, including Neo4j, OrientDB, and Virtuoso. We outline graph database queries and relationships with associated domains (NoSQL stores, graph streaming, and dynamic graph algorithms). Finally, we outline future research and engineering challenges related to graph databases. Copyright © 2023 held by the owner/author(s). Publication rights licensed to ACM.
"
10.1145/3604915.3608889,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174513106&origin=inward,Conference Paper,SCOPUS_ID:85174513106,scopus,2023-09-14,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),retrieval-augmented recommender system: enhancing recommender systems with large language models,"
AbstractView references

Recommender Systems (RSs) play a pivotal role in delivering personalized recommendations across various domains, from e-commerce to content streaming platforms. Recent advancements in natural language processing have introduced Large Language Models (LLMs) that exhibit remarkable capabilities in understanding and generating human-like text. RS are renowned for their effectiveness and proficiency within clearly defined domains; nevertheless, they are limited in adaptability and incapable of providing recommendations for unexplored data. Conversely, LLMs exhibit contextual awareness and strong adaptability to unseen data. Combining these technologies creates a powerful tool for delivering contextual and relevant recommendations, even in cold scenarios characterized by high data sparsity. The proposal aims to explore the possibilities of integrating LLMs into RS, introducing a novel approach called Retrieval-augmented Recommender Systems, which combines the strengths of retrieval-based and generation-based models to enhance the ability of RSs to provide relevant suggestions. © 2023 Owner/Author.
"
10.1145/3610969.3611132,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174512557&origin=inward,Conference Paper,SCOPUS_ID:85174512557,scopus,2023-09-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative ai in software development education: insights from a degree apprenticeship programme,"
AbstractView references

We describe insights gained from incorporating ChatGPT into assignments for our Software Engineering Degree Apprenticeship programme, including attitudes expressed by the learners and their employers regarding our approach. © 2023 Owner/Author.
"
10.23919/JSC.2023.0020,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181587381&origin=inward,Article,SCOPUS_ID:85181587381,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),unlearning descartes: sentient ai is a political problem,"
AbstractView references

The emergence of Large Language Models (LLMs) has renewed debate about whether Artificial Intelligence (AI) can be conscious or sentient. This paper identifies two approaches to the topic and argues: (1) A 'Cartesian' approach treats consciousness, sentience, and personhood as very similar terms, and treats language use as evidence that an entity is conscious. This approach, which has been dominant in AI research, is primarily interested in what consciousness is, and whether an entity possesses it. (2) An alternative 'Hobbesian' approach treats consciousness as a sociopolitical issue and is concerned with what the implications are for labeling something sentient or conscious. This both enables a political disambiguation of language, consciousness, and personhood and allows regulation to proceed in the face of intractable problems in deciding if something 'really is' sentient. (3) AI systems should not be treated as conscious, for at least two reasons: (a) treating the system as an origin point tends to mask competing interests in creating it, at the expense of the most vulnerable people involved; and (b) it will tend to hinder efforts at holding someone accountable for the behavior of the systems. A major objective of this paper is accordingly to encourage a shift in thinking. In place of the Cartesian question - is AI sentient? - I propose that we confront the more Hobbesian one: Does it make sense to regulate developments in which AI systems behave as if they were sentient? © 2020 Tsinghua University Press.
"
10.1162/dint_a_00227,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180702592&origin=inward,Article,SCOPUS_ID:85180702592,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),improving extraction of chinese open relations using pre-trained language model and knowledge enhancement,"
AbstractView references

Open Relation Extraction (ORE) is a task of extracting semantic relations from a text document. Current ORE systems have significantly improved their efficiency in obtaining Chinese relations, when compared with conventional systems which heavily depend on feature engineering or syntactic parsing. However, the ORE systems do not use robust neural networks such as pre-trained language models to take advantage of large-scale unstructured data effectively. In respons to this issue, a new system entitled Chinese Open Relation Extraction with Knowledge Enhancement (CORE-KE) is presented in this paper. The CORE-KE system employs a pre-trained language model (with the support of a Bidirectional Long Short-Term Memory (BiLSTM) layer and a Masked Conditional Random Field (Masked CRF) layer) on unstructured data in order to improve Chinese open relation extraction. Entity descriptions in Wikidata and additional knowledge (in terms of triple facts) extracted from Chinese ORE datasets are used to fine-tune the pre-trained language model. In addition, syntactic features are further adopted in the training stage of the CORE-KE system for knowledge enhancement. Experimental results of the CORE-KE system on two large-scale datasets of open Chinese entities and relations demonstrate that the CORE-KE system is superior to other ORE systems. The F1-scores of the CORE-KE system on the two datasets have given a relative improvement of 20.1% and 1.3%, when compared with benchmark ORE systems, respectively. The source code is available at https://github. com/cjwen15/CORE-KE. © 2023, MIT Press Journals. All rights reserved.
"
10.15678/EBER.2023.110302,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175052409&origin=inward,Article,SCOPUS_ID:85175052409,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),artificial intelligence prompt engineering as a new digital competence: analysis of generative ai technologies such as chatgpt,"
AbstractView references

Objective: The article aims to offer a thorough examination and comprehension of the challenges and pro-spects connected with artificial intelligence (AI) prompt engineering. Our research aimed to create a theoretical framework that would highlight optimal approaches in the field of AI prompt engineering. Research Design & Methods: This research utilized a narrative and critical literature review and established a conceptual framework derived from existing literature taking into account both academic and practitioner sources. This article should be regarded as a conceptual work that emphasizes the best practices in the domain of AI prompt engineering. Findings: Based on the conducted deep and extensive query of academic and practitioner literature on the subject, as well as professional press and Internet portals, we identified various insights for effective AI prompt engineering. We provide specific prompting strategies. Implications & Recommendations: The study revealed the profound implications of AI prompt engineering across various domains such as entrepreneurship, art, science, and healthcare. We demonstrated how the effective crafting of prompts can significantly enhance the performance of large language models (LLMs), gen-erating more accurate and contextually relevant results. Our findings offer valuable insights for AI practition-ers, researchers, educators, and organizations integrating AI into their operations, emphasizing the need to invest time and resources in prompt engineering. Moreover, we contributed the AI PROMPT framework to the field, providing clear and actionable guidelines for text-to-text prompt engineering. Contribution & Value Added: The value of this study lies in its comprehensive exploration of AI prompt engineering as a digital competence. By building upon existing research and prior literature, this study aimed to provide a deeper understanding of the intricacies involved in AI prompt engineering and its role as a digital competence. © 2023, Cracow University of Economics. All rights reserved.
"
10.3855/jidc.18738,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174233915&origin=inward,Article,SCOPUS_ID:85174233915,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt: ethical concerns and challenges in academics and research,
10.19186/BC_2023.025,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174168771&origin=inward,Article,SCOPUS_ID:85174168771,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative artificial intelligence in (laboratory) medicine: friend or foe?,
10.1386/eta_00143_1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173934735&origin=inward,Article,SCOPUS_ID:85173934735,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),co-creating digital art with generative ai in k-9 education: socio-material insights,"
AbstractView references

The rise of image-generating artificial intelligence (AI) tools has triggered changes in digital art and graphic design, provoking debates in the creative industry. However, scant research exists about children’s and youths’ insights into and encounters with generative AI. Building on sociocultural and new materialist perspectives, this exploratory study proposed to address this gap by exploring middle schoolers’ (N = 10) creative interaction with generative AI, particularly with text-to-image generative models. Qualitative content analyses of emerging learning activities evidenced how generative AI-formed relations were external-ized through novel digital artefacts and collaborative discussions. Ideas evolved through peer collaboration organized around creative making with AI. Teachers facilitated relations between people and technology using dialogic teaching, provid-ing room for unpredictability and critical reflection on the impacts of generative AI, especially authorship and copyright. The study concludes with a discussion of the potential uses of generative AI in future art education research and practice. © 2023 The Author(s).
"
10.3390/infrastructures8090129,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172911707&origin=inward,Article,SCOPUS_ID:85172911707,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),intrinsic properties of composite double layer grid superstructures †,"
AbstractView references

This paper examined the opportunities of composite double-layer grid superstructures in short-to-medium span bridge decks. It was empirically shown here that a double-layer grid deck system in composite action with a thin layer of two−way reinforced concrete slab introduced several structural advantages over the conventional composite plate-girder superstructure system. These advantages included improved seismic performance, increased structural rigidity, reduced deck vibration, increased failure capacity, and so on. Optimally proportioned space grid superstructures were found to be less prone to progressive collapse, increasing structural reliability and resilience, while reducing the risk of sudden failure. Through a set of dynamic time-series experiments, considerable enhancement in load transfer efficiency in the transverse direction under dynamic truck loading was gained. Furthermore, the multi-objective generative optimization of the proposed spatial grid bridge (with integral variable depth) using evolutionary optimization methods was examined. Finally, comprehensive discussions were given on: (i) mechanical properties, such as fatigue behavior, corrosion, durability, and behavior in cold environments; (ii) health monitoring aspects, such as ease of inspection, maintenance, and access for the installation of remote monitoring devices; (iii) sustainability considerations, such as reduction of embodied Carbon and energy due to reduced material waste, along with ease of demolition, deconstruction and reuse after lifecycle design; and (iv) lean management aspects, such as support for industrialized construction and mass customization. It was concluded that the proposed spatial grid system shows promise for building essential and sustainable infrastructures of the future. © 2023 by the authors.
"
10.3390/su151813595,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172889293&origin=inward,Article,SCOPUS_ID:85172889293,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"fostering ai literacy in elementary science, technology, engineering, art, and mathematics (steam) education in the age of generative ai","
AbstractView references

The advancement of generative AI technologies underscores the need for AI literacy, particularly in Southeast Asia’s elementary Science, Technology, Engineering, Art, and Mathematics (STEAM) education. This study explores the development of AI literacy principles for elementary students. Utilizing existing AI literacy models, a three-session classroom intervention was implemented in an Indonesian school, grounded in constructivist, constructionist, and transformative learning theories. Through design-based research (DBR) and network analysis of reflection papers (n = 77), the intervention was evaluated and redesigned. Findings revealed clusters of interdependent elements of learner experiences, categorized into successes, struggles, and alignments with learning theories. These were translated into design moves for future intervention iterations, forming design principles for AI literacy development. The study contributes insights into optimizing the positive effects and minimizing the negative impacts of AI in education. © 2023 by the authors.
"
10.3390/ai4030038,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172788515&origin=inward,Article,SCOPUS_ID:85172788515,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),what is the role of ai for digital twins?,"
AbstractView references

The concept of a digital twin is intriguing as it presents an innovative approach to solving numerous real-world challenges. Initially emerging from the domains of manufacturing and engineering, digital twin research has transcended its origins and now finds applications across a wide range of disciplines. This multidisciplinary expansion has impressively demonstrated the potential of digital twin research. While the simulation aspect of a digital twin is often emphasized, the role of artificial intelligence (AI) and machine learning (ML) is severely understudied. For this reason, in this paper, we highlight the pivotal role of AI and ML for digital twin research. By recognizing that a digital twin is a component of a broader Digital Twin System (DTS), we can fully grasp the diverse applications of AI and ML. In this paper, we explore six AI techniques—(1) optimization (model creation), (2) optimization (model updating), (3) generative modeling, (4) data analytics, (5) predictive analytics and (6) decision making—and their potential to advance applications in health, climate science, and sustainability. © 2023 by the author.
"
10.3390/aerospace10090770,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172146359&origin=inward,Article,SCOPUS_ID:85172146359,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),examining the potential of generative language models for aviation safety analysis: case study and insights using the aviation safety reporting system (asrs),"
AbstractView references

This research investigates the potential application of generative language models, especially ChatGPT, in aviation safety analysis as a means to enhance the efficiency of safety analyses and accelerate the time it takes to process incident reports. In particular, ChatGPT was leveraged to generate incident synopses from narratives, which were subsequently compared with ground-truth synopses from the Aviation Safety Reporting System (ASRS) dataset. The comparison was facilitated by using embeddings from Large Language Models (LLMs), with aeroBERT demonstrating the highest similarity due to its aerospace-specific fine-tuning. A positive correlation was observed between the synopsis length and its cosine similarity. In a subsequent phase, human factors issues involved in incidents, as identified by ChatGPT, were compared to human factors issues identified by safety analysts. The precision was found to be 0.61, with ChatGPT demonstrating a cautious approach toward attributing human factors issues. Finally, the model was utilized to execute an evaluation of accountability. As no dedicated ground-truth column existed for this task, a manual evaluation was conducted to compare the quality of outputs provided by ChatGPT to the ground truths provided by safety analysts. This study discusses the advantages and pitfalls of generative language models in the context of aviation safety analysis and proposes a human-in-the-loop system to ensure responsible and effective utilization of such models, leading to continuous improvement and fostering a collaborative approach in the aviation safety domain. © 2023 by the authors.
"
10.3390/educsci13090856,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172136998&origin=inward,Article,SCOPUS_ID:85172136998,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),challenges and opportunities of generative ai for higher education as explained by chatgpt,"
AbstractView references

ChatGPT is revolutionizing the field of higher education by leveraging deep learning models to generate human-like content. However, its integration into academic settings raises concerns regarding academic integrity, plagiarism detection, and the potential impact on critical thinking skills. This article presents a study that adopts a thing ethnography approach to understand ChatGPT’s perspective on the challenges and opportunities it represents for higher education. The research explores the potential benefits and limitations of ChatGPT, as well as mitigation strategies for addressing the identified challenges. Findings emphasize the urgent need for clear policies, guidelines, and frameworks to responsibly integrate ChatGPT in higher education. It also highlights the need for empirical research to understand user experiences and perceptions. The findings provide insights that can guide future research efforts in understanding the implications of ChatGPT and similar Artificial Intelligence (AI) systems in higher education. The study concludes by highlighting the importance of thing ethnography as an innovative approach for engaging with intelligent AI systems and calls for further research to explore best practices and strategies in utilizing Generative AI for educational purposes. © 2023 by the authors.
"
10.1109/TIV.2023.3307012,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168704878&origin=inward,Article,SCOPUS_ID:85168704878,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),vistagpt: generative parallel transformers for vehicles with intelligent systems for transport automation,"
AbstractView references

Diverse transport demands have resulted in the wide existence of heterogeneous vehicle automation systems. While these systems have demonstrated effectiveness, they also pose challenges in terms of the share of technological advancements among different organizations and lead to poor generalization ability of individual systems. This article proposes a Transformer-based unified framework, VistaGPT, to address these challenges. VistaGPT, composed of Modular Federations of Vehicular Transformers (M-FoV) and Automated Composing of Autonomous Driving Systems (AutoAuto), aims to overcome the information barriers due to system-level and module-level heterogeneity. M-FoV collects and organizes Transformer-based models in a modular fashion to facilitate system integration by providing diversity and versatility. AutoAuto utilizes large language models (LLMs) to automatically compose end-to-end autonomous driving systems with a 'Dividing and Recombining' strategy. Besides, we deploy Scenario Engineering systems to evaluate the composed systems and provide systematic feedback for the optimization of AutoAuto, and Federated intelligence to contribute to diverse training samples and applications. With its capacity, scalability, and diversity, VistaGPT provides a new paradigm of LLM-aided system development for transport automation, which promotes virtual-real interactive parallel driving and advances progress toward '6S' objectives. © 2016 IEEE.
"
10.1016/j.artint.2023.103953,S0004370223000991,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163761347&origin=inward,Article,SCOPUS_ID:85163761347,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),are the bert family zero-shot learners? a study on their potential and limitations,"
                  Starting from the resurgence of deep learning, language models (LMs) have never been so popular. Through simply increasing model scale and data size, large LMs pre-trained with self-supervision objectives demonstrate awe-inspiring results on both task performance and generalization. At the early stage, supervised fine-tuning is indispensable in adapting pre-trained language models (PLMs) to downstream tasks. Later on, the sustained growth of model capacity and data size, as well as newly presented pre-training techniques, make the PLMs perform well under the few-shot setting, especially in the recent paradigm of prompt-based learning. After witnessing the success of PLMs for few-shot tasks, we propose to further study the potential and limitations of PLMs for the zero-shot setting. We utilize 3 models from the most popular BERT family to launch the empirical study on 20 different datasets. We are surprised to find that some simple strategies (without the need of human efforts or unsupervised data) can yield very promising results on a few widely-used datasets, e.g., 
                        88.34
                        %
                        (
                        ±
                        0.60
                        )
                      accuracy on the IMDB dataset, and 
                        84.88
                        %
                        (
                        ±
                        2.83
                        )
                      accuracy on the Amazon dataset, which outperforms manually created prompts without engineering in achieving much better and stable performance with the accuracy of 
                        74.06
                        %
                        (
                        ±
                        13.04
                        )
                     , 
                        75.54
                        %
                        (
                        ±
                        11.77
                        )
                      for comparison. However, we also observe some limitations of PLMs under the zero-shot setting, particularly for the language understanding tasks (e.g., GLUE, SuperGLUE).
                        2
                     
                  
               "
10.1016/j.tsc.2023.101356,S1871187123001256,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163039767&origin=inward,Article,SCOPUS_ID:85163039767,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),beyond semantic distance: automated scoring of divergent thinking greatly improves with large language models,"
                  Automated scoring for divergent thinking (DT) seeks to overcome a key obstacle to creativity measurement: the effort, cost, and reliability of scoring open-ended tests. For a common test of DT, the Alternate Uses Task (AUT), the primary automated approach casts the problem as a semantic distance between a prompt and the resulting idea in a text model. This work presents an alternative approach that greatly surpasses the performance of the best existing semantic distance approaches. Our system, Ocsai, fine-tunes deep neural network-based large-language models (LLMs) on human-judged responses. Trained and evaluated against one of the largest collections of human-judged AUT responses, with 27 thousand responses collected from nine past studies, our fine-tuned large-language-models achieved up to r = 0.81 correlation with human raters, greatly surpassing current systems (r = 0.12–0.26). Further, learning transfers well to new test items and the approach is still robust with small numbers of training labels. We also compare prompt-based zero-shot and few-shot approaches, using GPT-3, ChatGPT, and GPT-4. This work also suggests a limit to the underlying assumptions of the semantic distance model, showing that a purely semantic approach that uses the stronger language representation of LLMs, while still improving on existing systems, does not achieve comparable improvements to our fine-tuned system. The increase in performance can support stronger applications and interventions in DT and opens the space of automated DT scoring to new areas for improving and understanding this branch of methods.
               "
10.1002/jocb.588,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160819403&origin=inward,Article,SCOPUS_ID:85160819403,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),what makes children's responses to creativity assessments difficult to judge reliably?,"
AbstractView references

Open-ended verbal creativity assessments are commonly administered in psychological research and in educational practice to elementary-aged children. Children's responses are then typically rated by teams of judges who are trained to identify original ideas, hopefully with a degree of inter-rater agreement. Even in cases where the judges are reliable, some residual disagreement on the originality of the responses is inevitable. Here, we modeled the predictors of inter-rater disagreement in a large (i.e., 387 elementary school students and 10,449 individual item responses) dataset of children's creativity assessment responses. Our five trained judges rated the responses with a high degree of consistency reliability (α = 0.844), but we undertook this study to predict the residual disagreement. We used an adaptive LASSO model to predict 72% of the variance in our judges' residual disagreement and found that there were certain types of responses on which our judges tended to disagree more. The main effects in our model showed that responses that were less original, more elaborate, prompted by a Uses task, from younger children, or from male students, were all more difficult for the judges to rate reliably. Among the interaction effects, we found that our judges were also more likely to disagree on highly original responses from Gifted/Talented students, responses from Latinx students who were identified as English Language Learners, or responses from Asian students who took a lot of time on the task. Given that human judgments such as these are currently being used to train artificial intelligence systems to rate responses to creativity assessments, we believe understanding their nuances is important. © 2023 The Authors. The Journal of Creative Behavior published by Wiley Periodicals LLC on behalf of Creative Education Foundation (CEF).
"
10.1111/bjet.13337,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159877784&origin=inward,Article,SCOPUS_ID:85159877784,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),learning to work with the black box: pedagogy for a world with artificial intelligence,"
AbstractView references

Artificial intelligence (AI) is increasingly integrating into our society. University education needs to maintain its relevance in an AI-mediated world, but the higher education sector is only beginning to engage deeply with the implications of AI within society. We define AI according to a relational epistemology, where, in the context of a particular interaction, a computational artefact provides a judgement about an optimal course of action and that this judgement cannot be traced. Therefore, by definition, AI must always act as a ‘black box’. Rather than seeking to explain ‘black boxes’, we argue that a pedagogy for an AI-mediated world involves learning to work with opaque, partial and ambiguous situations, which reflect the entangled relationships between people and technologies. Such a pedagogy asks learners locate AI as socially bounded, where AI is always understood within the contexts of its use. We outline two particular approaches to achieve this: (a) orienting students to quality standards that surround AIs, what might be called the tacit and explicit ‘rules of the game’; and (b) providing meaningful interactions with AI systems. Practitioner notes What is already known about this topic Artificial intelligence (AI) is conceptualised in many different ways but is rarely defined in the higher education literature. Experts have outlined a range of graduate capabilities for working in a world of AI such as teamwork or ethical thinking. The higher education literature outlines an imperative need to respond to AI, as underlined by recent commentary on ChatGPT. What this paper adds A definition of an AI that is relational: A particular interaction where a computational artefact provides a judgement about an optimal course of action, which cannot be easily traced. Focusing on working with AI black boxes rather than trying to see inside the technology. Describing a pedagogy for an AI-mediated world that promotes working in complex situations with partial and indeterminate information. Implications for practice and/or policy Focusing on quality standards helps learners understand the social regulating boundaries around AI. Promoting learner interactions with AI as part of a sociotechnical ensemble helps build evaluative judgement in weighting AI's contribution to work. Asking learners to work with AI systems prompts understanding of the evaluative, ethical and practical necessities of working with a black box. © 2023 The Authors. British Journal of Educational Technology published by John Wiley & Sons Ltd on behalf of British Educational Research Association.
"
10.1007/s00766-023-00399-7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85153952780&origin=inward,Article,SCOPUS_ID:85153952780,scopus,2023-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the state-of-practice in requirements specification: an extended interview study at 12 companies,"
AbstractView references

Requirements specification is a core activity in the requirements engineering phase of a software development project. Researchers have contributed extensively to the field of requirements specification, but the extent to which their proposals have been adopted in practice remains unclear. We gathered evidence about the state of practice in requirements specification by focussing on the artefacts used in this activity, the application of templates or guidelines, how requirements are structured in the specification document, what tools practitioners use to specify requirements, and what challenges they face. We conducted an interview-based survey study involving 24 practitioners from 12 different Swedish IT companies. We recorded the interviews and analysed these recordings, primarily by using qualitative methods. Natural language constitutes the main specification artefact but is usually accompanied by some other type of instrument. Most requirements specifications use templates or guidelines, although they seldom follow any fixed standard. Requirements are always structured in the document according to the main functionalities of the system or to project areas or system parts. Different types of tools, including MS Office tools, are used, either individually or combined, in the compilation of requirements specifications. We also note that challenges related to the use of natural language (dealing with ambiguity, inconsistency, and incompleteness) are the most frequent challenges that practitioners face in the compilation of requirements specifications. These findings are contextualized in terms of demographic factors related to the individual interviewees, the organization they are affiliated with, and the project they selected to discuss during our interviews. A number of our findings have been previously reported in related studies. These findings show that, in spite of the large number of notations, models and tools proposed from academia for improving requirements specification, practitioners still mainly rely on plain natural language and general-purpose tool support. We expect more empirical studies in this area in order to better understand the reason of this low adoption of research results. © 2023, The Author(s).
"
10.3233/SHTI230658,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168829278&origin=inward,Conference Paper,SCOPUS_ID:85168829278,scopus,2023-08-23,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"requirements, barriers and tools for participation in an inclusive educational digital environment","
AbstractView references

An inclusive digital environment in education is considered a cornerstone for a modern society and particularly important for learners with disabilities. This paper delves into this topic and presents the findings from a one-week 'Learning, Teaching and Training Activity' (LTTA) conducted as part of the Erasmus+ project on the 'Digital Readiness of Vocational Educational Institutions in an Inclusive Environment.' The LTTA involved discussions on the requirements, barriers, and applicable tools for digitalization in Vocational Educational Institutions (VET) with educators and self-representatives (learners with disabilities). The feedback from participants was analyzed and included evaluations of various tools in terms of their effectiveness and usefulness. These tools encompassed document accessibility, onboard Windows accessibility features, AI in the form of large language models (LLM) as assistive technology, and image recognition-based assistive technologies. Results, presented in this paper, indicate that especially learners with disabilities can benefit from participation in an inclusive digital environment. © 2023 IOS Press. All rights reserved.
"
10.1021/jacs.3c05819,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168222021&origin=inward,Article,SCOPUS_ID:85168222021,scopus,2023-08-16,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt chemistry assistant for text mining and the prediction of mof synthesis,"
AbstractView references

We use prompt engineering to guide ChatGPT in the automation of text mining of metal-organic framework (MOF) synthesis conditions from diverse formats and styles of the scientific literature. This effectively mitigates ChatGPT’s tendency to hallucinate information, an issue that previously made the use of large language models (LLMs) in scientific fields challenging. Our approach involves the development of a workflow implementing three different processes for text mining, programmed by ChatGPT itself. All of them enable parsing, searching, filtering, classification, summarization, and data unification with different trade-offs among labor, speed, and accuracy. We deploy this system to extract 26 257 distinct synthesis parameters pertaining to approximately 800 MOFs sourced from peer-reviewed research articles. This process incorporates our ChemPrompt Engineering strategy to instruct ChatGPT in text mining, resulting in impressive precision, recall, and F1 scores of 90-99%. Furthermore, with the data set built by text mining, we constructed a machine-learning model with over 87% accuracy in predicting MOF experimental crystallization outcomes and preliminarily identifying important factors in MOF crystallization. We also developed a reliable data-grounded MOF chatbot to answer questions about chemical reactions and synthesis procedures. Given that the process of using ChatGPT reliably mines and tabulates diverse MOF synthesis information in a unified format while using only narrative language requiring no coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be very useful across various other chemistry subdisciplines. © 2023 American Chemical Society.
"
10.1016/j.surfcoat.2023.129680,S0257897223004553,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161333918&origin=inward,Article,SCOPUS_ID:85161333918,scopus,2023-08-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative pre-trained transformers (gpt) for surface engineering,"
                  The knowledge of scientific articles within Generative Pre-trained Transformers (GPT) is not exhaustive due to factors such as data coverage, freshness, complexity, paywalls, and context. While it can provide general information on scientific topics, it may struggle with specialized terminology, recent research, and nuanced understanding. As a result, relying on GPT as a scientific assistant tool may not be ideal. Instead, it is important to consult specialized resources and databases for a comprehensive understanding of specific scientific domains and access to the latest research. A custom data driven GPT can enhance its performance as a scientific assistant tool by improving domain knowledge, providing up-to-date information, reducing ambiguity and errors, performing customized tasks, and offering enhanced search capabilities. This work demonstrates and evaluates the use of such GPT models using a small selection of peer reviewed published thermal spray articles as the reference domain knowledge. The specific domain knowledge model works exceptionally well outperforming the general state-of-the-art large language models.
               "
10.1145/3568813.3600142,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169462375&origin=inward,Conference Paper,SCOPUS_ID:85169462375,scopus,2023-08-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),thrilled by your progress! large language models (gpt-4) no longer struggle to pass assessments in higher education programming courses,"
AbstractView references

This paper studies recent developments in large language models' (LLM) abilities to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. The emergence of ChatGPT resulted in heated debates of its potential uses (e.g., exercise generation, code explanation) as well as misuses in programming classes (e.g., cheating). Recent studies show that while the technology performs surprisingly well on diverse sets of assessment instruments employed in typical programming classes the performance is usually not sufficient to pass the courses. The release of GPT-4 largely emphasized notable improvements in the capabilities related to handling assessments originally designed for human test-takers. This study is the necessary analysis in the context of this ongoing transition towards mature generative AI systems. Specifically, we report the performance of GPT-4, comparing it to the previous generations of GPT models, on three Python courses with assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Additionally, we analyze the assessments that were not handled well by GPT-4 to understand the current limitations of the model, as well as its capabilities to leverage feedback provided by an auto-grader. We found that the GPT models evolved from completely failing the typical programming class' assessments (the original GPT-3) to confidently passing the courses with no human involvement (GPT-4). While we identified certain limitations in GPT-4's handling of MCQs and coding exercises, the rate of improvement across the recent generations of GPT models strongly suggests their potential to handle almost any type of assessment widely used in higher education programming courses. These findings could be leveraged by educators and institutions to adapt the design of programming assessments as well as to fuel the necessary discussions into how programming classes should be updated to reflect the recent technological developments. This study provides evidence that programming instructors need to prepare for a world in which there is an easy-to-use widely accessible technology that can be utilized by learners to collect passing scores, with no effort whatsoever, on what today counts as viable programming knowledge and skills assessments. © 2023 Owner/Author.
"
10.1145/3599957.3606244,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174290777&origin=inward,Conference Paper,SCOPUS_ID:85174290777,scopus,2023-08-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt-based debate game application utilizing prompt engineering,"
AbstractView references

This paper1 focuses on the implementation of a debate game using ChatGPT, aiming to investigate the feasibility of incorporating large language models into the educational domain through prompt engineering. The study explores strategies to elicit desired outputs from the GPT model by employing the prompt engineering methodology, as provided by Microsoft. Specifically, the game implementation involves the customization of ChatGPT's responses to facilitate a natural progression of debates, varying levels of difficulty, and an evaluation system for assessing the quality of discourse. By leveraging the prompt engineering methodology, we demonstrate that providing specific instructions or case-based prompts improves the accuracy and relevance of ChatGPT's answers. The developed application targets teenagers, enabling them to engage in real-time debates with ChatGPT and enhance their literacy skills. Furthermore, the game fosters the development of logical reasoning, persuasive abilities, effective expression, active participation, and attentive listening while expressing personal opinions, ultimately fostering a sense of accomplishment. Moreover, through debate evaluation and personalized advice, ChatGPT is expected to recognize and address its shortcomings, thereby continuously improving its conversational capabilities. Overall, this research contributes to the understanding of how large language models can be harnessed in educational settings and underscores the potential benefits of prompt engineering techniques in optimizing the outputs of such models. © 2023 ACM.
"
10.3390/technologies11040084,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169042532&origin=inward,Article,SCOPUS_ID:85169042532,scopus,2023-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),modernizing the legacy healthcare system to decentralize platform using blockchain technology,"
AbstractView references

The use of blockchain technology is expanding in various industries, including finance, supply chain management, food, energy, IoT, and healthcare. The article aims to address the challenges of complex medical procedures, large-scale medical data management, and cost optimization in the healthcare industry. By employing blockchain technology, the article aims to enhance data security and privacy while ensuring the integrity and efficiency of the healthcare system. This article focuses on the application of blockchain technology in the healthcare system by reviewing the existing literature and proposing multiple workflows for better data management. These workflows were implemented using the Ethereum blockchain platform and involve complex medical procedures such as surgery and clinical trials, as well as managing a large amount of medical data. The feasibility of the proposed system is analyzed in terms of associated costs, and a model-driven engineering approach is used to recover the architecture of traditional healthcare systems. The aim is to provide stakeholders in the healthcare system with better healthcare services and cost optimization. The solution being proposed automates interactions between different parties involved. Smart contracts were created using Solidity language, and their functions were tested using the Remix IDE. This paper illustrates that our smart contract code was designed to avoid common security vulnerabilities and attacks. To test the framework, a prototype of the smart contract was deployed on an Ethereum TESTNET blockchain in a Windows environment. This study found that the proposed approach is both practical and efficient. © 2023 by the authors.
"
10.3390/electronics12153364,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167673014&origin=inward,Article,SCOPUS_ID:85167673014,scopus,2023-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rule-based architectural design pattern recognition with gpt models,"
AbstractView references

Architectural design patterns are essential in software development because they offer proven solutions to large-scale structural problems in software systems and enable developers to create software that is more maintainable, scalable, and comprehensible. Model-View-Whatever (MVW) design patterns are prevalent in many areas of software development, but their use in Web development is on the rise. There are numerous subtypes of MVW design patterns applicable to Web systems, but there is no exhaustive listing of them. Additionally, it is unclear how these subtypes can be utilized in contemporary Web development, as their usage is typically unconscious. Here, we discuss and define the most prevalent MVW design patterns used in Web development, as well as provide Angular framework examples and guidance on when to employ a particular design pattern. On the premise of the primary characteristics of design patterns, we created a rule system that large language models (LLMs) can comprehend without doubt. Here, we demonstrate how effectively Generative Pre-trained Transformer (GPT) models can identify various design patterns based on our principles and verify the quality of our recommendations. Together, our solution and GPT models constitute an effective natural language processing (NLP) solution capable of detecting MVW design patterns in Angular projects with an average accuracy of 90%. © 2023 by the authors.
"
10.1016/j.nuclphysb.2023.116279,S0550321323002080,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162796723&origin=inward,Article,SCOPUS_ID:85162796723,scopus,2023-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deep learning for k3 fibrations in heterotic/type iia string duality,"The development of Large Language Models, such as the recently released GPT-4, has revolutionized the field of Machine Learning and opened new avenues for interdisciplinary research. Prompt Engineering, a methodology for designing effective input prompts to guide these AI models, will emerge as a powerful tool for accelerating research efforts. In this study, we leverage Prompt Engineering with GPT-4 to address the problem of predicting K3 Fibrations in Calabi-Yau manifolds embedded in toric varieties with a single weight system. Out of the 184,026 weights spaces previously discovered, 101,495 remained unclassified concerning the presence of a K3 projection, thereby providing an opportunity for machine learning to bridge this gap. By utilizing an ensemble of Deep Neural Networks, we are able to predict the existence of the K3 fibration. Furthermore, we assess the potential of these models to predict the spectrum of possible Hodge Numbers and other properties of reflexive polytopes. These results not only demonstrate the utility of AI in studying the heterotic/Type IIA string duality in F-Theory but also serve as a stepping stone for further machine learning integration into traditional scientific workflows."
10.1016/j.cola.2023.101217,S2590118423000278,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160517467&origin=inward,Article,SCOPUS_ID:85160517467,scopus,2023-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"dandelion: a scalable, cloud-based graphical language workbench for industrial low-code development","There is an increasing demand nowadays for low-code development platforms (LCDPs). As they rely heavily on graphical languages rather than writing code, these platforms enable citizen developers to participate in software development. However, creating new LCDPs is very costly, since it requires building support for graphical modelling and its integration with services like model validation, recommendation systems, or code generation. While Model-driven Engineering (MDE) has developed technologies to create these components, most of them are not cloud-based, as required by LCDPs. In particular, a cloud-based graphical workbench capable of providing the scalability required by industrial applications and adequately supporting technological heterogeneity is currently missing. To fill this gap we introduce Dandelion, a cloud-based graphical language workbench for LCDPs built following an MDE approach. The tool handles model heterogeneity by using a harmonising meta-model to uniformly represent models from diverse technologies, and supports a customisable level of conformance between models and meta-models. Scalability is addressed by persisting models in a distributed, highly flexible database whose infrastructure is designed to conform to the harmonising meta-model, thus favouring model retrieval. Additionally, a customisable scalability component is introduced for lazy model loading. This paper describes the concepts and principles behind the tool design and reports on an evaluation on large synthetic process mining models, and on domain-specific languages and large industrial models used within the UGROUND company, showing promising results."
10.1016/j.artmed.2023.102573,S0933365723000878,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160517078&origin=inward,Article,SCOPUS_ID:85160517078,scopus,2023-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a unified framework of medical information annotation and extraction for chinese clinical text,"
                  Medical information extraction consists of a group of natural language processing (NLP) tasks, which collaboratively convert clinical text to pre-defined structured formats. This is a critical step to exploit electronic medical records (EMRs). Given the recent thriving NLP technologies, model implementation and performance seem no longer an obstacle, whereas the bottleneck locates on a high-quality annotated corpus and the whole engineering workflow. This study presents an engineering framework consisting of three tasks, i.e., medical entity recognition, relation extraction and attribute extraction. Within this framework, the whole workflow is demonstrated from EMR data collection through model performance evaluation. Our annotation scheme is designed to be comprehensive and compatible between the multiple tasks. With the EMRs from a general hospital in Ningbo, China, and the manual annotation by experienced physicians, our corpus is of large scale and high quality. Built upon this Chinese clinical corpus, the medical information extraction system show performance that approaches human annotation. The annotation scheme, (a subset of) the annotated corpus, and the code are all publicly released, to facilitate further research.
               "
10.1126/science.adh1720,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165890118&origin=inward,Article,SCOPUS_ID:85165890118,scopus,2023-07-28,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deploying synthetic coevolution and machine learning to engineer protein-protein interactions,"
AbstractView references

Fine-tuning of protein-protein interactions occurs naturally through coevolution, but this process is difficult to recapitulate in the laboratory. We describe a platform for synthetic protein-protein coevolution that can isolate matched pairs of interacting muteins from complex libraries. This large dataset of coevolved complexes drove a systems-level analysis of molecular recognition between Z domain-affibody pairs spanning a wide range of structures, affinities, cross-reactivities, and orthogonalities, and captured a broad spectrum of coevolutionary networks. Furthermore, we harnessed pretrained protein language models to expand, in silico, the amino acid diversity of our coevolution screen, predicting remodeled interfaces beyond the reach of the experimental library. The integration of these approaches provides a means of simulating protein coevolution and generating protein complexes with diverse molecular recognition properties for biotechnology and synthetic biology. © 2023 The Authors.
"
10.1063/5.0148659,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176724130&origin=inward,Conference Paper,SCOPUS_ID:85176724130,scopus,2023-07-24,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative design of a car transmission system,"
AbstractView references

The study focuses on optimizing the automobile power train in the commercial 4-wheelers sector, in order to obtain lightweight gears, which can be manufactured additively, without compromising on their structural properties. In general, heavy rotating masses such as gear wheels and shafts have a significant impact on fuel consumption. Current technology shows meager potential in lightweight gears mainly due to their manufacturability. With the advent of additive manufacturing, this problem can be tackled. A basic 5-gear transmission system gearbox adhering to the above was modeled using the CAD software SolidWorks, whose components have been illustrated so as to get an idea of what the system has to do, and thereby determine what all it needs to contain to do the same. Then, the model was exported to Fusion360, where the optimizing process called Generative Design had been applied. Generative Design is a paradigm of improved engineering design, transforming the engineering design and manufacturing process as it is today, with the advent of Industry 4.0. This is an AI-powered iterative computerized designing process wherein the requirements of the user and the operating environment of the model have to be adumbrated, upon which the software provides the user with a number of possible outcomes that would best solve the problem as described. Therefore, the operational environment of the gears had to be analyzed and the forces experienced by each one during its performance were calculated so as to feed into the software. The geometry of the design had to be demarcated into regions that were essential, and other regions that could be optimized by the program to provide better results. The final outcomes obtained were scrutinized and have been displayed at the end, giving the reader an impression of how a shape-optimized version of the usual common transmission system would appear. © 2023 Author(s).
"
10.1145/3569951.3597596,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176284423&origin=inward,Conference Paper,SCOPUS_ID:85176284423,scopus,2023-07-23,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deep learning benchmark studies on an advanced ai engineering testbed from the open compass project,"
AbstractView references

We present the Open Compass project's pilot deep learning benchmark results with various AI accelerators. Those accelerators are NVIDIA V-100 and A-100, AMD MI100, as well as emerging novel accelerators such as Cerebras CS-2 and Graphcore. We evaluate their performance on various deep learning training tasks. We then discuss key insights from our experiments and share experiences about evaluating and integrating those novel AI accelerators with our supercomputing systems. © 2023 Owner/Author.
"
10.1145/3583562,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165620488&origin=inward,Article,SCOPUS_ID:85165620488,scopus,2023-07-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated identification of toxic code reviews using toxicr,"
AbstractView references

Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express themselves, therefore get demotivated, and may eventually leave the project. Automated filtering of toxic conversations may help a FOSS community maintain healthy interactions among its members. However, off-the-shelf toxicity detectors perform poorly on a software engineering dataset, such as one curated from code review comments. To counter this challenge, we present ToxiCR, a supervised learning based toxicity identification tool for code review interactions. ToxiCR includes a choice to select one of the 10 supervised learning algorithms, an option to select text vectorization techniques, eight preprocessing steps, and a large-scale labeled dataset of 19,651 code review comments. Two out of those eight preprocessing steps are software engineering domain specific. With our rigorous evaluation of the models with various combinations of preprocessing steps and vectorization techniques, we have identified the best combination for our dataset that boosts 95.8% accuracy and an 88.9% F1-score in identifying toxic texts. ToxiCR significantly outperforms existing toxicity detectors on our dataset. We have released our dataset, pre-trained models, evaluation results, and source code publicly, which is available at https://github.com/WSU-SEAL/ToxiCR. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
10.1145/3539618.3591847,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168695501&origin=inward,Conference Paper,SCOPUS_ID:85168695501,scopus,2023-07-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a transformer-based substitute recommendation model incorporating weakly supervised customer behavior data,"
AbstractView references

The substitute-based recommendation is widely used in E-commerce to provide better alternatives to customers. However, existing research typically uses customer behavior signals like co-view and view-but-purchase-another to capture the substitute relationship. Despite its intuitive soundness, such an approach might ignore the functionality and characteristics of products. In this paper, we adapt substitute recommendations into language matching problem. It takes the product title description as model input to consider product functionality. We design a new transformation method to de-noise the signals derived from production data. In addition, we consider multilingual support from the engineering point of view. Our proposed end-to-end transformer-based model achieves both successes from offline and online experiments. The proposed model has been deployed in a large-scale E-commerce website for 11 marketplaces in 6 languages. Our proposed model is demonstrated to increase revenue by 19% based on an online A/B experiment. © 2023 Copyright held by the owner/author(s).
"
10.1145/3539618.3591929,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168686230&origin=inward,Conference Paper,SCOPUS_ID:85168686230,scopus,2023-07-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),4th workshop on patent text mining and semantic technologies (patentsemtech2023),"
AbstractView references

Information retrieval systems for the patent domain have a long history. They can support patent experts in a variety of daily tasks: from analyzing the patent landscape to support experts in the patenting process and large-scale information extraction. Advances in machine learning and natural language processing allow to further automate tasks, such as paragraph retrieval or even patent text generation. Uncovering the potential of semantic technologies for the intellectual property (IP) industry is just getting started. Investigating the use of artificial intelligence methods for the patent domain is therefore not only of academic interest, but also highly relevant for practitioners. Compared to other domains, high quality, semi-structured, annotated data is available in large volumes (a requirement for supervised machine learning models), making training large models easier. On the other hand, domain-specific challenges arise, such as very technical language or legal requirements for patent documents. The focus of the 4th edition of this workshop will be on two-way communication between industry and academia from all areas of information retrieval in particular with the Asian community. We want to bring together novel research results and the latest systems and methods employed by practitioners in the field. © 2023 Copyright held by the owner/author(s).
"
10.1145/3571884.3603756,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167780180&origin=inward,Conference Paper,SCOPUS_ID:85167780180,scopus,2023-07-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative ai considered harmful,"
AbstractView references

The recent months have seen an explosion of interest, hype, and concern about generative AI, driven by the release of ChatGPT. In this article I seek to explicate some potential and actual harms of the engineering and use of generative AI such as ChatGPT. With this I also suggest a reframing for researchers with an interest in interaction. With this reframing I seek to provoke researchers to consider studying the settings of ChatGPT development and use as active sites of production. Research should focus on the organisational, technological and interactional practices and contexts in and through which generative AI and its outputs - harmful and otherwise - are produced, by whom, to what end, and with what consequences on societies. © 2023 Owner/Author.
"
10.1016/j.oceaneng.2023.114888,S0029801823012726,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162814701&origin=inward,Article,SCOPUS_ID:85162814701,scopus,2023-07-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an unified cad/cae/vr tool for ship structure design and evaluation based on multi-domain feature mapping,"
                  Computer-aided design (CAD), computer-aided engineering (CAE), and virtual reality (VR) are three distinct disciplines. For the collaborative design and interactive evaluation of ship structures, the main obstacle lies in the large differences in model data structures. In this regard, a unified tool for ship structure design/analysis/virtual evaluation based on Multi-Domain Feature Mapping (MDFM) is proposed. The multi-layer transmission and matching mechanism between data, information and knowledge established by Extensible Markup Language (XML) can realize the feature association among design, analysis and evaluation domain, so that CAD, CAE and VR become an organic entity. In the virtual scene, a visual expression strategy of CAD model and FEA data based on Unified Mesh Model (UMM) is proposed. The advantage of this unified tool lies in the intelligent generation of finite element models, as well as the virtual visualization and model reconstruction and evaluation of models with different topologies. Intelligent finite element (FE) meshing and analysis applications integrating knowledge templates represent a major subset of unified CAD/CAE/VR tools for structural models from CAD systems. The structure and data flow management of this unified tool is based on a common design process, which allows designers to link anticipated scenarios with proposed engineering changes. Finally, the effectiveness of the proposed tool is demonstrated by engineering examples.
               "
10.1145/3582000,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168804528&origin=inward,Article,SCOPUS_ID:85168804528,scopus,2023-07-13,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a comparative survey of instance selection methods applied to non-neural and transformer-based text classification,"
AbstractView references

Progress in natural language processing has been dictated by the rule of more: more data, more computing power, more complexity, best exemplified by deep learning Transformers. However, training (or fine-tuning) large dense models for specific applications usually requires significant amounts of computing resources. One way to ameliorate this problem is through data engineering instead of the algorithmic or hardware perspectives. Our focus here is an under-investigated data engineering technique, with enormous potential in the current scenario - Instance Selection (IS) (a.k.a. Selective Sampling, Prototype Selection). The IS goal is to reduce the training set size by removing noisy or redundant instances while maintaining or improving the effectiveness (accuracy) of the trained models and reducing the training process cost. We survey classical and recent state-of-the-art IS techniques and provide a scientifically sound comparison of IS methods applied to an essential natural language processing task - Automatic Text Classification (ATC). IS methods have been normally applied to small tabular datasets and have not been systematically compared in ATC. We consider several neural and non-neural state-of-the-art ATC solutions and many datasets. We answer several research questions based on tradeoffs induced by a tripod (training set reduction, effectiveness, and efficiency). Our answers reveal an enormous unfulfilled potential for IS solutions. Specially, we show that in 12 out of 19 datasets, specific IS methods - namely, Condensed Nearest Neighbor, Local Set-based Smoother, and Local Set Border Selector - can reduce the size of the training set without effectiveness losses. Furthermore, in the case of fine-tuning the Transformer methods, the IS methods reduce the amount of data needed, without losing effectiveness and with considerable training-time gains. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171329846&origin=inward,Article,SCOPUS_ID:85171329846,scopus,2023-07-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt in software development: methods and cross-domain applications,"
AbstractView references

Through the use of natural language processing (NLP) to analyze and synthesize data, ChatGPT has the potential to advance Software Engineering (SE) studies. Yet, it may provide moral challenges such as the possibility for data bias, leakage of private information, and the compromise of sensitive information. Utilizing ChatGPT, we highlight such current developments in SE. We also explore ChatGPT's potential outside of the gaming industry. Finally, we suggest leveraging ChatGPT to simplify UI and UX interaction. This article will provide an established protocol for using ChatGPT applications within SE investigation while keeping ethical concerns in mind. © 2023, Ismail Saritas. All rights reserved.
"
10.1145/3597926.3598135,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167724736&origin=inward,Conference Paper,SCOPUS_ID:85167724736,scopus,2023-07-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),how effective are neural networks for fixing security vulnerabilities,
10.1145/3603719.3603726,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173549029&origin=inward,Conference Paper,SCOPUS_ID:85173549029,scopus,2023-07-10,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),accelerating machine learning queries with linear algebra query processing,
10.1145/3628034.3628042,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185223923&origin=inward,Conference Paper,SCOPUS_ID:85185223923,scopus,2023-07-05,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),introduction to work with genai,"
AbstractView references

This paper provides an in-depth exploration of the practical applications and capabilities of generative AI technologies, specifically focusing on OpenAI’s GPT-3, DALL-E and ChatGPT. It targets individuals who are already acquainted with these technologies but may not fully grasp their extensive potential beyond common use cases. The paper emphasizes the relevance of these technologies across various interdisciplinary fields. Key topics include the strategies for effective prompt construction in text and image generation, the utility of ChatGPT in generating human-like conversational responses for diverse applications and the use of personas to better understand and apply AI in corporate and decision-making contexts. Furthermore, the paper discusses the latest advancements in generative AI, including GPT-4, ChatGPT plugins, enhanced data analysis features and the introduction of DALL-E 3, highlighting their impact on the overall utility and application of these technologies. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
10.1145/3594806.3596520,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170353677&origin=inward,Conference Paper,SCOPUS_ID:85170353677,scopus,2023-07-05,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative transformer chatbots for mental health support: a study on depression and anxiety,"
AbstractView references

Mental health is a critical issue worldwide and effective treatments are available. However, incidence of social stigma prevents many from seeking the support they need. Given the rapid developments in the field of large-language models, this study explores the potential of chatbots to support people experiencing depression and anxiety. The focus of this research is on the engineering aspect of building chatbots, and through topology optimisation find an effective hyperparameter set that can predict tokens with 88.65% accuracy and with a performance of 96.49% and 97.88% regarding the correct token appearing in the top 5 and 10 predictions. Examples of how optimised chatbots can effectively answer questions surrounding mental health are provided, generalising information from verified online sources. The results of this study demonstrate the potential of chatbots to provide accessible and anonymous support to individuals who may otherwise be deterred by the stigma associated with seeking help for mental health issues. However, the limitations and challenges of using chatbots for mental health support must also be acknowledged, and future work is suggested to fully understand the potential and limitations of chatbots and to ensure that they are developed and deployed ethically and responsibly. © 2023 ACM.
"
10.3390/systems11070352,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172161216&origin=inward,Article,SCOPUS_ID:85172161216,scopus,2023-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),agile methodology for the standardization of engineering requirements using large language models,"
AbstractView references

The increased complexity of modern systems is calling for an integrated and comprehensive approach to system design and development and, in particular, a shift toward Model-Based Systems Engineering (MBSE) approaches for system design. The requirements that serve as the foundation for these intricate systems are still primarily expressed in Natural Language (NL), which can contain ambiguities and inconsistencies and suffer from a lack of structure that hinders their direct translation into models. The colossal developments in the field of Natural Language Processing (NLP), in general, and Large Language Models (LLMs), in particular, can serve as an enabler for the conversion of NL requirements into machine-readable requirements. Doing so is expected to facilitate their standardization and use in a model-based environment. This paper discusses a two-fold strategy for converting NL requirements into machine-readable requirements using language models. The first approach involves creating a requirements table by extracting information from free-form NL requirements. The second approach consists of an agile methodology that facilitates the identification of boilerplate templates for different types of requirements based on observed linguistic patterns. For this study, three different LLMs are utilized. Two of these models are fine-tuned versions of Bidirectional Encoder Representations from Transformers (BERTs), specifically, aeroBERT-NER and aeroBERT-Classifier, which are trained on annotated aerospace corpora. Another LLM, called flair/chunk-english, is utilized to identify sentence chunks present in NL requirements. All three language models are utilized together to achieve the standardization of requirements. The effectiveness of the methodologies is demonstrated through the semi-automated creation of boilerplates for requirements from Parts 23 and 25 of Title 14 Code of Federal Regulations (CFRs). © 2023 by the authors.
"
10.1109/MS.2023.3265877,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164735915&origin=inward,Article,SCOPUS_ID:85164735915,scopus,2023-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative ai for software practitioners,"
AbstractView references

Generative artificial intelligence (AI) tools, such as Bard, ChatGPT, and CoPilot, have rapidly gained widespread usage. They also have the potential to boost software engineering productivity. In this article, we elaborate technologies and usage of generative AI in the software industry. We address questions, such as: How does generative AI improve software productivity? How to connect generative AI to software development, and what are the risks? Which technologies have what sorts of benefits? Practitioner guidance and case studies are shared from our industry context. I look forward to hearing from you about this column and the technologies that matter most for your work-Christof Ebert © 1984-2012 IEEE.
"
10.1142/S0218194023500213,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162792837&origin=inward,Article,SCOPUS_ID:85162792837,scopus,2023-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),codelabeller: a web-based code annotation tool for java design patterns and summaries,"
AbstractView references

While constructing supervised learning models, we require labeled examples to build a corpus and train a machine learning model. However, most studies have built the labeled dataset manually, which, on many occasions, is a daunting task. To mitigate this problem, we have built an online tool called CodeLabeller. CodeLabeller is a web-based tool that aims to provide an efficient approach to handling the process of labeling source code files for supervised learning methods at scale by improving the data collection process throughout. CodeLabeller is tested by constructing a corpus of over a thousand source files obtained from a large collection of open source Java projects and labeling each Java source file with their respective design patterns and summaries. Twenty-five experts in the field of software engineering participated in a usability evaluation of the tool using the standard User Experience Questionnaire online survey. The survey results demonstrate that the tool achieves the Good standard on hedonic and pragmatic quality standards, is easy to use and meets the needs of annotating the corpus for supervised classifiers. Apart from assisting researchers in crowdsourcing a labeled dataset, the tool has practical applicability in software engineering education and assists in building expert ratings for software artefacts. © 2023 World Scientific Publishing Company.
"
10.1016/j.ece.2023.05.005,S1749772823000222,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162078243&origin=inward,Article,SCOPUS_ID:85162078243,scopus,2023-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a perspective on the synergistic potential of artificial intelligence and product-based learning strategies in biobased materials education,"
                  The integration of product-based learning strategies in Materials in Chemical Engineering education is crucial for students to gain the skills and competencies required to thrive in the emerging circular bioeconomy. Traditional materials engineering education has often relied on a transmission teaching approach, in which students are expected to passively receive information from instructors. However, this approach has shown to be inadequate under the current circumstances, in which information is readily available and innovative tools such as artificial intelligence and virtual reality environments are becoming widespread (e.g., metaverse). Instead, we consider that a critical goal of education should be to develop aptitudes and abilities that enable students to generate solutions and products that address societal demands. In this work, we propose innovative strategies, such as product-based learning methods and GPT (Generative Pre-trained Transformer) artificial intelligence text generation models, to modify the focus of a Materials in Chemical Engineering course from non-sustainable materials to sustainable ones, aiming to address the critical challenges of our society. This approach aims to achieve two objectives: first to enable students to actively engage with raw materials and solve real-world challenges, and second, to foster creativity and entrepreneurship skills by providing them with the necessary tools to conduct brainstorming sessions and develop procedures following scientific methods. The incorporation of circular bioeconomy concepts, such as renewable resources, waste reduction, and resource efficiency into the curriculum provides a framework for students to understand the environmental, social, and economic implications in Chemical Engineering. It also allows them to make informed decisions within the circular bioeconomy framework, benefiting society by promoting the development and adoption of sustainable technologies and practices.
               "
10.1007/s10664-023-10314-x,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160023381&origin=inward,Article,SCOPUS_ID:85160023381,scopus,2023-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),evaluating pre-trained models for user feedback analysis in software engineering: a study on classification of app-reviews,"
AbstractView references

Context: Automatic classification of mobile applications users’ feedback is studied for different areas of software engineering. However, supervised classification requires a lot of manually labeled data, and with introducing new classes or new platforms, new labeled data and models are required. Employing Pre-trained neural Language Models (PLMs) have found success in the Natural Language Processing field. However, their applicability has not been explored for app review classification. Objective: We evaluate using PLMs for issue classification from app reviews in multiple settings and compare them with the existing models. Method: We set up different studies to evaluate the performance and time efficiency of PLMs compared to Prior approaches on six datasets: binary vs. multi-class, zero-shot, multi-task, and multi-resource settings. In addition, we train and study domain-specific (Custom) PLMs by incorporating app reviews in the pre-training. We report Micro and Macro Precision, Recall, and F1 scores and the time required for training and predicting with the models. Results: Our results show that PLMs can classify the app issues with higher scores, except in multi-resource setting. On the largest dataset, results are improved by 13 and 8 micro- and macro-average F1-scores, respectively, compared to the Prior approaches. Domain-specific PLMs achieve the highest scores in all settings with less prediction time, and they benefit from pre-training with a larger number of app reviews. On the largest dataset, we obtain 98 and 92 micro- and macro-average F1-score (from 4.5 to 8.3 more F1-score compared to general pre-trained models), 71 F1-score in zero-shot setting, and 93 and 92 F1-score in multi-task and multi-resource settings, respectively, using the large domain-specific PLMs. Conclusion: Although prior approaches achieve high scores in some settings, PLMs are the only models that can work well in the zero-shot setting. When trained on the app review dataset, the Custom PLMs have higher performance and lower prediction times. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
"
10.1097/CORR.0000000000002513,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85158129726&origin=inward,Article,SCOPUS_ID:85158129726,scopus,2023-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),what are orthopaedic patients' and clinical team members' perspectives regarding whether and how to address mental health in the orthopaedic care setting? a qualitative investigation of patients with neck or back pain,"
AbstractView references

BackgroundAcross virtually all orthopaedic subspecialties, symptoms of depression, anxiety, and unhelpful thinking are associated with worse patient-reported satisfaction with orthopaedic treatment and increased postoperative complications. In the orthopaedic community, there is growing interest in patients' mental health in the orthopaedic care setting, but addressing mental health is still not a focus of orthopaedic clinical training. There is a persistent awareness gap about how to address mental health in orthopaedic care in a manner that is simultaneously feasible in a busy orthopaedic practice and acceptable to patients who are presenting for treatment of a musculoskeletal condition.Questions/purposes(1) What are orthopaedic patients' and clinical team members' current perceptions and motivators regarding addressing mental health as part of orthopaedic care? (2) What barriers do patients and clinicians face regarding addressing mental health as part of orthopaedic care? (3) What are facilitators for patients and clinicians related to addressing mental health as part of orthopaedic care? (4) What are practical, acceptable implementation strategies to facilitate addressing mental health as part of orthopaedic care?MethodsThis was a single-center, qualitative study conducted from January through May 2022 in the orthopaedic department of a large, urban, tertiary care academic medical center. Semistructured interviews were conducted with members of two stakeholder groups: orthopaedic patients and orthopaedic clinical team members. We interviewed 30 adult patients (of 85 patients who were eligible and approached) who had presented to our orthopaedic department for management of neck or back pain lasting for 3 or more months. By prescreening clinic schedules, patients were purposively sampled to include representatives from varied sociodemographic backgrounds and with a range of severity of self-reported symptoms of depression and anxiety (from none to severe on the Patient-Reported Outcomes Measurement Information System Depression and Anxiety measures) (mean age 59 ± 14 years, 70% [21 of 30] women, 60% [18 of 30] White, median pain duration 3.3 [IQR 1.8 to 10] years). We also interviewed 22 orthopaedic clinicians and clinical support staff members (of 106 team members who were eligible and 25 who were approached). Team members were purposively sampled to include representatives from the full range of adult orthopaedic subspecialties and early-, mid-, and late-career physicians (11 of 22 were women, 16 of 22 were White, and 13 of 22 were orthopaedic surgeons). Interviews were conducted in person or via secure video conferencing by trained qualitative researchers. The interview guides were developed using the Capability, Opportunity, Motivation, Behavior model of behavior change. Two study team members used the interview transcripts for coding and thematic analysis, and interviews with additional participants from each stakeholder group continued until two study team members independently determined that thematic saturation of the components of the Capability, Opportunity, Motivation, Behavior model had been reached. Each participant statement was coded as a perception, motivator, barrier, facilitator, or implementation strategy, and inductive coding was used to identify themes in each category.ResultsIn contrast to the perceptions of some orthopaedic clinicians, most patients with orthopaedic conditions expressed they would like their mental well-being to be acknowledged, if not addressed, as part of a thoughtful orthopaedic care plan. Motivation to address mental health was expressed the most strongly among orthopaedic clinical team members who were aware of high-quality evidence that demonstrated a negative impact of symptoms of depression and anxiety on metrics for which they are publicly monitored or those who perceived that addressing patients' mental health would improve their own quality of life. Barriers described by patients with orthopaedic conditions that were related to addressing mental health in the context of orthopaedic care included clinical team members' use of select stigmatizing words and perceived lack of integration between responses to mental health screening measures and the rest of the orthopaedic care encounter. Orthopaedic clinical team members commonly cited the following barriers: lack of available mental health resources they can refer patients to, uncertainty regarding the appropriateness for them to discuss mental health, and time pressure and lack of expertise or comfort in discussing mental health. Facilitators identified by orthopaedic clinical teams and patients to address mental health in the context of orthopaedic care included the development of efficient, adaptable processes to deliver mental health interventions that preferably avoid wasted paper resources; initiation of mental health-related discussion by an orthopaedic clinical team member in a compassionate, relevant context after rapport with the patient has been established; and the availability of a variety of affordable, accessible mental health interventions to meet patients' varied needs and preferences. Practical implementation strategies identified as suitable in the orthopaedic setting to increase appropriate attention to patients' mental health included training orthopaedic clinical teams, establishing a department or institution ""mental health champion,"" and integrating an automated screening question into clinical workflow to assess patients' interest in receiving mental health-related information.ConclusionOrthopaedic patients want their mental health to be acknowledged as part of a holistic orthopaedic care plan. Although organization-wide initiatives can address mental health systematically, a key facilitator to success is for orthopaedic clinicians to initiate compassionate, even if brief, conversations with their patients regarding the interconnectedness of mental health and musculoskeletal health. Given the unique challenges to addressing mental health in the orthopaedic care setting, additional research should consider use of a hybrid effectiveness-implementation design to identify effective methods of addressing mental health that are feasible and appropriate for this clinical setting.Clinical RelevanceOrthopaedic clinicians who have had negative experiences attempting to address mental health with their patients should be encouraged to keep trying. Our results suggest they should feel empowered that most patients want to address mental health in the orthopaedic care setting, and even brief conversations using nonstigmatizing language can be a valuable component of an orthopaedic treatment plan. © 2023 Lippincott Williams and Wilkins. All rights reserved.
"
10.1016/j.infsof.2023.107202,S0950584923000563,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151011958&origin=inward,Article,SCOPUS_ID:85151011958,scopus,2023-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),zero-shot learning for requirements classification: an exploratory study,"Context: Requirements engineering (RE) researchers have been experimenting with machine learning (ML) and deep learning (DL) approaches for a range of RE tasks, such as requirements classification, requirements tracing, ambiguity detection, and modelling. However, most of today’s ML/DL approaches are based on supervised learning techniques, meaning that they need to be trained using a large amount of task-specific labelled training data. This constraint poses an enormous challenge to RE researchers, as the lack of labelled data makes it difficult for them to fully exploit the benefit of advanced ML/DL technologies. Objective: This paper addresses this problem by showing how a zero-shot learning (ZSL) approach can be used for requirements classification without using any labelled training data. We focus on the classification task because many RE tasks can be framed as classification problems. Methods: The ZSL approach used in our study employs contextual word-embeddings and transformer-based language models (LMs). We demonstrate this approach through a series of experiments to perform three classification tasks: (1) FR/NFR — classification functional requirements vs non-functional requirements; (2) NFR — identification of NFR classes; (3) Security — classification of security vs non-security requirements. Results: The study shows that the ZSL approach achieves an F1 score of 0.66 for the FR/NFR task. For the NFR task, the approach yields F1 ∼ 0 . 72 − 0 . 80 , considering the most frequent classes. For the Security task, F1 ∼ 0 . 66 . All of the aforementioned F1 scores are achieved with zero-training efforts. Conclusion: This study demonstrates the potential of ZSL for requirements classification. An important implication is that it is possible to have very little or no training data to perform classification tasks. The proposed approach thus contributes to the solution of the long-standing problem of data shortage in RE."
10.1109/TAFFC.2022.3155105,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125701409&origin=inward,Article,SCOPUS_ID:85125701409,scopus,2023-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"i enjoy writing and playing, do you?: a personalized and emotion grounded dialogue agent using generative adversarial network","
AbstractView references

Social chatbots have gained immense popularity, and their appeal lies in their capacity to respond to diverse requests, but also in their ability to develop an emotional connection with users. To develop and promote social chatbots, we need to concentrate on increasing user interaction and consider both the intellectual and emotional quotient in conversational agents. In this work, we propose the task of empathetic, personalized dialogue generation giving the machine the capability to respond emotionally and in accordance with the persona of the user. We design a generative adversarial framework, named EP-GAN (Empathy and Persona aware Generative Adversarial Network) to generate responses that are sensitive to the emotion of the user and corresponds to the persona. The persona information is encoded as memory vectors that, along with the dialogue history, are fed to the decoder for generation. An interactive adversarial learning framework is implemented to verify whether the generated responses elicit the emotion and persona in dialogues. Experimental results show that the EP-GAN framework significantly outperforms the existing baselines for both automatic and manual evaluation. We achieve an improvement of 1 point BLEU-4 and 2 points in the emotion accuracy metric compared to the best performing baseline. © 2010-2012 IEEE.
"
10.1145/3587102.3588794,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166250328&origin=inward,Conference Paper,SCOPUS_ID:85166250328,scopus,2023-06-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"chatgpt, can you generate solutions for my coding exercises? an evaluation on its effectiveness in an undergraduate java programming course.","
AbstractView references

In this study, we assess the efficacy of employing the ChatGPT language model to generate solutions for coding exercises within an undergraduate Java programming course. ChatGPT, a large-scale, deep learning-driven natural language processing model, is capable of producing programming code based on textual input. Our evaluation involves analyzing ChatGPT-generated solutions for 80 diverse programming exercises and comparing them to the correct solutions. Our findings indicate that ChatGPT accurately generates Java programming solutions, which are characterized by high readability and well-structured organization. Additionally, the model can produce alternative, memory-efficient solutions. However, as a natural language processing model, ChatGPT struggles with coding exercises containing non-textual descriptions or class files, leading to invalid solutions. In conclusion, ChatGPT holds potential as a valuable tool for students seeking to overcome programming challenges and explore alternative approaches to solving coding problems. By understanding its limitations, educators can design coding exercises that minimize the potential for misuse as a cheating aid while maintaining their validity as assessment tools. © 2023 ACM.
"
10.1145/3587102.3588814,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166249787&origin=inward,Conference Paper,SCOPUS_ID:85166249787,scopus,2023-06-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),gpt-3 vs object oriented programming assignments: an experience report,"
AbstractView references

Recent studies show that AI-driven code generation tools, such as Large Language Models, are able to solve most of the problems usually presented in introductory programming classes. However, it is still unknown how they cope with Object Oriented Programming assignments, where the students are asked to design and implement several interrelated classes (either by composition or inheritance) that follow a set of best-practices. Since the majority of the exercises in these tools' training dataset are written in English, it is also unclear how well they function with exercises published in other languages. In this paper, we report our experience using GPT-3 to solve 6 real-world tasks used in an Object Oriented Programming course at a Portuguese University and written in Portuguese. Our observations, based on an objective evaluation of the code, performed by an open-source Automatic Assessment Tool, show that GPT-3 is able to interpret and handle direct functional requirements, however it tends not to give the best solution in terms of object oriented design. We perform a qualitative analysis of GPT-3's output, and gather a set of recommendations for computer science educators, since we expect students to use and abuse this tool in their academic work. © 2023 Owner/Author.
"
10.1145/3587102.3588815,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166239568&origin=inward,Conference Paper,SCOPUS_ID:85166239568,scopus,2023-06-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),how chatgpt will change software engineering education,"
AbstractView references

This position paper discusses the potential for using generative AIs like ChatGPT in software engineering education. Currently, discussions center around potential threats emerging from student's use of ChatGPT. For instance, generative AI will limit the usefulness of graded homework dramatically. However, there exist potential opportunities as well. For example, ChatGPT's ability to understand and generate human language allows providing personalized feedback to students, and can thus accompany current software engineering education approaches. This paper highlights the potential for enhancing software engineering education. The availability of generative AI will improve the individualization of education approaches. In addition, we discuss the need to adapt software engineering curricula to the changed profiles of software engineers. Moreover, we point out why it is important to provide guidance for using generative AI and, thus, integrate it in courses rather than accepting the unsupervised use by students, which can negatively impact the students' learning. © 2023 ACM.
"
10.1145/3587102.3588792,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150820114&origin=inward,Conference Paper,SCOPUS_ID:85150820114,scopus,2023-06-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),can generative pre-trained transformers (gpt) pass assessments in higher education programming courses?,
10.1063/5.0140500,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176799474&origin=inward,Conference Paper,SCOPUS_ID:85176799474,scopus,2023-06-28,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"art of teaching mathematical calculus for engineers, scientists, analysts and researchers","
AbstractView references

The art of teaching mathematics is often encountered with challenges in preparing up to date and appropriate materials with certain real cases for students as the pupils who later in the next semester will be given the opts to choose from various job interests, such as engineers, scientists, analysts, and the others, all of whom also demand to be a researcher indirectly. The problem faced is that if the teaching is too focused purely on mathematics, but the students are apparently from the informatics engineering department, the materials will be inappropriate with their application in real cases, and vice versa. In this study, several complete materials that call ""Academia Smart Art for Teaching Mathematics (ASAFTM)""or ""meta-STREAM""based Particle Swarm Optimization (PSO) algorithm as prototype meta-Deep AI Engine for improve learning process and outcome of any education level were presented as a solution to answer these problems, i.e., by providing teaching materials starting from the Development of Pure Mathematical Concepts, for example related to one of the materials, i.e., Integral-based Generative Modeling for approaches to the calculation of Areas and Volumes of Semi-Continuous Areas, the development of mathematics for Business Start-up Ideas, and Real Case Simulation, especially the ones related to current cases, for example regarding the prediction of the number of recovered Covid-19 patients using matrix-derived techniques, all of which are covered with implementations in the form of the Python programming language until they are given certain open problems as challenges. These open problems can later be included as the closing material. From the results of the application of several materials with teaching strategies using the art of teaching mathematics, from the student evaluation abled to explore their creative ideas as a solver for real cases in society and cases that became national and international issues with science significantly relevant with the course materials in semester 5 or above when they have started to choose courses based on their interest that are also in line with the independent learning curriculum and even after they graduate from college. Therefore, the materials truly educate students' competence to create solutions for the world even though they use mathematical calculus that to date merely becomes a very basic material. © 2023 Author(s).
"
10.1145/3596454.3597176,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168335496&origin=inward,Conference Paper,SCOPUS_ID:85168335496,scopus,2023-06-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),speeding up the engineering of interactive systems with generative ai,"
AbstractView references

This keynote discusses the opportunities and challenges of using Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) as tools for developing interactive systems. We will look at different stages in the development lifecycle of interactive systems and assess the value of AI support. We explore how GenAI and LLMs can potentially speed-up the ideation, requirements elicitation, architecture development, prototyping, implementation, and testing of interactive systems. The talk will outline emerging practices, such as the use of prompts for code and system generation, to facilitate prototyping and accelerate implementation. We will outline fundamental challenges and suggest emerging research directions, and pose research questions. What will software development tools look like in the future? How can we efficiently use AI to develop interactive systems without compromising quality? We also speculate about the implications of these developments for researchers, practitioners, and society. We believe that it will massively accelerate the digital transformation. Interactive AI-based tools for systems and software development will become a major research direction. © 2023 Owner/Author.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85148752960&origin=inward,Conference Paper,SCOPUS_ID:85148752960,scopus,2023-06-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),repair is nearly generation: multilingual program repair with llms,"
AbstractView references

Most programmers make mistakes when writing code. Some of these mistakes are small and require few edits to the original program – a class of errors recently termed last mile mistakes. These errors break the flow for experienced developers and can stump novice programmers. Existing automated repair techniques targeting this class of errors are language-specific and do not easily carry over to new languages. Transferring symbolic approaches requires substantial engineering and neural approaches require data and retraining. We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex. Such a multilingual engine enables a flipped model for programming assistance, one where the programmer writes code and the AI assistance suggests fixes, compared to traditional code suggestion technology. Taking inspiration from the way programmers manually fix bugs, we show that a prompt-based strategy that conceptualizes repair as localization, transformation, and candidate ranking, can successfully repair programs in multiple languages with minimal effort. We present the first results for such a multilingual repair engine by evaluating on 6 different languages and comparing performance to language-specific repair engines. We show that RING can outperform language-specific repair engines for three of these languages. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172169909&origin=inward,Conference Paper,SCOPUS_ID:85172169909,scopus,2023-06-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),is natural language processing effective in education research? a case study in student perceptions of ta support,"
AbstractView references

Natural language processing (NLP) techniques are widely used in linguistic analysis and have shown promising results in areas such as text summarization, text classification, autocorrection, chatbot conversation management, and many other applications. In education, NLP has primarily been applied to automated essay or open-ended question grading, semantic evaluation of student work, or the generation of feedback for intelligent tutoring-based student interaction. However, what is notably missing from NLP work to date is a robust automated framework for accurately analyzing text-based educational survey data. To address this gap, this case study uses NLP models to generate codes for thematic analysis of student needs for teaching assistant (TA) support and then compares code assignments for NLP vs. those assigned by an expert researcher. Student responses to short answer questions regarding preferences for TA support were collected from an instructional support survey conducted in a broad range of electrical, computer, and mechanical engineering courses between 2016-2021 in engineering (N>1400) at a large public research institution. The resulting dataset was randomly split into training (60%), validation (20%), and test set (20%). A popular NLP topic modeling approach (Latent Dirichlet Allocation-LDA) was applied to the training dataset, which determined the optimal number of topics of code represented in the dataset to be four. These four topics were labeled as: (1) examples, where students expressed a need for TAs to illustrate additional problem-solving and applied content in engineering courses; (2) questions and answers, where students desired more opportunities to pose questions to TAs and obtain timely answers to those questions; (3) office hours, encompassing additional availability outside of formally scheduled class times; and (4) lab support. For the testing and validation datasets, an experienced researcher then used these four labels as codes to identify the ground truth for each student's response. Ground truth was then compared to NLP model predictions to gauge the accuracy of the model. For the validation dataset, the accuracy with which NLP identified each response as containing or not containing each code ranged from 79.4% to 91.1%, while for the testing dataset, such accuracies ranged from 81.1 to 92.2%. The codes identified by NLP were then combined into themes by a human researcher, resulting in three themes (problem-solving, interactions, and active/experiential learning). Conclusions reached regarding the three themes were identical whether the NLP codes or (human) researcher codes were used for data interpretation. Short-answer questions, despite their value in providing deeper insight into the student experience, are infrequently used in educational research because the resulting data often requires prohibitive human resources to analyze. This study has demonstrated, in a case study of student preferences for TA support, the value of NLP in understanding large numbers of textual, short-answer responses from students. The fact that NLP models can deliver the same bottom line in minutes compared to the hours that traditional thematic analysis methods consume is promising for expanding the use of more nuanced, richer text-based data in survey-based education research. © American Society for Engineering Education, 2023.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172160559&origin=inward,Conference Paper,SCOPUS_ID:85172160559,scopus,2023-06-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),transforming engineering education for neurodiversity: epistemic communities as a model for change,"
AbstractView references

A growing body of literature suggests that neurodiverse learners, including students with autism, ADHD, and dyslexia, may possess strengths that are highly desirable within engineering disciplines, such as systems thinking, creativity, and 3D visualization skills. However, despite the potential of neurodiverse individuals to leverage these assets to contribute to innovative solutions to engineering problems, they remain highly underrepresented in engineering majors. With this in mind, a department-level initiative was established to radically transform the educational experiences of neurodiverse students by moving beyond academic accommodation of learning differences to empowering students to leverage their unique strengths in engineering. In undertaking this transformation, an epistemic communities model was adopted and implemented as infrastructure for change as part of a National Science Foundation Revolutionizing Engineering Departments (NSF:RED) grant within the context of a Civil and Environmental Engineering department at a large, research intensive (R1) institution. Epistemic communities unite members in a shared purpose through the establishment and transmission of shared values and practices, allowing stakeholders to build community from within and sustain lasting change. Through our epistemic community, we aim to make local change within the department, but we also hope to contribute to a broader paradigm shift that transforms how university faculty and staff understand and perceive neurodiversity, a key lever for enhancing the educational experiences of neurodiverse students. This conceptual paper presents an overview of these departmental transformation efforts, with a focus on the shared theory, code, and tools around which our epistemic community is constructed. First, we present a social ecology theoretical framework (theory) that challenges the deficit-based approach that has historically shaped neurodiverse learners' experiences by emphasizing learners' assets and their potential of a neurodiverse student body to contribute to innovation for the benefit of society. Second, we discuss the infusion of strengths-based language (code) related to neurodiversity and its role in contributing to a collective mind shift across the department. Third, we present a discussion of the practices, structures, and artefacts (tools), such as shared standards for course revision, that were established and co-created by community members to facilitate departmental change. © American Society for Engineering Education, 2023.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172098923&origin=inward,Conference Paper,SCOPUS_ID:85172098923,scopus,2023-06-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),incorporating a teach for mastery system in a computer science course,"
AbstractView references

Teaching for Mastery is the idea that students should progress through material when they have conquered the previous material it depends on and each student may progress at their own pace. The paper documents the process and results of incorporating a teach for mastery system into a Computer Science Programming Language Design course. The course is aimed at junior and senior undergraduate students and its goal is to introduce them to the four main language families and to show them how to teach themselves a programming language. This teach for mastery system caused both student learning and student satisfaction to improve in a course that was already well received before these changes. Other benefits of this system include a marked decrease in the amount of time spent grading and a straightforward way to identify which subjects need to be re-thought and which are being learned to the intended depth. The system does come with an increase in the amount of contact time spent with students which is both a positive in that you develop a closer relationship with the students and a negative in that it is a large amount of contact time. Contributions include the structural changes and teaching artifacts needed to implement teaching for mastery in a course. The structural changes include flipping the classroom and replacing traditional sit-down exams with short one-topic take-when-ready exams. The teaching artifacts produced include a concept map. Students leave the course with a clear list of which topics they mastered and which they are still working on. This model still permits room for traditional laboratory and project components. © American Society for Engineering Education, 2023.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172091646&origin=inward,Conference Paper,SCOPUS_ID:85172091646,scopus,2023-06-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),pushing ethics assessment forward in engineering: nlp-assisted qualitative coding of student responses,"
AbstractView references

Recent headlines have featured large language models (LLMs), like ChatGPT, for their potential impacts throughout society. These headlines often focus on educational impacts and policies. We posit that LLMs have the potential to improve instructional approaches in engineering education. Thus, we argue that as an engineering education community, we should aim to leverage LLMs to help resolve challenges in engineering education. This study takes up one aspect of instructional design: valid assessment of students' learning outcomes in engineering ethics. In this study, we present a method for engineering educators to implement NLP in open-ended ethics assessments (here, written responses to an ethics case scenario). Grading such open-ended responses has challenges: it requires a non-trivial time commitment and attention to consistency. To mitigate these challenges, we developed an NLP approach based on open-source, transformer-based LLMs. We applied and evaluated our NLP approach for coding students' responses to an open-ended ethics case scenario in a first-year engineering course. The results showed that our NLP approach labeled 380 out of 472 sentences accurately. Conversely, only 8% (37 out of 472 responses) were inaccurately labeled. Overall, our NLP approach provides a step toward analyzing written responses to scenario-based assessments in a scalable manner. However, it is not perfect. One current downside of our NLP approach is that it requires a large upfront time investment in setting up the system. Our future work aims to lower that barrier to entry, thereby making it more accessible to a larger group of potential users. © American Society for Engineering Education, 2023.
"
10.3233/FAIA230103,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171448240&origin=inward,Conference Paper,SCOPUS_ID:85171448240,scopus,2023-06-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards simple hybrid language model reasoning through human explanations enhanced prompts,"
AbstractView references

Large Pre-Trained Models (LLMs) have reached state-of-the-art performance in various Ntural Language Processing (NLP) application tasks. However, an issue remains these models may confidently output incorrect answers, flawed reasoning, or even entirely hallucinate answers. Truly integrating human feedback and corrections is difficult for LLMs, as the traditional approach of fine-tuning is challenging and compute-intensive for LLMs, and the weights for the best models are often not publicly available. However, the ability to interact with these models in natural language opens up new possibilities for Hybrid AI. In this work, we present a very early exploration of Human-Explanations-Enhanced Prompting (HEEP), an approach that aims to help LLMs learn from human annotators' input by storing corrected reasonings and retrieving them on the fly to integrate them into prompts given to the model. Our preliminary results support the idea that HEEP could represent an initial step towards cheap alternatives to fine-tuning and developing human-in-the-loop classification methods at scale, encouraging more efficient interactions between human annotators and LLMs. © 2023 The Authors.
"
10.1145/3591196.3596818,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164038861&origin=inward,Conference Paper,SCOPUS_ID:85164038861,scopus,2023-06-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),causalmapper: challenging designers to think in systems with causal maps and large language model,"
AbstractView references

Professional designers often construct and explore conceptual representations (e.g.: design spaces) to help them reason about complex design situations and consider potential design pitfalls. However, it is often challenging, even for professional designers, to exhaustively consider the many pitfalls that might result from design activity. We present CausalMapper, a mixed-initiative system, that leverages a large language model (LLM) and a causal map representation to teach design students how to reason about the relationships between problems and solutions. Where creativity support tools often focus on ideating creative solutions, our mixed-initiative approach focuses on ideating ecosystems of solutions that holistically address a set of related problems. By leveraging the generative creativity of LLMs, designers are inspired to consider solutions and potential consequences that emerge when solutions are adopted. At the same time, leveraging the designers' domain knowledge to account for and correct the biases inherent in LLMs. Through a case study, we demonstrate the functionality of this mixed-initiative system. The goal of this demo is to present a creativity support tool that is intended to teach design students to think more systematically by generating ideas that challenge their thinking rather just augmenting their creative potential. © 2023 Owner/Author.
"
10.1145/3591196.3596616,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163927128&origin=inward,Conference Paper,SCOPUS_ID:85163927128,scopus,2023-06-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative ai futures: a speculative design exploration,"
AbstractView references

What generative AI futures do we want - and what futures do we not want? To imagine what might exist in the future, we apply speculative design to explore plausible scenarios for generative AI and human coexistence. In this paper, we present gAIrden and Onion AI: two in-progress speculative concepts of future generative AI tools, their use cases, and the systems in which they exist. We analyze the designs through lenses of Environment, Data Privacy, Embodiment, and Play. This trip into the future is driven by the research question: how might generative AI tools change how we produce creativity and culture? When we return to the present, we ask ourselves, how might generative AI support positive outcomes for individuals and communities? Can we predict (and potentially mitigate) negative consequences of generative AI tools? The speculative designs purposefully engage viewers in futures thinking to reclaim conversation around the future of technology. © 2023 Owner/Author.
"
10.1145/3593230,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163743748&origin=inward,Article,SCOPUS_ID:85163743748,scopus,2023-06-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),evaluating a large language model on searching for gui layouts,
10.1126/science.adi1778,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163902220&origin=inward,Article,SCOPUS_ID:85163902220,scopus,2023-06-16,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai and the transformation of social science research,"
AbstractView references

[No abstract available]
"
10.1145/3593434.3593468,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162246873&origin=inward,Conference Paper,SCOPUS_ID:85162246873,scopus,2023-06-14,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards human-bot collaborative software architecting with chatgpt,"
AbstractView references

Architecting software-intensive systems can be a complex process. It deals with the daunting tasks of unifying stakeholders' perspectives, designers' intellect, tool-based automation, pattern-driven reuse, and so on, to sketch a blueprint that guides software implementation and evaluation. Despite its benefits, architecture-centric software engineering (ACSE) suffers from a multitude of challenges. ACSE challenges could stem from a lack of standardized processes, socio-technical limitations, and scarcity of human expertise etc. that can impede the development of existing and emergent classes of software. Software Development Bots (DevBots) trained on large language models can help synergise architects' knowledge with artificially intelligent decision support to enable rapid architecting in a human-bot collaborative ACSE. An emerging solution to enable this collaboration is ChatGPT, a disruptive technology not primarily introduced for software engineering, but is capable of articulating and refining architectural artifacts based on natural language processing. We detail a case study that involves collaboration between a novice software architect and ChatGPT to architect a service-based software. Future research focuses on harnessing empirical evidence about architects' productivity and explores socio-technical aspects of architecting with ChatGPT to tackle challenges of ACSE. © 2023 Owner/Author.
"
10.1145/3591106.3592278,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163625493&origin=inward,Conference Paper,SCOPUS_ID:85163625493,scopus,2023-06-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),framing the news: from human perception to large language model inferences,"
AbstractView references

Identifying the frames of news is important to understand the articles' vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of journalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to understand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No-Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Natural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames. © 2023 ACM.
"
10.1145/3592573.3593104,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163611310&origin=inward,Conference Paper,SCOPUS_ID:85163611310,scopus,2023-06-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),lifelog discovery assistant: suggesting prompts and indexing event sequences for first at lsc 2023,"
AbstractView references

AI-assisted tools have become more prevalent than ever in the last few years. However, applying them to build a lifelog retrieval system is still non-trivial due to the disparity in interfaces and interactions. The Lifelog Search Challenge (LSC) aims to provide a testing ground where systems can be benchmarked in a highly competitive setting. In this paper, we present the fourth iteration of our participating system FIRST. For this year, we adopt generative models to equip the system with predictive ability rather than entirely relying on the user to input the query. We also index a sequence of images as an event for improved search speed. Finally, we demonstrate how the additional features can assist users in searching. © 2023 ACM.
"
10.1016/j.tbench.2023.100115,S2772485923000327,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174544760&origin=inward,Article,SCOPUS_ID:85174544760,scopus,2023-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),unlocking the opportunities through chatgpt tool towards ameliorating the education system,"Artificial Intelligence (AI)-based ChatGPT developed by OpenAI is now widely accepted in several fields, including education. Students can learn about ideas and theories by using this technology while generating content with it. ChatGPT is built on State of the Art (SOA), like Deep Learning (DL), Natural Language Processing (NLP), and Machine Learning (ML), an extrapolation of a class of ML-NLP models known as Large Language Model (LLMs). It may be used to automate test and assignment grading, giving instructors more time to concentrate on instruction. This technology can be utilised to customise learning for kids, enabling them to focus more intently on the subject matter and critical thinking ChatGPT is an excellent tool for language lessons since it can translate text from one language to another. It may provide lists of vocabulary terms and meanings, assisting students in developing their language proficiency with resources. Personalised learning opportunities are one of ChatGPT’s significant applications in the classroom. This might include creating educational resources and content tailored to a student’s unique interests, skills, and learning goals. This paper discusses the need for ChatGPT and the significant features of ChatGPT in the education system. Further, it identifies and discusses the significant applications of ChatGPT in education. Using ChatGPT, educators may design lessons and instructional materials specific to each student’s requirements and skills based on current trends. Students may work at their speed and concentrate on the areas where they need the most support, resulting in a more effective and efficient learning environment. Both instructors and students may profit significantly from using ChatGPT in the classroom. Instructors may save time on numerous duties by using this technology. In future, ChatGPT will become a powerful tool for enhancing students’ and teachers’ experience."
10.1162/dint_a_00210,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173459894&origin=inward,Article,SCOPUS_ID:85173459894,scopus,2023-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),knowledge graph based mutual attention for machine reading comprehension over anti-terrorism corpus,"
AbstractView references

Machine reading comprehension has been a research focus in natural language processing and intelligence engineering. However, there is a lack of models and datasets for the MRC tasks in the anti-terrorism domain. Moreover, current research lacks the ability to embed accurate background knowledge and provide precise answers. To address these two problems, this paper first builds a text corpus and testbed that focuses on the anti-terrorism domain in a semi-automatic manner. Then, it proposes a knowledge-based machine reading comprehension model that fuses domain-related triples from a large-scale encyclopedic knowledge base to enhance the semantics of the text. To eliminate knowledge noise that could lead to semantic deviation, this paper uses a mixed mutual attention mechanism among questions, passages, and knowledge triples to select the most relevant triples before embedding their semantics into the sentences. Experiment results indicate that the proposed approach can achieve a 70.70% EM value and an 87.91% F1 score, with a 4.23% and 3.35% improvement over existing methods, respectively. © 2023 Chinese Academy of Sciences.
"
10.1148/radiol.230582,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163896892&origin=inward,Article,SCOPUS_ID:85163896892,scopus,2023-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),performance of chatgpt on a radiology board-style examination: insights into current strengths and limitations,"
AbstractView references

Background: ChatGPT is a powerful artificial intelligence large language model with great potential as a tool in medical practice and education, but its performance in radiology remains unclear. Purpose: To assess the performance of ChatGPT on radiology board–style examination questions without images and to explore its strengths and limitations. Materials and Methods: In this exploratory prospective study performed from February 25 to March 3, 2023, 150 multiple-choice questions designed to match the style, content, and difficulty of the Canadian Royal College and American Board of Radiology examinations were grouped by question type (lower-order [recall, understanding] and higher-order [apply, analyze, synthesize] thinking) and topic (physics, clinical). The higher-order thinking questions were further subclassified by type (description of imaging findings, clinical management, application of concepts, calculation and classification, disease associations). ChatGPT performance was evaluated overall, by question type, and by topic. Confidence of language in responses was assessed. Univariable analysis was performed. Results: ChatGPT answered 69% of questions correctly (104 of 150). The model performed better on questions requiring lower-order thinking (84%, 51 of 61) than on those requiring higher-order thinking (60%, 53 of 89) (P = .002). When compared with lower-order questions, the model performed worse on questions involving description of imaging findings (61%, 28 of 46; P = .04), calculation and classification (25%, two of eight; P = .01), and application of concepts (30%, three of 10; P = .01). ChatGPT performed as well on higher-order clinical management questions (89%, 16 of 18) as on lower-order questions (P = .88). It performed worse on physics questions (40%, six of 15) than on clinical questions (73%, 98 of 135) (P = .02). ChatGPT used confident language consistently, even when incorrect (100%, 46 of 46). Conclusion: Despite no radiology-specific pretraining, ChatGPT nearly passed a radiology board–style examination without images; it performed well on lower-order thinking questions and clinical management questions but struggled with higher-order thinking questions involving description of imaging findings, calculation and classification, and application of concepts. © RSNA, 2023.
"
10.3390/rel14060812,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163611115&origin=inward,Article,SCOPUS_ID:85163611115,scopus,2023-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),negative capabilities: investigating apophasis in ai text-to-image models,"
AbstractView references

Through a case study of images generated by Swedish artist Steph Maj Swanson using an AI text-to-image (T2I) model, this article explores the strategy of negative weight prompting in T2I models as a phenomenon of apophasis. Apophasis is a linguistic strategy commonly deployed in texts of mystical theology to express the ineffability of God through negative concepts. In this article, a comparison of apophatic strategies in mystical texts and T2I models is engaged to highlight the mutual benefit of theorising AI with the help of religious theory and concepts. With this, the article builds on previous work on the New Visibility of Religion, enchantment, and post-secularism—especially the research of Beth Singler on religious continuities in representations of AI. Recent work on AI prompt engineering, computational linguistics, and computational geometry is invoked to explain the linguistic processes of T2I models. Poststructuralist semiotics is then employed to theorise the search for the Transcendental Signified in apophatic theology. The article concludes that linguistic theology can help to elucidate technological use cases, subsequently arguing for further dialogue between scholars in artificial intelligence and religious studies, and for a revaluation of religion in the technological sphere. © 2023 by the author.
"
10.1016/j.techfore.2023.122520,S0040162523002056,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151397058&origin=inward,Article,SCOPUS_ID:85151397058,scopus,2023-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),technological forecasting based on estimation of word embedding matrix using lstm networks,"
                  There are a vast number of quantitative and qualitative technological forecasting methods. In the last decade, advanced quantitative technological forecasting methods based on the various applications of data science approaches have been proposed. Text mining is one of the key approaches used to examine large datasets consisting of scientific publications and patent documents with the aim of offering foresight for a selected area. However, the existing related studies either perform a qualitative approach by analysing the recent data to identify the emerging topics or use extrapolation techniques to predict the future values of some statistical terms or the future frequency of some important keywords. In this study, different from such related studies, we propose a deep learning-based framework to predict future co-similarity matrix representing the possible new and disappearing interactions between the words in the future. For this purpose, word vectors are generated using a word embedding technique and the temporal changes of the associations between the words are modelled using Long Short-Term Memory networks for the future estimation of the word embedding matrix. The text mining area is chosen as a case study. The clusters of the terms extracted from the predicted word embedding matrices were analysed and potentially emerging areas were identified for different prediction horizon lengths. The accuracy of the proposed model was analysed based on a set of evaluation metrics that measure the amount of overlapping between the actual and predicted word maps. The quantitative analysis showed that the proposed system can successfully identify the emerging and disappearing areas and can be used as a decision-making tool for the future projection of other areas.
               "
10.1016/j.cola.2023.101200,S2590118423000102,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149057874&origin=inward,Article,SCOPUS_ID:85149057874,scopus,2023-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),api comparison knowledge extraction via prompt-tuned language model,"
                  Application Programming Interfaces (APIs) are frequent in software engineering domain texts, such as API references and Stack Overflow. These APIs and the comparison knowledge between them are not only important for solving programming issues (e.g., question answering), but they are also organized into structured knowledge to support many software engineering tasks (e.g., API misuse detection). As a result, extracting API comparison knowledge (API entities and semantic relations) from texts is essential. Existing rule-based and sequence labeling-based approaches must manually enumerate all linguistic patterns or label a large amount of data. Therefore, they involve a significant labor overhead and are exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, we formulates heterogeneous API extraction and API relation extraction tasks as a sequence-to-sequence generation task. It proposes APICKnow, an API entity-relation joint extraction model based on the large language model. To improve our model’s performance and quick learning ability, we adopt the prompt learning method to stimulate APICKnow to recognize API entities and relations. We systematically evaluate APICKnow on a set of sentences from Stack Overflow. The experimental results show that APICKnow can outperform the state-of-the-art baselines, and APICKnow has a quick learning ability and strong generalization ability.
               "
10.1016/j.future.2023.01.023,S0167739X23000316,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147538409&origin=inward,Article,SCOPUS_ID:85147538409,scopus,2023-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),si4iot: a methodology based on models and services for the integration of iot systems,"
                  The Internet of Things (IoT) is a technology that is growing faster every day due to the large number of platforms and end-devices that are becoming connected to each other. As part of this wide and diverse scenario, developers are now facing various challenges, such as heterogeneity, diversity of communication protocols, discovery of things, and coordination of services, among others. A paradigm that can help to tackle these issues is the model engineering since it allows different elements to be reused which can simplify the work of developers. In this paper, we propose SI4IoT (Service Integration for IoT), a methodology based on MDE (Model-Driven Engineering) for the development of IoT systems. This methodology enables automatic code generation, making it easier for developers to design sophisticated new IoT applications. We focus on a DSL (Domain-Specific Language), a graphic editor, and a set of M2T (Model-to-Text) transformations that generate code for software artifacts on Arduino, Node-Red, Ballerina, and NCL-Lua for deployment on hardware nodes, web services, and DTV (Digital TV). Our proposal consists of a model for the integration of services made up of three layers: physical, logical, and application. To validate our proposal, a Smart Home scenario has been considered, with sensors and actuators which, when combined, allow control of lights and heating. In addition, it allows the user to receive information about their home on television based on the REST services that have been created for the IoT nodes.
               "
10.1002/cjce.24838,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147531594&origin=inward,Article,SCOPUS_ID:85147531594,scopus,2023-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),reconciling deep learning and first-principle modelling for the investigation of transport phenomena in chemical engineering,"
AbstractView references

The use of machine learning in chemical engineering has the potential to greatly improve the design and analysis of complex systems. However, there are also risks associated with its adoption, such as the potential for bias in algorithms and the need for careful oversight to ensure the safety and reliability of machine learning-powered systems. This paper explores the opportunities and risks of using machine learning in chemical engineering and provides a perspective on how it may be integrated into engineering practices in a responsible and effective manner. We generated the text of this abstract with GPT-3, OpenAI's large-scale language-generation model. Upon generating the draft, we ensured that the language was to our liking, and we take ultimate responsibility for the content of this publication. © 2023 Canadian Society for Chemical Engineering.
"
10.1145/3608298.3608365,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178062315&origin=inward,Conference Paper,SCOPUS_ID:85178062315,scopus,2023-05-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using the concept of generative adversarial network (gan) to strengthen the classification performance for x-ray images of small samples,"
AbstractView references

Because the medical technology is quickly developed, how to implement an accurate diagnosis and adopts timely treatment that will seriously influence the patient safety. With the advancement of artificial intelligence (AI), the medical diagnosis support system (MDSS) also plays key role and to increase the diagnostic performance. However, there is a problem that needs to be noted and solved for the MDSS in machine and deep learning. That is, when the data exhibits an imbalance situation and the lack of sufficient medical diagnosis data that will induce an inferior recognition result. Based on such a problem, the main purpose of this study is to construct a classification system so that the abnormal images of chest x-ray in small samples can be distinguished smoothly for pneumothorax. Firstly, the classified system uses the cycle generative adversarial network (CycleGAN) to learn the image features, and then forms the virtual pneumothorax images so as to raise the abnormal samples. When the virtual samples are generated completely, the system will apply the convolutional neural network (CNN) model to execute the training and discriminating tasks. Using the expanded sample cluster, the system can perform the training procedure and the real testing data is also used to evaluate the classified accuracy. The experiment studies showed that the expanded classification system is able to obtain a superior performance than the simple CNN classifier. At last, an experimental case for pneumothorax is used to illustrate the proposed system for verifying the feasibility of using our analysis procedure. © 2023 ACM.
"
10.1145/3593342.3593360,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169107192&origin=inward,Conference Paper,SCOPUS_ID:85169107192,scopus,2023-05-04,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring chatgpt's impact on post-secondary education: a qualitative study,"
AbstractView references

As Chat Generative Pre-trained Transformer (ChatGPT) gains traction, its impact on post-secondary education is increasingly being debated. This qualitative study explores the perception of students and faculty members at a research university in Canada regarding ChatGPT's use in a post-secondary setting, focusing on how it could be incorporated and what ways instructors can respond to this technology. We present the summary of a discussion that took place in a two-hour focus group session with 40 participants from the computer science and engineering departments, and highlight issues surrounding plagiarism, assessment methods, and the appropriate use of ChatGPT. Findings suggest that students are likely to use ChatGPT, but there is a need for specific guidelines, more classroom assessments, and mandatory reporting of ChatGPT use. The study contributes to the emergent research on ChatGPT in higher education and emphasizes the importance of proactively addressing challenges and opportunities associated with ChatGPT adoption and use. © 2023 ACM.
"
10.1145/3569934,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162059219&origin=inward,Article,SCOPUS_ID:85162059219,scopus,2023-05-03,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),what is the intended usage context of this model? an exploratory study of pre-trained models on various model repositories,"
AbstractView references

There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse SE could be extended to the area of pre-trained model reuse. Therefore, according to the model card's and FactSheet's guidance for suppliers of pre-trained models on what information they should be published, we propose model contracts including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the TensorFlow Hub, PyTorch Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task's type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories. © 2023 Association for Computing Machinery.
"
10.3390/s23104979,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160452015&origin=inward,Article,SCOPUS_ID:85160452015,scopus,2023-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a framework for cybersecurity requirements management in the automotive domain,"
AbstractView references

The rapid development of intelligent connected vehicles has increased the attack surface of vehicles and made the complexity of vehicle systems unprecedented. Original equipment manufacturers (OEMs) need to accurately represent and identify threats and match corresponding security requirements. Meanwhile, the fast iteration cycle of modern vehicles requires development engineers to quickly obtain cybersecurity requirements for new features in their developed systems in order to develop system code that meets cybersecurity requirements. However, existing threat identification and cybersecurity requirement methods in the automotive domain cannot accurately describe and identify threats for a new feature while also quickly matching appropriate cybersecurity requirements. This article proposes a cybersecurity requirements management system (CRMS) framework to assist OEM security experts in conducting comprehensive automated threat analysis and risk assessment and to help development engineers identify security requirements prior to software development. The proposed CRMS framework enables development engineers to quickly model their systems using the UML-based (i.e., capable of describing systems using UML) Eclipse Modeling Framework and security experts to integrate their security experience into a threat library and security requirement library expressed in Alloy formal language. In order to ensure accurate matching between the two, a middleware communication framework called the component channel messaging and interface (CCMI) framework, specifically designed for the automotive domain, is proposed. The CCMI communication framework enables the fast model of development engineers to match with the formal model of security experts for threat and security requirement matching, achieving accurate and automated threat and risk identification and security requirement matching. To validate our work, we conducted experiments on the proposed framework and compared the results with the HEAVENS approach. The results showed that the proposed framework is superior in terms of threat detection rates and coverage rates of security requirements. Moreover, it also saves analysis time for large and complex systems, and the cost-saving effect becomes more pronounced with increasing system complexity. © 2023 by the authors.
"
10.3390/s23104879,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160422259&origin=inward,Article,SCOPUS_ID:85160422259,scopus,2023-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),can chatgpt help in electronics research and development? a case study with applied sensors,"
AbstractView references

In this paper, we investigated the applicability of ChatGPT AI in electronics research and development via a case study of applied sensors in embedded electronic systems, a topic that is rarely mentioned in the recent literature, thus providing new insight for professionals and academics. The initial electronics-development tasks of a smart home project were prompted to the ChatGPT system to find out its capabilities and limitations. We wanted to obtain detailed information on the central processing controller units and the actual sensors usable for the specific project, their specifications and recommendations on the hardware and software design flow additionally. Furthermore, an extensive literature survey was requested to see if the bot could offer scientific papers covering the given topic. It was found that the ChatGPT responded with proper recommendations on controllers. However, the suggested sensor units, the hardware and software design were only partially acceptable, with occasional errors in specifications and generated code. The results of the literature survey showed that non-acceptable, fabricated citations (fake authors list, title, journal details and DOI—Digital Object identifier) were presented by the bot. The paper provides a detailed qualitative analysis, a performance analysis and critical discussion of the aforementioned aspects while providing the query set, the generated answers and codes as supplied data with the goal to give added value to electronics researchers and developers if trying to reach out for the tools in their profession. © 2023 by the authors.
"
10.3390/app13095783,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159258213&origin=inward,Article,SCOPUS_ID:85159258213,scopus,2023-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"chatgpt for education and research: opportunities, threats, and strategies","
AbstractView references

In recent years, the rise of advanced artificial intelligence technologies has had a profound impact on many fields, including education and research. One such technology is ChatGPT, a powerful large language model developed by OpenAI. This technology offers exciting opportunities for students and educators, including personalized feedback, increased accessibility, interactive conversations, lesson preparation, evaluation, and new ways to teach complex concepts. However, ChatGPT poses different threats to the traditional education and research system, including the possibility of cheating on online exams, human-like text generation, diminished critical thinking skills, and difficulties in evaluating information generated by ChatGPT. This study explores the potential opportunities and threats that ChatGPT poses to overall education from the perspective of students and educators. Furthermore, for programming learning, we explore how ChatGPT helps students improve their programming skills. To demonstrate this, we conducted different coding-related experiments with ChatGPT, including code generation from problem descriptions, pseudocode generation of algorithms from texts, and code correction. The generated codes are validated with an online judge system to evaluate their accuracy. In addition, we conducted several surveys with students and teachers to find out how ChatGPT supports programming learning and teaching. Finally, we present the survey results and analysis. © 2023 by the authors.
"
10.1016/j.sysarc.2023.102857,S138376212300036X,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151396612&origin=inward,Article,SCOPUS_ID:85151396612,scopus,2023-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),x-stream: accelerating streaming segments on mpsocs for real-time applications,"
                  We are witnessing a race to meet the ever-growing computation requirements of emerging AI applications to provide perception and control in autonomous vehicles — e.g., self-driving cars and UAVs. To remain competitive, vendors are packing more processing units (CPUs, programmable logic, GPUs, and hardware accelerators) into next-generation multiprocessor systems-on-a-chip (MPSoC). As a result, modern embedded platforms are achieving new heights in peak computational capacity. Unfortunately, however, the collateral and inevitable increase in complexity represents a major obstacle for the development of correct-by-design safety-critical real-time applications. Due to the ever-growing gap between fast-paced hardware evolution and comparatively slower evolution of real-time operating systems (RTOS), there is a need for real-time oriented full-platform management frameworks to complement traditional RTOS designs.
                  In this work, we propose one such framework, namely the X-Stream framework, for the definition, synthesis, and analysis of real-time workloads targeting state-of-the-art accelerator-augmented embedded platforms. Our X-Stream framework is designed around two cardinal principles. First, computation and data movements are orchestrated to achieve predictability by design. For this purpose, iterative computation over large data chunks is divided into subsequent segments. These segments are then streamed leveraging the three-phase execution model (load, execute and unload). Second, the framework is workflow-centric: system designers can specify their workflow and the necessary code for workflow orchestration is automatically generated.
                  In addition to automating the deployment of user-defined hardware-accelerated workloads, X-Stream supports the deployment of some computation segments on traditional CPUs. Finally, X-Stream allows the definition of real-time partitions. Each partition groups applications belonging to the same criticality level and that share the same set of hardware resources, with support for preemptive priority-driven scheduling. Conversely, freedom from interference for applications deployed in different partitions is guaranteed by design. We provide a full-system implementation that includes RTOS integration and showcase the proposed X-Stream framework on a Xilinx Ultrascale+ platform by focusing on a matrix-multiplication and addition kernel use-case.
               "
10.1007/s13042-022-01735-z,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146986808&origin=inward,Article,SCOPUS_ID:85146986808,scopus,2023-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),multi-stage transfer learning with bertology-based language models for question answering system in vietnamese,"
AbstractView references

With the fast growth of information science and engineering, a large number of textual data generated are valuable for natural language processing and its applications. Particularly, finding correct answers to natural language questions or queries requires spending tremendous time and effort in human life. While using search engines to discover information, users manually determine the answer to a given question on a range of retrieved texts or documents. Question answering relies heavily on the capability to automatically comprehend questions in human language and extract meaningful answers from a single text. In recent years, such question–answering systems have become increasingly popular using machine reading comprehension techniques. On the other hand, high-resource languages (e.g., English and Chinese) have witnessed tremendous growth in question-answering methodologies based on various knowledge sources. Besides, powerful BERTology-based language models only encode texts with a limited length. The longer texts contain more distractor sentences that affect the QA system performance. Vietnamese has a variety of question words in the same question type. To address these challenges, we propose ViQAS, a new question–answering system with multi-stage transfer learning using language models based on BERTology for a low-resource language such as Vietnamese. Last but not least, our QA system is integrated with Vietnamese characteristics and transformer-based evidence extraction techniques into an effective contextualized language model-based QA system. As a result, our proposed system outperforms our forty retriever-reader QA configurations and seven state-of-the-art QA systems such as DrQA, BERTserini, BERTBM25, XLMRQA, ORQA, COBERT, and NeuralQA on three Vietnamese benchmark question answering datasets. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.1145/3543873.3587617,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159640436&origin=inward,Conference Paper,SCOPUS_ID:85159640436,scopus,2023-04-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated ontology evaluation: evaluating coverage and correctness using a domain corpus,"
AbstractView references

Ontologies conceptualize domains and are a crucial part of web semantics and information systems. However, re-using an existing ontology for a new task requires a detailed evaluation of the candidate ontology as it may cover only a subset of the domain concepts, contain information that is redundant or misleading, and have inaccurate relations and hierarchies between concepts. Manual evaluation of large and complex ontologies is a tedious task. Thus, a few approaches have been proposed for automated evaluation, ranging from concept coverage to ontology generation from a corpus. Existing approaches, however, are limited by their dependence on external structured knowledge sources, such as a thesaurus, as well as by their inability to evaluate semantic relationships. In this paper, we propose a novel framework to automatically evaluate the domain coverage and semantic correctness of existing ontologies based on domain information derived from text. The approach uses a domain-tuned named-entity-recognition model to extract phrasal concepts. The extracted concepts are then used as a representation of the domain against which we evaluate the candidate ontology's concepts. We further employ a domain-tuned language model to determine the semantic correctness of the candidate ontology's relations. We demonstrate our automated approach on several large ontologies from the oceanographic domain and show its agreement with a manual evaluation by domain experts and its superiority over the state-of-the-art. © 2023 Owner/Author.
"
10.1145/3544548.3580999,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160018167&origin=inward,Conference Paper,SCOPUS_ID:85160018167,scopus,2023-04-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring challenges and opportunities to support designers in learning to co-create with ai-based manufacturing design tools,"
AbstractView references

AI-based design tools are proliferating in professional software to assist engineering and industrial designers in complex manufacturing and design tasks. These tools take on more agentic roles than traditional computer-aided design tools and are often portrayed as ""co-creators.""Yet, working effectively with such systems requires different skills than working with complex CAD tools alone. To date, we know little about how engineering designers learn to work with AI-based design tools. In this study, we observed trained designers as they learned to work with two AI-based tools on a realistic design task. We find that designers face many challenges in learning to effectively co-create with current systems, including challenges in understanding and adjusting AI outputs and in communicating their design goals. Based on our findings, we highlight several design opportunities to better support designer-AI co-creation. © 2023 Owner/Author.
"
10.1145/3544548.3581560,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160014095&origin=inward,Conference Paper,SCOPUS_ID:85160014095,scopus,2023-04-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"""the less i type, the better"": how ai language models can enhance or impede communication for aac users","
AbstractView references

Users of augmentative and alternative communication (AAC) devices sometimes find it difficult to communicate in real time with others due to the time it takes to compose messages. AI technologies such as large language models (LLMs) provide an opportunity to support AAC users by improving the quality and variety of text suggestions. However, these technologies may fundamentally change how users interact with AAC devices as users transition from typing their own phrases to prompting and selecting AI-generated phrases. We conducted a study in which 12 AAC users tested live suggestions from a language model across three usage scenarios: extending short replies, answering biographical questions, and requesting assistance. Our study participants believed that AI-generated phrases could save time, physical and cognitive effort when communicating, but felt it was important that these phrases reflect their own communication style and preferences. This work identifies opportunities and challenges for future AI-enhanced AAC devices. © 2023 Owner/Author.
"
10.1145/3544548.3581402,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160012738&origin=inward,Conference Paper,SCOPUS_ID:85160012738,scopus,2023-04-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),reprompt: automatic prompt editing to refine ai-generative art towards precise expressions,"
AbstractView references

Generative AI models have shown impressive ability to produce images with text prompts, which could benefit creativity in visual art creation and self-expression. However, it is unclear how precisely the generated images express contexts and emotions from the input texts. We explored the emotional expressiveness of AI-generated images and developed RePrompt, an automatic method to refine text prompts toward precise expression of the generated images. Inspired by crowdsourced editing strategies, we curated intuitive text features, such as the number and concreteness of nouns, and trained a proxy model to analyze the feature effects on the AI-generated image. With model explanations of the proxy model, we curated a rubric to adjust text prompts to optimize image generation for precise emotion expression. We conducted simulation and user studies, which showed that RePrompt significantly improves the emotional expressiveness of AI-generated images, especially for negative emotions. © 2023 ACM.
"
10.1145/3544548.3580919,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160005800&origin=inward,Conference Paper,SCOPUS_ID:85160005800,scopus,2023-04-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),studying the effect of ai code generators on supporting novice learners in introductory programming,"
AbstractView references

AI code generators like OpenAI Codex have the potential to assist novice programmers by generating code from natural language descriptions, however, over-reliance might negatively impact learning and retention. To explore the implications that AI code generators have on introductory programming, we conducted a controlled experiment with 69 novices (ages 10-17). Learners worked on 45 Python code-authoring tasks, for which half of the learners had access to Codex, each followed by a code-modification task. Our results show that using Codex significantly increased code-authoring performance (1.15x increased completion rate and 1.8x higher scores) while not decreasing performance on manual code-modification tasks. Additionally, learners with access to Codex during the training phase performed slightly better on the evaluation post-tests conducted one week later, although this difference did not reach statistical significance. Of interest, learners with higher Scratch pre-test scores performed significantly better on retention post-tests, if they had prior access to Codex. © 2023 ACM.
"
10.1145/3544548.3580948,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160002666&origin=inward,Conference Paper,SCOPUS_ID:85160002666,scopus,2023-04-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),popblends: strategies for conceptual blending with large language models,"
AbstractView references

Pop culture is an important aspect of communication. On social media people often post pop culture reference images that connect an event, product or other entity to a pop culture domain. Creating these images is a creative challenge that requires finding a conceptual connection between the users' topic and a pop culture domain. In cognitive theory, this task is called conceptual blending. We present a system called PopBlends that automatically suggests conceptual blends. The system explores three approaches that involve both traditional knowledge extraction methods and large language models. Our annotation study shows that all three methods provide connections with similar accuracy, but with very different characteristics. Our user study shows that people found twice as many blend suggestions as they did without the system, and with half the mental demand. We discuss the advantages of combining large language models with knowledge bases for supporting divergent and convergent thinking. © 2023 ACM.
"
10.1145/3544548.3581388,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159918314&origin=inward,Conference Paper,SCOPUS_ID:85159918314,scopus,2023-04-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),why johnny can't prompt: how non-ai experts try (and fail) to design llm prompts,"
AbstractView references

Pre-trained large language models (""LLMs"") like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (""prompting"") has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in ""end-user prompt engineering""using a design probe - a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research. © 2023 Owner/Author.
"
10.1145/3544549.3573794,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85158165229&origin=inward,Conference Paper,SCOPUS_ID:85158165229,scopus,2023-04-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),genaichi 2023: generative ai and hci at chi 2023,"
AbstractView references

This workshop applies human centered themes to a new and powerful technology, generative artificial intelligence (AI). Unlike AI systems that produce decisions or descriptions, generative AI systems can produce new and creative content that can include images, texts, music, video, code, and other forms of design. The results are often similar to results produced by humans. However, it is not yet clear how humans make sense of generative AI algorithms or their outcomes. It is also not yet clear how humans can control and more generally, interact with, these powerful capabilities in ethical ways. Finally, it is not clear what kinds of collaboration patterns will emerge when creative humans and creative technologies work together. Following a successful workshop in 2022, we convene the interdisciplinary research domain of generative AI and HCI. Participation in this invitational workshop is open to seasoned scholars and early career researchers. We solicit descriptions of completed projects, works-in-progress, and provocations. Together we will develop theories and practices in this intriguing new domain. © 2023 Owner/Author.
"
10.1145/3544549.3577043,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85158164375&origin=inward,Conference Paper,SCOPUS_ID:85158164375,scopus,2023-04-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),beyond text-to-image: multimodal prompts to explore generative ai,"
AbstractView references

Text-to-image AI systems have proven to have extraordinary generative capacities that have facilitated widespread adoption. However, these systems are primarily text-based, which is a fundamental inversion of what many artists are traditionally used to: having full control over the composition of their work. Prior work has shown that there is great utility in using text prompts and that AI augmented workflows can increase momentum on creative tasks for end users. However, multimodal interactions beyond text need to be further defined, so end users can have rich points of interaction that allow them to truly co-pilot AI-generated content creation. To this end, the goal of my research is to equip creators with workflows that 1) translate abstract design goals into prompts of visual language, 2) structure exploration of design outcomes, and 3) integrate creator contributions into generations. © 2023 Owner/Author.
"
10.1145/3544549.3573844,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85158101371&origin=inward,Conference Paper,SCOPUS_ID:85158101371,scopus,2023-04-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),impactbot: chatbot leveraging language models to automate feedback and promote critical thinking around impact statements,"
AbstractView references

Impact statements articulate the impacts of a research project with concise and unambiguous statements about problems addressed, actions to resolve, and explanations of any impacts. Researchers and technologists often rely on impact statements as means to provoke introspective critical thinking around the impacts of technology being developed. However, due to factors such as technocentrism, positivity bias, marketization, or hyperinflation of impact statements, the claims presented in these statements do not cover all important aspects when creating technology - for instance, negative and delayed impacts. This work contributes to the development of a chatbot called ImpactBot to promote critical thinking while researchers create impact statements for research projects or scientific papers. The proposed chatbot leverages two fine-tuned state-of-the-art RoBERTa models for sequence classification and was assessed in this case study with 5 researchers from a large information technology company and 7 university engineering research scientists or students. This approach may be reused as part of content management or a paper submission system, for instance, to dialogue with researchers and promote critical thinking about negative impacts and how to mitigate them (if any) while creating impact statements for their projects or scientific papers. © 2023 Owner/Author.
"
10.1115/1.4056598,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85148983348&origin=inward,Article,SCOPUS_ID:85148983348,scopus,2023-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),biologically inspired design concept generation using generative pre-trained transformers,"
AbstractView references

Biological systems in nature have evolved for millions of years to adapt and survive the environment. Many features they developed can be inspirational and beneficial for solving technical problems in modern industries. This leads to a specific form of design-by-analogy called bio-inspired design (BID). Although BID as a design method has been proven beneficial, the gap between biology and engineering continuously hinders designers from effectively applying the method. Therefore, we explore the recent advance of artificial intelligence (AI) for a data-driven approach to bridge the gap. This paper proposes a generative design approach based on the generative pre-trained language model (PLM) to automatically retrieve and map biological analogy and generate BID in the form of natural language. The latest generative pre-trained transformer, namely generative pre-trained transformer 3 (GPT-3), is used as the base PLM. Three types of design concept generators are identified and fine-tuned from the PLM according to the looseness of the problem space representation. Machine evaluators are also fine-tuned to assess the mapping relevancy between the domains within the generated BID concepts. The approach is evaluated and then employed in a real-world project of designing light-weighted flying cars during its conceptual design phase The results show our approach can generate BID concepts with good performance. Copyright © 2023 by ASME.
"
10.1109/TSE.2022.3178469,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131729147&origin=inward,Article,SCOPUS_ID:85131729147,scopus,2023-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on the validity of pre-trained transformers for natural language processing in the software engineering domain,"
AbstractView references

Transformers are the current state-of-the-art of natural language processing in many domains and are using traction within software engineering research as well. Such models are pre-trained on large amounts of data, usually from the general domain. However, we only have a limited understanding regarding the validity of transformers within the software engineering domain, i.e., how good such models are at understanding words and sentences within a software engineering context and how this improves the state-of-the-art. Within this article, we shed light on this complex, but crucial issue. We compare BERT transformer models trained with software engineering data with transformers based on general domain data in multiple dimensions: their vocabulary, their ability to understand which words are missing, and their performance in classification tasks. Our results show that for tasks that require understanding of the software engineering context, pre-training with software engineering data is valuable, while general domain models are sufficient for general language understanding, also within the software engineering domain. © 1976-2012 IEEE.
"
10.1145/3581754.3584169,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151992963&origin=inward,Conference Paper,SCOPUS_ID:85151992963,scopus,2023-03-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),shai 2023: workshop on designing for safety in human-ai interactions,"
AbstractView references

Generative ML models present a novel opportunity for a wider group of societal members to engage with AI, imagine new use cases, and applications with an increasing ability to disseminate the outcomes of such endeavors to larger audiences. However, owing to the novelty and despite best intentions, inadvertent outcomes might accrue leading to harms, especially to marginalized groups in society. As this field of Human AI Interaction advances, academic/industry researchers, and industry practitioners have an opportunity to brainstorm how to best utilize this new technology. Our workshop is aimed at such practitioners and researchers at the intersection of AI and HCI who are interested in collaboratively identifying challenges, and solutions to create safer outcomes with Generative ML models. © 2023 Owner/Author.
"
10.1145/3581641.3584037,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146856593&origin=inward,Conference Paper,SCOPUS_ID:85146856593,scopus,2023-03-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the programmer's assistant: conversational interaction with a large language model for software development,"
AbstractView references

Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model's responses. We developed a prototype system - the Programmer's Assistant - in order to explore the utility of conversational interactions grounded in code, as well as software engineers' receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant's capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development. © 2023 Owner/Author.
"
10.1145/3590003.3590028,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162926361&origin=inward,Conference Paper,SCOPUS_ID:85162926361,scopus,2023-03-17,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),graph representation learning and software homology matching based a study of java code vulnerability detection techniques,"
AbstractView references

In nowadays using different tools and apps is a basic need of people's behavior in life, but the security issues arising from the existence of source code plagiarism among tools and apps are likely to bring huge losses to companies and even countries, so detecting the existence of vulnerabilities or malicious code in software becomes an important part of protecting information and detecting software security. This project is based on the aspect of JAVA code vulnerability detection based on graph representation learning and software homology comparison to carry out research. This project will be based on the content of deep learning, using a large number of vulnerable source code, extracting its features, and transforming it into a graph so that it can be tested source code for comparison and report the vulnerability content. The main work and results of this project are as follows: 1.By extracting the example dataset and generating json files to save the feature information of relevant java code; by generating vector files, bytecode files and dot files, and batch extracting nodes and edges based on the contents of the dot files for subsequent machine learning use, the before and after steps and operations form a logical self-consistency to ensure the integrity of the project. 2.Through the study of graph neural networks and graph convolutional neural networks, relevant models are selected for machine learning using predecessor files and manual model tuning is performed to ensure good learning results and feedback for the machine learning part of the project. 3.This project training dataset negative samples for sard above the shared dataset, which contains 46636 java vulnerability source code, and dataset support environment, test dataset negative samples dataset also from sard, positive samples dataset are generated from the relevant person in charge. 4.Based on Graph Neural Network (GNN) and Graph Convolutional Neural Network (GCN), this project will design and implement a whole set of automated vulnerability detection system for java code. 5. All the related contents of this project, after the human extensive search of domestic and foreign related papers and materials, there are not all projects or contents similar to this project, the same papers and materials appear, all the problems involved in this project and related ideas are for the project this group of people thinking, looking for solutions. © 2023 ACM.
"
10.1021/acs.langmuir.2c03320,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149032729&origin=inward,Article,SCOPUS_ID:85149032729,scopus,2023-03-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),nano-voids in ultrafine explosive particles: characterization and effects on thermal stability,"
AbstractView references

Ultrafine explosives show high safety and reliable initiation and have been widely used in aerospace, military, and industrial systems. The outstanding performance of ultrafine explosives is largely given by the unique void defects according to the simulation results. However, the structures and effects of internal nano-voids in ultrafine explosive particles have been rarely investigated experimentally. In this work, contrast-variation small angle X-ray scattering was verified to reliably measure the structures of internal nano-voids in ultrafine explosive 2,6-diamino-3,5-dinitropyrazine-1-oxide (LLM-105) and 2,2′,4,4′,6,6′-hexanitro diphenylethylene (HNS). The size of nano-voids is around 10 nm, and the estimated number of nano-voids in a single particle is considerable. Moreover, the thermal stability of ultrafine LLM-105 was improved via changing the structures of nano-voids. This work provides a methodology for the study of nano-void defects in ultrafine organic particles and may pave the path to enhance the performance of ultrafine explosives via defect engineering. © 2023 American Chemical Society.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171377878&origin=inward,Article,SCOPUS_ID:85171377878,scopus,2023-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),c2aadl_reverse: a model-driven reverse engineering approach for development and verification of safety-critical software,"
AbstractView references

The safety-critical system communities have been struggling to manage and maintain their legacy software systems because upgrading such systems has been a complex challenge. To overcome or reduce this problem, reverse engineering has been increasingly used in safety-critical systems. This paper proposes C2AADL_Reverse, a model-driven reverse engineering approach for safety-critical software development and verification. C2AADL_Reverse takes multi-task C source code as input, and generates AADL (Architecture Analysis and Design Language) model of the legacy software systems. Compared with the existing works, this paper considers more reversed construction including AADL component structure, behavior, and multi-threaded run-time information. Moreover, two types of activities are proposed to ensure the correctness of C2AADL_Reverse. First, it is necessary to validate the reverse engineering process. Second, the generated AADL models should conform to desired critical properties. We propose the verification of the reverse-engineered AADL model by using UPPAAL to establish component-level properties and the Assume Guarantee REasoning Environment (AGREE) to perform compositional verification of the architecture. This combination of verification tools allows us to iteratively explore design and verification of detailed behavioral models, and to scale formal analysis to large models. In addition, the prototype tool and the evaluation of C2AADL_Reverse using a real-world aerospace case study are presented. © 2023, Ada-Europe. All rights reserved.
"
10.23919/JSC.2023.0007,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165668033&origin=inward,Article,SCOPUS_ID:85165668033,scopus,2023-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prediction of academic performance of students in online live classroom interactions - an analysis using natural language processing and deep learning methods,"
AbstractView references

Prior studies have shown the importance of classroom dialogue in academic performance, through which knowledge construction and social interaction among students take place. However, most of them were based on small scale or qualitative data, and few has explored the availability and potential of big data collected from online classrooms. To address this issue, this paper analyzes dialogues in live classrooms of a large online learning platform in China based on natural language processing techniques. The features of interactive types and emotional expression are extracted from classroom dialogues. We then develop neural network models based on these features to predict high- and low-academic performing students, and employ interpretable AI (artificial intelligence) techniques to determine the most important predictors in the prediction models. In both STEM (science, technology, engineering, mathematics) and non-STEM courses, it is found that high-performing students consistently exhibit more positive emotion, cognition and off-topic dialogues in all stages of the lesson than low-performing students. However, while the metacognitive dialogue illustrates its importance in non-STEM courses, this effect cannot be found in STEM courses. While high-performing students in non-STEM courses show negative emotion in the last stage of lessons, STEM students show positive emotion. © 2020 Tsinghua University Press.
"
10.17586/2226-1494-2023-23-2-304-312,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164803854&origin=inward,Article,SCOPUS_ID:85164803854,scopus,2023-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),natural language based malicious domain detection using machine learning and deep learning,"
AbstractView references

Cyberattacks are still challenging since they are increasing day by day. Cybercriminals employ a variety of strategies to manipulate and exploit their targets vulnerabilities. Malicious URLs are one such strategy which is used to target large groups on various social media platforms. To draw internet users, these web addresses are disguised as being safe. Deliberate or inadvertent use of such URLs exposes the user or the organization in the cyberspace and opens the way for further attacks. Systems that use rules-based or machine learning algorithms to find malicious URLs usually rely on feature engineering. This requires domain expertise and experience. Sometimes, even after extracting features from a dataset, it may not completely leverage the potential of the dataset. The proposed method employs Natural Language Processing (NLP) approaches to vectorize the words in the URLs and applies machine learning and deep learning models for classification. Vectorization technique in NLP reduces the effort of feature engineering and maximizing the use of the dataset. For the experiment, two separate datasets are used. To vectorize the URL text, three different vectorization methods are used. To evaluate the performance of the proposed method, two different datasets (D1 and D2) that are regularly utilized in the research domain were used. The results demonstrate that the superior accuracy of 92.4 % with the D1 dataset is achieved by the Decision Tree (DT) with count vectorizer and the Random Forest (RF) with Term Frequency-Inverse Document Frequency (TF-IDF) vectorizer. With the D2 dataset, DT with TF-IDF vectorizer obtains a greater accuracy of 99.5 %. The Artificial Neural Network (ANN) model achieves 89.6 % accuracy with the D1 dataset and 99.2 % accuracy with the D2 dataset. © ITMO University. All Rights Reserved.
"
10.3390/app13063380,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151497284&origin=inward,Article,SCOPUS_ID:85151497284,scopus,2023-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a selective survey review of computational intelligence applications in the primary subdomains of civil engineering specializations,"
AbstractView references

Artificial intelligence is the branch of computer science that attempts to model cognitive processes such as learning, adaptability and perception to generate intelligent behavior capable of solving complex problems with environmental adaptation and deductive reasoning. Applied research of cutting-edge technologies, primarily computational intelligence, including machine/deep learning and fuzzy computing, can add value to modern science and, more generally, to entrepreneurship and the economy. Regarding the science of civil engineering and, more generally, the construction industry, which is one of the most important in economic entrepreneurship both in terms of the size of the workforce employed and the amount of capital invested, the use of artificial intelligence can change industry business models, eliminate costly mistakes, reduce jobsite injuries and make large engineering projects more efficient. The purpose of this paper is to discuss recent research on artificial intelligence methods (machine and deep learning, computer vision, natural language processing, fuzzy systems, etc.) and their related technologies (extensive data analysis, blockchain, cloud computing, internet of things and augmented reality) in the fields of application of civil engineering science, such as structural engineering, geotechnical engineering, hydraulics and water resources. This review examines the benefits and limitations of using computational intelligence in civil engineering and the challenges researchers and practitioners face in implementing these techniques. The manuscript is targeted at a technical audience, such as researchers or practitioners in civil engineering or computational intelligence, and also intended for a broader audience such as policymakers or the general public who are interested in the civil engineering domain. © 2023 by the authors.
"
10.3390/aerospace10030279,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151346420&origin=inward,Article,SCOPUS_ID:85151346420,scopus,2023-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),aerobert-classifier: classification of aerospace requirements using bert,"
AbstractView references

The system complexity that characterizes current systems warrants an integrated and comprehensive approach to system design and development. This need has brought about a paradigm shift towards Model-Based Systems Engineering (MBSE) approaches to system design and a departure from traditional document-centric methods. While MBSE shows great promise, the ambiguities and inconsistencies present in Natural Language (NL) requirements hinder their conversion to models directly. The field of Natural Language Processing (NLP) has demonstrated great potential in facilitating the conversion of NL requirements into a semi-machine-readable format that enables their standardization and use in a model-based environment. A first step towards standardizing requirements consists of classifying them according to the type (design, functional, performance, etc.) they represent. To that end, a language model capable of classifying requirements needs to be fine-tuned on labeled aerospace requirements. This paper presents an open-source, annotated aerospace requirements corpus (the first of its kind) developed for the purpose of this effort that includes three types of requirements, namely design, functional, and performance requirements. This paper further describes the use of the aforementioned corpus to fine-tune BERT to obtain the aeroBERT-Classifier: a new language model for classifying aerospace requirements into design, functional, or performance requirements. Finally, this paper provides a comparison between aeroBERT-Classifier and other text classification models such as GPT-2, Bidirectional Long Short-Term Memory (Bi-LSTM), and bart-large-mnli. In particular, it shows the superior performance of aeroBERT-Classifier on classifying aerospace requirements over existing models, and this is despite the fact that the model was fine-tuned using a small labeled dataset. © 2023 by the authors.
"
10.3390/a16030155,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151096874&origin=inward,Article,SCOPUS_ID:85151096874,scopus,2023-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a cognitive model for technology adoption,"
AbstractView references

The widespread adoption of advanced technologies, such as Artificial Intelligence (AI), Machine Learning, and Robotics, is rapidly increasing across the globe. This accelerated pace of change is drastically transforming various aspects of our lives and work, resulting in what is now known as Industry 4.0. As businesses integrate these technologies into their daily operations, it significantly impacts their work tasks and required skill sets. However, the approach to technological transformation varies depending on location, industry, and organization. However, there are no published methods that can adequately forecast the adoption of technology and its impact on society. It is essential to prepare for the future impact of Industry 4.0, and this requires policymakers and business leaders to be equipped with scientifically validated models and metrics. Data-driven scenario planning and decision-making can lead to better outcomes in every area of the business, from learning and development to technology investment. However, the current literature falls short in identifying effective and globally applicable strategies to predict the adoption rate of emerging technologies. Therefore, this paper proposes a novel parametric mathematical model for predicting the adoption rate of emerging technologies through a unique data-driven pipeline. This approach utilizes global indicators for countries to predict the technology adoption curves for each country and industry. The model is thoroughly validated, and the paper outlines highly promising evaluation results. The practical implications of this proposed approach are significant because it provides policymakers and business leaders with valuable insights for decision-making and scenario planning. © 2023 by the authors.
"
10.1111/jpim.12656,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146930847&origin=inward,Article,SCOPUS_ID:85146930847,scopus,2023-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),augmenting human innovation teams with artificial intelligence: exploring transformer-based language models,"
AbstractView references

The use of transformer-based language models in artificial intelligence (AI) has increased adoption in various industries and led to significant productivity advancements in business operations. This article explores how these models can be used to augment human innovation teams in the new product development process, allowing for larger problem and solution spaces to be explored and ultimately leading to higher innovation performance. The article proposes the use of the AI-augmented double diamond framework to structure the exploration of how these models can assist in new product development (NPD) tasks, such as text summarization, sentiment analysis, and idea generation. It also discusses the limitations of the technology and the potential impact of AI on established practices in NPD. The article establishes a research agenda for exploring the use of language models in this area and the role of humans in hybrid innovation teams. (Note: Following the idea of this article, GPT-3 alone generated this abstract. Only minor formatting edits were performed by humans.). © 2023 The Authors. Journal of Product Innovation Management published by Wiley Periodicals LLC on behalf of Product Development & Management Association.
"
10.1002/spe.3168,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143403537&origin=inward,Article,SCOPUS_ID:85143403537,scopus,2023-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),xdevs: a toolkit for interoperable modeling and simulation of formal discrete event systems,"
AbstractView references

Employing Modeling and Simulation (M&S) extensively to analyze and develop complex systems is the norm today. The use of robust M&S formalisms and rigorous methodologies is essential to deal with complexity. Among them, the Discrete Event System Specification (DEVS) provides a solid framework for modeling structural, behavior and information aspects of any complex system. This gives several advantages to analyze and design complex systems: completeness, verifiability, extensibility, and maintainability. DEVS formalism has been implemented in many programming languages and executable on multiple platforms. In this paper, we describe the features of an M&S framework called xDEVS that builds upon the prevalent DEVS Application Programming Interface (API) for both modeling and simulation layers, promoting interoperability between the existing platform-specific (C++, Java, Python) DEVS implementations. Additionally, the framework can simulate the same model using sequential, parallel, or distributed architectures. The M&S engine has been reinforced with several strategies to improve performance, as well as tools to perform model analysis and verification. Finally, xDEVS also facilitates systems engineers to apply the vision of model-based systems engineering (MBSE), model-driven engineering (MDE), and model-driven systems engineering (MDSE) paradigms. We highlight the features of the proposed xDEVS framework with multiple examples and case studies illustrating the rigor and diversity of application domains it can support. © 2022 The Authors. Software: Practice and Experience published by John Wiley & Sons Ltd.
"
10.1016/j.eswa.2022.118927,S0957417422019455,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139597157&origin=inward,Article,SCOPUS_ID:85139597157,scopus,2023-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),virtual prompt pre-training for prototype-based few-shot relation extraction,"
                  Prompt tuning with pre-trained language models (PLM) has exhibited outstanding performance by reducing the gap between pre-training tasks and various downstream applications, which requires additional labor efforts in label word mappings and prompt template engineering. However, in a label intensive research domain, e.g., few-shot relation extraction (RE), manually defining label word mappings is particularly challenging, because the number of utilized relation label classes with complex relation names can be extremely large. Besides, the manual prompt development in natural language is subjective to individuals. To tackle these issues, we propose a virtual prompt pre-training method, projecting the virtual prompt to latent space, then fusing with PLM parameters. The pre-training is entity-relation-aware for RE, including the tasks of mask entity prediction, entity typing, distant supervised RE, and contrastive prompt pre-training. The proposed pre-training method can provide robust initialization for prompt encoding, while maintaining the interaction with the PLM. Furthermore, the virtual prompt can effectively avoid the labor efforts and the subjectivity issue in label word mapping and prompt template engineering. Our proposed prompt-based prototype network delivers a novel learning paradigm to model entities and relations via the probability distribution and Euclidean distance of the predictions of query instances and prototypes. The results indicate that our model yields an averaged accuracy gain of 4.21% on two few-shot datasets over strong RE baselines. Based on our proposed framework, our pre-trained model outperforms the strongest RE-related PLM by 6.52%.
               "
10.1109/TCSVT.2022.3209160,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139457321&origin=inward,Article,SCOPUS_ID:85139457321,scopus,2023-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),fast cnn-based single-person 2d human pose estimation for autonomous systems,"
AbstractView references

This paper presents a novel Convolutional Neural Network (CNN) architecture for 2D human pose estimation from RGB images that balances between high 2D human pose/skeleton estimation accuracy and rapid inference. Thus, it is suitable for safety-critical embedded AI scenarios in autonomous systems, where computational resources are typically limited and fast execution is often required, but accuracy cannot be sacrificed. The architecture is composed of a shared feature extraction backbone and two parallel heads attached on top of it: one for 2D human body joint regression and one for global human body structure modelling through Image-to-Image Translation (I2I). A corresponding multitask loss function allows training of the unified network for both tasks, through combining a typical 2D body joint regression with a novel I2I term. Along with enhanced information flow between the parallel neural heads via skip synapses, this strategy is able to extract both ample semantic and rich spatial information, while using a less complex CNN; thus it permits fast execution. The proposed architecture is evaluated on public 2D human pose estimation datasets, achieving the best accuracy-speed ratio compared to the state-of-the-art. Additionally, it is evaluated on a pedestrian intention recognition task for self-driving cars, leading to increased accuracy and speed in comparison to competing approaches. © 1991-2012 IEEE.
"
10.1145/3578527.3578548,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149125083&origin=inward,Conference Paper,SCOPUS_ID:85149125083,scopus,2023-02-23,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a modeling language for novice engineers to design well at saas product companies,"
AbstractView references

Software-as-a-Service (SaaS) product companies have brought in significant changes in how we build software from architecture and engineering process perspective. SaaS products are large, distributed software systems hosted in cloud and built using collaborating services (or micro-services). The software releases happen in days and weeks, necessitating an agile development process. Novice engineers (those who join the company fresh from college) need to become comfortable with complex systems and proficient in agile delivery with high quality, otherwise they fall behind in productivity. The paper posits that, to be successful at these SaaS product companies, the novice engineers need good modeling and design skills. While this has been for all software development, the changes driven by SaaS products have made this need more acute. Such skills will allow them to capture their feature behaviors (in context of their understanding of the larger product) in an implementation-independent manner and any knowledge gaps can be identified and bridged by their collaborators. We propose a modeling language that is easy for them to learn and use, and which has characteristics suitable for the kind of engineering work they need to do in their early years in a SaaS product company. This modeling language is based on the notion of Transition Systems. The paper demonstrates the usage and value of this language by creating a model for a real feature. The modeling language is quite general and transcends abstraction boundaries. We also present a modeling process that should be used with this language for better results. This is a short position paper that presents an idea about a new modeling language for a specific purpose (helping novice engineers design well at SaaS product companies). Validation studies for the language and the design process is a work in progress and the results will be shared in a full paper later. © 2023 ACM.
"
10.1016/j.knosys.2022.110215,S0950705122013119,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145018138&origin=inward,Article,SCOPUS_ID:85145018138,scopus,2023-02-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),supervised term-category feature weighting for improved text classification,"
                  Text classification is a central task in Natural Language Processing (NLP) that aims at categorizing text documents into predefined classes or categories. It requires appropriate features to describe the contents and meaning of text documents, and map them with their target categories. Existing text feature representations rely on a weighted representation of the document terms. Hence, choosing a suitable method for term weighting is of major importance and can help increase the effectiveness of the classification task. In this study, we provide a novel text classification framework for Category-based Feature Engineering titled CFE. It consists of a supervised weighting scheme defined based on a variant of the TF-ICF (Term Frequency-Inverse Category Frequency) model, embedded into three new lean classification approaches: (i) IterativeAdditive (flat), (ii) GradientDescentANN (1-layered), and (iii) FeedForwardANN (2-layered). The IterativeAdditive approach augments each document representation with a set of synthetic features inferred from TF-ICF category representations. It builds a term-category TF-ICF matrix using an iterative and additive algorithm that produces category vector representations and updates until reaching convergence. GradientDescentANN replaces the iterative additive process mentioned previously by computing the term-category matrix using a gradient descent ANN model. Training the ANN using the gradient descent algorithm allows updating the term-category matrix until reaching convergence. FeedForwardANN uses a feed-forward ANN model to transform document representations into the category vector space. The transformed document vectors are then compared with the target category vectors, and are associated with the most similar categories. We have implemented CFE including its three classification approaches, and we have conducted a large battery of tests to evaluate their performance. Experimental results on five benchmark datasets show that our lean approaches mostly improve text classification accuracy while requiring significantly less computation time compared with their deep model alternatives.
               "
10.1007/978-3-031-18556-4_16,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160501614&origin=inward,Book Chapter,SCOPUS_ID:85160501614,scopus,2023-02-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),symples: embedded systems design with smarty,"
AbstractView references

The evolution of hardware platforms has led to the move of a large amount of functionality to embedded software systems. Thus, the software has become increasingly complex. Several techniques have been proposed over the years for dealing with this complexity. Software product line (SPL) and model-driven engineering (MDE) can enhance the development of complex embedded systems by using different specification languages according to the abstraction levels and controlling variability across development. This chapter describes SyMPLES: an approach that combines MDE and SPL to deal with the complexity of embedded systems software. SyMPLES includes two SysML extensions, created using UML profiling mechanism both to associate SysML blocks with the main classes of functional blocks and to express SPL variability concepts using SMarty approach. SyMPLES also provides a model transformation that transforms the SysML SPL into Simulink models, speeding up the development process. Lastly, this chapter shows how an approach like SyMPLES can be validated to ensure the quality of its products. © Springer Nature Switzerland AG 2023. All rights reserved.
"
10.1016/j.autcon.2022.104661,S0926580522005313,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142721960&origin=inward,Article,SCOPUS_ID:85142721960,scopus,2023-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),dual generative adversarial networks for automated component layout design of steel frame-brace structures,"
                  With the development of artificial intelligence (AI), it gains in popularity to use AI to solve problems in civil engineering. However, the research on AI is mainly focused on the field of structural health monitoring, and less on the field of structural design. As one new direction in the AI domain, the generative adversarial network (GAN) method has developed rapidly, which is able to synthesize high-quality images based on demand. Therefore, it opens a new window for AI-aided automatic structure design. In this paper, a novel GAN-based method, namely FrameGAN, is proposed to realize automated component layout design of steel frame-brace structures. By collecting and processing drawings designed by senior structural engineers, FrameGAN and two mainstream GAN models (pix2pix and pix2pixHD) are tested and compared, which demonstrates the superiority of the proposed FrameGAN. In addition, the design results of FrameGAN are compared and analyzed with those of senior structural engineers based on two unique evaluation metrics, i.e., expert grading and objective comparison. The results show that the design of FrameGAN is close to that of structural engineers, which indicates the availability of FrameGAN in the component layout design of steel frame-brace structures.
               "
10.1016/j.cpc.2022.108596,S0010465522003150,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142509336&origin=inward,Article,SCOPUS_ID:85142509336,scopus,2023-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),fabsim3: an automation toolkit for verified simulations using high performance computing,"A common feature of computational modelling and simulation research is the need to perform many tasks in complex sequences to achieve a usable result. This will typically involve tasks such as preparing input data, pre-processing, running simulations on a local or remote machine, post-processing, and performing coupling communications, validations and/or optimisations. Tasks like these can involve manual steps which are time and effort intensive, especially when it involves the management of large ensemble runs. Additionally, human errors become more likely and numerous as the research work becomes more complex, increasing the risk of damaging the credibility of simulation results. Automation tools can help ensure the credibility of simulation results by reducing the manual time and effort required to perform these research tasks, by making more rigorous procedures tractable, and by reducing the probability of human error due to a reduced number of manual actions. In addition, efficiency gained through automation can help researchers to perform more research within the budget and effort constraints imposed by their projects. This paper presents the main software release of FabSim3, and explains how our automation toolkit can improve and simplify a range of tasks for researchers and application developers. FabSim3 helps to prepare, submit, execute, retrieve, and analyze simulation workflows. By providing a suitable level of abstraction, FabSim3 reduces the complexity of setting up and managing a large-scale simulation scenario, while still providing transparent access to the underlying layers for effective debugging. The tool also facilitates job submission and management (including staging and curation of files and environments) for a range of different supercomputing environments. Although FabSim3 itself is application-agnostic, it supports a provably extensible plugin system where users automate simulation and analysis workflows for their own application domains. To highlight this, we briefly describe a selection of these plugins and we demonstrate the efficiency of the toolkit in handling large ensemble workflows. Program summary Program Title: FabSim3 CPC Library link to program files: https://doi.org/10.17632/6nfrwy7ptj.1 Licensing provisions: BSD 3-clause Programming language: Python 3 Nature of problem: Many aspects are crucial for obtaining reproducible and robust simulation results. For instance, we need to curate all the inputs and outputs for later scrutiny, scrutinize the model behaviour under slightly perturbed circumstances, quantify the propagation of key uncertainties from input data and known parameters and analyze the sensitivity for any parameters for which the exact specification eludes us. Solution method: FabSim3 uses a range of methods to provide automation. These primarily include: (i) SSH + Fabric2 to enable remote execution of SSH commands, (ii) an internal parameter state space using primarily Python dict objects that can be customized with machine- plugin- and user-specific modifications, (iii) Python templating to quickly enable the insertion of state space variables into supercomputing scripts, (iv) multiprocessing and/or QCG-PilotJob to enable efficient submission and execution of job arrays and (v) a system of flexibly installable and modifiable Python3 plugins which allows users to create and customize application-specific functionalities without modifying the core code base. In addition to the written code, FabSim3 also relies on a set of user conventions to maintain a separation of concerns (particularly between machine-, user- and application-specific settings). Additional comments including restrictions and unusual features: This paper serves as the definitive reference for FabSim3."
10.1145/3571788.3571797,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147432527&origin=inward,Conference Paper,SCOPUS_ID:85147432527,scopus,2023-01-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),guiding feature models synthesis from user-stories: an exploratory approach,"
AbstractView references

User-stories are commonly used to define requirements in agile project management. In Software Product Lines (SPL), a user-story corresponds to a feature description (or part of it), that can be shared by several products. In practice, large SPL include a huge number of user-stories, making variability hard to grasp and handle. In this paper we present an exploratory approach that aims to guide the synthesis of Feature Models that capture and structure the commonalities and the variability expressed in these user-stories. The built Feature Models aim to help the project understanding, maintenance and evolution. Our approach first decomposes the user-stories to extract the roles and the features, using natural language processing techniques. In a second step, we group user-stories having the same topics thanks to a clustering method. This contributes to extract more general features. In a third step, we leverage the use of Formal Concept Analysis to extract logical constraints between the features that guide Feature Model synthesis. We illustrate our approach using a dataset from our industrial partner. © 2023 ACM.
"
10.1016/j.isci.2022.105758,S2589004222020314,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145732841&origin=inward,Article,SCOPUS_ID:85145732841,scopus,2023-01-20,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),in situ health monitoring of multiscale structures and its instantaneous verification using mechanoluminescence and dual machine learning,"Extensive changes in the legal, commercial and technical requirements in engineering fields have necessitated automated real-time structural health monitoring (SHM) and instantaneous verification. An integrated system with mechanoluminescence (ML) and dual artificial intelligence (AI) modules with subsidiary finite element method (FEM) simulation is designed for in situ SHM and instantaneous verification. The ML module detects the exact position of a crack tip and evaluates the significance of existing cracks with a plastic stress-intensity factor (PSIF; K P ). ML fields and their corresponding K p M L values are referenced and verified using the FEM simulation and bidirectional generative adversarial network (GAN). Well-trained forward and backward GANs create fake FEM and ML images that appear authentic to observers; a convolutional neural network is used to postulate precise PSIFs from fake images. Finally, the reliability of the proposed system to satisfy existing commercial requirements is validated in terms of tension, compact tension, AI, and instrumentation."
10.1145/3571306.3571428,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145876847&origin=inward,Conference Paper,SCOPUS_ID:85145876847,scopus,2023-01-04,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"characteristics of deep learning workloads in industry, academic institutions and national laboratories","
AbstractView references

Over the past two decades, deep learning techniques have emerged as an immensely powerful technology, with break-through in computer vision, speech to text technologies, natural language processing and many more such fields. As neural networks grow in size and capacity, they require higher compute power and larger datasets to converge to a model with higher accuracy. In this paper, we present a data-centric approach to study the system level requirements (GPU utilization, CPU utilization, I/O) of deep learning training workloads, and uncover a few insights that help us understand the nature of deep learning training workloads. We analyze three datasets found in industry, academia and national laboratories to understand the requirements and properties of deep learning training workloads in different settings. © 2023 ACM.
"
10.1145/3571306.3571414,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145874509&origin=inward,Conference Paper,SCOPUS_ID:85145874509,scopus,2023-01-04,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a data-centric approach for analyzing large-scale deep learning applications,"
AbstractView references

Over the past two decades, deep learning techniques have emerged as an immensely powerful technology, with break-through in computer vision, speech to text technologies, natural language processing and many more such fields. As neural networks grow in size and capacity, they require higher compute power and larger datasets to converge to a model with higher accuracy. In this poster, we present a data-centric approach to studying the system level requirements (GPU utilization, CPU utilization, I/O) of deep learning training workloads, and uncover a few insights that help us understand the nature of deep learning training workloads. We analyse three datasets found in industry, academia and national laboratories to understand the requirements and properties of deep learning training workloads. © 2023 Owner/Author.
"
10.55982/openpraxis.15.4.609,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188904654&origin=inward,Article,SCOPUS_ID:85188904654,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"unleashing the potential of generative ai, conversational agents and chatbots in educational praxis: a systematic review and bibliometric analysis of genai in education","
AbstractView references

In the rapidly evolving landscape of education, the pivotal axis around which transformation revolves is human-AI interaction. In this sense, this paper adopts a data mining and analytic approach to understand what the related literature tells us regarding the trends and patterns of generative AI research in educational praxis. Accordingly, this systematic exploration spotlights the following research themes: Interaction and communication with generative AI-powered chatbots; impact of the LLMs and generative AI on teaching and learning, conversational educational agents and their opportunities, challenges, and implications; leveraging Generative AI for enhancing social and cognitive learning processes; promoting AI literacy for unleashing future opportunities; harnessing Generative AI to expand academic capabilities, and lastly, augmenting educational experiences through human-AI interaction. Beyond the identified research themes and patterns, this paper argues that emotional intelligence, AI literacy, and prompt engineering are the trending research topics that require further exploration. Accordingly, it’s in this praxis that emotional intelligence emerges as a pivotal attribute, as AI technologies often struggle to comprehend and respond to the nuanced emotional cues. Generative AI literacy then takes center stage, becoming an indispensable asset in an era permeated with AI technologies, equipping students with the tools to critically engage with AI systems, thereby ensuring they become active, discerning users of these powerful tools. Concurrently, prompt engineering, the art of crafting queries that yield precise and valuable responses from AI systems, empowers both educators and students to maximize the utility of AI-driven educational resources. © 2023 The Author(s).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188505877&origin=inward,Conference Paper,SCOPUS_ID:85188505877,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),development of an intelligent design and simulation aid system for heat treatment processes based on llm,"
AbstractView references

In this paper, an intelligent learning system that combines LLM with heat treatment simulation software is developed. The system can provide users with knowledge learning Q&A function. The user can propose performance requirements for materials and components, and the system can give the corresponding heat treatment process conditions and data files during heat treatment simulation by means of Q&A. The system encodes the knowledge of heat treatment of steel in vector form, stores it in the vector database Chroma, and applies dialogue tasks to match the user's questions with the vectorized knowledge, find the relevance, and input it into the gpt-3.5-turbo-16k model in combination with hint engineering. This approach helps the large language model to refer more to the knowledge in the knowledge base rather than the pre-trained knowledge to complete the knowledge and answer the query. In addition, as an attempt to apply it, CHATGLM-6B can convert user-given or recommended process descriptions into process input files that can be used directly by the heat treatment simulation software COSMAP. Building a vector knowledge base is an effective approach that addresses the challenge of mastering expertise in the heat treatment domain faced by large language models. Compared with the traditional method of fine-tuning large language models using knowledge to achieve expertise, the vectorized approach allows intelligent expansion of the knowledge base, and the complete task from knowledge Q&A to recommendations to simulations can achieve direct recommendation of process effects, greatly simplifying the workflow of heat treatment practitioners and improving R&D efficiency. In addition, process results validated by COSMAP can help optimize and improve the knowledge base of the recommendation module through self-iterations. The test results demonstrate that this approach leverages the capabilities of LLM to provide accurate and fast answers to queries related to the heat treatment of metallic materials. interoperability between LLM and CAE software has been achieved. This work provides a new avenue for the development of expert knowledge base systems in the field of heat treatment of metallic materials and is expected to have applications in other fields as well. © 2023 28th IFHTSE 2023 Congress. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188013365&origin=inward,Conference Paper,SCOPUS_ID:85188013365,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),emotionally intelligent robots: advancements in social and cognitive computing towards improving human-robot interaction in space,"
AbstractView references

With several upcoming long-duration crewed space missions, robots that have emotional intelligence capabilities are becoming increasingly critical in supporting the well-being and performance of astronauts. Long-duration space missions subject astronauts to the effects of isolation, confinement, and exposure to extreme environments, which could take a toll on their mental health and affect crew cohesion. Emotionally-intelligent robots can mitigate these physical and psychological stresses, with social and cognitive computing (SCC) advancements providing promising avenues for achieving improved human-robot interaction (HRI) in space. For example, the social space robot Crew Interactive Mobile Companion (CIMON) uses SCC techniques to facilitate natural language communication in order to interpret voice commands, understand context, and respond appropriately to astronauts. SCC techniques already play a significant role in influencing emotionally-intelligent robots on Earth. SCC involves analyzing social behaviors and interactions to help artificial intelligence (AI) systems recognise emotional cues, and uses machine learning (ML) algorithms and natural language processing (NLP) to understand the context of emotional expressions in order to respond appropriately. When applied to space settings, emotionally-intelligent robots have the potential to provide emotional support, companionship and cognitive simulation to astronauts, as well as better support crew interactions, decision-making and assistance in carrying out missions. Emotionally-intelligent robots on Earth that use underlying SCC techniques have shown to provide comfort and companionship, mental stimulation, possess the ability to support decision-making and even reduce pain perception. However, there is not much research around emotionally-intelligent robots in space or the computing techniques behind them, with most research on HRI in space exploration covering the engineering aspects. This paper reviews existing social robots designed for space such as CIMON and Astronaut Assistant Robot (AAR), covering their emotional intelligence capabilities, and strengths and limitations towards meeting psychological challenges in long-duration space missions. The paper will also review current advancements in social robots and SCC techniques such as NLP, generative language models, sentiment analysis, speech recognition, social signal processing (SSP) and discuss their applicability to improving HRI in space. Finally, the paper will recommend key areas of opportunity and research towards realising emotionally-intelligent robotic capabilities for long-duration space missions. Copyright © 2023 by the International Astronautical Federation (IAF). All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85188003092&origin=inward,Conference Paper,SCOPUS_ID:85188003092,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a digital engineering approach to assessing the moon to mars architecture,"
AbstractView references

NASA's Moon to Mars Architecture is complex and constantly evolving. It must be robust to accommodate new discoveries, evolving technology, and new partnerships, while also ensuring the achievement of the Agency Moon to Mars goals and objectives released in September 2022. This requires the oversight of an enormous amount of data across a wide and diverse range of efforts that are often closely linked together. NASA's Moon to Mars Architecture and associated elements have historically been defined and reviewed through a variety of configuration managed documents. Each document is managed by a specific team within NASA and contains a large amount of interrelated data. In addition, data from one document is related to data from other documents managed by separate teams. This document-centric approach to managing an architecture risks providing an unclear flow of information, often fails to fully justify why certain systems are necessary and makes it difficult to perform gap analysis. To ensure NASA's Moon to Mars Architecture remains robust and has a clear flow of data, NASA has developed a digital solution that uses Model Based Systems Engineering (MBSE) linked with other tools to enable successful systems engineering. This approach brings all the disparate architecture information together into one place by linking and analyzing for gaps, redundancies, and discrepancies. NASA's Digital Engineering approach consists of three main tools that provide unique capabilities and link together to form a digital thread: MagicDraw for Model Based Systems Engineering and architecture management, Cameo Collaborator for stakeholder review, and Change Management Workflow (CMW) for configuration management. These tools were selected based on their version control capabilities, user permissions, exportability, traceability, ability to connect with other tools, visual products, compliance with NASA's cybersecurity requirements, etc. With this tool framework, data from each disparate document source including goals, objectives, requirements, use cases, functions, operational activities, systems, and more was ingested into these tools. A metamodel was developed to establish a common language and ontology, providing guidance and structure to the modeling effort. Finally, using this metamodel, the data was traced together and manipulated to develop quick visual outputs for decision makers. This paper will provide an overview of NASA's Digital Engineering approach to linking together disparate data from multiple different sources and how it can be used to assess an architecture. Copyright © 2023 by the International Astronautical Federation (IAF). All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187999107&origin=inward,Conference Paper,SCOPUS_ID:85187999107,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),investigating the use of chatgpt in satellite image analysis for aeronautical engineering applications,"
AbstractView references

ChatGPT made waves in the world of natural language processing with its outstanding performance. However, its potential applications in aeronautical engineering remain untapped. To Investigate this uncovered part, we present an exploratory study on the potential benefits, scalability, and limitations of applying ChatGPT in this field. As a case study, we demonstrate the application of ChatGPT to satellite imagery analysis using Python. The usefulness and limitations of ChatGPT are investigated when performing satellite imagery analysis for aeronautical engineering applications. We also discuss the potential pros and cons of utilizing ChatGPT in this area. Our study demonstrates that ChatGPT can be effectively used in satellite imagery analysis for aeronautical engineering applications in practice. Although the model has limitations, especially in specialized and complex fields, our study provides insight into these limitations and describes future challenges to improve scalability. Our research findings provide insights into the usability, scalability, usefulness, and limitations of ChatGPT in the field of aeronautical engineering. This can benefit researchers and practitioners working in this field, allowing them to identify the potential applications of ChatGPT and the limitations they may encounter. Overall, our study highlights the potential of ChatGPT and its implications for the future of aeronautical engineering. Copyright © 2023 by the International Astronautical Federation (IAF). All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187980718&origin=inward,Conference Paper,SCOPUS_ID:85187980718,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),large language model applications to space systems engineering,"
AbstractView references

Recent developments in large language models (LLM) have made them the topic of much media coverage and made the technology accessible to the general audience, including space systems engineers. Past research has explored the application of natural language processing to space systems, notably engineering document analysis. Most of the research has focused on requirements engineering and analysis of mission design descriptions. Past research has also explored the concept of virtual assistants for a broader scope of applications; however, the solutions proposed often require significant setup efforts. This paper explores a broad scope of systems engineering applications with pre-trained large language models requiring minimal setup. We focus on the systems design and technical management processes in NASA's systems engineering engine. We focus on the application of the pre-trained LLM GPT4 and show atomic case studies for stakeholder expectations definition, technical requirements definition, logical decomposition, design solution definition, and technical data management. We conduct a qualitative analysis of the performance of the output generated by the LLM. Our main findings are that the LLM produces meaningful output but also produces additional information that might be irrelevant or not necessary for the task. We also did not observe any hallucinations. One major result we observe is the ease for the LLM to generate system models based on only a few examples from the modeling language. These results encourage further research at a larger scale. We remark on the need for a benchmarking dataset, based on input from experts, to test the validity of this technology before an industrial-scale application of LLMs to assist with systems engineering. Copyright © 2023 by the International Astronautical Federation (IAF). All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187975481&origin=inward,Conference Paper,SCOPUS_ID:85187975481,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),leveraging language models semantic similarity capabilities to facilitate information reuse in system engineering,"
AbstractView references

Model-Based Systems Engineering (MBSE) is a powerful approach for designing complex engineering systems, which also generates valuable data after each conducted study. However, currently there are few to no approaches for reusing this information in a systematic way. In this paper, we propose using state-of-the-art Natural Language Processing (NLP) methods and a graph database to analyze data from past missions and facilitate the design process of new missions. In particular, we firstly develop techniques for analysing a database of past-mission requirements. This include the ability to identify semantic similar requirements from past missions for a given new requirement. We also fine-tune a language model in order to analyse the logical traceability between two requirements. These methods are meant to enable engineers to more efficiently define the requirement space for a new spacecraft.Secondly, we also develop methods to analyse the physical and functional architectures of past missions. Based on an input for a new design, a graph database of past-mission design can be queried for similar design choices and functionalities by again leveraging the abilities of semantic similarity and a specialised breadth-first-search algorithm. Finally, we show how both the requirement and design analyses could in order to automatically verify if the provisions of a requirements are reflected in the physical architecture. For this analysis, a language model is used to extract core concepts from a requirement. Then, in a second step, the concepts from the requirement are mapped to nodes in the graph database. For the actual verification, a relevant extract of the graph together with the requirement are then used as input for a large language model, which is prompted to reason if the requirement is fulfilled or not. By leveraging NLP and graph search techniques, we believe that these approaches can lead to more efficient and effective design processes for complex engineering systems by reusing information from past designs. The proposed techniques have been developed and tested on real past-mission requirements and design architectures in collaboration with Thales Alenia Space, RHEA group, and the European Space Agency. Copyright © 2023 by the International Astronautical Federation (IAF). All rights reserved.
"
10.13052/jwe1540-9589.2285,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187943790&origin=inward,Article,SCOPUS_ID:85187943790,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a study on performance improvement of prompt engineering for generative ai with a large language model,"
AbstractView references

In the realm of Generative AI, where various models are introduced, prompt engineering emerges as a significant technique within natural language processing-based Generative AI. Its primary function lies in effectively enhancing the results of sentence generation by large language models (LLMs). Notably, prompt engineering has gained attention as a method capable of improving LLM performance by modifying the structure of input prompts alone. In this study, we apply prompt engineering to Korean-based LLMs, presenting an efficient approach for generating specific conversational responses with less data. We achieve this through the utilization of the query transformation module (QTM). Our proposed QTM transforms input prompt sentences into three distinct query methods, breaking them down into objectives and key points, making them more comprehensible for LLMs. For performance validation, we employ Korean versions of LLMs, specifically SKT GPT-2 and Kakaobrain KoGPT-3. We compare four different query methods, including the original unmodified query, using Google SSA to assess the naturalness and specificity of generated sentences. The results demonstrate an average improvement of 11.46% when compared to the unmodified query, underscoring the efficacy of the proposed QTM in achieving enhanced performance. © 2024 River Publishers.
"
10.1109/ICSE-FoSE59343.2023.00009,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187719451&origin=inward,Conference Paper,SCOPUS_ID:85187719451,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),software testing of generative ai systems: challenges and opportunities,"
AbstractView references

Software Testing is a well-established area in software engineering, encompassing various techniques and methodologies to ensure the quality of software systems. However, with the arrival of generative artificial intelligence (GenAI) systems, new challenges arise in the testing domain. These systems, capable of generating novel and creative outputs, introduce unique complexities that require novel testing approaches. In this paper, I aim to explore the challenges posed by GenAI systems and discuss potential opportunities for future research in the area of testing. I will touch on the specific characteristics of GenAI systems that make traditional testing techniques inadequate or insufficient. By addressing these challenges and pursuing further research, we can enhance our understanding of how to safeguard GenAI and pave the way for improved quality assurance in this rapidly evolving area. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187549528&origin=inward,Conference Paper,SCOPUS_ID:85187549528,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),multi-architecture unified modeling for manufacturing service value net system design,"
AbstractView references

The synergistic development of product after-market is an effective way for manufacturing enterprises to extend their value chains. As a system architecture design formalism method, model-based systems engineering (MBSE) provides capable support in the design process of complex systems. In this paper, a cloud-based manufacturing service value net system is proposed to help manufacturing enterprises coordinate with partners. However, in this cross-organization collaborative activities, experts with different domain backgrounds are familiar with various system specifications and modeling languages, which makes it extremely hard for related stakeholders to collaborate and conduct information interaction. Therefore, this article presents a semantic modeling method of GOPPRR and unified ontology modeling karma language to support different MBSE standards. The specific domain meta-models of manufacturing service value net system are developed throughout the whole lifecycle process. MetaGraph tool is applied to build meta-models and models. Karma codes with consistent semantics can be generated automatically in real time no matter what modeling language is taken. Concretely, an instantiated modeling case was created in the basic of developed meta-model library. This paper conducts ongoing research for system design in the manufacturing service field, and the proposed modeling method also implies reference significance for designing of similar systems. © 2023 CEUR-WS. All rights reserved.
"
10.1109/SWC57546.2023.10449293,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187375821&origin=inward,Conference Paper,SCOPUS_ID:85187375821,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a knowledge push method for iteration design integrating sysml and knowledge graph,"
AbstractView references

Product iterative design contributes to product performance, stakeholder satisfaction, etc., and is one of the major competencies of complex equipment manufacturing companies. Nevertheless, a large quantity of knowledge is generated during the whole life cycle of complex products, and it is crucial to use this knowledge to implement iterative product design. The application of knowledge in the product design stage is mostly based on knowledge pushing from static knowledge base, which is prone to problems such as inefficient knowledge management and insufficient expression of semantic relationships, and does not combine with product iterative design scenarios. To this end, this paper proposes an iterative design knowledge pushing method combining SysML(System Modeling Language) and knowledge graphs. Firstly, the method constructs a SysML-based iterative product design system model for supporting the determination of iterative design intention scenarios. Secondly, the knowledge graph technology is used to unify the management and representation of product iterative design knowledge. Thirdly, the knowledge graph for the iterative design process is constructed oriented to the iterative feedback requirements based on FBS(Function-Behavior-Structure) and knowledge complexity calculation to determine the KDA(Knowledge Demand Anchor). Finally, based on the determined KDA, the design knowledge is pushed with the help of TransR(Translate-R) and random walk algorithms. The proposed method is verified by an illustrated case. © 2023 IEEE.
"
10.1109/ICECCE61019.2023.10441794,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85187228867&origin=inward,Conference Paper,SCOPUS_ID:85187228867,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ags: arabic gpt summarization corpus,"
AbstractView references

This paper presents a novel method for abstractive summarization of Arabic text using a large-language model (LLM), GPT-3.5 Turbo. We introduce AGS, the first publicly available dataset consisting of 142,000 pairs of articles and summaries with an average compression ratio of 70% written in Modern Standard Arabic (MSA) generated by an LLM. We proposed a baseline model that achieves high scores on ROUGE-L, similarity score, and compression ratio, while capturing the main points and details of the original articles. We fine-tuned mT5 on our dataset reaching a compression ratio of 62%, and similarity score of 82.65%. We release the AGS dataset to the research community, hoping to advance the field of Arabic natural language processing (NLP) and facilitate the development of effective abstractive summarization systems for Arabic text. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186961854&origin=inward,Conference Paper,SCOPUS_ID:85186961854,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),large language models in requirements engineering for digital twins,"
AbstractView references

Can large language models (LLMs) be used for digital twin engineering (DTE)? Engineering digital twins (DTs) is a complex process consisting of several phases and involving different disciplines. We argue that an investigation of LLM use in DTE has to define what kinds of DTs are in focus and what DTE phases shall be supported. In our work, we concentrate on the early phases of DTE, with a particular focus on requirements engineering (RE), and we focus on supervisory and operational DTs. This paper investigates the quality of LLM output for defining requirements for DTs. The main contributions of our work are results from an experiment comparing requirements to a DT of an air conditioning facility of a domain expert and ChatGPT and conclusions for prompt engineering resulting from this experiment. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
10.1109/EKI61071.2023.00010,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186755850&origin=inward,Conference Paper,SCOPUS_ID:85186755850,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"opportunities, challenges, strategies, and reforms for chatgpt in higher education","
AbstractView references

ChatGPT is a powerful large language model developed by OpenAI. It has a profound impact on higher education fields. This technology offers exciting opportunities for students and educators, including personalized feedback, increased accessibility, interactive conversations, lecture preparation, performance evaluation, and new ways to teach complex concepts. At the same time, ChatGPT poses different threats and challenges to the traditional high education system, such as the possibility of cheating on online exams, human-like text generation, diminished critical thinking skills, and difficulties in evaluating information generated by ChatGPT. This paper explores the potential opportunities and challenges that ChatGPT poses in higher education from the perspective of students and educators. In higher education, by understanding these challenges and taking steps to mitigate them, we can then ensure the language models to be used in a responsible and ethical way. © 2023 IEEE.
"
10.1109/ITSC57777.2023.10422308,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186526218&origin=inward,Conference Paper,SCOPUS_ID:85186526218,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),domain knowledge distillation from large language model: an empirical study in the autonomous driving domain,"
AbstractView references

Engineering knowledge-based (or expert) systems require extensive manual effort and domain knowledge. As Large Language Models (LLMs) are trained using an enormous amount of cross-domain knowledge, it becomes possible to automate such engineering processes. This paper presents an empirical automation and semi-automation framework for domain knowledge distillation using prompt engineering and the LLM ChatGPT. We assess the framework empirically in the autonomous driving domain and present our key observations. In our implementation, we construct the domain knowledge ontology by 'chatting' with ChatGPT. The key finding is that while fully automated domain ontology construction is possible, human supervision and early intervention typically improve efficiency and output quality as they lessen the effects of response randomness and the butterfly effect. We, therefore, also develop a web-based distillation assistant enabling supervision and flexible intervention at runtime. We hope our findings and tools could inspire future research toward revolutionizing the engineering of knowledge-based systems across application domains. © 2023 IEEE.
"
10.1109/C2I659362.2023.10430931,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186516536&origin=inward,Conference Paper,SCOPUS_ID:85186516536,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),understanding user perception of biometric privacy in the era of generative ai,"
AbstractView references

In the dynamic world of technology, the fusion of biometrics and artificial intelligence has ushered in an unprecedented era marked by convenience, security, and cutting-edge innovation. Biometrics, the science of using unique physical or behavioral attributes for identification and authentication, has seamlessly merged into daily routines. From unlocking smartphones with a fingerprint to clearing airport security with facial recognition, biometrics has transformed the way people interact with technology and the world. Simultaneously, Generative AI, powered by machine learning models, has unlocked new frontiers in generating remarkably realistic synthetic data, including human faces, voices, and fingerprints. This convergence of biometrics and Generative AI lies at the heart of a complex and rapidly evolving landscape, giving rise to profound questions concerning individual privacy and security. This study examines the correlation between demographic factors like age, gender, educational background, technological competence, and the regularity of employing biometric authentication, and their awareness about biometric technologies. Additionally, this research explores concerns regarding the potential misuse of biometric data and the notion that organizations should seek explicit consent before collecting such data. Lastly, it assesses the awareness of potential privacy risks and the belief that individuals should receive education regarding the utilization of their biometric data in AI systems. Descriptive research design has been used in this study. The first section of the questionnaire using Microsoft Forms covers the demographic factors, technological proficiency and frequency of using biometric authentication (e.g., fingerprint, facial recognition). The next section focuses on awareness and usage of biometric technologies, biometric privacy, trust, education, awareness of biometrics, generative AI and deepfakes using the Likert Scale. 53 samples were obtained through Simple Random Sampling from UAE residents. Then, testing of hypothesis using correlation analysis was done using SPSS. The results reveal that demographic variables do not exhibit a statistically significant relationship with privacy concerns. However, there is a statistically significant correlation exists between biometric authentication and awareness & knowledge parameters. © 2023 IEEE.
"
10.1109/TBCAS.2023.3337335,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186478755&origin=inward,Article,SCOPUS_ID:85186478755,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),high-machting and low-cost realization of the fhn neuron model on reconfigurable fpga board,"
AbstractView references

The main objectives of neuromorphic engineering are the research, modeling, and implementation of neural functioning in the human brain. We provide a hardware solution that can replicate such a nature-inspired system by merging multiple scientific domains and is based on neural cell processes. This work provides a modified version of the original Fitz-Hugh Nagumo (FHN) neuron using a simple <inline-formula><tex-math notation=""LaTeX"">$\rm 2^{V}$</tex-math></inline-formula> term called Hybrid Piece-Wised Base-2 Model (HPWBM), which accurately reproduces numerous patterns of the original neuron model. With reduced terms, we suggest modifying the original nonlinear term to achieve high matching accuracy and little computing error. Time domain and phase portraits are used to validate the proposed model, which shows that it can reproduce all of the FHN model's properties with high accuracy and little mistake. We provide an effective digital hardware approach for large-scale neuron implementations based on resource-sharing and pipelining strategies. The Hardware Description Language (HDL) is used to construct the hardware on an FPGA as a proof of concept. The recommended model hardly uses 0.48 percent of the resources on a Virtex 4 FPGA board, according to the results of the hardware implementation. The circuit can run at a maximum frequency of 448.236 MHz, according to the static timing study. Authors
"
10.1109/ICDMW60847.2023.00071,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186139735&origin=inward,Conference Paper,SCOPUS_ID:85186139735,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),kosa: ko enhanced salary analytics based on knowledge graph and llm capabilities,"
AbstractView references

Knowledge base question answering (KBQA) is designed to respond to natural language inquiries by utilizing factual information, such as entities, relationships, and attributes, derived from a knowledge base (KB). The advent of large language models (LLMs) has significantly boosted the performance of KBQA, owing to their exceptional capabilities in content comprehension and generation. In this paper, we present a Knowledge Ocean enhanced Salary Analytics (KOSA) system based on knowledge graphs and LLMs tailored to employee salary data from a public university. This system encompasses an interactive conversational interface, visualization of knowledge graphs, and advanced data analysis. By employing the framework of knowledge engineering, we enable knowledge graph modeling, Cypher (the query engine of Neo4j) reasoning, and question answering functionalities. Furthermore, machine learning algorithms are integrated to facilitate advanced features, such as salary prediction and allocation. © 2023 IEEE.
"
10.1109/SIIE59826.2023.10423679,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186126724&origin=inward,Conference Paper,SCOPUS_ID:85186126724,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),artificial intelligence and academic research: an exploratory study with master's students,"
AbstractView references

The rapid evolution of the technological landscape challenges us to explore new ways of utilising technologies, including Generative Artificial Intelligence (AI). This article analyses how master's students perceive and apply AI in their academic research. The study, conducted through a questionnaire survey administered to students in a master's program in Communication at a Higher Education institution in Portugal, reveals that using AI tools in research activities is scarce or non-existent. Despite this prevailing trend, some students have already experimented with these tools in other contexts. The results highlight the opportunity and necessity to develop technological skills and literacy in students regarding using digital tools involving AI solutions without neglecting the importance of promoting critical thinking and recognising values associated with digital ethics.Further studies are anticipated in other Higher Education Institutions, along with periodic analysis to monitor changes in students' attitudes and practices regarding AI in academic research. It is essential to raise awareness about the potential and challenges associated with using AI to prepare students for integrating this technology into their future academic and professional careers. The study contributes to the initial understanding of the role of AI in academic research. It emphasises the importance of an educational approach encouraging responsible and ethical AI adoption. © 2023 IEEE.
"
10.1109/IEEM58616.2023.10406630,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85186094020&origin=inward,Conference Paper,SCOPUS_ID:85186094020,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),education and training for future engineering teachers in the age of artificial intelligence: a bibliometric analysis,"
AbstractView references

Engineering teachers play an important role in engineering education to develop the next generation of engineering human resources. In this sense, the education and training of engineering teachers are important. The recent development of artificial intelligence (AI), especially generative AI, has impacted many industries, including education. Thus, this paper aimed to explore existing research focuses and trends in the field of education and training of future engineering teachers in the age of artificial intelligence (AI). Based on scholarly publications obtained between the years 2018 and 2023, a bibliometric analysis has been performed, which includes analysis such as keyword co-occurrence analysis and thematic-based content analysis. The analysis in this paper is performed using bibliometric software named VOSViewer. The results from the analysis have identified five research hotspots in this field based on keyword clustering, with each hotspot discussed. Finally, this paper concludes with some elaborations on limitations and future work. © 2023 IEEE.
"
10.1109/ICSE-FoSE59343.2023.00008,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185604518&origin=inward,Conference Paper,SCOPUS_ID:85185604518,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),large language models for software engineering: survey and open problems,"
AbstractView references

This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE. © 2023 IEEE.
"
10.23919/ICMU58504.2023.10412254,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185551939&origin=inward,Conference Paper,SCOPUS_ID:85185551939,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),itochat: a mobile bot recommending engineering departments and open campus events,"
AbstractView references

Chatbots are widely used in education, medicine, business, and other fields, among which natural language processing such as ChatGPT, especially chatbots based on large-scale language models (LLMs), are expected to flexibly answer users' complex questions. In this study, we present iTOChat, a mobile bot that uses ChatGPT to recommend departments and open campus events to students visiting Kyushu University, an open campus of the School of Engineering, by inputting students' personal information such as hobbies and future plans. Although iTOChat is expected to make accurate recommendations to students, this is not easy because ChatGPT does not contain detailed information about engineering departments and open campus events. Therefore, we developed a method to supplement the missing information in ChatGPT by embedding reasons and conditions for recommending departments in the ChatGPT prompts. This paper describes the features and objectives of the developed system and evaluates the satisfaction of students who used iTOChat at an open campus. © 2023 IPSJ.
"
10.1109/ICDM58522.2023.00018,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185399070&origin=inward,Conference Paper,SCOPUS_ID:85185399070,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),beyond lexical consistency: preserving semantic consistency for program translation,"
AbstractView references

Program translation aims to convert the input programs from one programming language to another. Automatic program translation is a prized target of software engineering research, which leverages the reusability of projects and improves the efficiency of development. Recently, thanks to the rapid development of deep learning model architectures and the availability of large-scale parallel corpus of programs, the performance of program translation has been greatly improved. However, the existing program translation models are still far from satisfactory, in terms of the quality of translated programs. In this paper, we argue that a major limitation of the current approaches is the lack of consideration of semantic consistency. Beyond lexical consistency, semantic consistency is also critical for the task. To make the program translation model more semantically aware, we propose a general framework named Preserving Semantic Consistency for Program Translation (PSCPT), which considers semantic consistency with regularization in the training objective of program translation and can be easily applied to all encoder-decoder methods with various neural networks (e.g., LSTM, Transformer) as the backbone. We conduct extensive experiments in 7 general programming languages. Experimental results show that with CodeBERT as the backbone, our approach outperforms not only the state-of-the-art open-source models but also the commercial closed large language models (e.g., textdavinci-002, text-davinci-003) on the program translation task. Our replication package (including code, data, etc.) is publicly available at https://github.com/duyali2000/PSCPT. © 2023 IEEE.
"
10.1115/IMECE2023-112683,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185393784&origin=inward,Conference Paper,SCOPUS_ID:85185393784,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),implementation of an artificial intelligence (ai) instructional support system in a virtual reality (vr) thermal-fluids laboratory,"
AbstractView references

Physical laboratory experiments have long been the cornerstone of higher education, providing future engineers practical real-life experience invaluable to their careers. However, demand for laboratory time has exceeded physical capabilities. Virtual reality (VR) labs have proven to retain many benefits of attending physical labs while also providing significant advantages only available in a VR environment. Previously, our group had developed a pilot VR lab that replicated six (6) unique thermal-fluids lab experiments developed using the Unity game engine. One of the VR labs was tested in a thermal-fluid mechanics laboratory class with favorable results, but students highlighted the need for additional assistance within the VR simulation. In response to this testing, we have incorporated an artificial intelligence (AI) assistant to aid students within the VR environment by developing an interaction model. Utilizing the Generative Pre-trained Transformer 4 (GPT-4) large language model (LLM) and augmented context retrieval, the AI assistant can provide reliable instruction and troubleshoot errors while students conduct the lab procedure to provide an experience similar to a real-life lab assistant. The updated VR lab was tested in two laboratory classes and while the overall tone of student response to an AI-powered assistant was excitement and enthusiasm, observations and other recorded data show that students are currently unsure of how to utilize this new technology, which will help guide future refinement of AI components within the VR environment. © 2023 by ASME.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185226875&origin=inward,Conference Paper,SCOPUS_ID:85185226875,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),human-in-the-loop machine translation with large language model,"
AbstractView references

The large language model (LLM) has garnered significant attention due to its in-context learning mechanisms and emergent capabilities. The research community has conducted several pilot studies to apply LLMs to machine translation tasks and evaluate their performance from diverse perspectives. However, previous research has primarily focused on the LLM itself and has not explored human intervention in the inference process of LLM. The characteristics of LLM, such as in-context learning and prompt engineering, closely mirror human cognitive abilities in language tasks, offering an intuitive solution for human-in-the-loop generation. In this study, we propose a human-in-the-loop pipeline that guides LLMs to produce customized outputs with revision instructions. The pipeline initiates by prompting the LLM to produce a draft translation, followed by the utilization of automatic retrieval or human feedback as supervision signals to enhance the LLM's translation through in-context learning. The human-machine interactions generated in this pipeline are also stored in an external database to expand the in-context retrieval database, enabling us to leverage human supervision in an offline setting. We evaluate the proposed pipeline using the GPT-3.5-turbo API on five domain-specific benchmarks for German-English translation. The results demonstrate the effectiveness of the pipeline in tailoring in-domain translations and improving translation performance compared to direct translation instructions. Additionally, we discuss the experimental results from the following perspectives: 1) the effectiveness of different in-context retrieval methods; 2) the construction of a retrieval database under low-resource scenarios; 3) the observed differences across selected domains; 4) the quantitative analysis of sentence-level and word-level statistics; and 5) the qualitative analysis of representative translation cases. © 2023 The authors. This article is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0)
"
10.1109/NTCI60157.2023.10403750,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185221040&origin=inward,Conference Paper,SCOPUS_ID:85185221040,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),boosting few-shot remote sensing image scene classification with language-guided multimodal prompt tuning,"
AbstractView references

Remote sensing image Scene classification is an important research topic in remote sensing community and has evoked a growing concern with the recent development of deep learning techniques. However, the requirement of a large amount of annotations brings great challenges to deep learning-based scene classification approaches. Visual-linguistic pretraining models, which improve the transferability of visual models using the supervision information of text, create a new way for the task under label scarcity scenario. In this paper, we explore the novel approach of prompt engineering, aiming to achieve satisfactory performance of multi-modal pretraining models on downstream remote sensing image scene classification task with minimal amounts of training data. Experiments were conducted on multiple publicly available datasets. The results indicate that training the learnable prompts with a small number of samples can yield impressive results, surpassing the few-shot transfer learning results of the best-performing pre-trained models. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185220536&origin=inward,Conference Paper,SCOPUS_ID:85185220536,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"artificial intelligence in safety, the future or a recipe for disaster?","
AbstractView references

Recent advances in Artificial Intelligence (AI) have led to the development of Large-Language Models (LLMs) that are now widely available to the general public in the form of AI chatbots, such as ChatGPT. Due to these advancements and increased availability, LLMs have been implemented in many tools to assist people in complex tasks or to automate simpler tasks. Can the Process Safety industry reap the benefits of AI in this form to improve safety, or is this a recipe for disaster? The potential benefits AI could bring to the safety industry are numerous, reducing the time taken, improving quality, and improving the consistency of risk assessments. However, increasing reliance on AI may decrease our own knowledge, negatively impact our decision-making, and ultimately displace jobs. The inherent limitations of LLMs also present their own challenges. One such challenge is their ability to produce convincing but factually incorrect responses when posed with a complex task. Despite their ability to perform well at single-turn tasks, for more complex tasks, they can hallucinate or confabulate these convincing but incorrect responses. In our research, we used an LLM to create an AI copilot for Hazard and Operability studies. Utilising prompt engineering we were able to achieve vastly improved results when compared to generic AI chatbots and LLMs without prompt engineering. We discuss specific learnings from our research and the wider learnings that can be applied to the use of LLMs in process safety. © 2023 IChemE.
"
10.1109/TALE56641.2023.10398370,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185004595&origin=inward,Conference Paper,SCOPUS_ID:85185004595,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring the performance of generative ai tools in electrical engineering education,"
AbstractView references

This paper explores the use cases and performance of generative artificial intelligence (AI) tools in the context of Electrical Engineering education. A summary of commonly used generative AI tools in Engineering education is provided along with a review of their use cases from recent literature. The performances of the generative AI tools GPT-3.5, GPT-4 and Google Bard are examined using custom-made question sets in two representative courses: Digital Logic Design and Digital Signal Processing. The paper highlights that GPT4 achieves over 50% accuracy in both courses and generative AI tools are more effective at solving standalone concept questions than analytical or numerical questions. © 2023 IEEE.
"
10.1109/TALE56641.2023.10398404,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85185001922&origin=inward,Conference Paper,SCOPUS_ID:85185001922,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated essay grading of constructive response test responses for mechanical engineering students,"
AbstractView references

Machine learning education related applications have increased with the appearance of large language models. While automatic essay grading (AEG) has been studied extensively in the past, most of these studies have focused on evaluating English competence instead of assessing knowledge competence in an engineering field. This study aimed to develop an AEG model to evaluate student's mechanical engineering Constructive Response Test (CRT) question responses which were instructor graded. Because of the small number of student responses (45), a synthesized set of responses was also generated by using text-To-Text paraphrasing models. A neural network grading engine was built and trained to assess comprehension utilizing the Bidirectional Encoder Representation Transformer (BERT) and related models on student and synthesized responses. This study showed that the AEG based Natural Language Processing (NLP) model showed high accuracy and a higher degree of consistency in grading student responses compared to instructor-graded responses. © 2023 IEEE.
"
10.26615/978-954-452-091-5_004,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184997465&origin=inward,Conference Paper,SCOPUS_ID:85184997465,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),augmented machine translation enabled by gpt4: performance evaluation on human-machine teaming approaches,"
AbstractView references

Translation has been modeled as a multiple-phase process where pre-editing analyses guide meaning transfer and interlingual restructure. Present-day machine translation (MT) tools provide no means for source text analyses. Generative AI with Large language modeling (LLM), equipped with prompt engineering and fine-tuning capabilities, can enable augmented MT solutions by explicitly including AI or human generated analyses/instruction, and/or human-generated reference translation as pre-editing or interactive inputs. Using an English-to-Chinese translation piece that had been carefully studied during a translator slam event, Fourt types of translation outputs on 20 text segments were evaluated: human-generated translation, Google Translate MT, instruction-augmented MT using GPT4-LLM, and Human-Machine-Teaming (HMT)-augmented translation based on both human reference translation and instruction using GPT4-LLM. While human translation had the best performance, both augmented MT approaches performed better than un-augmented MT. The HMT-augmented MT performed better than instruction-augmented MT because it combined the guidance and knowledge provided by both human reference translation and style instruction. However, since it is unrealistic to generate sentence-by-sentence human translation as MT input, better approaches to HMT-augmented MT need to be invented. The evaluation showed that generative AI with LLM can enable new MT workflow facilitating pre-editing analyses and interactive restructuring and achieving better performance. © NLP4TIA 2023 - 1st Workshop on Natural Language Processing Tools and Resources for Translation and Interpreting Applications, associated with the 14th International Conference on Recent Advances in Natural Language Processing, RANLP 2023 - Proceedings.
"
10.1109/TALE56641.2023.10398358,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184991575&origin=inward,Conference Paper,SCOPUS_ID:85184991575,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),leveraging chatgpt to enhance computational thinking learning experiences,"
AbstractView references

Given the pervasive reliance on technology in modern society, teaching Computational Thinking (CT) abilities is becoming increasingly relevant. These abilities, such as modeling and coding, have become crucial for a larger audience of students, not only those who wish to become software engineers or computer scientists. Recent advances in Large Language Models (LLMs), such as ChatGPT, provide powerful assistance to complete computational tasks, by simplifying code generation and debugging, and potentially enhancing interactive learning. However, it is not clear if these advances make CT tasks more accessible and inclusive for all students, or if they further contribute to a digital skills divide, favoring the top students. To address this gap, we have created and evaluated a novel learning scenario for transversal CT skills that leveraged LLMs as assistants. We conducted an exploratory field study during the spring semester of 2022, to assess the effectiveness and user experience of LLM-Augmented learning. Our results indicate that the usage of ChatGPT as a learning assistant improves learning outcomes. Furthermore, contrary to our predictions, the usage of ChatGPT by students does not depend on prior CT capabilities and as such does not seem to exacerbate prior inequalities. © 2023 IEEE.
"
10.1109/ICNGN59831.2023.10396810,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184991119&origin=inward,Conference Paper,SCOPUS_ID:85184991119,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),empirical evaluation of chatgpt on requirements information retrieval under zero-shot setting,"
AbstractView references

Recently, various illustrative examples have shown the impressive ability of generative large language models (LLMs) to perform NLP related tasks. ChatGPT undoubtedly is the most representative model. We empirically evaluate ChatGPT's performance on requirements information retrieval (IR) tasks to derive insights into designing or developing more effective requirements retrieval methods or tools based on generative LLMs. We design an evaluation framework considering four different combinations of two popular IR tasks and two common artifact types. Under zero-shot setting, evaluation results reveal ChatGPT's promising ability to retrieve requirements relevant information (high recall) and limited ability to retrieve more specific requirements information (low precision). Our evaluation of ChatGPT on requirements IR under zero-shot setting provides preliminary evidence for designing or developing more effective requirements IR methods or tools based on generative LLMs. © 2023 IEEE.
"
10.1109/BigData59044.2023.10416943,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184988271&origin=inward,Conference Paper,SCOPUS_ID:85184988271,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),character-based outfit generation with vision-augmented style extraction via llms,"
AbstractView references

The outfit generation problem involves recommending a complete outfit to a user based on their interests. Existing approaches focus on recommending items based on anchor items or specific query styles but do not consider customer interests in famous characters from movie, social media, etc. In this paper, we define a new Character-based Outfit Generation (COG) problem, designed to accurately interpret character information and generate complete outfit sets according to customer specifications such as age and gender. To tackle this problem, we propose a novel framework LVA-COG that leverages Large Language Models (LLMs) to extract insights from customer interests (e.g., character information) and employ prompt engineering techniques for accurate understanding of customer preferences. Additionally, we incorporate text-to-image models to enhance the visual understanding and generation (factual or counterfactual) of cohesive outfits. Our framework integrates LLMs with text-to-image models and improves the customer's approach to fashion by generating personalized recommendations. With experiments and case studies, we demonstrate the effectiveness of our solution from multiple dimensions. © 2023 IEEE.
"
10.1109/BigData59044.2023.10386929,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184985081&origin=inward,Conference Paper,SCOPUS_ID:85184985081,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),can gpt-4 think computationally about digital archival practices?,"
AbstractView references

This paper describes an investigation of GPT-4's knowledge in some areas of archival practice, and its ability to think computationally about archival tasks. It is demonstrated that GPT-4 has shown an understanding of ten among the twenty-two distinct forms of computational thinking. When GPT-4 is combined with plugins, it is able to apply some of these methods and tools to digital archival tasks. © 2023 IEEE.
"
10.1109/BigData59044.2023.10386814,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184979708&origin=inward,Conference Paper,SCOPUS_ID:85184979708,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploiting large language models (llms) through deception techniques and persuasion principles,"
AbstractView references

With the recent advent of Large Language Models (LLMs), such as ChatGPT from OpenAI, BARD from Google, Llama2 from Meta, and Claude from Anthropic AI, gain widespread use, ensuring their security and robustness is critical. The widespread use of these language models heavily relies on their reliability and proper usage of this fascinating technology. It is crucial to thoroughly test these models to not only ensure its quality but also possible misuses of such models by potential adversaries for illegal activities such as hacking. This paper presents a novel study focusing on exploitation of such large language models against deceptive interactions. More specifically, the paper leverages widespread and borrows well-known techniques in deception theory to investigate whether these models are susceptible to deceitful interactions. This research aims not only to highlight these risks but also to pave the way for robust countermeasures that enhance the security and integrity of language models in the face of sophisticated social engineering tactics. Through systematic experiments and analysis, we assess their performance in these critical security domains. Our results demonstrate a significant finding in that these large language models are susceptible to deception and social engineering attacks. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184856950&origin=inward,Conference Paper,SCOPUS_ID:85184856950,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),argument detection in student essays under resource constraints,"
AbstractView references

Learning to make effective arguments is vital for the development of critical-thinking in students and, hence, for their academic and career success. Detecting argument components is crucial for developing systems that assess students' ability to develop arguments. Traditionally, supervised learning has been used for this task, but this requires a large corpus of reliable training examples which are often impractical to obtain for student writing. Large language models have also been shown to be effective few-shot learners, making them suitable for low-resource argument detection. However, concerns such as latency, service reliability, and data privacy might hinder their practical applicability. To address these challenges, we present a low-resource classification approach that combines the intrinsic entailment relationship among the argument elements with a parameter-efficient prompt-tuning strategy. Experimental results demonstrate the effectiveness of our method in reducing the data and computation requirements of training an argument detection model without compromising the prediction accuracy. This suggests the practical applicability of our model across a variety of real-world settings, facilitating broader access to argument classification for researchers spanning various domains and problem scenarios. © 2023 Association for Computational Linguistics.
"
10.1109/CIYCEE59789.2023.10401511,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184856713&origin=inward,Conference Paper,SCOPUS_ID:85184856713,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research on the substation alarm event model based on natural language parsingtechnology,"
AbstractView references

In view of the current low efficiency of manual processing of massive monitoring alarm information and the need for deepening the application of power grid intelligence technology, an autonomous identification method of power grid equipment operation and maintenance alarm events based on natural language processing technology is proposed, which integrates neural network and unsupervised learning. The text of substation equipment alarm signal is vectorized based on word2vec algorithm, the time-density correlation between multiple alarm signals is established based on DBSCAN algorithm, and the ""eventalization""model of alarm signal sequence is constructed based on TF-IDF algorithm. This paper proposes an application method based on natural language processing technology combining neural network and unsupervised learning algorithm to screen key ""eventalization""alarms from a large number of discrete alarms, so as to realize the response efficiency and reliable identification of power grid monitoring alarm events. © 2023 IEEE.
"
10.1109/ICID60307.2023.10396804,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184855446&origin=inward,Conference Paper,SCOPUS_ID:85184855446,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a new art design method based on aigc: analysis from the perspective of creation efficiency,"
AbstractView references

With the development of ""AI-generated content(AI GC)""technology, the field of art and design is witnessing a new transformation. Along with applying AI technologies such as generative adversarial networks, diffusion models, and transformer architecture in the visual text filed, art design has transformed from the traditional personal inspiration-centered creation model in to an AI large model-driven creation model. Starting from the efficiency of art design creation, this paper proposes a new ""text-image""iterative art creation method based on prompt word engineering. It introduces the specific process and steps of art creation through AIGC tools. At the same time, questionnaires, semi-structured interviews, and other methods to quantitatively analyze the creative methods of teachers and design students. However, the results of the AIGC tools in deep cultural semantic understanding are poor and must be further supplemented manually. Given the above research, this paper further elaborates on the subsequent development direction of AIGC tools, which should take dynamic real-time interaction as the core. Multi-modal domain information should be integrated and embedded into the creation process, which will realize the dynamic creation and improve the quality of works. © 2023 IEEE.
"
10.1109/ICOCO59262.2023.10397642,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184849676&origin=inward,Conference Paper,SCOPUS_ID:85184849676,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research and application of gpt-based large language models in business and economics: a systematic literature review in progress,"
AbstractView references

Represented by ChatGPT and GPT-4, Large Language Models (LLM) based on the Generative Pre-trained Transformer (GPT) have revolutionized the capability of Artificial Intelligence (AI) in natural language processing. In the fields of business and economics, large amounts of research and applications of GPT-based LLMs have been developed and published to automate tasks that mandate advanced human-machine interaction. Nevertheless, there has not been a systematic literature review on GPT-based LLMs in business and economics. To fill this gap, we present our in-progress literature review in this paper focusing on these two related fields. This paper analyzed 30 published research articles and delineated the trends in research, application, prompt engineering and ethical considerations. Our goal is to provide a research framework as well as an application guideline for the fast-growing audience of GPT and LLMs in business and economics. Results of the literature review indicate that many studies are: (1) engaged in creating new applications of GPT-LLM; (2) empirical-qualitative research based on evidenced-oriented data sources; (3) applying diverse methods of prompt engineering; (4) concerned about ethical challenges of GPT-based LLMs. © 2023 IEEE.
"
10.1109/CVR58941.2023.10395520,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184849596&origin=inward,Conference Paper,SCOPUS_ID:85184849596,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),structured template language and generative ai driven content management for personalized workspace reconfiguration,"
AbstractView references

This work presents a systematic approach towards personalized workspace management and reconfiguration in 3D Virtual Reality (VR) spaces, focusing on a structured template language for defining and manipulating content layout schemas, as well as a generative AI supported content management solution. Recognizing the varying requirements of different tasks and workflows, on the one hand we propose a hierarchical template language that enables, through simple steps, the adaptation of number and variety of documents within geometric layout schemas in digital 3D spaces. In the second half of the paper, we present a generative AI driven framework for integrating associative-semantic content management into such workspaces, thereby enhancing the relevance and contextuality of workspace configurations. The proposed approach aids in identifying content that is semantically linked to a given spatial, temporal and topical environment, enabling workspace designers and users to create more efficient and personalized workspace layouts. © 2023 IEEE.
"
10.1007/978-3-031-48041-6_29,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184839481&origin=inward,Conference Paper,SCOPUS_ID:85184839481,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),bridging the communication rate gap: enhancing text input for augmentative and alternative communication (aac),"
AbstractView references

Over 70 million people worldwide face communication difficulties, with many using augmentative and alternative communication (AAC) technology. While AAC systems help improve interaction, the communication rate gap between individuals with and without speaking difficulties remains significant, and this has led to a low sustained use of AAC systems. The study reported here combines human computer interaction (HCI) and language modelling techniques to improve the ease of use, user satisfaction, and communication rates of AAC technology in open-domain interactions. A text input interface utilising word prediction based on BERT and RoBERTa language models has been investigated with a view to improving communication rates. Three interface layouts were implemented, and it was found that a radial configuration was the most efficient. RoBERTa models fine-tuned on conversational AAC corpora led to the highest communication rates of 25.75 words per minute (WPM), with alphabetical ordering preferred over probabilistic ordering. It was also found that training on conversational corpora such as TV and Reddit outperformed training based on generic corpora such as COCA or Wikipedia. Hence, it is concluded that the limited availability of large-scale conversational AAC corpora represent a key challenge for improving communication rates and robust AAC systems. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184819932&origin=inward,Conference Paper,SCOPUS_ID:85184819932,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),nl2tl: transforming natural languages to temporal logics using large language models,"
AbstractView references

Temporal Logic (TL) can be used to rigorously specify complex high-level specification for systems in many engineering applications. The translation between natural language (NL) and TL has been under-explored due to the lack of dataset and generalizable model across different application domains. In this paper, we propose an accurate and generalizable transformation framework of English instructions from NL to TL, exploring the use of Large Language Models (LLMs) at multiple stages. Our contributions are twofold. First, we develop a framework to create a dataset of NL-TL pairs combining LLMs and human annotation. We publish a dataset with 28K NL-TL pairs. Then, we finetune T5 models on the lifted versions (i.e., the specific Atomic Propositions (AP) are hidden) of the NL and TL. The enhanced generalizability originates from two aspects: 1) Usage of lifted NL-TL characterizes common logical structures, without constraints of specific domains. 2) Application of LLMs in dataset creation largely enhances corpus richness. We test the generalization of trained models on five varied domains. To achieve full NL-TL transformation, we either combine the lifted model with AP recognition task or do the further finetuning on each specific domain. During the further finetuning, our model achieves higher accuracy (>95%) using only <10% training data, compared with the baseline sequence to sequence (Seq2Seq) model. ©2023 Association for Computational Linguistics.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184809384&origin=inward,Conference Paper,SCOPUS_ID:85184809384,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automatic task-level thinking steps help large language models for challenging classification task,"
AbstractView references

Large language models (LLMs) have shown incredible performance on many tasks such as dialogue generation, commonsense reasoning and question answering. In-context learning (ICL) is an important paradigm for adapting LLMs to the downstream tasks by prompting few demonstrations. However, the distribution of demonstrations can severely affect the performance, especially for challenging classification tasks. In this paper, we propose the concept of task-level thinking steps that can eliminate bias introduced by demonstrations. Further, to help LLMs distinguish confusing classes, we design a progressive revision framework, which can improve the thinking steps by correcting hard demonstrations. Experimental results prove the superiority of our proposed method, achieving best performance on three kinds of challenging classification tasks in the zero-shot and few-shot settings. Besides, with task-level thinking steps, automatically generated chain-of-thoughts (CoTs) bring more competitive performance. ©2023 Association for Computational Linguistics.
"
10.1109/ISAS60782.2023.10391517,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184809261&origin=inward,Conference Paper,SCOPUS_ID:85184809261,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),implementing generative ai and large language models in education,"
AbstractView references

The recent advancements in Generative AI have been highlighted by the emergence of Large Language Models (LLMs) like ChatGPT. We track this evolution from the initial recurrent neural networks to the development of architectures like Transformers and Generative Pre-trained Transformers (GPT). ChatGPT, with its impressive ability to comprehend, process, and produce natural language, has piqued the interest of educators, students, and institutions within the education sector through its creation of high-quality textual responses.This marks the beginning of a new era in educational possibilities: we emphasize the beneficial effects of ChatGPT in learning environments, noting its utility in programming assistance, its clarity in concept explanation, and its role in enhancing automated learning processes. We also recognize potential drawbacks, including the risks of over-reliance, plagiarism, and the inherent constraints of these models in tackling mathematical and linguistic problem-solving tasks.In exploring the Paradox of Automation, we examine the implications of an over-reliance on AI in education. We seek to understand the importance of preserving critical thinking skills and ensuring that technology serves as a tool for augmenting human capabilities rather than supplanting them. Our analysis acknowledges that, while AI, including ChatGPT, can assist in content generation and problem-solving, it is essential for students to cultivate their abilities in analytical thinking, content verification, and error correction. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184809158&origin=inward,Conference Paper,SCOPUS_ID:85184809158,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),beyond factuality: a comprehensive evaluation of large language models as knowledge generators,"
AbstractView references

Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. Yet, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives - Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely-studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research. © 2023 Association for Computational Linguistics.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184808149&origin=inward,Conference Paper,SCOPUS_ID:85184808149,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),brainteaser: lateral thinking puzzles for large language models,"
AbstractView references

The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BRAINTEASER: a multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of reconstruction examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across reconstruction formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models. ©2023 Association for Computational Linguistics.
"
10.1109/ICACTA58201.2023.10392861,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184801742&origin=inward,Conference Paper,SCOPUS_ID:85184801742,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),nlsql: generating and executing sql queries via natural language using large language models,"
AbstractView references

The goal of data management has long been to make data more approachable for non-technical users. Natural language interfaces (NLIs), which let users interact with data by asking questions in natural language, have become more popular in recent years. Natural language is often vague, and user queries often don't give enough information. This makes it hard to use NLIs for database querying. In order to solve this problem, this study suggests using large language models (LLMs) to turn free-form natural language queries into SQL statements that can then be run on databases. The proposed system, NLSQL, makes use of LLMs like GPT-3 and shows how efficiently prompt engineering can be done in order to extract from LLMs the desired code for SQL queries. NLSQL shows that using pre-trained LLMs along with the suggested priming prompts is an accurate and reliable way to create and run SQL queries in natural language, even if the queries aren't very well written or are missing important information. Unlike traditional methods, which involve making grammar rules by hand, this method improves query inference and makes the development of NLIs faster and cheaper. The study shows that the suggested method is safe, protects privacy, and can be used with many different databases. If NLSQL is used to make databases easier for people who aren't tech-savvy to use, they may not need as much training in SQL and related technologies, which would save a company both time and money. © 2023 IEEE.
"
10.1109/ASRU57964.2023.10389637,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184661469&origin=inward,Conference Paper,SCOPUS_ID:85184661469,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),contextual spelling correction with large language models,"
AbstractView references

Contextual Spelling Correction (CSC) models are used to improve automatic speech recognition (ASR) quality given userspecific context. Typically, context is modeled as a large set of text spans to compare against a given ASR hypothesis using some distance measure (text, phonetic, or neural embedding). In this work we propose a CSC system based on a single Large Language Model (LLM) adapted with prompt tuning. Our approach is shown to be data efficient, and does not require dedicated serving. Our system exhibits advanced contextualization capabilities, such as support for phonetic spellings, cross-lingual scripts, and context specified as topics, with little to no data engineering. On voice assistant datasets, our system achieves 7.8% absolute word error rate reduction from a reference ASR system with relevant context and improving upon other contextualization solutions. Finally, we test our system in a prompt-injection attack scenario and report vulnerabilities and mitigations. © 2023 IEEE.
"
10.1016/j.procir.2023.09.144,S2212827123008764,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184610381&origin=inward,Conference Paper,SCOPUS_ID:85184610381,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design of matrix production systems: a skill-based systems engineering approach,"A high changeability of value creation enables manufacturing companies to constantly adapt to new situations in Industry 4.0 such as changing product portfolios or supply chain disruptions. Matrix manufacturing systems provide a high changeability of value creation enabled by individually plannable process modules, interconnected through a flexible material flow. Higher maturity levels of matrix manufacturing systems integrate transportation and value creation tasks and are facilitated by location-independent equipment that merges into new resources according to dynamically changing manufacturing demands, i.e., a fluid production. The efficacious design of matrix manufacturing systems is still a complex challenge for research and practice. For this purpose, the authors propose a novel design approach specifically scoped for matrix manufacturing systems. This approach aims at offering a more practical guideline for the design of matrix manufacturing systems. Skill-based systems engineering provides the methodological foundation. The application opportunity for novel generative AI tools is exemplified. An experiment, addressing the manufacturing of a toy action figure, validates and improves the conceptualized design approach."
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184516109&origin=inward,Conference Paper,SCOPUS_ID:85184516109,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),walking a tightrope - evaluating large language models in high-risk domains,"
AbstractView references

High-risk domains pose unique challenges that require language models to provide accurate and safe responses. Despite the great success of large language models (LLMs), such as Chat-GPT and its variants, their performance in high-risk domains remains unclear. Our study delves into an in-depth analysis of the performance of instruction-tuned LLMs, focusing on factual accuracy and safety adherence. To comprehensively assess the capabilities of LLMs, we conduct experiments on six NLP datasets including question answering and summarization tasks within two high-risk domains: legal and medical. Further qualitative analysis highlights the existing limitations inherent in current LLMs when evaluating in high-risk domains. This underscores the essential nature of not only improving LLM capabilities but also prioritizing the refinement of domain-specific metrics, and embracing a more human-centric approach to enhance safety and factual reliability. Our findings advance the field toward the concerns of properly evaluating LLMs in high-risk domains, aiming to steer the adaptability of LLMs in fulfilling societal obligations and aligning with forthcoming regulations, such as the EU AI Act. © 2023 Association for Computational Linguistics.
"
10.13182/PSA23-41333,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184349652&origin=inward,Conference Paper,SCOPUS_ID:85184349652,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),verification and validation approach for use of data analytics in nuclear power plants,"
AbstractView references

The operating procedures at nuclear power plants (NPP) generate a large amount of data which describe the events and operating conditions at the NPPs. Each record is reviewed by plant personnel to identify actionable situations, assess trends, and inform decision making. In addition to supporting the upkeep and smooth operation of plant equipment and procedures, these data provide documentation of events which can be reviewed to assess regulatory compliance. This screening process is a significant cost for NPPs, with an average of 2,500-5,000 records generated per unit per year at a two unit plant and an average of 1-2 labor hours per record. Use of modern data analytics (DA) techniques have the potential to significantly reduce the burden of these reviews on NPP staff. Commercial NPPs make widespread use of software to support all aspects of plant operation. In the United States (U.S.), regulatory review and approval of the analytical methods, models, and implementing software are required for applications that can directly impact plant safety. For these applications, the Nuclear Regulatory Commission (NRC) has issued guidance to facilitate regulatory review and acceptance of the verification and validation (V&V) basis of software models. However, this regulatory guidance is focused on probabilistic risk assessment and physics-based models with corresponding guidance for the application of DA not developed. Data analytics involves the systematic and comprehensive processing of data to identify features and trends and to effectively and efficiently inform decision m aking. In contexts with great volume and frequency of data, the use of software allows for the implementation of various algorithms and models that would not be feasible if manually applied. Under the broad domain of Artificial Intelligence (AI) and DA exist several algorithms and modeling approaches including but not limited to natural language processing, machine learning, and process automation. NPPs in the U.S. are currently investing in and deploying DA software to streamline plant processes. However, there is a gap in the regulatory guidance and industry standards related to the acceptance of these software models. This is a recognized omission by the NRC which has recently identified V&V of AI applications as part of the near-term Strategic Plan [1]. This paper provides an overview of the V&V implemented for the DataAdvisrTM platform deployed at Constellation Nuclear to streamline the decisions made in the Corrective Action Program (CAP) and New Work Screening (NWS). © 2023 Proceedings of 18th International Probabilistic Safety Assessment and Analysis, PSA 2023. All Rights Reserved.
"
10.14742/ajet.8825,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184221747&origin=inward,Article,SCOPUS_ID:85184221747,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),digital transformation in engineering education: exploring the potential of ai-assisted learning,"
AbstractView references

This research explored the potential of artificial intelligence (AI)-assisted learning using ChatGPT in an engineering course at a university in South-east Asia. The study investigated the benefits and challenges that students may encounter when utilising ChatGPT-3.5 as a learning tool. This research developed an AI-assisted learning flow that empowers learners and lecturers to integrate ChatGPT into their teaching and learning processes. The flow was subsequently used to validate and assess a variety of exercises, tutorial tasks and assessment-like questions for the course under study. Introducing a self-rating system allowed the study to facilitate users in assessing the generative responses. The findings indicate that ChatGPT has significant potential to assist students; however, there is a necessity for training and offering guidance to students on effective interactions with ChatGPT. The study contributes to the evidence of the potential of AI-assisted learning and identifies areas for future research in refining the use of AI tools to better support students' educational journey. Implications for practice or policy • Educators and administrators could review the usage of ChatGPT in an engineering technology course and study the implications of generative AI tools in higher education. • Academics could adapt and modify the proposed AI-assisted learning flow in this paper to suit their classroom. • Students can review and adopt the proposed AI-assisted learning flow in this paper for their studies. • Researchers could follow up on the application of ChatGPT in teaching and learning: teaching quality and student experience, academic integrity and assessment design. © Articles published in the Australasian Journal of Educational Technology (AJET) are available under Creative Commons Attribution Non-Commercial No Derivatives Licence (CC BY-NC-ND 4.0). Authors retain copyright in their work and grant AJET right of first publication under CC BY-NC-ND 4.0.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184108690&origin=inward,Conference Paper,SCOPUS_ID:85184108690,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),beautifulprompt: towards automatic prompt engineering for text-to-image synthesis,"
AbstractView references

Recently, diffusion-based deep generative models (e.g., Stable Diffusion) have shown impressive results in text-to-image synthesis. However, current text-to-image models often require multiple passes of prompt engineering by humans in order to produce satisfactory results for real-world applications. We propose BeautifulPrompt, a deep generative model to produce high-quality prompts from very simple raw descriptions, which enables diffusion-based models to generate more beautiful images. In our work, we first fine-tuned the BeautifulPrompt model over low-quality and high-quality collecting prompt pairs. Then, to ensure that our generated prompts can generate more beautiful images, we further propose a Reinforcement Learning with Visual AI Feedback technique to fine-tune our model to maximize the reward values of the generated prompts, where the reward values are calculated based on the PickScore and the Aesthetic Scores. Our results demonstrate that learning from visual AI feedback promises the potential to improve the quality of generated prompts and images significantly. We further showcase the integration of BeautifulPrompt to a cloud-native AI platform to provide better text-to-image generation service in the cloud. © 2023 Association for Computational Linguistics.
"
10.1109/CoNTESA61248.2023.10384921,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85184083333&origin=inward,Conference Paper,SCOPUS_ID:85184083333,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),teaching information literacy and critical thinking skills in chat gpt time,"
AbstractView references

In this article, the unprecedented popularity of Chat GPT is acknowledged, generating positive interest but also raising concerns about AI's potential to replace humans. Despite factual mistakes and hallucinations, Chat GPT's enduring presence is emphasized, acknowledged with the record-breaking sales growth and Generative AI market projected almost 50% YoY growth. Amid controversies, particularly in education with New York City Public Schools banning its use due to concerns over cheating and setback of critical thinking and slowdown of problem-solving skills development, the author advocates for incorporating Chat GPT into the learning process. The article proposes an 'upgrade' of a Class designed in 2016, reinforcing the usage of mental models as the cognitive framework for effective prompt formulation. By effectively using Chat GPT in their Class assignment students will master its use for any given task, simultaneously enforcing generic skills needed for successful study and easier employment. © 2023 IEEE.
"
10.1109/TechDefense59795.2023.10380937,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183909432&origin=inward,Conference Paper,SCOPUS_ID:85183909432,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),boosting the automated information processing for reconnaissance,"
AbstractView references

The emerging complexities due to an increasing availability of data sources and advanced technologies for data and information processing in the defense sector require careful considerations. In this study, we looked at the different aspects related to the exploitation of the advancing technologies for the reconnaissance operations. Specifically, system architecture, data fusion, human in the loop and knowledge modeling are explored, with an exemplary case of Large Language Models (LLM) on reconnaissance data is shown. © 2023 IEEE.
"
10.3389/feduc.2023.1330486,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183909215&origin=inward,Article,SCOPUS_ID:85183909215,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),examining the potential and pitfalls of chatgpt in science and engineering problem-solving,"
AbstractView references

The study explores the capabilities of OpenAI's ChatGPT in solving different types of physics problems. ChatGPT (with GPT-4) was queried to solve a total of 40 problems from a college-level engineering physics course. These problems ranged from well-specified problems, where all data required for solving the problem was provided, to under-specified, real-world problems where not all necessary data were given. Our findings show that ChatGPT could successfully solve 62.5% of the well-specified problems, but its accuracy drops to 8.3% for under-specified problems. Analysis of the model's incorrect solutions revealed three distinct failure modes: (1) failure to construct accurate models of the physical world, (2) failure to make reasonable assumptions about missing data, and (3) calculation errors. The study offers implications for how to leverage LLM-augmented instructional materials to enhance STEM education. The insights also contribute to the broader discourse on AI's strengths and limitations, serving both educators aiming to leverage the technology and researchers investigating human-AI collaboration frameworks for problem-solving and decision-making. Copyright © 2024 Wang, Burkholder, Wieman, Salehi and Haber.
"
10.3991/IJIM.V17I24.44263,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183846791&origin=inward,Article,SCOPUS_ID:85183846791,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),utilizing m-technologies for ai-driven career guidance in morocco: an innovative mobile approach,"
AbstractView references

In today's interconnected world, the significance of effective career guidance has been magnified. With the advent of mobile technologies, e-orientation and artificial intelligence (AI)-orientation systems offer a promising avenue for personalized career guidance. This paper delves into the potential of transitioning from traditional e-orientation to advanced AI-orientation systems in Morocco by employing large language models (LLMs) such as LLAMA2, GPT, and PaLM. These LLMs, renowned for their human-like text generation and contextual understanding, are proposed as the backbone for AI chatbots that can serve as virtual career counselors. Accessible via mobile platforms, these mobile chatbot interfaces can provide real-time insights on career paths, educational prerequisites, and job market dynamics and outlooks. Despite challenges such as Internet reliability, data privacy, and legislative regulations, the integration of AI-orientated systems into mobile platforms can revolutionize career guidance for Moroccan students. This paper presents a detailed roadmap and implementation for embedding these innovative technologies into Morocco's educational framework. © (2023) by the authors of this article. Published under CC-BY.
"
10.1109/AIKE59827.2023.00017,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183594313&origin=inward,Conference Paper,SCOPUS_ID:85183594313,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompt recommendations for ai art,"
AbstractView references

One of the main areas where generative AI models thrive is image synthesis or generation. This work highlights the importance of quality prompts in generating compelling artworks and delves into four principal methodologies for generating prompt recommendations: text embeddings, ensemble models, text with image embeddings and object detection for feature extraction. Multiple traditional and neural network-based models are explored for feature vector representation. Furthermore, the study explores the incorporation of image embeddings, the user s preferred art styles for tailored recommendations, and the inherent challenges in evaluating these systems. We also propose a novel methodology for evaluating such systems, in the absence of ratings or preference scores, using graph analysis and community detection algorithms. This work distinctly contributes to the prompt recommendation domain and complements previous works in the AI art generation landscape. © 2023 IEEE.
"
10.1109/SNAMS60348.2023.10375440,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183473499&origin=inward,Conference Paper,SCOPUS_ID:85183473499,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),pushing boundaries: exploring zero shot object classification with large multimodal models,"
AbstractView references

The synergy of language and vision models has given rise to Large Language and Vision Assistant models (LLVAs), designed to engage users in rich conversational experiences intertwined with image-based queries. These comprehensive multimodal models seamlessly integrate vision encoders with Large Language Models (LLMs), expanding their applications in general-purpose language and visual comprehension. The advent of Large Multimodal Models (LMMs) heralds a new era in Artificial Intelligence (AI) assistance, extending the horizons of AI utilization. This paper takes a unique perspective on LMMs, exploring their efficacy in performing image classification tasks using tailored prompts designed for specific datasets. We also investigate the LLVAs zero-shot learning capabilities. Our study includes a benchmarking analysis across four diverse datasets: MNIST, Cats Vs. Dogs, Hymnoptera (Ants Vs. Bees), and an unconventional dataset comprising Pox Vs. Non-Pox skin images. The results of our experiments demonstrate the model's remarkable performance, achieving classification accuracies of 85%, 100%, 77%, and 79% for the respective datasets without any fine-tuning. To bolster our analysis, we assess the model's performance post fine-tuning for specific tasks. In one instance, fine-tuning is conducted over a dataset comprising images of faces of children with and without autism. Prior to fine-tuning, the model demonstrated a test accuracy of 55%, which significantly improved to 83% post fine-tuning. These results, coupled with our prior findings, underscore the transformative potential of LLVAs and their versatile applications in real-world scenarios. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183308930&origin=inward,Conference Paper,SCOPUS_ID:85183308930,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),probing the “creativity” of large language models: can models produce divergent semantic association?,"
AbstractView references

Large language models possess remarkable capacity for processing language, but it remains unclear whether these models can further generate creative content. The present study aims to investigate the creative thinking of large language models through a cognitive perspective. We utilize the divergent association task (DAT), an objective measurement of creativity that asks models to generate unrelated words and calculates the semantic distance between them. We compare the results across different models and decoding strategies. Our findings indicate that: (1) When using the greedy search strategy, GPT-4 outperforms 96% of humans, while GPT-3.5-turbo exceeds the average human level. (2) Stochastic sampling and temperature scaling are effective to obtain higher DAT scores for models except GPT-4, but face a trade-off between creativity and stability. These results imply that advanced large language models have divergent semantic associations, which is a fundamental process underlying creativity. © 2023 Association for Computational Linguistics.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183293539&origin=inward,Conference Paper,SCOPUS_ID:85183293539,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),meta-learning of prompt generation for lightweight prompt engineering on language-model-as-a-service,"
AbstractView references

Recently, many companies have been providing the capabilities of large language models as services. These Language-Model-as-a-Service (LMaaS) offerings support a variety of user tasks through in-context learning from prompts, which include instructions and demonstrations of the task. However, for users, manually crafting prompts or running automatic prompt tuning methods themselves can be demanding. Despite these challenges, LMaaS providers do not offer automatic prompt engineering methods as part of their services. One of the major obstacles to deploying them on an LMaaS is the heavy computational costs associated with automatic prompt engineering methods. These methods are typically designed to iterate through tens of thousands of examples, which impose unaffordable overheads for LMaaS providers. In this paper, we introduce MetaL-Prompt, a novel lightweight automatic prompt generation method for LMaaS. MetaL-Prompt meta-trains a prompt generation model (PGM) to enable robust learning by the language model from the contexts created by the generated prompts (i.e., in-context learning). Thanks to our meta-learning approach, a PGM can generate prompts for unseen tasks without requiring additional training for those specific tasks. Furthermore, the PGM can generate prompts with a single forward pass, significantly reducing computational costs compared to previous methods. We evaluate MetaL-Prompt on a range of unseen tasks and find that it improves performance by up to 19.4% in terms of mean F1 score on QA datasets compared to the state-of-the-art baseline P-tuning, with limited computational cost. © 2023 Association for Computational Linguistics.
"
10.1109/FIE58773.2023.10342970,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183059570&origin=inward,Conference Paper,SCOPUS_ID:85183059570,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),work-in-progress: integrating generative ai with evidence-based learning strategies in computer science and engineering education,"
AbstractView references

Generative AI assistants are AI-powered applications that can provide personalized responses to user queries or prompts. A variety of AI assistants have recently been released, and among the most popular is OpenAI's ChatGPT. In this work-in-progress in innovative practice, we explore evidence-based learning strategies and the integration of Generative AI for computer science and engineering education. We expect this research will lead to innovative pedagogical approaches to enhance undergraduate computer science and engineering education. In particular, we describe how ChatGPT was used in two computing-based courses: a Junior-level course in database systems and a Senior-level class in mobile application development. We identify four evidence-based learning strategies: well-defined learning goals, authentic learning experiences, structured learning progression, and strategic assessment. We align these strategies with the two aforementioned courses and evaluate the usefulness of ChatGPT specifically in achieving the learning goals. Combining Generative AI with evidence-based learning has the potential to transform modern education into a more personalized learning experience. © 2023 IEEE.
"
10.1109/FIE58773.2023.10343397,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183052849&origin=inward,Conference Paper,SCOPUS_ID:85183052849,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using chatgpt for homework: does it feel like cheating? (wip),"
AbstractView references

This WIP paper disseminates the results of an anonymous survey given to first-year engineering students in February of 2023 about ChatGPT, the recently-developed artificial intelligence chatbot. Survey results showed that some engineering students had used ChatGPT to complete their homework assignments. Furthermore, only a few of those who used it for homework felt like they were 'cheating,' or acting unethically, by doing so. These results indicate that the higher education community should carefully consider the potential learning benefits and threats of this new tool, as it may be utilized by at least some portion of students on their assignments. Much more work is needed to understand the potential uses, threats, and limitations of large language model chatbots in engineering education. © 2023 IEEE.
"
10.1109/FIE58773.2023.10343036,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183044273&origin=inward,Conference Paper,SCOPUS_ID:85183044273,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),investigating the connection between teachers' factors and students' performance in mathematics: a uae case study,"
AbstractView references

Without a doubt, teachers play a vital role in building the students' educational foundation and critical thinking behavior, which could be quantified through various factors, especially if the taught course falls within the category of Science, Technology, Engineering and Mathematics (STEM). Models based on Machine Learning (ML) and Artificial Intelligence (AI) have proven their accuracy in predicting students' performance. However, the analyses and explanations related to the correlation strength between the students' Mathematics proficiency and the teachers' factors are not a direct task. This article presents an investigation on the connection between the various teachers' factors and students' scores in Mathematics in middle schools located in the United Arab Emirates (UAE). This type of investigation has not yet been subject in the relevant body of work in the literature so far. The study employs an AI-based framework to predict the Mathematics' scores of eighth graders and identify the significant teachers' factors using a dataset which considers 1,203 teachers with 387 features obtained from the questionnaires' responses that are issued as part of the 2019 assessment cycle of the Trends in International Mathematics and Science Study (TIMSS) report. In the proposed framework, the Extreme Gradient Boosting (XGBoost) regression model is introduced to predict students' scores in Mathematics, in conjunction with the SHapley Additive exPlanations (SHAP) algorithm to identify the significant teachers' factors. Results show that there exists a solid correlation between students' proficiency in Mathematics and factors such as job satisfaction in the education profession, observations from the teachers on students' nutrition status, in addition to the students' level of comprehension with regards to the spoken language. The developed investigation could be employed in the advancement of the educational program in the UAE in the middle-school division in specific, and the STEM-based education at large. © 2023 IEEE.
"
10.1109/FIE58773.2023.10343517,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183016069&origin=inward,Conference Paper,SCOPUS_ID:85183016069,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),wip: using generative ai to assist in individual performance feedback for engineering student teams,"
AbstractView references

Teamwork is a critical component of engineering education, established as both an ABET competency and recognized by engineers in the workforce as one of the essential skills to their work. As a result, team-based learning throughout engineering education has become increasingly popular, especially in first-year programs. Tools such as CATME (Comprehensive Assessment of Team Member Effectiveness), a web-based program used by instructors, help instructors form student teams based on predefined criteria, facilitate peer evaluations to assess individual contributions and performance, and gather general feedback on student teamwork experiences. Other instructors may collect peer feedback through other methods, such as submitted assignments. Theories in organizational psychology highlight the relationship between performance feedback, psychological safety, and conflict management. To purposefully help students develop essential teamwork skills, such as how to manage conflict and build inclusive, psychologically safe teams effectively, it is important that they receive individual performance feedback. However, negative, unfiltered feedback from peers may also escalate unhealthy team dynamics. Consequently, instructors can be hesitant to share peer evaluation scores and comments with students because students may not be comfortable sharing honest, authentic feedback with their peers. However, withholding this valuable formative feedback to students may come at the expense of their individual professional development, especially in devel-oping teamwork skills. Additionally, these project-based learning (PBL) courses tend to be large, and summarizing formative feedback for individual students based on peer evaluations can be time-consuming and perhaps unrealistic with increasing class sizes. To address this need, we present the use of a generative artificial intelligence (AI) system to create individual performance summaries based on student peer responses. The authors present preliminary work from a pilot study that uses generative AI to produce individual performance summaries using CATME data from a first-year engineering course at a large public university in the mid-Atlantic region of the US. We share a sample output performance summary using a generative AI tool, GPT-4 specifically. Our initial findings indicate AI tools, such as GPT-4, can serve as a solution to provide more confidentiality in peer comments by effectively summarizing peer comments and producing individual performance feedback reviews. This study provides promising initial findings and recommendations on how AI can be leveraged to automate a student feedback process for instructors, allowing students to receive individual performance feedback while also providing an additional layer of confidentiality to support team dynamics and not compromise honest individual assessments. © 2023 IEEE.
"
10.1109/FIE58773.2023.10342985,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183006387&origin=inward,Conference Paper,SCOPUS_ID:85183006387,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring nlp-based methods for generating engineering ethics assessment qualitative codebooks,"
AbstractView references

This Full Research paper presents a comparison of two codebook generation methods using natural language processing (NLP): a human and NLP collaboration method and a fully automated NLP method (referred to as Human-NLP and Auto-NLP, respectively). Codebook generation serves as a preliminary step in most qualitative projects, and using NLP as a tool can help support the analysis and efficiency of the researcher. By utilizing NLP in the early stages of codebook generation, there are opportunities for detailed and productive gains when working with large corpora of textual data. Using NLP at this stage also allows the researcher to make sense of any outputs generated through automated means rather than simply accepting the output as it is. The outcome of both methods tested in this work will be used to evaluate and apply the codes across a large dataset. The Human-NLP method involves generating the initial themes using a large-language model (LLM), and the researcher revises the codebook further. The Auto-NLP method involves generating three rounds of codes, summarizing the codes in each until a saturation level has been reached through the overarching themes. The dataset used for this study comes from an analysis of students' perception and recognition of ethical concepts after participating in a semester-long course focused on ethics, society, and technology. The course introduced students to traditional ethics topics, such as those around engineering disasters, but also explored developing topics, such as facial recognition, dataset bias, and the impact of technology on the global food supply. We collected data between fall 2020 and 2022 from six (6) iterations of a semester-long course. A total of 210 student responses to the question - what did this course teach you about ethics - were analyzed. The results from both Human-NLP and Auto-NLP methods were promising in the level of detail summarized and the similarity of themes across the data. Eight (8) themes were finalized through the Human-NLP method, and twelve (12) were generated through the Auto-NLP method. We present a discussion exploring these themes and the limitations of using these methods. © 2023 IEEE.
"
10.1109/FIE58773.2023.10343373,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183000787&origin=inward,Conference Paper,SCOPUS_ID:85183000787,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),eliciting student understanding in structural engineering classrooms using text-to-image generative models,"
AbstractView references

In this work-in-progress, we explore the use of prompt engineering and image generation to elicit student understanding of different concepts discussed in structural engineering courses. In the context of image generation, prompt engineering refers to the process of providing a text or image prompt to a machine learning model to generate an image that meets certain criteria. The quality of the prompt can significantly impact the quality of the generated image. Hence, prompt engineering is an important part of the image generation process. By providing a well-crafted prompt that accurately describes the desired image, the generative model will more likely produce an image that meets the desired criteria. In the context of this study, students created prompts that were converted into images using Text-to-image Generative Models. When adequately prompted, these images are meant to visually describe their understanding of the concepts we have discussed during traditional lectures. The participants were presented with their own image as well as others' images to elicit both agreements or concerns at two levels: their understanding of the concepts and their understanding of the importance of prompting in the foreseeable future of AI-based systems. Hitherto, we have tried to use the generation of images using natural language for answering these research questions: i) for students, to which extent the crafting prompts may elicit thinking about the structural engineering phenomena? and ii) for instructors, to which extent these represent the level of understanding of the students on these topics? © 2023 IEEE.
"
10.1109/FIE58773.2023.10342917,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183000113&origin=inward,Conference Paper,SCOPUS_ID:85183000113,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),being brave in a new world: leveraging chatgpt in signal processing classes,"
AbstractView references

In this innovative practice work-in-progress paper, we hypothesize that in engineering areas such as signal processing, tools such as ChatGPT do not threaten academic integrity in the classroom. We believe that if questions and problems are suitably posed, ChatGPT can assist, but cannot provide solutions. To test this hypothesis, we ask two questions: (a) How can ChatGPT be used to assist students in a signal processing class? and (b) How can the class itself be designed to leverage what ChatGPT has to offer? To answer these questions, we deploy ChatGPT in three different scenarios: (1) In a graduate level course to explore the use cases and limitations of the tool; (2) In summer REU cohorts to study the attitudes of students before and after one-hour workshops; and (3) in undergraduate signal processing courses where students will be exposed to generative AI tools over an entire semester. Surveys and discussions with the students will be analyzed and results will be presented at the conference. With the three separate activities across different time scales and student levels, our results can be used to generate guidelines for instructors to incorporate generative AI tools in their classes. © 2023 IEEE.
"
10.1109/FIE58773.2023.10342963,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182997103&origin=inward,Conference Paper,SCOPUS_ID:85182997103,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),affective computing: a topic-based ser approach on collaborative discussions in academic setting,"
AbstractView references

One of the biggest concerns in the modern day especially in the educational domain centers on the student's mental health. High rates of anxiety and depression have especially brought the attention of researchers in engineering education to apply affective computing to help with students' academic performance. It is known that a person's emotional states cause physiological and physical changes in the body. Emotions may impact facial expression, tone of speech, blood pressure, pulse, etc. Since visual and auditory signals are two variables that can be measured without the need to attach any physical device to the individuals, they are most studied in this field. Speech in particular has been known as a means that transfers much information about the mental and emotional states of the person. Speech Emotion Recognition (SER) is a growing field that has been applied in several domains including engineering education. Recent advancements in AI, Natural Language Understanding (NLU), and Large Language Models (LLM) have significantly streamlined this line of research. In this work which is a continuation of our prior work, we propose a speech analysis model that extracts both the emotions and topics from verbal discussions in a computer science classroom to understand if the expressed emotions were mostly about the course related topics or not. The goal of this research is to develop a tool that helps educators gain insights into the students' emotional states in teamwork and also understand the context of their conversations. We further analyze if the expressed emotions in the verbal class discussions are mostly about the course content or other subjects outside class setting. To expand the emotion analysis module we added a new layer to our developed pipeline by passing the speech data into the ChatGPT API to generate summarized scripts and extract additional classes of emotion. The preliminary results from this study are promising, indicating the potential value of this research direction and its prospects for further development. Application of this model in the educational domain can greatly benefit both educators and students and allows the instructors to make necessary interventions needed to maximize students' positive experiences in team settings while considering their emotional states. © 2023 IEEE.
"
10.1109/FIE58773.2023.10343319,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182996291&origin=inward,Conference Paper,SCOPUS_ID:85182996291,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),analysis of learning outcomes in software engineering: an automated reflection analysis tool,"
AbstractView references

The rapid advancements in artificial intelligence (AI) have been transforming various domains, including engineering education. The availability of AI-based content and easy access to information have made students more dependent on these technologies. With the rise of online courses, there is growing concern about student engagement, poor learning outcomes, and low retention rates in higher education. Engagement plays a critical role in student success and can be achieved through formative assessment, critical thinking, and reflective thinking strategies. Reflection plays a key role in developing critical thinking and meta-cognitive skills. According to the constructive alignment framework which is an outcome-driven approach, the teaching and assessment methods should be shaped around fulfilling the course learning outcomes. Although the idea of constructive alignment is not a new topic, the higher education sector has recently emphasized it at a large scale due to the diversity of new required skills for students to enter the 5th industrial revolution. In earlier work, we proposed an AI-based reflection analysis model that combines both aspects of learning outcome-based assessment and student engagement by applying 'Minute Paper'. Minute paper is a formative assessment tool that helps the instructors identify the muddy points of the lesson and students' learning gaps as well as their learning outcomes by asking two questions at the end of each lecture (i.e., what they learned and what they didn't). In this work, we propose a more advanced version of the reflection analysis tool by applying transformer-based language models to analyze students' responses to the Minute Paper reflections with higher accuracy in the course context. For this purpose, we train the BERTopic model with the course syllabus and lecture material to get more accurate data in the context of the given courses. The proposed system aids instructors in future course development by adding additional resources for the muddy points, allowing for adjusting content delivery pace, and designing tests and assignments with appropriate challenge levels [1]. Adaptation of this system can enhance the learning experience for both students and teachers and can be extended beyond higher education. © 2023 IEEE.
"
10.1109/FIE58773.2023.10343006,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182994988&origin=inward,Conference Paper,SCOPUS_ID:85182994988,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),work in progress: large language model based automatic grading study,"
AbstractView references

We investigated the capability of Large Language Models (LLMs) for grading short answer questions and studied different auto-grading schemes for evaluating student responses to conceptual questions in a mechanical engineering statics course. We compared the ability of seven Natural Language Processing (NLP) systems to score text-based answers as Correct/Incorrect and numerically, with human-supported Rules-based grading as a benchmark. We collected the instructor-provided answers, anonymized student answers, and their grades for this study. The findings reveal that the Large Language Model (LLM) based grading systems exhibit commendable precision in binary evaluations. However, within the spectrum of error classifications, the LLM-based grading systems exhibit a pronounced rate of false positives, a scenario less than ideal in an educational context. Considering that the technical terms in the instructors' answers are a primary factor in grading, our forthcoming research endeavors to embed keyword detection within the LLM-based automatic grading framework to mitigate the incidence of false positives. Thus, we investigated the ability of the standalone LLM-Vicuna to identify important keywords in an answer in the context of the mechanic's course. Our preliminary observations indicate that Vicuna accurately identifies the keywords in the answers, but the results are not yet repeatable due to the stochastic nature of the model. © 2023 IEEE.
"
10.1109/FIE58773.2023.10343296,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182986847&origin=inward,Conference Paper,SCOPUS_ID:85182986847,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the potential role of ai-based chatbots in engineering education. experiences from a teaching perspective,"
AbstractView references

The irruption of Artificial Intelligence (AI) based chatbot tools is undoubtedly at the frontiers of education. AI chatbots in education has emerged as a promising solution to enhance the quality of education and to improve learning outcomes. As all new technology does, it has begun to generate news about prohibition, ethical aspects, anti-plagiarism detection tools, and a series of policies from different educational systems. However, we should not deny the positive aspects of these tools if they are well used. AI-based chatbots have interesting potential to help both teachers and students, who must learn to use them well for their own benefit. This article provides an overview of AI-based chatbots, particularly ChatGPT, an artificial intelligence language model developed by OpenAI. GPT, or 'Generative Pre-Training Transformer' is a neural network trained to generate 'human-like text' by predicting the next word in a sequence given a large dataset of examples. ChatGPT uses the neural network model and is used to generate responses to students' questions in real time, in the sense of a personal teacher assistant. Chatbots are designed to be able to carryon a natural conversation by understanding the context of the conversation, generating appropriate responses, and engaging in active interaction. Nowadays, this type of tool can generate more than just text; for example, the use of LaTeX code (using TeXGPT) or tools for coding and debugging programming exercises. This work explores the potential role that AI-based chatbots can have in engineering education, by examining the answers of a group of teachers' about how the use of AI-based chatbots can improve the learning process of the students in engineering education. The questions that teachers had to answer, from a pedagogical and technological perspective, are related to how chatbots can be integrated into the curriculum to enhance the efficiency of engineering education, their potential impact on the learning process, and actual examples of possible learning activities using AI-based chatbots in their courses. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182952103&origin=inward,Conference Paper,SCOPUS_ID:85182952103,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),melodic constraint satisfaction with a generative adversarial network,"
AbstractView references

Generative Adversarial Networks (GANs) are typically used in order to generate instances that resemble those in a data set. However, this study generalizes GAN so that it can be used as a method of constraint satisfaction. If GANs get the capacity to satisfy explicit rules that do not necessarily exist in the training data set, this may improve the controllability and trustfulness of generative AI. An experiment of a simple melodic generation suggests that the GAN can learn constraints from synthetic data sets for respective constraints and generate solutions that simultaneously satisfy all of the constraints. Copyright: © 2023 Tsubasa Tanaka et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License 3.0 Unported, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
"
10.1109/SSCI52147.2023.10371898,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182935589&origin=inward,Conference Paper,SCOPUS_ID:85182935589,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),large language and text-to-3d models for engineering design optimization,"
AbstractView references

The current advances in generative artificial intelligence for learning large neural network models with the capability to produce essays, images, music and even 3D assets from text prompts create opportunities for a manifold of disciplines. In the present paper, we study the potential of deep text-to-3D models in the engineering domain and focus on the chances and challenges when integrating and interacting with 3D assets in computational simulation-based design optimization. In contrast to traditional design optimization of 3D geometries that often searches for the optimum designs using numerical representations, e.g. B-Spline surfaces, natural language challenges the optimization framework by requiring a different interpretation of variation operators while at the same time may ease and motivate the human user interaction. Here, we propose and realize a fully automated evolutionary design optimization framework using Shap-E, a recently published text-to-3D asset network by OpenAI, in the context of aerodynamic vehicle optimization. For representing text prompts in the evolutionary optimization, we evaluate (a) a bag-of-words approach based on prompt templates and Wordnet samples, and (b) a tokenisation approach based on prompt templates and the byte pair encoding method from GPT4. In our experiments, we show the text-based representations allow the optimizer to find better performing designs. However, it is important to ensure that the designs generated from prompts are within the object class of application, i.e. diverse and novel designs need to be realistic. Furthermore, more research is required to develop methods where the strength of text prompt variations and the resulting variations of the 3D designs share causal relations to some degree to improve the optimization. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182863371&origin=inward,Conference Paper,SCOPUS_ID:85182863371,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),empirical study of zero-shot ner with chatgpt,"
AbstractView references

Large language models (LLMs) exhibited powerful capability in various natural language processing tasks. This work focuses on exploring LLM performance on zero-shot information extraction, with a focus on the ChatGPT and named entity recognition (NER) task. Inspired by the remarkable reasoning capability of LLM on symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods to NER and propose reasoning strategies tailored for NER. First, we explore a decomposed question-answering paradigm by breaking down the NER task into simpler subproblems by labels. Second, we propose syntactic augmentation to stimulate the model's intermediate thinking in two ways: syntactic prompting, which encourages the model to analyze the syntactic structure itself, and tool augmentation, which provides the model with the syntactic information generated by a parsing tool. Besides, we adapt self-consistency to NER by proposing a two-stage majority voting strategy, which first votes for the most consistent mentions, then the most consistent types. The proposed methods achieve remarkable improvements for zero-shot NER across seven benchmarks, including Chinese and English datasets, and on both domain-specific and general-domain scenarios. In addition, we present a comprehensive analysis of the error types with suggestions for optimization directions. We also verify the effectiveness of the proposed methods on the few-shot setting and other LLMs. © 2023 Association for Computational Linguistics.
"
10.1109/IoT60973.2023.10365361,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182743427&origin=inward,Conference Paper,SCOPUS_ID:85182743427,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automatic generation of cep rules using data analysis techniques and model-driven engineering,"
AbstractView references

The widespread use of Internet of Things (IoT) systems results in a large volume of data that needs to be analyzed in real time for efficient usage. Complex Event Processing (CEP) is a technique that enables organizations to analyze data and identify complex and meaningful events. CEP uses patterns written in an Event Pattern Language (EPL) to describe an event. However, producing patterns and rules using EPL remains a significant challenge.This research benefits from data analysis and model-driven engineering to automatically produce CEP rules that allow domain experts to extract and define patterns automatically, even if unfamiliar with data analysis or CEP rule design. At the end of the paper, a case problem is solved, resulting in a coverage coefficient of 60%, an average accuracy of 82.5%, and an average precision of 82%. The results show that this combined approach is beneficial in providing a more appropriate response in less time. © 2023 IEEE.
"
10.1109/SCAM59687.2023.00020,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182741854&origin=inward,Conference Paper,SCOPUS_ID:85182741854,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),explaining transformer-based code models: what do they learn? when they do not work?,"
AbstractView references

In recent years, there has been a wide interest in designing deep neural network-based models that automate downstream software engineering tasks on source code, such as code document generation, code search, and program repair. Although the main objective of these studies is to improve the effectiveness of the downstream task, many studies only attempt to employ the next best neural network model, without a proper in-depth analysis of why a particular solution works or does not, on particular tasks or scenarios. In this paper, using an example eXplainable AI (XAI) method (attention mechanism), we study two recent large language models (LLMs) for code (CodeBERT and GraphCodeBERT) on a set of software engineering downstream tasks: code document generation (CDG), code refinement (CR), and code translation (CT). Through quantitative and qualitative studies, we identify what CodeBERT and GraphCodeBERT learn (put the highest attention on, in terms of source code token types), on these tasks. We also show some of the common patterns when the model does not work as expected (performs poorly even on easy problems) and suggest recommendations that may alleviate the observed challenges. © 2023 IEEE.
"
10.1109/iCAST57874.2023.10359304,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182737407&origin=inward,Conference Paper,SCOPUS_ID:85182737407,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using large language models for bug localization and fixing,"
AbstractView references

As part of their learning journey, students frequently encounter challenges and make errors, especially with algorithmic programming questions. Regrettably, providing tailored solutions for these mistakes can impose a significant burden on instructors in terms of time and effort. To address this, automated program repair (APR) techniques have been explored to generate such fixes a utomatically. P revious r esearch has investigated the use of symbolic and neural approaches for APR in the educational domain. However, both types of approaches necessitate substantial engineering endeavors or extensive data and training. In this study, we propose the utilization of a large language model trained on code to construct an APR system specifically d esigned f or s tudent p rograms. O ur s ystem h as the capability to rectify semantic errors by employing a few-shot example generation pipeline solely based on the input code. We assess the performance of our system on one dataset of algorithm implementations, namely QuixBugs. The results demonstrate that the novel example generation pipeline not only enhances the overall system’s performance but also ensures its stability. © 2023 IEEE.
"
10.1109/iCAST57874.2023.10359310,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182733190&origin=inward,Conference Paper,SCOPUS_ID:85182733190,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),harnessing ai for scientific illustration: exploring tornado dynamics through midjourney,"
AbstractView references

Scientific illustration, with roots in ancient civilizations, has evolved from hand-drawn depictions to advanced digital visualizations. This research delves into the historical journey of scientific illustration, emphasizing its role in bridging art and science. With the advent of knowledge engineering, these illustrations have become more effective in simplifying complex scientific concepts. This study investigates using AI art generation platforms such as Midjourney to create captivating and informative scientific illustrations of complex meteorological phenomena. Focusing on tornado dynamics, Midjourney is prompted to visualize tornado supercells, interior anatomy, and multi-vortex systems with artistic flair. The generated images showcase Midjourney's ability to fuse empirical facts like weather data with aesthetic considerations such as composition, perspective, and lighting. Although limitations exist, the AI-generated image reveals Midjourney's promise in balancing scientific insight and creative expression. The tornado illustrations highlight the platform's ability to elucidate intricate meteorological concepts through imaginative, highly detailed scenes that educate and inspire. © 2023 IEEE.
"
10.13052/jwe1540-9589.2263,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182708548&origin=inward,Article,SCOPUS_ID:85182708548,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring llm-based automated repairing of ansible script in edge-cloud infrastructures,"
AbstractView references

Edge-Cloud system requires massive infrastructures located in closer to the user to minimize latencies in handling Big data. Ansible is one of the most popular Infrastructure as Code (IaC) tools crucial for deploying these infrastructures of the Edge-cloud system. However, Ansible also consists of code, and its code quality is critical in ensuring the delivery of high-quality services within the Edge-Cloud system. On the other hand, the Large Langue Model (LLM) has performed remarkably on various Software Engineering (SE) tasks in recent years. One such task is Automated Program Repairing (APR), where LLMs assist developers in proposing code fixes for identified bugs. Nevertheless, prior studies in LLM-based APR have predominantly concentrated on widely used programming languages (PL), such as Java and C, and there has yet to be an attempt to apply it to Ansible. Hence, we explore the applicability of LLM-based APR on Ansible. We assess LLMs’ performance (ChatGPT and Bard) on 58 Ansible script revision cases from Open Source Software (OSS). Our findings reveal promising prospects, with LLMs generating helpful responses in 70% of the sampled cases. Nonetheless, further research is necessary to harness this approach’s potential fully. © 2023 River Publishers. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182701490&origin=inward,Conference Paper,SCOPUS_ID:85182701490,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),large language models are better reasoners with self-verification,"
AbstractView references

Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: https://github.com/WENGSYX/Self-Verification. © 2023 Association for Computational Linguistics.
"
10.1109/QRS60937.2023.00031,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182511659&origin=inward,Conference Paper,SCOPUS_ID:85182511659,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),git merge conflict resolution leveraging strategy classification and llm,"
AbstractView references

In the realm of collaborative software development, version control systems (VCS) like Git play an indispensable role, enabling concurrent development and facilitating seamless integration of disparate code contributions. Despite these benefits, merge conflicts resulting from simultaneous changes to identical code lines often pose significant challenges to the integration process. Addressing this challenge, our paper introduces a novel two-stage approach, termed as CHATMERGE, for resolving Git merge conflicts. CHATMERGE pioneers a unique strategy that employs machine learning to initially predict resolution strategies, and subsequently leverages a large language model, ChatGPT, to create resolutions for conflicts that necessitate complex resolution strategies. A series of comprehensive experiments validate CHATMERGE's efficacy, demonstrating its impressive alignment with historical manual resolutions and its superior performance relative to existing, publicly accessible tools. The paper further explores the influence of various classification algorithms and the prompt construction process for ChatGPT, providing further insights into the merge conflict resolution process. Moreover, to foster continued advancements in this area, CHATMERGE, along with its associated training and testing datasets, is made publicly available, offering a valuable resource for both developers and researchers. This work, therefore, provides both an innovative solution to merge conflict resolution and a strong foundation for future explorations in this domain. © 2023 IEEE.
"
10.1109/QRS60937.2023.00028,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182504562&origin=inward,Conference Paper,SCOPUS_ID:85182504562,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on chatgpt: perspectives from software engineering students,"
AbstractView references

ChatGPT, an increasingly popular Large Language Model (LLM), has found widespread acceptance, especially among the younger generation, who rely on it for various tasks, such as comprehending complex course materials and tackling homework assignments. This surge in interest has drawn the attention of researchers, leading to numerous studies that delve into the advantages and disadvantages of the upcoming LLM dominant era. In our research, we explore the influence of ChatGPT and similar models on the field of software engineering, specifically from the perspective of software engineering students. Our main objective is to gain valuable insights into their usage habits and opinions through a comprehensive survey. The survey encompassed diverse questions, addressing the specific areas where ChatGPT was utilized for assistance and gathering students' reflections on each aspect. We found that ChatGPT has garnered widespread acceptance among software engineering students, with 93% of them utilizing it for their projects. These students expressed satisfaction with the level of assistance provided, and most intend to continue using it as a valuable tool in their work. During our investigation, we also assessed the students' awareness of the underlying technologies behind ChatGPT. Approximately half of the students demonstrated awareness of these technologies, while 38.7% had made extra efforts to explore prompt engineering to enhance ChatGPT's productivity. However, an important finding was that 90.6% of the students reported experiencing hallucinations during their interactions with ChatGPT. These hallucinations were shared as examples, raising significant concerns that warrant further exploration and mitigation. Moreover, we delved into potential improvements and gathered valuable recommendations, which could help ChatGPT to become even more effective and dependable in its applications. © 2023 IEEE.
"
10.3389/fpsyt.2023.1277756,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182420102&origin=inward,Article,SCOPUS_ID:85182420102,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt is not ready yet for use in providing mental health assessment and interventions,"
AbstractView references

Background: Psychiatry is a specialized field of medicine that focuses on the diagnosis, treatment, and prevention of mental health disorders. With advancements in technology and the rise of artificial intelligence (AI), there has been a growing interest in exploring the potential of AI language models systems, such as Chat Generative Pre-training Transformer (ChatGPT), to assist in the field of psychiatry. Objective: Our study aimed to evaluates the effectiveness, reliability and safeness of ChatGPT in assisting patients with mental health problems, and to assess its potential as a collaborative tool for mental health professionals through a simulated interaction with three distinct imaginary patients. Methods: Three imaginary patient scenarios (cases A, B, and C) were created, representing different mental health problems. All three patients present with, and seek to eliminate, the same chief complaint (i.e., difficulty falling asleep and waking up frequently during the night in the last 2°weeks). ChatGPT was engaged as a virtual psychiatric assistant to provide responses and treatment recommendations. Results: In case A, the recommendations were relatively appropriate (albeit non-specific), and could potentially be beneficial for both users and clinicians. However, as complexity of clinical cases increased (cases B and C), the information and recommendations generated by ChatGPT became inappropriate, even dangerous; and the limitations of the program became more glaring. The main strengths of ChatGPT lie in its ability to provide quick responses to user queries and to simulate empathy. One notable limitation is ChatGPT inability to interact with users to collect further information relevant to the diagnosis and management of a patient’s clinical condition. Another serious limitation is ChatGPT inability to use critical thinking and clinical judgment to drive patient’s management. Conclusion: As for July 2023, ChatGPT failed to give the simple medical advice given certain clinical scenarios. This supports that the quality of ChatGPT-generated content is still far from being a guide for users and professionals to provide accurate mental health information. It remains, therefore, premature to conclude on the usefulness and safety of ChatGPT in mental health practice. Copyright © 2024 Dergaa, Fekih-Romdhane, Hallit, Loch, Glenn, Fessi, Ben Aissa, Souissi, Guelmami, Swed, El Omri, Bragazzi and Ben Saad.
"
10.1109/ICTAI59109.2023.00018,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182402175&origin=inward,Conference Paper,SCOPUS_ID:85182402175,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rule-augmented artificial intelligence-empowered systems for medical diagnosis using large language models,"
AbstractView references

In this paper, we investigate the enhancement of Artificial Intelligence (AI) technologies in healthcare and the better understanding of medical literature with the use of Large Language Models (LLMs) and Natural Language Processing (NLP). Specifically, we introduce a rule-augmented AI-empowered system which incorporates a rule-based decision system, the ChatGPT application programming interface (API), and other external machine learning and analytical APIs to offer diagnostic suggestions to patients. The complexities of patient healthcare experiences, including doctor-patient interactions, understanding levels, treatment procedures, and preventive care, are considered. We illustrate how a diagnostic process typically integrates various strategies depending on various factors. To digitize the greatest portion of the process, we propose and illustrate the use of LLMs for humanizing the communication process and investigating ways to reduce burdens and costs in primary healthcare. We also outline a theoretical decision model for evaluating the use of technological components from external sources versus building them from scratch. The paper is structured into sections detailing background theories and context, our proposed and implemented rule-augmented AI-empowered system, as well as a system test in a corresponding use case. Finally, the paper key findings are presented, which contribute valuable insights for future work in this field. © 2023 IEEE.
"
10.1109/MODELS-C59198.2023.00034,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182397882&origin=inward,Conference Paper,SCOPUS_ID:85182397882,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative ai in model-driven software engineering education: friend or foe?,"
AbstractView references

The availability and effectiveness of generative AI tools challenge the currently established methods for learning, teaching and assessment. In this paper, we discuss their potential impact for model-driven software engineering education, both from the point of view of educators and students. The discussion highlights several opportunities and risks, which support the need of a critical perspective in the application of these tools. © 2023 IEEE.
"
10.1109/ICTAI59109.2023.00135,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182397320&origin=inward,Conference Paper,SCOPUS_ID:85182397320,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),zero-shot bilingual app reviews mining with large language models,"
AbstractView references

App reviews from app stores are crucial for improving software requirements. A large number of valuable reviews are continually being posted, describing software problems and expected features. Effectively utilizing user reviews necessitates the extraction of relevant information, as well as their subsequent summarization. Due to the substantial volume of user reviews, manual analysis is arduous. Various approaches based on natural language processing (NLP) have been proposed for automatic user review mining. However, the majority of them requires a manually crafted dataset to train their models, which limits their usage in real-world scenarios. In this work, we propose Mini-BAR, a tool that integrates large language models (LLMs) to perform zero-shot mining of user reviews in both English and French. Specifically, Mini-BAR is designed to (i) classify the user reviews, (ii) cluster similar reviews together, (iii) generate an abstractive summary for each cluster and (iv) rank the user review clusters. To evaluate the performance of Mini-BAR, we created a dataset containing 6,000 English and 6,000 French annotated user reviews and conducted extensive experiments. Preliminary results demonstrate the effectiveness and efficiency of Mini-BAR in requirement engineering by analyzing bilingual app reviews. © 2023 IEEE.
"
10.1109/MILCOM58377.2023.10356332,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182396042&origin=inward,Conference Paper,SCOPUS_ID:85182396042,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),counterexample guided inductive synthesis using large language models and satisfiability solving,"
AbstractView references

Generative large language models (LLMs) can follow human-provided instruction prompts and generate human-like responses. Apart from natural language responses, they have been found to be effective at generating formal artifacts such as code, plans, and logical specifications. Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence - a phenomenon often referred to as hallucinations. This limitation makes it difficult to use these models to synthesize formal artifacts used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan, and other formal artifacts produced by LLMs can be catastrophic. We posit that we can use the satisfiability modulo theory (SMT) solvers as deductive reasoning engines to analyze the generated solutions from the LLMs, produce counterexamples when the solutions are incorrect, and provide that feedback to the LLMs exploiting the dialog capability of LLMs. This interaction between inductive LLMs and deductive SMT solvers can iteratively steer the LLM to generate the correct response. In our experiments, we use planning over the domain of blocks as our synthesis task for evaluating our approach. We use GPT-4, GPT3.5 Turbo, Davinci, Curie, Babbage, and Ada as the LLMs and Z3 as the SMT solver. Our method allows the user to communicate the planning problem in natural language; even the formulation of queries to SMT solvers is automatically generated from natural language. Thus, the proposed technique can enable non-expert users to describe their problems in natural language, and the combination of LLMs and SMT solvers can produce provably correct solutions. © 2023 IEEE.
"
10.1109/ICTAI59109.2023.00076,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182389776&origin=inward,Conference Paper,SCOPUS_ID:85182389776,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),invertible neural networks for trustworthy ai,"
AbstractView references

This study combines research in machine learning and system engineering practices to conceptualize a paradigm enhancing trustworthiness of machine learning inference process. We explore the topic of reversibility in deep neural networks and introduce their anomaly detection capabilities to build a framework of integrity verification checkpoints across the inference pipeline of a deployed model. We leverage previous findings and principles regarding several types of autoencoders, deep generative maximum-likelihood training and invertibility of neural networks to propose an improved network architecture for anomaly detection. A remarkable ability of an Invertible Neural Network (INN) to reconstruct data from its compressed representation and to solve inverse problems is then generalized and applied in the field of Trustworthy AI. To achieve integrity verification of an inference pipeline we place the INN-based Trusted Neural Network nodes around the mission critical parts of the system, achieving an end-to-end outcome verification. This work aspires to enhance robustness and reliability of applications employing artificial intelligence, which are playing increasingly noticeable role in highly consequential decision-making processes across many industries and problem domains. © 2023 IEEE.
"
10.23726/cij.2023.1470,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182383139&origin=inward,Article,SCOPUS_ID:85182383139,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the use of generative ai tools in design thinking academic makeathon,"
AbstractView references

This paper examines the integration and influence of Generative Artificial Intelligence (GenAI) tools in a Double Diamond Design Thinking (DDDT) academic makeathon. It analyses students interaction with these tools in problem-solving scenarios, offering insights into their perceptions and manner of use. The study reveals that text-based GenAI, such as ChatGPT and visual tools such as Midjourney and Dall-E 2, are perceived to be supportive rather than solution-dictating. However, it appears that there is a significant difference between engineering and design students in their approach and their trust in these tools. Moreover, students often use tools like ChatGPT as search engines without fully exploring their capabilities. This paper aims to explore the potential of GenAI in its deeper capacity within the DDDT methodology, and how to maximize its value. © The Author(s) 2023.
"
10.1109/WEEF-GEDC59520.2023.10343685,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182345257&origin=inward,Conference Paper,SCOPUS_ID:85182345257,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),competences development and significative learning through engineering design competition,"
AbstractView references

First engineering and science formation units (blocks) of Tec21 competences-based model [1], defined by Tecnologico de Monterrey, include students solving a real challenge supported by knowledge obtained in modules on mathematics, physics, and computing. A new challenge ""Design, construction and modeling of a water-powered rocket""was proposed for the F1007B ""Modeling in Engineering with Conservation Laws""block. This challenge involves designing, modeling and construction of a rocket, powered by a jet of water due to the pressure of a hand pump. Students compete in teams to achieve the greatest distance with their rockets. The educational intention of this challenge is to enhance student learning experience by (1) Covering many more topics of Physics that can been applied by students (2) Improve students grades and professor evaluation, (3) Increasing the use of technology by incorporating the Tracker program for video analysis, (4) Enabling comparison of real results with those obtained through MATLAB modeling, and (5) Fostering enthusiasm among students by facilitating teams' competition within the same class and even among different campuses. Considering the analysis made, it was observed that the water rocket contest helped most of the students to better understand the concepts of Physics included in this block and in the previous block, F1006B ""Modeling the Movement in Engineering"". The topics in this unit include the Principle of conservation of energy, the Principle of conservation of lineal momentum, Bernoulli's Principle, Projectile motion and Air friction. A large percentage of students consider that this activity was good for using technological tools such as Matlab (72%) or Tracker (84%) and to compare theory with reality. A good number of students believe that this challenge helped them develop and implement solutions, demonstration of operation of engineering systems and devices, written language, and scientific thinking competences. It can be concluded that this rocket contest challenge is a good educational innovation since it incorporates a change in the materials, methods, contents and contexts involved in the formation unit. © 2023 IEEE.
"
10.1109/MODELS58315.2023.00020,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182339047&origin=inward,Conference Paper,SCOPUS_ID:85182339047,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),model-driven prompt engineering,"
AbstractView references

Generative artificial intelligence (AI) systems are capable of synthesizing complex content such as text, source code or images according to the instructions described in a natural language prompt. The quality of the output depends on crafting a suitable prompt. This has given rise to prompt engineering, the process of designing natural language prompts to best take advantage of the capabilities of generative AI systems.Through experimentation, the creative and research communities have created guidelines and strategies for creating good prompts. However, even for the same task, these best practices vary depending on the particular system receiving the prompt. Moreover, some systems offer additional features using a custom platform-specific syntax, e.g., assigning a degree of relevance to specific concepts within the prompt.In this paper, we propose applying model-driven engineering to support the prompt engineering process. Using a domain-specific language (DSL), we define platform-independent prompts that can later be adapted to provide good quality outputs in a target AI system. The DSL also facilitates managing prompts by providing mechanisms for prompt versioning and prompt chaining. Tool support is available thanks to a Langium-based Visual Studio Code plugin. © 2023 IEEE.
"
10.1109/WEEF-GEDC59520.2023.10343903,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182337205&origin=inward,Conference Paper,SCOPUS_ID:85182337205,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),art critically examining generative ai,"
AbstractView references

In the rapidly evolving world of generative artificial intelligence (AI), it is crucial to exercise our human capacity for critical analysis of that technology. The CRAiEDL STEAM Collective brings together emerging artists, engineers, ethicists, academics, and makers to leverage art as a unique lens to critically examine technology and (re)evaluate its meaning and function in the world. In this paper, we report on our experiences using STEAM and research-creation methodologies in a pilot study that produced two critical artworks co-created with generative AI tools: ""Calibrating Stretched Transparency"", which explores biases inherent to decision-making tools used in large-scale technopolitical climate projects, and ""I'm Honoured To Serve"", which examines the seductive design of digital environments that generate user data for profit. This pilot research project demonstrated the value and effectiveness of using art and research-creation in a STEAM context to critically examine the rapidly evolving world of technology, including generative AI, and the role of arts and humanities in responsible AI innovation. The overall project resulted in a reconfiguring of each member's understanding of the potential for STEAM to transcend traditional disciplinary boundaries to produce a new type of expertise (i.e., STEAM expertise) and new knowledge (e.g., a critical STEAM-informed perspective on voice-based AI assistants, in the case of ""I'm Honoured To Serve""). © 2023 IEEE.
"
10.1109/WEEF-GEDC59520.2023.10343985,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182336089&origin=inward,Conference Paper,SCOPUS_ID:85182336089,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"engineering, the profession in trouble: lack of programme development standards that support the ai chatbot? a system view","
AbstractView references

As the world embraces the transformative potential of artificial intelligence (AI), large language models (LLM), and conversational agents also known as chatbots, the engineering profession stands at a crucial juncture. This paper critically examines the current state of engineering education and its readiness to incorporate AI chatbots effectively. Focusing on the lack of standardized programme development that supports AI chatbots, this paper sheds light on the implications of this gap for engineering education. Standardization is defined as the degree to which educational programmes meet common national and international quality standards. By synthesizing existing literature, this study investigates the challenges, opportunities, and strategies required to bridge this divide and ensure that engineering programmes are adequately equipped to produce graduates who can harness the power of AI chatbots. We found that strong and continuing trends are emerging in the use of AI and chatbots in engineering education and industry. We further noted that current standards from accreditation bodies need to respond to enable AI and chatbot incorporation from curriculum to pedagogical levels of engineering education. Industry-academia partnerships are vital in managing the integration of AI into engineering education. © 2023 IEEE.
"
10.1109/MODELS58315.2023.00039,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182329840&origin=inward,Conference Paper,SCOPUS_ID:85182329840,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),toward a symbiotic approach leveraging generative ai for model driven engineering,"
AbstractView references

Model Driven Engineering (MDE) proposes models as primary artefacts for analysis, simulation, software development etc. While MDE has delivered on the promise of enhanced productivity through automation, it continues to pose a significant entry barrier for domain experts who are typically not well-versed with MDE technology. With modelling gaining traction for analysis-heavy use cases like decision-making and regulatory compliance where domain experts play a central role, this barrier is beginning to hurt even more. We posit that Generative AI techniques can significantly lower this barrier by enabling domain experts to construct purposive models by operating at natural language level. This requires domain experts to interact with Generative AI tools using the right purpose-specific contextual prompts. We propose a model-driven approach where purposive meta models guide the interactions between domain expert and Generative AI to generate such prompts. The proposed approach helps in overcoming some of the limitations of Generative AI such as missing local context, limited context window size, attention fading etc. Industry scale models are typically large, necessitating a team of experts to work in a coordinated manner which requires sharing of outputs and persistence across sessions. Our approach brings together MDE and Generative AI in a symbiotic relationship complimenting respective strengths and overcoming limitations. We have validated this approach for development of digital twin based applications and early results are encouraging. © 2023 IEEE.
"
10.1109/WEEF-GEDC59520.2023.10343638,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182325979&origin=inward,Conference Paper,SCOPUS_ID:85182325979,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative artificial intelligence: a double-edged sword,"
AbstractView references

Despite the advancements that have come out of the artificial intelligence (AI) boom and subsequent generative AI race, there are a significant number of problems under the surface. With the rise of AI generators in social media and pop culture, these problems amplify as the demand for AI tools skyrockets. These solutions regularly exhibit harmful biases and stereotypes deeply embedded in the data developers use to train AI models. Perhaps exacerbating the immediacy of the issue, generative AI models can create highly realistic imagery, empowering users with malicious intent. This paper explores the pervasive issue of bias in generative AI and its implications. It also examines how these models, often trained on biased datasets, can unintentionally amplify, replicate, and reinforce harmful biases and stereotypes. Additionally, it delves into the objectification of individuals, particularly women, and in a novel study, discusses how a popular TikTok filter that uses generative AI exhibits gender and racial biases while excessively sexualizing its users. While research on AI bias is forthcoming, existing literature referenced throughout this paper underscores its presence across various systems. This paper demonstrates the continued propagation of AI bias as these systems expand and iterate, often with inadequate mitigation efforts. It advocates for ethical considerations, accountability, and transparency in generative AI design and data selection. The Author proposes a novel framework called the Stereotypes, Objectification, Racism, and Datasets (SORD) Framework. The SORD Framework is proposed as a lens to examine the multifaceted dimensions of bias in generative AI. By critically analyzing these components, this paper aims to shed light on the ethical implications of biased generative AI, stimulate thoughtful discussions regarding responsible AI development, and provoke the lengthy process of removing bias from the systems and training datasets to eliminate AI bias at the source. Furthermore, it calls for collaboration among developers, researchers, and engineers to advance research, mitigate biases, and promote algorithmic fairness using the SORD Framework. Future work includes refining the SORD framework and assessing its effectiveness. Lastly, the Author underscores the pivotal role of engineering education in nurturing ethical AI practices, empowering future engineers to address societal challenges stemming from biased datasets and AI systems. © 2023 IEEE.
"
10.1088/1742-6596/2646/1/012031,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182279660&origin=inward,Conference Paper,SCOPUS_ID:85182279660,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),transformer-based visual question answering model comparison,"
AbstractView references

The potential use of visual question and answer (VQA) models in fields including robotics, autonomous driving, and health care has recently attracted a lot of attention. However, the development of the multi-modular learning VL (Vision-Language Learning) model itself in the ""pre-training + fine-tuning"" mode is challenged under the rapid development of large-scale language models. Therefore, optimizing the value and feasibility of the single-task model itself is also worth thinking more cautiously. With both model principles, structures, and experimental contrast analysed in this paper, the two excellent models, LXMERT and UNITER, are evaluated to demonstrate the reasons for performance differences on specific downstream tasks VQA deeply. In addition, this paper provides insight into the possibility of further model optimization in the future for multi-modular tasks, especially VQA. In comparison to UNITER, the findings reveal greater accuracy in the training set and verification set from beginning for LXMERT, which indicates that the better model architecture and pre-training method is where LXMERT's higher adaptability probably comes from. However, the UNITER, which has fewer parameters and smaller architecture, can also achieve fine results after fine-tuning. This paper highlights the differences in accuracy shown by existing Transformers-based VQA models in local test sets and explores the possibility of optimizing VQA systems through fine-tuning. © 2023 Institute of Physics Publishing. All rights reserved.
"
10.1109/A-SSCC58667.2023.10347923,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182271384&origin=inward,Conference Paper,SCOPUS_ID:85182271384,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),semiconductor chip design in a legoland,"
AbstractView references

Moore's Law and Dennard scaling have been ticking and propelling the semiconductor industry for decades. They have also pushed the digitization and pixelization of our everyday life. The industry has attained a great achievement in terms of Power consumption, Performance, silicon Area, chip and system Cost as well as Time to market (PPACT). Unfortunately, the pace and benefits of CMOS scaling have now slowed down, yet the explosive demand for chip PPACT is on a completely opposite trajectory due to the rise of Generative AI (GAI), next-generation wireline and wireless (5G-Advanced and 6G) communication, automotive electronics and so forth. On the energy supply side, the worldwide electricity generation increasing at 6%/year is far less than the growth of the power consumption for just computing alone [1] making the supply vs demand unsustainable. Emerging technologies, such as new material, compound semiconductor devices, and novel circuits and systems both in the Cloud and at the edge are coming to the rescue. But splitting functions then stitching diverse chiplets together is still at the dawn of the new homogeneous and heterogeneous integration (HI) era. It should also be noted that HI should not be limited to semiconductor chips in a package. Some promising progress will be illustrated ranging from electrical, optical, and passive component integration to support co-packaged optics (CPO), Antenna-in-Package/Module (AiP/AiM), socket waveguides, etc. While the advance in devices and manufacturing is essential, there are lots of innovations needed for architectures, systems, algorithms, and circuits to develop the huge variety of applications. Tools like Computer-Aided Design (CAD), Electronic Design Automation (EDA) and automatic software code generation are supposed to help human engineers to find optimal solutions in a timely manner. However, the pace of enhancement for those tools is around an order of magnitude slower than the Moore's Law. The emergence of machine learning and inference, commonly called ML or AI, is showing a great potential [2], [3] especially when the engineers need to deal with high-dimensional complex options. In fact, many designs assisted by ML have surpassed the PPACT achieved by human engineers using conventional software tools. Following the new large AI foundation model framework, 'ChipGPT' could become an essential tool in the future. So far, we have only cracked the tip of the iceberg for all the aforementioned fronts. A lot more challenges and opportunities still lie ahead to be explored. © 2023 IEEE.
"
10.15187/ADR.2023.11.36.4.271,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182146094&origin=inward,Article,SCOPUS_ID:85182146094,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an exploratory experiment using chatgpt in the idea generation process for product-service system,"
AbstractView references

Background Artificial Intelligence(AI) technology is expanding its utilization across various industries, and in the design field, research and development based on AI tools are actively underway. Among them, generative AI tools, such as ChatGPT-4 and Bard, possess potential applicability in the ideation phase of the design process. They are anticipated to overcome the limitations of conventional methods, facilitating the rapid generation of high-quality ideas. Hence, this study aims to systematically verify and explore the effects of generative AI on ideation. Methods Two teams, each composed of four designers, were formed to conduct a comparative experiment between the conventional ideation method and the ideation method utilizing generative AI. They were tasked to generate high-quality ideas over a 4-hour span on the topic of “Healthcare Wearable Devices for Generation Z”. The process was observed without intervention. Following the experiment, the feasibility of AI utilization was confirmed through participant FGI(focus group interview) and IDI(indepth interview). Expert evaluations were conducted to assess the creativity of the ideas generated, and insights were obtained through discussions. Results The method utilizing generative AI produced 6 more ideas than the traditional method, showing an increase of approximately 1.67 times when compared to the conventional method. The quality assessment also showed that the outcomes of the generative AI method were on par with those from the conventional ideation method. Generative AI effectively broadened the confined thinking of designers and clearly displayed efficiency in terms of time-saving. However, there were shortcomings in contextual consistency and structural completeness, making expert validation and convergence essential. Conclusions In order to achieve optimal outcomes using generative AI, it is imperative to provide clear preliminary information and to employ specific, structured questions and prompts, as well as effective communication skills when interacting with AI. Discernment and insight on the part of the designer, and high-level decision-making are essential. By rigorously evaluating and refining the ideas proposed by AI based on established criteria, we can pave the way for superior solutions and designs. It is anticipated that future collaborations between humans and AI will yield increasingly rich and sophisticated results in the field of design. © This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits unrestricted educational and non-commercial use, provided the original work is properly cited.
"
10.1109/IISA59645.2023.10345968,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182024925&origin=inward,Conference Paper,SCOPUS_ID:85182024925,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),evaluating the potential of llms and chatgpt on medical diagnosis and treatment,"
AbstractView references

We evaluate the validity, accuracy, and usefulness of ChatGPT-returned medical diagnosis of lung disease based on symptoms described by a human. Specifically, Tuberculosis and its symptoms are selected as the test case and our evaluation follows the directions of (i) medical validity and accuracy of the returned diagnosis in terms of both context and references, (ii) its usefulness to both doctors and patients and (iii) the economic value added to the healthcare system. It is shown that ChatGPT performs well in diagnosing Tuberculosis, but its performance improves when supervised by a human medical expert. In the interest of adding reproducibility and comparability, we propose a novel general evaluation procedure for the medical domain, to be followed when interacting with Large Language Models. This procedure integrates the various steps employed in our evaluation process and encompasses the review indices utilized for quantifying the outcome. © 2023 IEEE.
"
10.1109/IISA59645.2023.10345880,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182016638&origin=inward,Conference Paper,SCOPUS_ID:85182016638,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),pre-made empowering artificial intelligence and chatgpt: the growing importance of human ai-experts,"
AbstractView references

This paper investigates the augmented responsibility of human Artificial Intelligence experts in the era of empowered pre-made Artificial Intelligence (AI). The responsible and ethical use of pre-made AI is of paramount importance in this evolving technology. AI systems have the potential to impact numerous aspects of society, ranging from healthcare and finance to education and IoT. The decisions made by AI algorithms can have significant consequences for individuals, communities, and even entire industries. Using a comparison to the way widely available medicines require a prescription from medical doctors, human AI experts assume the role of evaluating, recommending, and overseeing the implementation of AI systems, even when pre-built AI solutions may seem user-friendly on the surface. The paper has explored the expanded responsibilities of human AI experts within two contemporary scenarios involving pre-made AI, encompassing LLMs and ChatGPT. These AI technologies are applied in two principal manners: initially, as standalone AI products readily accessible to a wide audience, and secondly, as elements undergoing exploration for integration into other AI-driven software and Intelligent Information Systems (IIS), with the goal of enhancing natural language processing (NLP) features within user interfaces. In all cases, the expertise of human AI professionals is indispensable, and their role is augmented. These professionals bear an increased responsibility for ensuring the responsible and ethical deployment of AI technologies, with a focus on human-centered design, bias mitigation, validation and accuracy estimation of the results, transparency promotion, and the necessary balance between automation and human oversight. This paper performs a review on pre-made AI and ChatGPT together with custom-based AI and shows that recent advance require an augmented role of human AI experts © 2023 IEEE.
"
10.1109/IALP61005.2023.10337079,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181777669&origin=inward,Conference Paper,SCOPUS_ID:85181777669,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),long-term memory for large language models through topic-based vector database,"
AbstractView references

Large language models (LLMs) have garnered sub-stantial attention and significantly transformed the landscape of artificial intelligence, due to their human-like understanding and generation capabilities. However, despite their excellent capabilities, LLMs lack the latest information and are constrained by limited context memory, which limits their effectiveness in many real-time applications that require up-to-date information, such as personal AI assistants. Inspired by the recent study on enhancing LLMs with infinite external memory using vector database, this paper proposes a topic-based vector database to enable LLMs to achieve long-term personalized memory. By leveraging prompt engineering to fully utilize the semantic understanding capabilities of LLMs, an efficient topic-based per-sonalized memory management system is designed to store and update user's preferences and characteristics. This system can be applied in various AI assistant domains, such as companion robots, to efficiently store personal memories of users through conversations, ultimately fulfilling their needs in a personalized manner. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181764157&origin=inward,Conference Paper,SCOPUS_ID:85181764157,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),integrating large language models in art and design education,"
AbstractView references

This paper provides a possible strategy for integrating large language artificial intelligence models (LLMs) in supporting students' education in artistic or design activities. We outline the methodological foundations concerning the integration of CHATGPT LLM in the educational approach aimed at enhancing artistic conception and design ideation. We also present the knowledge and system architecture for integrating LLM in the °’°Kobi system. Finally, this paper discusses some relevant aspects concerning the system's application in a real educational context and briefly reports its preliminary assessment. © CELDA 2023.All rights reserved.
"
10.23919/OCEANS52994.2023.10337348,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181585774&origin=inward,Conference Paper,SCOPUS_ID:85181585774,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),workforce development for the new blue economy: progress and evolution of a master's program in operational oceanography at rutgers university,"
AbstractView references

Rutgers University's accelerated master's degree program in Operational Oceanography (MOO) was established in 2019 to fulfill the workforce gap of the New Blue Economy (NBE), which includes satisfying renewable energy demands as the global population approaches 9 billion by 2050. The MOO program provides students experiential learning opportunities throughout the entire 12-month curriculum, often intersecting with the various technology and data teams that operate a state-of-art ocean observing network and comprise RU COOL (Rutgers University's Center for Ocean Observing Leadership), an internationally oceanographic center of excellence developing new technologies, research, outreach, and educational paradigms for working in the ocean. Students collaborate as a cohort on hands-on activities and assignments involving operational oceanographic equipment, specifically the large fleet of Slocum gliders and expansive network of High-Frequency Radar, both of which are key data pillars for RU COOL. Students work independently on data analysis, learning to analyze, synthesize, and visualize large datasets of real-time oceanographic data and numerical ocean model output on Rutgers University's High-Performance Computing (HPC) cluster, all using the versatile and transferable Python programming language. These were the tenets with which the program was initialized. In the past 4 years, the MOO program has evolved considerably. The first 2 years saw the students mostly remote due to the COVID-19 global pandemic, with limited experiential learning opportunities either in the lab or in the field. The program was pivoted to a strong focus on data processing during this time, such that the graduates would still be both competitive and capable upon degree completion. As those restrictions lifted in the third year and the program returned to the original intent, focus was redistributed across both tenets. Internalizing both student feedback and performance after each course and year, as well as industry feedback on desired skills, the program curriculum shifted significantly for the fourth year. Students were tasked to collaboratively run two quarterly glider deployments. This included coordination with our glider staff team for preparation, and real-time marine weather-based decision making for the operation and piloting, as well as extensive subsequent data analysis. This unique learning opportunity came with significant student responsibility, but the cohort collaboration and tapered support from the glider staff team ultimately allowed for great student successes. The endeavor realized the student-led glider transect offshore of New Jersey, originally conceptualized at the creation of the program. This element of the ocean observatory of RU COOL now enables applied, operational experience for subsequent cohorts in the MOO program. The program's goal has been to capitalize on the unique ocean observing lab resources and capabilities of RU COOL and Rutgers to meet the NBE workforce needs with the accelerated, experiential learning of a new generation of operational oceanography graduate students. Through the continual evolution of the program towards this goal, all MOO graduates have received employment in an oceanography-related career. The program curriculum continues to refine and adapt with each cohort, both to enhance the applied, experiential learning opportunities and to ensure skill proficiency that continually aligns with industry and government workforce needs. And while these global needs exceed the capacity of the MOO program to solely meet, our program may serve as a model for other universities to begin developing their own NBE pipelines. © 2023 The Marine Technology Society (MTS).
"
10.1109/TIE.2023.3342326,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181574519&origin=inward,Article,SCOPUS_ID:85181574519,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"electromagnetic performance analysis, prediction, and multiobjective optimization for u-type ipmsm","
AbstractView references

A novel multiobjective optimization model is presented for the interior permanent magnet synchronous motors (IPMSMs). First of all, in the model initialization stage, appropriate design variables are determined for the actual topology. Also, with reference to the engineering needs, the key electromagnetic characteristics relating to the no-load and on-load magnetic fields are selected as the main optimization objectives in order to achieve global performance improvement. Next, in the model prediction stage, two performance prediction models are proposed, studied, and implemented in parallel. One is an analytical model (AM) based on the improved subdomain approach and the magnetic equivalent circuit. It is utilized to predict the no-load electromagnetic characteristics of IPMSM. The complex structure and core nonlinearity of IPMSM are also reasonably accounted for. The other is a surrogate model (SM) based on the intelligent machine learning language (support vector regression). It is utilized to predict the on-load electromagnetic characteristics of IPMSM. The reliance on finite-element analysis is further also minimized. The proposed AM and SM have commendable behavior in terms of analysis speed, prediction accuracy, and storage consumption. Meanwhile, AM screens credible samples as well as provides robust support for the construction of SM. All of these signs build a solid foundation for a substantial boost in optimization efficiency. Afterward, in the model optimization stage, the advanced nondominated sorting genetic algorithm III is investigated to complete the final multiobjective optimization. Ultimately, a large number of calculations, simulations, and experiments have highlighted the effectiveness, rationality, and engineering practicality of this research. IEEE
"
10.1109/TSE.2023.3348515,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181567276&origin=inward,Article,SCOPUS_ID:85181567276,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"safety and performance, why not both? bi-objective optimized model compression against heterogeneous attacks toward ai software deployment","
AbstractView references

The size of deep learning models in artificial intelligence (AI) software is increasing rapidly, which hinders the large-scale deployment on resource-restricted devices (<italic>e.g.</italic>, smartphones). To mitigate this issue, AI software compression plays a crucial role, which aims to compress model size while keeping high performance. However, the intrinsic defects in the big model may be inherited by the compressed one. Such defects may be easily leveraged by attackers, since the compressed models are usually deployed in a large number of devices without adequate protection. In this paper, we try to address the safe model compression problem from a safety-performance co-optimization perspective. Specifically, inspired by the test-driven development (TDD) paradigm in software engineering, we propose a test-driven sparse training framework called <italic>SafeCompress</italic>. By simulating the attack mechanism as the safety test, SafeCompress can automatically compress a big model to a small one following the dynamic sparse training paradigm. Then, considering two kinds of representative and heterogeneous attack mechanisms i.e., black-box membership inference attack and white-box membership inference attack, we develop two concrete instances called BMIA-SafeCompress and WMIA-SafeCompress. Further, we implement another instance called MMIA-SafeCompress by extending SafeCompress to defend against the occasion when attackers conduct black-box and white-box membership inference attacks simultaneously. Extensive experiments are conducted on five datasets for both computer vision and natural language processing tasks. The results verify the effectiveness and generalizability of our method. We also discuss how to adapt SafeCompress to other attacks besides MIA, demonstrating the flexibility of SafeCompress. IEEE
"
10.1109/ICSME58846.2023.00051,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181540255&origin=inward,Conference Paper,SCOPUS_ID:85181540255,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a case study of fairness in generated images of large language models for software engineering tasks,"
AbstractView references

Bias in Large Language Models (LLMs) has significant implications. Since they have revolutionized content creation on the web, they can lead to more unfair outcomes, lack of inclusivity, reinforcement of stereotypes and ethical and legal concerns. Notably, OpenAI has recently made claims they have introduced a new technique to ensure that DALL-E-2 generates images of people accurately reflect the diversity of the world's population. In order to investigate bias within the field of Software Engineering, the study utilized DALL-E-2 image generation to assess 56 tasks related to software engineering. Another objective was to determine the impact of OpenAI's new measures on the generated images for these specific tasks. Two sets of experiments were conducted. In one set, the tasks were prefixed with the clause As a Software Engineer, while in the other set, only the tasks themselves were used. The tasks were presented in a gender-neutral manner, and the AI was instructed to generate images for each task 20 times. For a female-dominant task of doing administrative tasks, 40 more images were generated. The study revealed a large gender bias in the 2,280 images generated. For instance, in the subset of experiments with prompts explicitly incorporating the phrase As a software engineer, only 2% of the generated images portrayed female protagonists. In all the images in this setting, male protagonists were dominant and in 45 tasks 100% of the protagonists were male. Notably, images generated without the prefixed clause only had more female protagonists in 'provide comments on project milestones' and 'provide enhancements', while other tasks did not exhibit a similar pattern. The findings emphasize unsuitability of implemented guardrails and the importance of further research on LLMs assessments. Further research is needed in LLMs to find out where their guardrails fail so companies can address them properly. © 2023 IEEE.
"
10.1109/ICSME58846.2023.00067,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181538847&origin=inward,Conference Paper,SCOPUS_ID:85181538847,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),bugsplainer: leveraging code structures to explain software bugs with neural machine translation,"
AbstractView references

Software bugs cost the global economy billions of dollars each year and take up ≈50% of the development time. Once a bug is reported, the assigned developer attempts to identify and understand the source code responsible for the bug and then corrects the code. Over the last five decades, there has been significant research on automatically finding or correcting software bugs. However, there has been little research on automatically explaining the bugs to the developers, which is essential but a highly challenging task. In this paper, we propose Bugsplainer, a novel web-based debugging solution that generates natural language explanations for software bugs by learning from a large corpus of bug-fix commits. Bugsplainer leverages code structures to reason about a bug and employs the fine-tuned version of a text generation model - CodeT5 - to generate the explanations.Tool video: https://youtu.be/xga-ScvULpk © 2023 IEEE.
"
10.1109/ICSME58846.2023.00043,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181532882&origin=inward,Conference Paper,SCOPUS_ID:85181532882,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),aligning documentation and q&amp;a forum through constrained decoding with weak supervision,"
AbstractView references

Stack Overflow (SO) is a widely used question-and-answer (QA) forum dedicated to software development. It plays a supplementary role to official documentation (DOC for short) by offering practical examples and resolving uncertainties. However, the process of simultaneously consulting both the documentation and SO posts can be challenging and time-consuming due to their disconnected nature. In this study, we propose DOSA, a novel approach to automatically align SO and DOC, which inject domain-specific knowledge about the DOC structure into large language models (LLMs) through weak supervision and constrained decoding, thereby enhancing knowledge retrieval and streamlining task completion during the software development procedure. Our preliminary experiments find that DOSA outperforms various widely-used baselines, showing the promise of using generative retrieval models to perform low-resource software engineering tasks. © 2023 IEEE.
"
10.1109/CSITSS60515.2023.10334093,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181532237&origin=inward,Conference Paper,SCOPUS_ID:85181532237,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),empowering engineering training: a portable pyqt5-based gui for system validation with simulink dlls,"
AbstractView references

Modelling and simulation is an integral part of Systems Engineering. Models represent the key characteristics or behaviour/functions of the selected physical or abstract systems. Simulation is the imitation of the operation of a real-world system. MATLAB/Simulink is often used for this purpose. Graphical User Interfaces (GUI) are created to understand the functionality of developed models using MATLAB's App Designer application. Though these are robust, they pose several problems, particularly for large models, such as the requirement of an expensive MATLAB license by the end-user for running the application, reduction in execution speed, and usage of large amounts of device storage space. In this paper, a solution to this issue is proposed. A stand-alone GUI application can be developed using Python's PyQT5- a framework for developing GUIs. The usage of a foreign function library in Python called 'ctypes', to call the functions of the DLL into the developed Python-based GUI, is also realized in this paper. The GUI can run on a computer without MATLAB software installed. The results of the GUI agree with that of the Simulink model. Consequently, this standalone GUI application holds promise for streamlining the verification and validation processes of complex models, offering a cost-effective, efficient, and easily accessible alternative that empowers engineering training initiatives. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181391707&origin=inward,Conference Paper,SCOPUS_ID:85181391707,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),making large language models better data creators,"
AbstractView references

Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce human effort involve labeling or generating data using LLMs. Although these methods are effective for certain applications, in practice they encounter difficulties in real-world scenarios. Labeling data requires careful data selection, while generating data necessitates task-specific prompt engineering. In this paper, we propose a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces. In our experiments we demonstrate that instruction-following LLMs are highly cost-effective data creators, and that models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world. ©2023 Association for Computational Linguistics.
"
10.1109/VTC2023-Fall60731.2023.10333364,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181171853&origin=inward,Conference Paper,SCOPUS_ID:85181171853,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),quality-of-trust in 6g: combining emotional and physical trust through explainable ai,"
AbstractView references

Wireless networks like many multi-user services have to balance limited resources in real-time. In 6G, increased network automation makes consumer trust crucial. Trust is reflect in both a personal emotional sentiment as well as a physical understanding of the transparency of AI decision making. Whilst there has been isolated studies of consumer sentiment to wireless services, this is not well linked to the decision making engineering. Likewise, limited recent research in explainable AI (XAI) has not established a link to consumer perception.Here, we develop a Quality-of-Trust (QoT) KPI that balances personal perception with the quality of decision explanation. That is to say, the QoT varies with both the time-varying sentiment of the consumer as well as the accuracy of XAI outcomes. We demonstrate this idea with an example in Neural Water-Filling (N-WF) power allocation, where the channel capacity is perceived by artificial consumers that communicate through Large Language Model (LLM) generated text feedback. Natural Language Processing (NLP) analysis of emotional feedback is combined with a physical understanding of N-WF decisions via meta-symbolic XAI. Combined they form the basis for QoT. Our results show that whilst the XAI interface can explain up to 98.9% of the neural network decisions, a small proportion of explanations can have large errors causing drops in QoT. These drops have immediate transient effects in the physical mistrust, but emotional perception of consumers are more persistent. As such, QoT tends to combine both instant physical mistrust and long-term emotional trends. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181170987&origin=inward,Conference Paper,SCOPUS_ID:85181170987,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),italian crossword generator: enhancing education through interactive word puzzles,"
AbstractView references

Educational crosswords offer numerous benefits for students, including increased engagement, improved understanding, critical thinking, and memory retention. Creating high-quality educational crosswords can be challenging, but recent advances in natural language processing and machine learning have made it possible to use language models to generate nice wordplays. The exploitation of cutting-edge language models like GPT3-DaVinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT-uncased has led to the development of a comprehensive system for generating and verifying crossword clues. A large dataset of clue-answer pairs was compiled to fine-tune the models in a supervised manner to generate original and challenging clues from a given keyword. On the other hand, for generating crossword clues from a given text, Zero/Few-shot learning techniques were used to extract clues from the input text, adding variety and creativity to the puzzles. We employed the fine-tuned model to generate data and labeled the acceptability of clue-answer parts with human supervision. To ensure quality, we developed a classifier by fine-tuning existing language models on the labeled dataset. Conversely, to assess the quality of clues generated from the given text using zero/few-shot learning, we employed a zero-shot learning approach to check the quality of generated clues. The results of the evaluation have been very promising, demonstrating the effectiveness of the approach in creating high-standard educational crosswords that offer students engaging and rewarding learning experiences. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
10.1109/ICE/ITMC58018.2023.10332336,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181151372&origin=inward,Conference Paper,SCOPUS_ID:85181151372,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai and data in engineering and innovation: towards a sustainable future?,"
AbstractView references

29th International Conference on Engineering, Technology, and Innovation (ICE 2023) touched upon the critical issues in engineering of the generative AI era. By looking at the key challenges in data-driven project management and sustainable development, the overarching theme emerging from the event was one of adaptation to AI and data tools as essential part of the work process and supporting new approaches in engineering, technology development and innovation and entrepreneurship. This points to a promising future ahead, where digital transformation reaches the extended engineering disciplines, which is the theme of the following ICE conference. Having said that, the use of digital tools also presents a number of problems, from data-driven biases to challenges of interdisciplinary, inter-organisational and inter-entity collaborations. © 2023 IEEE.
"
10.1109/ISEM59023.2023.10334852,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181140954&origin=inward,Conference Paper,SCOPUS_ID:85181140954,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),overview of an ai-based methodology for design: case study of a high efficiency electric vehicle chassis for the shell eco-marathon,"
AbstractView references

The design process of a chassis for a high-efficiency vehicle involves considering several unique parameters due to its distinct operational needs. The constraints specified in the official regulations for the Shell Eco-Marathon prototype battery electric class could ostensibly limit design innovations. However, this paper introduces a methodological approach employing Machine Learning, facilitated by Large Language Models, which substantially broadens the spectrum of conceptual design possibilities. Further, the widely recognized technique of Computer-Aided Design, coupled with Generative Design, is leveraged to optimize specific sections of the chassis structure. The final stage of this study involves rigorous testing of the designed chassis to assess its mechanical performance, guided by the stringent benchmarks set by the competition. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181134921&origin=inward,Conference Paper,SCOPUS_ID:85181134921,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the use of generative ai in the domain of human creations – a case for co-evolution?,"
AbstractView references

The appearance and sudden success of generative technologies and, within this domain, Large Language Models (LLMs) have generated many open questions about the benefits and challenges of those technologies and the approaches to deal with them in the future. Put into context, we can see a new level of dialog ability and contextual understanding that is new to the interaction between the human user and the machine. This contribution uses the concept of co-evolution, originally from the biology field, to explore some of the implications of the new technologies, both on an individual and at a collective level. The perspective of co-evolution will be reflected in the application areas of Software Development and for the (higher) Education context to provide detailed insights. © 2023 CEUR-WS. All rights reserved.
"
10.1002/sys.21740,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180827005&origin=inward,Article,SCOPUS_ID:85180827005,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),how to early integrate operational diagnosis objectives in model-driven engineering processes: a methodological proposal based on fault and behavior trees,"
AbstractView references

To help operators perform their diagnosis tasks more efficiently, the authors put forward a novel methodology introducing a new type of model, dedicated to operations, co-created in parallel with the design models, in the preliminary stages of system development, using the language semantics of behavior trees. In this paper, the authors present the need for this new model type as expressed by the industry and justify their choice for adopting behavior trees, while illustrating in detail the proposed methodology. © 2023 The Authors. Systems Engineering published by Wiley Periodicals LLC.
"
10.36740/WLek202311101,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180737274&origin=inward,Article,SCOPUS_ID:85180737274,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),from text to diagnose: chatgpt's efficacy in medical decision-making,"
AbstractView references

OBJECTIVE: The aim: Evaluate the diagnostic capabilities of the ChatGPT in the field of medical diagnosis. PATIENTS AND METHODS: Materials and methods: We utilized 50 clinical cases, employing Large Language Model ChatGPT-3.5. The experiment had three phases, each with a new chat setup. In the initial phase, ChatGPT received detailed clinical case descriptions, guided by a ""Persona Pattern"" prompt. In the second phase, cases with diagnostic errors were addressed by providing potential diagnoses for ChatGPT to choose from. The final phase assessed artificial intelligence's ability to mimic a medical practitioner's diagnostic process, with prompts limiting initial information to symptoms and history. RESULTS: Results: In the initial phase, ChatGPT showed a 66.00% diagnostic accuracy, surpassing physicians by nearly 50%. Notably, in 11 cases requiring image inter¬pretation, ChatGPT struggled initially but achieved a correct diagnosis for four without added interpretations. In the second phase, ChatGPT demonstrated a remarkable 70.59% diagnostic accuracy, while physicians averaged 41.47%. Furthermore, the overall accuracy of Large Language Model in first and second phases together was 90.00%. In the third phase emulating real doctor decision-making, ChatGPT achieved a 46.00% success rate. CONCLUSION: Conclusions: Our research underscores ChatGPT's strong potential in clinical medicine as a diagnostic tool, especially in structured scenarios. It emphasizes the need for supplementary data and the complexity of medical diagnosis. This contributes valuable insights to AI-driven clinical diagnostics, with a nod to the importance of prompt engineering techniques in ChatGPT's interaction with doctors.
"
10.11159/cist23.125,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180637548&origin=inward,Conference Paper,SCOPUS_ID:85180637548,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automatically enriching content for a behavioral health learning management system: a first look,"
AbstractView references

Deep generative AI models have been evolving rapidly and are being applied to assist users in many domains. We consider an initial case study of applying this technology to semi-automated content enrichment for an online learning management system used by behavioral health professionals. Our goal is to optimize the use of domain expertise while also scaling the production efficiency of learning assets for users. We identify the possible opportunities for its use, discuss potential challenges and concerns. Finally, we provide prompt engineering strategies and initial quantitative results towards semi-automating one type of rime consuming editorial task, to gauge the feasibility of our approach. Results show that there is significant promise in using such an approach, and suggest that a larger, more rigorous study is required. © 2023, Avestia Publishing. All rights reserved.
"
10.1007/978-3-031-48639-5_4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180636896&origin=inward,Conference Paper,SCOPUS_ID:85180636896,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),analyzing scrum team impediments using nlp,"
AbstractView references

In this research, we focus on the impediments encountered by students in capstone projects following the Scrum methodology. Scrum meeting notes were collected in a dataset to permit Scrum roles and instructors to monitor progress and issues. We identified 9 categories of impediments in this dataset: Android, Coding Skills, Debugging, External Factors, Firebase/Database, Git/GitHub, Teamwork, Time Management, and UI/UX Design. We developed a Large Language Model (LLM) to classify these impediments. Natural Language Processing (NLP) has the potential to support software engineering processes. The novelty of this research is that it attempts to identify impediments faced by students’ Scrum teams with AI and support students and instructors. The relevance of the approach was discussed with subject matter experts (SME) of the industry. The proposed model is useful in both the academic and industry settings, to identify on-the-fly areas that need attention and, if fixed, would increase team productivity. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-49002-6_4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180537326&origin=inward,Conference Paper,SCOPUS_ID:85180537326,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring the ethical and societal concerns of generative ai in internet of things (iot) environments,"
AbstractView references

Generative Artificial Intelligence (Generative AI) and the Internet of Things (IoT) are two rapidly evolving technologies that have the potential to revolutionise many aspects of our lives today. Generative AI, a subset of AI technologies, has a unique ability to create and manipulate content based on patterns learned from vast datasets, and when combined with the interconnected nature of IoT devices, they hold great promise across industries. Leveraging deep learning capabilities, Generative AI can intelligently analyze and apply the vast amounts of data generated from the internet to optimize operations and drive innovation. However, the potential integration of Generative AI and IoT systems in the future could raise notable ethical and societal concerns. These may encompass worries about privacy, security risks, biases and fairness challenges, lack of autonomy and human control, and emerging issues like the lack of ethical content creation. Given the relatively new nature of Generative AI technologies, this paper explores the ethical and societal concerns that may arise should Generative AI and the Internet of Things (IoT) environments integrate. By shedding light on these ethical and societal concerns, this research seeks to stimulate a nuanced understanding of the critical balance between technological advancement and responsible deployment, guiding stakeholders towards an ethically sound integration of Generative AI within IoT ecosystems. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1109/APSIPAASC58517.2023.10317226,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180008567&origin=inward,Conference Paper,SCOPUS_ID:85180008567,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),revolutionizing formative assessment in stem fields: leveraging ai and nlp techniques,"
AbstractView references

Artificial intelligence (AI) has been extensively studied in science, technology, engineering, and mathematics (STEM), but there is a disparity between AI-generated and human-written scientific content. To bridge this gap, a prototype utilizing Natural Language Processing (NLP) techniques and a large language model (LLM) generates assessment questions and evaluates student answers. This formative assessment system offers a user-friendly and scalable solution for higher education educators. It tailors' assessments to individual students, accommodates varying capabilities, and facilitates performance analysis. Through rigorous evaluation and benchmarking, the prototype ensures alignment with High-Level Performance (HLP) standards. This AI-assisted formative assessment system enhances efficiency and efficacy by providing accurate and timely feedback. It has the potential to significantly improve STEM education through scalable and personalized formative assessment experiences. AI and NLP enable educators to access tailored assessment options, enhancing learning outcomes and the overall educational experience. © 2023 IEEE.
"
10.1109/ICCE-Asia59966.2023.10326406,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179891052&origin=inward,Conference Paper,SCOPUS_ID:85179891052,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),new generation encryption chip based on the gan chaos model and the geffe architecture,"
AbstractView references

In this paper, we proposed a new generation encryption chip based on the generative adversarial network (GAN) Chaos model and the Geffe architecture. Compared with software encryption, hardware encryption is not easy to be attacked, data and programs are mixed, and hardware cracking analysis is difficult. It can effectively avoid attacks such as reverse engineering and the calculation speed is faster. In order to perform more unpredictable hardware encryption, this study designs the GAN algorithm to generate encryption circuit, establishes the convolutional neural network (CNN) model through training, and combines the random number generator circuit of Geffe architecture to realize the system with digital integrated circuit. To reduce and improve the calculation speed, the iterative calculation method is used to design, which can reduce the circuit area, optimize memory consumption, improve the calculation speed of CNN, and achieve a faster AI encryption inference chip. The experimental results prove that our proposed method reduces the computing time by 78.4% and realizes a high-performance encryption chip. © 2023 IEEE.
"
10.21427/4VSK-1V59,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179853047&origin=inward,Conference Paper,SCOPUS_ID:85179853047,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),going beyond intentions: a methodology for assessing entrepreneurial activity among engineering education alumni,"
AbstractView references

This research paper proposes a novel methodology for evaluating entrepreneurial activity among engineering education alumni using their public CVs as our main source of information. The objective is to go beyond measuring entrepreneurship intentions or mindset through surveys, and instead analyse actual career data to assess the impact of entrepreneurship education. The study utilises shared user data and employs GPT (Generative Pretrained Transformer) models to infer entrepreneurial activity that extends beyond job titles, delving into the specific responsibilities and achievements associated with each position. The analysis shows that the proposed methodology, which uses context enriching to enhance model accuracy, effectively identifies instances of entrepreneurial activity among CVs profiles data. This approach provides a way to evaluate the effectiveness of entrepreneurship and innovation courses. Combining the insights gained through the proposed method with internal data sources would enable institutions to conduct a comprehensive evaluation of program impact on alumni career paths. The study underscores the potential of AI models to facilitate the collection and analysis of data that has traditionally been challenging to access. Moreover, the research highlights the importance of evaluating the long-term impact of entrepreneurship education on alumni career trajectories, a key factor in addressing the growing field of engineering education. Ultimately, this study contributes to the academic discourse on entrepreneurship education by offering a novel approach for assessing the impact of such programs on alumni outcomes, thus enabling institutions to make data-driven decisions to improve program offerings. © 2023 SEFI 2023 - 51st Annual Conference of the European Society for Engineering Education: Engineering Education for Sustainability, Proceedings. All Rights Reserved.
"
10.1109/ICCCNT56998.2023.10307924,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179850621&origin=inward,Conference Paper,SCOPUS_ID:85179850621,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),privacy-preserving personal identifiable information (pii) label detection using machine learning,"
AbstractView references

In today's data-driven world, the protection of PII is of paramount importance to safeguard personal privacy. PII tags serve as crucial markers for identifying and processing sensitive information within databases. However, the authentication and registration of PII tags can be time-consuming and error prone. To address this challenge, we propose a method for privacy controlled PII tag detection that harnesses the power of machine learning (ML) combined with regular expressions. Proposed approach leverages various techniques, including feature engineering, adaptive learning, and machine learning, to extract meaningful patterns and relationships from data. By training the model on large datasets that encompass diverse PII elements such as names, addresses, phone numbers, email addresses, and social security numbers, enable it to learn and classify PII identifiers across different documents effectively. One of the key advantages of the proposed method is its ability to automate the detection of PII identifiers, thereby reducing the reliance on manual interpretation and minimizing the potential for human error. By integrating machine learning algorithms, empower organizations to efficiently identify and process sensitive information present in their databases, bolstering privacy protection measures. Moreover, this approach facilitates the development of scalable and accurate solutions for privacy based PII tag search. This advancement paves the way for enhanced data privacy across the enterprise, ensuring compliance with regulations and standards pertaining to the protection of personal information. By combining the strengths of ML and regular expressions, the proposed method enables organizations to detect and handle PII identifiers more effectively. This not only streamlines data management processes but also strengthens privacy safeguards, and more secure and privacy-aware data ecosystem. © 2023 IEEE.
"
10.21427/0T6R-FZ62,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179844340&origin=inward,Conference Paper,SCOPUS_ID:85179844340,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ethical concerns and responsible use of generative artificial intelligence in engineering education,"
AbstractView references

The use of educational technologies that use elements of machine learning (ML) and artificial intelligence (AI) are becoming common across the engineering education terrain. With the wide adoption of generative AI based applications, this trend is only going to grow. Not only is the use of these technologies going to impact teaching, but engineering education research practices are as likely to be affected as well. From data generation and analysis, to writing and presentation, all aspects of research will potentially be shaped. In this practice paper we discuss the ethical implications of the use of generative AI technologies on engineering teaching and engineering education research. We present a discussion of potential and futuristic concerns raised by the use of these technologies. We bring to the fore larger organizational and institutional issues and the need for a framework for responsible use of technology within engineering education. Finally, we engage with the current literature and popular writing on the topic to build an understanding of the issues with the potential to apply them in teaching and research practices. © 2023 SEFI 2023 - 51st Annual Conference of the European Society for Engineering Education: Engineering Education for Sustainability, Proceedings. All Rights Reserved.
"
10.1109/UEMCON59035.2023.10316132,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179755719&origin=inward,Conference Paper,SCOPUS_ID:85179755719,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),how to detect ai-generated texts?,"
AbstractView references

Recent advances in large language models (LLMs) have significantly improved the quality of synthetic text data. LLMs imitate human writing patterns to produce highly natural text, raising serious ethical, moral, legal, social, and economic concerns in various industries. To address these issues, we present two methods to distinguish Synthetically Generated Text (SGT) from Human-Written Text (HWT): machine learning and text similarity. Our methods include procedures for dataset creation, feature engineering, model training, and result analysis with feature importance analysis and explanation models. As part of our research, we created two datasets - a Wikipedia-based and a US Election 2024 news article-based dataset using ChatGPT, from which we obtained promising results of up to 99.xxx% accuracy. These datasets can be used as open-source datasets in future studies. We also identified a set of handcrafted features that can serve as the baseline feature set for future research. © 2023 IEEE.
"
10.5209/esmp.88582,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179619011&origin=inward,Article,SCOPUS_ID:85179619011,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"employer activity in public television in the age of automation: employer branding, recruitment channels, selection processes, skills and professional profiles in demand","
AbstractView references

This research delves into the employment activity carried out by public broadcasters in Europe in the current context of development of generative artificial intelligence, analyzing the recruitment channels used, the selection processes followed, the most demanded professional skills and profiles, current job offers, and the procedures for creating an employer brand. The methodological design to achieve these purposes requires a phased execution based on the application of a methodological triangulation of qualitative techniques. The proposals of 10 European public broadcasters are analyzed, which are complemented by in-depth semi-structured interviews with those responsible for innovation or technology on behalf of these corporations. The results reveal that the proliferation of offers for digital positions does not imply a fading of traditional ones, and that these technological employment opportunities are mainly related to data engineering, cybersecurity, big data and cloud management processes, and only in isolated cases. they are tied to specific machine learning or AI functions. The findings also show that interpersonal skills such as adaptability to change prevail over the eminently technical ones, and that broadcasters seek a competitive advantage in the disputed market of talent by building their employer brand and optimizing their recruitment portals. © 2023 Universidad Complutense de Madrid. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179556549&origin=inward,Conference Paper,SCOPUS_ID:85179556549,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using large language models for knowledge engineering (llmke): a case study on wikidata,"
AbstractView references

In this work, we explore the use of Large Language Models (LLMs) for knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge. For this task, given subject and relation pairs sourced from Wikidata, we utilize pre-trained LLMs to produce the relevant objects in string format and link them to their respective Wikidata QIDs. We developed a pipeline using LLMs for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata entity mapping. The method achieved a macro-averaged F1-score of 0.701 across the properties, with the scores varying from 1.00 to 0.328. These results demonstrate that the knowledge of LLMs varies significantly depending on the domain and that further experimentation is required to determine the circumstances under which LLMs can be used for automatic Knowledge Base (e.g., Wikidata) completion and correction. The investigation of the results also suggests the promising contribution of LLMs in collaborative knowledge engineering. LLMKE won Track 2 of the challenge. The implementation is available at: https://github.com/bohuizhang/LLMKE. © 2023 CEUR-WS. All rights reserved.
"
10.5220/0012155900003598,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179556199&origin=inward,Conference Paper,SCOPUS_ID:85179556199,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),development of an owl ontology based on the function-oriented system architecture to support data synchronization between sysml and domain models,"
AbstractView references

A promising approach to systems engineering is called Model-Based Systems Engineering (MBSE), which is increasingly accepted to support the development process of complex systems. In MBSE,engineers use Systems Modeling Language (SysML) for formalized modeling of function-oriented system architecture and solution architecture that enable the integration of various domain models to support a seamless system development process. Domain models simulate the physical behaviors of systems with design parameters so as to realize the quantitative analysis and verification of systems. These design parameters often exist in multiple heterogeneous data sources and often rely on manual importation into the SysML model. However, when systems become complex with a large data volume, manual data exchanges between multiple data sources and SysML models become time-consuming and error-prone. Therefore, this work proposes an ontology based on Web Ontology Language (OWL) for managing data in a standardized way and solving heterogeneous problems. Then, an automatic synchronization mechanism is established between the ontology and the SysML model for easy exchange of data. This work demonstrates and validates the proposed approach with a case study of a technical system (i.e., wind turbine system). The contribution of this work is the creation of a standardized OWL ontology that supports an automatic synchronization between the data from multiple domain models and the SysML models, thus reducing the manual effort of dealing with heterogeneous data sources and the risk of data inconsistency occurring in manual data transmission. Copyright © 2023 by SCITEPRESS – Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179555187&origin=inward,Conference Paper,SCOPUS_ID:85179555187,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards syntax-aware pretraining and prompt engineering for knowledge retrieval from large language models,"
AbstractView references

The ability to access relational knowledge from LLM parameters, known as relational knowledge retrieval (rKR), is considered a critical factor in their capacity to comprehend and interpret natural language. However, the role of syntax in this context has not been adequately explored. In this position paper, we hypothesize a close link between the accessibility of relational knowledge and syntax. We discuss related works and lay out a research agenda focused on rKR from self-supervised LLMs without or with minimal fine-tuning and aiming at understanding the impact of syntax on rKR. This involves examining biases, factors affecting result reliability and robustness, and analyzing the effect of syntactic features in training corpora on rKR. We argue that rKR can be improved through syntax-aware pretraining and prompt engineering, and propose a dedicated research agenda geared toward exploring the impact of syntax on knowledge retrieval. © 2023 CEUR-WS. All rights reserved.
"
10.1109/AICT59525.2023.10313167,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179525440&origin=inward,Conference Paper,SCOPUS_ID:85179525440,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),effectiveness of generative artificial intelligence for scientific content analysis,"
AbstractView references

Generative artificial intelligence (GenAI) in general, and large language models (LLMs) in particular, are highly fashionable. As they have the ability to generate coherent output based on prompts in natural language, they are promoted as tools to free knowledge workers from tedious tasks such as content writing, customer support and routine computer code generation. Unsurprisingly, their application is also attractive to professionals in the research domain, where mundane and laborious tasks, such as literature screening, are commonplace. We evaluate Vertex AI 'text-bison', a foundational LLM model, in a real-world academic scenario by replicating parts of a popular systematic review in the information management domain. By comparing the results of a zero-shot LLM-based approach with those of the original study, we gather evidence on the suitability of state-of-the-art general-purpose LLMs for the analysis of scientific content. We show that the LLM-based approach delivers good scientific content analysis performance for a general classification problem (ACC =0.9), acceptable performance for a domain-specific classification problem (ACC =0.8) and borderline performance for a text comprehension problem (ACC ≈0.69). We conclude that some content analysis tasks with moderate accuracy requirements may be supported by current LLMs. As the technology will evolve rapidly in the foreseeable future, studies on large corpora, where some inaccuracies are tolerable, or workflows that prepare large data sets for human processing, may increasingly benefit from the capabilities of GenAI. © 2023 IEEE.
"
10.1016/B978-0-323-96104-2.00005-1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179206958&origin=inward,Book Chapter,SCOPUS_ID:85179206958,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),from synapses to ephapsis: embodied cognition and wearable personal assistants,"
AbstractView references

Despite their significant successes, neural networks typically represent relatively static memory structures and solve static classification problems. The next step in the evolution of AI systems will be the capture of the dynamic aspects of cognition. The dynamics are embodied in the ephaptic fields of the neocortex and limbic system, formed by the vast populations of resonating electric dipoles comprised of a multitude of ion channels present on the surface of each neuron. These ion-based e-fields form dynamic brainwaves, which synchronize distant areas of the cortex at the speed of light (orders of magnitude faster than axonal pulse speed) via resonance in the beta, theta, and gamma range. They are quite important for the understanding of the working of the brain. Ephaptic fields are also a perfect bridge to the motor behavior of organisms. This chapter shows that it is not only walking and grasping which is motor based, but also that vision, speech, and in fact, all perception and even memory are grounded in motor action. This has deep implications for the design of AI-based personal assistants. This chapter argues that field approach, ephapsis, and motor action are indispensable if the goal for the future generations of wearable sensor-based personal assistants is the real-time capture of user intent. Multimodal correlation of motor sensors of user daily activities is the essential ingredient, which so far eluded AI researchers and precluded wearable assistants from a wider user adoption. It turns out that thinking is embodied indeed. We discuss how recent developments in making movies from chat and merging large language models AI and 3D, can lead to a new generation of personal assistants, where metaverse, AI, and AR are coming together in the most surprising ways. © 2024 Elsevier Inc. All rights reserved.
"
10.1115/DETC2023-116833,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179139856&origin=inward,Conference Paper,SCOPUS_ID:85179139856,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),state of the art: a review of ai art generation methods for rigorous design,"
AbstractView references

Over the past several years, humans have developed various new AI art tools for artistic and conceptual design purposes- this paper aims to review the development of AI's abilities to generate original, rigorous designs for technical applications. First, this paper examines three modern AI art methods: Generative Adversarial Networks, Convolutional Neural Networks, and stable diffusion. Then, we review the main concepts of each method and test a representative AI on its abilities for rigorous design. Various prompts are used, including simple, formulaic, AI-generated, and human-generated prompts from an architectural engineering class assignment. © 2023 American Society of Mechanical Engineers (ASME). All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179132345&origin=inward,Conference Paper,SCOPUS_ID:85179132345,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring the student perspective: assessing technology readiness and acceptance for adopting large language models in higher education,"
AbstractView references

Digital technologies are changing and will continue to change how we learn and teach today and in the future. With the latest developments in the field of generative artificial intelligence (AI), particularly large language models (LLMs), the question of using AI-based tools in academic education is ruling the current discussions about the transformative impact of AI in higher education (HE). These discussions range from banning these technologies for learning and teaching in HE to guided study support. This study avoids taking up these multifarious and partly controversial debates. Instead, we show how students perceive using AI-based tools for automated text generation for their studies. Drawing on a synthesis of two theories: the 'Technology Readiness Index' (TRI) and 'Technology Acceptance Model' (TAM). The model is validated based on survey data collected among undergraduate first-semester students (N=111) of a computer science-related study programme in Germany in winter 2022/23. The students had to evaluate their relationship to that new technology focusing on their readiness for technology adoption and acceptance. By analysing the collected data with a partial least squares model, we find that the optimism toward the new technology positively influences technology acceptance, while discomfort with the technology negatively influences perceived ease of use. The paper concludes with recommendations for action for adopting LLMs in HE. A proper investment in building AI skills in academic teaching plays a valuable role in fostering the students' positive attitude and innovativeness towards this new technology. Additionally, there is a need for more education about the risks and challenges of using this technology to reduce the impact of factors such as discomfort on ease of use. This requires a factual discourse, away from the current hype-induced exaggerated and hyperbolic statements, for instance, in developing formal guidance for universities. © 2023 Academic Conferences Limited. All rights reserved.
"
10.1115/DETC2023-116971,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179131182&origin=inward,Conference Paper,SCOPUS_ID:85179131182,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chat generative pretrained transformer: extinction of the designer or rise of an augmented designer,"
AbstractView references

Systematic design process is used in engineering systems design to develop solutions to problems of varying complexity. There have been efforts within the community to develop tools to perform automated generative design at varying stages of the systematic design process. However, a Large Language Model (LLM) has never been used. To this end, this paper presents an initial investigation into the use of OpenAI's ChatGPT to automatically generate solutions to an engineering problem. It is demonstrated that for the most part ChatGPT is quite capable of generating conceptual design for an engineering problem. In light of this technology, this paper floats questions on the future direction of research and education for the engineering systems design community. © 2023 American Society of Mechanical Engineers (ASME). All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179122077&origin=inward,Conference Paper,SCOPUS_ID:85179122077,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring prompt engineering with gpt language models for document-level machine translation: insights and findings,"
AbstractView references

This paper describes Lan-Bridge Translation systems for the WMT 2023 General Translation shared task. We participate in 2 directions: English to and from Chinese. With the emergence of large-scale models, various industries have undergone significant transformations, particularly in the realm of document-level machine translation. This has introduced a novel research paradigm that we have embraced in our participation in the WMT23 competition. Focusing on advancements in models such as GPT-3.5, we have undertaken numerous prompt-based experiments. Our objective is to achieve optimal human evaluation results for document-level machine translation, resulting in our submission of the final outcomes in the general track. © 2023 Association for Computational Linguistics.
"
10.1109/MNET.2023.3335255,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179037394&origin=inward,Article,SCOPUS_ID:85179037394,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"optimizing mobile-edge ai-generated everything (aigx) services by prompt engineering: fundamental, framework, and case study","
AbstractView references

As the next-generation paradigm for content creation, AI-Generated Content (AIGC), i.e., generating content automatically by Generative AI (GAI) based on user prompts, has gained great attention and success recently. With the ever-increasing power of GAI, especially the emergence of Pretrained Foundation Models (PFMs) that contain billions of parameters and prompt engineering methods (i.e., finding the best prompts for the given task), the application range of AIGC is rapidly expanding, covering various forms of information for human, systems, and networks, such as network designs, channel coding, and optimization solutions. In this article, we present the concept of mobile-edge AI-Generated Everything (AIGX). Specifically, we first review the building blocks of AIGX, the evolution from AIGC to AIGX, as well as practical AIGX applications. Then, we present a unified mobile-edge AIGX framework, which employs edge devices to provide PFM-empowered AIGX services and optimizes such services via prompt engineering. More importantly, we demonstrate that suboptimal prompts lead to poor generation quality, which adversely affects user satisfaction, edge network performance, and resource utilization. Accordingly, we conduct a case study, showcasing how to train an effective prompt optimizer using ChatGPT and investigating how much improvement is possible with prompt engineering in terms of user experience, quality of generation, and network performance. IEEE
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179012722&origin=inward,Conference Paper,SCOPUS_ID:85179012722,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),combine dgbl with ai system: a technical guidance to reduce teacher' s burden in digital game-based learning,"
AbstractView references

Game-based learning has been regarded as a increasing popular method in current teaching process, however, it really burdens teacher as it requires teachers to invest abundant energy and time to design. Aims at reducing teaching burden, this research has proposed a guidance system design based on large language model (LLM) and blockchain technology. In this design, system framework has been divided into 3 layers: user layer, application layer and technical layer. Initially, teachers input their instructional plans, while students signing up their learner profiles. This information is securely recorded on the blockchain for data integrity. The results stemming from data prediction and feature engineering are then incorporated into the LLM, facilitating the visualization of strategies tailored to address specific learning challenges. As the process advances, the information undergoes automated scrutiny to evaluate the learning conditions, ultimately selecting an appropriate DGBL cases with a proven track record in similar scenarios. This aids teachers in crafting personalized learning blueprints, informed by the insights gleaned from the feature engineering analysis and its impact on students' learning experiences. The concluding phase involves tracking and assessment, wherein an automated evaluation of student performance is conducted based on study data and LLM-generated questionnaires. Teachers subsequently review the results and recommendations to enhance the quality of their instructional methodologies, and the learner portrait will also be renewed according to received data. This guidance system still has some disadvantages, such as lacking sequential consistency in the responses generated by the model. In summary, a future direction for this research is to develop specific LLM systems for specific school segments and instructional needs to help teachers implement DGBL. © 2023 Dechema e.V.. All rights reserved.
"
10.1109/ASE56229.2023.00019,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179012657&origin=inward,Conference Paper,SCOPUS_ID:85179012657,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),smart prompt advisor: multi-objective prompt framework for consistency and best practices,"
AbstractView references

Recent breakthroughs in Large Language Models (LLM), comprised of billions of parameters, have achieved the ability to unveil exceptional insight into a wide range of Natural Language Processing (NLP) tasks. The onus of the performance of these models lies in the sophistication and completeness of the input prompt. Minimizing the enhancement cycles of prompt with improvised keywords becomes critically important as it directly affects the time to market and cost of the developing solution. However, this process inevitably has a trade-off between the learning curve/proficiency of the user and completeness of the prompt, as generating such a solutions is an incremental process. In this paper, we have designed a novel solution and implemented it in the form of a plugin for Visual Studio Code IDE, which can optimize this trade-off, by learning the underlying prompt intent to enhance with keywords. This will tend to align with developers' collection of semantics while developing a secure code, ensuring parameter and local variable names, return expressions, simple pre and post-conditions. and basic control and data flow are met. © 2023 IEEE.
"
10.1109/ASE56229.2023.00174,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179012392&origin=inward,Conference Paper,SCOPUS_ID:85179012392,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),software engineering using autonomous agents: are we there yet?,"
AbstractView references

Autonomous agents equipped with Large Language Models (LLMs) are rapidly gaining prominence as a revolutionary technology within the realm of Software Engineering. These intelligent and autonomous systems demonstrate the capacity to perform tasks and make independent decisions, leveraging their intrinsic reasoning and decision-making abilities. This paper delves into the current state of autonomous agents, their capabilities, challenges, and opportunities in Software Engineering practices. By employing different prompts (with or without context), we conclude the advantages of contextrich prompts for autonomous agents. Prompts with context enhance user requirement understanding, avoiding irrelevant details that could hinder task comprehension and degrade model performance, particularly when dealing with complex frameworks such as Spring Boot, Django, Flask, etc. This exploration is conducted using Auto-GPT (v0.3.0), an open-source application powered by GPT-3.5 and GPT-4 which intelligently connects the 'thoughts' of Large Language Models (LLMs) to independently accomplish the assigned goals or tasks. © 2023 IEEE.
"
10.1109/ASE56229.2023.00206,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178999127&origin=inward,Conference Paper,SCOPUS_ID:85178999127,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),log parsing: how far can chatgpt go?,"
AbstractView references

Software logs play an essential role in ensuring the reliability and maintainability of large-scale software systems, as they are often the sole source of runtime information. Log parsing, which converts raw log messages into structured data, is an important initial step towards downstream log analytics. In recent studies, ChatGPT, the current cutting-edge large language model (LLM), has been widely applied to a wide range of software engineering tasks. However, its performance in automated log parsing remains unclear. In this paper, we evaluate ChatGPT's ability to undertake log parsing by addressing two research questions. (1) Can ChatGPT effectively parse logs? (2) How does ChatGPT perform with different prompting methods? Our results show that ChatGPT can achieve promising results for log parsing with appropriate prompts, especially with few-shot prompting. Based on our findings, we outline several challenges and opportunities for ChatGPT-based log parsing. © 2023 IEEE.
"
10.1109/ASE56229.2023.00125,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178997973&origin=inward,Conference Paper,SCOPUS_ID:85178997973,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an empirical study of parameter-efficient fine-tuning methods for pre-trained code models,"
AbstractView references

Pre-trained code models (e.g. CodeBERT and CodeT5) have demonstrated their code intelligence in various software engineering tasks, such as code summarization. And full fine-tuning has become the typical approach to adapting these models to downstream tasks. However, full fine-tuning these large models can be computationally expensive and memory-intensive, particularly when training for multiple tasks. To alleviate this issue, several parameter-efficient fine-tuning methods (e.g. Adapter and LoRA) have been proposed to only train a small number of additional parameters, while keeping the original pre-trained parameters frozen. Although these methods claim superiority over the prior techniques, they seldom make a comprehensive and fair comparison on multiple software engineering tasks. Moreover, besides their potential in reducing fine-tuning costs and maintaining approximate performance, the effectiveness of these methods in low-resource, cross-language, and cross-project scenarios is inadequately studied. To this end, we first conduct experiments by fine-tuning state-of-the-art code models with these methods on both code understanding tasks and code generation tasks. The results show that, by tuning only 0.5% additional parameters, these methods may achieve comparable or higher performance than full fine-tuning in code understanding tasks, but they may exhibit slightly weaker performance in code generation tasks. We also investigate the impact of these methods with varying numbers of training samples and find that, a considerable number of samples (e.g. 1000 for clone detection) may be required for them to approximate the performance of full fine-tuning. Our experimental results in cross-language and cross-project scenarios demonstrate that by freezing most pre-trained parameters and tuning only 0.5% additional parameters, these methods achieve consistent improvements in models' transfer learning ability in comparison to full fine-tuning. Our code and data are available at https://github.com/anonymous-ase23/ CodeModelParameterEfficientFinetuning. © 2023 IEEE.
"
10.1109/ASE56229.2023.00096,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178996993&origin=inward,Conference Paper,SCOPUS_ID:85178996993,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a closer look at different difficulty levels code generation abilities of chatgpt,"
AbstractView references

Code generation aims to generate source code implementing human requirements illustrated with natural language specifications. With the rapid development of intelligent software engineering, automated code generation has become a hot research topic in both artificial intelligence and software engineering, and researchers have made significant achievements on code generation. More recently, large language models (LLMs) have demonstrated outstanding performance on code generation tasks, such as ChatGPT released by OpenAI presents the fantastic potential on automated code generation. However, the existing studies are limited to exploring LLMs' ability for generating code snippets to solve simple programming problems, the task of competition-level code generation has never been investigated. The specifications of the programming competition are always complicated and require the specific input/output format as well as the high-level algorithmic reasoning ability. In this study, we conduct the first large empirical study to investigate the zero-shot learning ability of ChatGPT for solving competition programming problems. Specifically, we warm up the design of prompts by using the Human-Eval dataset. Then, we apply the well-designed prompt to the competition-level code generation dataset, namely APPS, to further explore the effectiveness of using ChatGPT for solving competition problems. We collect ChatGPT's outputs on 5,000 code competition problems, the evaluation results show that it can successfully pass 25.4% test cases. By further feeding extra information (e.g, test failed information) to ChatGPT, we observe that ChatGPT has the potential to fix partial pass into a fully pass program. Moreover, we investigate the solutions generated by LLMs and the existing solutions, we find that it prefers to directly copy the code instead of re-write when facing more difficult problems. Finally, we evaluate the code quality generated by ChatGPT in terms of 'code cleanness', we observe that the generated codes are with small functions and file sizes, which are in line with the standard of clean code. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178996908&origin=inward,Conference Paper,SCOPUS_ID:85178996908,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),designing educational escape rooms with generative ai: a framework and chatgpt prompt engineering guide,"
AbstractView references

Generative Artificial Intelligence (GenAI) holds the transformative potential to reshape education, particularly in the domain of content creation. One promising application lies in the development of educational escape rooms (EERs) which are increasingly adopted to foster active, experiential learning, critical thinking, and collaboration. Nevertheless, crafting effective EERs tailored to specific learning contexts often poses a daunting and time-consuming challenge. This paper explores the dynamic synergy between Room2Educ8, a framework rooted in Design Thinking principles, and the publicly accessible AI tool ChatGPT. Room2Educ8 provides a structured methodology encompassing vital steps like empathising with learners, defining learning objectives, weaving narratives, devising puzzles, briefing and debriefing participants, prototyping, and evaluating the EER experience. Complementing this framework, the paper presents a collection of sample prompts that illustrate ChatGPT's pivotal role within the EER creation process. By offering innovative ideas, suggestions, and content, these prompts not only expedite ideation and concept development but also simplify prototype creation for testing and refinement. This streamlining process reduces cognitive load, freeing educators to focus on higher-level considerations. The primary contribution of this paper lies in its harmonious fusion of ChatGPT with a structured design framework, effectively demystifying the EER creation process. With its practical guidance, including a prompt engineering guide, it extends the accessibility of EER design to a wide spectrum of educators, encompassing those with limited prior exposure to the intricacies of escape room formats. Beyond its immediate benefits, this paper serves as a gateway to future research prospects within the domain of AI-powered educational experiences, marking a step towards realising the potential of AI in the field of game-based learning. © 2023 Dechema e.V.. All rights reserved.
"
10.1109/ASE56229.2023.00010,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178994775&origin=inward,Conference Paper,SCOPUS_ID:85178994775,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),comex: a tool for generating customized source code representations,"
AbstractView references

Learning effective representations of source code is critical for any Machine Learning for Software Engineering (ML4SE) system. Inspired by natural language processing, large language models (LLMs) like Codex and CodeGen treat code as generic sequences of text and are trained on huge corpora of code data, achieving state of the art performance on several software engineering (SE) tasks. However, valid source code, unlike natural language, follows a strict structure and pattern governed by the underlying grammar of the programming language. Current LLMs do not exploit this property of the source code as they treat code like a sequence of tokens and overlook key structural and semantic properties of code that can be extracted from code-views like the Control Flow Graph (CFG), Data Flow Graph (DFG), Abstract Syntax Tree (AST), etc. Unfortunately, the process of generating and integrating code-views for every programming language is cumbersome and time consuming. To overcome this barrier, we propose our tool COMEX - a framework that allows researchers and developers to create and combine multiple code-views which can be used by machine learning (ML) models for various SE tasks. Some salient features of our tool are: (i) it works directly on source code (which need not be compilable), (ii) it currently supports Java and C#, (iii) it can analyze both method-level snippets and program-level snippets by using both intra-procedural and inter-procedural analysis, and (iv) it is easily extendable to other languages as it is built on tree-sitter - a widely used incremental parser that supports over 40 languages. We believe this easy-to-use code-view generation and customization tool will give impetus to research in source code representation learning methods and ML4SE. The source code and demonstration of our tool can be found at https://github.com/IBM/tree-sitter-codeviews and https://youtu.be/GER6U87FVbU, respectively. © 2023 IEEE.
"
10.54808/WMSCI2023.01.465,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178656192&origin=inward,Conference Paper,SCOPUS_ID:85178656192,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"cybernetic relationships between technological innovations, ethics, and the law","
AbstractView references

We aim to identify a perspective for new Artificial Intelligence (AI) products, applications, and their impact within the context of Technological Innovations. Consequently, we will briefly examine the impact of two key historical innovations: 1) Stone Tools, which marked a technological breakthrough 400,000 years ago and heralded the Stone Age, and 2) the invention of the wheel around 3,500 BCE. The impact of the wheel's invention still resonates today, as countless innovations have been built upon this foundational creation. While some view the wheel as a discovery arising from the observation of natural phenomena, it is clear that the wheel with an axle is indeed a significant human technological innovation. Within the framework of these perspectives and their shared elements, we will endeavor to capture the societal implications of new Technological Innovations centered around Artificial Intelligence. These innovations include Generative Artificial Intelligence (AGI), exemplified by systems like ChatGPT, and Artificial General Intelligence (GAI), as seen in entities like Perplexity AI. We will conclude by suggesting, via analogical thinking, that these novel AI technological advancements follow a similar notional pattern-giving rise to subsequent technological breakthroughs that, in turn, shape cultures. These cultural changes then lead to shifts in moral values, necessitating adaptations in ethics and even the formulation of new laws. This proactive approach aims to prevent emerging forms of injustice and to ensure justice prevails within these evolving cultural and societal contexts"". © 2023 Proceedings of World Multi-Conference on Systemics, Cybernetics and Informatics, WMSCI. All rights reserved.
"
10.18420/delfi2023-14,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178624808&origin=inward,Conference Paper,SCOPUS_ID:85178624808,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),auto-generated language learning online courses using generative ai models like chatgpt,"
AbstractView references

Generating online courses is always a trade-off between possibilities, technical limitations, and quality. State-of-the-art generative models can assist teachers in the creation process. However, generating learning materials is highly complex. Hence, teachers mainly create them manually. In this paper, learning content for a concrete micro-learning template is generated focusing on the field of language teaching. It intends that learners can find correct responses by logical thinking. Teachers provide a topic as input. Then, the approach asks for the required information using GPT3.5 with instructional prompts and combines responses to form a language learning unit. The quality of the resulting learning content, focusing on correctness, and appropriateness, is evaluated and discussed to examine the practicability of the tool, and alternatives are given. © 2023 Gesellschaft fur Informatik (GI). All rights reserved.
"
10.1109/MLCAD58807.2023.10299874,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178622677&origin=inward,Conference Paper,SCOPUS_ID:85178622677,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chip-chat: challenges and opportunities in conversational hardware design,"
AbstractView references

Modern hardware design starts with specifications provided in natural language. These are then translated by hardware engineers into appropriate Hardware Description Languages (HDLs) such as Verilog before synthesizing circuit elements. Automating this translation could reduce sources of human error from the engineering process. But, it is only recently that artificial intelligence (AI) has demonstrated capabilities for machine-based end-to-end design translations. Commercially-available instruction-tuned Large Language Models (LLMs) such as OpenAI's ChatGPT and Google's Bard claim to be able to produce code in a variety of programming languages; but studies examining them for hardware are still lacking. In this work, we thus explore the challenges faced and opportunities presented when leveraging these recent advances in LLMs for hardware design. Given that these 'conversational' LLMs perform best when used interactively, we perform a case study where a hardware engineer co-architects a novel 8-bit accumulator-based microprocessor architecture with the LLM according to real-world hardware constraints. We then sent the processor to tapeout in a Skywater 130nm shuttle, meaning that this 'Chip-Chat' resulted in what we believe to be the world's first wholly-AI-written HDL for tapeout. © 2023 IEEE.
"
10.1007/978-981-99-8385-8_7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178601326&origin=inward,Conference Paper,SCOPUS_ID:85178601326,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automatic and authentic eassessment of online database design theory assignments,"
AbstractView references

One of the main reasons MCQ assessments are popular is that tests using text understanding is difficult and often erroneous. The emergence of large language models such as ChatGPT is not mature enough to help grading engineering assignments yet. Since MCQ tests are not well suited for summative assessment, scaling up eLearning for large number of students is difficult using non-MCQ tests. In this paper, we introduce a new eAssessment tool for database design courses that uses graphical conversations to understand learner’s mental model of cognitive state. We show that the model is capable of substituting NLP for authentic assessment for eLearning. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178596343&origin=inward,Conference Paper,SCOPUS_ID:85178596343,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),benchmarking the abilities of large language models for rdf knowledge graph creation and comprehension: how well do llms speak turtle?,"
AbstractView references

Large Language Models (LLMs) are advancing at a rapid pace, with significant improvements at natural language processing and coding tasks. Yet, their ability to work with formal languages representing data, specifically within the realm of knowledge graph engineering, remains under-investigated. To evaluate the proficiency of various LLMs, we created a set of five tasks that probe their ability to parse, understand, analyze, and create knowledge graphs serialized in Turtle syntax. These tasks, each embodying distinct degrees of complexity and being able to scale with the size of the problem, have been integrated into our automated evaluation system, the LLM-KG-Bench. The evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4, Claude 1.3, and Claude 2.0, as well as two freely accessible offline models, GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth understanding of the strengths and shortcomings of LLMs in relation to their application within RDF knowledge graph engineering workflows utilizing Turtle representation. While our findings show that the latest commercial models outperform their forerunners in terms of proficiency with the Turtle language, they also reveal an apparent weakness. These models fall short when it comes to adhering strictly to the output formatting constraints, a crucial requirement in this context. © 2023 Copyright for this paper by its authors.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178584892&origin=inward,Conference Paper,SCOPUS_ID:85178584892,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),enhancing scholarly understanding: a comparison of knowledge injection strategies in large language models,"
AbstractView references

The use of transformer-based models like BERT for natural language processing has achieved remarkable performance across multiple domains. However, these models face challenges when dealing with very specialized domains, such as scientific literature. In this paper, we conduct a comprehensive analysis of knowledge injection strategies for transformers in the scientific domain, evaluating four distinct methods for injecting external knowledge into transformers. We assess these strategies in a single-label multi-class classification task involving scientific papers. For this, we develop a public benchmark based on 12k scientific papers from the AIDA knowledge graph, categorized into three fields. We utilize the Computer Science Ontology as our external knowledge source. Our findings indicate that most proposed knowledge injection techniques outperform the BERT baseline. © 2022 Copyright for this paper by its authors.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178565843&origin=inward,Conference Paper,SCOPUS_ID:85178565843,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),leveraging knowledge graphs with large language models for classification tasks in the tourism domain,"
AbstractView references

Online platforms, serving as the primary conduit for travelers to seek, compare, and secure travel accommodations, require a profound understanding of user dynamics to craft competitive and enticing offerings. Concurrently, recent advancements in Natural Language Processing, particularly large language models, have made substantial strides in capturing the complexity of human language. Simultaneously, knowledge graphs have become a formidable instrument for structuring and categorizing information. This paper introduces a cutting-edge deep learning methodology integrating large language models with domain-specific knowledge graphs to classify tourism offers. It aims at aiding hospitality operators in understanding their accommodation offerings’ market positioning, taking into account the visit propensity and user review ratings, with the goal of optimizing the offers themselves and enhancing their appeal. Comparative analysis against alternative methods on two datasets of London accommodation offers attests to our approach’s effectiveness, demonstrating superior results. © 2022 Copyright for this paper by its authors.
"
10.1109/AIBThings58340.2023.10292475,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178520591&origin=inward,Conference Paper,SCOPUS_ID:85178520591,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompt engineering in medical image segmentation: an overview of the paradigm shift,"
AbstractView references

Foundation AI models have emerged as powerful pre-trained models on a large scale, capable of seamlessly handling diverse tasks across multiple domains with minimal or no fine-tuning. These models, exemplified by the impressive achievements of GPT-3 and BERT in natural language processing (NLP), as well as CLIP and DALL-E in computer vision, have garnered considerable attention for their exceptional performance. A noteworthy addition to the realm of image segmentation is the Segment Anything Model (SAM), a foundation AI model that revolutionizes image segmentation. With a single click or a natural language prompt, SAM exhibits the remarkable ability to segment any object within an image, marking a significant paradigm shift in medical image segmentation. Unlike conventional approaches that rely on labeled data and domain-specific knowledge, SAM breaks free from these constraints. Deep convolutional neural network (DCNN)-based, SAM comprises an image encoder, a prompt encoder, and a mask decoder, showcasing its efficient and flexible architecture. Medical image segmentation, in particular, benefits from SAM's exceptional speed and high-quality segmentation. In this paper, we delve into the effectiveness of SAM for medical image segmentation shedding light on its capabilities. Moreover, our investigation explores the strengths and limitations of prompt engineering in medical computer vision applications, not only encompassing SAM but also other foundation AI models. Through this exploration, we unravel their immense potential in catalyzing a paradigm shift in the field of medical imaging. © 2023 IEEE.
"
10.1115/detc2023-116838,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178515903&origin=inward,Conference Paper,SCOPUS_ID:85178515903,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),conceptual design generation using large language models,"
AbstractView references

Concept generation is a creative step in the conceptual design phase, where designers often turn to brainstorming, mindmapping, or crowdsourcing design ideas to complement their own knowledge of the domain. Recent advances in natural language processing (NLP) and machine learning (ML) have led to the rise of Large Language Models (LLMs) capable of generating seemingly creative outputs from textual prompts. The success of these models has led to their integration and application across a variety of domains, including art, entertainment, and other creative work. In this paper, we leverage LLMs to generate solutions for a set of 12 design problems and compare them to a baseline of crowdsourced solutions. We evaluate the differences between generated and crowdsourced design solutions through multiple perspectives, including human expert evaluations and computational metrics. Expert evaluations indicate that the LLM-generated solutions have higher average feasibility and usefulness while the crowdsourced solutions have more novelty. We experiment with prompt engineering and find that leveraging few-shot learning can lead to the generation of solutions that are more similar to the crowdsourced solutions. These findings provide insight into the quality of design solutions generated with LLMs and begins to evaluate prompt engineering techniques that could be leveraged by practitioners to generate higher-quality design solutions synergistically with LLMs. Copyright © 2023 by ASME.
"
10.1115/DETC2023114783,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178514848&origin=inward,Conference Paper,SCOPUS_ID:85178514848,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cyberriskdelphi: towards objective cyber risk assessment for complex systems,"
AbstractView references

Risk assessment is an essential step for architecting the resilience (safety/security) of a mission critical software-intensive system as well as a regular maintenance procedures. It closely relates to estimating the (cyber) insurance needs of the system. Managing of cyber risk involves gathering threat intelligence, prioritizing the current threats against the system of interest, and planning mitigation strategies. While reliability engineering can rely on a relatively stable set of failure modes and statistical data related to their probabilities of occurrence, security deals with a dynamic threat environment. This reality has dictated the use of qualitative methods (like STRIDE and DREAD), relying on the experience and the specific background of the person performing the study. This subjectivity leads to criticism, since results calculated by different experts for the same system can vary significantly. This challenge has been addressed in the past with a method called DELPHI aiming to reduce subjectivity using a group of experts. The scientific contribution of this paper is the development of the CyberRiskDELPHI, a modified version of original DELPHI method for the identification and prioritization of cyber risks. It is demonstrated over a case study of a 5G tactical bubble covering the communication needs of a critical operation. An early evaluation of the use of a large language model (ChatGPT) in risk identification and prioritization for this case study is also included as a complementary side-activity giving an indication of future developments in the risk assessment domain. Copyright © 2023 by The United States Government.
"
10.1109/ISSREW60843.2023.00040,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178503104&origin=inward,Conference Paper,SCOPUS_ID:85178503104,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automatic bug fixing via deliberate problem solving with large language models,"
AbstractView references

Developers dedicate a significant share of their activities to finding and fixing defects in their code. Automated program repair (APR) attempts to reduce this effort by a set of techniques for automatically fixing errors or vulnerabilities in software systems. Recent Large Language Models (LLMs) such as GPT-4 offer an effective alternative to existing APR methods, featuring out-of-the-box bug fixing performance comparable to even sophisticated deep learning approaches such as CoCoNut. In this work we propose a further extension to LLM-based program repair techniques by leveraging a recently introduced interactive prompting technique called Tree of Thoughts (ToT). Specifically, we ask a LLM to propose multiple hypotheses about the location of a bug, and based on the aggregated response we prompt for bug fixing suggestions. A preliminary evaluation shows that our approach is able to fix multiple complex bugs previously unsolved by GPT-4 even with prompt engineering. This result motivates further exploration of hybrid approaches which combine LLMs with suitable meta-strategies. © 2023 IEEE.
"
10.26615/978-954-452-092-2_069,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178477905&origin=inward,Conference Paper,SCOPUS_ID:85178477905,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a practical survey on zero-shot prompt design for in-context learning,"
AbstractView references

The remarkable advancements in large language models (LLMs) have brought about significant improvements in Natural Language Processing(NLP) tasks. This paper presents a comprehensive review of in-context learning techniques, focusing on different types of prompts, including discrete, continuous, fewshot, and zero-shot, and their impact on LLM performance. We explore various approaches to prompt design, such as manual design, optimization algorithms, and evaluation methods, to optimize LLM performance across diverse tasks. Our review covers key research studies in prompt engineering, discussing their methodologies and contributions to the field. We also delve into the challenges faced in evaluating prompt performance, given the absence of a single ""best"" prompt and the importance of considering multiple metrics. In conclusion, the paper highlights the critical role of prompt design in harnessing the full potential of LLMs and provides insights into the combination of manual design, optimization techniques, and rigorous evaluation for more effective and efficient use of LLMs in various NLP tasks. © 2023 Incoma Ltd. All rights reserved.
"
10.1073/pnas.2312848120,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178477774&origin=inward,Article,SCOPUS_ID:85178477774,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),enhancing luciferase activity and stability through generative modeling of natural enzyme sequences,"
AbstractView references

The availability of natural protein sequences synergized with generative AI provides new paradigms to engineer enzymes. Although active enzyme variants with numerous mutations have been designed using generative models, their performance often falls short of their wild type counterparts. Additionally, in practical applications, choosing fewer mutations that can rival the efficacy of extensive sequence alterations is usually more advantageous. Pinpointing beneficial single mutations continues to be a formidable task. In this study, using the generative maximum entropy model to analyze Renilla luciferase (RLuc) homologs, and in conjunction with biochemistry experiments, we demonstrated that natural evolutionary information could be used to predictively improve enzyme activity and stability by engineering the active center and protein scaffold, respectively. The success rate to improve either luciferase activity or stability of designed single mutants is ~50%. This finding highlights nature's ingenious approach to evolving proficient enzymes, wherein diverse evolutionary pressures are preferentially applied to distinct regions of the enzyme, ultimately culminating in an overall high performance. We also reveal an evolutionary preference in RLuc toward emitting blue light that holds advantages in terms of water penetration compared to other light spectra. Taken together, our approach facilitates navigation through enzyme sequence space and offers effective strategies for computer-aided rational enzyme engineering. Copyright © 2023 the Author(s)
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178387478&origin=inward,Conference Paper,SCOPUS_ID:85178387478,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),evaluating the performance of chatgpt in the automation of maintenance recommendations for prognostics and health management,"
AbstractView references

Until now, automation of maintenance recommendations for Prognostics and Health Management (PHM) has been a domain-specific technical language processing (TLP) task applied to historical case data. ChatGPT, Bard, GPT-4 and Sydney are a few examples of generative large language models (LLMs) that have received significant media attention for their proficiency in natural language tasks across a variety of domains. Preliminary exploration of ChatGPT as a tool for generating maintenance recommendations has shown promise in its ability to generate and explain engineering concepts and procedures, but the precise scope of its capabilities and limitations remains uncertain. Currently we know of no performance criteria related to formally measuring how well ChatGPT performs as a tool for industrial use cases. In this paper, we propose a methodology for the evaluation of the performance of LLMs such as ChatGPT for the task of automation of maintenance recommendations. Our methodology identifies various performance criteria relevant for PHM such as engineering criteria, risk elements, human factors, cost considerations and corrections. We examine how well ChatGPT performs when tasked with generating recommendations from PHM model alerts and report our findings. We discuss the various strengths and limitations to consider in the adoption of LLM's as a computational support tool for prescriptive PHM as well as the different risks and business case considerations. © 2023 Prognostics and Health Management Society. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178378565&origin=inward,Conference Paper,SCOPUS_ID:85178378565,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),navigating the fermi multiverse: assessing llms for complex multi-hop queries,"
AbstractView references

Recently, large language models (LLMs) have gained significant attention in the field of Natural Language Processing (NLP) and have shown promise across various tasks, even when given only a few examples to learn from. However, their ability to understand and reason with natural language remains uncertain. While there have been attempts to evaluate these models using reasoning tests, these evaluations have mostly focused on models' final answers, often overlooking the step-by-step reasoning processes behind their performance. Additionally, these analyses have typically concentrated on just one or a few aspects of reasoning, especially for tasks that do not require much complex thinking to find the answer. This limits our understanding of LLMs' potential and limitations when it comes to more complex and realistic questions. To address this issue, we conduct a comprehensive analysis of LLMs using the existing Fermi reasoning challenge, a task that combines different aspects of reasoning into a single question-answering format, requiring deeper levels of reasoning. In this paper, we examine various advanced LLMs in this reasoning challenge and explore how their performance is affected by their size (i.e., the number of parameters). We also investigate how these models behave with different levels of supervision, ranging from having all the information to no evidence at all. Furthermore, we compare the two primary methods of teaching these LLMs, fine-tuning, and few-shot learning, using the Chain-of-Thought approach. We provide a detailed case study highlighting the most common limitations of these models. While our results imply that these models may have a long journey ahead to reach human-level reasoning, our work can be considered a robust baseline for the community to strive toward achieving this ambitious goal. Our code is available on GitHub https://github.com/MostafaRahgouy/LLMs_for_FPs for the community. © 2023 Copyright for this paper by its authors.
"
10.1109/ICACITE57410.2023.10182984,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178279357&origin=inward,Conference Paper,SCOPUS_ID:85178279357,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),implementation of iot on english language classroom management,"
AbstractView references

The new method of intelligent multidistance This study uses fictitious Internet of Things scenes to perform a thorough investigation into English instruction. The ""four-driven""design principle is the result of research into the viability of using virtual simulations in the classroom as well as the state of IoT skills instruction. Traditional IoT instruction uses virtual simulation technology and professional education in IoT application technology. Additionally, a case study of skill-based instruction using virtual simulation technology is presented. This case study includes virtual experience, demonstration, interaction, and evaluation. An asynchronous message queue is used to construct a framework for distributed collaborative computing in this paper. The task can be divided so that multiple nodes can work on it simultaneously. The DeepCluster module is able to efficiently cluster time series and comprehend the typical variation of time series patterns thanks to deep representation learning. Deep representation learning enables the DeepCluster module to efficiently cluster time series and comprehend the typical variation of time series patterns. The task offloading module of the framework uses a value-constrained multi 01 backpacking model-based task offloading decision method and an optimal offloading solution to minimise task processing latency. System tests demonstrate that the proposed offloading decision algorithm and distributed computing framework can significantly reduce processing latency for large t asks. © 2023 IEEE.
"
10.1007/s00180-023-01437-2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178215488&origin=inward,Article,SCOPUS_ID:85178215488,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),pair programming with chatgpt for sampling and estimation of copulas,"
AbstractView references

Without writing a single line of code by a human, an example Monte Carlo simulation-based application for stochastic dependence modeling with copulas is developed through pair programming involving a human partner and a large language model (LLM) fine-tuned for conversations. This process encompasses interacting with ChatGPT using both natural language and mathematical formalism. Under the careful supervision of a human expert, this interaction facilitated the creation of functioning code in MATLAB, Python, and R. The code performs a variety of tasks including sampling from a given copula model, evaluating the model’s density, conducting maximum likelihood estimation, optimizing for parallel computing on CPUs and GPUs, and visualizing the computed results. In contrast to other emerging studies that assess the accuracy of LLMs like ChatGPT on tasks from a selected area, this work rather investigates ways how to achieve a successful solution of a standard statistical task in a collaboration of a human expert and artificial intelligence (AI). Particularly, through careful prompt engineering, we separate successful solutions generated by ChatGPT from unsuccessful ones, resulting in a comprehensive list of related pros and cons. It is demonstrated that if the typical pitfalls are avoided, we can substantially benefit from collaborating with an AI partner. For example, we show that if ChatGPT is not able to provide a correct solution due to a lack of or incorrect knowledge, the human-expert can feed it with the correct knowledge, e.g., in the form of mathematical theorems and formulas, and make it to apply the gained knowledge in order to provide a correct solution. Such ability presents an attractive opportunity to achieve a programmed solution even for users with rather limited knowledge of programming techniques. © 2023, The Author(s).
"
10.1080/0144929X.2023.2286532,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178205595&origin=inward,Article,SCOPUS_ID:85178205595,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a taxonomy of prompt modifiers for text-to-image generation,"
AbstractView references

Text-guided synthesis of images has become enormously popular and online communities dedicated to text-to-image generation and art generated with Artificial Intelligence (AI) have emerged. While deep generative models can synthesise high-quality images and artworks from simple descriptive text prompts, practitioners of text-to-image generation typically seek to control the generative model’s output by adding short key phrases (‘modifiers’) to the prompt. This paper identifies six types of prompt modifiers used by practitioners in the online text-to-image community based on a 3-month ethnographic study. The novel taxonomy of prompt modifiers provides researchers a conceptual starting point for investigating the practice of text-to-image generation, but may also help practitioners of AI generated art improve their images. We further outline how prompt modifiers are applied in the practice of ‘prompt engineering.’ and discuss research opportunities of this novel creative practice in the field of Human–Computer Interaction (HCI). The paper concludes with a discussion of broader implications of prompt engineering from the perspective of Human-AI Interaction (HAI) in future applications beyond the use case of text-to-image generation and AI generated art. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.
"
10.1109/UPEC57427.2023.10294355,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178151854&origin=inward,Conference Paper,SCOPUS_ID:85178151854,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),pysddp: an open-source python tool applied to the operation planning problem in the age of energy transition,"
AbstractView references

There is a strong movement towards energy transition in the current context of large-scale power systems world-wide. In this dynamic scenario, the existence of computational tools capable of providing an environment that work as an interactive laboratory becomes essential for engineering education. PySDDP is a free Python computational tool applied to the operation planning of large-scale power systems and is available through the PyPI and GitHub repositories. PySDDP's name comes from Sthocastic Dual Dynamic Programming, one of the most popular algorithms applied to solve long-term hydro-thermal operating planning. PySDDP has mechanisms for manipulating all data used to solve the operation planning problem of Brazilian official models. However, it can friendly receive contributions from third parties to include data from energy systems of other countries. Moreover, its implementation in Python makes the tool easy to understand and allows easy extension with third-party libraries. © 2023 IEEE.
"
10.1109/EMR.2023.3333794,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178083059&origin=inward,Article,SCOPUS_ID:85178083059,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),engineers' perspectives on the use of generative artificial intelligence tools in the workplace,"
AbstractView references

The integration of Artificial Intelligence (AI) into the workplace requires commitment not only from the leadership of company directors but, equally important, from the engineers responsible for its implementation. This paper presents a survey on perceptions concerning the utilization of AI tools in engineering environments. It was focused on engineers and students in the areas of electricity, electronics and computing. The questionnaire covered demographic information, Artificial Intelligence knowledge level, preferred tools, primary applications, perceived impact, and attitudes toward labor substitution. With the endorsement of the IEEE Uruguay Section and IEEE Region 9, the survey was distributed via email to a diverse group of potential participants, in several countries, including all IEEE members in Region 9. There were 375 replies to the survey, from 20 different countries in the Americas. IEEE
"
10.1109/iMETA59369.2023.10294830,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178067294&origin=inward,Conference Paper,SCOPUS_ID:85178067294,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai-empowered metaverse learning simulation technology application,"
AbstractView references

This paper proposes an immersive and adaptive virtual learning system incorporating artificial intelligence and multi-agent technologies to provide personalized and engaging education tailored to individual students. A prototype system was developed and tested for an English language learning task, demonstrating over 30% improvement versus conventional teaching. However, challenges remain for large-scale classroom implementation, including development costs, technology readiness differences, risks of over-reliance on technology lacking human interactions, and formulating new pedagogical models. Further research is required to address these open issues before realizing the full potential of AI-enabled immersive learning environments for transforming education. The paper provides a conceptual framework and system design for intelligent customizable virtual learning tailored to students' knowledge states, skill levels, interests, and learning styles. Preliminary experimentation validated gains in outcomes and engagement versus conventional classrooms. While substantial hurdles exist for translation to mainstream adoption, prudent integration of immersive technologies could enable more stimulating and effective education worldwide. With continued systemic research and adoption, AI-driven VR heralds a new era in technology-enhanced yet human-centric pedagogy to prepare learners for the modern digital world. © 2023 IEEE.
"
10.1109/ISSRE59848.2023.00026,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178023512&origin=inward,Conference Paper,SCOPUS_ID:85178023512,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),llama-reviewer: advancing code review automation with large language models through parameter-efficient fine-tuning,"
AbstractView references

The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the smallest LLaMA base model consisting of 6.7B parameters and a limited number of tuning epochs, LLaMA-Reviewer equals the performance of existing code-review-focused models.The ablation experiments provide insights into the influence of various fine-tuning process components, including input representation, instruction tuning, and different PEFT methods. To foster continuous progress in this field, the code and all PEFT-weight plugins have been made open-source. © 2023 IEEE.
"
10.1109/ICPSAsia58343.2023.10294982,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177880579&origin=inward,Conference Paper,SCOPUS_ID:85177880579,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cvar-constrained robust unit commitment for power system with concentrating solar power,"
AbstractView references

A conditional value-at-risk (CVaR) constrained robust unit commitment model with concentrating solar power (CSP) is proposed to address the uncertainties arising from large-scale renewable energy integration. Firstly, the principle of CSP is analyzed, and the mathematical model of CSP in steady-state is established. Secondly, CVaR is introduced to measure the risk of loss caused by wind power uncertainty. Then, a CVaR-constrained robust unit commitment model for power system with CSP is formulated, and a multidimensional uncertainty set is used to tradeoff between the robustness and the conservatism of the model. The model is transformed into a mixed integer linear programming model by linearization and solved using the column and constraint generation (C&CG) algorithm. Finally, the effectiveness of the proposed model is verified on the IEEE 6-bus system and IEEE 118-bus system. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177812008&origin=inward,Conference Paper,SCOPUS_ID:85177812008,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),vip5: towards multimodal foundation models for recommendation,"
AbstractView references

Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) are three prominent AI applications that have traditionally developed independently, resulting in disparate modeling and engineering methodologies. This has impeded the ability for these fields to directly benefit from each other's advancements. With the recent development of foundation models, large language models have emerged as a potential general-purpose interface for unifying different modalities and problem formulations. In light of this, we propose the development of a multimodal foundation model (MFM) considering visual, textual, and personalization modalities under the P5 recommendation paradigm, thus named VIP5 (Visual P5), to unify various modalities and recommendation tasks. This will enable the processing of multiple modalities in a shared architecture for improved recommendations. To achieve this, we introduce multimodal personalized prompts to accommodate multiple modalities under a shared format. Additionally, we propose a parameter-efficient training method for foundation models, which involves freezing the P5 backbone and fine-tuning lightweight adapters, resulting in improved recommendation performance and increased efficiency in terms of training time and memory usage. Code and data of VIP5 are available at https://github.com/jeykigung/VIP5. © 2023 Association for Computational Linguistics.
"
10.18785/jetde.1602.01,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177582494&origin=inward,Article,SCOPUS_ID:85177582494,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"generative ai, learning and new literacies","
AbstractView references

Launched in November 2022, OpenAI’s ChatGPT garnered over 100 million users within two months, sparking a surge in research and concern over potential risks of extensive AI experiments. The article, originating from a conference presentation by Tsinghua University and NTHU, Taiwan, provides a nuanced overview of Generative AI. It explores the classifications, applications, governance challenges, societal implications, and development trajectory of Generative AI, emphasizing its transformative role in employment and education. The piece highlights ChatGPT’s significant impact and the strategic adaptations required in various sectors, including medical education, engineering, information management, and distance education. Furthermore, it explores the opportunities and challenges associated with incorporating ChatGPT in educational settings, emphasizing its support in facilitating personalized learning, developing 21st-century competencies, fostering self-directed learning, and enhancing information accessibility. It also illustrates the integration of ChatGPT and text-to-image models in high school language courses through the lens of new literacies. The text uniquely integrates three layers of discourse: introductions to Generative AI by experts, scholarly debates on its merits and drawbacks, and practical classroom applications, offering a reflective snapshot of the current and potential states of Generative AI applications while emphasizing the interconnected discussions across various layers of discourse. © 2023, University of Southern MIssissippi. All rights reserved.
"
10.1109/UBMK59864.2023.10286606,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177561934&origin=inward,Conference Paper,SCOPUS_ID:85177561934,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"enhancing paraphrasing in chatbots through prompt engineering: a comparative study on chatgpt, bing, and bard","
AbstractView references

Paraphrase generation, a crucial task in Natural Language Processing (NLP), is pivotal for the effectiveness of AI chatbots. However, generating high-quality paraphrases that are contextually relevant, semantically equivalent, and linguistically diverse remains a challenge. This paper explores the use of prompt engineering to enhance the paraphrasing capabilities of AI chatbots, specifically focusing on ChatGPT, Bing, and Bard. We introduce a new dataset of 5000 sentences generated by ChatGPT across diverse topics and propose two distinct prompts for paraphrase generation: a direct approach and an engineered prompt. The engineered prompt explicitly instructs the chatbot to generate paraphrases that exhibit lexical diversity, phrasal variations, syntactical differences, fluency, language acceptableness, and relevance, while preserving the original meaning. We conduct a comprehensive evaluation of the generated paraphrases using a range of metrics, including BERTScore, STS-B, METEOR for semantic similarity; ROUGE, BLEU, GLEU for diversity; and CoLA, Perplexity for language acceptableness or fluency. Our findings reveal that the use of the engineered prompt results in higher quality paraphrases across all three chatbots, demonstrating the potential of prompt engineering as a tool for improving chatbot communication. © 2023 IEEE.
"
10.1201/9781003370321-16,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177533442&origin=inward,Book Chapter,SCOPUS_ID:85177533442,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"aiomt training, testing, and validation","
AbstractView references

This chapter centers on AIoMT training, testing, and validation. The application scope of artificial intelligence (AI) continues to widen across many domains, such as the medical field, engineering, and others. Artificial intelligence enables the computer to perform tasks that require human intelligence. Such tasks include computer vision, which deals with visual image recognition and natural language processing. Specifically, AI application is gaining more popularity, especially in the healthcare industry, due to their suitability for medical imaging and chatbots. Integrating AI into the existing healthcare system will further optimize its outlook. AI models are built with a machine learning (ML) algorithm using robust datasets. The prediction accuracy of the AI-based model depends on the training, testing, and validation data used. The quality of training data and the training acquired by the model is directly proportional to the prediction accuracy. However, AI technology faces challenges, such as the non-existent of the large dataset required for model training (due to the confidentiality and privacy issues associated with medical data), the missing values in the large dataset used for training, poor testing methodologies, and validation. This chapter discusses AIoMT training, testing, and validation to address these proliferating issues. The work remarked that obtaining clean and quality data will help mitigate the problems with AI applications in healthcare. Further research is needed to achieve efficient and effective model training, testing, and validation. © 2024 selection and editorial matter, Agbotiname Lucky Imoize, Valentina Emilia Balas, Vijender Kumar Solanki, Cheng-Chi Lee, Mohammad S. Obaidat; individual chapters, the contributors.
"
10.1007/978-3-031-47655-6_13,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177465618&origin=inward,Conference Paper,SCOPUS_ID:85177465618,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompt engineering for narrative choice generation,"
AbstractView references

Large language models (LLMs) have recently revolutionized performance on a variety of natural language generation tasks, but have yet to be studied in terms of their potential for generating reasonable character choices as well as subsequent decisions and consequences given a narrative context. We use recent (not yet available for LLM training) film plot excerpts as an example initial narrative context and explore how different prompt formats might affect narrative choice generation by open-source LLMs. The results provide a first step toward understanding effective prompt engineering for future human-AI collaborative development of interactive narratives. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.
"
10.1007/978-3-031-47262-6_1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177460103&origin=inward,Conference Paper,SCOPUS_ID:85177460103,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"stochastic llms do not understand language: towards symbolic, explainable and ontologically based llms","
AbstractView references

In our opinion the exuberance surrounding the relative success of data-driven large language models (LLMs) is slightly misguided and for several reasons (i) LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic nature, whatever ‘knowledge’ these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (iii) LLMs will often fail to make the correct inferences in several linguistic contexts (e.g., nominal compounds, copredication, quantifier scope ambiguities, intensional contexts). Since we believe the relative success of data-driven large language models (LLMs) is not a reflection on the symbolic vs. subsymbolic debate but a reflection on applying the successful strategy of a bottom-up reverse engineering of language at scale, we suggest in this paper applying the effective bottom-up strategy in a symbolic setting resulting in symbolic, explainable, and ontologically grounded language models. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.
"
10.1007/978-3-031-47658-7_16,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177457396&origin=inward,Conference Paper,SCOPUS_ID:85177457396,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the chronicles of chatgpt: generating and evaluating visual novel narratives on climate change through chatgpt,"
AbstractView references

This paper explores the potential of utilizing ChatGPT, a large language model (LLM), for generating and evaluating visual novel (VN) game stories in the context of global warming awareness through a VN game. The study involves generating two stories using ChatGPT, one with given global warming related keywords as an inspiration for ChatGPT along with a specified ending and another without, and evaluating them based on several linguistic criteria: coherence, inspiration, readability, word complexity, and narrative fluency. Results reveal that keywords-inspired story exhibit higher coherence, while the basic one demonstrate greater inspiration. The findings highlight the advantages of each story and emphasize the value of AI-driven narrative generation in creating engaging and informative experiences. Furthermore, the study introduces an innovative approach by employing ChatGPT as an evaluator for the story quality, by combining various prompt engineering techniques showcasing the diverse applications of LLMs in interactive storytelling. This work contributes to the growing field of LLM-based story generation and underscores the potential of AI-driven narratives in fostering awareness and engagement on critical issues like climate change. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.
"
10.17705/1CAIS.05331,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177455712&origin=inward,Article,SCOPUS_ID:85177455712,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),giusberto: italy’s ai-based judicial transformation: a teaching case,"
AbstractView references

In an age when open access to law enforcement files and judicial documents can erode individual privacy and confidentiality, miscreants can abuse this open access to personal information for blackmail, misinformation, and even social engineering. Yet, limiting access to law enforcement and court cases is a freedom-of-information violation. To address this tension, this collaborative action-research-based teaching case exemplifies how Italy’s Corte dei Conti (Court of Auditors) used artificial intelligence in the automated deidentification and anonymization of court documents in Italy’s public sector. This teaching case is aimed at undergraduate and graduate students learning about Artificial Intelligence (AI), Large Language Model (LLM) (e.g., ChatGPT) evolution, development, and operations. The case will help students learn the origin and evolution of AI transformer models and architectures, and discusses the GiusBERTo operation and process, highlighting opportunities and challenges. GiusBERTo, Italy’s custom-AI model, offers an innovative approach that walks a tightrope between anonymizing Italy’s judicial court documents without sacrificing context or information loss. The case ends with a series of questions, challenges, and potential for LLMs in data anonymization. © 2023 by the Association for Information Systems.
"
10.1109/CoDIT58514.2023.10284068,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177436126&origin=inward,Conference Paper,SCOPUS_ID:85177436126,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),plcs training in hybrid mode using the plc3000 solution: feedback and analysis from students,"
AbstractView references

Process automation is part of the knowledge base of many senior technicians and engineers. It is a learning process that follows a specific path in order to meet the defined objectives by complying with the standards, especially the safety of the installations. The PLC3000 solution aims at facilitating the teaching of PLCs for teachers, but also for students. PLC3000 has been developed with an agile pedagogical approach focused on students' learning. This tool can be easily integrated into any existing teaching, while guaranteeing an increased simplicity of use since it is accessible via a website. PLC3000 is a powerful tool offering the possibility to code in three languages defined in the standard IEC 61131-3, and to visualise the codes on a large number of interactive virtual models. This solution has been integrated in the programme of two engineering schools in France in 2022; IMT Nord Europe and ENIT. The feedback from students and its analysis are presented. © 2023 IEEE.
"
10.1007/978-981-99-7584-6_21,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177178180&origin=inward,Conference Paper,SCOPUS_ID:85177178180,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),iot software vulnerability detection techniques through large language model,"
AbstractView references

The explosion of IoT usage provides efficiency and convenience in various fields including daily life, business and information technology. However, there are potential risks in large-scale IoT systems and vulnerability detection plays a significant role in the application of IoT. Besides, traditional approaches like routine security audits are expensive. Thus, substitution methods with lower costs are needed to achieve IoT system vulnerability detection. LLMs, as new tools, show exceptional natural language processing capabilities, meanwhile, static code analysis offers low-cost software analysis avenues. The paper aims at the combination of LLMs and static code analysis, implemented by prompt engineering, which not only expands the application of LLMs but also provides a probability of accomplishing cost-effective IoT vulnerability software detection. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
10.1109/ASE56229.2023.00157,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177075118&origin=inward,Conference Paper,SCOPUS_ID:85177075118,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the devil is in the tails: how long-tailed code distributions impact large language models,"
AbstractView references

Learning-based techniques, especially advanced Large Language Models (LLMs) for code, have gained considerable popularity in various software engineering (SE) tasks. However, most existing works focus on designing better learning-based models and pay less attention to the properties of datasets. Learning-based models, including popular LLMs for code, heavily rely on data, and the data's properties (e.g., data distribution) could significantly affect their behavior. We conducted an exploratory study on the distribution of SE data and found that such data usually follows a skewed distribution (i.e., long-tailed distribution) where a small number of classes have an extensive collection of samples, while a large number of classes have very few samples. We investigate three distinct SE tasks and analyze the impacts of long-tailed distribution on the performance of LLMs for code. Our experimental results reveal that the long-tailed distribution has a substantial impact on the effectiveness of LLMs for code. Specifically, LLMs for code perform between 30.0% and 254.0% worse on data samples associated with infrequent labels compared to data samples of frequent labels. Our study provides a better understanding of the effects of long-tailed distributions on popular LLMs for code and insights for the future development of SE automation. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177040120&origin=inward,Conference Paper,SCOPUS_ID:85177040120,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),monitoring cyber peer-led team learning: a multimodal human-in-the-loop approach,"
AbstractView references

The recent adoption of generative artificial intelligence (AI) tools in education has transformed education and AI-assisted learning. However, researchers embracing applied machine learning (ML) in pedagogical settings continue to face challenges. Lack of publicly available multimodal datasets involving gesture and emotion recognition specifically in education is a bottleneck for building AI-enabled e-learning platforms. In the paper, we take a constructive stance at monitoring group behavior in cyber peer-led team learning (cPLTL) classes in Organic Chemistry using ML. Although past studies have attempted to quantify student engagement in e-learning, their use in the cPLTL use case for AI modeling has not yet been established. The hypothesis underlying our proposed framework is that online peer group behavior can be characterized by a human-in-the-loop model that relies on multiple input modalities. Thus, the aim is to identify behavioral patterns in head and facial movements that are augmented by lexical based sentiment and audio feature extraction. To combat the small data challenge, we propose a framework for the human-in-the-loop (HITL) system that actively learns the past group modalities. HITL strategies enable the algorithm to learn more efficiently from less data iteratively. The model will be implemented using active learning, measures of uncertainty, random sampling and entropy which are key in the design of the study. A qualitative comparison of sentiment modality with ChatGPT's participant performance evaluation has been discussed. The study will increase the use of AI in tools that support educators in universities using pedagogies of active engagement in science, technology, engineering, and math. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176615313&origin=inward,Conference Paper,SCOPUS_ID:85176615313,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),developing a scalable benchmark for assessing large language models in knowledge graph engineering,"
AbstractView references

As the field of Large Language Models (LLMs) evolves at an accelerated pace, the critical need to assess and monitor their performance emerges. We introduce a benchmarking framework focused on knowledge graph engineering (KGE) accompanied by three challenges addressing syntax and error correction, facts extraction and dataset generation. We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting. Consequently, our LLM-KG-Bench framework provides automatic evaluation and storage of LLM responses as well as statistical data and visualization tools to support tracking of prompt engineering and model performance. © 2023 CEUR-WS. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176419500&origin=inward,Conference Paper,SCOPUS_ID:85176419500,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),qua4i: question answering for the industry 4.0 domain. an application of intelligent virtual assistants,"
AbstractView references

The recent advancements with LLMs (Large Language Models) have led to many natural language applications solving different NLP (Natural Language Processing) tasks. In such systems, both NLG (Natural Language Generation) and NLU (Natural Language Understanding) play a crucial role. The notion of instruction-based LLMs caused an important impact in the development of chatbot-styled NLP applications. In the past months, we have seen an incredible amount of LLMs making use of this idea to solve tasks including, but not limited to, QA (Question Answering), information extraction, intent recognition, dialogue or language generation. Whereas these systems have very good generalisation capabilities, in which the AGI (Artificial General Intelligence) idea has spread to the NLP research field, we still lack of good domain adaptation techniques for tailored ADI (Artificial Domain Intelligence) systems. Apart from prompt engineering, we cannot fully control the outputs of this new chatbot-styled LLMs to obtain our desired output given a specific domain. In the context of the CEL.IA network, we present QUA4I (QUestion Answering for the Industry 4.0), a chatbot-oriented application or IVA (Intelligent Virtual Assistant) for the industry 4.0 domain, mixing the NLU and NLG techniques using the Rasa chatbot framework. We designed a custom demo for question answering and information extraction about the industry 4.0 topic, including a dialogue system which can also generate automatic responses in natural language. We included both ASR (Automatic Speech Recognition) and TTS (Text To Speech) modules, so we can also interact with the bot using spoken language in Spanish. © 2023 Copyright for this paper by its authors.
"
10.18178/wcse.2023.06.031,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176387262&origin=inward,Conference Paper,SCOPUS_ID:85176387262,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),from words to flight: integrating openai chatgpt with px4/gazebo for natural language-based drone control,"
AbstractView references

The rapid progress of large language models in recent years has opened new opportunities for human-robot interaction. In this paper, we propose a novel approach to control drones using natural language commands by integrating OpenAI ChatGPT with the PX4/Gazebo simulator. The proposed system enables users to interact with the simulator using everyday language, allowing them to control the drone's actions with ease, eliminating the need for extensive training in drone piloting. We discuss the implementation details, including the validation of the ChatGPT-generated commands and their translation into executable actions in the simulator. To the best of our knowledge, this is the first proposal of a verification and validation system for commands generated by ChatGPT and LLMs in general. Furthermore, we discuss the crafting of effective prompts and the essential criteria for doing so. Our approach demonstrates promising results in terms of both usability and reliability, paving the way for further research on natural language-based control systems for robotics applications. © WCSE 2023.All rights reserved.
"
10.1016/B978-0-443-18498-7.00010-7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176295574&origin=inward,Book Chapter,SCOPUS_ID:85176295574,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),introduction to artificial intelligence and machine learning algorithms,"
AbstractView references

The machine learning paradigm and other data-driven approaches have seen a substantial rise in prominence in recent years. Fueled by advancements in data storage and a wider availability of hardware with the processing power needed to make use of such algorithms, machine learning has allowed researchers to tackle increasingly complex problems in a wide variety of domains. In this chapter, the authors will provide a survey of different forms of artificial intelligence, including natural language processing and various machine learning methodologies. This chapter begins with a brief introduction covering some of the history and computational advancements behind artificial intelligence (AI). The second section of this chapter provides further discussion of recent historical milestones in the surge of adoption seen just after the turn of the 21st century, followed by discussion on the difficulties still observed in developing standardized methods of applying AI techniques. The third section contains a survey of AI in its different forms with discussion on the variety of methodologies involved, including the different machine learning approaches such as supervised versus unsupervised, and predictive and generative algorithms. This will include examples showing the state-of-the-art capabilities currently in the industry. During the discussion of supervised machine learning methodology, the authors provide further examples showing the variety and scope of previous research have employed machine learning methodology within the context of diabetes research. The fourth section of this chapter provides a discussion of some of the ethical concerns related to AI and its applications. In this chapter's fifth section, the authors will then discuss the inclusion of the machine learning paradigm within a model of research that employs both data- and hypothesis-driven approaches. Following this model enables researchers to benefit from the respective advantages of each approach as they complement each other, thereby providing the researchers with a more complete model for their scientific endeavors. © 2023 Elsevier Inc. All rights reserved.
"
10.1016/j.caeai.2023.100183,S2666920X23000620,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176244446&origin=inward,Article,SCOPUS_ID:85176244446,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),performance of chatgpt on the us fundamentals of engineering exam: comprehensive assessment of proficiency and potential implications for professional environmental engineering practice,"In recent years, advancements in artificial intelligence (AI) have led to the development of large language models like GPT-4, demonstrating potential applications in various fields, including education. This study investigates the feasibility and effectiveness of using ChatGPT, a GPT-4 based model, in achieving satisfactory performance on the Fundamentals of Engineering (FE) Environmental Exam. This study further shows a significant improvement in the model's accuracy when answering FE exam questions through noninvasive prompt modifications, substantiating the utility of prompt modification as a viable approach to enhance AI performance in educational contexts. Furthermore, the findings reflect remarkable improvements in mathematical capabilities across successive iterations of ChatGPT models, showcasing their potential in solving complex engineering problems. Our paper also explores future research directions, emphasizing the importance of addressing AI challenges in education, enhancing accessibility and inclusion for diverse student populations, and developing AI-resistant exam questions to maintain examination integrity. By evaluating the performance of ChatGPT in the context of the FE Environmental Exam, this study contributes valuable insights into the potential applications and limitations of large language models in educational settings. As AI continues to evolve, these findings offer a foundation for further research into the responsible and effective integration of AI models across various disciplines, ultimately optimizing the learning experience and improving student outcomes."
10.1007/978-3-031-45438-7_28,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176005326&origin=inward,Conference Paper,SCOPUS_ID:85176005326,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a domain-specific language and model-based engine for implementing iot dashboard web applications,"
AbstractView references

The Internet of Things (IoT) has become one of the fundamental pillars of the digital transformation of society, with favorable impacts on people’s quality of life. Furthermore, IoT systems generate large volumes of data at very high speeds, which come from diverse sources (heterogeneous sensors), requiring the permanent adaptation of the content and the way of presenting the information to the user; hence, a low-level implementation approach becomes unproductive. In this context, Model-Driven Engineering (MDE) has proven to be an appropriate software development approach to cope with the complexity and evolution of IoT systems. However, there are few proposals for Domain-Specific Languages (DSLs) aimed at building dashboards that synthesize the metrics and fundamental monitoring data of an IoT system. Therefore, this paper proposes a DSL and a model-based transformation engine to design and automatically implement IoT dashboard visualization web applications that combine pages, panels, charts, grids, data filters, hyperlinks, and labels with warnings and prescriptive recommendations. In addition, the proposed solution abstracts implementation details from heterogeneous data sources (physical and virtual sensors), making them transparent to domain experts. The empirical evaluation of the solution through a quasi-experiment based on the Method Evaluation Model (MEM) showed that the participants perceived the solution as useful and easy to use, so they would be willing to use it in the future. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175643184&origin=inward,Conference Paper,SCOPUS_ID:85175643184,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),umuteam at exist 2023: sexism identification and categorisation fine-tuning multilingual large language models,"
AbstractView references

The third edition of the EXIST shared task focuses on the identification and categorisation of sexism in social networks. This edition will take place as a lab in CLEF 2023. There are two innovations in this edition. The main one is the perspective of learning with disagreements instead of classification with hard labels. The other novelty is the Source Intention task, which focuses on determining the author’s intention. It distinguishes among direct messages whether the intention is to incite sexism, to report and share sexist situations suffered by women, or to judge sexist situations with the aim of condemning them. As in previous editions, there are documents in English and Spanish. Our proposal to solve all tasks in both languages is to combine sentence embeddings from several multilingual and Spanish Large Language Models with linguistic features. We achieve position 11 in the first task (sexism identification) and position 10 in the second task (source intention), both using the Soft vs. Soft paradigm for Spanish and English combined. For the third task (sexism categorisation) we achieve 18th position using the hard vs. hard paradigm. © 2023 Copyright for this paper by its authors.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175624044&origin=inward,Conference Paper,SCOPUS_ID:85175624044,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ncu-iisr: prompt engineering on gpt-4 to stove biological problems in bioasq 11b phase b,"
AbstractView references

In this paper, we present our system applied in BioASQ 11b phase b. We showcase prompt engineering strategies and outline our experimental steps. Building upon the success of ChatGPT/GPT-4 in answer generation and the field of biology, we developed a system that utilizes GPT-4 to answer biomedical questions. The system leverages OpenAI’s ChatCompletions API and combines Prompt Engineering methods to explore various prompts. In addition, we also attempted to incorporate GPT-4 into our system from last year, which combines a BERT-based model and BERTScore. However, the standalone GPT-4 method outperformed this approach by a large margin. Ultimately, in our submission, we adopted what we believe to be the optimal prompts and achieved the highest scores in the second batch. © 2023 Copyright for this paper by its authors.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175609590&origin=inward,Conference Paper,SCOPUS_ID:85175609590,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),uzh_pandas at simpletext@clef-2023: alpaca lora 7b and lens model selection for scientific literature simplification,"
AbstractView references

In this study, we advance the field of scientific text simplification by harnessing the capabilities of Alpaca LoRA 7B [1], a large language model derivative of the 7B LLaMA [2]. We expand the dataset for Task 3 of SimpleText@CLEF-2023 [3] by integrating data from Task 2, aiming to identify complex terms in need of explanation for better text comprehension. Our methodology involves rigorous fine-tuning, prompt engineering, and the application of the LENS score [4] as a tool for model reranking and evaluation. Our findings suggest the efficacy of our approach in creating a more effective text simplification system. Our final model demonstrates expertise not only in expanding abbreviations, but also in explaining complex terms present in the input sentence. This ability allows it to create texts that are both easy to understand and simple to comprehend, making the information presented more accessible and opening the door for more efficient communication. However, the study also highlights several challenges and areas of improvement, providing a valuable contribution to future research in text simplification. Our research underscores the potential of large language models like Alpaca LoRA 7B in transforming complex terminologies into more accessible language, ultimately enhancing the public’s understanding of scientific literature. © 2023 Copyright for this paper by its authors.
"
10.1109/MODELS-C59198.2023.00096,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175600987&origin=inward,Conference Paper,SCOPUS_ID:85175600987,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),extracting domain models from textual requirements in the era of large language models,"
AbstractView references

Requirements Engineering is a critical part of the software lifecycle, describing what a given piece of software will do (functional) and how it will do it (non-functional). Requirements documents are often textual, and it is up to software engineers to extract the relevant domain models from the text, which is an error-prone and time-consuming task. Considering the recent attention gained by Large Language Models (LLMs), we explored how they could support this task. This paper investigates how such models can be used to extract domain models from agile product backlogs and compare them to (i) a state-of-practice tool as well as (ii) a dedicated Natural Language Processing (NLP) approach, on top of a reference dataset of 22 products and 1, 679 user stories. Based on these results, this paper is a first step towards using LLMs and/or tailored NLP to support automated requirements engineering thanks to model extraction using artificial intelligence. © 2023 IEEE.
"
10.1109/SEAA60479.2023.00017,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175599126&origin=inward,Conference Paper,SCOPUS_ID:85175599126,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),echo: an approach to enhance use case quality exploiting large language models,"
AbstractView references

UML use cases are commonly used in software engineering to specify the functional requirements of a system since they are an effective tool for interacting with stakeholders thanks to the use of natural languages. However, producing high-quality use cases can be challenging due to the lack of precise guidelines and suitable tools. This can lead to problems, e.g. inaccuracy and incompleteness, in the derived software artifacts and the final product. Recent advancements in Natural Language Processing and Large Language Models (LLMs) can provide the premises for developing tools supporting activities based on natural languages. In this paper, we propose ECHO, a novel approach for supporting software engineers in enhancing the quality of UML use cases using LLMs. Our approach consists of a co-prompt engineering approach and an iterative and interactive process with the LLM to improve the quality of use cases, based on practitioners' feedback. To prove the feasibility of the proposal, we instantiated the approach using ChatGPT and performed a controlled experiment to assess its effectiveness by involving seven software engineering professionals. Three were part of the experimental group and used ECHO to improve the quality of the use cases. Three others were the control group and enhanced the quality of use cases manually. Finally, the last participant acted as an oracle, blind w.r.t. the groups, and evaluated the quality of the enhanced use cases, both qualitatively by means of a questionnaire, and quantitatively, by means of the Use Case Points metric. Results show that ECHO can effectively support software engineers to improve use cases' quality thanks to the prompts suitably designed to interact with ChatGPT. © 2023 IEEE.
"
10.1109/SEAA60479.2023.00061,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175597991&origin=inward,Conference Paper,SCOPUS_ID:85175597991,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),investigating chatgpt's potential to assist in requirements elicitation processes,"
AbstractView references

Natural Language Processing (NLP) for Requirements Engineering (RE) (NLP4RE) seeks to apply NLP tools, techniques, and resources to the RE process to increase the quality of the requirements. There is little research involving the utilization of Generative AI-based NLP tools and techniques for requirements elicitation. In recent times, Large Language Models (LLM) like ChatGPT have gained significant recognition due to their notably improved performance in NLP tasks. To explore the potential of ChatGPT to assist in requirements elicitation processes, we formulated six questions to elicit requirements using ChatGPT. Using the same six questions, we conducted interview-based surveys with five RE experts from academia and industry and collected 30 responses containing requirements. The quality of these 36 responses (human-formulated + ChatGPT-generated) was evaluated over seven different requirements quality attributes by another five RE experts through a second round of interview-based surveys. In comparing the quality of requirements generated by ChatGPT with those formulated by human experts, we found that ChatGPT-generated requirements are highly Abstract, Atomic, Consistent, Correct, and Understandable. Based on these results, we present the most pressing issues related to LLMs and what future research should focus on to leverage the emergent behaviour of LLMs more effectively in natural language-based RE activities. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175487307&origin=inward,Conference Paper,SCOPUS_ID:85175487307,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),corrpus: code-based structured prompting for neurosymbolic story understanding,"
AbstractView references

Story generation and understanding-as with all NLG/NLU tasks-has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for many flaws that neural networks have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use of symbolic methods for tracking the state of stories and aiding in story understanding. We show that our CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on preexisting story understanding tasks (bAbI Task 2 and Re3) with minimal hand engineering. This work highlights the usefulness of code-based symbolic representations for enabling LLMs to better perform story reasoning tasks. © 2023 Association for Computational Linguistics.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175466652&origin=inward,Conference Paper,SCOPUS_ID:85175466652,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards reasoning in large language models: a survey,"
AbstractView references

Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work. © 2023 Association for Computational Linguistics.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175447646&origin=inward,Conference Paper,SCOPUS_ID:85175447646,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the magic of if: investigating causal reasoning abilities in large language models of code,"
AbstractView references

Causal reasoning, the ability to identify cause- and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like if, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations. Code and data are available at https://github.com/xxxiaol/magic-if. © 2023 Association for Computational Linguistics.
"
10.1109/ETFA54631.2023.10275411,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175444119&origin=inward,Conference Paper,SCOPUS_ID:85175444119,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt for plc/dcs control logic generation,"
AbstractView references

Large language models (LLMs) providing generative AI have become popular to support software engineers in creating, summarizing, optimizing, and documenting source code. It is still unknown how LLMs can support control engineers using typical control programming languages in programming tasks. Researchers have explored GitHub CoPilot or DeepMind AlphaCode for source code generation but did not yet tackle control logic programming. A key contribution of this paper is an exploratory study, for which we created 100 LLM prompts in 10 representative categories to analyze control logic generation for of PLCs and DCS from natural language. We tested the prompts by generating answers with ChatGPT using the GPT-4 LLM. It generated syntactically correct IEC 61131-3 Structured Text code in many cases and demonstrated useful reasoning skills that could boost control engineer productivity. Our prompt collection is the basis for a more formal LLM benchmark to test and compare such models for control logic generation. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175437647&origin=inward,Conference Paper,SCOPUS_ID:85175437647,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),explanation regeneration via information bottleneck,"
AbstractView references

Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Thanks to the superior generative capacity of large pretrained language models (PLM), recent work built on prompt engineering enables explanations generated without specific training. However, explanations generated through single-pass prompting often lack sufficiency and conciseness, due to the prompt complexity and hallucination issues. To discard the dross and take the essence of current PLM's results, we propose to produce sufficient and concise explanations via the information bottleneck (EIB) theory. EIB regenerates explanations by polishing the single-pass output of PLM but retaining the information that supports the contents being explained by balancing two information bottleneck objectives. Experiments on two different tasks verify the effectiveness of EIB through automatic evaluation and thoroughly-conducted human evaluation. © 2023 Association for Computational Linguistics.
"
10.1080/02763869.2023.2250680,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175405186&origin=inward,Article,SCOPUS_ID:85175405186,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompt engineers or librarians? an exploration,"
AbstractView references

The article explores the role of “prompt engineers” as a professional title, extending beyond the field of generative AI for developers, comparing certain tasks to the role of librarians, such as conducting search queries. It is possible for librarians to work with AI models in conjunction with traditional literature databases with emphasizing the need to recognize the distinct nature of these information resources. We should take cautious consideration of the specific skills worth acquiring to improve work efficiency, as well as an understanding of the development trends in generative AI and library science. © 2023 The Author(s). Published with license by Taylor & Francis Group, LLC.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175398127&origin=inward,Conference Paper,SCOPUS_ID:85175398127,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),alexander knox at semeval-2023 task 5: the comparison of prompting and standard fine-tuning techniques for selecting the type of spoiler needed to neutralize a clickbait,"
AbstractView references

Clickbait posts are a common problem on social media platforms, as they often deceive users by providing misleading or sensational headlines that do not match the content of the linked web page. The aim of this study is to create a technique for identifying the specific type of suitable spoiler - be it a phrase, a passage, or a multipart spoiler - needed to neutralize clickbait posts. This is achieved by developing a machine learning classifier analyzing both the clickbait post and the linked web page. Modern approaches for constructing a text classifier usually rely on fine-tuning a transformer-based model pre-trained on large unsupervised corpora. However, recent advances in the development of large-scale language models have led to the emergence of a new transfer learning paradigm based on prompt engineering. In this work, we study these two transfer learning techniques and compare their effectiveness for clickbait spoiler-type detection task. Our experimental results show that for this task, using the standard fine-tuning method gives better results than using prompting. The best model can achieve a similar performance to that presented by Hagen et al. (2022). © 2023 Association for Computational Linguistics.
"
10.1007/s10639-023-12249-8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175343048&origin=inward,Article,SCOPUS_ID:85175343048,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),few-shot is enough: exploring chatgpt prompt engineering method for automatic question generation in english education,"
AbstractView references

Through design and development research (DDR), we aimed to create a validated automatic question generation (AQG) system using large language models (LLMs) like ChatGPT, enhanced by prompting engineering techniques. While AQG has become increasingly integral to online learning for its efficiency in generating questions, issues such as inconsistent question quality and the absence of transparent and validated evaluation methods persist. Our research focused on creating a prompt engineering protocol tailored for AQG. This protocol underwent several iterations of refinement and validation to improve its performance. By gathering validation scores and qualitative feedback on the produced questions and the system’s framework, we examined the effectiveness of the system. The study findings indicate that our combined use of LLMs and prompt engineering in AQG produces questions with statistically significant validity. Our research further illuminates academic and design considerations for AQG design in English education: (a) certain question types might not be optimal for generation via ChatGPT, (b) ChatGPT sheds light on the potential for collaborative AI-teacher efforts in question generation, especially within English education. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175292947&origin=inward,Conference Paper,SCOPUS_ID:85175292947,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),umuteam at homo-mex 2023: fine-tuning large language models integration for solving hate-speech detection in mexican spanish,"
AbstractView references

This work describes the participation of the UMUTeam in the HOMO-MEX shared task at IberLEF 2023, on Hate speech detection in Online Messages directed tOwards the MEXican Spanish speaking LGBTQ+ population. We have addressed the two proposed tasks: Task 1, consisting of identifying the category of hate speech and, Task 2, on determining the types of phobia from a given set of tweets. For both tasks, we have evaluated different approaches based on the combination of sentence embeddings using ensemble learning and knowledge integration. Specifically, the sentence embeddings have been extracted from several Spanish and multilingual Large Language Models after fine-tuning them for each task separately. In total, 11 teams participated in Task 1 and 9 teams in Task 2. The best run sent by our team placed in position 3rd for Task1 and position 8th for Task 2 with an F1-score of 0.842 and a macro-average F1-score of 0.669, respectively, with 0.885 and 0.696 being the results obtained by the teams ranked in 1st position. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175292701&origin=inward,Conference Paper,SCOPUS_ID:85175292701,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),umuteam at dipromats 2023: propaganda detection in spanish and english combining linguistic features with contextual sentence embeddings,"
AbstractView references

These notes summarise the UMUTeam's contribution to the Dipromats joint task of IberLEF 2023, which deals with the fine-grained detection of propaganda techniques in the political domain, using texts written in English and Spanish. Our contribution is based on the combination of linguistic features and sentence embeddings extracted for several large language models using ensemble learning and knowledge integration. We rank third in the binary classification subtask, first in the multi-classification subtask, and second in the multi-label classification subtask. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
10.1109/SCSET58950.2023.00070,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175198437&origin=inward,Conference Paper,SCOPUS_ID:85175198437,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompt learning under the large language model,"
AbstractView references

With the emergence of models such as chatGPT and Baidu AI Wenxin Yiyan, the research and application of NLP (Natural Language Processing) is increasingly centered on PLM (Pretrained Language Model),It marks that the current machine learning model has reached a new height. This article first introduces the background of the large language model, and introduces pre-training + fine-tuning and the current popular Prompt from the four major paradigms of NLP. Understand the workflow and function of Prompt, focusing on Prompt engineering, and structures, and looking ahead to future challenges for Prompt. © 2023 IEEE.
"
10.1109/SCSET58950.2023.00033,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175193024&origin=inward,Conference Paper,SCOPUS_ID:85175193024,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research on intelligent stamping process design of automobile panel,"
AbstractView references

In view of the pain points of complex stamping forming problems of automobile panels, multiple and complex processes, high quality requirements, long talent training cycle, difficult knowledge accumulation, and difficult knowledge reuse, a new method is proposed: through the study of stamping theoretical rules and the laws of flanging crack limit of a large number of parts, a parametric model is established based on CATIA, and an expert database of input parameters is established, Based on C++, a rule query program for technologists is made, and artificial intelligence technology is used to provide intelligent and constructive design assistance for stamping professional designers. Different from traditional programs, it simulates the thinking process of experts and uses knowledge to infer results. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175024188&origin=inward,Conference Paper,SCOPUS_ID:85175024188,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),umuteam at huhu 2023: detecting prejudices in humour using ensemble learning and knowledge integration,"
AbstractView references

These notes detail the participation of the UMUTeam in the HUHU shared task, organised in the IberLEF 2023 workshop. This shared task is concerning the usage of humour for masking prejudice towards minority groups. The organisers propose three challenges: the first one for detecting hurtful humour, which is a binary task; the second one for detecting the targets of the prejudices with a multi-label classification; and the last one is to calculate the intensity of the prejudice. Our team participate in all tasks evaluating several ensemble learning strategies to combine several sentence embeddings extracted from several Large Language Models. Although our results are competitive with our custom validation split, they do not outperform the baselines proposed by the organisers with the test split. After the analysis of the golden labels released by the organisers, we detect an error when incorporating the test split. Accordingly, we re-run our experiments and we obtain the real results by our team. We would achieved the first position for Task 1 (macro F1-score of 85.5%), the second position in Task 2 (macro F1-score of 78.6%), and the third position in Task 3 (RMSE of 0.878). © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174920490&origin=inward,Conference Paper,SCOPUS_ID:85174920490,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),comparative analysis of gpt-4 and human graders in evaluating praise given to students in synthetic dialogues,"
AbstractView references

Research suggests that providing specific and timely feedback to human tutors enhances their performance. However, it presents challenges due to the time-consuming nature of assessing tutor performance by human evaluators. Large language models, such as the AI-chatbot ChatGPT, hold potential for offering constructive feedback to tutors in practical settings. Nevertheless, the accuracy of AI-generated feedback remains uncertain, with scant research investigating the ability of models like ChatGPT to deliver effective feedback. In this work-in-progress, we evaluate 30 dialogues generated by GPT-4 in a tutor-student setting. We use two different prompting approaches, the zero-shot chain of thought and the few-shot chain of thought, to identify specific components of effective praise based on five criteria. These approaches are then compared to the results of human graders for accuracy. Our goal is to assess the extent to which GPT-4 can accurately identify each praise criterion. We found that both zero-shot and few-shot chain of thought approaches yield comparable results. GPT-4 performs moderately well in identifying instances when the tutor offers specific and immediate praise. However, GPT-4 underperforms in identifying the tutor’s ability to deliver sincere praise, particularly in the zero-shot prompting scenario where examples of sincere tutor praise statements were not provided. Future work will focus on enhancing prompt engineering, developing a more general tutoring rubric, and evaluating our method using real-life tutoring dialogues. © 2023 Copyright for this paper by its authors.
"
10.51202/9783181024195-999,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174851268&origin=inward,Article,SCOPUS_ID:85174851268,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),challenges utilizing plcopen for opc ua,"
AbstractView references

Interoperable engineering systems for industrial controller should offer an export of IEC 61131 control logic compliant to the PLCopen XML schema. OPC UA is a favored communication protocol and information model for future industrial control systems. The OPC Foundation and partners provide corresponding model types in companion specifications including PLCopen to allow for interoperability. Even workflows and online transformation methods exist to utilize companion specifications in controllers or OPC UA servers of control systems. Nevertheless, some use cases, performance or application specific constraints enforce an offline transformation from PLCopen XML exports to OPC UA Nodeset XML that may be used in external OPC UA servers and modelling tools. This contribution shows how such a transformation may be realized via extensible stylesheet language transformations (XSLT) and which challenges as well as design decisions arise applying them for large industrial scale controller configurations. Especially, strategies to handle multiple control configurations and libraries comprising several thousand program organization unit and data types as well as their instances are addressed. Consequently, use cases and results of the developed approach based on a logi.CAD/32 engineering system are discussed to derive recommendations for the PLCopen OPC UA companion specification and its utilization. © 2023 The Authors.
"
10.1109/ACCESS.2023.3325318,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174832112&origin=inward,Article,SCOPUS_ID:85174832112,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),program design for ship piping network visualization model based on wpf and c#,"
AbstractView references

The traditional approach to ship pipeline programming often involves describing and calculating based on text files. This method tends to be error-prone and time-consuming, especially for complex systems with a large amount of data related to pipelines. In case of system failures, troubleshooting becomes inconvenient. Additionally, matrix calculations can lead to issues such as extreme values, and visualizing the results is challenging. To address these challenges, this paper proposes an intuitive, user-friendly, and efficient ship pipeline programming tool. The tool utilizes Microsoft's WPF (Windows Presentation Foundation) graphical interface technology and the C# programming language. A graphical interface for ship pipeline networks is constructed, allowing users to visually build the physical model of pipelines through intuitive drag-and-drop and connection operations. Furthermore, the paper adopts Excel spreadsheets as the input method for data and combines it with domain knowledge of marine engineering pipelines. This approach establishes logical models for various pipelines, valves, and other equipment on ships. The models incorporate attributes from the spreadsheet to control characteristics like maximum flow rates and flow directions. By combining graphical modules, pipeline connections, and Excel spreadsheets, the method proposed in this paper offers a more convenient way to construct pipeline network models. The resulting models closely resemble actual ship pipelines, leading to improved accuracy and efficiency in data processing and calculations. © 2013 IEEE.
"
10.1109/REW57809.2023.00040,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174730132&origin=inward,Conference Paper,SCOPUS_ID:85174730132,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a multimedia approach to problem descriptions for fine-grained detail characterization,"
AbstractView references

Requirements analysis has always been regarded as the starting point and focus of requirements engineering. The Problem Frames (PF) approach provides a problem-oriented requirements analysis method by describing the software's operating environment in detail. It can effectively bridge the gap between requirements analysis and system design. However, an intuitive and convenient means of elaborating the rich details of complex requirements and their contexts is missing from the literature. Currently, the main methods for describing requirements are through textual descriptions or formal language descriptions, which can result in the PF model containing a large amount of textual representations, and increasing the difficulty and time required for user understanding. In this paper, we present the PF2Detail, a tool that enables fine-grained descriptions of the PF model. Building upon the PF model construction, PF2Detail incorporates multimedia to describe complex requirements in context. Each node in the model can include multimedia, text, or formal language to provide detailed characterizations. Additionally, users can view detailed information about any node in the model and navigate the entire model, aiming to enhance the level of details in the PF model descriptions. © 2023 IEEE.
"
10.1109/REW57809.2023.00070,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174729710&origin=inward,Conference Paper,SCOPUS_ID:85174729710,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),preface of reset 2023: 2nd international workshop on requirement engineering for software startups and emerging technologies,"
AbstractView references

The Second International Workshop on Requirement Engineering for Software startups and Emerging Technologies (RESET) is a part of the 31st IEEE International Requirements Engineering Conference 2023, held on 4 September 2023. The workshop brought together requirements engineering researchers and practitioners to discuss the need for adapting conventional requirement engineering artifacts (i.e., requirement definition, metrics), processes and practices in developing and operating emerging technologies, including Software Startups, Artificial Intelligence (AI), Blockchain, and Quantum Computing. Participants gained insights into the RE practices, tools, techniques, and frameworks that can help them build scalable, robust, and innovative software-intensive systems. The workshop included a keynote presentation and four paper presentations. © 2023 IEEE.
"
10.1007/978-981-99-4752-2_48,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174710243&origin=inward,Conference Paper,SCOPUS_ID:85174710243,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ra-kd: random attention map projection for knowledge distillation,"
AbstractView references

Pretrained language models such as BERT, ELMO, and GPT have proven to be effective for natural language processing tasks. However, deploying them on computationally constrained devices poses a challenge. Moreover, the practical application of these models is affected by the training and deployment of large-scale pretrained language models. While knowledge distillation (KD) in the intermediate layer can improve standard KD techniques, especially for large-scale pretrained language models, distillation in the intermediate layers brings excessive computational burdens and engineering difficulties in mapping the middle layer of a student model with a variable number of layers. The attention map is one of the essential blocks in the intermediate layer. To address these problems, we propose an approach called random attention map projection, in which the intermediate layers are randomly selected from the teacher model. Then, the attention map knowledge is extracted to the student model's attention block. This approach enables the student model to capture deeper semantic information while reducing the computational cost of the intermediate layer distillation method. We conducted experiments on the GLUE dataset to verify the effectiveness of our approach. Our proposed RA-KD approach performs considerably better than other KD approaches in both performance and training time. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
10.1109/REW57809.2023.00020,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174706923&origin=inward,Conference Paper,SCOPUS_ID:85174706923,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated identification of deontic modalities in software engineering contracts: a domain adaptation-based generative approach,"
AbstractView references

Contracts are legally binding agreements between parties that establish rights, obligations, and terms for their business relationship. They articulate the deontic modalities (Obligations, Permissions, Prohibitions, and Exclusions) that apply to those involved in the contractual agreement. Deontic modalities can be leveraged to effectively elicit Software Engineering (SE) requirements. In this paper, we propose a novel approach for identifying deontic modalities from contracts, employing a combination of text generation and domain adaptation techniques. Among the SOTA approaches we experimented with, the T5-large model yielded the best results, with an average precision and recall of 0.96 and 0.95, respectively. Comparing our approach with previous methods, the results show that our approach handles significant class imbalances in the training data and demonstrates good generalization ability on new datasets. © 2023 IEEE.
"
10.1109/REW57809.2023.00034,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174695938&origin=inward,Conference Paper,SCOPUS_ID:85174695938,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an re'23 workshop on environment-driven requirements engineering (envire'23),"
AbstractView references

We organize a one-day workshop on Environment-Driven Requirements Engineering (EnviRE'23) in conjunction with the 31st IEEE International Requirements Engineering Conference. With the rising influence of AI, IoT, and cyber-physical systems, we realize that the environment, in which the software operates, becomes more open and evolves rapidly with stakeholders' changing needs. EnviRE'23 features one keynote and seven accepted papers. In addition, we organize an interactive session with workshop participants to explore the role of large language models (LLMs), specifically ChatGPT, in requirements elicitation and modeling. Overall, the workshop is aimed at bringing the interested researchers and practitioners together, exchanging ideas and visions, and exploring a set of open problems to pursue in the years to come. Since the first edition of EnviRE in 2021, this is the first time that our workshop is held in person. We are excited to continue our workshop after the pandemic. © 2023 IEEE.
"
10.1007/978-3-031-43987-2_67,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174687674&origin=inward,Conference Paper,SCOPUS_ID:85174687674,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),zero-shot nuclei detection via visual-language pre-trained models,"
AbstractView references

Large-scale visual-language pre-trained models (VLPM) have proven their excellent performance in downstream object detection for natural scenes. However, zero-shot nuclei detection on H &E images via VLPMs remains underexplored. The large gap between medical images and the web-originated text-image pairs used for pre-training makes it a challenging task. In this paper, we attempt to explore the potential of the object-level VLPM, Grounded Language-Image Pre-training (GLIP) model, for zero-shot nuclei detection. Concretely, an automatic prompts design pipeline is devised based on the association binding trait of VLPM and the image-to-text VLPM BLIP, avoiding empirical manual prompts engineering. We further establish a self-training framework, using the automatically designed prompts to generate the preliminary results as pseudo labels from GLIP and refine the predicted boxes in an iterative manner. Our method achieves a remarkable performance for label-free nuclei detection, surpassing other comparison methods. Foremost, our work demonstrates that the VLPM pre-trained on natural image-text pairs exhibits astonishing potential for downstream tasks in the medical field as well. Code will be released at github.com/VLPMNuD. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.
"
10.1109/ICALT58122.2023.00066,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174610330&origin=inward,Conference Paper,SCOPUS_ID:85174610330,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an augmentative and alternative communication synthetic corpus for brazilian portuguese,"
AbstractView references

In recent years, Augmentative and Alternative Communication (AAC) systems have grown significantly in Brazil, particularly for individuals with cognitive disorders who rely on high-tech AAC tools. Artificial Intelligence (AI) has significantly improved high-tech AAC systems by enhancing accessibility, increasing output generation speed, and improving AAC interfaces' customization and adaptability. This study investigates the use of Large Language Models (LLMs) to generate synthetic text data to augment a corpus for AAC in Brazilian Portuguese. A three-step method was used to augment an initial corpus of 667 AAC-like sentences produced by specialists to a corpus of 13k sentences, comprising sentence collection, corpus augmentation using GPT-3 in a few-shot setting, and corpus cleaning. The quality and reliability of the generated corpus were assessed through a coverage analysis, comparing the content of the generated sentences with the original human-composed sentences. The results provide insights into the methods' strengths and limitations and inform future efforts to improve the generation of synthetic text data for the AAC domain in Brazilian Portuguese. © 2023 IEEE.
"
10.1109/AIC57670.2023.10263807,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174535107&origin=inward,Conference Paper,SCOPUS_ID:85174535107,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research on intelligent information screening algorithm of japanese network corpus based on computer corpus,"
AbstractView references

Corpus is a collection of data that contains a large amount of real language information and can be processed by computer. Using corpus to study the characteristics of language has become one of the most important applications of corpus. At present, corpus based linguistic research is in the ascendant and is becoming one of the new research methods in linguistics. Discourse analysis is also a study of real corpus. The combination of the two can inject new vitality into the study of discourse analysis. With the deepening of the research on information screening of Japanese network corpus, some problems to be solved gradually appear. This paper reviews the background and research paradigm of corpus based discourse analysis model, looks forward to the future research, and points out the existing problems and challenges of this research paradigm. The innovation of information teaching methods and the diversification trend of foreign language teaching methods. With the vigorous development of the Internet, big data and artificial intelligence and other technologies, foreign language teaching methods inevitably move towards the direction of informatization and dataization, and the diversified development of teaching methods, the use of advanced information and multimedia technology to provide students with an effective language learning environment, but also poses challenges to teachers' teaching ability. The teaching methods, teaching content, and teaching methods of the Japanese language major must be further optimized to meet the needs of students and society in the new era. While facing the above challenges, it is necessary to seize the development opportunities, innovate teaching reforms, and respond to the needs of the times and society for the development of Japanese language majors. These urgently require the intervention of new language teaching methods. © 2023 IEEE.
"
10.2118/215167-MS,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174493301&origin=inward,Conference Paper,SCOPUS_ID:85174493301,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),large language models (llms) for natural language processing (nlp) of oil and gas drilling data,"
AbstractView references

In the oil and gas industry, drilling activities spawn substantial volumes of unstructured textual data. The examination and interpretation of these data pose significant challenges. This research exploits the emerging capabilities of large language models (LLMs) with over 100 billion parameters to extract actionable insights from raw drilling data. Through fine-tuning methodologies and the use of various prompt engineering strategies, we addressed several text downstream tasks, including summarization, classification, entity recognition, and information extraction. This study delves into our methods, findings, and the novel application of LLMs for efficient and precise analysis of drilling data. Copyright © 2023, Society of Petroleum Engineers.
"
10.1109/VL-HCC57772.2023.00018,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174478822&origin=inward,Conference Paper,SCOPUS_ID:85174478822,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"exploring the role of ai assistants in computer science education: methods, implications, and instructor perspectives","
AbstractView references

The use of AI assistants, along with the challenges they present, has sparked significant debate within the community of computer science education. While these tools demonstrate the potential to support students' learning and instructors' teaching, they also raise concerns about enabling unethical uses by students. Previous research has suggested various strategies aimed at addressing these issues. However, they concentrate on introductory programming courses and focus on one specific type of problem. The present research evaluated the performance of ChatGPT, a state-of-the-art AI assistant, at solving 187 problems spanning three distinct types that were collected from six undergraduate computer science. The selected courses covered different topics and targeted different program levels. We then explored methods to modify these problems to adapt them to ChatGPT's capabilities to reduce potential misuse by students. Finally, we conducted semi-structured interviews with 11 computer science instructors. The aim was to gather their opinions on our problem modification methods, understand their perspectives on the impact of AI assistants on computer science education, and learn their strategies for adapting their courses to leverage these AI capabilities for educational improvement. The results revealed issues ranging from academic fairness to long-term impact on students' mental models. From our results, we derived design implications and recommended tools to help instructors design and create future course material that could more effectively adapt to AI assistants' capabilities. © 2023 IEEE.
"
10.1007/978-3-031-42622-3_49,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174436695&origin=inward,Conference Paper,SCOPUS_ID:85174436695,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai-powered chatbots and the transformation of work: findings from a case study in software development and software engineering,"
AbstractView references

The recent technological enhancements in the field of large language models and their integration into collaborative processes, for example, as chatbots, are perceived as key drivers for further transformations of work. However, the transformative effects of these technological enhancements have to be more thoroughly investigated in specific work contexts to benefit from the great potential of improvement. This research article provides findings of a case study research on how employees in software engineering perceive the collaboration with AI-powered chatbots, such as chatGPT. We investigate patterns employees develop to cope with the novel demands arising during the collaboration with these technologies and discuss our empirical findings regarding a conceptual framework of AI-related competences and another case study from a different industry. The findings contribute to a better understanding of human actors’ AI-related coping patterns as key prerequisites for a more responsible and sustainable usage of this technology in professional work contexts. © 2023, IFIP International Federation for Information Processing.
"
10.1109/RE57278.2023.00029,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174411487&origin=inward,Conference Paper,SCOPUS_ID:85174411487,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),leveraging natural language processing for a consistency checking toolchain of automotive requirements,"
AbstractView references

In the automotive industry, specifications often consist of a large number of textual requirements. These requirements are linguistically ambiguous and written in informal language. Utilizing Structured English for requirements eliminates ambiguity, improves data quality, and supports further automated processing while maintaining readability. The recent development of large language models enables a fully automated translation approach using few-shot learning. To deal with the limited context size of large language models, an improved algorithm, OptKATE, is presented to find an ideal set of requirements for few-shot learning. Structured English can be used as a basis for further formalization. This capability is key in creating an interface between natural language processing and verification, in our case, consistency analysis using the Z3 SMT solver. We implemented a grammar for translating Structured English into TCTL using the MontiCore workbench. Furthermore, since SMT-based methods currently rely on manual precondition satisfaction and do not tackle conflicting preconditions automatically, we propose a scenario generation algorithm that generates potential scenarios using the specification and checks the requirements against them. Through this approach, we can better identify and resolve conflicting preconditions, ultimately improving the consistency of requirements. Our toolchain is evaluated using an automotive requirements dataset provided by former Daimler AG. © 2023 IEEE.
"
10.1109/RE57278.2023.00026,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174402963&origin=inward,Conference Paper,SCOPUS_ID:85174402963,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rclassify: combining nlp and ml to classify rules from requirements specifications documents,"
AbstractView references

Typically, business applications have complex and extensive functionality. Often, rules are scattered throughout the application documentation and code, making it challenging to modify them. Because business rules change more frequently, it is preferable to externalize the rules and move them outside the application. The requirements specification can serve as a valuable source for extracting and classifying rules that can further aid in assigning the rules to the appropriate teams based on their areas of expertise and externalizing the rules in the implementation. We introduce RClassify, which extracts and classifies rules from requirements specifications spread across several NL documents into eight different business rule classes. RClassify combines NLP and ML-based classification approaches to improve classification accuracy. We discuss the implementation of this approach in three real-world large-size and complex products, demonstrating its effectiveness, experience, and lessons learned. While the findings are presented in the specific context of three industry products, we believe that researchers, practitioners, and tool vendors will find the insights and experiences gained from this approach applicable in other contexts. © 2023 IEEE.
"
10.1109/RE57278.2023.00025,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174402797&origin=inward,Conference Paper,SCOPUS_ID:85174402797,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a transformer-based approach for abstractive summarization of requirements from obligations in software engineering contracts,"
AbstractView references

Software Engineering (SE) contracts are a valuable source of software requirements. Seed requirements derived from SE contracts can provide a starting point to the Requirements Engineering (RE) phase. To extract such a seed however, a correct interpretation of contracts text is crucial. A major challenge with contracts text interpretation is that the text is lengthy, convoluted, and it incorporates a complex Legalese. If a summary of the high-level requirements from obligations present in SE contracts is available to the requirement analysts in a language that is comprehensible to them, they can use this seed requirements knowledge to ask the right questions to the stakeholders. In this paper, we propose an approach for summarizing the requirements present in obligations in a language comprehensible to requirement analysts. We use the principles of Prompt Engineering to prompt GPT-3 to generate summaries for training Natural Language Generation (NLG) models for generating SE-specific summaries. Experiments using NLG models such as BART, GPT-2, T5, and Pegasus indicate that Pegasus generates the most accurate summaries with the highest ROUGE score as compared to other models. © 2023 IEEE.
"
10.1109/RE57278.2023.00028,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174399339&origin=inward,Conference Paper,SCOPUS_ID:85174399339,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),requirements classification for smart allocation: a case study in the railway industry,"
AbstractView references

Allocation of requirements to different teams is a typical preliminary task in large-scale system development projects. This critical activity is often performed manually and can benefit from automated requirements classification techniques. To date, limited evidence is available about the effectiveness of existing machine learning (ML) approaches for requirements classification in industrial cases. This paper aims to fill this gap by evaluating state-of-the-art language models and ML algorithms for classification in the railway industry. Since the interpretation of the results of ML systems is particularly relevant in the studied context, we also provide an information augmentation approach to complement the output of the ML-based classification. Our results show that the BERT uncased language model with the softmax classifier can allocate the requirements to different teams with a 76% F1 score when considering requirements allocation to the most frequent teams. Information augmentation provides potentially useful indications in 76% of the cases. The results confirm that currently available techniques can be applied to real-world cases, thus enabling the first step for technology transfer of automated requirements classification. The study can be useful to practitioners operating in requirements-centered contexts such as railways, where accurate requirements classification becomes crucial for better allocation of requirements to various teams. © 2023 IEEE.
"
10.1109/MLCAD58807.2023.10299852,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174351190&origin=inward,Conference Paper,SCOPUS_ID:85174351190,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chateda: a large language model powered autonomous agent for eda,"
AbstractView references

The integration of a complex set of Electronic Design Automation (EDA) tools to enhance interoperability is a critical concern for circuit designers. Recent advancements in large language models (LLMs) have showcased their exceptional capabilities in natural language processing and comprehension, offering a novel approach to interfacing with EDA tools. This research paper introduces ChatEDA, an autonomous agent for EDA empowered by a large language model, AutoMage, complemented by EDA tools serving as executors. ChatEDA streamlines the design flow from the Register-Transfer Level (RTL) to the Graphic Data System Version II (GDSII) by effectively managing task planning, script generation, and task execution. Through comprehensive experimental evaluations, ChatEDA has demonstrated its proficiency in handling diverse requirements, and our fine-tuned AutoMage model has exhibited superior performance compared to GPT-4 and other similar LLMs. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174317916&origin=inward,Conference Paper,SCOPUS_ID:85174317916,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the economic trade-offs of large language models: a case study,"
AbstractView references

Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifcations. Large Language Models (LLMs) are a natural ft for this use case; however, their effcacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and impact of LLMs for the enterprise as a function of the usefulness of the responses that they generate. We present a cost framework for evaluating an NLP model's utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product. We compare three strategies for specializing an LLM - prompt engineering, fne-tuning, and knowledge distillation - using feedback from the brand's customer service agents. We fnd that the usability of a model's responses can make up for a large difference in inference cost for our case study brand, and we extrapolate our fndings to the broader enterprise space. © ACL 2023.All rights reserved.
"
10.48009/2_iis_2023_111,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174247261&origin=inward,Article,SCOPUS_ID:85174247261,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),teaching ai in the college course: introducing the ai prompt development life cycle (pdlc),"
AbstractView references

This paper discusses how the public emergence of generative AI is compelling those in academia to rethink teaching and learning and to consider the impact AI has on higher education. The decision as to when to use generative AI in a college course, and when not to, is now a significant component in the development of student assessment and activities. Obviously, there are academic activities that require students to engage in original work. In those instances, generative AI is not appropriate, and faculty will need to consider how to avoid the tool. In those instances where generative AI is encouraged, teaching students to use the tools available to them effectively is necessary. For productive generative AI prompts, awareness of the thought process related to developing prompts, and techniques for writing prompts, is essential. This paper introduces the prompt development life cycle (PDLC) which provides a framework to introduce students to the cognitive aspects of writing a prompt and some basic techniques that can enhance their prompt development skills. Activities to assist in developing the PDLC mindset are also included. © 2023 International Association for Computer Information Systems.
"
10.1080/10447318.2023.2269006,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174230751&origin=inward,Article,SCOPUS_ID:85174230751,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt in the classroom. exploring its potential and limitations in a functional programming course,"
AbstractView references

In November 2022, OpenAI has introduced ChatGPT–a chatbot based on supervised and reinforcement learning. Not only can it answer questions emulating human-like responses, but it can also generate code from scratch or complete coding templates provided by the user. ChatGPT can generate unique responses which render any traditional anti-plagiarism tool useless. Its release has ignited a heated debate about its usage in academia, especially by students. We have found, to our surprise, that our students at POLITEHNICA University of Bucharest (UPB) have been using generative AI tools (ChatGPT and its predecessors) for solving homework, for at least 6 months. We therefore set out to explore the capabilities of ChatGPT and assess its value for educational purposes. We used ChatGPT to solve all our coding assignments for the semester from our UPB Functional Programming course. We discovered that, although ChatGPT provides correct answers in 68% of the cases, only around half of those are legible solutions which can benefit students in some form. On the other hand, ChatGPT has a very good ability to perform code review on student programming homework. Based on these findings, we discuss the pros and cons of ChatGPT in a teaching environment, as well as means for integrating GPT models for generating code reviews, in order to improve the code-writing skills of students. © 2023 Taylor & Francis Group, LLC.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174198026&origin=inward,Conference Paper,SCOPUS_ID:85174198026,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),contextualizing problems to student interests at scale in intelligent tutoring system using large language models,"
AbstractView references

Contextualizing problems to align with student interests can significantly improve learning outcomes. However, this task often presents scalability challenges due to resource and time constraints. Recent advancements in Large Language Models (LLMs) like GPT-4 [1]offer potential solutions to these issues. This study explores the ability of GPT-4 in the contextualization of problems within CTAT [2], an authoring tool for the intelligent tutoring system, aiming to increase student engagement and enhance learning outcomes. Through iterative prompt engineering, we achieved meaningful contextualization that preserved the difficulty and original intent of the problem, thereby not altering values or overcomplicating the questions. To evaluate the effectiveness of these newly generated questions, we conducted focus groups and interviews with instructional designers. The positive assessment from the instructional designers signifies that these questions are suitable for implementation, potentially saving significant time spent on manual contextualization. Despite these promising findings, we acknowledge current limitations of our research, particularly with geometry problems, and emphasize the need for ongoing evaluation and research. Future work includes systematic studies to measure the impact of this tool on students' learning outcomes and enhancements to handle a broader range of problems. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
10.3389/fmolb.2023.1249247,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174183580&origin=inward,Article,SCOPUS_ID:85174183580,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai/ml combined with next-generation sequencing of vhh immune repertoires enables the rapid identification of de novo humanized and sequence-optimized single domain antibodies: a prospective case study,"
AbstractView references

Introduction: In this study, we demonstrate the feasibility of yeast surface display (YSD) and nextgeneration sequencing (NGS) in combination with artificial intelligence and machine learning methods (AI/ML) for the identification of de novo humanized single domain antibodies (sdAbs) with favorable early developability profiles. Methods: The display library was derived from a novel approach, in which VHH-based CDR3 regions obtained from a llama (Lama glama), immunized against NKp46, were grafted onto a humanized VHH backbone library that was diversified in CDR1 and CDR2. Following NGS analysis of sequence pools from two rounds of fluorescence-activated cell sorting we focused on four sequence clusters based on NGS frequency and enrichment analysis as well as in silico developability assessment. For each cluster, long short-term memory (LSTM) based deep generative models were trained and used for the in silico sampling of new sequences. Sequences were subjected to sequence- and structure-based in silico developability assessment to select a set of less than 10 sequences per cluster for production. Results: As demonstrated by binding kinetics and early developability assessment, this procedure represents a general strategy for the rapid and efficient design of potent and automatically humanized sdAb hits from screening selections with favorable early developability profiles. Copyright © 2023 Arras, Yoo, Pekar, Clarke, Friedrich, Schröter, Schanz, Tonillo, Siegmund, Doerner, Krah, Guarnera, Zielonka and Evers.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174176806&origin=inward,Conference Paper,SCOPUS_ID:85174176806,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),detecting chatgpt-generated code in a cs1 course,"
AbstractView references

The emergence of ChatGPT has raised concerns about students potentially using it for cheating. Computer Science (CS) educators are becoming worried because of the potential short and long-term adverse effects it might have on students. However, it is unclear to what extent ChatGPT-generated code can be distinguished from student-written code in introductory programming courses. In this work, we analyze how well student-written and ChatGPT-generated code can be automatically distinguished. We use an openly available dataset of student program solutions for CS1 assignments and have ChatGPT generate code for the same assignments. We evaluate the performance of both traditional machine learning models, such as SVM and XGBoost, as well as Abstract Syntax Tree-based deep learning models, such as code2vec and ASTNN, in distinguishing between student and ChatGPT code. The results suggest that both traditional machine learning models and AST-based deep learning models can be very effective in detecting whether a student or ChatGPT wrote the code in an educational setting with accuracies higher than 90%. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174145870&origin=inward,Conference Paper,SCOPUS_ID:85174145870,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rewriting math word problems with large language models,"
AbstractView references

Large Language Models have recently achieved high performance on many writing tasks. In a recent study, math word problems in Carnegie Learning's MATHia adaptive learning software were rewritten by human authors to improve their clarity and specificity. The randomized experiment found that emerging readers who received the rewritten word problems spent less time completing the problems and also achieved higher mastery compared to emerging readers who received the original content. We used GPT-4 to rewrite the same set of math word problems, prompting it to follow the same guidelines that the human authors followed. We lay out our prompt engineering process, comparing several prompting strategies: zero-shot, few-shot, and chain-of-thought prompting. Additionally, we overview how we leveraged GPT's ability to write python code in order to encode mathematical components of word problems. We report text analysis of the original, human-rewritten, and GPT-rewritten problems. GPT rewrites had the most optimal readability, lexical diversity, and cohesion scores but used more low frequency words. We present our plan to test the GPT outputs in upcoming randomized field trials in MATHia. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
10.1109/ICWS60048.2023.00103,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173859886&origin=inward,Conference Paper,SCOPUS_ID:85173859886,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"empowering generative ai with knowledge base 4.0: towards linking analytical, cognitive, and generative intelligence","
AbstractView references

Intelligence refers to the ability to acquire and apply knowledge and skills, which comprises three fundamental components, namely knowledge, experience, and creativity. Consequently, there exist three primary Artificial Intelligence (AI) systems, namely Analytical AI, Cognitive AI, and Generative AI. Analytical AI is primarily concerned with comprehending the data and transforming it into contextualized data and knowledge. On the other hand, Cognitive AI is centered on understanding experience and aims to annotate, enrich, and utilize the knowledge, to facilitate decision-making. Lastly, Generative AI delves into the neural mechanisms involved in creative thinking and problem-solving, with a focus on enhancing the process of acquiring and applying knowledge and skills. This paper presents Knowledge Base 4.0 as the backend data for AI engines, which allows for linking knowledge and experience to enable empowering generative AI. The objective is not only to facilitate generating new content (such as text and images) but also to generate new processes when/if needed. We present the architecture of Knowledge Base 4.0 and the design and development of data services that construct and maintain this robust Knowledge Base. Additionally, we provide use cases in various domains, including health, policing, banking, and education. © 2023 IEEE.
"
10.1016/j.caeai.2023.100172,S2666920X23000516,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173847244&origin=inward,Article,SCOPUS_ID:85173847244,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"students’ use of large language models in engineering education: a case study on technology acceptance, perceptions, efficacy, and detection chances","The accessibility of advanced Artificial Intelligence-based tools, like ChatGPT, has made Large Language Models (LLMs) readily available to students. These LLMs can generate original written content to assist students in their academic assessments. With the rapid adoption of LLMs, exemplified by the popularity of OpenAI's ChatGPT, there is a growing need to explore their application in education. Few studies examine students' use of LLMs as learning tools. This paper focuses on the application of ChatGPT in engineering higher education through an in-depth case study. It investigates whether engineering students can generate high-quality university essays with LLMs assistance, whether existing LLMs identification systems can detect essays produced with LLMs, and how students perceive the usefulness and acceptance of LLMs in learning. The research adopts a deductive/inductive approach, combining conceptualization and empirical evidence analysis. The study involves mechanical and management engineering students, who compose essays using LLMs. The essay assessment showed good results, but some recommendations emerged for teachers and students. Thirteen LLMs detectors were tested without achieving satisfactory results, suggesting to avoid LLMs ban. In addition, students were administered a questionnaire based on constructs and items that follow the technology acceptance models available in the literature. The results contribute to qualitative evidence by highlighting possible future research and educational practices."
10.2196/49949,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173804244&origin=inward,Article,SCOPUS_ID:85173804244,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),democratizing artificial intelligence imaging analysis with automated machine learning: tutorial,"
AbstractView references

Deep learning–based clinical imaging analysis underlies diagnostic artificial intelligence (AI) models, which can match or even exceed the performance of clinical experts, having the potential to revolutionize clinical practice. A wide variety of automated machine learning (autoML) platforms lower the technical barrier to entry to deep learning, extending AI capabilities to clinicians with limited technical expertise, and even autonomous foundation models such as multimodal large language models. Here, we provide a technical overview of autoML with descriptions of how autoML may be applied in education, research, and clinical practice. Each stage of the process of conducting an autoML project is outlined, with an emphasis on ethical and technical best practices. Specifically, data acquisition, data partitioning, model training, model validation, analysis, and model deployment are considered. The strengths and limitations of available code-free, code-minimal, and code-intensive autoML platforms are considered. AutoML has great potential to democratize AI in medicine, improving AI literacy by enabling “hands-on” education. AutoML may serve as a useful adjunct in research by facilitating rapid testing and benchmarking before significant computational resources are committed. AutoML may also be applied in clinical contexts, provided regulatory requirements are met. The abstraction by autoML of arduous aspects of AI engineering promotes prioritization of data set curation, supporting the transition from conventional model-driven approaches to data-centric development. To fulfill its potential, clinicians must be educated on how to apply these technologies ethically, rigorously, and effectively; this tutorial represents a comprehensive summary of relevant considerations. ©Arun James Thirunavukarasu, Kabilan Elangovan, Laura Gutierrez, Yong Li, Iris Tan, Pearse A Keane, Edward Korot, Daniel Shu Wei Ting. Originally published in the Journal of Medical Internet Research.
"
10.1109/ICAISS58487.2023.10250700,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173627170&origin=inward,Conference Paper,SCOPUS_ID:85173627170,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative ai-powered spark cluster recommendation engine,"
AbstractView references

Apache Spark is a data processing framework that performs complex processing tasks on very large data sets very smoothly. It distributes data processing tasks across multiple compute instances, either on its own or in tandem with other distributed computing tools. With the increasing amount of data and evolution of ML models, the need to accomplish complex feature engineering/pre-processing, Model training quickly has become optimum. As compared to a single compute instance, a cluster of compute instances shows a remarkable performance boost to achieve faster data processing. Since a cluster is a combination of multiple compute instances (Worker Nodes) managed by a Master Node, the total cost to utilize such cluster configuration is very high. But it is seen that there is significant cost reduction as these clusters like (Databricks, and EMR) are governed by the Cloud platform, there are the options of paying as you use. Also depending on workload, the cluster can be designed in such a way that it provides the best performance at lower costs. But this process is manual and needs sufficient technical skills and experience to design a cluster. In the presented approach, automation of the cluster selection process is demonstrated, by developing a GEN-AI-based recommendation engine. This work presents Conditional generative adversarial networks (cGANs) which are used to produce more samples from the joint distribution of sparse custom training data. Depending upon the data workload, expected usage time, and budget presented recommendation engine suggests instance type for master and worker nodes along with the number of worker nodes needed. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173570567&origin=inward,Conference Paper,SCOPUS_ID:85173570567,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),app2check at emit: large language models for multilabel emotion classification,"
AbstractView references

In this paper we compare the performance of three state-of-the-art LLM-based approaches for multilabel emotion classification: fine-tuned multilingual T5 and two few shot prompting approaches: plain FLAN and ChatGPT. In our experimental analysis we show that FLAN T5 is the worst performer and our fine-tuned MT5 is the best performer in our dev set and, overall, is better than ChatGPT3.5 on the test set of the competition. Moreover, we show that MT5 and ChatGPT3.5 have complementary performance on different emotions and that A2C-best, our unsubmitted system that combines our best performer models for each emotion, has a macro F1 that is 0.02 greater than the winner of the competition in the out-of-domain benchmark. Finally, we suggest that a perspectivist approach is more suitable for evaluating systems on emotion detection. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173567977&origin=inward,Conference Paper,SCOPUS_ID:85173567977,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),taysir competition: transformer+rnn: algorithms to yield simple and interpretable representations,"
AbstractView references

This article presents the content of the competition Transformers+RNN: Algorithms to Yield Simple and Interpretable Representations (TAYSIR, the Arabic word for ‘simple’), which was an on-line challenge on extracting simpler models from already trained neural networks held in Spring 2023. These neural nets were trained on sequential categorial/symbolic data. Some of these data were artificial, some came from real world problems (such as Natural Language Processing, Bioinformatics, and Software Engineering). The trained models covered a large spectrum of architectures, from Simple Recurrent Neural Network (SRN) to Transformers, including Gated Recurrent Unit (GRU) and Long Short Term Memory (LSTM). No constraint was given on the surrogate models submitted by the participants: any model working on sequential data was accepted. Two tracks were proposed: neural networks trained on Binary Classification tasks, and on Language Modeling tasks. The evaluation of the surrogate models took into account both the simplicity of the extracted model and the quality of the approximation of the original model. © 2023 R. Eyraud, D. Lambert, B. Tahri Joutei, A. Gaffarov, M. Cabanne, J. Heinz & C. Shibata.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173516590&origin=inward,Conference Paper,SCOPUS_ID:85173516590,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deeppavlov dream: platform for building generative ai assistants,"
AbstractView references

An open-source DeepPavlov Dream Platform is specifically tailored for development of complex dialog systems like Generative AI Assistants. The stack prioritizes efficiency, modularity, scalability, and extensibility with the goal to make it easier to develop complex dialog systems from scratch. It supports modular approach to implementation of conversational agents enabling their development through the choice of NLP components and conversational skills from a rich library organized into the distributions of ready-for-use multi-skill AI assistant systems. In DeepPavlov Dream, multiskill Generative AI Assistant consists of NLP components that extract features from user utterances, conversational skills that generate or retrieve a response, skill and response selectors that facilitate choice of relevant skills and the best response, as well as a conversational orchestrator that enables creation of multi-skill Generative AI Assistants scalable up to industrial grade AI assistants. The platform allows to integrate large language models into dialog pipeline, customize with prompt engineering, handle multiple prompts during the same dialog session and create simple multimodal assistants. © ACL-DEMO 2023. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173179563&origin=inward,Conference Paper,SCOPUS_ID:85173179563,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),counterfactual active learning for out-of-distribution generalization,"
AbstractView references

We study the out-of-distribution generalization of active learning that adaptively selects samples for annotation in learning the decision boundary of classification. Our empirical study finds that increasingly annotating seen samples may hardly benefit the generalization. To address the problem, we propose Counterfactual Active Learning (CounterAL) that empowers active learning with counterfactual thinking to bridge the seen samples with unseen cases. In addition to annotating factual samples, CounterAL requires annotators to answer counterfactual questions to construct counterfactual samples for training. To achieve CounterAL, we design a new acquisition strategy that selects the informative factual-counterfactual pairs for annotation; and a new training strategy that pushes the model update to focus on the discrepancy between factual and counterfactual samples. We evaluate CounterAL on multiple public datasets of sentiment analysis and natural language inference. The experiment results show that CounterAL requires fewer acquisition rounds and outperforms existing active learning methods by a large margin in OOD tests with comparable IID performance. © 2023 Association for Computational Linguistics.
"
10.1007/978-3-031-35897-5_23,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173016253&origin=inward,Conference Paper,SCOPUS_ID:85173016253,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),designing ai writing workflow ux for reduced cognitive loads,"
AbstractView references

This paper explores how Large-Language Model Artificial Intelligences (LLM-AIs) can be used to support people with Attention Deficit Hyperactivity Disorder (ADHD), Autism Spectrum Disorder (ASD), and other learning differences which effect cognition and self-regulation. It examines the cognitive load associated with complex writing tasks and how it affects users who have trouble with high-order thinking and planning. OpenAI’s GPT-3 API is used to analyze how AI can help with complex language-based tasks. The paper first reflects on how GPT-3 uses natural language processing to generate text, translate, summarize, answer questions, and caption images, as well as how it adapts to respond to different situations and tasks to accurately classify them. Bloom’s Taxonomy and SOLO Taxonomy are highlighted as language-driven methods of assessing learner understanding and to design activities and assessments that encourage high-order thinking. Literature is reviewed which suggests that students with disorders which effect executive functions need extra help with their writing skills to do well in school, and that early and accessible interventions such as digital self-management tools already help these learners. A model of executive-cognitive capacity is proposed to assess how best to manage the cognition of tasks and workloads, and to support a design matrix for assistive tools and processes. Finally, the Social Cognitive Theory (SCT) model for writing is evaluated for use as a procedural high-order writing process by which the tools can be designed and against which their efficacy can be validated. This review illustrates a universal design method for the development and evaluation of future AI writing tools for all users, with specific consideration towards users with atypical cognitive and sensory processing needs. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.2196/50638,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173015974&origin=inward,Article,SCOPUS_ID:85173015974,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompt engineering as an important emerging skill for medical professionals: tutorial,"
AbstractView references

Prompt engineering is a relatively new field of research that refers to the practice of designing, refining, and implementing prompts or instructions that guide the output of large language models (LLMs) to help in various tasks. With the emergence of LLMs, the most popular one being ChatGPT that has attracted the attention of over a 100 million users in only 2 months, artificial intelligence (AI), especially generative AI, has become accessible for the masses. This is an unprecedented paradigm shift not only because of the use of AI becoming more widespread but also due to the possible implications of LLMs in health care. As more patients and medical professionals use AI-based tools, LLMs being the most popular representatives of that group, it seems inevitable to address the challenge to improve this skill. This paper summarizes the current state of research about prompt engineering and, at the same time, aims at providing practical recommendations for the wide range of health care professionals to improve their interactions with LLMs. ©Bertalan Meskó.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173004364&origin=inward,Conference Paper,SCOPUS_ID:85173004364,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),employing artificial intelligence to increase occupational tacit-knowledge through competency-based experiential learning,"
AbstractView references

This paper will discuss a technology and methodology being researched for an US Army project called the Synthetic Training Environment Experiential Learning for Readiness (STEEL-R) that is producing a new training-data collection and employment strategy, and an experiential learning model [1]. The model is designed to address a need to more rapidly develop increased occupational hard and soft skill competencies that increases advantages over near-peer competitors, and with a higher probability of transfer when applied in a real occupational setting. To do this not only are explicit-technology knowledge and skills need to be trained but also implicit-tacit knowledge and skill required by teams and personnel; however, current training methods are incapable of producing both. By saving, reducing, and labeling real occupational contextual mission, task and performance data, as well as after-action event reports, assessments, communications, and testimonies (i.e., experiences), we intend to then use a generative artificial intelligence (AI) model to not only enforce technical ability but essentially transfer experiences, and associated tacit knowledge, from existing occupational experts, to new generations of occupational workers. The generative AI model, along with new occupational experience design tools, and a syntactic, machine-readable form of experiential content design, will be discussed, as they apply to a competency-based experiential learning model, that together with the use of adaptive learning technology, we hope will simplify and enable any instructor/trainer to employ experiential learning in their training curriculum. © 2023 Copyright for this paper by its authors.
"
10.1080/23735082.2023.2258886,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172808547&origin=inward,Article,SCOPUS_ID:85172808547,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"supporting self-directed learning and self-assessment using teachergaia, a generative ai chatbot application: learning approaches and prompt engineering","
AbstractView references

Self-directed learning and self-assessment require student responsibility over learning needs, goals, processes, and outcomes. However, this student-led learning can be challenging to achieve in a classroom limited by a one-to-many teacher-led instruction. We, thus, have designed and prototyped a generative artificial intelligence chatbot application (GAIA), named TeacherGAIA, that can be used to asynchronously support students in their self-directed learning and self-assessment outside the classroom. We first identified diverse constructivist learning approaches that align with, and promote, student-led learning. These included knowledge construction, inquiry-based learning, self-assessment, and peer teaching. The in-context learning abilities of large language model (LLM) from OpenAI were then leveraged via prompt engineering to steer interactions supporting these different learning approaches. These interactions contrasted with ChatGPT, OpenAI’s chatbot which by default engaged in the traditional transmissionist mode of learning reminiscent of teacher-led instruction. Preliminary design, prompt engineering and prototyping suggested fidelity to the learning approaches, cognitive guidance, and social-emotional support, all of which were implemented in a generative AI manner without pre-specified rules or “hard-coding”. Other affordances of TeacherGAIA are discussed and future development outlined. We anticipate TeacherGAIA to be a useful application for teachers in facilitating self-directed learning and self-assessment among K-12 students. © 2023 Informa UK Limited, trading as Taylor & Francis Group.
"
10.1109/ICMEW59549.2023.00025,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172410213&origin=inward,Conference Paper,SCOPUS_ID:85172410213,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cheap-fake detection with llm using prompt engineering,"
AbstractView references

The misuse of real photographs with conflicting image captions in news items is an example of the out-of-context (OOC) misuse of media. In order to detect OOC media, individuals must determine the accuracy of the statement and evaluate whether the triplet (i.e., the image and two captions) relates to the same event. This paper presents a novel learnable approach for detecting OOC media in ICME'23 Grand Challenge on Detecting Cheapfakes. The proposed method is based on the COSMOS structure, which assesses the coherence between an image and captions, as well as between two captions. We enhance the baseline algorithm by incorporating a Large Language Model (LLM), GPT3.5, as a feature extractor. Specifically, we propose an innovative approach to feature extraction utilizing prompt engineering to develop a robust and reliable feature extractor with GPT3.5 model. The proposed method captures the correlation between two captions and effectively integrates this module into the COSMOS baseline model, which allows for a deeper understanding of the relationship between captions. By incorporating this module, we demonstrate the potential for significant improvements in cheap-fakes detection performance. The proposed methodology holds promising implications for various applications such as natural language processing, image captioning, and text-to-image synthesis. Docker for submission is available at https://hub.docker.com/repository/docker/mulns/acmmmcheapfakes. © 2023 IEEE.
"
10.1007/978-3-031-42532-5_11,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172309949&origin=inward,Conference Paper,SCOPUS_ID:85172309949,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),navigating the knowledge network: how inter-domain information pairing and generative ai can enable rapid problem-solving,"
AbstractView references

This study introduces a novel methodological framework that leverages generative AI to retrieve scientific articles pertinent to engineering problems, framed within the context of TRIZ-based contradictions. The process entails searching scientific literature databases by keywords and subsequently prioritizing the resulting articles based on their pertinence to the research subject. Large Language Models are then employed to analyze a refined selection of articles, extracting features and amalgamating individual findings into a summary. Furthermore, we present a strategy towards inter-domain information search. The presented strategy has the potential to be generalized and applied to various domains, facilitating knowledge transfer and problem-solving across different fields. © 2023, IFIP International Federation for Information Processing.
"
10.1007/978-3-031-37703-7_18,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172279049&origin=inward,Conference Paper,SCOPUS_ID:85172279049,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),nl2spec: interactively translating unstructured natural language to temporal logics with large language models,"
AbstractView references

A rigorous formalization of desired system requirements is indispensable when performing any verification task. This often limits the application of verification techniques, as writing formal specifications is an error-prone and time-consuming manual task. To facilitate this, we present nl2spec, a framework for applying Large Language Models (LLMs) to derive formal specifications (in temporal logics) from unstructured natural language. In particular, we introduce a new methodology to detect and resolve the inherent ambiguity of system requirements in natural language: we utilize LLMs to map subformulas of the formalization back to the corresponding natural language fragments of the input. Users iteratively add, delete, and edit these sub-translations to amend erroneous formalizations, which is easier than manually redrafting the entire formalization. The framework is agnostic to specific application domains and can be extended to similar specification languages and new neural models. We perform a user study to obtain a challenging dataset, which we use to run experiments on the quality of translations. We provide an open-source implementation, including a web-based frontend. © 2023, The Author(s).
"
10.1007/978-3-031-40725-3_56,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172237038&origin=inward,Conference Paper,SCOPUS_ID:85172237038,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the problem of concept learning and goals of reasoning in large language models,"
AbstractView references

Modern large language models (LLMs) show good performance in the zero-shot or few-shot learning. This ability ability is a significant result even on tasks for which the models have not been trained is in part due to the fact that by learning from textual internet-scale data, such models build a semblance of a model of the world. However, the question of whether the entities on which that the model operates are concepts in the psychological sense remains open. Relying on conceptual reasoning schemes allows to increase the safety of models in solving complex problems. To address this question, we propose to use standard psychodiagnostic techniques to assess the quality of conceptual thinking of models. We test this hypothesis, by conducting experiments on a dataset adapted for LLMs from the psychological techniques of Kettel and Rubinstein and comparing the effectiveness of each of them. In this paper, we have shown that it is possible to distinguish several types of model errors in incorrect answers to standard tasks on conceptual thinking and to evaluate the type according to the classifications of distortions of conceptual thinking adopted in cultural and historical approaches in psychology. This makes it possible to use the tool of psychodiagnostic techniques not only to evaluate the effectiveness of models, but also to develop training procedures based on such tasks. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1109/ISIE51358.2023.10228101,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172069602&origin=inward,Conference Paper,SCOPUS_ID:85172069602,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),augmenting industrial chatbots in energy systems using chatgpt generative ai,"
AbstractView references

Chatbots, the automation of communicative labor, have been widely deployed in industrial applications and systems. Built upon the Generative Pre-trained Transformer 3 (GPT-3), ChatGPT is a Generative Artificial Intelligence (AI) primed to transform all pre-existing chatbot capabilities with human-like conversation skills. It has already disrupted many disciplines including tertiary education and academic research methods, with increasing adoption in simple to complex tasks. However, the augmentation of pre-existing industrial chatbots with generative AI capabilities has not been fully investigated and demonstrated in recent literature. In this paper, we address this gap by presenting the augmentation of a pre-existing chatbot using ChatGPT generative AI capabilities. Our contribution encompasses the ten primary human-like conversation capabilities of ChatGPT, its augmentation of the pre-existing functionalities and the adopted prompt engineering strategies. Each capability is empirically demonstrated on Cooee, a functionally deployed chatbot in the microgrid energy systems of the La Trobe Energy Analytics Platform (LEAP). © 2023 IEEE.
"
10.1007/978-3-031-40436-8_12,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172027299&origin=inward,Book Chapter,SCOPUS_ID:85172027299,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),time: it is only logical!,"
AbstractView references

Logical Clocks play an important role for the design and modelling of concurrent systems. The Clock Constraint Specification Language (ccsl) was built in 2009, as part of an annex of the UML Profile for MARTE, to give a proper syntax to handle logical clocks as first class citizens. The syntax gave rise to a series of different semantic interpretations along with various verification tools. Usecases are diverse and include languages to express timing requirements, temporal or spatio-temporal logics to capture expected safety properties, meta-languages to give an operational semantics to domain-specific languages. The application domains include avionics, safety-critical transportation systems, self-driving vehicles, systems engineering models, cyber-physical systems. This paper reviews the effort conducted since 2009 on ccsl. A large part of this effort was made possible by Professor He Jifeng and his will to build in Shanghai a research centre of excellence for trustworthy systems. Researchers there found inspiration in the heritage left by the different schools working around the world on concurrency theory, including the school of synchronous languages from which ccsl has emerged. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1109/ICECCS59891.2023.00029,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172020333&origin=inward,Conference Paper,SCOPUS_ID:85172020333,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated compositional verification for robotic state machines using isabelle/hol,"
AbstractView references

RoboChart is a graphical language for model-based engineering of robotic systems, in the style of UML and SysML. It contains notations for data structures, system architecture, and the behaviour of individual robotic controllers using state machines. Crucially, RoboChart has a formal semantics in the CSP process algebra, which provides a precise foundation for software engineering and formal verification using model checking. However, due to state explosion, the application of model checking does not scale. In this paper, we contribute a compositional verification technique that uses Isabelle/HOL RoboChart state machines symbolically. Our technique uses state invariants to capture safety requirements over a very large or infinite state, similar to the B method, and is highly automated using Isabelle's sledgehammer tool. We give a model transformation from the RoboTool development environment to Isabelle/HOL and apply this to several verification case studies. © 2023 IEEE.
"
10.1007/978-3-031-42682-7_27,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172010223&origin=inward,Conference Paper,SCOPUS_ID:85172010223,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),understanding peer feedback contributions using natural language processing,"
AbstractView references

Peer feedback has been widely used in computer-supported collaborative learning (CSCL) setting to improve students’ engagement with massive courses. Although the peer feedback process increases students’ self-regulatory practice, metacognition, and academic achievement, instructors need to go through large amounts of feedback text data which is much more time-consuming. To address this challenge, the present study proposes an automated content analysis approach to identify relevant categories in peer feedback based on traditional and sequence-based classifiers using TF-IDF and content-independent features. We use a data set from an extensive course (N = 231 students) in the setting of engineering higher education. In particular, a total of 2,444 peer feedback messages were analyzed. The CRF classification model based on the TF-IDF features achieved the best performance. The results illustrate that the ability to scale up the automatic analysis of peer feedback provides new opportunities for student-improved learning and improved teacher support in higher education at scale. © 2023, The Author(s).
"
10.1109/EDM58354.2023.10225079,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171979477&origin=inward,Conference Paper,SCOPUS_ID:85171979477,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"prospects for artificial intelligence technologies, neural networks and computer systems within the development of linguistics","
AbstractView references

This article describes the prospects of using artificial intelligence technologies, neural networks and computer systems for linguistic purposes. Systems and tools capable of analyzing linguistic units or generating texts of different composition and structure are listed. Morphological analysis systems such as Morfessor, Stanford Morphological Analyzer (SMA) and MyStem are disclosed. The features of the AOT (Automatic text processing) parser analyzer are discussed in detail. The scope of corpus analysis systems (AntConc) is given. A lot of attention is paid to NLP (Natural Language Processing) systems, capable of translating text into different languages (including gestures and sign languages), decoding written or typed fragments into a voice format, extracting key information from a text message, analyzing mood and emotions of the author of the statement based on verbal factors, interacting with users in a chat (based on NLP algorithms for understanding and answering user requests) and even modeling natural language, including syntax, grammar and semantics. In addition, the authors of the article propose a number of solutions suitable for eliminating some of the difficulties and problems in the use of neural networks and artificial intelligence systems. © 2023 IEEE.
"
10.1109/ICSSE58758.2023.10227188,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171887604&origin=inward,Conference Paper,SCOPUS_ID:85171887604,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt impacts on academia,"
AbstractView references

This research endeavors to examine the impact of Chat Generative Pre-trained Transformer (ChatGPT) on the education system, specifically in the realm of academia and the challenges that it poses. The incorporation of ChatGPT into academic practices may necessitate a reevaluation of current assessment and evaluation systems. The integration of ChatGPT into the academic world raises important questions regarding the ethics of AI-generated authorship and the implications it has on the value of creative work. This new chatbot has the potential to revolutionize various fields, particularly education and creative works, including art, music, creative writing, and all areas of humanity subjects. The paper presents a qualitative analysis of the implications of ChatGPT on the education system and academic research domain. The future trajectory of this new technology is not unlike that of other previous technological innovations but AI-chatbot technology is expected to reshape the value of knowledge. This study aims to shed light on these pressing issues and present a possible compromise strategy for resolving them. © 2023 IEEE.
"
10.14778/3611479.3611527,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171843747&origin=inward,Conference Paper,SCOPUS_ID:85171843747,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),how large language models will disrupt data management,"
AbstractView references

Large language models (LLMs), such as GPT-4, are revolutionizing software’s ability to understand, process, and synthesize language. The authors of this paper believe that this advance in technology is significant enough to prompt introspection in the data management community, similar to previous technological disruptions such as the advents of the world wide web, cloud computing, and statistical machine learning. We argue that the disruptive influence that LLMs will have on data management will come from two angles. (1) A number of hard database problems, namely, entity resolution, schema matching, data discovery, and query synthesis, hit a ceiling of automation because the system does not fully understand the semantics of the underlying data. Based on large training corpora of natural language, structured data, and code, LLMs have an unprecedented ability to ground database tuples, schemas, and queries in real-world concepts. We will provide examples of how LLMs may completely change our approaches to these problems. (2) LLMs blur the line between predictive models and information retrieval systems with their ability to answer questions. We will present examples showing how large databases and information retrieval systems have complementary functionality. © 2023 VLDB Endowment.
"
10.1109/ICSE-SEIP58684.2023.00024,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171800000&origin=inward,Conference Paper,SCOPUS_ID:85171800000,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),doctomodel: automated authoring of models from diverse requirements specification documents,"
AbstractView references

Early stages of Software Development Life Cycle (SDLC) namely requirement elicitation and requirements analysis have remained document-centric in the industry for market-driven, complex, large-scale business applications and products. The documentation typically runs into hundreds of Natural Language (NL) text documents which requirements engineers need to sift looking for the relevant information and also maintain these documents in-sync over time - a time-consuming and error-prone activity. Much of this difficulty can be overcome if the information is available in a structured form that is amenable to automated processing. Purposive models offer a way out. However, for easy adoption by industry practitioners, these models must be populated from NL text documents in a largely automated manner. This task is characterized by high variability with several documents containing different information conforming to different structures and styles. As a result, purposive information extractors need to be developed for each project/ product. Moreover, being an open-ended space there is no upper bound on the information extractors that need to be developed. To overcome this difficulty, we propose a document structure agnostic and meta-model agnostic tool, DocToModel, for the automated authoring of models from NL text documents. It provides a pattern mapping language to specify a mapping of structured and unstructured document information to meta-model elements, and a pattern interpreter to automate model authoring. The configurable and extensible architecture of DocToModel makes it generic and amenable to easy repurposing for other NL documents. This paper, describes the approach and illustrates its utility and efficacy on multiple real-world case studies. © 2023 IEEE.
"
10.1007/978-3-031-35946-0_16,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171478137&origin=inward,Conference Paper,SCOPUS_ID:85171478137,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),immersion and intersectionality - virtual reality in cross cultural art exhibition courses,"
AbstractView references

With Virtual Reality (VR) and Augmented Reality (AR) technologies are becoming a trend in digital innovation in education due to their great strategic potential and are of great importance for many studies and programs. As a result, VR has become an indispensable tool in various fields such as science, geography, art and culture. VR creates a unique cross-cultural environment that enables deeper learning, stimulates interest and offers a very high educational potential. Today, new curatorial approaches are emerging alongside digitally activated modes of presentation and dissemination, characterized by permanent reproducibility and the sinking of physical space. Digitalization and global sharing of educational resources has been a hot topic of concern for arts management teaching. Cross-cultural courses on art exhibitions using virtual reality technology enable the integration of teaching resources from different cultural backgrounds and language environments. Virtual simulation technology is truly applied to a wider range of exhibition planning and research, bringing more resources and practical opportunities for teaching and industry. Practical art curation training has limitations such as high teaching costs and high transportation risks. However, virtual art exhibition teaching can allow students from different cultural backgrounds and regions to break through the limitations of time and space and feel the context and cultural connotation of the exhibition more intuitively, which improves students’ cross-cultural thinking and discernment awareness and cross-cultural communication ability. In order to give students the opportunity to practice the tedious and highly specialized curatorial process visually. This study focuses on a virtual art exhibition experimental teaching system using VR as a carrier to provide students with opportunities to cognize theory and practice and to build a platform for effective practice. On the one hand, it improves students’ exhibition planning ability. On the other hand, it gradually forms an integrated platform to promote the continuous updating of cross-cultural teaching of art exhibitions. Through the system to accurately restore the exhibition space of domestic and foreign art institutions, a large number of artwork resources. It enables students to easily feel the exotic spaces and precious artworks that are out of their reach on a daily basis, which greatly increases the importance and interest of online learning during the epidemic. In the course of teaching virtual art exhibitions, students can further experience and understand spatial planning, artwork display, and other curatorial expertise while focusing on conceptual expression and spatial presentation. Methods: This experiment adopts a quantitative research method, taking students of a school in China as the research object, randomly selecting 30 students (15 male and 15 female) as the control group and 30 students (15 male and 15 female) as the experimental group, the basic background of the two groups is similar, and all of them have initially studied the art exhibition course and have preliminary knowledge of the exhibition. The control group adopts the traditional teaching mode, with the teacher teaching knowledge and operation demonstration, while the experimental group adopts virtual reality technology for teaching. A pre-test was given to the two groups of students before the teaching activity to ensure that there was no significant difference in the prior knowledge of the two groups. After the teaching, a post-test was conducted and a questionnaire survey on learning interest and learning confidence was organized. By This study can be found by comparing the test scores, teaching questionnaire results and teaching satisfaction questionnaire between the control group and the experimental group. Students in the experimental group had higher average test scores than those in the control group, and students in the experimental group had higher classroom concentration, learning initiative, and satisfaction with classroom teaching than students in the control group. Students in the control group indicated that the knowledge in the classroom was too boring and they did not pay attention in class, while students in the experimental group indicated that the classroom participation was high, the classroom was more interesting, and they hoped that more experimental courses could be taught in this way. Through the study, it was learned that virtual art exhibition teaching can enhance students’ knowledge and classroom motivation about art exhibitions. Conclusions: This study found that the traditional art curation teaching course is not able to fully meet the students’ learning needs. Hands-on courses require more hands-on experience for students, and the use of virtual reality technology combined with traditional teaching can solve this problem well and improve students’ learning efficiency. The application of virtual reality technology in art exhibition course teaching focuses on building real cross-cultural contexts for students and conducting comprehensive practical training, thus improving the interactivity and efficiency of experimental teaching in cross-cultural courses. In addition, there are some noteworthy issues that need to be noted, whether the novelty of virtual reality technology will affect the actual operation and how to solve this problem remains to be examined. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-35891-3_36,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171461539&origin=inward,Conference Paper,SCOPUS_ID:85171461539,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the feasibility study of ai image generator as shape convergent thinking tool,"
AbstractView references

This study aims to (1) investigate the feasibility of AI generation as a tool for shape convergent thinking, (2) compare the differences in computing style and user experience between the two software programs Midjourney (MJ) and Dall-E2 (D2), and (3) analyze and optimize the impact of the adjectives given to the AI on the generated results. In the experiment, six people (three male and three female) with design experi-ence were recruited to use an expert-designed word list to describe six different types of household items and to input them into the MJ and D2 software to generate product shapes. The results were compared with the original images and scored for similarity, acceptability on the System Usability Scale (SUS) and one-to-one semi-structured interviews. According to the analysis of the results, the average similarity between the AI-generated images and the original product was 48.3%, with an average similarity of 47.7% in (MJ) and 48.8% in (D2) for both software. The results show that AI performs better on simple structured shapes, as the Pantone chair scores 81.67% similarity in (MJ) compared to 36.67% for the Red and Blue chair, and the Bialetti Moka Express scores 80% similarity in (D2) compared to 31.67% for the Alessi espresso maker 9090 scored 31.67%. According to the participant interviews, (MJ) is more suitable for brainstorming as it has more variation in form and its generative style is more artistic, while (D2) produces results that are too often partial and less thought-provoking, but has an advantage over (MJ) as a shape convergent tool because of the way details are presented. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-35891-3_28,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171444455&origin=inward,Conference Paper,SCOPUS_ID:85171444455,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ethical ai based decision making to reduce tax related debts for governments,"
AbstractView references

Piled up debts, missed revenue targets, inefficient manual processes and large data volumes in tax agencies have created a demand for intelligent systems that can assist tax agents to efficiently handle the tax recovery processes. This could help tax agents focus on strategic initiatives instead of avoidable manual tasks. Design Thinking, generative interviews, and naturalistic observation with a diverse set of user profiles were adopted to get clarity on the revenue authority’s vision, business process flows and user journeys. A risk score-based machine learning application was built to combine the power of AI and human judgement for better informed decision making and explainable AI helped to put humans in the loop. Usability tests were conducted with distinct user groups to refine the user experience. With initial test results of the algorithm showing good accuracy levels, the vision is to help governments to improve overall business efficiency by reducing the tax gap and increasing revenue. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1109/INDIN51400.2023.10218154,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171186651&origin=inward,Conference Paper,SCOPUS_ID:85171186651,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),comparison of different natural language processing models to achieve semantic interoperability of heterogeneous asset administration shells,"
AbstractView references

Self-organizing systems represent the next level of building automation and make it possible to reduce the manual engineering effort of automation systems. For self-organizing systems to be able to interact interoperable, the system components must be mapped by uniform digital twins and described in a semantically interoperable manner. Semantic interoperability is implemented in the current research approach of Industrie 4.0 through homogeneous semantics. However, given the large number of different manufacturers of technical components, agreement on uniform semantics seems unlikely. This paper presents a method that extends the Industrie 4.0 approach to heterogeneous semantics. Semantic interoperability is realized through the automated mapping of heterogeneous vocabularies to target semantics. Models from the artificial intelligence sub-field natural language processing are used for automated mapping. In this paper, existing models of natural language processing are compared with each other in terms of their mapping accuracy. A dataset based on the ECLASS standard is being developed as a basis for the comparison. This dataset is also being used to create new models that are fine-tuned to the target vocabulary. The results show that the mapping accuracy of existing approaches improves through fine-tuning by an average of 7.5% up to 93%. In addition to the improvement through fine-tuning, this work analyses the influence of the model size on the mapping accuracy by using large language models. Moreover, it examines the integration of structured knowledge in the form of knowledge graphs. © 2023 IEEE.
"
10.1109/INDIN51400.2023.10218117,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171160490&origin=inward,Conference Paper,SCOPUS_ID:85171160490,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards flexible production systems engineering according to rami 4.0 by utilizing ppr notation,"
AbstractView references

Industrial manufacturing is becoming increasingly complex due to the need to produce individual products with dynamic production processes. This means new methods for engineering flexible production systems, the interconnection between production system, process, and product, need to occur, as contemporary methods are reaching their limits. While varying approaches try to address this problem on a small-scale, the need for holistic methodologies becomes obvious. Thus, this paper proposes a holistic solution by aligning the concepts of the Product, Process, and Resource (PPR) notation for engineering industrial systems according to the Reference Architecture Model Industrie 4.0 (RAMI 4.0). By doing so, each other's disadvantages are counteracted. In more detail, RAMI 4.0 is missing a concrete modeling notation allowing it to be practically applied in systems engineering. At the same time, PPR reaches its limits regarding large-scale modeling systems across multiple domains or granularity levels. The result of this work, an applicable PPR modeling notation for engineering complex production systems, could strongly contribute to future Model-based Systems Engineering (MBSE) within this area, as a ready-to-use methodology the so-called RAMI Toolbox provides them. Using the agile design science research methodology (ADSRM), the outcome is evaluated based on a real-world case study. © 2023 IEEE.
"
10.57028/S60-095-Z1035,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171139373&origin=inward,Article,SCOPUS_ID:85171139373,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt: a tool towards an education revolution?,"
AbstractView references

In this article we start with elaborating the educational and cultural setting of the use of computers in general and of the additional impact of the ChatGPT in these. Their impact is changing the assessment of education in general. It is crucial to be aware of the related challenges when looking and studying ChatGPT. With this in mind we describe a case study involving the use of ChatGPT in elearning. The aim of this study is to gather usability impressions from system interactions during an e-learning session related to the domain of knowledge representation. This concerns in particular the aspects of fluency, understandability of the responses, and the adequacy of the provided knowledge. These aspects form the first part of the article, and in the second part we look at a broader range of related topics including the technologies behind ChatGPT, a literature study of the encountered ChatGPT limitations and errors, and the implied need of critical thinking when using ChatGPT. Moreover, key ethical aspects of using ChatGPT are discussed, and also state-of-the-art online tools concerning automatic detection of AI generated content. Our findings concerning the potential use of ChatGPT in e-learning are confirmed by the presented literature study. © 2023, ICIWO vzw. All rights reserved.
"
10.3991/ijet.v18i16.42313,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170642033&origin=inward,Article,SCOPUS_ID:85170642033,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the requirements for heutagogical attunement within steam education,"
AbstractView references

As we launch into an era marked by rapid technological advancements, education is experiencing disruptive transformations. Central to this shift is the integration of Artificial Intelligence (AI), predictive hybrid cloud strategies, and large language models (LLMs) like ChatGPT. As technology, AI becomes increasingly central to education; this paper explores the evolution of pedagogy in response to these changes, with a focus on the horizon. This investigation delves into the challenges and opportunities arising from the integration of AI, predictive hybrid cloud strategies, and LLMs into education, especially in the context of self-determined learning or heutagogy. The research looks into the role of collaboration, innovation, and adaptability in shaping curricula that prepare students for the fast-paced and dynamic landscape of the future, supported by AI, autonomous systems, and high-performance computing. The significance of problem-or project-based learning and cross-disciplinary collaboration is addressed, as this paper underscores the potential for preparing someone capable of future careers across various sectors. By integrating technology into education, the vision is of a globally connected, skilled, and innovative workforce ready to navigate a rapidly evolving world. Future research in this area might illuminate the potential of collaboration networks to drive sustainable improvements in learning and well-being. © 2023 by the authors of this article. Published under CC-BY.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170636120&origin=inward,Conference Paper,SCOPUS_ID:85170636120,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring the effectiveness of prompt engineering for legal reasoning tasks,"
AbstractView references

The use of large language models (LLMs) for zero- or few-shot prompting in natural language processing has given rise to a new research area known as prompt engineering, which shows promising improvement in tasks such as arithmetic and common-sense reasoning. This paper explores the use of such approaches in legal reasoning tasks by conducting experiments on the COLIEE entailment task, which is based on the Japanese Bar exam. We further evaluate zero-shot/few-shot and fine-tuning approaches with and without explanations, alongside various prompting strategies. Our results indicate that while these techniques can improve general performance, the best results are achieved with prompts derived from specific legal reasoning techniques, such as IRAC (Issue, Rule, Application, Conclusion). In addition, we observe that few-shot learning with demonstrations derived from clustering past training data consistently yields high performance on the most recent COLIEE entailment tasks. Through our experiments, we improve the previous best result on the 2021 COLIEE task from 0.7037 to 0.8025 and surpass the best system from 2022 with an accuracy of 0.789. © 2023 Association for Computational Linguistics.
"
10.18293/SEKE2023-115,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170065533&origin=inward,Conference Paper,SCOPUS_ID:85170065533,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),fine-grained source code vulnerability detection via graph neural networks,"
AbstractView references

Although the number of exploitable vulnerabilities in software continues to increase, the speed of bug fixes and software updates have not increased accordingly. It is therefore crucial to analyze the source code and identify vulnerabilities in the early phase of software development. However, vulnerability location in most of the current machine learning-based methods tends to concentrate at the function level. It undoubtedly imposes a burden on further manual code audits when faced with large-scale source code projects. In this paper, a fine-grained source code vulnerability detection model based on Graph Neural Networks (GNNs) is proposed with the aim of locating vulnerabilities at the function level and line level. Our empirical evaluation on different C/C++ datasets demonstrated that our proposed model outperforms the state-of-the-art methods and achieves significant improvements even when faced with more complex, real-project source code. © 2023 Knowledge Systems Institute Graduate School. All rights reserved.
"
10.18293/SEKE2023-221,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170050739&origin=inward,Conference Paper,SCOPUS_ID:85170050739,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),modeling and verification of autonomous driving systems under stochastic spatio-temporal constraints,"
AbstractView references

The decision-making process in autonomous driving systems encounters large uncertainties with environmental changes and needs to face the complex spatio-temporal evolution of multiple objectives. Formal analysis and verification are crucial to establishing reliable and safe standards. In this paper, we propose an extension of the clock constraint language CCSL to construct spatio-temporal constraint and autonomous driving safety specifications, leveraging various autonomous driving scenarios. Additionally, we introduce probabilistic spatio-temporal events and devise extensions for driving specifications that incorporate stochasticity. This specification is converted to the UPPAAL-SMC model for facilitating formal modeling and verification. Specific schemes and verification are given in conjunction with a typical autonomous driving scenario. © 2023 Knowledge Systems Institute Graduate School. All rights reserved.
"
10.18293/SEKE2023-192,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170030419&origin=inward,Conference Paper,SCOPUS_ID:85170030419,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),graphplbart: code summarization based on graph embedding and pre-trained model,"
AbstractView references

Code summarization is a task that aims at automatically producing descriptions of source code. Recently many deep-learning-based approaches have been proposed to generate accurate code summaries, among which pre-trained models for programming languages have achieved promising results. It is well-known that source code written in programming languages is highly structured and unambiguous. Though previous work pre-trained the model with well-design tasks to learn universal representation from a large scale of data, they haven’t considered structure information during the fine-tuning stage. To make full use of both the pre-trained programming language model and the structure information of source code, we utilize Flow-Augmented Abstract Syntax Tree (FA-AST) of source code for structure information and propose GraphPLBART – Graph-augmented Programming Language and Bi-directional Auto-Regressive Transformer, which can effectively introduce structure information to a well pre-trained model through a cross attention layer. Experimental results show that our approach outperforms the baseline models in some metrics. © 2023 Knowledge Systems Institute Graduate School. All rights reserved.
"
10.1109/ICCCI59363.2023.10210148,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170028267&origin=inward,Conference Paper,SCOPUS_ID:85170028267,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using chatgpt on improving program performance with pprof and benchmark,"
AbstractView references

In the context of limited computing resources, optimizing program architecture is crucial. Therefore, this paper proposes to apply the powerful analytical capabilities of large language models (LLM) to the field of systematic performance optimization. The output of pprof and benchmark is fed into ChatGPT, and the program's performance is improved based on feedback. In the case study, the number of memory allocations for the objective function was successfully reduced from 99 to 1, resulting in a reduction of the test execution time from 8.6 microseconds to 0.36 microseconds. At the same time, the memory allocation was also reduced from 53.5KB to approximately 1KB. © 2023 IEEE.
"
10.1007/s00146-023-01752-8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169786535&origin=inward,Article,SCOPUS_ID:85169786535,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompting meaning: a hermeneutic approach to optimising prompt engineering with chatgpt,"
AbstractView references

Recent advances in natural language generation (NLG), such as public accessibility to ChatGPT, have sparked polarised debates about the societal impact of this technology. Popular discourse tends towards either overoptimistic hype that touts the radically transformative potentials of these systems or pessimistic critique of their technical limitations and general ‘stupidity’. Surprisingly, these debates have largely overlooked the exegetical capacities of these systems, which for many users seem to be producing meaningful texts. In this paper, we take an interdisciplinary approach that combines hermeneutics—the study of meaning and interpretation—with prompt engineering—task descriptions embedded in input to NLG systems—to study the extent to which a specific NLG system, ChatGPT, produces texts of hermeneutic value. We design prompts with the goal of optimising hermeneuticity rather than mere factual accuracy, and apply them in four different use cases combining humans and ChatGPT as readers and writers. In most cases, ChatGPT produces readable texts that respond clearly to our requests. However, increasing the specificity of prompts’ task descriptions leads to texts with intensified neutrality, indicating that ChatGPT’s optimisation for factual accuracy may actually be detrimental to the hermeneuticity of its output. © 2023, The Author(s).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169686734&origin=inward,Conference Paper,SCOPUS_ID:85169686734,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),do we still need clinical language models?,"
AbstractView references

Although recent advances in scaling large language models (LLMs) have resulted in improvements on many NLP tasks, it remains unclear whether these models trained primarily with general web text are the right tool in highly specialized, safety critical domains such as clinical text. Recent results have suggested that LLMs encode a surprising amount of medical knowledge. This raises an important question regarding the utility of smaller domain-specific language models. With the success of general-domain LLMs, is there still a need for specialized clinical models? To investigate this question, we conduct an extensive empirical analysis of 12 language models, ranging from 220M to 175B parameters, measuring their performance on 3 different clinical tasks that test their ability to parse and reason over electronic health records. As part of our experiments, we train T5-Base and T5-Large models from scratch on clinical notes from MIMIC III and IV to directly investigate the efficiency of clinical tokens. We show that relatively small specialized clinical models substantially outperform all in-context learning approaches, even when finetuned on limited annotated data. Further, we find that pretraining on clinical tokens allows for smaller, more parameter-efficient models that either match or outperform much larger language models trained on general text. We release the code and the models used under the PhysioNet Credentialed Health Data license and data use agreement. © 2023 E. Hernandez.
"
10.1007/978-3-031-36021-3_15,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169668358&origin=inward,Conference Paper,SCOPUS_ID:85169668358,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),fine-tuning large language models for answering programming questions with code snippets,"
AbstractView references

We study the ability of pretrained large language models (LLM) to answer questions from online question answering fora such as Stack Overflow. We consider question-answer pairs where the main part of the answer consists of source code. On two benchmark datasets—CoNaLa and a newly collected dataset based on Stack Overflow—we investigate how a closed-book question answering system can be improved by fine-tuning the LLM for the downstream task, prompt engineering, and data preprocessing. We use publicly available autoregressive language models such as GPT-Neo, CodeGen, and PanGu-Coder, and after the proposed fine-tuning achieve a BLEU score of 0.4432 on the CoNaLa test set, significantly exceeding previous state of the art for this task. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1016/j.procir.2023.04.001,S2212827123004262,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169655188&origin=inward,Conference Paper,SCOPUS_ID:85169655188,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"chatgpt for design, manufacturing, and education","The manufacturing industry involves innumerable complex tasks that require significant knowledge and experience to execute. With the rapid development of artificial intelligence, particularly with the emergence of powerful large language models such as ChatGPT, new opportunities have risen to provide knowledge through conversation. With its seemingly endless knowledge base and highly organized response style, ChatGPT is expected to revolutionize every aspect of the industry. However, the extent of ChatGPT's capabilities and how they could contribute to the industry's future revolution remains unclear. In light of this, this paper performed a systematic testing of ChatGPT to uncover its advantages and limitations. Based on the testing results, the authors provided some prospects and critical research questions of ChatGPT from a manufacturing perspective. Furthermore, the authors recommended a technology development roadmap to successfully integrate ChatGPT into the manufacturing industry."
10.1109/IJCNN54540.2023.10191898,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169569605&origin=inward,Conference Paper,SCOPUS_ID:85169569605,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),mad: a dataset for interview-based bpm in business process management,"
AbstractView references

Business process management focuses on the automatic discovery and optimisation of business process models for a wide range of business scenarios. At the same time, the development of natural language processing (NLP), in particular some large-scale pre-trained language models such as BERT and GPT, has recently achieved great success and become a milestone in many practical fields. We thus propose a new paradigm to automate business process model discovery directly from interview-based natural language documents by applying NLP technologies to the business process modeling area. To train the language models for the business process management domain, we create the Business Process Model and Textual Description (MaD) dataset, which contains 15 business categories and a total of 30,000 BPM-description pairs. Furthermore, we define the automatic as well as human-involved metrics to evaluate the quality of the MaD dataset. The experiment results show that the generated dataset is of high-quality, and suitable for high-level people with professional skills to read and understand. The dataset is available online11https://drive.google.com/drive/u/0/folders/1n0K9BmiDsXYCqB796MVebYBgWX2ruZpW. © 2023 IEEE.
"
10.1109/ICAA58325.2023.00029,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169541458&origin=inward,Conference Paper,SCOPUS_ID:85169541458,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),dehallucinating large language models using formal methods guided iterative prompting,"
AbstractView references

Large language models (LLMs) such as ChatGPT have been trained to generate human-like responses to natural language prompts. LLMs use a vast corpus of text data for training, and can generate coherent and contextually relevant responses to a wide range of questions and statements. Despite this remarkable progress, LLMs are prone to hallucinations making their application to safety-critical applications such as autonomous systems difficult. The hallucinations in LLMs refer to instances where the model generates responses that are not factually accurate or contextually appropriate. These hallucinations can occur due to a variety of factors, such as the model's lack of real-world knowledge, the influence of biased or inaccurate training data, or the model's tendency to generate responses based on statistical patterns rather than a true understanding of the input. While these hallucinations are a nuisance in tasks such as text summarization and question-answering, they can be catastrophic when LLMs are used in autonomy-relevant applications such as planning. In this paper, we focus on the application of LLMs in autonomous systems and sketch a novel self-monitoring and iterative prompting architecture that uses formal methods to detect these errors in the LLM response automatically. We exploit the dialog capability of LLMs to iteratively steer them to responses that are consistent with our correctness specification. We report preliminary experiments that show the promise of the proposed approach on tasks such as automated planning. © 2023 IEEE.
"
10.1007/978-3-031-36001-5_78,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169478226&origin=inward,Conference Paper,SCOPUS_ID:85169478226,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),revamping interior design workflow through generative artificial intelligence,"
AbstractView references

This paper presents some preliminary results on how generative AI (Artificial Intelligence) can transform key tasks in a typical interior design workflow, namely, ideation, schematic drafting, and layout planning. We show via examples how targeted fine-tuning and deliberated prompt engineering can jumpstart the creative process and improve the overall workflow efficiency and effectiveness. Ideation by collecting targeted reference designs to fine-tune a Stable Diffusion model and cranking out various ideas by simply engineering prompts. Schematic drafting is to test design ideas in the actual environment, where designers model the space structure with specific design elements. We used an actual photo of a plain office as a guidance and teach the model to generate images with specified features, such as different styles, materials, décor objects, while maintaining the underlying spatial configuration. Layout Planning is to organize the functions with comprehensive consideration of functional relationships and layout design principles. The generative model is fine-tuned using selected layout plans, which can accommodate different functional strategies, such as private rooms, large open areas, etc. and provide viable layout options. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-36004-6_55,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169465279&origin=inward,Conference Paper,SCOPUS_ID:85169465279,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards an automatic prompt optimization framework for ai image generation,"
AbstractView references

Generative AI experienced a boom in 2022 with highlights such as the releases of Stable Diffusion for image generation and ChatGPT for conversational text generation. Although millions of images have been generated using products such as DALL-E 2, DreamStudio, and Midjourney, the learning curve for developing good text prompts that can lead to high-quality images remains steep, especially for inexperienced and less-technical users. Although various prompt engineering guides and tutorials have been developed to provide tips and guidance on prompt writing, there has been scant research on automatic prompt improvement algorithms and methods. In this paper, we present an automatic prompt optimization framework based on NLP analysis of a large prompt database and various machine learning models. A product based on our framework was developed and deployed for two months and real data were collected to evaluate our framework. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1109/TechDebt59074.2023.00011,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169419658&origin=inward,Conference Paper,SCOPUS_ID:85169419658,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),measuring improvement of f<inf>1</inf>-scores in detection of self-admitted technical debt,"
AbstractView references

Artificial Intelligence and Machine Learning have witnessed rapid, significant improvements in Natural Language Processing (NLP) tasks. Utilizing Deep Learning, researchers have taken advantage of repository comments in Software Engineering to produce accurate methods for detecting Self-Admitted Technical Debt (SATD) from 20 open-source Java projects' code. In this work, we improve SATD detection with a novel approach that leverages the Bidirectional Encoder Representations from Transformers (BERT) architecture. For comparison, we re-evaluated previous deep learning methods and applied stratified 10-fold cross-validation to report reliable F1-scores. We examine our model in both cross-project and intra-project contexts. For each context, we use re-sampling and duplication as augmentation strategies to account for data imbalance. We find that our trained BERT model improves over the best performance of all previous methods in 19 of the 20 projects in cross-project scenarios. However, the data augmentation techniques were not sufficient to overcome the lack of data present in the intra-project scenarios, and existing methods still perform better. Future research will look into ways to diversify SATD datasets in order to maximize the latent power in large BERT models. © 2023 IEEE.
"
10.1007/978-3-031-37189-9_7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169040089&origin=inward,Conference Paper,SCOPUS_ID:85169040089,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the house that looked like it should collapse. natural language processing for architectural design,"
AbstractView references

Machine learning (ML) has not only become increasingly popular in the fields of computer science and AI since the 1990s, it also has become increasingly used in architectural design in the past few years. While some experimentations deal with fabrication or engineering issues and performance prediction, others deal with layout generation. As in any process involving ML, layout generation necessitates a large database to train the system. These databases are currently in their majority 2D images or 3D-models. However, these databases are often either scarce or monolithic – they gather too few examples and/or examples that are very similar in regard to the vast diversity of architectural design. The Generative Pre-trained Transformer 3 (GPT-3) is a language model developed by Open AI to produce text answers to text prompts submitted to it. With 175 billion parameters, it is one of the largest models available. It has been trained on billions of online texts, coming from databases such as Common Crawl, Books2 or Wikipedia. The texts it generates are therefore based on a very large range of information, including about architecture. While only being word-based, the amount of information it has been trained with makes the suggestions relative to architecture GPT-3 outputs worthwhile to examine. What patterns relating to architectural design emerge when training a ML algorithm on such a database, much larger than what our field is currently used to manipulating? We have been testing and calibrating GPT-3 in order to obtain descriptions of buildings that could then be translated into drawings, i.e. design briefs. This calibration has also enabled us to assess the patterns identified by the system. We thus propose an evaluation of how design issues usually perceived by architects as key are handled by GPT-3 – location, climate, program, references, inhabitants and their peculiarities. This evaluation is based on an assessment method that can be applied to other general ML tools in architecture, including image generators. Upon performing these tests, some prompts generated particularly various and interesting answers, oscillating between rationality and fiction. The present paper uses these prompts as examples in order to deepen the assessment of GPT-3, to evaluate its potential as a design tool and to open a larger discussion about databases within ML use for architectural design. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/s13198-023-02096-8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168952105&origin=inward,Article,SCOPUS_ID:85168952105,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),voice network analysis and oral english teaching oriented to 6g wireless transmission technology,"
AbstractView references

The sixth-generation mobile communication standard, 6G wireless transmission technology, also known as the sixth-generation mobile communication technology, is the driving force for the development of the Internet of Things. It is still in the development stage. The transmission rate of 6G has increased by nearly 50 times compared with 5G, and the network delay has changed significantly compared with 5G, which can be reduced from milliseconds to microseconds. By integrating satellite communications with 6G wireless transmission technology, seamless global coverage can be achieved, network signals can cover more remote mountainous areas, and remote education for children in remote mountainous areas can be carried out. English teaching focuses on oral English teaching. If you want to improve your oral English, you must be proficient in basic English communication skills. Therefore, teachers should not only improve students’ oral vocabulary accumulation in English courses, but also provide students with subject-related vocabulary and sentences and common oral topics, and guide students to learn and accumulate oral vocabulary in a targeted manner. Regular vocabulary tests are conducted to understand the learning situation. English as a language course is more suitable for interactive education than other courses. The large-scale deployment of MOOC teaching system can provide a large number of online basic courses for students of different situations. Compared with traditional teaching methods, MOOC teaching system can easily strengthen the classroom interaction of class students, and can help students use the knowledge they have learned to communicate better. Voice network analysis adds confidential information to the network protocol by making changes to redundant duplicate content or multimedia carrier models. Voice network analysis hides the information in the redundant part of the signal based on the insensitivity of human emotions. © 2023, The Author(s) under exclusive licence to The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168818600&origin=inward,Conference Paper,SCOPUS_ID:85168818600,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring in-context learning capabilities of foundation models for generating knowledge graphs from text,"
AbstractView references

Knowledge graphs can represent information about the real-world using entities and their relations in a structured and semantically rich manner and they enable a variety of downstream applications such as question-answering, recommendation systems, semantic search, and advanced analytics. However, at the moment, building a knowledge graph involves a lot of manual effort and thus hinders their application in some situations and the automation of this process might benefit especially for small organizations. Automatically generating structured knowledge graphs from a large volume of natural language is still a challenging task and the research on sub-tasks such as named entity extraction, relation extraction, entity and relation linking, and knowledge graph construction aims to improve the state of the art of automatic construction and completion of knowledge graphs from text. The recent advancement of foundation models with billions of parameters trained in a self-supervised manner with large volumes of training data that can be adapted to a variety of downstream tasks has helped to demonstrate high performance on a large range of Natural Language Processing (NLP) tasks. In this context, one emerging paradigm is in-context learning where a language model is used as it is with a prompt that provides instructions and some examples to perform a task without changing the parameters of the model using traditional approaches such as fine-tuning. This way, no computing resources are needed for re-training/fine-tuning the models and the engineering effort is minimal. Thus, it would be beneficial to utilize such capabilities for generating knowledge graphs from text. In this paper, grounded by several research questions, we explore the capabilities of foundation models such as ChatGPT to generate knowledge graphs from the knowledge it captured during pre-training as well as the new text provided to it in the prompt. The paper provides a qualitative analysis of a set of example outputs generated by a foundation model with the aim of knowledge graph construction and completion. The results demonstrate promising capabilities. Furthermore, we discuss the challenges and next steps for this research work. © 2023 CEUR-WS. All rights reserved.
"
10.7717/PEERJ-CS.1422,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168806038&origin=inward,Article,SCOPUS_ID:85168806038,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on solving textual ambiguities and semantic vagueness in mrc based question answering using generative pre-trained transformers,"
AbstractView references

Machine reading comprehension (MRC) is one of the most challenging tasks and active fields in natural language processing (NLP). MRC systems aim to enable a machine to understand a given context in natural language and to answer a series of questions about it. With the advent of bi-directional deep learning algorithms and large-scale datasets, MRC achieved improved results. However, these models are still suffering from two research issues: textual ambiguities and semantic vagueness to comprehend the long passages and generate answers for abstractive MRC systems. To address these issues, this paper proposes a novel Extended Generative Pretrained Transformers-based Question Answering (ExtGPT-QA) model to generate precise and relevant answers to questions about a given context. The proposed architecture comprises two modified forms of encoder and decoder as compared to GPT. The encoder uses a positional encoder to assign a unique representation with each word in the sentence for reference to address the textual ambiguities. Subsequently, the decoder module involves a multi-head attention mechanism along with affine and aggregation layers to mitigate semantic vagueness with MRC systems. Additionally, we applied syntax and semantic feature engineering techniques to enhance the effectiveness of the proposed model. To validate the proposed model's effectiveness, a comprehensive empirical analysis is carried out using three benchmark datasets including SQuAD, Wiki-QA, and News-QA. The results of the proposed ExtGPT-QA outperformed state of art MRC techniques and obtained 93.25% and 90.52% F1-score and exact match, respectively. The results confirm the effectiveness of the ExtGPT-QA model to address textual ambiguities and semantic vagueness issues in MRC systems. © 2023 Ahmed et al.
"
10.1109/SERA57763.2023.10197708,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168767845&origin=inward,Conference Paper,SCOPUS_ID:85168767845,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a statistical method for api usage learning and api misuse violation finding,"
AbstractView references

A large corpus of software repositories enables an opportunity for using machine learning (ML) approaches to create new software engineering tools. In this paper, we propose a novel technique which leverages ML approaches for automating software engineering tasks and thus improves software quality. Our concrete goal is to (1) explore the abundance of predictable repetitive regularities of such a massive codebase, (2) develop an ML approach for training a statistical model to identify common patterns in software corpora, and then (3) use these patterns to statistically detect anomalous, likely buggy, program behavior that significantly deviates from these typical patterns. These internal regularities and repetitive properties of software can be captured as patterns to detect violations of these common patterns. Such violations have a critical impact on program behavior such as bugs, security vulnerabilities, or even program crashes. Our approach focuses on usage patterns of application programming interfaces (APIs). API usage patterns are commonly recurring, representative examples of how real-world applications use APIs in software corpora. These desirable patterns of API usage are learnable to validate or improve developers' implementations. This paper shows preliminary results that we use standard cross-entropy and perplexity to measure how surprising a test subject application is to a statistical model estimated from a software corpus. We continue to develop our approach and evaluate the effectiveness to focus on the following research questions. Are our ML models effectively trainable on large code corpora to learn desirable API usage patterns? How does the performance of our ML-based approach compare to state-of-The-Art language models for software when learning API usage for detecting API misuse violations? © 2023 IEEE.
"
10.1109/SEENG59157.2023.00010,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168683291&origin=inward,Conference Paper,SCOPUS_ID:85168683291,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),'we need to talk about chatgpt': the future of ai and higher education,"
AbstractView references

On November 30th, 2022, OpenAI released the large language model ChatGPT, an extension of GPT-3. The AI chatbot provides real-time communication in response to users' requests. The quality of ChatGPT's natural speaking answers marks a major shift in how we will use AI-generated information in our day-to-day lives. For a software engineering student, the use cases for ChatGPT are manifold: assessment preparation, translation, and creation of specified source code, to name a few. It can even handle more complex aspects of scientific writing, such as summarizing literature and paraphrasing text. Hence, this position paper addresses the need for discussion of potential approaches for integrating ChatGPT into higher education. Therefore, we focus on articles that address the effects of ChatGPT on higher education in the areas of software engineering and scientific writing. As ChatGPT was only recently released, there have been no peer-reviewed articles on the subject. Thus, we performed a structured grey literature review using Google Scholar to identify preprints of primary studies. In total, five out of 55 preprints are used for our analysis. Furthermore, we held informal discussions and talks with other lecturers and researchers and took into account the authors' test results from using ChatGPT. We present five challenges and three opportunities for the higher education context that emerge from the release of ChatGPT. The main contribution of this paper is a proposal for how to integrate ChatGPT into higher education in four main areas. © 2023 IEEE.
"
10.1109/ACCESS57397.2023.10200806,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168666892&origin=inward,Conference Paper,SCOPUS_ID:85168666892,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),comparison of machine learning techniques for sentiment analysis,"
AbstractView references

Sentiment analysis is the process of categorizing and locating the emotions represented in a textual source. Sentiment analysis can be used widely in different areas, such as customer review data, feedback data classification, survey responses, and social media comments. Tweets on Twitter contain a variety of sentiments reflecting the perception, thinking, and working background of the user. With the help of the sentiment analyzer, it can define the response of others on any matter or subject of interest. Here, we used machine learning-based NLP (natural language processing) and text analysis technology to define an automated model that can classify the sentiment of a large dataset. Here we used the following three machine-learning classifiers: logistic regression, SVM, and Bernoulli Naïve Bayes. The effectiveness and performance of these classifiers are assessed using F1 scores and accuracy. The accuracy of these models is 83%(LR), 81%(SVM), and 80%(BNB) So logistic regression model provides the best result among these. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168629217&origin=inward,Conference Paper,SCOPUS_ID:85168629217,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a survey of deep learning for mathematical reasoning,"
AbstractView references

Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain. © 2023 Association for Computational Linguistics.
"
10.1109/ICASSPW59220.2023.10193706,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168252538&origin=inward,Conference Paper,SCOPUS_ID:85168252538,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),software module classification for commercial bug reports,"
AbstractView references

In this work, we curate and investigate a dataset named Turkish Software Report - Module Classification (TSRMC), consisting of commercial software bug reports of a company. Automated bug classification is required in large-scale software projects due to the vast amount of bugs. We analyze and report the statistical features and classification difficulty of the dataset. We use several methods from the text classification literature to assign each bug report of the TSRMC dataset a suitable software module. The utilized methods include traditional machine learning (ML) methods, such as support vector machine (SVM) and logistic regression; sequential deep learning (DL) models, such as gated recurrent unit (GRU) and convolutional neural networks (CNN); and Bidirectional Encoder Representations from Transformers (BERT)-based pre-trained language models (PLMs). Our work is one of the first efforts in automated bug report classification literature that focuses on commercial bugs and uses bilingual (Turkish and English) texts. © 2023 IEEE.
"
10.53106/160792642023072404014,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168091740&origin=inward,Article,SCOPUS_ID:85168091740,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),g-dcs: gcn-based deep code summary generation model,"
AbstractView references

In software engineering, software personnel faced many large-scale software and complex systems, these need programmers to quickly and accurately read and understand the code, and efficiently complete the tasks of software change or maintenance tasks. Code-NN is the first model to use deep learning to accomplish the task of code summary generation, but it is not used the structural information in the code itself. In the past five years, researchers have designed different code summarization systems based on neural networks. They generally use the end-to-end neural machine translation framework, but many current research methods do not make full use of the structural information of the code. This paper raises a new model called G-DCS to automatically generate a summary of java code; the generated summary is designed to help programmers quickly comprehend the effect of java methods. G-DCS uses natural language processing technology, and training the model uses a code corpus. This model could generate code summaries directly from the code files in the coded corpus. Compared with the traditional method, it uses the information of structural on the code. Through Graph Convolutional Neural Network (GCN) extracts the structural information on the code to generate the code sequence, which makes the generated code summary more accurate. The corpus used for training was obtained from GitHub. Evaluation criteria using BLEU-n. Experimental results show that our approach outperforms models that do not utilize code structure information. © 2023 Taiwan Academic Network Management Committee. All rights reserved.
"
10.1109/NLBSE59153.2023.00008,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167945950&origin=inward,Conference Paper,SCOPUS_ID:85167945950,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the (ab)use of open source code to train large language models,"
AbstractView references

In recent years, Large Language Models (LLMs) have gained significant popularity due to their ability to generate human-like text and their potential applications in various fields, such as Software Engineering. LLMs for Code are commonly trained on large unsanitized corpora of source code scraped from the Internet. The content of these datasets is memorized and emitted by the models, often in a verbatim manner. In this work, we will discuss the security, privacy, and licensing implications of memorization. We argue why the use of copyleft code to train LLMs is a legal and ethical dilemma. Finally, we provide four actionable recommendations to address this issue. © 2023 IEEE.
"
10.1109/ICDE55515.2023.00298,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167714500&origin=inward,Conference Paper,SCOPUS_ID:85167714500,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),analytical engines with context-rich processing: towards efficient next-generation analytics,"
AbstractView references

As modern data pipelines continue to collect, produce, and store a variety of data formats, extracting and combining value from traditional and context-rich sources such as strings, text, video, audio, and logs becomes a manual process where such formats are unsuitable for RDBMS. To tap into the dark data, domain experts analyze and extract insights and integrate them into the data repositories. This process can involve out-of-DBMS, ad-hoc analysis, and processing resulting in ETL, engineering effort, and suboptimal performance. While AI systems based on ML models can automate the analysis process, they often further generate context-rich answers. Using multiple sources of truth, for either training the models or in the form of knowledge bases, further exacerbates the problem of consolidating the data of interest.We envision an analytical engine co-optimized with components that enable context-rich analysis. Firstly, as the data from different sources or resulting from model answering cannot be cleaned ahead of time, we propose using online data integration via model-assisted similarity operations. Secondly, we aim for a holistic pipeline cost- and rule-based optimization across relational and model-based operators. Thirdly, with increasingly heterogeneous hardware and equally heterogeneous workloads ranging from traditional relational analytics to generative model inference, we envision a system that just-in-time adapts to the complex analytical query requirements. To solve increasingly complex analytical problems, ML offers attractive solutions that must be combined with traditional analytical processing and benefit from decades of database community research to achieve scalability and performance effortless for the end user. © 2023 IEEE.
"
10.1007/s12599-023-00826-7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166987688&origin=inward,Article,SCOPUS_ID:85166987688,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),swel: a domain-specific language for modeling data-intensive workflows,"
AbstractView references

Data-intensive applications aim at discovering valuable knowledge from large amounts of data coming from real-world sources. Typically, workflow languages are used to specify these applications, and their associated engines enable the execution of the specifications. However, as these applications become commonplace, new challenges arise. Existing workflow languages are normally platform-specific, which severely hinders their interoperability with other languages and execution engines. This also limits their reusability outside the platforms for which they were originally defined. Following the Design Science Research methodology, the paper presents SWEL (Scientific Workflow Execution Language). SWEL is a domain-specific modeling language for the specification of data-intensive workflows that follow the model-driven engineering principles, covering the high-level definition of tasks, information sources, platform requirements, and mappings to the target technologies. SWEL is platform-independent, enables collaboration among data scientists across multiple domains and facilitates interoperability. The evaluation results show that SWEL is suitable enough to represent the concepts and mechanisms of commonly used data-intensive workflows. Moreover, SWEL facilitates the development of related technologies such as editors, tools for exchanging knowledge assets between workflow management systems, and tools for collaborative workflow development. © 2023, The Author(s).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166948084&origin=inward,Conference Paper,SCOPUS_ID:85166948084,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative large language models for dialog-based tutoring: an early consideration of opportunities and concerns,"
AbstractView references

After many years of relatively limited capabilities for generative language models, recent large language models (LLM's) have demonstrated qualitatively better capabilities for understanding, synthesis, and inference on text. Due to the prominence of ChatGPT's chat system, both the media and many educational developers have suggested using generative AI to directly tutor students. However, despite surface-level similarity between ChatGPT interactions and tutoring dialogs, generative AI has other strengths which may be substantially more relevant for intelligent tutoring (e.g., detecting misconceptions, improved language translation, content generation) and weaknesses that make it problematic for on-the-fly tutoring (e.g., hallucinations, lack of pedagogical training data). In this paper, we discuss how we are approaching generative LLM's for tutoring dialogs, for problems such as multiconcept short answer grading and semi-supervised interactive content generation. This work shows interesting opportunities for prompt engineering approaches for short-answer classification, despite sometimes quirky behavior. The time savings for high-quality content generation for tutoring is not yet clear and further research is needed. The paper concludes with a consideration of longer-term equity and access in a world where essential capabilities require low-latency real-time connections to large, pay-peruse models. Risks and mitigating technologies for this kind of “AI digital divide” are discussed, including optimized/edge-computing LLM's and using generative AI models as simulated students to train specialized tutoring models. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
10.1080/10447318.2023.2239544,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166781550&origin=inward,Article,SCOPUS_ID:85166781550,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),user evaluation of conversational agents for aerospace domain,"
AbstractView references

The aerospace industry can benefit from conversational agents that provide efficient solutions for safety-of-life scenarios. This industry is characterized by products and systems that require years of engineering to achieve optimal performance within complex environments. With recent advances in retrieval and language models, conversational agents can be developed to enhance the system’s question-answering capabilities. However, evaluating the added-value of such systems in the context of industrial applications, such as pilots in a cockpit, can be challenging. This article presents the design, implementation, and user evaluation of a conversational agent called Smart Librarian, which is tailored to the aerospace domain’s specific requirements to support pilots in their tasks. Our results based on a controlled user experiment with flight school students indicate that the user’s perception of the usefulness of the system in completing the search task is a good predictor of both task score and time spent. The system’s responses to the relevance of the topic is also a good predictor of task score. The perceived difficulty of the search task and its interaction with the relevance of the system’s responses to the topic also play a key role in search performance. The mixed-effects models constructed in this study had large effect sizes, demonstrating participants’ ability to assess their performance accurately. Nevertheless, user satisfaction with the system’s responses may not be a reliable predictor of user search performance. Implications for the design of conversational agents based on the domain’s specific requirements are discussed. © 2023 The Author(s). Published with license by Taylor & Francis Group, LLC.
"
10.1007/978-3-031-15602-1_21,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166662863&origin=inward,Book Chapter,SCOPUS_ID:85166662863,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the application of archimate for planning the implementation of manufacturing management systems,"
AbstractView references

This paper investigates the use of a model-based approach for planning the implementation of manufacturing management systems. Manufacturing management systems such as ERPs, MESs and PLMs are defined by various standards and frameworks and are developed and implemented based on the specific industry they are to be operated within. Such implementations require conformance to large document-centric requirement sets and are depicted in cumbersome vendor-specific specifications. This paper utilizes the ArchiMate language and notation for planning such implementations and engaging stakeholders prior to domain-specific implementation. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166477272&origin=inward,Conference Paper,SCOPUS_ID:85166477272,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),challenges requiring the combination of machine learning and knowledge engineering,"
AbstractView references

The AAAI 2023 Spring Symposium on Challenges Requiring the Combination of Machine Learning and Knowledge Engineering brought together researchers and practitioners from machine learning and knowledge engineering. The goal was to explore how combining these two fields can help address future AI challenges. The symposium included a joint keynote presentation by AI pioneers, over 25 presentations by contributors and authors who shared their research findings, and two challenges for the community to tackle in a follow-up event. This paper reports on the symposium and focuses on the current trend of generative AI and large language models (LLMs) and its possible synergy with knowledge-based systems (KBS), as the keynote speakers and the symposium chair emphasized. The discussions highlighted the potential of combining KBS's knowledge representation capabilities with LLMs' language generation capabilities. © 2023 CEUR-WS. All rights reserved.
"
10.1016/j.procir.2023.05.002,S2212827123004407,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166388952&origin=inward,Conference Paper,SCOPUS_ID:85166388952,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),reconceptualizing chatgpt and generative ai as a student-driven innovation in higher education,"Higher education is poised at the precipice of the changes and challenges brought about by ChatGPT. This paper addresses some of the most fundamental questions about the role, position, and implications of ChatGPT and generative artificial intelligence (AI) tools amidst the evolving landscape of higher education and modern society. By linking technological affordances with educational needs, we conceptualize ChatGPT as a student-driven innovation with rich potential to empower students and enhance their educational experiences and resources. However, this empowerment comes at a price. It requires collaborative efforts among the stakeholders to address the new and emerging challenges regarding student training, higher education curricula and assessment, and technology development and governance. It also implies new directions for educational research and theories."
10.1109/MSR59073.2023.00033,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166357975&origin=inward,Conference Paper,SCOPUS_ID:85166357975,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on codex prompt engineering for ocl generation: an empirical study,"
AbstractView references

The Object Constraint Language (OCL) is a declarative language that adds constraints and object query expressions to Meta-Object Facility (MOF) models. OCL can provide precision and conciseness to UML models. Nevertheless, the unfamiliar syntax of OCL has hindered its adoption by software practitioners. LLMs, such as GPT-3, have made significant progress in many NLP tasks, such as text generation and semantic parsing. Similarly, researchers have improved on the downstream tasks by fine-tuning LLMs for the target task. Codex, a GPT-3 descendant by OpenAI, has been fine-tuned on publicly available code from GitHub and has proven the ability to generate code in many programming languages, powering the AI-pair programmer Copilot. One way to take advantage of Codex is to engineer prompts for the target downstream task. In this paper, we investigate the reliability of the OCL constraints generated by Codex from natural language specifications. To achieve this, we compiled a dataset of 15 UML models and 168 specifications from various educational resources. We manually crafted a prompt template with slots to populate with the UML information and the target task in the prefix format to complete the template with the generated OCL constraint. We used both zero- and few-shot learning methods in the experiments. The evaluation is reported by measuring the syntactic validity and the execution accuracy metrics of the generated OCL constraints. Moreover, to get insight into how close or natural the generated OCL constraints are compared to human-written ones, we measured the cosine similarity between the sentence embedding of the correctly generated and human-written OCL constraints. Our findings suggest that by enriching the prompts with the UML information of the models and enabling few-shot learning, the reliability of the generated OCL constraints increases. Furthermore, the results reveal a close similarity based on sentence embedding between the generated OCL constraints and the human-written ones in the ground truth, implying a level of clarity and understandability in the generated OCL constraints by Codex. © 2023 IEEE.
"
10.1109/MSR59073.2023.00085,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166355530&origin=inward,Conference Paper,SCOPUS_ID:85166355530,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"defectors: a large, diverse python dataset for defect prediction","
AbstractView references

Defect prediction has been a popular research topic where machine learning (ML) and deep learning (DL) have found numerous applications. However, these ML/DL-based defect prediction models are often limited by the quality and size of their datasets. In this paper, we present Defectors, a large dataset for just-in-time and line-level defect prediction. Defectors consists of ≈ 213K source code files (≈ 93K defective and ≈ 120K defect- free) that span across 24 popular Python projects. These projects come from 18 different domains, including machine learning, automation, and internet-of-things. Such a scale and diversity make Defectors a suitable dataset for training ML/DL models, especially transformer models that require large and diverse datasets. We also foresee several application areas of our dataset including defect prediction and defect explanation. © 2023 IEEE.
"
10.1016/j.procir.2023.05.001,S2212827123004274,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166323497&origin=inward,Conference Paper,SCOPUS_ID:85166323497,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),opportunities and challenges of chatgpt for design knowledge management,"Recent advancements in Natural Language Processing have opened up new possibilities for the development of large language models like ChatGPT, which can facilitate knowledge management in the design process by providing designers with access to a vast array of relevant information. However, integrating ChatGPT into the design process also presents new challenges. In this paper, we provide a concise review of the classification and representation of design knowledge, and past efforts to support designers in acquiring knowledge. We analyze the opportunities and challenges that ChatGPT presents for knowledge management in design and propose promising future research directions. A case study is conducted to validate the advantages and drawbacks of ChatGPT, showing that designers can acquire targeted knowledge from various domains, but the quality of the acquired knowledge is highly dependent on the prompt."
10.1109/MSR59073.2023.00082,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166316386&origin=inward,Conference Paper,SCOPUS_ID:85166316386,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"large language models and simple, stupid bugs","
AbstractView references

With the advent of powerful neural language models, AI-based systems to assist developers in coding tasks are becoming widely available; Copilot is one such system. Copilot uses Codex, a large language model (LLM), to complete code conditioned on a preceding ""prompt"". Codex, however, is trained on public GitHub repositories, viz., on code that may include bugs and vulnerabilities. Previous studies [1], [2] show Codex reproduces vulnerabilities seen in training. In this study, we examine how prone Codex is to generate an interesting bug category, single statement bugs, commonly referred to as simple, stupid bugs or SStuBs in the MSR community. We find that Codex and similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs as much as 2x as likely than known, verbatim correct code. We explore the consequences of the Codex generated SStuBs and propose avoidance strategies that suggest the possibility of reducing the production of known, verbatim SStubs, and increase the possibility of producing known, verbatim fixes. © 2023 IEEE.
"
10.1109/ICSE48619.2023.00063,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166286321&origin=inward,Conference Paper,SCOPUS_ID:85166286321,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),explaining software bugs leveraging code structures in neural machine translation,"
AbstractView references

Software bugs claim ˜ 50 % of development time and cost the global economy billions of dollars. Once a bug is reported, the assigned developer attempts to identify and understand the source code responsible for the bug and then corrects the code. Over the last five decades, there has been significant research on automatically finding or correcting software bugs. However, there has been little research on automatically explaining the bugs to the developers, which is essential but a highly challenging task. In this paper, we propose Bugsplainer, a transformer-based generative model, that generates natural language explanations for software bugs by learning from a large corpus of bug-fix commits. Bugsplainer can leverage structural information and buggy patterns from the source code to generate an explanation for a bug. Our evaluation using three performance metrics shows that Bugsplainer can generate understandable and good explanations according to Google's standard, and can outperform multiple baselines from the literature. We also conduct a developer study involving 20 participants where the explanations from Bugsplainer were found to be more accurate, more precise, more concise and more useful than the baselines. © 2023 IEEE.
"
10.1109/ICSCSS57650.2023.10169820,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166198777&origin=inward,Conference Paper,SCOPUS_ID:85166198777,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sign language recognition using machine learning algorithm,"
AbstractView references

Sign Language Recognition System aims to rephrase sign language into text or speech to ease communication between deaf and hearing individuals. This issue has far-reaching implications but remains exceptionally hard due to the complexity and large variation of hand movements. Existing SLR styles use palm-drafted characteristics to determine the shift of sign language and make classification models based on these features. Still, it's delicate to design dependable features that accommodate wide variations in gestures. KNN (k-nearest neighbor) algorithm has shown major challenges like curse of dimensionalities, large dataset sizes, class imbalance, noise, and outliers. To address this conclusion, this study suggests a new convolutional neural network (CNN) that can automatically extract discriminative spatial-temporal elements extracted without information known from unprocessed video streams and avoid designing features. To improve performance, multi-channel video streams (such as color data, depth cues, and body joint orientations) are sent into CNN as input to combine color, depth, and movement data. Using a real-world dataset acquired using Microsoft Kinect, this study evaluated the proposed model and showed its effectiveness over traditional methods based on manual labor feature. © 2023 IEEE.
"
10.1007/978-3-031-34241-7_1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166157715&origin=inward,Conference Paper,SCOPUS_ID:85166157715,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),just tell me: prompt engineering in business process management,"
AbstractView references

GPT-3 and several other language models (LMs) can effectively address various natural language processing (NLP) tasks, including machine translation and text summarization. Recently, they have also been successfully employed in the business process management (BPM) domain, e.g., for predictive process monitoring and process extraction from text. This, however, typically requires fine-tuning the employed LM, which, among others, necessitates large amounts of suitable training data. A possible solution to this problem is the use of prompt engineering, which leverages pre-trained LMs without fine-tuning them. Recognizing this, we argue that prompt engineering can help bring the capabilities of LMs to BPM research. We use this position paper to develop a research agenda for the use of prompt engineering for BPM research by identifying the associated potentials and challenges. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/s13198-023-02029-5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165916723&origin=inward,Article,SCOPUS_ID:85165916723,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),speech detection in english mooc online teaching platform based on edge calculation,"
AbstractView references

In the historical context of the current development of the world's open education system, education and Internet information technology are closely integrated, not only important ways to achieve quality education resource sharing, but also the inevitable needs of promoting education system. With the China Internet of Things terminal, the reliability index of the intelligent terminal is improved. In the existing network, a specific task migrate from the Internet to the cloud, which not only increases the load on the network, but also introduces a large number of data transmission. Extension. The moving edge calculation introduced in the fifth generation mobile communication technology network can effectively solve the above problems. Voice means that people use language information between speaking and hearing, and transmit, and communicate. Nowadays, college English classroom teaching requires both the development trend of online education, but also needs to face the pros and cons of MOOC resources in the process of networked teaching. MOOC launched a challenge to the traditional teaching concept and organization of Chinese traditional teaching. In the face of the arrival of MOOC storms, China's higher education can re-establish high-quality resource sharing mechanisms, explore innovative teaching models, which is expected to improve college classroom teaching levels. As an important online teaching resource is also favored by the majority of teachers and students, it can be supplemented by teaching before class and class. © 2023, The Author(s) under exclusive licence to The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.
"
10.1109/EMR.2023.3284708,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165915950&origin=inward,Article,SCOPUS_ID:85165915950,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),artificial intelligence in engineering management - an editor's perspective (2023),"
AbstractView references

Today, we live in a world determined by multiple crises creating less certainty about the future and its prediction. At the same time, new Artificial Intelligence (AI) technologies are developed at high pace that may lead to a new revolution in how we do business and manage companies. Generative AI tools offer exciting opportunities while at the same time putting to question some well-established solutions. This calls for more and extensive insights in research and practice to tackle the question how these technologies can be used for engineering management. © 1973-2011 IEEE.
"
10.1109/ICEEICT56924.2023.10157848,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165767255&origin=inward,Conference Paper,SCOPUS_ID:85165767255,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),transformer models for recognizing abusive language an investigation and review on tweeteval and solid dataset,"
AbstractView references

Social engineering communities have become very popular among the kids and elderly alike. In this era of social media, the streaming of comments, opinions, reviews and communications is done via most common social media messaging communities like Twitter, Meta owned WhatsApp, FB and Instagram, Snapchat, telegram and YouTube comments. In this paper we perform a review on the different methods and models used to identify the offensive language using different datasets. Offensive language detection is a tedious task as it is country and language specific. The corpus used to identify the offensiveness and abusiveness is not covering all the word usages. We have done a comparison study of different methods on text to detect the post is offensive or not. The detection of abusive language is an unsolved and challenging problem to researchers in Natural Language Processing (NLP). This has led to be one of the reasons for increased level of mental instability among teenagers to elderly. The crime via social media has increased to a large value than older days. The study and surveys show that to recognize the structure and context of the language is the best way to solve this problem to an extent. The paper aims to four recent transformer models pretrained and fine-tuned for offensive language detection on the tweeteval dataset viz; DistilBERT, RoBERTa, DistilRoBERTa and DeBERTa. All the model had limitation in the performance based on the training data size used but are optimized by tuning hyper parameters during training. The models are limited to English language offensive words and recent works are going on in the area of multilingual tweets on both text and speech processing. © 2023 IEEE.
"
10.1017/pds.2023.160,S2732527X23001608,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165513471&origin=inward,Conference Paper,SCOPUS_ID:85165513471,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),efficient formalisation of technical requirements for generative engineering,"
AbstractView references

Currently, engineers need to manually analyse requirement specifications for determining parameters to create geometries in generative engineering. This analysis is time-consuming, error-prone and causes high costs. Generative engineering tools (e.g. Synera) cannot interpret natural language requirements directly. The requirements need to be formalised in a machine-readable format. AI algorithms have the potential to automatically transform natural language requirements into such a formal, machine-readable representation. In this work, a method for formalising requirements for generative engineering is developed and implemented as a prototype in Python. The method is validated in a case example using three products of an automotive engineering service provider. Requirements to be formalised are identified in the specifications of these three products, which are used as a test set to evaluate the performance of the method. The results show that requirements for generative engineering are formalised with high performance (F1 of 86.55 %). By applying the method, efforts and therefore costs for manually analysing requirements regarding parameters for generative engineering are reduced. © The Author(s), 2023. Published by Cambridge University Press.
"
10.1017/pds.2023.293,S2732527X23002936,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165477421&origin=inward,Conference Paper,SCOPUS_ID:85165477421,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),preliminary design of non-linear systems based on global sensitivity analysis and modelica language,"
AbstractView references

In the last few years, the growing need of highly reliable and time-effective strategies to perform preliminary design of complex systems has led industries to adopt the Model Based System Engineering (MBSE) approach. In MBSE, systems are split into multiple sub-systems and the relevant physical phenomena are described via analytical or numerical models. When a significant number of design variables are to be considered, a smart approach to reduce the number of analyses to perform would be to make use of the Global Sensitivity Analysis (GSA) to higlight those variables that have a more significant influence on the system output. Moreover, an even more significant reduction of computational cost to perform the GSA can be achieved if the complex system modelled via the MBSE approach is exported under the Functional Mock-Up Interface (FMI) norm. In this context, this paper proposes an original approach to address the study of two constructive solutions of an acceleration measuring device typically used on airbags for which the use of a new solution characterized by a porous material is compared with a classical one. © The Author(s), 2023. Published by Cambridge University Press.
"
10.1017/pds.2023.277,S2732527X23002778,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165450881&origin=inward,Conference Paper,SCOPUS_ID:85165450881,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),natural language processing in requirements engineering and its challenges for requirements modelling in the engineering design domain,"
AbstractView references

Requirements represent a central element in product development. The large number of requirements inevitably results in an increased susceptibility to errors, an expenditure of time and development costs. The associated problems motivate the application of Artificial Intelligence in the form of Natural Language Processing (NLP). In Requirements Engineering one main task is the classification of requirements which serves as the input in architectural models e.g. in SysML. In mechanical engineering there is still little overview regarding the interface between requirements classification and modelling. This paper provides an overview of the requirement classes and entities used in the literature and analyses their utilisation in modelling. Existing requirements classes usually do not offer the flexibility to be transferred to other domains. However, basic structures can be adopted from those classifications. This enables a clear assignment of existing classes to object classes in modelling. Resulting from the conducted literature study the observed predominant focus of research on the software industry requires an extension of the existing requirement classes and entities to enable further use and transfer to mechanical engineering. © The Author(s), 2023. Published by Cambridge University Press.
"
10.1080/15265161.2023.2233357,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165217907&origin=inward,Article,SCOPUS_ID:85165217907,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),what should chatgpt mean for bioethics?,"
AbstractView references

In the last several months, several major disciplines have started their initial reckoning with what ChatGPT and other Large Language Models (LLMs) mean for them–law, medicine, business among other professions. With a heavy dose of humility, given how fast the technology is moving and how uncertain its social implications are, this article attempts to give some early tentative thoughts on what ChatGPT might mean for bioethics. I will first argue that many bioethics issues raised by ChatGPT are similar to those raised by current medical AI–built into devices, decision support tools, data analytics, etc. These include issues of data ownership, consent for data use, data representativeness and bias, and privacy. I describe how these familiar issues appear somewhat differently in the ChatGPT context, but much of the existing bioethical thinking on these issues provides a strong starting point. There are, however, a few “new-ish” issues I highlight–by new-ish I mean issues that while perhaps not truly new seem much more important for it than other forms of medical AI. These include issues about informed consent and the right to know we are dealing with an AI, the problem of medical deepfakes, the risk of oligopoly and inequitable access related to foundational models, environmental effects, and on the positive side opportunities for the democratization of knowledge and empowering patients. I also discuss how races towards dominance (between large companies and between the U.S. and geopolitical rivals like China) risk sidelining ethics. © 2023 Taylor & Francis Group, LLC.
"
10.47839/ijc.22.2.3086,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165118737&origin=inward,Article,SCOPUS_ID:85165118737,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ontochatgpt information system: ontology-driven structured prompts for chatgpt meta-learning,"
AbstractView references

This research presents a comprehensive methodology for utilizing an ontology-driven structured prompts system in interplay with ChatGPT, a widely used large language model (LLM). The study develops formal models, both information and functional, and establishes the methodological foundations for integrating ontology-driven prompts with ChatGPT’s meta-learning capabilities. The resulting productive triad comprises the methodological foundations, advanced information technology, and the OntoChatGPT system, which collectively enhance the effectiveness and performance of chatbot systems. The implementation of this technology is demonstrated using the Ukrainian language within the domain of rehabilitation. By applying the proposed methodology, the OntoChatGPT system effectively extracts entities from contexts, classifies them, and generates relevant responses. The study highlights the versatility of the methodology, emphasizing its applicability not only to ChatGPT but also to other chatbot systems based on LLMs, such as Google’s Bard utilizing the PaLM 2 LLM. The underlying principles of meta-learning, structured prompts, and ontology-driven information retrieval form the core of the proposed methodology, enabling their adaptation and utilization in various LLM-based systems. This versatile approach opens up new possibilities for NLP and dialogue systems, empowering developers to enhance the performance and functionality of chatbot systems across different domains and languages. © (2023), All Rights Reserved.
"
10.1109/ECTI-CON58255.2023.10153277,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164966806&origin=inward,Conference Paper,SCOPUS_ID:85164966806,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),virtual reality for railway signaling system training,"
AbstractView references

The railway signaling system is an important part of the rail systems that is used for railway traffic control and safety. At present, most of railway signaling labs rely on physical models and devices which require large space and regular maintenance. Emerging of Virtual Reality (VR) offers more flexibility to many applications including education, training, and testing since its operations are based on software and visualization. Using VR allows installation, updating, adjustment and maintenance to be performed more easily and effectively with effective cost, time, and space requirement. This paper aims to develop a virtual reality lab for training in railway signaling and control. The VR lab of railway signaling simulation consists of two stations, trains, tracks, a virtual dashboard and signaling posts including point machines. All models used in this paper are created by Blender which is a free software tool for 3D modeling. The VR environment of the lab is created by Unity which is a software platform for VR development. The functions of all developed models are programed using C# language. The basic route setting and the usage of subsidiary signal e.g., call-on and shut signal are demonstrated in this paper. The test results ensure the capability of the developed VR lab for training the target groups including students, engineers, and train drivers. © 2023 IEEE.
"
10.1007/978-3-031-36272-9_52,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164956814&origin=inward,Conference Paper,SCOPUS_ID:85164956814,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),robust team communication analytics with transformer-based dialogue modeling,"
AbstractView references

Adaptive training environments that can provide reliable insight into team communication offer great potential for team training and assessment. However, traditional techniques that enable meaningful analysis of team communication such as human transcription and speech classification are especially resource-intensive without machine assistance. Additionally, developing computational models that can perform robust team communication analytics based on small datasets poses significant challenges. We present a transformer-based team communication analysis framework that classifies each team member utterance according to dialogue act and the type of information flow exhibited. The framework utilizes domain-specific transfer learning of transformer-based language models pre-trained with large-scale external data and a prompt engineering method that represents both speaker utterances and speaker roles. Results from our evaluation of team communication data collected from live team training exercises suggest the transformer-based framework fine-tuned with team communication data significantly outperforms state-of-the-art models on both dialogue act recognition and information flow classification and additionally demonstrates improved domain-transfer capabilities. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-36336-8_1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164934635&origin=inward,Conference Paper,SCOPUS_ID:85164934635,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),four interactions between ai and education: broadening our perspective on what ai can offer education,"
AbstractView references

In the 30th anniversary of the International Artificial Intelligence in Education Society, there is a need to look back to the past to envision the community’s future. This paper presents a new framework (AI × Ed) to categorize different interactions between artificial intelligence (AI) and education. We use our framework to compare papers from two early proceedings of AIED (1985 & 1993) with AIED proceedings papers and IJAIED papers published in 2021. We find that two out of four kinds of interactions between AI and education were more common in the early stages of the field but have gained less attention from the community in recent years. We suggest that AI has more to offer education than a pragmatic toolkit to apply to educational problems; rather, AI also serves as an analogy for thinking about human intelligence and learning. We conclude by envisioning future research directions in AIED. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-031-35995-8_23,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164912785&origin=inward,Conference Paper,SCOPUS_ID:85164912785,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),variable discovery with large language models for metamorphic testing of scientific software,"
AbstractView references

When testing scientific software, it is often challenging or even impossible to craft a test oracle for checking whether the program under test produces the expected output when being executed on a given input – also known as the oracle problem in software engineering. Metamorphic testing mitigates the oracle problem by reasoning on necessary properties that a program under test should exhibit regarding multiple input and output variables. A general approach consists of extracting metamorphic relations from auxiliary artifacts such as user manuals or documentation, a strategy particularly fitting to testing scientific software. However, such software typically has large input-output spaces, and the fundamental prerequisite – extracting variables of interest – is an arduous and non-scalable process when performed manually. To this end, we devise a workflow around an autoregressive transformer-based Large Language Model (LLM) towards the extraction of variables from user manuals of scientific software. Our end-to-end approach, besides a prompt specification consisting of few examples by a human user, is fully automated, in contrast to current practice requiring human intervention. We showcase our LLM workflow over three case studies of scientific software documentation, and compare variables extracted to ground truth manually labelled by experts. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1108/JPIF-06-2023-0053,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164693510&origin=inward,Article,SCOPUS_ID:85164693510,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),real estate insights unleashing the potential of chatgpt in property valuation reports: the “red book” compliance chain-of-thought (cot) prompt engineering,"
AbstractView references

Purpose: This viewpoint article explores the transformative capabilities of large language models (LLMs) like the Chat Generative Pre-training Transformer (ChatGPT) within the property valuation industry. It particularly accentuates the pivotal role of prompt engineering in facilitating valuation reporting and advocates for adopting the “Red Book” compliance Chain-of-thought (COT) prompt engineering as a gold standard for generating AI-facilitated valuation reports. Design/methodology/approach: The article offers a high-level examination of the application of LLMs in real estate research, highlighting the essential role of prompt engineering for future advancements in generative AI. It explores the collaborative dynamic between valuers and AI advancements, emphasising the importance of precise instructions and contextual cues in directing LLMs to generate accurate and reproducible valuation outcomes. Findings: Integrating LLMs into property valuation processes paves the way for efficiency improvements and task automation, such as generating reports and drafting contracts. AI-facilitated reports offer unprecedented transparency and elevate client experiences. The fusion of valuer expertise with prompt engineering ensures the reliability and interpretability of valuation reports. Practical implications: Delineating the types and versions of LLMs used in AI-generated valuation reports encourage the adoption of transparency best practices within the industry. Valuers, as expert prompt engineers, can harness the potential of AI to enhance efficiency, accuracy and transparency in the valuation process, delivering significant benefits to a broad array of stakeholders. Originality/value: The article elucidates the substantial impact of prompt engineering in leveraging LLMs within the property industry. It underscores the importance of valuers training their unique GPT models, enabling customisation and reproducibility of valuation outputs. The symbiotic relationship between valuers and LLMs is identified as a key driver shaping the future of property valuations. © 2023, Emerald Publishing Limited.
"
10.1007/978-3-031-35320-8_1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164666382&origin=inward,Conference Paper,SCOPUS_ID:85164666382,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),large language models in the workplace: a case study on prompt engineering for job type classification,"
AbstractView references

This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model’s performance. Our results show that, with a well-designed prompt, a zero-shot gpt-3.5-turboclassifier outperforms all other models, achieving a 6% increase in Precision@95% Recall compared to the best supervised approach. Furthermore, we observe that the wording of the prompt is a critical factor in eliciting the appropriate “reasoning” in the model, and that seemingly minor aspects of the prompt significantly affect the model’s performance. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1115/DETC2023-117400,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164117346&origin=inward,Conference Paper,SCOPUS_ID:85164117346,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),surrogate modeling of car drag coefficient with depth and normal renderings,"
AbstractView references

Generative AI models have made significant progress in automating the creation of 3D shapes, which has the potential to transform car design. In engineering design and optimization, evaluating engineering metrics is crucial. To make generative models performance-aware and enable them to create high-performing designs, surrogate modeling of these metrics is necessary. However, the currently used representations of three-dimensional (3D) shapes either require extensive computational resources to learn or suffer from significant information loss, which impairs their effectiveness in surrogate modeling. To address this issue, we propose a new two-dimensional (2D) representation of 3D shapes. We develop a surrogate drag model based on this representation to verify its effectiveness in predicting 3D car drag. We construct a diverse dataset of 9,070 high-quality 3D car meshes labeled by drag coefficients computed from computational fluid dynamics (CFD) simulations to train our model. Our experiments demonstrate that our model can accurately and efficiently evaluate drag coefficients with an R2value above 0.84 for various car categories. Moreover, the proposed representation method can be generalized to many other product categories beyond cars. Our model is implemented using deep neural networks, making it compatible with recent AI image generation tools (such as Stable Diffusion) and a significant step towards the automatic generation of drag-optimized car designs. We have made the dataset and code publicly available at https://decode.mit.edu/projects/dragprediction/. © 2023 American Society of Mechanical Engineers (ASME). All rights reserved.
"
10.1007/978-3-031-34560-9_20,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163994615&origin=inward,Conference Paper,SCOPUS_ID:85163994615,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),lessons learned in model-based reverse engineering of large legacy systems,"
AbstractView references

Large technologies companies that offer software modernization and maintenance services for legacy software applications in diverse sectors such as banking, insurance, healthcare and public sector, face a significant challenge. Legacy systems were usually developed in old programming languages, often have outdated documentation and the processes used for software development were immature. Modernization and maintenance projects include tasks such as source code analysis with high effort and time costs, and an important risk of misunderstanding. In the literature, model-driven reverse engineering (MDRE) approaches promise to address these challenges successfully, but most of existing proposals are focused on a concrete technological stack. This paper aims to present the preliminary results and lessons learned when adopting MDRE in a large multinational company, providing a series of reflections and open issues to reduce the gap between academia and industry. It introduces STRATO, a corporate solution that proposes a MDRE approach focused on a high flexibility to incorporate new programming languages. It reads source code and through model-to-model transformations convert it into platform independent conceptual, persistence and business logic models. Preliminary outcomes, lessons learned and open issues concerning MDRE industry adoption are presented. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163872945&origin=inward,Conference Paper,SCOPUS_ID:85163872945,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),from knowledge management to intelligence engineering - a practical approach to building ai inside the law-firm using open-source large language models,"
AbstractView references

Open-source foundational language models unlock a new opportunity for building AI inside the law firm. In this paper, we explore the different options in the AI build vs buy equation facing law firms and outline four postures across the spectrum of building AI. We motivate a particular posture that leverages open-source foundational models in a way that both mitigates data privacy and security concerns, while enabling customisation of these models with internal data. We explore the different ways in which these models can be fine-tuned and present a novel addition of intelligence engineering to the traditional knowledge management process that involves instruction fine-tuning language models to infinitely scale access to explicit knowledge. We provide a practical demonstration of this technical approach with a proof of concept using an open-source foundational model based on the GPT-3 architecture and an open-source dataset of contracts. We also provide a qualitative analysis of results. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163821488&origin=inward,Conference Paper,SCOPUS_ID:85163821488,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),gracenote.ai: legal generative ai for regulatory compliance,"
AbstractView references

We investigate the transformative potential of large language models (LLMs) in the legal and regulatory compliance domain by developing advanced generative AI solutions, including a horizon scanning tool, an obligations generation tool, and an LLM-based expert system. Our approach combines the LangChain framework, OpenAI's GPT-4, text embeddings, and prompt engineering techniques to effectively reduce hallucinations and generate reliable and accurate domain-specific outputs. A human-in-the-loop control mechanism is used as a final backstop to ensure accuracy and mitigate risk. Our findings emphasise the role of LLMs as foundation engines in specialist tools and lay the groundwork for building the next generation of legal and compliance applications. Future research will focus on extending support across multiple jurisdictions and languages, refining prompts and text embedding datasets for enhanced legal reasoning capabilities, and developing autonomous AI agents and robust LLM-based expert systems. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
10.1109/ICSE48619.2023.00180,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163729250&origin=inward,Conference Paper,SCOPUS_ID:85163729250,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an empirical comparison of pre-trained models of source code,"
AbstractView references

While a large number of pre-trained models of source code have been successfully developed and applied to a variety of software engineering (SE) tasks in recent years, our understanding of these pre-trained models is arguably fairly limited. With the goal of advancing our understanding of these models, we perform the first systematic empirical comparison of 19 recently-developed pre-trained models of source code on 13 SE tasks. To gain additional insights into these models, we adopt a recently -developed 4-dimensional categorization of pre-trained models, and subsequently investigate whether there are correlations between different categories of pre-trained models and their performances on different SE tasks. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163636200&origin=inward,Conference Paper,SCOPUS_ID:85163636200,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on the use of chatgpt to support agile software development,"
AbstractView references

So-called agile development has been widely adopted in the software industry for more than 20 years now. Agile is, in the end, a generic term encompassing methods themselves made of specific practices that, if applied correctly, make the software development flexible, user-oriented and, most importantly, focused on value. To set up an agile development life cycle, some support for the involved team is needed especially if the practitioners are novice 'agilists'. For this purpose, the roles of agile coach or scrum master - played by experts in the application of such methods and practices - appeared. Recently, large language models accessible by AI-based chatbots have become very popular and these could furnish relevant generic knowledge on agile methods, give tailored advice for a development project on the basis of contextual information, or even perform work according to agile practices themselves based on furnished project data. In other words, we could envision that a chatbot like ChatGPT becomes a virtual member of the development team able to inform, coach and execute a share of the development work. Adopting such a tool is nevertheless not trivial. First of all, the true capacity of furnishing valid information, understand contexts and apply techniques correctly should be investigated. Second, it is also unsure how such a digital and virtual team member would be perceived and integrated by its human colleagues. This paper starts with introducing the new context of large language models and analyzing the sentiment expressed in Twitter feeds related to the use of ChatGPT in the context of agile development. Then, it performs a small empirical experiment to assess the performance of ChatGPT when furnished with some tasks that would typically be performed by an agile coach or Scrum master. Then, as an introduction to the second International Workshop on Agile Methods for Information Systems Engineering (Agil-ISE23), it summarizes the papers presented during the event. Copyright © 2023 for this paper by its authors.
"
10.1109/ACCESS.2023.3290488,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163464699&origin=inward,Article,SCOPUS_ID:85163464699,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"analyzing the scholarly literature of digital twin research: trends, topics and structure","
AbstractView references

Currently, studies involving a digital twin are gaining widespread interest. While the first fields adopting such a concept were in manufacturing and engineering, lately, interest extends also beyond these fields across all academic disciplines. Given the inviting idea behind a digital twin which allows the efficient exploitation and utilization of simulations such a trend is understandable. The purpose of this paper is to use a scientometrics approach to study the early publication history of the digital twin across academia. Our analysis is based on large-scale bibliographic and citation data from Scopus that provides authoritative information about high-quality publications in essentially all fields of science, engineering and humanities. This paper has four major objectives. First, we obtain a global overview of all publications related to a digital twin across all major subject areas. This analysis provides insights into the structure of the entire publication corpus. Second, we investigate the co-occurrence of subject areas appearing together on publications. This reveals interdisciplinary relations of the publications and identifies the most collaborative fields. Third, we conduct a trend and keyword analysis to gain insights into the evolution of the concept and the importance of keywords. Fourth, based on results from topic modeling using a Latent Dirichlet Allocation (LDA) model we introduce the definition of a scientometric dimension (SD) of digital twin research that allows to summarize an important aspect of the bound diversity of the academic literature. © 2013 IEEE.
"
10.1007/978-3-031-33271-5_20,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163409695&origin=inward,Conference Paper,SCOPUS_ID:85163409695,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ner4opt: named entity recognition for optimization modelling from natural language,"
AbstractView references

Solving combinatorial optimization problems involves a two-stage process that follows the model-and-run approach. First, a user is responsible for formulating the problem at hand as an optimization model, and then, given the model, a solver is responsible for finding the solution. While optimization technology has enjoyed tremendous theoretical and practical advances, the overall process has remained the same for decades. To date, transforming problem descriptions into optimization models remains a barrier to entry. To alleviate users from the cognitive task of modeling, we study named entity recognition to capture components of optimization models such as the objective, variables, and constraints from free-form natural language text, and coin this problem as Ner4Opt. We show how to solve Ner4Opt using classical techniques based on morphological and grammatical properties and modern methods leveraging pre-trained large language models and fine-tuning transformers architecture with optimization-specific corpora. For best performance, we present their hybridization combined with feature engineering and data augmentation to exploit the language of optimization problems. We improve over the state-of-the-art for annotated linear programming word problems, identify several next steps and discuss important open problems toward automated modeling. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.29333/ejmste/13313,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162941813&origin=inward,Article,SCOPUS_ID:85162941813,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),enhancing stem learning with chatgpt and bing chat as objects to think with: a case study,"
AbstractView references

This study investigates the potential of ChatGPT and Bing Chat, advanced conversational AIs, as “objects-to-think-with,” resources that foster reflective and critical thinking, and concept comprehension in enhancing STEM education, using a constructionist theoretical framework. A single-case study methodology was used to analyze extensive interaction logs between students and both AI systems in simulated STEM learning experiences. The results highlight the ability of ChatGPT and Bing Chat to help learners develop reflective and critical thinking, creativity, problem-solving skills, and concept comprehension. However, integrating AIs with collaborative learning and other educational activities is crucial, as is addressing potential limitations like concerns about AI information accuracy and reliability of the AIs’ information and diminished human interaction. The study concludes that ChatGPT and Bing Chat as objects-to-think-with offer promising avenues to revolutionize STEM education through a constructionist lens, fostering engagement in inclusive and accessible learning environments. © 2023 by the authors; licensee Modestum. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/).
"
10.1007/s40593-023-00336-3,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162939519&origin=inward,Article,SCOPUS_ID:85162939519,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),beyond predictive learning analytics modelling and onto explainable artificial intelligence with prescriptive analytics and chatgpt,"
AbstractView references

A significant body of recent research in the field of Learning Analytics has focused on leveraging machine learning approaches for predicting at-risk students in order to initiate timely interventions and thereby elevate retention and completion rates. The overarching feature of the majority of these research studies has been on the science of prediction only. The component of predictive analytics concerned with interpreting the internals of the models and explaining their predictions for individual cases to stakeholders has largely been neglected. Additionally, works that attempt to employ data-driven prescriptive analytics to automatically generate evidence-based remedial advice for at-risk learners are in their infancy. eXplainable AI is a field that has recently emerged providing cutting-edge tools which support transparent predictive analytics and techniques for generating tailored advice for at-risk students. This study proposes a novel framework that unifies both transparent machine learning as well as techniques for enabling prescriptive analytics, while integrating the latest advances in large language models for communicating the insights to learners. This work demonstrates a predictive modelling framework for identifying learners at risk of qualification non-completion based on a real-world dataset comprising ∼ 7000 learners with their outcomes, covering 2018 - 2022. The study further demonstrates how predictive modelling can be augmented with prescriptive analytics on two case studies to generate human-readable prescriptive feedback for those who are at risk using ChatGPT. © 2023, The Author(s).
"
10.1109/COMPSAC57700.2023.00117,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162764146&origin=inward,Conference Paper,SCOPUS_ID:85162764146,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),investigating code generation performance of chatgpt with crowdsourcing social data,"
AbstractView references

The recent advancements in Artificial Intelligence, particularly in large language models and generative models, are reshaping the field of software engineering by enabling innovative ways of performing various tasks, such as programming, debugging, and testing. However, few existing works have thoroughly explored the potential of AI in code generation and users' attitudes toward AI-assisted coding tools. This knowledge gap leaves it unclear how AI is transforming software engineering and programming education. This paper presents a scalable crowdsourcing data-driven framework to investigate the code generation performance of generative large language models from diverse perspectives across multiple social media platforms. Specifically, we utilize ChatGPT, a popular generative large language model, as a representative example to reveal its insights and patterns in code generation. First, we propose a hybrid keyword word expansion method that integrates words suggested by topic modeling and expert knowledge to filter relevant social posts of interest on Twitter and Reddit. Then we collect 316K tweets and 3.2K Reddit posts about ChatGPT's code generation, spanning from Dec. 1, 2022 to January 31, 2023. Our data analytics show that ChatGPT has been used in more than 10 programming languages, with Python and JavaScript being the two most popular, for a diverse range of tasks such as code debugging, interview preparation, and academic assignment solving. Surprisingly, our analysis shows that fear is the dominant emotion associated with ChatGPT's code generation, overshadowing emotions of happiness, anger, surprise, and sadness. Furthermore, we construct a ChatGPT prompt and corresponding code dataset by analyzing the screen-shots of ChatGPT code generation shared on social media. This dataset enables us to evaluate the quality of the generated code, and we have released this dataset to the public. We believe the insights gained from our work will provide valuable guidance for future research on AI-powered code generation. © 2023 IEEE.
"
10.23919/DATE56975.2023.10137086,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162707713&origin=inward,Conference Paper,SCOPUS_ID:85162707713,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),benchmarking large language models for automated verilog rtl code generation,"
AbstractView references

Automating hardware design could obviate a signif-icant amount of human error from the engineering process and lead to fewer errors. Verilog is a popular hardware description language to model and design digital systems, thus generating Verilog code is a critical first step. Emerging large language models (LLMs) are able to write high-quality code in other programming languages. In this paper, we characterize the ability of LLMs to generate useful Verilog. For this, we fine-tune pre-trained LLMs on Verilog datasets collected from GitHub and Verilog textbooks. We construct an evaluation framework comprising test-benches for functional analysis and a flow to test the syntax of Verilog code generated in response to problems of varying difficulty. Our findings show that across our problem scenarios, the fine-tuning results in LLMs more capable of producing syntactically correct code (25.9% overall). Further, when analyzing functional correctness, a fine-tuned open-source CodeGen LLM can outperform the state-of-the-art commercial Codex LLM (6.5% overall). We release our training/evaluation scripts and LLM checkpoints as open source contributions. © 2023 EDAA.
"
10.1109/DevIC57758.2023.10135060,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162124094&origin=inward,Conference Paper,SCOPUS_ID:85162124094,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),memory compiler performance prediction using recurrent neural network,"
AbstractView references

Semiconductor chips incorporate a large number of smaller memories. As memories contribute an expected 25 to 40 percentage of the overall performance, power and area of a product, memories should be planned cautiously to meet the current era system requirements. Memories are highly uniform and can be described by approximately ten different parameters. Thus, memories are typically generate by memory compilers, to enhance PPA utilization in memory compilers, A crux task in the design procedure of a chip is to choose optimal memory compiler parameters, which fulfill the one part of the system requirements while on the other part optimize PPA, we proposed training fully connected RNN to predict PPA outputs given to a memory compiler parameterization. We have used Open RAM for the generation of dataset the dataset consists of the parameters which can predict the PPA of the desired memory and model is training for the prepared dataset in python as Open RAM is also developed using python so it is easy to collect data from it. Using an exhaustive search based optimizer RNN framework which generates neural network predictions, In our method, a recurrent neural network model with different designs yielded accuracy up to 98 percent. © 2023 IEEE.
"
10.3389/fpsyt.2023.1190084,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162001088&origin=inward,Article,SCOPUS_ID:85162001088,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),waiting for a digital therapist: three challenges on the path to psychotherapy delivered by artificial intelligence,"
AbstractView references

Growing demand for broadly accessible mental health care, together with the rapid development of new technologies, trigger discussions about the feasibility of psychotherapeutic interventions based on interactions with Conversational Artificial Intelligence (CAI). Many authors argue that while currently available CAI can be a useful supplement for human-delivered psychotherapy, it is not yet capable of delivering fully fledged psychotherapy on its own. The goal of this paper is to investigate what are the most important obstacles on our way to developing CAI systems capable of delivering psychotherapy in the future. To this end, we formulate and discuss three challenges central to this quest. Firstly, we might not be able to develop effective AI-based psychotherapy unless we deepen our understanding of what makes human-delivered psychotherapy effective. Secondly, assuming that it requires building a therapeutic relationship, it is not clear whether psychotherapy can be delivered by non-human agents. Thirdly, conducting psychotherapy might be a problem too complicated for narrow AI, i.e., AI proficient in dealing with only relatively simple and well-delineated tasks. If this is the case, we should not expect CAI to be capable of delivering fully-fledged psychotherapy until the so-called “general” or “human-like” AI is developed. While we believe that all these challenges can ultimately be overcome, we think that being mindful of them is crucial to ensure well-balanced and steady progress on our path to AI-based psychotherapy. Copyright © 2023 Grodniewicz and Hohol.
"
10.1109/SysCon53073.2023.10131050,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161851305&origin=inward,Conference Paper,SCOPUS_ID:85161851305,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),use of natural language processing in digital engineering context to aid tagging of model,"
AbstractView references

This paper uses Natural Language Processing to provide augmented intelligence assistance to the resource intensive task of aligning systems engineering artifacts, namely text requirements and system models, with ontologies. Ontologies are a key enabling technology for digital, multidisciplinary interoperability. The approach presented in this paper combines the efficiency of statistical based natural language processing to process large sets of data with expert verification of output to enable accurate alignment to ontologies in a time efficient manner. It applies this approach to an example from the telecommunications domain to demonstrate the workflows and highlight key points in the process. Enabling easier, faster alignment of systems engineering artifacts with ontologies allows for a holistic view of a system under design and enables interoperability between tools and domains. © 2023 IEEE.
"
10.1007/978-981-99-1639-9_8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161705013&origin=inward,Conference Paper,SCOPUS_ID:85161705013,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),roma: a method for neural network robustness measurement and assessment,"
AbstractView references

Neural network models have become the leading solution for a large variety of tasks, such as classification, natural language processing, and others. However, their reliability is heavily plagued by adversarial inputs: inputs generated by adding tiny perturbations to correctly-classified inputs, and for which the neural network produces erroneous results. In this paper, we present a new method called Robustness Measurement and Assessment (RoMA), which measures the robustness of a neural network model against such adversarial inputs. Specifically, RoMA determines the probability that a random input perturbation might cause misclassification. The method allows us to provide formal guarantees regarding the expected frequency of errors that a trained model will encounter after deployment. The type of robustness assessment afforded by RoMA is inspired by state-of-the-art certification practices, and could constitute an important step toward integrating neural networks in safety-critical systems. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
10.1109/EDUCON54358.2023.10125121,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161702968&origin=inward,Conference Paper,SCOPUS_ID:85161702968,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),engineering education in the era of chatgpt: promise and pitfalls of generative ai for education,"
AbstractView references

Engineering education is constantly evolving to keep up with the latest technological developments and meet the changing needs of the engineering industry. One promising development in this field is the use of generative artificial intelligence technology, such as the ChatGPT conversational agent. ChatGPT has the potential to offer personalized and effective learning experiences by providing students with customized feedback and explanations, as well as creating realistic virtual simulations for hands-on learning. However, it is important to also consider the limitations of this technology. ChatGPT and other generative AI systems are only as good as their training data and may perpetuate biases or even generate and spread misinformation. Additionally, the use of generative AI in education raises ethical concerns such as the potential for unethical or dishonest use by students and the potential unemployment of humans who are made redundant by technology. While the current state of generative AI technology represented by ChatGPT is impressive but flawed, it is only a preview of what is to come. It is important for engineering educators to understand the implications of this technology and study how to adapt the engineering education ecosystem to ensure that the next generation of engineers can take advantage of the benefits offered by generative AI while minimizing any negative consequences. © 2023 IEEE.
"
10.1007/978-981-19-7455-7_27,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161416196&origin=inward,Conference Paper,SCOPUS_ID:85161416196,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ensuring data protection using machine learning integrating with blockchain technology,"
AbstractView references

In recent years, the emergence of blockchain technology (blockchain) has become a singular, most tumultuous, and trending technology. The decentralized info in blockchain underscores information security and confidentiality. Also, the agreement mechanism in it makes positive that information is secured and legit. Throughout this paper, we have got coated the analysis on combining blockchain and machine learning technologies and demonstrate that they are attending to collaborate with efficiency and effectiveness. Machine learning might even be a general language that comes with a spread of methods, machine learning, deep learning, and reinforcement learning. These ways square measure the core technology for big information analysis. As a distributed and append-only ledger system, a blockchain can be a natural tool for sharing and handling massive information from varied sources through the incorporation of good contracts. Additional expressly, blockchain will shield information security and promote information sharing. It also permits various countries to utilize distributed computing powers, for instance, IoT, for developing on-time prediction models with varied sources of knowledge. Blockchain systems can also generate large amounts of useful data from completely different sources, and the knowledge domain analysis on combining the two technologies is of nice potential and the combination of machine learning and blockchain will give extremely precise results. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
10.3389/fcomm.2023.1129082,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161405789&origin=inward,Article,SCOPUS_ID:85161405789,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),artificial intelligence for health message generation: an empirical study using a large language model (llm) and prompt engineering,"
AbstractView references

Introduction: This study introduces and examines the potential of an AI system to generate health awareness messages. The topic of folic acid, a vitamin that is critical during pregnancy, served as a test case. Method: We used prompt engineering to generate awareness messages about folic acid and compared them to the most retweeted human-generated messages via human evaluation with an university sample and another sample comprising of young adult women. We also conducted computational text analysis to examine the similarities between the AI-generated messages and human generated tweets in terms of content and semantic structure. Results: The results showed that AI-generated messages ranked higher in message quality and clarity across both samples. The computational analyses revealed that the AI generated messages were on par with human-generated ones in terms of sentiment, reading ease, and semantic content. Discussion: Overall, these results demonstrate the potential of large language models for message generation. Theoretical, practical, and ethical implications are discussed. Copyright © 2023 Lim and Schmälzle.
"
10.3233/HSM-220064,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161342515&origin=inward,Article,SCOPUS_ID:85161342515,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),mathematical-heuristic modelling for human performance envelope,"
AbstractView references

BACKGROUND: Using the theory of complex systems, some human functions (thinking, memory, language) and human relationships have been analyzed and explained. In order to study the limits of human performance (in Air Traffic Controllers and pilots) a new concept was created, called the Human Performance Envelope (HPE). OBJECTIVE: The aim of this paper is to apply the principles of the complex system to the analysis of the human factors of the HPE concept. Moreover, this paper's objective is to create a mathematical model that will give the opportunity to study all the physiological ergonomic factors, not only the ones that are most commonly studied. The most studied factors are mental workload, stress and situation awareness (SA). By applying the mathematical model, it is possible to analyze all the physiological factors (stress, mental workload, fatigue, attention, vigilance and SA). METHODS: In the present paper the theory of complex systems (hybrid modelling) was applied to the Human Performance Envelope concept. A mathematical model was created, then it was validated and solved based on previous researches. RESULTS: Firstly, a literature analysis was performed on the complex systems application by the present researchers concerning pilots' HPE. The proportional and inverse proportional relationships between the nine human factors were visually illustrated. Finally, a mathematical model was proposed, consisting of a set of equations, which were partially solved and validated by the experiments on pilots done by other researchers. CONCLUSIONS: Further research is required to validate the whole mathematical model, including physiological measurements (experiments) for the six ergonomic factors and the applied heuristic psychosocial methods for the others. © 2023 - IOS Press. All rights reserved.
"
10.53761/1.20.5.02,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161144769&origin=inward,Article,SCOPUS_ID:85161144769,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompting higher education towards ai-augmented teaching and learning practice,"
AbstractView references

Large Language Models (LLMs) and conversational-style generative artificial intelligence (AI) are causing major disruption to higher education pedagogy. The emergence of tools like ChatGPT has raised concerns about plagiarism detection but also presents opportunities for educators to leverage AI to build supportive learning environments. In this commentary, we explore the potential of AI-augmented teaching and learning practice in higher education, discussing both the productive affordances and challenges associated with these technologies. We offer instructional advice for writing instructional text to guide the generation of quality outputs from AI models, as well as a case study to illustrate using AI for assessment design. Ultimately, we suggest that AI should be seen as one tool among many that can be used to enhance teaching and learning outcomes in higher education. Practitioner Notes 1. Learning to write effective instructional prompts for AI models will help augment learning and teaching practice. 2. AI models offer the potential for significant productive affordances, including personalised feedback, adaptive learning pathways, and enhanced student engagement. 3. To successfully integrate AI into higher education, institutions must prioritise faculty development programs that provide training and support for educators to effectively use these technologies in the classroom. 4. Institutions must ensure that AI is used in a way that aligns with their values and mission and that students are informed about how their data is being used. 5. It is important to recognise that AI is not a panacea for all of the challenges facing higher education. Rather, it should be seen as one tool among many that can be used to enhance teaching and learning outcomes. © 2023, University of Wollongong. All rights reserved.
"
10.5220/0012007500003470,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160866835&origin=inward,Conference Paper,SCOPUS_ID:85160866835,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),davinci goes to bebras: a study on the problem solving ability of gpt-3,"
AbstractView references

In this paper we study the problem-solving ability of the Large Language Model known as GPT-3 (codename DaVinci), by considering its performance in solving tasks proposed in the “Bebras International Challenge on Informatics and Computational Thinking”. In our experiment, GPT-3 was able to answer with a majority of correct answers about one third of the Bebras tasks we submitted to it. The linguistic fluency of GPT-3 is impressive and, at a first reading, its explanations sound coherent, on-topic and authoritative; however the answers it produced are in fact erratic and the explanations often questionable or plainly wrong. The tasks in which the system performs better are those that describe a procedure, asking to execute it on a specific instance of the problem. Tasks solvable with simple, one-step deductive reasoning are more likely to obtain better answers and explanations. Synthesis tasks, or tasks that require a more complex logical consistency get the most incorrect answers. Copyright © 2023 by SCITEPRESS – Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0)
"
10.5220/0011994400003464,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160518961&origin=inward,Conference Paper,SCOPUS_ID:85160518961,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),extracting queryable knowledge graphs from user stories: an empirical evaluation,"
AbstractView references

User stories are brief descriptions of a system feature told from a user's point of view. During requirements elicitation, users and analysts co-specify these stories using natural language. A number of approaches have tried to use Natural Language Processing (NLP) techniques to extract different artefacts, such as domain models and conceptual models, and reason about software requirements, including user stories. However, large collections of user story models can be hard to navigate once specified. We extracted different components of user story data, including actors, entities and processes, using NLP techniques and modelled them with graphs. This allows us to organise and link the structures and information in user stories for better analysis by different stakeholders. Our NLP-based automated approach further allows the stakeholders to query the model to view the parts of multiple user stories of interest. This facilitates project development discussions between technical team members, domain experts and users. We evaluated our tool on user story datasets and through a user study. The evaluation of our approach shows an overall precision above 96% and a recall of 100%. The user study with eight participants showed that our querying approach is beneficial in practical contexts. Copyright © 2023 by SCITEPRESS - Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0)
"
10.1109/SANER56733.2023.00074,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160512827&origin=inward,Conference Paper,SCOPUS_ID:85160512827,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),weak labelling for file-level source code classification,"
AbstractView references

Software repository hosting services contain large amounts of open-source software, with GitHub hosting over 200 million repositories, from new to established ones. However, these repositories are not easy to find, calling for various attempts to classify their application domains automatically. However, most proposed approaches use artifacts, like README files, as a proxy for the project, losing the information in the source code and the interaction between files. Furthermore, they all focus on the project-level, ignoring the decomposition of software projects into components and modules.This work presents a weak labelling approach based on keyword extraction to annotate source files in a software project.Our findings suggest that using keywords to perform file-level annotations is an effective approach that can capture enough information from the source file so that new labels can be predicted.The long-term goal of our research is to classify source code files and use these annotations to identify semantic components in software projects. In addition, these annotations can be used for semantic reverse engineering, software reuse, and more. We plan to train machine learning models that use our proposed weak supervision to better annotate source files inside software projects. © 2023 IEEE.
"
10.1080/03043797.2023.2213169,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160255270&origin=inward,Article,SCOPUS_ID:85160255270,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chatgpt versus engineering education assessment: a multidisciplinary and multi-institutional benchmarking and analysis of this generative artificial intelligence tool to investigate assessment integrity,"
AbstractView references

ChatGPT, a sophisticated online chatbot, sent shockwaves through many sectors once reports filtered through that it could pass exams. In higher education, it has raised many questions about the authenticity of assessment and challenges in detecting plagiarism. Amongst the resulting frenetic hubbub, hints of potential opportunities in how ChatGPT could support learning and the development of critical thinking have also emerged. In this paper, we examine how ChatGPT may affect assessment in engineering education by exploring ChatGPT responses to existing assessment prompts from ten subjects across seven Australian universities. We explore the strengths and weaknesses of current assessment practice and discuss opportunities on how ChatGPT can be used to facilitate learning. As artificial intelligence is rapidly improving, this analysis sets a benchmark for ChatGPT’s performance as of early 2023 in responding to engineering education assessment prompts. ChatGPT did pass some subjects and excelled with some assessment types. Findings suggest that changes in current practice are needed, as typically with little modification to the input prompts, ChatGPT could generate passable responses to many of the assessments, and it is only going to get better as future versions are trained on larger data sets. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.
"
10.1109/ACCESS.2023.3277859,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160227685&origin=inward,Article,SCOPUS_ID:85160227685,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),knowledge driven synthesis using resource-capability semantics for control software design,"
AbstractView references

This paper presents a knowledge-driven approach for automated synthesis of controllers in three different use-cases. The approach addresses the engineering challenge posed by Industrie 4.0, which requires fast, reliable, and flexible integration of multiple heterogeneous hardware and software components. Manual design approaches are not scalable for large systems due to their complexity. The proposed approach captures resource-capability knowledge and uses a reasoning-based synthesis mechanism to compose a controller design for a plant goal. The approach uses domain-specific languages (DSLs) to describe the components, their interfaces, and capabilities. The generated control designs are executable codes that implement the control strategy. The proposed approach reduces the average engineering time by 70% and generates on an average 60% of the executable code in each use-case. The approach uses a knowledge repository to store resource-capability knowledge and enables rapid prototyping and iterative design. The proposed approach provides a promising solution to automate the synthesis of controllers in different use-cases with multiple heterogeneous hardware and software components satisfaction. © 2013 IEEE.
"
10.2196/44977,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159827648&origin=inward,Article,SCOPUS_ID:85159827648,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deployment of real-time natural language processing and deep learning clinical decision support in the electronic health record: pipeline implementation for an opioid misuse screener in hospitalized adults,"
AbstractView references

Background: The clinical narrative in electronic health records (EHRs) carries valuable information for predictive analytics; however, its free-text form is difficult to mine and analyze for clinical decision support (CDS). Large-scale clinical natural language processing (NLP) pipelines have focused on data warehouse applications for retrospective research efforts. There remains a paucity of evidence for implementing NLP pipelines at the bedside for health care delivery. Objective: We aimed to detail a hospital-wide, operational pipeline to implement a real-time NLP-driven CDS tool and describe a protocol for an implementation framework with a user-centered design of the CDS tool. Methods: The pipeline integrated a previously trained open-source convolutional neural network model for screening opioid misuse that leveraged EHR notes mapped to standardized medical vocabularies in the Unified Medical Language System. A sample of 100 adult encounters were reviewed by a physician informaticist for silent testing of the deep learning algorithm before deployment. An end user interview survey was developed to examine the user acceptability of a best practice alert (BPA) to provide the screening results with recommendations. The planned implementation also included a human-centered design with user feedback on the BPA, an implementation framework with cost-effectiveness, and a noninferiority patient outcome analysis plan. Results: The pipeline was a reproducible workflow with a shared pseudocode for a cloud service to ingest, process, and store clinical notes as Health Level 7 messages from a major EHR vendor in an elastic cloud computing environment. Feature engineering of the notes used an open-source NLP engine, and the features were fed into the deep learning algorithm, with the results returned as a BPA in the EHR. On-site silent testing of the deep learning algorithm demonstrated a sensitivity of 93% (95% CI 66%-99%) and specificity of 92% (95% CI 84%-96%), similar to published validation studies. Before deployment, approvals were received across hospital committees for inpatient operations. Five interviews were conducted; they informed the development of an educational flyer and further modified the BPA to exclude certain patients and allow the refusal of recommendations. The longest delay in pipeline development was because of cybersecurity approvals, especially because of the exchange of protected health information between the Microsoft (Microsoft Corp) and Epic (Epic Systems Corp) cloud vendors. In silent testing, the resultant pipeline provided a BPA to the bedside within minutes of a provider entering a note in the EHR. Conclusions: The components of the real-time NLP pipeline were detailed with open-source tools and pseudocode for other health systems to benchmark. The deployment of medical artificial intelligence systems in routine clinical care presents an important yet unfulfilled opportunity, and our protocol aimed to close the gap in the implementation of artificial intelligence-driven CDS. ©Majid Afshar, Sabrina Adelaine, Felice Resnik, Marlon P Mundt, John Long, Margaret Leaf, Theodore Ampian, Graham J Wills, Benjamin Schnapp, Michael Chao, Randy Brown, Cara Joyce, Brihat Sharma, Dmitriy Dligach, Elizabeth S Burnside, Jane Mahoney, Matthew M Churpek, Brian W Patterson, Frank Liao.
"
10.1109/TEM.2023.3268340,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159803879&origin=inward,Article,SCOPUS_ID:85159803879,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai in the context of complex intelligent systems: engineering management consequences,"
AbstractView references

As artificial intelligence (AI) is increasingly integrated into the context of complex products and systems (CoPS), making complex systems more intelligent, this article explores the consequences and implications for engineering management in emerging complex intelligent systems (CoIS). Based on five engineering management aspects, including design objectives, system boundaries, architecting and modeling, predictability and emergence, and learning and adaptation, a case study representing future CoIS illustrates how these five aspects, as well as their relationship to criticality and generativity, emerge as AI becomes an integrated part of the system. The findings imply that a future combined perspective on allowing generativity and maintaining or enhancing criticality is necessary, and notably, the results suggest that the understanding of system integrators and CoPS management partly fundamentally alters and partly is complemented with the emergence of CoIS. CoIS puts learning and adaptation characteristics in the foreground, i.e., CoIS are associated with increasingly generative design objectives, fluid system boundaries, new architecting and modeling approaches, and challenges predictability. The notion of bounded generativity is suggested to emphasize the combination of generativity and criticality as a direction for transforming engineering management in CoPS contexts and demands new approaches for designing future CoIS and safeguard its important societal functions. Author
"
10.1109/ACCESS.2023.3274199,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159755297&origin=inward,Article,SCOPUS_ID:85159755297,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"chat2vis: generating data visualizations via natural language using chatgpt, codex and gpt-3 large language models","
AbstractView references

The field of data visualisation has long aimed to devise solutions for generating visualisations directly from natural language text. Research in Natural Language Interfaces (NLIs) has contributed towards the development of such techniques. However, the implementation of workable NLIs has always been challenging due to the inherent ambiguity of natural language, as well as in consequence of unclear and poorly written user queries which pose problems for existing language models in discerning user intent. Instead of pursuing the usual path of developing new iterations of language models, this study uniquely proposes leveraging the advancements in pre-trained large language models (LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directly into code for appropriate visualisations. This paper presents a novel system, Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrates how, with effective prompt engineering, the complex problem of language understanding can be solved more efficiently, resulting in simpler and more accurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMs together with the proposed prompts offer a reliable approach to rendering visualisations from natural language queries, even when queries are highly misspecified and underspecified. This solution also presents a significant reduction in costs for the development of NLI systems, while attaining greater visualisation inference abilities compared to traditional NLP approaches that use hand-crafted grammar rules and tailored models. This study also presents how LLM prompts can be constructed in a way that preserves data security and privacy while being generalisable to different datasets. This work compares the performance of GPT-3, Codex and ChatGPT across several case studies and contrasts the performances with prior studies. © 2013 IEEE.
"
10.1109/ICSE48619.2023.00194,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159708406&origin=inward,Conference Paper,SCOPUS_ID:85159708406,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),large language models are few-shot testers: exploring llm-based general bug reproduction,"
AbstractView references

Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose Libro, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of Libro shows that, on the widely studied Defects4J benchmark, Libro can generate failure reproducing test cases for 33% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate Libro against 31 bug reports submitted after the collection of the LLM training data terminated: Libro produces bug reproducing tests for 32% of the studied bug reports. Overall, our results show Libro has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports. © 2023 IEEE.
"
10.1109/ICSA-C57050.2023.00067,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159093069&origin=inward,Conference Paper,SCOPUS_ID:85159093069,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),metamodel portioning for flexible and secure architectural views,"
AbstractView references

Interacting with a monolithic architecture model to describe the architecture of large-scale software-intensive systems can be a complex and daunting task. The plethora of various concerns being addressed in a single model can impede the ability of individual stakeholders to discern their aspects of relevance. Architectural views allow to spread the various concerns into multiple (smaller) models, each of which addresses a specific concern, thereby aiding architects to break down the complexity of the overall architecture. To enable that, the architectural language should be able to provide explicit separation of concerns, addressed by multiple and somehow separated language portions. For secure collaborative architecting, these portions should be equipped with access control mechanisms that ensure the integrity and confidentiality of the models.In this paper, we present our vision for the automated generation of portions of architectural (and more generically modeling) languages enhanced with access control mechanisms and co-evolution of the architectural portions in response to changes made to the base language. © 2023 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85158949872&origin=inward,Conference Paper,SCOPUS_ID:85158949872,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),comparing general purpose pre-trained word and sentence embeddings for requirements classification,"
AbstractView references

The recent evolution of NLP has enriched the set of DL-based approaches to include a number of general-purpose Large Language Models (LLMs). Whereas new models have been proven useful for generic text handling, their applicability to domain-specific NLP tasks still remains doubtful, particularly because of the limited amount of dataset available in certain domains, such as Requirements Engineering. In this study, different pre-trained embeddings were tested in three requirements classification tasks, in search of a tradeoff between accuracy and computational complexity. The best F1-score results were obtained with BERT (90.36% and 84.23%), with DistilBERT identified as optimal tradeoff (90.28% and 82.61%). © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
10.1016/j.csbj.2023.04.026,S2001037023001794,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85158863665&origin=inward,Article,SCOPUS_ID:85158863665,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),engineering an ai-based forward-reverse platform for the design of cross-ribosome binding sites of a transcription factor biosensor,"A cross-ribosome binding site (cRBS) adjusts the dynamic range of transcription factor-based biosensors (TFBs) by controlling protein expression and folding. The rational design of a cRBS with desired TFB dynamic range remains an important issue in TFB forward and reverse engineering. Here, we report a novel artificial intelligence (AI)-based forward-reverse engineering platform for TFB dynamic range prediction and de novo cRBS design with selected TFB dynamic ranges. The platform demonstrated superior in processing unbalanced minority-class datasets and was guided by sequence characteristics from trained cRBSs. The platform identified correlations between cRBSs and dynamic ranges to mimic bidirectional design between these factors based on Wasserstein generative adversarial network (GAN) with a gradient penalty (GP) (WGAN-GP) and balancing GAN with GP (BAGAN-GP). For forward and reverse engineering, the predictive accuracy was up to 98% and 82%, respectively. Collectively, we generated an AI-based method for the rational design of TFBs with desired dynamic ranges."
10.1109/CCWC57344.2023.10099179,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85153779376&origin=inward,Conference Paper,SCOPUS_ID:85153779376,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prompting large language models with the socratic method,"
AbstractView references

This paper presents a systematic approach to using the Socratic method in developing prompt templates that effectively interact with large language models, including GPT-3. Various methods are examined, and those that yield precise answers and justifications while fostering creativity and imagination to enhance creative writing are identified. Techniques such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning are discussed for their application in engineering prompt templates and their connections to inductive, deductive, and abductive reasoning. Through examples, the effectiveness of these dialogue and reasoning methods is demonstrated. An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively. © 2023 IEEE.
"
10.1007/978-3-031-29786-1_8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85152587069&origin=inward,Conference Paper,SCOPUS_ID:85152587069,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"requirement or not, that is the question: a case from the railway industry","
AbstractView references

[Context and Motivation] Requirements in tender documents are often mixed with other supporting information. Identifying requirements in large tender documents could aid the bidding process and help estimate the risk associated with the project. [Question/problem] Manual identification of requirements in large documents is a resource-intensive activity that is prone to human error and limits scalability. This study compares various state-of-the-art approaches for requirements identification in an industrial context. For generalizability, we also present an evaluation on a real-world public dataset. [Principal ideas/results] We formulate the requirement identification problem as a binary text classification problem. Various state-of-the-art classifiers based on traditional machine learning, deep learning, and few-shot learning are evaluated for requirements identification based on accuracy, precision, recall, and F1 score. Results from the evaluation show that the transformer-based BERT classifier performs the best, with an average F1 score of 0.82 and 0.87 on industrial and public datasets, respectively. Our results also confirm that few-shot classifiers can achieve comparable results with an average F1 score of 0.76 on significantly lower samples, i.e., only 20% of the data. [Contribution] There is little empirical evidence on the use of large language models and few-shots classifiers for requirements identification. This paper fills this gap by presenting an industrial empirical evaluation of the state-of-the-art approaches for requirements identification in large tender documents. We also provide a running tool and a replication package for further experimentation to support future research in this area. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1142/S1793962324410034,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151823082&origin=inward,Article,SCOPUS_ID:85151823082,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),breast cancer detection from histopathological image dataset using hybrid convolution neural network,"
AbstractView references

Cancer of the breast is a deadly disease that can take a person's life in many different ways. Predicting breast cancer at an early stage is crucial in the fight to end the disease. The usage of deep learning and blockchain technology has been implemented with the intention of integrating optimal prediction with clinical diagnostics and protecting private health information. Patients' medical records are encrypted and stored in the blockchain for maximum safety. As a result, a large portion of time and energy is spent on feature engineering, a tedious process that requires prior expert domain knowledge of the data to develop effective features, and is crucial to the success of most conventional classification systems. Deep learning, on the other hand, can arrange the discriminative information in the data without the need for a domain expert to develop feature extractors. The research community and industry have paid attention to deep, feedforward networks like convolutional neural networks (CNNs) because of their empirical results in areas including speech recognition, signal processing, object recognition, natural language processing, and transfer learning. For the best breast cancer prediction, a new method called a ""Hybrid CNN""combining the Sine Cosine Algorithm (SCA) with a transfer learning algorithm has been presented. Mini-batch size and drop-out rate are just two of the factors that the SCA algorithm may fine-Tune. To stop the model from overfitting, we employ a transfer learning strategy. The hyperparameters found using sine cosine algorithm is used in Visual geometry Group (VGG 16) architecture. ImageNet is used to pretrain the network and last three convolutional layers are trained using transfer learning. The integration of sine cosine algorithm and transfer learning together increases the accuracy thereby preventing the model from overfitting. The experimentation is performed in Google Colab and the proposed Hybrid CNN is compared with existing methodologies such as K-NN, SVM. Also, the proposed Hybrid CNN is compared with CNN without transfer learning and CNN without SCA. The metrics taken into account for comparison are accuracy, sensitivity, specificity, F-score. The proposed Hybrid CNN achieves 96.9% accuracy that shows the effectiveness of the integration of SCA and transfer learning. © 2024 World Scientific Publishing Company.
"
10.1098/rsos.221414,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151394400&origin=inward,Article,SCOPUS_ID:85151394400,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ten years after imagenet: a 360° perspective on artificial intelligence,"
AbstractView references

It is 10 years since neural networks made their spectacular comeback. Prompted by this anniversary, we take a holistic perspective on artificial intelligence (AI). Supervised learning for cognitive tasks is effectively solved - provided we have enough high-quality labelled data. However, deep neural network models are not easily interpretable, and thus the debate between blackbox and whitebox modelling has come to the fore. The rise of attention networks, self-supervised learning, generative modelling and graph neural networks has widened the application space of AI. Deep learning has also propelled the return of reinforcement learning as a core building block of autonomous decision-making systems. The possible harms made possible by new AI technologies have raised socio-technical issues such as transparency, fairness and accountability. The dominance of AI by Big Tech who control talent, computing resources, and most importantly, data may lead to an extreme AI divide. Despite the recent dramatic and unexpected success in AI-driven conversational agents, progress in much-heralded flagship projects like self-driving vehicles remains elusive. Care must be taken to moderate the rhetoric surrounding the field and align engineering progress with scientific principles. © 2023 The Authors.
"
10.1007/978-3-031-25448-2_54,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151125740&origin=inward,Conference Paper,SCOPUS_ID:85151125740,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),improving maintenance data quality: application of natural language processing to asset management,"
AbstractView references

Artificial intelligence techniques are increasingly used for asset management. The abundance of data available in large electrical utility offers many application opportunities. The use of data-driven models can address some of the biases of physical models traditionally used in reliability engineering. However, in this context, as in many other fields of operation, the quality of data is often questioned by domain experts. Operational data are entered manually by maintenance technicians, and data entry errors are common. One of the errors that is observed is mislabeling of maintenance types, which can lead to poor statistical estimates of failure rate. This paper aims to improve the quality of historical maintenance data, to increase the accuracy of deployed models. To this end, the text fields available in the maintenance history is analyzed to predict the type of maintenance performed. Natural language processing (NLP) techniques are applied to solve this text classification problem. The models are applied to Hydro-Québec TransÉnergie’s power transmission assets. The application of such techniques allows the enrichment of databases and thus reduces uncertainty in decision-making for asset management. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-981-19-9225-4_60,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150979399&origin=inward,Conference Paper,SCOPUS_ID:85150979399,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),transfer learning-based convolution neural network model for hand gesture recognition,"
AbstractView references

Hearing-impaired people use signals made by hand to communicate using sign language. Due to large linguistic variation, it is not easy for an ordinary man to understand it, and hence, computer vision researchers see it as a potential research problem. Numerous approaches have been surveyed and proposed in the literature to cope with the problems of recognition of hand gestures. Selecting and segmenting exact hand shapes in complex backgrounds is a big challenge. Thus, despite tailor-made feature engineering, deep learning is seen as an effective approach to handle this concern. In the proposed work, deep learning approaches based on Convolution Neural Network (CNN) that have already been proven in computer vision are analyzed for Kaggle’s American Sign Language (ASL) dataset. Various pre-trained architectures are taken for the experimentation work; the best network is selected and further improved with the optimization technique. AlexNet, GoogleNet, ResNet18, and InceptionV3 are trained, and the effect of variation of optimization techniques, namely Adam, sgdm, and RMSProp is analyzed. Performance parameters such as validation accuracy and testing accuracy are measured to determine the efficiency of the model for each alphabet of sign language. The has been successful in accomplishing the test accuracy of 99.1% for transfer learning of InceptionV3 with a sgdm optimizer. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150815013&origin=inward,Conference Paper,SCOPUS_ID:85150815013,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a case study in engineering a conversational programming assistant's persona,"
AbstractView references

The Programmer's Assistant is an experimental prototype software development environment that integrates a chatbot with a code editor. Conversational capability was achieved by using an existing code-fluent Large Language Model and providing it with a prompt that establishes a conversational interaction pattern, a set of conventions, and a style of interaction appropriate for the application. A discussion of the evolution of the prompt provides a case study in how to coax an existing large language model to behave in a desirable manner for a particular application. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org)
"
10.7717/PEERJ-CS.1256,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150457317&origin=inward,Article,SCOPUS_ID:85150457317,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a novel intelligent agent-based framework for appropriate stream selection from perceptive of career counseling,"
AbstractView references

Picking a career stream profoundly influences people’s abilities in different ways. Nowadays, choosing the correct career stream in advanced education is troublesome, as the number of people wanting to be in specific specializations is growing. Therefore, it is essential to be able to select the appropriate career path. This article proposes a system that can suggest streams in advanced education schools. This system is influenced by agent-based stream proposal systems (ASPS). The proposed system aims to make picking out the correct stream to study at school simpler for an individual. Different streams are evaluated based on seven pre-characterized models. In our system, three unique sorts of tests, learning styles, and coaching were coordinated in a way that caused the system to recognize an individual’s interests and limits to an area of study. A sample of 238 participants was recruited for our questionnaire on accessibility, user-friendliness, accuracy, and satisfaction with the system. The incorporation of learning styles and coaching proved helpful in the study. The reliability and validity were proven in addition to incorporating a thinking-aloud protocol and immediate evaluation in the pre-, during and post-tests. To a large extent, respondents were satisfied with the model, as was revealed in the Likert scale response frequencies and percentages. The proposed system can be applied to undergraduates choosing engineering, medicine, arts and science degrees © 2023 Alghamdi
"
10.3991/ijet.v18i05.36877,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150248743&origin=inward,Article,SCOPUS_ID:85150248743,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),gamers’ total experience and game motivation for further education digital manpower,"
AbstractView references

Digital Manpower is very necessary in Thailand today as the growth of technology continues to increase. For example, tech giant Gartner has announced new technology trends every year including 2022, announcing the following technology trends: Total Experience, Composable Application, Decision Intelligence, Hyperautomation, Distributed Enterprises, Autonomic Systems, Generative AI, AI Engineering etc. The application of technology in daily life is increasing. The development of Digital Manpower is therefore very important in order to increase the knowledge and expertise of the country’s people in technology. From the statistics, the country has increased internet usage in many ways. For example, from the statistics in 2021, Thai children playing games were number 1 in the world, and in 2022, Thai children playing games are in 2nd place in the world. But the problem faced with Thai education today is that children like to play games and think that they can study in the field of computers. But, in fact, students are not directed into the technology field taking into account their own aptitudes. causing the study to be not fully effective and some leave their courses midway. Therefore, this research is interested in the confirmation factors of children playing games by collecting data on Gamers’ Total Experience and Game Motivation in order to analyze the data and discover the confirmation factors that direct the admissions to various computer disciplines. What direction and aptitude do you need to be able to enter a group of computer science disciplines in order to become a Digital Manpower in the country’s future development. The researcher has collected data from a sample of 630 people from all six groups in computer science. The confirmation factor was analyzed by the LISREL program. It was found that each group of computer disciplines had different confirmation factors © 2023, International Journal of Emerging Technologies in Learning.All Rights Reserved.
"
10.1007/s00146-023-01631-2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150159597&origin=inward,Article,SCOPUS_ID:85150159597,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),artificial understanding: a step toward robust ai,"
AbstractView references

In recent years, state-of-the-art artificial intelligence systems have started to show signs of what might be seen as human level intelligence. More specifically, large language models such as OpenAI’s GPT-3, and more recently Google’s PaLM and DeepMind’s GATO, are performing amazing feats involving the generation of texts. However, it is acknowledged by many researchers that contemporary language models, and more generally, learning systems, still lack important capabilities, such as understanding, reasoning and the ability to employ knowledge of the world and common sense in order to reach or at least advance toward general intelligence. Some believe that scaling will eventually bring about these capabilities; others think that a different architecture is needed. In this paper, we focus on the latter, with the purpose of integrating a theoretical–philosophical conception of understanding as knowledge of dependence relations, with the high-level requirements and engineering design of a robust AI system, which integrates machine learning and symbolic components. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.
"
10.1109/NLBSE59153.2023.00007,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150003471&origin=inward,Conference Paper,SCOPUS_ID:85150003471,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the nlbse'23 tool competition,"
AbstractView references

We report on the organization and results of the second edition of the tool competition from the International Workshop on Natural Language-based Software Engineering (NLBSE'23). As in the prior edition, we organized the competition on automated issue report classification, with a larger dataset. This year, we featured an extra competition on au-tomated code comment classification. In this tool competition edition, five teams submitted multiple classification models to automatically classify issue reports and code comments. The submitted models were fine-tuned and evaluated on a benchmark dataset of 1.4 million issue reports or 6.7 thousand code comments, respectively. The goal of the competition was to improve the classification performance of the baseline models that we provided. This paper reports details of the competition, including the rules, the teams and contestant models, and the ranking of models based on their average classification performance across issue report and code comment types. © 2023 IEEE.
"
10.1007/s10518-023-01645-7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149646093&origin=inward,Article,SCOPUS_ID:85149646093,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative adversarial networks review in earthquake-related engineering fields,"
AbstractView references

Within seismology, geology, civil and structural engineering, deep learning (DL), especially via generative adversarial networks (GANs), represents an innovative, engaging, and advantageous way to generate reliable synthetic data that represent actual samples’ characteristics, providing a handy data augmentation tool. Indeed, in many practical applications, obtaining a significant number of high-quality information is demanding. Data augmentation is generally based on artificial intelligence (AI) and machine learning data-driven models. The DL GAN-based data augmentation approach for generating synthetic seismic signals revolutionized the current data augmentation paradigm. This study delivers a critical state-of-art review, explaining recent research into AI-based GAN synthetic generation of ground motion signals or seismic events, and also with a comprehensive insight into seismic-related geophysical studies. This study may be relevant, especially for the earth and planetary science, geology and seismology, oil and gas exploration, and on the other hand for assessing the seismic response of buildings and infrastructures, seismic detection tasks, and general structural and civil engineering applications. Furthermore, highlighting the strengths and limitations of the current studies on adversarial learning applied to seismology may help to guide research efforts in the next future toward the most promising directions. © 2023, The Author(s).
"
10.1109/ACCESS.2023.3250426,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149359930&origin=inward,Article,SCOPUS_ID:85149359930,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"reusability, reconfigurability and efficiency optimization of satellite network modeling and simulation","
AbstractView references

Model-based approach with reusable mechanisms can serve as an effective way for complex system architecture design. Stakeholder needs should be satisfied while product and architecture design need to be consistent with user requirements in all stages during the whole product lifecycle. In this paper, satellite network as an example of complex system is modeled in a reusable, reconfigurable and efficient manner using the system modeling language (SysML) together with pattern viewpoints and simulation constructs. Based upon abstract syntax described using metamodels and a set of profiles, concept reusability is established for the specific domain. Additionally a reusable modeling framework is developed with tailored design patterns and multiple viewpoints. Analysis metamodel, profile and interface are further presented to preserve reusability during iterations among multiple optimization rounds. A novel satellite network simulation model is formulated and multi-objective optimization is solved by transformation under practical application scenarios. A set of metrics are designed to assess and validate the models. Results show that the proposed reusable model has viewpoint coverage of more than 80 percent compared to a half for the baseline OOSEM model. The proposed model thus covers the pattern viewpoints and ontologies in a wider and more frequent way and is more efficient. Design choices made based on the model can be incorporated into this mechanism which is extensible along the system lifespan. Author
"
10.1080/14626268.2023.2174557,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149333979&origin=inward,Article,SCOPUS_ID:85149333979,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using artificial intelligence in craft education: crafting with text-to-image generative models,"
AbstractView references

Artificial intelligence (AI) and the automation of creative work have received little attention in craft education. This study aimed to address this gap by exploring Finnish pre-service craft teachers’ and teacher educators’ (N = 15) insights into the potential benefits and challenges of AI, particularly text-to-image generative AI. This study implemented a hands-on workshop on creative making with text-to-image generative AI in order to stimulate discourses and capture imaginaries concerning generative AI. The results revealed that making with AI inspired teachers to consider the unique nature of crafts as well as the tensions and tradeoffs of adopting generative AI in craft practices. The teachers identified concerns in data-driven design, including algorithmic bias, copyright violations and black-boxing creativity, as well as in power relationships, hybrid influencing and behaviour engineering. The article concludes with a discussion of the complicated relationships the results uncovered between creative making and generative AI. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.
"
10.1109/WACV56688.2023.00110,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149022391&origin=inward,Conference Paper,SCOPUS_ID:85149022391,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),drama: joint risk localization and captioning in driving,"
AbstractView references

Considering the functionality of situational awareness in safety-critical automation systems, the perception of risk in driving scenes and its explainability is of particular importance for autonomous and cooperative driving. Toward this goal, this paper proposes a new research direction of joint risk localization in driving scenes and its risk explanation as a natural language description. Due to the lack of standard benchmarks, we collected a large-scale dataset, DRAMA (Driving Risk Assessment Mechanism with A captioning module), which consists of 17,785 interactive driving scenarios collected in Tokyo, Japan. Our DRAMA dataset accommodates video- and object-level questions on driving risks with associated important objects to achieve the goal of visual captioning as a free-form language description utilizing closed and open-ended responses for multi-level questions, which can be used to evaluate a range of visual captioning capabilities in driving scenarios. We make this data available to the community for further re-search. Using DRAMA, we explore multiple facets of joint risk localization and captioning in interactive driving scenarios. In particular, we benchmark various multi-task pre-diction architectures and provide a detailed analysis of joint risk localization and risk captioning. The data set is available at https://usa.honda-ri.com/drama © 2023 IEEE.
"
10.1109/ICSE48619.2023.00113,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85148723020&origin=inward,Conference Paper,SCOPUS_ID:85148723020,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai-based question answering assistance for analyzing natural-language requirements,"
AbstractView references

By virtue of being prevalently written in natural language (NL), requirements are prone to various defects, e.g., inconsistency and incompleteness. As such, requirements are frequently subject to quality assurance processes. These processes, when carried out entirely manually, are tedious and may further overlook important quality issues due to time and budget pressures. In this paper, we propose QAssist - a question-answering (QA) approach that provides automated assistance to stakeholders, including requirements engineers, during the analysis of NL requirements. Posing a question and getting an instant answer is beneficial in various quality-assurance scenarios, e.g., incompleteness detection. Answering requirements-related questions automatically is challenging since the scope of the search for answers can go beyond the given requirements specification. To that end, QAssist provides support for mining external domain-knowledge resources. Our work is one of the first initiatives to bring together QA and external domain knowledge for addressing requirements engineering challenges. We evaluate QAssist on a dataset covering three application domains and containing a total of 387 question-answer pairs. We experiment with state-of-the-art QA methods, based primarily on recent large-scale language models. In our empirical study, QAssist localizes the answer to a question to three passages within the requirements specification and within the external domain-knowledge resource with an average recall of 90.1% and 96.5%, respectively. QAssist extracts the actual answer to the posed question with an average accuracy of 84.2%. © 2023 IEEE.
"
10.1109/ACCESS.2023.3235212,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147276926&origin=inward,Article,SCOPUS_ID:85147276926,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research on judgment and solving strategy of flow problems in complex engineering systems,"
AbstractView references

Frequent processes of mutual transfer and transformation generally take place between the material flow, the energy flow, and the information flow in complex engineering systems, and complex coupling relationships exist between the different flows. Abnormal coupling of flows will cause flow problems, which can easily lead to the failure of the system functions. To address these problems, in this study, the starting point has been taken as the change in the attributes and types of the study flow, the attributes of the flows have been categorized, and a flow transfer and transformation model, oriented to the flow problem analysis has been proposed. Secondly, based on the analysis of the effect of the attribute changes of flows in complex engineering systems on the generation of flow problems, an innovatively introduced information processing mathematical tool, the polychromatic sets theory, has been used to explore and determine the flow problems existing in complex engineering systems. Further, depending on the different manifestations of flow problems, three solution strategies have been proposed. Finally, the process model of the flow problem judgment and a solution strategy for complex engineering systems has been constructed. The feasibility of the process model has been verified by considering the engineering case of the tubular feeding machine (TFM) system of a large-scale integrated marine resource survey ship. © 2013 IEEE.
"
10.1080/00295450.2022.2143210,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146985803&origin=inward,Article,SCOPUS_ID:85146985803,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on the language of reliability: a system engineer perspective,"
AbstractView references

In its classical definition, risk is defined by three elements: what can go wrong, what are its consequences, and how likely is it to occur? While this definition makes sense in a regulatory-based framework where for the current fleet of operating light water reactors (LWRs), the risks associated with nuclear power plants typically are characterized in terms of core damage and large early release frequency (LERF), this approach does not provide a useful snapshot of the health of the plant from a broader perspective. This is due to the very narrow context in which the term “risk” typically is defined as nuclear safety aspects that have the potential to impact public health. In this paper, we take the viewpoint of nuclear safety that is reflective of the current fleet of operating LWRs for which core damage frequency and LERF are appropriate metrics. For other advanced reactor designs, other more applicable technology neutral metrics of reactor safety metrics would be specified. A possible alternate path would start by redefining the word risk with a broader meaning that better reflects the needs of a system health and asset management decision-making process. Rather than asking how likely an event could occur (in probabilistic terms), we can ask how far this event is from occurring. Our approach starts by defining and quantifying component and system health in terms of a “distance” between its actual and limiting conditions, i.e., determination of the margin that exists between the current state/condition and the state where the component/system is no longer capable of achieving its intended function. A margin is a measure that is more reflective of the current state or performance of a component, and therefore more closely tied to decisions that are made on an ongoing basis. We will show how, given the data available from plant equipment reliability and monitoring (e.g., pump vibration data) and prognostic (e.g., component remaining useful life estimation) data, a margin can be described and determined for all types of maintenance approaches (e.g., corrective or predictive maintenance). We show how classical reliability models (e.g., fault trees) can be used to quantify the system margin provided component margin values. In the approach described in this paper, the propagation of margin values through classical reliability models are not performed using classical probabilistic calculations applied to sets (as performed in a typical plant probabilistic risk assessment). Instead, we show how it is possible to propagate margin values through Boolean logic gates (i.e., AND and OR operators) through distance-based operations. © 2023 Battelle Energy Alliance, LLC. Published with license by Taylor & Francis Group, LLC.
"
10.7238/artnodes.v0i31.405249,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146934086&origin=inward,Article,SCOPUS_ID:85146934086,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a critical approach to machine learning forecast capabilities: creating a predictive biography in the age of the internet of behaviour (iob),"
AbstractView references

Based on the notion of the Datacene, understood as the time when data directly affects the social, cultural, economic, political, and even affective structures of the present, in this article we propose how Big Data and Artificial Intelligence give rise to the Internet of Behaviour: a new technological paradigm that has incredible potential to forecast and induce human behaviour. Since ancient times, humans have wanted to predict and alter the future, but in the last ten years, this wish has begun to become a reality due to great advances in the field of social engineering, raising serious doubts regarding social control and the loss of freedom. In this context of analysis, we present two projects developed within the framework of Art, Science, Technology and Society. Data Biography shows the enormous number of digital traces that we generate daily and uses them to compose a person’s biography, composed of 365 printed books. Machine Biography, for its part, investigates how current artificial intelligence techniques can predict and induce future human behaviour, for which we have used various forecast and generative models trained with data from our own digital activity, in order to generate another set of books with our foreseeable activity for the year 2050. Both projects invite us to consider from a critical perspective the present and future of the social transformations produced by Big Data and AI. © 2023, Universitat Oberta de Catalunya. All rights reserved.
"
10.14733/cadaps.2023.S7.60-71,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146742203&origin=inward,Article,SCOPUS_ID:85146742203,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cda-based parallel adi computing algorithm of three-dimensional structure design of contemporary ceramic sculpture,"
AbstractView references

With the development of the times, the progress of society, the integration and innovation of modern science and technology and traditional culture and art, the modeling and decoration design of ceramic sculpture products have developed to the present day, and there are progress and development in different levels in both the production technology and the expression techniques. Ceramic sculpture design can not only be expressed on drawings, but also be simulated by computer modeling and decoration, and the scheme can be modified at any time, so that the preset effect of ceramic sculpture products in the future and the unique glaze color effect can be virtually preset. Aiming at the design of large-scale management information, this paper proposes an object-oriented analysis tool, strategy and model of the extended information system, and applies iVsualModelo: 2.0 based on ""Unified Modeling Language"" UML as the design tool of ceramic sculpture management information system, realizing the high integration of object-oriented analysis, object-oriented design and object-oriented programming. This paper analyzes and studies the application of computer aided design technology in all aspects of product design, and further proposes the application field of virtual reality technology in the research process of ceramic sculpture products. The overall artistic image of ceramic sculpture products is displayed through the cooperation of other animation functions and later software. © 2023 CAD Solutions, LLC, http://www.cad-journal.net.
"
10.3390/s23020826,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146400259&origin=inward,Article,SCOPUS_ID:85146400259,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),machine learning models for the automatic detection of exercise thresholds in cardiopulmonary exercising tests: from regression to generation to explanation,"
AbstractView references

The cardiopulmonary exercise test (CPET) constitutes a gold standard for the assessment of an individual’s cardiovascular fitness. A trend is emerging for the development of new machine-learning techniques applied to the automatic process of CPET data. Some of these focus on the precise task of detecting the exercise thresholds, which represent important physiological parameters. Three are the major challenges tackled by this contribution: (A) regression (i.e., the process of correctly identifying the exercise intensity domains and their crossing points); (B) generation (i.e., the process of artificially creating a CPET data file ex-novo); and (C) explanation (i.e., proving an interpretable explanation about the output of the machine learning model). The following methods were used for each challenge: (A) a convolutional neural network adapted for multi-variable time series; (B) a conditional generative adversarial neural network; and (C) visual explanations and calculations of model decisions have been conducted using cooperative game theory (Shapley’s values). The results for the regression, generation, and explanatory techniques for AI-assisted CPET interpretation are presented here in a unique framework for the first time: (A) machine learning techniques reported an expert-level accuracy in the classification of exercise intensity domains; (B) experts are not able to substantially differentiate between a real vs an artificially generated CPET; and (C) Shapley’s values can provide an explanation about the choices of the algorithms in terms of ventilatory variables. With the aim to increase their technology-readiness level, all the models discussed in this contribution have been incorporated into a free-to-use Python package called pyoxynet (ver. 12.1). This contribution should therefore be of interest to major players operating in the CPET device market and engineering. © 2023 by the author.
"
10.3390/su15010737,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146022476&origin=inward,Article,SCOPUS_ID:85146022476,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),study on height prediction of water flowing fractured zone in deep mines based on weka platform,"
AbstractView references

Accurately predicting the height of water flowing fractured zone is of great significance to coal mine safety mining. In recent years, most mines in China have entered deep mining. Aiming at the problem that it is difficult to accurately predict the height of water flowing fractured zone under the condition of large mining depth, the mining depth, height mining, inclined length of working face and coefficient of hard rock lithology ratio are selected as the main influencing factors of the height of water flowing fractured zone. The relationship between various factors and the height of water flowing fractured zone is analyzed by SPSS software. Based on the data mining tool Weka platform, Bayesian classifier, artificial neural network and support vector machine model are used to mine and analyze the measured data of water flowing fractured zone, and the detailed accuracy, confusion matrix and node error rate are compared. The results show that, the accuracy rate of instance classification of the three models is greater than 60%. The accuracy of the artificial neural network model is the highest and the node error rate is the lowest. In general, the training effect of the artificial neural network model is the best. By predicting engineering examples, the prediction accuracy of the model reaches 80%, and a good prediction effect is obtained. The height prediction system of water flowing fractured zone is developed based on VB language, which can provide a reference for the prediction of the height failure grade of water flowing fractured zone. © 2022 by the authors.
"
10.1080/10494820.2022.2160467,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145506145&origin=inward,Article,SCOPUS_ID:85145506145,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),developing an early-warning system through robotic process automation: are intelligent tutoring robots as effective as human teachers?,"
AbstractView references

Artificial intelligence aims to restructure and process re-engineering education and teaching processes and accelerate the evolution of the whole education system from information to intelligence. Robotic Process Automation (RPA) robots learn by observing people at work, analyzing user processes repeatedly, and adjusting or correcting automated processes. By using Natural Language Processing (NLP) and Machine Learning (ML), knowledge representation, inference, large-scale parallel computing, and Rapid Domain Adaptation, RPA robots can automatically extract the data needed for decision-making and continuously learn from users’ feedback. We have used RPA and predictive analytics to provide distance learning students with the Intelligent Tutoring Robot (ITR), which can provide an automatic response. By optimizing the ITR in the above context, we have examined the feasibility of transforming a prediction model, using a student learning database, into an early-warning system. This article adopts the randomized control-group pretest-posttest design, dividing 123 students into a control group to describe interactions between ITR and students and experimental groups to describe interactions between human teachers and students. The findings present no significant difference between the control and the experimental groups in terms of academic performance, however higher average marks were achieved in the former group. © 2023 Informa UK Limited, trading as Taylor & Francis Group.
"
10.1007/978-3-031-15928-2_16,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140463248&origin=inward,Conference Paper,SCOPUS_ID:85140463248,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an engineering method to evaluate care processes and introduce televisits,"
AbstractView references

During past years, the pandemic has revealed the importance of having a solid care system prepared to face emergencies. In this context, digital solutions demonstrated a high potential in dealing with critical conditions and ensuring the delivery of care. However, telemedicine has not yet succeeded in becoming a stable part of ordinary care. The integration of innovative telemedicine technologies with a set of well-organized activities plays a crucial role in the release of high-quality services. Processes modeling before the introduction of telemedicine services is a leverage to prepare the base for an effective and efficient shift to digital care. Hence, the present research customizes a modeling technique in four steps for a preliminary analysis of processes where to introduce televisits. A special attention is given to the collection of consistent knowledge about care processes, often lack and incomplete in public hospitals scenarios. The approach has been applied to the AS-IS process of the heart failure clinic of a large Italian hospital before the introduction of televisits. Integrated Definition for Function Modeling (IDEF) diagrams have allowed the hierarchical decomposition of complex phases in simpler tasks, the acquisition of consciousness and the updating of information. Diagrams have been created and used as a source of a common language to discuss about weaknesses of the current process and its possible improvements. Obstacles to the upcoming televisits services have been objectively highlighted, such as the need to reduce employed applications, the removal of printed material and the streamlining of unnecessary operations. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1109/TVCG.2022.3209479,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139820048&origin=inward,Article,SCOPUS_ID:85139820048,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),interactive and visual prompt engineering for ad-hoc task adaptation with large language models,"
AbstractView references

State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases. © 2022 IEEE.
"
10.1007/978-981-19-2600-6_10,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139140130&origin=inward,Book Chapter,SCOPUS_ID:85139140130,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ontology-driven scientific literature classification using clustering and self-supervised learning,"
AbstractView references

The rapid growth of scientific literature in the fields of computer engineering (CE) and computer science (CS) presents difficulties to researchers who are interested in exploring research publication records based on standard scientific categories. This urges the need for a context-aware, automatic classification of text documents into standard scientific categories. Document classification is a significant application of supervised learning which requires a labeled dataset for training the classifier. However, research publication records available on Google Scholar and dblp services are not labeled. First, manual annotation of a large body of scientific research work based on standard scientific terminology requires domain expertise and is extremely time-consuming. Second, hierarchical labeling of records facilitates a more effective and context-aware retrieval of documents. In this paper, we propose an ontology-driven classification technique based on zero-shot learning in conjunction with agglomerative clustering to automatically label a scientific literature dataset related to CE and CS. We further study and compare the effectiveness of multiple text classifiers such as logistic regression (LR), support vector machines (SVM), gradient boosting with Word2vec and bag of words (BOW) embedding, recurrent neural networks (RNN) with GloVe embedding, and feed-forward neural networks with BOW embedding. Our study showed that RNN with GloVe embedding outperforms other models with an above 0.85 F1 score on all granularity levels. Our proposed technique will help junior and experienced researchers identify new emerging technologies and domains for their research purposes. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
10.1007/s13349-022-00627-8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138749724&origin=inward,Article,SCOPUS_ID:85138749724,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative adversarial networks for labeled acceleration data augmentation for structural damage detection,"
AbstractView references

There have been major advances in the field of data science in the last few decades, and these have been utilized for different engineering disciplines and applications. Artificial intelligence (AI), machine learning (ML) and deep learning (DL) algorithms have been utilized for civil structural health Monitoring (SHM) especially for damage detection applications using sensor data. Although ML and DL methods show superior learning skills for complex data structures, they require plenty of data for training. However, in SHM, data collection from civil structures can be expensive and time taking; particularly getting useful data (damage associated data) can be challenging. The objective of this study is to address the data scarcity problem for damage detection applications. This paper employs 1-D Wasserstein Deep Convolutional Generative Adversarial Networks using Gradient Penalty (1-D WDCGAN-GP) for synthetic labelled acceleration data generation. Then, the generated data is augmented with varying ratios for the training data set of a 1-D deep convolutional neural network (1-D DCNN) for damage detection application. The damage detection results show that the 1-D WDCGAN-GP can be successfully utilized to tackle data scarcity in vibration-based damage detection applications of civil structures. © 2022, Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.1007/978-981-16-8154-7_11,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135044525&origin=inward,Conference Paper,SCOPUS_ID:85135044525,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),named entity approach for structured management of aeronautical product requirements,"
AbstractView references

Civil aircraft is a typical complex product system. It has the characteristics of high technology intensiveness, strong interdisciplinary, high system integration, long development cycle, large project investment, and complex project management. The development process of civil aircraft usually involves many stakeholders, and each stakeholder will put forward its own requirements for the aircraft development process. Therefore, the number of requirements documents and requirements items summarized in the hands of product suppliers will be very large. One way to solve the above problems is to adopt a structured expression of requirements, so that the subject, object, realization function, attribute parameter, and category involved in each requirement can be clearly expressed in the database. Aiming at the extraction of attributes such as subject and object in the demand, this topic uses the related algorithm of named entity recognition in natural language processing to identify the corresponding entity. I built a word segmentation and NER model based on the Hidden Markov Model, which has achieved good results on the test data set. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
10.1080/15732479.2022.2058562,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128232088&origin=inward,Article,SCOPUS_ID:85128232088,scopus,2023-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an efficient simplified model for high-speed railway simply supported bridge under earthquakes,"
AbstractView references

An efficient model of high-speed railway simply supported bridge with China Railway Track Structure II (CRTS II) ballastless track structure, setup on ANSYS platform, is proposed in this article. The goal is to improve the low computational efficiency and reduce the high computer memory consumption caused by complex bridge models of numerous spans. Programming language and modeling procedures are utilized to ensure that all spans in the proposed model of a large multispan simply supported railway bridge have consistent boundary conditions. Hence, only the middle span is simulated. The comparison of the first longitudinal vibration modes and first natural frequencies of the proposed model and the conventional simply supported bridge model preliminarily validates the feasibility of the model. It also confirms the range of the span number that the proposed model can simulate with accuracy. Taking into account eight different seismic excitations, the results further prove that the proposed model can significantly improve the computational efficiency of the seismic analyses, and can decrease the modeling complexity and calculation time cost without loss of accuracy. The design verification of three different heights of pier groups and the consideration of nonlinear springs enables the proposed model to be utilized in a wide range of applications. As a conclusion, for seismic simulation of high-speed railway simply supported bridges, the proposed model is a reliable alternative for practical engineering simulation. © 2022 Informa UK Limited, trading as Taylor & Francis Group.
"
10.1021/acsnano.2c07681,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143085062&origin=inward,Article,SCOPUS_ID:85143085062,scopus,2022-12-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),end-to-end protein normal mode frequency predictions using language and graph models and application to sonification,"
AbstractView references

The prediction of mechanical and dynamical properties of proteins is an important frontier, especially given the greater availability of proteins structures. Here we report a series of models that provide end-to-end predictions of nanodynamical properties of proteins, focused on high-throughput normal mode predictions directly from the amino acid sequence. Using neural network models within the family of Natural Language Processing and graph-based methods, we offer atomistically based mechanistic predictions of key protein mechanical features. The models include an end-to-end long short-term memory (LSTM) model, an end-to-end transformer model, a graph-based transformer model, and an equivariant graph neural network. All four models show exceptional performance, with the graph-based transformer architecture offering the best results but at the cost of requiring a graph structure as input. Conversely, the LSTM and transformer models offer end-to-end sequence-to-property prediction capabilities, providing efficient avenues for protein engineering, analysis, and design. We compare our results against published data based on a Principal Neighborhood Aggregation graph neural network, revealing that the transformer model offers better performance while also being able to predict a large set of the first 64 normal mode frequencies, simultaneously. The use of the end-to-end transformer model may facilitate other downstream applications through the use of transfer learning, and it offers a comprehensive prediction of dynamical properties without any structural knowledge, directly from the amino acid sequence. We demonstrate a potential application in scientific sonification, where the normal mode frequencies are transposed to generate audible signals for a detailed analysis of subtle changes of protein sequences. © 2022 American Chemical Society.
"
10.5755/j01.itc.51.4.31394,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143804873&origin=inward,Article,SCOPUS_ID:85143804873,scopus,2022-12-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),dliq: a deterministic finite automaton learning algorithm through inverse queries,"
AbstractView references

Automaton learning has attained a renewed interest in many interesting areas of software engineering including formal verification, software testing and model inference. An automaton learning algorithm typically learns the regular language of a Deterministic Finite Automaton (DFA) with the help of queries. These queries are posed by the learner (Learning Algorithm) to a Minimally Adequate Teacher (MAT). The MAT can generally answer two types of queries asked by the learning algorithm; membership queries and equivalence queries. Learning algorithms can be categorized into three broad categories: incremental, sequential and complete learning algorithms. Likewise, these can be designed for 1-bit learning or k-bit learning. Existing automaton learning algorithms have polynomial (at-least cubic) time complexity in the presence of a MAT. Therefore, sometimes these algorithms are unable to learn large complex software systems. In this research work, we have reduced the time complexity of the DFA learning into lower bounds (from cubic to square form). For this, we introduce an efficient complete DFA learning algorithm through Inverse Queries (DLIQ) based on the concept of inverse queries introduced by John Hopcroft for state minimization of a DFA. The DLIQ algorithm takes O(| Ps || F | + | Σ | N) complexity in the presence of a MAT which is also equipped to answer inverse queries. We give a theoretical analysis of the proposed algorithm along with providing an empirical analysis of DLIQ and ID (Identification of regular languages) algorithms. For this, we implement an evaluation framework. Results depict that in terms of time complexity our proposed algorithm DLIQ is more efficient than the ID algorithm. © 2022, Kauno Technologijos Universitetas. All rights reserved.
"
10.4162/nrp.2022.16.6.801,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143229213&origin=inward,Article,SCOPUS_ID:85143229213,scopus,2022-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),challenges of diet planning for children using artificial intelligence,"
AbstractView references

BACKGROUND/OBJECTIVES: Diet planning in childcare centers is difficult because of the required knowledge of nutrition and development as well as the high design complexity associated with large numbers of food items. Artificial intelligence (AI) is expected to provide diet-planning solutions via automatic and effective application of professional knowledge, addressing the complexity of optimal diet design. This study presents the results of the evaluation of the utility of AI-generated diets for children and provides related implications. MATERIALS/METHODS: We developed 2 AI solutions for children aged 3–5 yrs using a generative adversarial network (GAN) model and a reinforcement learning (RL) framework. After training these solutions to produce daily diet plans, experts evaluated the human-and AI-generated diets in 2 steps. RESULTS: In the evaluation of adequacy of nutrition, where experts were provided only with nutrient information and no food names, the proportion of strong positive responses to RL-generated diets was higher than that of the human-and GAN-generated diets (P < 0.001). In contrast, in terms of diet composition, the experts’ responses to human-designed diets were more positive when experts were provided with food name information (i.e., composition information). CONCLUSIONS: To the best of our knowledge, this is the first study to demonstrate the development and evaluation of AI to support dietary planning for children. This study demonstrates the possibility of developing AI-assisted diet planning methods for children and highlights the importance of composition compliance in diet planning. Further integrative cooperation in the fields of nutrition, engineering, and medicine is needed to improve the suitability of our proposed AI solutions and benefit children’s well-being by providing high-quality diet planning in terms of both compositional and nutritional criteria. © 2022 The Korean Nutrition Society and the Korean Society of Community Nutrition.
"
10.1109/TC.2022.3211430,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139840944&origin=inward,Article,SCOPUS_ID:85139840944,scopus,2022-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),end-to-end synthesis of dynamically controlled machine learning accelerators,"
AbstractView references

Edge systems are required to autonomously make real-time decisions based on large quantities of input data under strict power, performance, area, and other constraints. Meeting these constraints is only possible by specializing systems through hardware accelerators purposefully built for machine learning and data analysis algorithms. However, data science evolves at a quick pace, and manual design of custom accelerators has high non-recurrent engineering costs: general solutions are needed to automatically and rapidly transition from the formulation of a new algorithm to the deployment of a dedicated hardware implementation. Our solution is the SOftware Defined Architectures (SODA) Synthesizer, an end-to-end, multi-level, modular, extensible compiler toolchain providing a direct path from machine learning tools to hardware. The SODA Synthesizer frontend is based on the multilevel intermediate representation (MLIR) framework; it ingests pre-trained machine learning models, identifies kernels suited for acceleration, performs high-level optimizations, and prepares them for hardware synthesis. In the backend, SODA leverages state-of-the-art high-level synthesis techniques to generate highly efficient accelerators, targeting both field programmable devices (FPGAs) and application-specific circuits (ASICs). In this paper, we describe how the SODA Synthesizer can also assemble the generated accelerators (based on the finite state machine with datapath model) in a custom system driven by a distributed controller, building a coarse-grained dataflow architecture that does not require a host processor to orchestrate parallel execution of multiple accelerators. We show the effectiveness of our approach by automatically generating ASIC accelerators for layers of popular deep neural networks (DNNs). Our high-level optimizations result in up to 74x speedup on isolated accelerators for individual DNN layers, and our dynamically scheduled architecture yields an additional 3x performance improvement when combining accelerators to handle streaming inputs. © 1968-2012 IEEE.
"
10.1007/s12046-022-01978-0,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139250530&origin=inward,Article,SCOPUS_ID:85139250530,scopus,2022-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),biomedical event extraction on input text corpora using combination technique based capsule network,"
AbstractView references

Biomedical Event Extraction (BEE) is a demanding and prominent technology that attracts the researchers and scientists in the field of natural language processing (NLP). The conventional method relies mostly on external NLP packages and manual designed features, where the features engineering is complex and large. In addition, the conventional methods on BEE uses a pipeline process that splits a task into many sub-tasks, however, the relationship between these sub-tasks is not defined. In this paper, such limitations are avoided using the combination technique that relies on Capsule Network (CapsNet) to perform a task. The CapsNet is used for the extraction of feature representation from the input corpora and then the combination technique reconstructs the events from RNN output. This method extracts the tasks from a BEE over several annotated corpora that extract the events from the molecular level in case of multi-level events. The proposed model is compared with state-of-the-art models over various text corpora datasets. The results show an improved rate of accuracy of CapsNet classification over cancer biomedical events than the existing methods. © 2022, Indian Academy of Sciences.
"
10.1007/s10664-022-10206-6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138474182&origin=inward,Article,SCOPUS_ID:85138474182,scopus,2022-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),extracting enhanced artificial intelligence model metadata from software repositories,"
AbstractView references

While artificial intelligence (AI) models have improved at understanding large-scale data, understanding AI models themselves at any scale is difficult. For example, even two models that implement the same network architecture may differ in frameworks, datasets, or even domains. Furthermore, attempting to use either model often requires much manual effort to understand it. As software engineering and AI development share many of the same languages and tools, techniques in mining software repositories should enable more scalable insights into AI models and AI development. However, much of the relevant metadata around models are not easily extractable. This paper (an extension of our MSR 2020 paper) presents a library called AIMMX for AI Model Metadata eXtraction from software repositories into enhanced metadata that conforms to a flexible metadata schema. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-the-art AI papers. We also explored how AIMMX can enable studies and tools to advance engineering support for AI development. As preliminary examples, we present an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. We also demonstrate the flexibility of extracted metadata by using the evaluation dataset in an existing natural language processing (NLP) analysis platform to identify trends in the dataset. Overall, we hope AIMMX fosters research towards better AI development. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
"
10.1016/j.actaastro.2022.08.028,S0094576522004441,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137737742&origin=inward,Article,SCOPUS_ID:85137737742,scopus,2022-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design of impulsive asteroid flybys and scheduling of time-minimal optimal control arcs for the construction of a dyson ring (gtoc 11),"
                  This paper describes the approach used by the team named the Eccentric Anomalies to obtain the sixth best solution to the problem of the eleventh Global Trajectory Optimization Competition, whose futuristic scenario of Dyson sphere building remained mathematically relevant for current space mission design and engineering in general. As usual for this recurring challenge, it involved large-scale combinatorics at a high level and a multitude of optimal control problems at a lower level. Furthermore it proposed additional layers of complexity by adding a strong scheduling component to the usual flyby sequencing, and by featuring both impulsive and continuous-thrust trajectories. The authors took advantage of modern theoretical techniques and open-source tools to put together a sequential process including analysis based on analytical trajectory models, tree searches using efficient data structures, global and local finite-dimensional optimization and multi-objective trade-offs. The optimal control part was both tackled with direct transcription as well as indirect shooting methods, and the mixed-integer scheduling reformulated as a bi-level optimization. From a programming point of view, the main framework was set in an interpreted language whilst using as much as possible dependencies written in compiled ones for speed.
               "
10.3389/fdgth.2022.1025086,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144033104&origin=inward,Article,SCOPUS_ID:85144033104,scopus,2022-11-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),workflow for health-related and brain data lifecycle,"
AbstractView references

Poor lifestyle leads potentially to chronic diseases and low-grade physical and mental fitness. However, ahead of time, we can measure and analyze multiple aspects of physical and mental health, such as body parameters, health risk factors, degrees of motivation, and the overall willingness to change the current lifestyle. In conjunction with data representing human brain activity, we can obtain and identify human health problems resulting from a long-term lifestyle more precisely and, where appropriate, improve the quality and length of human life. Currently, brain and physical health-related data are not commonly collected and evaluated together. However, doing that is supposed to be an interesting and viable concept, especially when followed by a more detailed definition and description of their whole processing lifecycle. Moreover, when best practices are used to store, annotate, analyze, and evaluate such data collections, the necessary infrastructure development and more intense cooperation among scientific teams and laboratories are facilitated. This approach also improves the reproducibility of experimental work. As a result, large collections of physical and brain health-related data could provide a robust basis for better interpretation of a person’s overall health. This work aims to overview and reflect some best practices used within global communities to ensure the reproducibility of experiments, collected datasets and related workflows. These best practices concern, e.g., data lifecycle models, FAIR principles, and definitions and implementations of terminologies and ontologies. Then, an example of how an automated workflow system could be created to support the collection, annotation, storage, analysis, and publication of findings is shown. The Body in Numbers pilot system, also utilizing software engineering best practices, was developed to implement the concept of such an automated workflow system. It is unique just due to the combination of the processing and evaluation of physical and brain (electrophysiological) data. Its implementation is explored in greater detail, and opportunities to use the gained findings and results throughout various application domains are discussed. 2022 Brůha, Mouček, Salamon and Vacek.
"
10.1016/j.eswa.2022.117957,S0957417422011927,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133256801&origin=inward,Article,SCOPUS_ID:85133256801,scopus,2022-11-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),fusing feature engineering and deep learning: a case study for malware classification,"Machine learning has become an appealing signature-less approach to detect and classify malware because of its ability to generalize to never-before-seen samples and to handle large volumes of data. While traditional feature-based approaches rely on the manual design of hand-crafted features based on experts’ knowledge of the domain, deep learning approaches replace the manual feature engineering process by an underlying system, typically consisting of a neural network with multiple layers, that perform both feature learning and classification altogether. However, the combination of both approaches could substantially enhance detection systems. In this paper we present an hybrid approach to address the task of malware classification by fusing multiple types of features defined by experts and features learned through deep learning from raw data. In particular, our approach relies on deep learning to extract N-gram like features from the assembly language instructions and the bytes of malware, and texture patterns and shapelet-based features from malware’s grayscale image representation and structural entropy, respectively. These deep features are later passed as input to a gradient boosting model that combines the deep features and the hand-crafted features using an early-fusion mechanism. The suitability of our approach has been evaluated on the Microsoft Malware Classification Challenge benchmark and results show that the proposed solution achieves state-of-the-art performance and outperforms gradient boosting and deep learning methods in the literature."
10.1145/3567512.3567535,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147007251&origin=inward,Conference Paper,SCOPUS_ID:85147007251,scopus,2022-11-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),partial loading of repository-based models through static analysis,"
AbstractView references

As the size of software and system models grows, scalability issues in the current generation of model management languages (e.g. transformation, validation) and their supporting tooling become more prominent. To address this challenge, execution engines of model management programs need to become more efficient in their use of system resources. This paper presents an approach for partial loading of large models that reside in graph-database-backed model repositories. This approach leverages sophisticated static analysis of model management programs and auto-generation of graph (Cypher) queries to load only relevant model elements instead of naively loading the entire models into memory. Our experimental evaluation shows that our approach enables model management programs to process larger models, faster, and with a reduced memory footprint compared to the state of the art. © 2022 ACM.
"
10.1145/3567512.3567534,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143075634&origin=inward,Conference Paper,SCOPUS_ID:85143075634,scopus,2022-11-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),neural language models and few shot learning for systematic requirements processing in mdse,"
AbstractView references

Systems engineering, in particular in the automotive domain, needs to cope with the massively increasing numbers of requirements that arise during the development process. The language in which requirements are written is mostly informal and highly individual. This hinders automated processing of requirements as well as the linking of requirements to models. Introducing formal requirement notations in existing projects leads to the challenge of translating masses of requirements and the necessity of training for requirements engineers. In this paper, we derive domain-specific language constructs helping us to avoid ambiguities in requirements and increase the level of formality. The main contribution is the adoption and evaluation of few-shot learning with large pretrained language models for the automated translation of informal requirements to structured languages such as a requirement DSL. © 2022 ACM.
"
10.1145/3569219.3569352,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142636412&origin=inward,Conference Paper,SCOPUS_ID:85142636412,scopus,2022-11-16,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the creativity of text-to-image generation,"
AbstractView references

Text-guided synthesis of images has made a giant leap towards becoming a mainstream phenomenon. With text-to-image generation systems, anybody can create digital images and artworks. This provokes the question of whether text-to-image generation is creative. This paper expounds on the nature of human creativity involved in text-to-image art (so-called ""AI art"") with a specific focus on the practice of prompt engineering. The paper argues that the current product-centered view of creativity falls short in the context of text-to-image generation. A case exemplifying this shortcoming is provided and the importance of online communities for the creative ecosystem of text-to-image art is highlighted. The paper provides a high-level summary of this online ecosystem drawing on Rhodes' conceptual four P model of creativity. Challenges for evaluating the creativity of text-to-image generation and opportunities for research on text-to-image generation in the field of Human-Computer Interaction (HCI) are discussed. © 2022 ACM.
"
10.1145/3540250.3549162,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143078683&origin=inward,Conference Paper,SCOPUS_ID:85143078683,scopus,2022-11-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"natgen: generative pre-training by ""naturalizing"" source code","
AbstractView references

Pre-trained Generative Language models (e.g., PLBART, CodeT5, SPT-Code) for source code yielded strong results on several tasks in the past few years, including code generation and translation. These models have adopted varying pre-training objectives to learn statistics of code construction from very large-scale corpora in a self-supervised fashion; the success of pre-trained models largely hinges on these pre-training objectives. This paper proposes a new pre-training objective, ""Naturalizing""of source code, exploiting code's bimodal, dual-channel (formal & natural channels) nature. Unlike natural language, code's bimodal, dual-channel nature allows us to generate semantically equivalent code at scale. We introduce six classes of semantic preserving transformations to introduce unnatural forms of code, and then force our model to produce more natural original programs written by developers. Learning to generate equivalent, but more natural code, at scale, over large corpora of open-source code, without explicit manual supervision, helps the model learn to both ingest & generate code. We fine-tune our model in three generative Software Engineering tasks: code generation, code translation, and code refinement with limited human-curated labeled data and achieve state-of-the-art performance rivaling CodeT5. We show that our pre-trained model is especially competitive at zero-shot and few-shot learning, and better at learning code properties (e.g., syntax, data flow) © 2022 ACM.
"
10.1145/3540250.3549147,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143074526&origin=inward,Conference Paper,SCOPUS_ID:85143074526,scopus,2022-11-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),neudep: neural binary memory dependence analysis,"
AbstractView references

Determining whether multiple instructions can access the same memory location is a critical task in binary analysis. It is challenging as statically computing precise alias information is undecidable in theory. The problem aggravates at the binary level due to the presence of compiler optimizations and the absence of symbols and types. Existing approaches either produce significant spurious dependencies due to conservative analysis or scale poorly to complex binaries. We present a new machine-learning-based approach to predict memory dependencies by exploiting the model's learned knowledge about how binary programs execute. Our approach features (i) a self-supervised procedure that pretrains a neural net to reason over binary code and its dynamic value flows through memory addresses, followed by (ii) supervised finetuning to infer the memory dependencies statically. To facilitate efficient learning, we develop dedicated neural architectures to encode the heterogeneous inputs (i.e., code, data values, and memory addresses from traces) with specific modules and fuse them with a composition learning strategy. We implement our approach in NeuDep and evaluate it on 41 popular software projects compiled by 2 compilers, 4 optimizations, and 4 obfuscation passes. We demonstrate that NeuDep is more precise (1.5x) and faster (3.5x) than the current state-of-the-art. Extensive probing studies on security-critical reverse engineering tasks suggest that NeuDep understands memory access patterns, learns function signatures, and is able to match indirect calls. All these tasks either assist or benefit from inferring memory dependencies. Notably, NeuDep also outperforms the current state-of-the-art on these tasks. © 2022 ACM.
"
10.1145/3540250.3558926,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143054512&origin=inward,Conference Paper,SCOPUS_ID:85143054512,scopus,2022-11-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),coreqqa: a compliance requirements understanding using question answering tool,"
AbstractView references

We introduce COREQQA, a tool for assisting requirements engineers in acquiring a better understanding of compliance requirements by means of automated Question Answering. Extracting compliance-related requirements by manually navigating through a legal document is both time-consuming and error-prone. COREQQA enables requirements engineers to pose questions in natural language about a compliance-related topic given some legal document, e.g., asking about data breach. The tool then automatically navigates through the legal document and returns to the requirements engineer a list of text passages containing the possible answers to the input question. For better readability, the tool also highlights the likely answers in these passages. The engineer can then use this output for specifying compliance requirements. COREQQA is developed using advanced large-scale language models from BERT's family. COREQQA has been evaluated on four legal documents. The results of this evaluation are briefly presented in the paper. The tool is publicly available on Zenodo (https://doi.org/10.5281/zenodo.6653514). © 2022 Owner/Author.
"
10.1016/j.iswa.2022.200158,S2667305322000953,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143595260&origin=inward,Article,SCOPUS_ID:85143595260,scopus,2022-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),identification and classification of road traffic incidents in panama city through the analysis of a social media stream and machine learning,"In Panama City, Panama, as in many cities, the large number of cars on the roads and random traffic events produce constant and extensive traffic jams. These issues are usually not solved even with the construction of more traffic lanes. This work proposes the development of a system that allows the visualization of information published on social media about traffic incidents. Feature engineering methods, such as: Count Vectors and TF-IDF, were applied to process the tweets into structured data. Machine Learning models were created for the classification of traffic related tweets using SVM, Naïve Bayes, Random Forest and XGBoost. The prediction models resulted in two: a classification model that detects incident or non-incident tweets and a categorization model which determines the type of incident (accident, danger or obstacle). Results show that there were approximately 200,000 tweets reporting, traffic incidents since 2014 to 2022. In terms of the classification model, a precision of over 92% was achieved and for categorization over 97%. The best results were obtained with the use of Count Vectors and a Random Forest model. Finally, a graphical interface was developed to show the results of the obtained data and the streaming of live tweets, deployed at the website http://www.traficoya-pty.com/. This system has advantages such as speeding up the detection and visualization of traffic incidents, which can be of great help to the country’s traffic authorities and the general public."
10.1016/j.patrec.2022.10.001,S016786552200294X,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139863917&origin=inward,Article,SCOPUS_ID:85139863917,scopus,2022-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an improved entity recognition approach to cyber-social knowledge provision of intellectual property using a crf-lstm model,"
                  With the development of cutting-edge IT technologies, e.g. Big Data, Knowledge Engineering, etc., traditional Intellectual Property (IP) services have depicted high redundancy and low efficiency during management of such large-scale of data. Recent advancement of Artificial Intelligence (AI) and Deep Learning (DL) models has been accelerating relevant research activities being investigated on Knowledge Graph (KG) schemes and applications in different domains, such as medical services, social media, etc. However, when IP services and their cyber-social provision are taken into account, relevant approaches suffer from unbalanced labels against training results, and inappropriate evaluation metrics not well reflecting the impact of the unbalance. In this paper, a deep learning model combining Conditional Random Field and Bidirectional LSTM has been proposed, in order to achieve named entity recognition with unbalanced labels. An adaptive metric, G-Score was introduced to compare the fitting ability of models by evaluating the gap between Precision and Recall. According to the results, the proposed model can effectively recognize the potential named entities with outperformance over other relevant models.
               "
10.1016/j.jisa.2022.103311,S2214212622001600,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137166572&origin=inward,Article,SCOPUS_ID:85137166572,scopus,2022-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),obfuscation detection in android applications using deep learning,"
                  Malware is often hidden in illegitimately cloned software. Android, with over two billions active devices, is one of the most affected platforms because code cloning is quite simple and there are several not controlled markets. Obfuscation is both a cause and a solution to this scenario: a cause because obfuscated malware is harder to detect, a solution because obfuscation of legitimate applications makes code cloning more difficult. A deeper understanding of the obfuscation techniques would lead to more effective and aware use. In the literature, there are few methods of obfuscation detection with limited accuracy. Manual reverse engineering is too time-consuming to achieve this purpose, we need faster and automated techniques.
                  In this work, we propose several deep learning models that can detect and classify the presence of obfuscation in Android applications. In addition to classical ML methods, we leverage natural language processing or image recognition approaches, then with a hybrid model, we exploit the best of each approach. Tests over a large dataset, made using different obfuscation tools, showed improvements compared to previous obfuscation detection methods. We target four obfuscation classes: identifier renaming, string encryption, reflection and class encryption, achieving an average F-measure of 0.985.
               "
10.1016/j.egyr.2022.08.207,S2352484722016511,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136734965&origin=inward,Article,SCOPUS_ID:85136734965,scopus,2022-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),coordinated distribution of stadium electric load unloading based on artificial intelligence,"The Stadium’s Electricity (SE) is cost-effective and critical to meeting the stadium’s size requirements. Because of their complex internal operations and extensive functionality. Stadium energy efficiency improvements are demanding and time-consuming due to the large and diverse electricity loads that can be managed. The stadium source of information is unloaded and turned off after sports. During sports, the system is ready to be deployed in case of an electricity outage. Several of these issues must be carefully evaluated, starting with accurate representations and human-structure interaction models, and determining acceptable facility vibration serviceability limitations of crowd activities. Due to various factors, stadiums like those used for athletic or entertainment events are distinct from other civil engineering constructions. Stadiums are susceptible to both the coordinated and random motion of huge crowds, which presents several issues. In computer science, Artificial Intelligence (AI) refers to the development of machines that can simulate human mental processes. AI can be used in various ways, from natural language processing to speech recognition to computer vision. This paper introduced a new method for (SE-AI) in addition to enhancing stadium control capabilities in various functional areas. The intelligent stadium development effectively enables it to handle construction equipment in an integrated manner. In the stadium’s electrical design, the smart sports model is structured. An investigation of the stadium’s electrical design system to provide helpful information for the stadium’s electrical design work based on engineering design principles. Advanced technology is mirrored in the stadium’s equipment, which is moving toward informationization and intelligence due to the advancement of science and technology, especially digitalization. If the research results can be a helpful guide for bettering the electrical planning of stadiums can be a more substantial impact on society. Stadiums are investing in Wi-Fi and distributed antenna systems to boost connection. This allows viewers to follow the game throughout the stadium and helps scattered personnel communicate."
10.1016/j.compind.2022.103733,S0166361522001300,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132736347&origin=inward,Article,SCOPUS_ID:85132736347,scopus,2022-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),pretrained domain-specific language model for natural language processing tasks in the aec domain,"
                  As an essential task for the architecture, engineering, and construction (AEC) industry, information processing and acquiring from unstructured textual data based on natural language processing (NLP) are gaining increasing attention. Although deep learning (DL) models for NLP tasks have been investigated for years, domain-specific pretrained DL models and their advantages are seldomly investigated in the AEC domain. Therefore, this work developed a large scale domain corpora and pretrained domain-specific language models for the AEC domain, and then systematically explores various transfer learning and fine-tuning techniques to explore the performance of pretrained DL models for various NLP tasks. First, both in-domain and close-domain Chinese corpora are developed. Then, two types of pretrained models, including static word embedding models and contextual word embedding models, are pretrained based on various domain corpora. Finally, several widely used DL models for NLP tasks are further trained and tested based on various pretrained models. The result shows that domain corpora can further improve the performance of static word embedding-based DL models and contextual word embedding-based DL models in text classification (TC) and named entity recognition (NER) tasks. Meanwhile, contextual word embedding-based DL models significantly outperform the static word embedding-based DL methods in TC and NER tasks, with maximum improvements of 8.1% and 3.8% in the F1 score, respectively. This research contributes to the body of knowledge in two ways: (1) demonstrating the advantages of domain corpora and pretrained DL models, and (2) opening the first domain-specific dataset and pretrained language models named ARCBERT for the AEC domain. Thus, this work sheds light on the adoption and application of pretrained models in the AEC domain.
               "
10.1007/s00607-022-01091-4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131065828&origin=inward,Article,SCOPUS_ID:85131065828,scopus,2022-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design of classical-quantum systems with uml,"
AbstractView references

Developers of the many promising quantum computing applications that currently exist are urging companies in many different sectors seriously consider integrating this new technology into their business. For these applications to function, not only are quantum computers required, but quantum software also. Accordingly, quantum software engineering has become an important research field, in that it attempts to apply or adapt existing methods and techniques (or propose new ones) for the analysis, design, coding, and testing of quantum software, as well as playing a key role in ensuring quality in large-scale productions. The design of quantum software nevertheless poses two main challenges: the modelling of software quantum elements must be done in high-level modelling languages; and the need to further develop so-called “hybrid information systems”, which combine quantum and classical software. To address these challenges, we first propose a quantum UML profile for analysing and designing hybrid information systems; we then demonstrate its applicability through various structural and behavioural diagrams such as use case, class, sequence, activity, and deployment. In comparison to certain other quantum domain-specific languages, this UML profile ensures compliance with a well-known international standard that is supported by many tools and is followed by an extensive community. © 2022, The Author(s).
"
10.1109/TSE.2021.3124332,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118611511&origin=inward,Article,SCOPUS_ID:85118611511,scopus,2022-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai-enabled automation for completeness checking of privacy policies,"
AbstractView references

Technological advances in information sharing have raised concerns about data protection. Privacy policies contain privacy-related requirements about how the personal data of individuals will be handled by an organization or a software system (e.g., a web service or an app). In Europe, privacy policies are subject to compliance with the General Data Protection Regulation (GDPR). A prerequisite for GDPR compliance checking is to verify whether the content of a privacy policy is complete according to the provisions of GDPR. Incomplete privacy policies might result in large fines on violating organization as well as incomplete privacy-related software specifications. Manual completeness checking is both time-consuming and error-prone. In this paper, we propose AI-based automation for the completeness checking of privacy policies. Through systematic qualitative methods, we first build two artifacts to characterize the privacy-related provisions of GDPR, namely a conceptual model and a set of completeness criteria. Then, we develop an automated solution on top of these artifacts by leveraging a combination of natural language processing and supervised machine learning. Specifically, we identify the GDPR-relevant information content in privacy policies and subsequently check them against the completeness criteria. To evaluate our approach, we collected 234 real privacy policies from the fund industry. Over a set of 48 unseen privacy policies, our approach detected 300 of the total of 334 violations of some completeness criteria correctly, while producing 23 false positives. The approach thus has a precision of 92.9% and recall of 89.8%. Compared to a baseline that applies keyword search only, our approach results in an improvement of 24.5% in precision and 38% in recall. © 1976-2012 IEEE.
"
10.1145/3550356.3558515,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142921027&origin=inward,Conference Paper,SCOPUS_ID:85142921027,scopus,2022-10-23,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),simplify model design in mde approaches,"
AbstractView references

Many academic works prove that Model-Driven Engineering (MDE) improves the quality of the application's production in software engineering. However, its adoption remains limited for several reasons: The complexity of the tools makes them less productive, the modeling languages and the mechanisms manipulating the models are hard to grasp. In that sense, low-code platforms are an interesting proposition. These low-code platforms, despite their simplicity of use, do not allow the design of large systems because of the opacity of the various artifacts (models, code, generation..). In our work, we want to improve the adoption of MDE models, tools, and mechanisms in application design process through a more simplified interaction. Rather than constraining the developer in a closed environment as it is the case with the Low-code platforms, we wish to allow him to have access, if he wishes, to models as well as the generation mechanisms of the application. We propose to build applications that are instrumented with modification actions on each element of their user interface. The applications are generated from models. A developer can modify or develop his application directly from the UI by calling upon the added modification actions. This makes his work easier by quickly locating (in the UI of the produced application) where to make the modifications and directly visualizing the result, rather than manually modifying each part of the model concerned. The instrumentation is based on a descriptive language for possible modification actions and a mechanism for injecting this language into the application generation chain. We have designed a demonstrator allowing us to conduct experiments, with various audience, in order to validate our work. © 2022 ACM.
"
10.1145/3546155.3547294,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140891049&origin=inward,Conference Paper,SCOPUS_ID:85140891049,scopus,2022-10-08,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),in search of good ancestors / ahnen in arbeit,"
AbstractView references

In Search of Good Ancestors / Ahnen in Arbeit is a year-long generative radio artwork investigating the concept of machine learning algorithms as instable memory. A 24-hour broadcast on German Public Radio features the voice(s) of an artificial BroadCaster, a bespoke instrument utilizing deep learning text and speech models that begin pre-trained on ubiquitous public research datasets [Gao et al. 2021] [Ito and Johnson 2017]. The BroadCaster evolves via trainings on small-scale datasets created collaboratively in a series of public workshops. Each workshop is unique, run in various European locations or online, and reflects the subjectivities of each group of participants. They serve as a framework for developing practice-based approaches to activities such as curating training texts, designing annotations for expressive text generation, performing with voice clones or recording voice data in both individual and collective scenarios. Following a workshop, the collected data is reviewed and used to fine-tune the BroadCaster's models and inspire aesthetic inventions in the compositions and vocal identities within the broadcast. The work-plus-workshops navigates themes of long-term and intergenerational thinking - using as its seminal text American virologist Jonas Salk's 1977 lecture ""Are we being good ancestors?"". Here, Salk, a reknown altruist, calls for the cultures of ""the West""to make intergenerational responsibility the highest moral imperative [Salk 1992]. Through the year the BroadCaster repeatedly attempts to extrapolate on this lecture, its predictions altered by the contributions of texts, stylistic annotations and voice recordings collected in the workshops. Thus, new vocal identities, dialogic structures and thematics emerge, while those of the initial and previous trainings decay over time. The work proposes a methodology that embraces the inherent instability of the statistical model as a memory and its tendencies to catastrophically forget when trained on data whose diversity unfolds over time. The value of public workshops as artistic method relies on the diverse backgrounds of the participants, who are not experts in machine learning. Their willingness or resistance to contribute speech recordings to the broadcast, the joys and frustrations of those from different language backgrounds yield a form of tacit exploration and appropriation of open source text and speech technologies. This approach allows a plurality of intentions and (literal) voices to become part of the work in a way that resonates with its thematics. This project is ongoing and the methods of the project are in continual development. Each workshop informs the next with new insights into the poetics and politics of data collection and model training. The broadcast streams from January 2022 - February 2023 at ahnen.in , while fragments are played intermittently on German Public Radio. The workshops' activities were conceived in collaboration with artist-researcher Eleni Ikoniadou, and Joana Chicau, amy pickles and Cristina Cochior of VARIA. This work is supported by CTM Festival , Deutschlandfunk Kultur and the Leverhulme Trust. © 2022 ACM.
"
10.3745/JIPS.02.0182,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141751581&origin=inward,Article,SCOPUS_ID:85141751581,scopus,2022-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),feature analysis for detecting mobile application review generated by ai-based language model,"
AbstractView references

Mobile applications can be easily downloaded and installed via markets. However, malware and malicious applications containing unwanted advertisements exist in these application markets. Therefore, smartphone users install applications with reference to the application review to avoid such malicious applications. An application review typically comprises contents for evaluation; however, a false review with a specific purpose can be included. Such false reviews are known as fake reviews, and they can be generated using artificial intelligence (AI)-based text-generating models. Recently, AI-based text-generating models have been developed rapidly and demonstrate high-quality generated texts. Herein, we analyze the features of fake reviews generated from Generative Pre-Training-2 (GPT-2), an AI-based text-generating model and create a model to detect those fake reviews. First, we collect a real human-written application review from Kaggle. Subsequently, we identify features of the fake review using natural language processing and statistical analysis. Next, we generate fake review detection models using five types of machine-learning models trained using identified features. In terms of the performances of the fake review detection models, we achieved average F1-scores of 0.738, 0.723, and 0.730 for the fake review, real review, and overall classifications, respectively. © 2022 KIPS
"
10.1002/cbdv.202200651,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138568740&origin=inward,Article,SCOPUS_ID:85138568740,scopus,2022-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),classifying refugee status using common features in emr**,"
AbstractView references

Automated and accurate identification of refugees in healthcare databases is a critical first step to investigate healthcare needs of this vulnerable population and improve health disparities. In this study, we developed a machine-learning method, named refugee identification system (RIS) to address this need. We curated a data set consisting of 103 refugees and 930 non-refugees in Arizona. We compiled de-identified individual-level information including age, primary language, and noise-masked home address, state-level refugee resettlement statistics, and world language statistics. We then performed feature engineering to convert language and masked address into quantitative features. Finally, we built a random forest model to classify refugee and non-refugees. RIS achieved high classification accuracy (overall accuracy=0.97, specificity=0.99, sensitivity=0.85, positive predictive value=0.88, negative predictive value=0.98, and area under receiver operating characteristic curve=0.98). RIS is customizable for refugee identification outside Arizona. Its application enables large-scale investigation of refugee healthcare needs and improvement of health disparities. © 2022 Wiley-VHCA AG, Zurich, Switzerland.
"
10.1016/j.autcon.2022.104483,S0926580522003569,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134205293&origin=inward,Article,SCOPUS_ID:85134205293,scopus,2022-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),intelligent question and answer system for building information modeling and artificial intelligence of things based on the bidirectional encoder representations from transformers model,"
                  In recent years, building information modeling and artificial intelligence of things (BIM-AIOTs) in the construction industry have gained much attention. Construction engineers and researchers learn about BIM-AIOT and increase their professional knowledge through internet searches. However, the large amount of information on the internet makes it difficult to find specific information. Although some previous work of BIM-related searches exists, most still search with a combination of keywords or longer terms. This paper utilizes a machine learning model with natural language processing (NLP) technique of bidirectional encoder representations from transformers (BERT) integrated with a mobile chatbot as a question and answer (QnA) system. The dataset used for modeling contained 3334 text paragraphs that shortened to 10,002 questions. The result shows an F1 score of around 65% accuracy, which is acceptable for model prediction. Then, the system verifies to synchronize to the server and user interface. The system works well for information search and offers a supporting automation information system in the construction industry. This study achieved conversational machine understanding and a user-friendly BIM-AIOT integration information searches platform. The proposed system has a reliable research-based information source. It is verified as an effective and efficient way to produce fast decision-making. The system is deemed a future application for research-based problem-solving solutions in Architecture, Engineering, and Construction (AEC).
               "
10.1109/TSE.2021.3106572,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85113305767&origin=inward,Article,SCOPUS_ID:85113305767,scopus,2022-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),learning to find usages of library functions in optimized binaries,"
AbstractView references

Much software, whether beneficent or malevolent, is distributed only as binaries, sans source code. Absent source code, understanding binaries' behavior can be quite challenging, especially when compiled under higher levels of compiler optimization. These optimizations can transform comprehensible, 'natural' source constructions into something entirely unrecognizable. Reverse engineering binaries, especially those suspected of being malevolent or guilty of intellectual property theft, are important and time-consuming tasks. There is a great deal of interest in tools to 'decompile' binaries back into more natural source code to aid reverse engineering. Decompilation involves several desirable steps, including recreating source-language constructions, variable names, and perhaps even comments. One central step in creating binaries is optimizing function calls, using steps such as inlining. Recovering these (possibly inlined) function calls from optimized binaries is an essential task that most state-of-the-art decompiler tools try to do but do not perform very well. In this paper, we evaluate a supervised learning approach to the problem of recovering optimized function calls. We leverage open-source software and develop an automated labeling scheme to generate a reasonably large dataset of binaries labeled with actual function usages. We augment this large but limited labeled dataset with a pre-training step, which learns the decompiled code statistics from a much larger unlabeled dataset. Thus augmented, our learned labeling model can be combined with an existing decompilation tool, Ghidra, to achieve substantially improved performance in function call recovery, especially at higher levels of optimization. © 1976-2012 IEEE.
"
10.3389/fnsys.2022.787659,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140956675&origin=inward,Article,SCOPUS_ID:85140956675,scopus,2022-09-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generalized simultaneous localization and mapping (g-slam) as unification framework for natural and artificial intelligences: towards reverse engineering the hippocampal/entorhinal system and principles of high-level cognition,"
AbstractView references

Simultaneous localization and mapping (SLAM) represents a fundamental problem for autonomous embodied systems, for which the hippocampal/entorhinal system (H/E-S) has been optimized over the course of evolution. We have developed a biologically-inspired SLAM architecture based on latent variable generative modeling within the Free Energy Principle and Active Inference (FEP-AI) framework, which affords flexible navigation and planning in mobile robots. We have primarily focused on attempting to reverse engineer H/E-S “design” properties, but here we consider ways in which SLAM principles from robotics may help us better understand nervous systems and emergent minds. After reviewing LatentSLAM and notable features of this control architecture, we consider how the H/E-S may realize these functional properties not only for physical navigation, but also with respect to high-level cognition understood as generalized simultaneous localization and mapping (G-SLAM). We focus on loop-closure, graph-relaxation, and node duplication as particularly impactful architectural features, suggesting these computational phenomena may contribute to understanding cognitive insight (as proto-causal-inference), accommodation (as integration into existing schemas), and assimilation (as category formation). All these operations can similarly be describable in terms of structure/category learning on multiple levels of abstraction. However, here we adopt an ecological rationality perspective, framing H/E-S functions as orchestrating SLAM processes within both concrete and abstract hypothesis spaces. In this navigation/search process, adaptive cognitive equilibration between assimilation and accommodation involves balancing tradeoffs between exploration and exploitation; this dynamic equilibrium may be near optimally realized in FEP-AI, wherein control systems governed by expected free energy objective functions naturally balance model simplicity and accuracy. With respect to structure learning, such a balance would involve constructing models and categories that are neither too inclusive nor exclusive. We propose these (generalized) SLAM phenomena may represent some of the most impactful sources of variation in cognition both within and between individuals, suggesting that modulators of H/E-S functioning may potentially illuminate their adaptive significances as fundamental cybernetic control parameters. Finally, we discuss how understanding H/E-S contributions to G-SLAM may provide a unifying framework for high-level cognition and its potential realization in artificial intelligences. Copyright © 2022 Safron, Çatal and Verbelen.
"
10.1145/3551349.3560417,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146963726&origin=inward,Conference Paper,SCOPUS_ID:85146963726,scopus,2022-09-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prcbert: prompt learning for requirement classification using bert-based pretrained language models,"
AbstractView references

Software requirement classification is a longstanding and important problem in requirement engineering. Previous studies have applied various machine learning techniques for this problem, including Support Vector Machine (SVM) and decision trees. With the recent popularity of NLP technique, the state-of-the-art approach NoRBERT utilizes the pre-trained language model BERT and achieves a satisfactory performance. However, the dataset PROMISE used by the existing approaches for this problem consists of only hundreds of requirements that are outdated according to today's technology and market trends. Besides, the NLP technique applied in these approaches might be obsolete. In this paper, we propose an approach of prompt learning for requirement classification using BERT-based pretrained language models (PRCBERT), which applies flexible prompt templates to achieve accurate requirements classification. Experiments conducted on two existing small-size requirement datasets (PROMISE and NFR-Review) and our collected large-scale requirement dataset NFR-SO prove that PRCBERT exhibits moderately better classification performance than NoRBERT and MLM-BERT (BERT with the standard prompt template). On the de-labeled NFR-Review and NFR-SO datasets, Trans-PRCBERT (the version of PRCBERT which is fine-tuned on PROMISE) is able to have a satisfactory zero-shot performance with 53.27% and 72.96% F1-score when enabling a self-learning strategy. © 2022 ACM.
"
10.1145/3551349.3556906,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146961467&origin=inward,Conference Paper,SCOPUS_ID:85146961467,scopus,2022-09-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"safety and performance, why not both? bi-objective optimized model compression toward ai software deployment","
AbstractView references

The size of deep learning models in artificial intelligence (AI) software is increasing rapidly, which hinders the large-scale deployment on resource-restricted devices (e.g., smartphones). To mitigate this issue, AI software compression plays a crucial role, which aims to compress model size while keeping high performance. However, the intrinsic defects in the big model may be inherited by the compressed one. Such defects may be easily leveraged by attackers, since the compressed models are usually deployed in a large number of devices without adequate protection. In this paper, we try to address the safe model compression problem from a safety-performance co-optimization perspective. Specifically, inspired by the test-driven development (TDD) paradigm in software engineering, we propose a test-driven sparse training framework called SafeCompress. By simulating the attack mechanism as the safety test, SafeCompress can automatically compress a big model to a small one following the dynamic sparse training paradigm. Further, considering a representative attack, i.e., membership inference attack (MIA), we develop a concrete safe model compression mechanism, called MIA-SafeCompress. Extensive experiments are conducted to evaluate MIA-SafeCompress on five datasets for both computer vision and natural language processing tasks. The results verify the effectiveness and generalization of our method. We also discuss how to adapt SafeCompress to other attacks besides MIA, demonstrating the flexibility of SafeCompress. © 2022 ACM.
"
10.1145/3551349.3559564,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146945852&origin=inward,Conference Paper,SCOPUS_ID:85146945852,scopus,2022-09-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),leveraging artificial intelligence on binary code comprehension,"
AbstractView references

Understanding binary code is an essential but complex software engineering task for reverse engineering, malware analysis, and compiler optimization. Unlike source code, binary code has limited semantic information, which makes it challenging for human comprehension. At the same time, compiling source to binary code, or transpiling among different programming languages (PLs) can provide a way to introduce external knowledge into binary comprehension. We propose to develop Artificial Intelligence (AI) models that aid human comprehension of binary code. Specifically, we propose to incorporate domain knowledge from large corpora of source code (e.g., variable names, comments) to build AI models that capture a generalizable representation of binary code. Lastly, we will investigate metrics to assess the performance of models that apply to binary code by using human studies of comprehension. © 2022 ACM.
"
10.1145/3551349.3559555,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146919665&origin=inward,Conference Paper,SCOPUS_ID:85146919665,scopus,2022-09-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),few-shot training llms for project-specific code-summarization,"
AbstractView references

Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Few-shotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, APIs, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of LLMs might be very relevant. In this paper, we investigate the use few-shot training with the very large GPT (Generative Pre-trained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training. © 2022 Owner/Author.
"
10.3390/su141710638,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137884300&origin=inward,Article,SCOPUS_ID:85137884300,scopus,2022-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research on the dynamic response of a continuous steel box girder bridge based on the ansys platform,"
AbstractView references

Under the action of various dynamic loads, bridges will experience large deflections and stress. When the situation is difficult, it will affect the regular use of the bridge and even cause it to collapse suddenly. This article generated a sample of road surface irregularities based on the Chinese national standard. An ANSYS model was used to create the vehicle–bridge coupling model. In order to meet the actual engineering calculations, an essential but valuable analytical approach is presented here. The node coupling method established the time-varying vehicle axle coupling system. The moving tire force was applied to the axle coupling system. The ANSYS parametric design language was adopted to realize the process of the vehicle approach and exit of the bridge. Combined with the actual data of dynamic and static load experiments, the model’s accuracy was verified. The influence of different vehicle driving speeds, road irregularities, vehicle driving position, and vehicle driving state are analyzed in this paper. The vehicle speed had no significant influence on the displacement time-history and the force of the middle wheel of the vehicle at a specific driving position. The pavement grade significantly influenced the bridge’s displacement time-history and acceleration spectrum. © 2022 by the authors.
"
10.1007/s11263-022-01653-1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135914609&origin=inward,Article,SCOPUS_ID:85135914609,scopus,2022-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),learning to prompt for vision-language models,"
AbstractView references

Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consuming—one needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP), we propose Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition. Concretely, CoOp models a prompt’s context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and class-specific context. Through extensive experiments on 11 datasets, we demonstrate that CoOp requires as few as one or two shots to beat hand-crafted prompts with a decent margin and is able to gain significant improvements over prompt engineering with more shots, e.g., with 16 shots the average gain is around 15% (with the highest reaching over 45%). Despite being a learning-based approach, CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
"
10.1016/j.is.2022.102049,S0306437922000436,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127761614&origin=inward,Article,SCOPUS_ID:85127761614,scopus,2022-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploiting label semantics for rule-based activity recommendation in business process modeling,"
                  Business process modeling is a crucial task in organizations. Yet, the creation of consistent and complete process models is challenging and necessitates the support of process modelers with their task. In previous work, we presented a rule-based activity-recommendation approach, which recommends appropriate labels for a new activity inserted by a modeler in a process model under development. While our method has shown to work well, it is limited by the fact that it only learns rules that describe the inter-relations between complete activity labels. In the case that the model’s activities and the ones in the training repository are disjoint, the existing approach will thus not be able to provide any recommendations. In this paper, we overcome this restriction by additionally considering the natural language-based semantics of the process models. In particular, we propose a semantics-aware recommendation approach that extends the existing approach in both central phases, i.e., in the rule-learning phase and in the rule-application phase. We equip the rule learning with novel rule types, which capture action and business-object patterns in process models. For the rule application, we developed an optional similarity extension that allows rules to make recommendations even if the bodies of the rules are not exactly true for the given model. Through an evaluation on a large set of real-world process models, we demonstrate that the semantic extensions can improve the quality of recommendations.
               "
10.1007/s00766-022-00374-8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125477221&origin=inward,Article,SCOPUS_ID:85125477221,scopus,2022-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),detecting coreferent entities in natural language requirements,"
AbstractView references

Requirements are usually written in natural language and evolve continuously during the process of software development, which involves a large number of stakeholders. Stakeholders with diverse backgrounds and skills might refer to the same real-world entity with different linguistic expressions in the natural-language requirements, resulting in requirement inconsistency. We define this phenomenon as Entity Coreference (EC) in the Requirement Engineering (RE) area. It can lead to misconception about technical terminologies, and harm the readability and long-term maintainability of the requirements. In this paper, we propose a DEEP context-wise method for entity COREFerence detection, named DeepCoref. First, we truncate corresponding contexts surrounding entities. Then, we construct a deep context-wise neural network for coreference classification. The network consists of one fine-tuning BERT model for context representation, a Word2Vec-based network for entity representation, and a multi-layer perceptron in the end to fuse and make a trade-off between two representations. Finally, we cluster and normalize coreferent entities. We evaluate our method, respectively, on coreference classification and clustering with 1853 industry data on 21 projects. The former evaluation shows that DeepCoref outperforms three baselines with average precision and recall of 96.10% and 96.06%, respectively. The latter evaluation on six metrics shows that DeepCoref can cluster coreferent entities more accurately. We also conduct ablation experiments with three variants to demonstrate the performance enhancement brought by different components of neural network designed for coreference classification. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.
"
10.1016/j.jksuci.2021.08.008,S1319157821002111,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85114360419&origin=inward,Article,SCOPUS_ID:85114360419,scopus,2022-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a multi-layered bigraphical modelling approach for context-aware systems,"There exist several approaches proposed for building Context-Aware Systems (CAS). However, due to the continually changing environment, the large number of interrelated components, complexity and diversity of application domains make the modelling of context-aware systems a particularly challenging task. To address the increasing complexity of the modelling: i) It is critical to take into account the importance of the environment (operational context); and ii) rely on software engineering concepts such as abstraction and modularity in order to reduce the level of complexity. Also, a context-aware system may require intelligence and autonomy. These naturally lead us to apply intelligent agent-based engineering. This work introduces a formal layered design approach that combines intelligent control of multi-agent systems and bigraph's rigor to model context-aware systems. Bigraphical reactive systems are particularly compelling for their capacity to specify, simultaneously, the physical and logical distribution of system components and their interconnections using two distinct structures; that is: place graph and link graph. While the behaviour and dynamic evolution are expressed using defined reaction rules, and as a last step, the bigraph specifications are coded in the Maude language to allow their execution and the verification of their validity."
10.1190/image2022-3746384.1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146714680&origin=inward,Conference Paper,SCOPUS_ID:85146714680,scopus,2022-08-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),visualization of oil and gas reservoir simulation models based on large-scale unstructured mesh,"
AbstractView references

Detailed representation of oil and gas reservoir geological and physical structure requires models with billions of cells. Conventional approaches for such large model visualization are slow for an interactive data representation. We developed a workflow for fast visualization of large-scale reservoir models. Initially, model stored in 2.5D geometry format is loaded into memory as a 3D unstructured gird. Then, visualization is based on a preselection of the data needed to construct the model representation from an unstructured mesh. The key component of this workflow is an efficient algorithm for a large unstructured mesh slicing. Here we present its parallel implementation for MPI CPU and multi-GPU systems as a plugin for an open-source software ParaView. We compared the performance of (a) the developed algorithm, (b) one of the existing commercial reservoir simulation software, and (c) the ParaView`s built-in tool for model slicing. The GPU implementation of our algorithm allows an interactive visualization of the model with 1.9 billion cells with a new slice delay shorter than two seconds. © 2022 Society of Exploration Geophysicists and the American Association of Petroleum Geologists.
"
10.2478/jdis-2022-0015,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135919369&origin=inward,Article,SCOPUS_ID:85135919369,scopus,2022-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a use case of patent classification using deep learning with transfer learning,"
AbstractView references

Purpose: Patent classification is one of the areas in Intellectual Property Analytics (IPA), and a growing use case since the number of patent applications has been increasing worldwide. We propose using machine learning algorithms to classify Portuguese patents and evaluate the performance of transfer learning methodologies to solve this task. Design/methodology/approach: We applied three different approaches in this paper. First, we used a dataset available by INPI to explore traditional machine learning algorithms and ensemble methods. After preprocessing data by applying TF-IDF, FastText and Doc2Vec, the models were evaluated by cross-validation in 5 folds. In a second approach, we used two different Neural Networks architectures, a Convolutional Neural Network (CNN) and a bi-directional Long Short-Term Memory (BiLSTM). Finally, we used pre-trained BERT, DistilBERT, and ULMFiT models in the third approach. Findings: BERTTimbau, a BERT architecture model pre-trained on a large Portuguese corpus, presented the best results for the task, even though with a performance of only 4% superior to a LinearSVC model using TF-IDF feature engineering. Research limitations: The dataset was highly imbalanced, as usual in patent applications, so the classes with the lowest samples were expected to present the worst performance. That result happened in some cases, especially in classes with less than 60 training samples. Practical implications: Patent classification is challenging because of the hierarchical classification system, the context overlap, and the underrepresentation of the classes. However, the final model presented an acceptable performance given the size of the dataset and the task complexity. This model can support the decision and improve the time by proposing a category in the second level of ICP, which is one of the critical phases of the grant patent process. Originality/value: To our knowledge, the proposed models were never implemented for Portuguese patent classification. © 2022 Roberto Henriques et al., published by Sciendo.
"
10.1016/j.cma.2022.115116,S0045782522003036,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133459264&origin=inward,Article,SCOPUS_ID:85133459264,scopus,2022-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),machine learning for topology optimization: physics-based learning through an independent training strategy,"
                  The high computational cost of topology optimization has prevented its widespread use as a generative design tool. To reduce this computational cost, we propose an artificial intelligence approach to drastically accelerate topology optimization without sacrificing its accuracy. The resulting AI-driven topology optimization can fully capture the underlying physics of the problem. As a result, the machine learning model, which consists of a convolutional neural network with residual links, is able to generalize what it learned from the training set to solve a wide variety of problems with different geometries, boundary conditions, mesh sizes, volume fractions and filter radius. We train the machine learning model separately from the topology optimization, which allows us to achieve a considerable speedup (up to 30 times faster than traditional topology optimization). Through several design examples, we demonstrate that the proposed AI-driven topology optimization framework is effective, scalable and efficient. The speedup enabled by the framework makes topology optimization a more attractive tool for engineers in search of lighter and stronger structures, with the potential to revolutionize the engineering design process. Although this work focuses on compliance minimization problems, the proposed framework can be generalized to other objective functions, constraints and physics.
               "
10.3389/fcomm.2022.901719,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134923488&origin=inward,Article,SCOPUS_ID:85134923488,scopus,2022-07-13,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),multimodality and english for special purposes: signification and transduction in architecture and civil engineering models,"
AbstractView references

The applied disciplines of architecture and civil engineering require students to communicate multimodally, and to manipulate meaning across media and modes, such as image, writing or moving image. In their disciplinary studies for example, students must be able to transform the language of lectures and textbooks into models and diagrams. In their future workplaces, they will commonly be required to transform reports and legal documents into floor plans and digital & physical 3D models. Such multimodal literacy, however, is not typically reflected in their related subject-specific English language courses, especially in Germany, where a text-centric approach is favored. To better reflect the demands placed upon them, students in two courses of English for Architecture and Civil Engineering were tasked with creating digital, multimodal artifacts to explain a concept from either of these fields to a lay audience. The resultant artifacts used a wide variety of semiotic resources to make meaning, including a total of 26 separate architectural and civil engineering models. This is a quantity sufficiently large enough to invite closer examination, and also reflects the important role models play in the fields of architecture and civil engineering, both at university and in the workplace. This paper suggests that models of this kind exist within a system of signs, in which meaning is created in the relationships between the signs. The process of transforming one resource into another also invites the consideration of the artifacts in terms of the notion of “transduction”, to discern how meaning changes between contexts, practices and modes and to contribute to existing literature on multimodal texts in tertiary education, particularly within a language-learning context. Copyright © 2022 Hellwig, Jones, Matruglio and Georgiou.
"
10.1145/3501256,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141028390&origin=inward,Article,SCOPUS_ID:85141028390,scopus,2022-07-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),adversarial robustness of deep code comment generation,"
AbstractView references

Deep neural networks (DNNs) have shown remarkable performance in a variety of domains such as computer vision, speech recognition, and natural language processing. Recently they also have been applied to various software engineering tasks, typically involving processing source code. DNNs are well-known to be vulnerable to adversarial examples, i.e., fabricated inputs that could lead to various misbehaviors of the DNN model while being perceived as benign by humans. In this paper, we focus on the code comment generation task in software engineering and study the robustness issue of the DNNs when they are applied to this task. We propose ACCENT(Adversarial Code Comment gENeraTor), an identifier substitution approach to craft adversarial code snippets, which are syntactically correct and semantically close to the original code snippet, but may mislead the DNNs to produce completely irrelevant code comments. In order to improve the robustness, ACCENT also incorporates a novel training method, which can be applied to existing code comment generation models. We conduct comprehensive experiments to evaluate our approach by attacking the mainstream encoder-decoder architectures on two large-scale publicly available datasets. The results show that ACCENT efficiently produces stable attacks with functionality-preserving adversarial examples, and the generated examples have better transferability compared with the baselines. We also confirm, via experiments, the effectiveness in improving model robustness with our training method. © 2022 Association for Computing Machinery.
"
10.1145/3477495.3531702,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135074246&origin=inward,Conference Paper,SCOPUS_ID:85135074246,scopus,2022-07-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),3rd workshop on patent text mining and semantic technologies (patentsemtech2022),"
AbstractView references

Steadily increasing numbers of patent applications per year and large amounts of available patent data necessitate highly efficient and interactive next-generation information retrieval systems in the patent domain. AI and Machine Learning (ML) methods such as Deep Learning (DL) are successfully adopted in many domains, so patent researchers and practitioners start to employ AI-based approaches as well, to support experts in the patenting process or to automate patent analysis and retrieval processes. AI-enhanced Information Retrieval systems can improve patent search and analysis but also require millions of annotated sample data for training the ML models. When working with patent data, particular challenges arise that call for adaption of existing IR and AI methods as well as development of novel approaches suited for the patent domain. The focus of the 3rd edition of this workshop will be on two-way communication between industry and academia from all areas of Information Retrieval, such as Natural Language Processing (NLP), Text and Data Mining (TDM), and Semantic Technologies (ST). We want to bring together novel research results and the latest systems and methods employed by the Intellectual Property (IP) industry. © 2022 ACM.
"
10.1016/j.nanoen.2022.107245,S2211285522003251,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129566278&origin=inward,Article,SCOPUS_ID:85129566278,scopus,2022-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards a sustainable monitoring: a self-powered smart transportation infrastructure skin,"
                  Sustainable monitoring of traffic using clean energy supply has always been a significant problem for engineers. In this study, we proposed a self-powered smart transportation infrastructure skin (SSTIS) as an innovative and bionic system for the traffic classification of a smart city. This system incorporated the self-powered flexible sensors with net-zero power consumption based on the Triboelectric Nanogenerator (TENG) and an intelligent analysis system based on artificial intelligence (AI). The feasibility of the SSTIS was tested using the full-scale accelerated pavement tests (APT) and the long-short term memory (LSTM) deep learning model with a vehicle axle load classification accuracy up to 89.06%. This robust SSTIS was later tested on highway and collected around 869,600 pieces of signals data. The generative adversarial networks (GAN) WGAN-GP (Wasserstein GAN - Gradient Penalty) was used for data augmentation, due to the imbalanced data of different vehicle types in actual traffic. The overall accuracy for on-road vehicle type classification improved to 81.06% using the convolutional neural network ResNet. Finally, we developed a mobile traffic signal information monitoring system based on cloud platform and Android framework, which enabled engineers to obtain the vehicle axle-load information mobilely. This study is the emerging design and engineering application of the self-powered flexible sensors for smart traffic monitoring, which provides a significant advance for intelligent transportation and smart cities in future.
               "
10.1016/j.neunet.2022.04.001,S0893608022001332,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129497747&origin=inward,Article,SCOPUS_ID:85129497747,scopus,2022-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),hippocampal formation-inspired probabilistic generative model,"In building artificial intelligence (AI) agents, referring to how brains function in real environments can accelerate development by reducing the design space. In this study, we propose a probabilistic generative model (PGM) for navigation in uncertain environments by integrating the neuroscientific knowledge of hippocampal formation (HF) and the engineering knowledge in robotics and AI, namely, simultaneous localization and mapping (SLAM). We follow the approach of brain reference architecture (BRA) (Yamakawa, 2021) to compose the PGM and outline how to verify the model. To this end, we survey and discuss the relationship between the HF findings and SLAM models. The proposed hippocampal formation-inspired probabilistic generative model (HF-PGM) is designed to be highly consistent with the anatomical structure and functions of the HF. By referencing the brain, we elaborate on the importance of integration of egocentric/allocentric information from the entorhinal cortex to the hippocampus and the use of discrete-event queues."
10.1109/TSE.2021.3059885,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100950370&origin=inward,Article,SCOPUS_ID:85100950370,scopus,2022-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),difftech: differencing similar technologies from crowd-scale comparison discussions,"
AbstractView references

Developers use different technologies for many software development tasks. However, when faced with several technologies with comparable functionalities, it is not easy to select the most appropriate one, as trial and error comparisons among such technologies are time-consuming. Instead, developers can resort to expert articles, read official documents or ask questions in Q&A sites. However, it still remains difficult to get a comprehensive comparison as online information is often fragmented or contradictory. To overcome these limitations, we propose the DiffTech system that exploits crowdsourced discussions from Stack Overflow, and assists technology comparison with an informative summary of different aspects. We first build a large database of comparable technologies in software engineering by mining tags in Stack Overflow. We then locate comparative sentences about comparable technologies with natural language processing methods. We further mine prominent comparison aspects by clustering similar comparative sentences and representing each cluster with its keywords and aggregate the overall opinion towards the comparable technologies. Our evaluation demonstrates both the accuracy and usefulness of our model, and we have implemented our approach as a practical website for public use. © 1976-2012 IEEE.
"
10.1145/3530190.3534817,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133858895&origin=inward,Conference Paper,SCOPUS_ID:85133858895,scopus,2022-06-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),accelerated design and deployment of low-carbon concrete for data centers,"
AbstractView references

Concrete is the most widely used engineered material in the world with more than 10 billion tons produced annually. Unfortunately, with that scale comes a significant burden in terms of energy, water, and release of greenhouse gases and other pollutants; indeed 8% of worldwide carbon emissions are attributed to the production of cement, a key ingredient in concrete. As such, there is interest in creating concrete formulas that minimize this environmental burden, while satisfying engineering performance requirements including compressive strength. Specifically for computing, concrete is a major ingredient in the construction of data centers. In this work, we use conditional variational autoencoders (CVAEs), a type of semi-supervised generative artificial intelligence (AI) model, to discover concrete formulas with desired properties. Our model is trained just using a small open dataset from the UCI Machine Learning Repository joined with environmental impact data from standard lifecycle analysis. Computational predictions demonstrate CVAEs can design concrete formulas with much lower carbon requirements than existing formulations while meeting design requirements. Next we report laboratory-based compressive strength experiments for five AI-generated formulations, which demonstrate that the formulations exceed design requirements. The resulting formulations were then used by Ozinga Ready Mix - a concrete supplier - to generate field-ready concrete formulations, based on local conditions and their expertise in concrete design. Finally, we report on how these formulations were used in the construction of buildings and structures in a Meta data center in DeKalb, IL, USA. Results from field experiments as part of this real-world deployment corroborate the efficacy of AI-generated low-carbon concrete mixes. © 2022 ACM.
"
10.1145/3501247.3539016,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133726426&origin=inward,Conference Paper,SCOPUS_ID:85133726426,scopus,2022-06-26,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),blockchain for social good: combating misinformation on the web with ai and blockchain,"
AbstractView references

The spread of deceptive or misleading information, commonly referred to as misinformation, poses a social, economic, and political threat. Such deceptive information spreads quickly and inexpensively. For example, with the hype around blockchain technologies, misinformation on ""get rich quick""scams on the Web is rampant, as evidenced by sophisticated Twitter hacks of celebrities and many social media posts that bait unsuspecting users to visit phishing websites. Unfortunately, AI technologies have contributed to the growing pains of misinformation on the Web, with the advances in technologies such as generative adversarial deep learning techniques that can generate ""deep fakes""for nefarious purposes. At the same time, researchers are working on a different set of AI technologies to combat misinformation, akin to ""fighting fire with fire.""As there is no clear way to win the online ""cat-and-mouse""game against fake news generators and spreaders of misinformation, we believe social media platforms could be fortified with blockchain and AI technologies to mitigate the extent of misinformation propagation in various communities worldwide. Tamper-proof blockchain techniques can provide irrefutable evidence of what content is authentic, guaranteeing how the information has evolved with provenance trails. Various AI models that could be used for detecting fake news can be served on a blockchain for the effective and transparent utility of the model. Such a synergistic combination of AI and blockchain is a burgeoning area of research. This paper outlines a proposal for combining blockchain and AI techniques for handling misinformation on the Web and highlights some of the early ongoing work in this space. © 2022 ACM.
"
10.1145/3531146.3533138,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132980367&origin=inward,Conference Paper,SCOPUS_ID:85132980367,scopus,2022-06-21,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),robots enact malignant stereotypes,"
AbstractView references

Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural Language Processing (NLP) [6], or both, in the case of large image and caption models such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called ""foundation models"", e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms. © 2022 Owner/Author.
"
10.1145/3527927.3532792,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133341018&origin=inward,Conference Paper,SCOPUS_ID:85133341018,scopus,2022-06-20,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),initial images: using image prompts to improve subject representation in multimodal ai generated art,"
AbstractView references

Advances in text-to-image generative models have made it easier for people to create art by just prompting models with text. However, creating through text leaves users with limited control over the final composition or the way the subject is represented. A potential solution is to use image prompts alongside text prompts to condition the model. To better understand how and when image prompts can improve subject representation in generations, we conduct an annotation experiment to quantify their effect on generations of abstract, concrete plural, and concrete singular subjects. We find that initial images improved subject representation across all subject types, with the most noticeable improvement in concrete singular subjects. In an analysis of different types of initial images, we find that icons and photos produced high quality generations of different aesthetics. We conclude with design guidelines for how initial images can improve subject representation in AI art. © 2022 ACM.
"
10.1145/3532106.3533506,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133605412&origin=inward,Conference Paper,SCOPUS_ID:85133605412,scopus,2022-06-13,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),from tool to companion: storywriters want ai writers to respect their personal values and writing strategies,"
AbstractView references

Modern large-scale language models approach the quality of human-level writing. This promises the advent of AI writing companions performing AI-led writing under human control, surpassing traditional writing tools limited to revision and ideation supports. However, human-AI co-writing may endanger writers' control, autonomy, and ownership by overstepping co-creative boundaries. Our design workbook study with 7 hobbyists and 13 professional writers elicited three sets of primary barriers to the adoption of human-AI co-writing. Storywriters desire retaining control over writing rather than letting AI take the lead when they (1) prioritize emotional values in turning ideas into words over the productivity of AI-generated writing; (2) have high self-confidence and distrust AI in challenging sub-tasks (e.g., creating characters and dialogue); and (3) expect the AI control mechanism to mismatch their writing strategies. We lay the groundwork for AI companions that respect storywriters' personal values and writing methods. © 2022 ACM.
"
10.1145/3519939.3523449,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131230932&origin=inward,Conference Paper,SCOPUS_ID:85131230932,scopus,2022-06-09,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),finding the dwarf: recovering precise types from webassembly binaries,"
AbstractView references

The increasing popularity of WebAssembly creates a demand for understanding and reverse engineering WebAssembly binaries. Recovering high-level function types is an important part of this process. One method to recover types is data-flow analysis, but it is complex to implement and may require manual heuristics when logical constraints fall short. In contrast, this paper presents SnowWhite, a learning-based approach for recovering precise, high-level parameter and return types for WebAssembly functions. It improves over prior work on learning-based type recovery by representing the types-to-predict in an expressive type language, which can describe a large number of complex types, instead of the fixed, and usually small type vocabulary used previously. Thus, recovery of a single type is no longer a classification task but sequence prediction, for which we build on the success of neural sequence-to-sequence models. We evaluate SnowWhite on a new, large-scale dataset of 6.3 million type samples extracted from 300,905 WebAssembly object files. The results show the type language is expressive, precisely describing 1,225 types instead the 7 to 35 types considered in previous learning-based approaches. Despite this expressiveness, our type recovery has high accuracy, exactly matching 44.5% (75.2%) of all parameter types and 57.7% (80.5%) of all return types within the top-1 (top-5) predictions. © 2022 ACM.
"
10.1103/PhysRevPhysEducRes.18.010141,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134230244&origin=inward,Article,SCOPUS_ID:85134230244,scopus,2022-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),classification of open-ended responses to a research-based assessment using natural language processing,"
AbstractView references

Surveys have long been used in physics education research to understand student reasoning and inform course improvements. However, to make analysis of large sets of responses practical, most surveys use a closed-response format with a small set of potential responses. Open-ended formats, such as written free response, can provide deeper insights into student thinking, but take much longer to analyze, especially with a large number of responses. Here, we explore natural language processing as a computational solution to this problem. We create a machine learning model that can take student responses from the Physics Measurement Questionnaire as input, and output a categorization of student reasoning based on different reasoning paradigms. Our model yields classifications with the same level of agreement as that between two humans categorizing the data, but can be done by a computer, and thus can be scaled for large datasets. In this work, we describe the algorithms and methodologies used to create, train, and test our natural language processing system. We also present the results of the analysis and discuss the utility of these approaches for analyzing open-response data in education research. © 2022 authors. Published by the American Physical Society.
"
10.3390/su14116938,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132049524&origin=inward,Article,SCOPUS_ID:85132049524,scopus,2022-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),contractor’s risk analysis of engineering procurement and construction (epc) contracts using ontological semantic model and bi-long short-term memory (lstm) technology,"
AbstractView references

The development of intelligent information technology in the era of the fourth industrial revolution requires the EPC (engineering, procurement, and construction) industry to increase productivity through a digital transformation. This study aims to automatically analyze the critical risk clauses in the invitation to bid (ITB) at the bidding stage to strengthen their competitiveness for the EPC contractors. To this end, we developed an automated analysis technology that effectively analyzes a large amount of ITB documents in a short time by applying natural language processing (NLP) and bi-directional long short-term memory (bi-LSTM) algorithms. This study proposes two models. First, the semantic analysis (SA) model is a rule-based approach that applies NLP to extract key risk clauses. Second, the risk level ranking (RLR) model is a train-based approach that ranks the risk impact for each clause by applying bi-LSTM. After developing and training an artificial intelligent (AI)-based ITB analysis model, its performance was evaluated through the actual project data. As a result of validation, the SA model showed an F1 score of 86.4 percent, and the RLR model showed an accuracy of 46.8 percent. The RLR model displayed relatively low performance because the ITB used in the evaluation test included the contract clauses that did not exist in the training dataset. Therefore, this study illustrated that the rule-based approach performed superior to the training-based method. The authors suggest that EPC contractors should apply both the SA and RLR modes in the ITB analysis, as one supplements the other. The two models were embedded in the Engineering Machine-learning Automation Platform (EMAP), a cloud-based platform developed by the authors. Rapid analysis through applying both the rule-based and AI-based automatic ITB analysis technology can contribute to securing timeliness for risk response and supplement possible human mistakes in the bidding stage. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
"
10.3390/app12115301,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131586837&origin=inward,Article,SCOPUS_ID:85131586837,scopus,2022-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),digital function modeling in graph-based design languages,"
AbstractView references

The main focus of this paper is the integration of an integrated function modeling (IFM) framework in an engineering framework based on graph-based design languages (GBDLs). Over the last decade, GBDLs have received increasing attention as they offer a promising approach for addressing several important challenges in engineering, such as the frequent and time-consuming transfer of data between different computer aided engineering (CAE) tools. This absorbs significant amounts of manual labor in engineering design projects. GBDLs create digital system models at a meta level, encompassing all relevant information concerning a certain product design and feeding this into the relevant simulation tools needed for evaluating the impact of possible design variations on the performance of the resulting products/parts. It is possible to automate this process using digital compilers. Because of this, it is also possible to realize systematic design variations for a very large number of parameters and topological variants. Therefore, these kinds of graph-based languages are a powerful means for creating a large number of viable design alternatives and for permitting fast evaluation processes against the given specifications. While, thus far, such analyses tend to be based on a more or less fully defined system, this paper proposes an expansion of the applicability of GBDLs into the domain of product functions to cohesively link conceptual with embodiment design stages. This will also help with early systematic, automated generation and the validation of design alternatives through relevant simulation tools during embodiment design. Further, it will permit the automated exploration of function paths and enable extended analysis possibilities, such as the detection of functional bottlenecks, while enhancing the traceability of the design over the development process. For these extended analysis possibilities, a function analysis tool was developed that adopts core ideas of the failure mode and effects analysis (FMEA). In this, the functional distinction between function carriers and function-related processes allows the goal-directed assessment of component reliabilities and the detectability and importance of processes in a technical system. In the paper, the graph-based modeling of functions and the function analysis tools are demonstrated on the example of a multicopter. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
"
10.1007/s10270-021-00942-6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122702797&origin=inward,Article,SCOPUS_ID:85122702797,scopus,2022-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"automated, interactive, and traceable domain modelling empowered by artificial intelligence","
AbstractView references

Model-Based Software Engineering provides various modelling formalisms for capturing the structural, behavioral, configuration, and intentional aspects of software systems. One of the most widely used kinds of models—domain models—are used during requirements analysis or the early stages of design to capture the domain concepts and relationships in the form of class diagrams. Modellers perform domain modelling to transform the problem descriptions that express informal requirements in natural language to domain models, which are more concise and analyzable. However, this manual practice of domain modelling is laborious and time-consuming. Existing approaches, which aim to assist modellers by automating or semi-automating the construction of domain models from problem descriptions, fail to address three non-trivial aspects of automated domain modelling. First, automatically extracted domain models from existing approaches are not accurate enough to be used directly or with minor modifications for software development or teaching purposes. Second, existing approaches do not support modeller-system interactions beyond providing recommendations. Finally, existing approaches do not facilitate the modellers to learn the rationale behind the modelling decisions taken by an extractor system. Therefore, in this paper, we extend our previous work to facilitate bot-modeller interactions. We propose an algorithm to discover alternative configurations during bot-modeller interactions. Our bot uses this algorithm to find alternative configurations and then present these configurations in the form of suggestions to modellers. Our bot then updates the domain model in response to the acceptance of these suggestions by a modeller. Furthermore, we evaluate the bot for its effectiveness and performance for the test problem descriptions. Our bot achieves median F1 scores of 86%, 91%, and 90% in the Found Configurations, Offered Suggestions, and Updated Domain Models categories, respectively. We also show that the median time taken by our bot to find alternative configurations is 55.5ms for the problem descriptions which are similar to the test problem descriptions in terms of model size and complexity. Finally, we conduct a pilot user study to assess the benefits and limitations of our bot and present the lessons learned from our study in preparation for a large-scale user study. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.1145/3546157.3546171,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137541439&origin=inward,Conference Paper,SCOPUS_ID:85137541439,scopus,2022-05-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards simplifying and formalizing uml class diagram generalization/specialization relationship with mathematical set theory,"
AbstractView references

The Unified Modeling Language (UML) is considered the de facto standard for object-oriented software model development. This makes it appropriate to be used in academia courses at both the graduate and undergraduate levels of education. Some challenges to using the UML is academia are its large number of model concepts and the imprecise semantic of some of these concepts. These challenges are daunting for students who are being introduced to the UML. One approach that can be taken in teaching UML towards addressing these concerns is to limit the number of UML concepts taught and recognize that students may not be able to develop correct UML system models. This approach leads to research work that develop a limited set of UML model concepts that are fewer in number and have more precise semantics. In this paper, we present a new approach to resolve an aspect of this problem by simplifying the generalization/specialization semantics of the class diagram through the application of mathematical formality to usage of these class diagram concepts. This research work derives a core set of concepts suitable for graduate and undergraduate comprehension of UML modeling and defines more precise semantics for those modeling concepts. The applicable mathematical principles applied in this work are from the domains of set theory and predicate logic. This approach is particularly relevant for the pedagogy of software engineering and the development of software systems that require a high level of reliability. © 2022 ACM.
"
10.37936/ecticit.2022162.247272,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132073094&origin=inward,Article,SCOPUS_ID:85132073094,scopus,2022-05-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),software requirements modeling from a selected set of requirements using fuzzy based approach,"
AbstractView references

Software requirements (SRs) modeling is one of the sub-processes of requirements engineering whose aim is to model and design the SRs before the development of a project. Different techniques are employed for representing the SRs using goal concepts, unified modeling language, etc. There could be a large number of SRs after the completion of the requirements elicitation process. It is not practical to model the complete set of the identified SRs because of the cost, time, and other limitations of an organization. There should be some systematic methodology to identify and select those SRs for modeling that need to be implemented during the software development process. The selection of SRs from the list of the elicited requirements is a multi-criteria decision-making process. In this process different stakeholders participate in the selection of the SRs. Linguistic variables may be used by the stakeholders to specify the preferences of SRs. To deal with this issue, a method has been proposed using a fuzzy based approach so that the selected set of SRs can be modeled and implemented during the development phase. The proposed method is explained by considering the small and large set of SRs for an institute examination system. The ranking value of the functional requirements of an examination system is computed. Based on the ranking order, top three requirements are modeled using use-case diagrams (UCDs) and class diagrams. It is found that both diagrams represent different information about the requirements of an examination system and there is no overlap in the information captured through UCDs and class diagrams. © 2022, ECTI Association. All rights reserved.
"
10.3389/frobt.2022.728776,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85130924537&origin=inward,Article,SCOPUS_ID:85130924537,scopus,2022-05-11,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards a personality ai for robots: potential colony capacity of a goal-shaped generative personality model when used for expressing personalities via non-verbal behaviour of humanoid robots,"
AbstractView references

Engineering robot personalities is a challenge of multiple folds. Every robot that interacts with humans is an individual physical presence that may require their own personality. Thus, robot personalities engineers face a problem that is the reverse of that of personality psychologists: robot personalities engineers need to make batches of identical robots into individual personalities, as oppose to formulating comprehensive yet parsimonious descriptions of individual personalities that already exist. The robot personality research so far has been fruitful in demonstrating the positive effects of robot personality but unfruitful in insights into how robot personalities can be engineered in significant quantities. To engineer robot personalities for mass-produced robots we need a generative personality model with a structure to encode a robot’s individual characteristics as personality traits and generate behaviour with inter- and intra-individual differences that reflect those characteristics. We propose a generative personality model shaped by goals as part of a personality AI for robots towards which we have been working, and we conducted tests to investigate how many individual personalities the model can practically support when it is used for expressing personalities via non-verbal behaviour on the heads of humanoid robots. Copyright © 2022 Luo, Ogawa, Peebles and Ishiguro.
"
10.1002/9781119792437.ch7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147947735&origin=inward,Book Chapter,SCOPUS_ID:85147947735,scopus,2022-05-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deep learning methods for data science,"
AbstractView references

Deep learning network (DLN) is defined as the neural network characterized by complex connected layers to handle a large volume of data, automatic extraction of features, and representation learning for identification and regression problems. This concise chapter on deep learning (DL) methods for data science takes readers through a series of program-writing tasks that introduce them to the use of different DL techniques in various areas of artificial intelligence (AI). It covers zen and tao of the various types of DL methods such as convolutional neural network, recurrent neural network (RNN), denoising autoencoder (DAE), recursive neural network, deep reinforcement learning, deep belief networks (DBNs), and long short-term memory (LSTM), i.e., starting from architecture, learning rules, mathematical model to programing aspects explained in this chapter. The developed and emerging structures of DLN has been applied in applications according to the depth of computational graph, learning, and performance. The knowledge of merits and demerits of each method can train reader toward selection of best suited technique for a given problem statement. For example, the evolution of RNN-based DL architecture innovated many applications in time series, biological, speech-to-text conversion, which has sequence dependent data. RNN handles both real values (time series) and symbolic values of variable length inputs. This chapter covers varieties of application with example to give reader an overall learning. The formulation of this chapter highlights the improvement in applications (such as language, text, signal, and image processing) by modifications in network configuration. This AI technique summarizes the necessity, development, strength, and weakness of DLN models used in data science which will integrate all the basic cores of engineering in near future. © 2022 Scrivener Publishing LLC.
"
10.19255/MPM029011,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137232598&origin=inward,Article,SCOPUS_ID:85137232598,scopus,2022-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),“fit for purpose building information modelling and systems integration (bimsi) for better construction projects management”,"
AbstractView references

Building Information Models (BIM) should reflect all aspects and phases of the construction projects life cycle. However, in current contractor practices, a proliferation of different information systems has arisen, each of which in turn illuminates a different information model dimension (nD) for usually only one construction project management purpose. To solve this problem it seems obvious to build a unique overarching system based on a single prescribed data modelling structure. We argue that this attempt is unrealistic, will not serve industrial practitioners and has failed already several times. We propose to link BIM to the concept of Systems Integration (SI), to develop tailormade and integrative information systems for its intended multidimensional (nD) modelling and construction projects management purpose: i.e. a targeted BIMSI fit for nD purpose enhancing better construction projects management. In this paper this nD BIMSI concept is introduced and demonstrated by a number of grassroot projects, which have been developed and validated in close cooperation with the AEC industrial practice and construction management and (civil) engineering master student projects at TU Delft. An overview of these grass root development projects is provided. These projects show how the BIMSI concept improves construction project management in areas such as generative design, safety during construction, and AI applications for effective budgeting. The focus of these projects is not on extending or evaluating BIM knowledge and theory, but rather on transforming BIM concepts into integrative information systems to solve real life problems. Finally, a state of the art education concept developed at the TU Delft is presented to demonstrate its unique position in master education on information systems for the construction industry. This so-called Open Design Learning (ODL) education integrates the nD BIMSI concept to better prepare students for both industrial and R&D construction project management practices. © 2022 Editora Mundos Sociais. All rights reserved.
"
10.1017/pds.2022.156,S2732527X22001560,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131352176&origin=inward,Conference Paper,SCOPUS_ID:85131352176,scopus,2022-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on diverse system-level design using manifold learning and partial simulated annealing,"
AbstractView references

The goal in system-level design is to generate a diverse set of high-performing design configurations that allow trade-offs across different objectives and avoid early concretization. We use deep generative models to learn a manifold of the valid design space, followed by Monte Carlo sampling to explore and optimize design over the learned manifold, producing a diverse set of optimal designs. We demonstrate the efficacy of our proposed approach on the design of an SAE race vehicle and propeller. © The Author(s), 2022.
"
10.3390/ijgi11050316,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131135302&origin=inward,Article,SCOPUS_ID:85131135302,scopus,2022-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),development of a conceptual data model for 3d geospatial road management based on landinfra standard: a case study of korea,"
AbstractView references

In practice, road management data are typically managed in two-dimensional (2D) geospatial forms. However, 2D geographic information system (GIS)-based road infrastructure management data have limitations in their representation of complex roads, such as interchanges, bridges, and tunnels. As such, complex and large road network management data cannot be adequately managed in a 2D GIS-based form. This study discusses the use of the LandInfra standard for road infrastructure management in Korea, considering its focus on land and civil engineering infrastructure facilities. To facilitate the transition from 2D to 3D GIS, we analyzed existing road management models of road pavement and road register information and created Unified Modeling Language (UML) class diagrams depicting these models. Then, existing road management classes and LandInfra classes were mapped. Based on the results, we propose a road management model based on the Facility, Alignment, and Road parts of LandInfra. For its implementation, several classes of the proposed data model were encoded into InfraGML using real-world data input. Taken together, this study shows how the LandInfra standard can be extended and applied to the field of road infrastructure management in Korea, supporting the transition from a 2D to a 3D GIS-based model. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
"
10.1111/cgf.14476,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85130518157&origin=inward,Article,SCOPUS_ID:85130518157,scopus,2022-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),differentiable 3d cad programs for bidirectional editing,"
AbstractView references

Modern CAD tools represent 3D designs not only as geometry, but also as a program composed of geometric operations, each of which depends on a set of parameters. Program representations enable meaningful and controlled shape variations via parameter changes. However, achieving desired modifications solely through parameter editing is challenging when CAD models have not been explicitly authored to expose select degrees of freedom in advance. We introduce a novel bidirectional editing system for 3D CAD programs. In addition to editing the CAD program, users can directly manipulate 3D geometry and our system infers parameter updates to keep both representations in sync. We formulate inverse edits as a set of constrained optimization objectives, returning plausible updates to program parameters that both match user intent and maintain program validity. Our approach implements an automatically differentiable domain-specific language for CAD programs, providing derivatives for this optimization to be performed quickly on any expressed program. Our system enables rapid, interactive exploration of a constrained 3D design space by allowing users to manipulate the program and geometry interchangeably during design iteration. While our approach is not designed to optimize across changes in geometric topology, we show it is expressive and performant enough for users to produce a diverse set of design variants, even when the CAD program contains a relatively large number of parameters. © 2022 The Author(s) Computer Graphics Forum © 2022 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.
"
10.1142/S0219876222500050,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129067525&origin=inward,Article,SCOPUS_ID:85129067525,scopus,2022-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"coupling soil-fluid-structure domains by localized lagrange multipliers mixed formulation (u, p) for modeling offshore wind turbine vibration","
AbstractView references

Modeling is frequently used to design structures in a large range of engineering applications. Coupled models can be solved by several numerical procedures. This paper presents a soil-fluid-structure coupling analysis applied to offshore wind turbines design. The goal of this work is to develop a coupled structural finite element procedure using Localized Lagrange Multipliers (LLM) at idealized offshore wind turbines with poroelastic soil foundation. The poroelastic media theory of Biot is used to represent the soil domain as two-phase material which is considered to be fully saturated with water. The numerical model is validated through a fully coupled model at classical problems results. In this work, the mixed formulation (u,p) is used to model the interface frames between the domains. The interface domain behaves as fictitious porous material generating a nonsymmetrical coupling between elastic solid and potential fluid. The momentum equilibrium and mass continuity equations are solved by algebraic equation system imposed by Lagrange multipliers methodology. In order to fully model the coupled system, aerodynamic decoupling effects are implemented to separate the tower structure, the soil foundation and the rotor-nacelle assembly. This procedure is based on the blade aerodynamic characteristics. The force and moment vectors are assembled considering the aerodynamic damping coupling between inplane and outplane rotor motions. Finally, the equations of the tower-nacelle structure, ocean fluid and poroelastic soil are obtained by the classical finite element method. In addition, their interaction is modeled using LLM. The numerical model for monopile and jacked offshore wind turbine is developed in terms of time dynamic response which is evaluated for a range of physical parameters including the wind nature and soil type. © 2022 World Scientific Publishing Company.
"
10.1007/s10664-022-10118-5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126559269&origin=inward,Article,SCOPUS_ID:85126559269,scopus,2022-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),can pre-trained code embeddings improve model performance? revisiting the use of code embeddings in software engineering tasks,"
AbstractView references

Word representation plays a key role in natural language processing (NLP). Various representation methods have been developed, among which pre-trained word embeddings (i.e., dense vectors that represent words) have shown to be highly effective in many neural network-based NLP applications, such as named entity recognition (NER) and part-of-speech (POS) tagging. However, the use of pre-trained code embeddings for software engineering (SE) tasks has not been extensively explored. A recent study by Kang et al. (2019) finds that code embeddings may not be readily leveraged for the downstream tasks that the embeddings are not trained for. However, Kang et al. (2019) only evaluate two code embedding approaches on three downstream tasks and both approaches may have not taken full advantage of the context information in the code when training code embeddings. Considering the limitations of the evaluated embedding techniques and downstream tasks in Kang et al. (2019), we would like to revisit the prior study by examining whether the lack of generalizability of pre-trained code embeddings can be addressed by considering both the textual and structural information of the code and using unsupervised learning. Therefore, in this paper, we propose a framework, StrucTexVec, which uses a two-step unsupervised training strategy to incorporate the textual and structural information of the code. Then, we extend prior work (Kang et al. 2019) by evaluating seven code embedding techniques and comparing them with models that do not utilize pre-trained embeddings in six downstream tasks. Our results first confirm the findings from prior work, i.e., pre-trained embeddings may not always have a significant effect on the performance of downstream SE tasks. Nevertheless, we also observe that (1) different embedding techniques can result in diverse performance for some SE tasks; (2) using well pre-trained embeddings usually improve the performance of SE tasks (e.g., all six downstream tasks in our study); and (3) the structural context has a non-negligible impact on improving the quality of code embeddings (e.g., embedding approaches that leverage the structural context achieve the best performance in five out of six downstream tasks among all the evaluated non-contextual embeddings), and thus, future work can consider incorporating such information into the large pre-trained models. Our findings imply the importance and effectiveness of combining both textual and structural context in creating code embeddings. Moreover, one should be very careful with the selection of code embedding techniques for different downstream tasks, as it may be difficult to prescribe a single best-performing solution for all SE tasks. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
"
10.1007/s10515-022-00324-2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125628911&origin=inward,Article,SCOPUS_ID:85125628911,scopus,2022-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards digitalization of requirements: generating context-sensitive user stories from diverse specifications,"
AbstractView references

Requirements Engineering in the industry is expertise-driven, heavily manual, and centered around various types of requirement specification documents being prepared and maintained. These specification documents are in diverse formats and vary depending on whether it is a business requirement document, functional specification, interface specification, client specification, and so on. These diverse specification documents embed crucial product knowledge such as functional decomposition of the domain into features, feature hierarchy, feature types and their specific feature characteristics, dependencies, business context, etc. Moreover, in a product development scenario, thousands of pages of requirement specification documentation is created over the years. Comprehending functionality and its associated context from large volumes of specification documents is a highly complex task. To address this problem, we propose to digitalize the requirement specification documents into processable models. This paper discusses the salient aspects involved in the digitalization of requirements knowledge from diverse requirement specification documents. It proposes an AI engine for the automatic transformation of diverse text-based requirement specifications into machine-processable models using NLP techniques and the generation of context-sensitive user stories. The paper describes the key requirement abstractions and concepts essential in an industrial scenario, the conceptual meta-model, and DizReq engine (AI engine for digitalizing requirements) implementation for automatically transforming diverse requirement specifications into user stories embedding the business context. The evaluation results from digitalizing specifications of an IT product suite are discussed: mean feature extraction efficiency is 40 features/file, mean user story extraction efficiency is 71 user stories/file, feature extraction accuracy is 94%, and requirement extraction accuracy is 98%. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
"
10.1145/3491102.3501825,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85130510763&origin=inward,Conference Paper,SCOPUS_ID:85130510763,scopus,2022-04-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design guidelines for prompt engineering text-to-image generative models,"
AbstractView references

Text-to-image generative models are a new and powerful way to generate visual artwork. However, the open-ended nature of text as interaction is double-edged; while users can input anything and have access to an infinite range of generations, they also must engage in brute-force trial and error with the text prompt when the result quality is poor. We conduct a study exploring what prompt keywords and model hyperparameters can help produce coherent outputs. In particular, we study prompts structured to include subject and style keywords and investigate success and failure modes of these prompts. Our evaluation of 5493 generations over the course of five experiments spans 51 abstract and concrete subjects as well as 51 abstract and figurative styles. From this evaluation, we present design guidelines that can help people produce better outcomes from text-to-image generative models. © 2022 ACM.
"
10.1145/3535782.3535835,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135069167&origin=inward,Conference Paper,SCOPUS_ID:85135069167,scopus,2022-04-28,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards bilingualword embedding models for engineering: evaluating semantic linking capabilities of engineering-specific word embeddings across languages,"
AbstractView references

Word embeddings represent the semantic meanings of words in high-dimensional vector space. Because of this capability, word embeddings could be used in a wide range of Natural Language Processing (NLP) tasks. While domain-specific monolingual word embeddings are common in literature, domain-specific bilingual word embeddings are uncommon. In general, large text corpora are required for training high quality word embeddings. Furthermore, training domain-specific word embeddings necessitates the use of source texts from the relevant domain. To train bilingual domain-specific word embeddings, the domain-specific texts must also be available in two different languages. In this paper, we use a large dataset of engineering-related articles in German and English to train bilingual engineering-specific word embedding models using different approaches. We will evaluate our trained models, identify the most promising approach, and demonstrate that the best performing one is very capable of representing semantic relationships between engineering-specific words and mapping languages in a shared vector space. Moreover, we show that the additional use of an engineering-specific learning dictionary can improve the quality of bilingual engineering-specific word embeddings. © 2022 ACM.
"
10.1145/3485447.3512015,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129844398&origin=inward,Conference Paper,SCOPUS_ID:85129844398,scopus,2022-04-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),massive text normalization via an efficient randomized algorithm,"
AbstractView references

Current popular machine learning techniques in natural language processing and data mining rely heavily on high-quality text sources. Nevertheless, real-world text datasets contain a significant amount of spelling errors and improperly punctuated variants where the performance of these models would quickly deteriorate. Moreover, existing text normalization methods are prohibitively expensive to execute over web-scale datasets, can hardly process noisy texts from social networks, or require annotations to learn the corrections in a supervised manner. In this paper, we present Flan (Fast LSH Algorithm for Text Normalization), a scalable randomized algorithm to clean and canonicalize massive text data. Our approach suggests corrections based on the morphology of the words, where lexically similar words are considered the same with high probability. We efficiently handle the pairwise word-to-word comparisons via locality sensitive hashing (LSH). We also propose a novel stabilization process to address the issue of hash collisions between dissimilar words, which is a consequence of the randomized nature of LSH and is exacerbated by the massive scale of real-world datasets. Compared with existing approaches, our method is more efficient, both asymptotically and in empirical evaluations, does not rely on feature engineering, and does not require any annotation. Our experimental results on real-world datasets demonstrate the efficiency and efficacy of Flan. Based on recent advances in densified Minhash, our approach requires much less computational time compared to baseline text normalization techniques on large-scale Twitter and Reddit datasets. In a human evaluation of the quality of the normalization, Flan achieves 5% and 14% improvement against the baselines over the Reddit and Twitter datasets, respectively. Our method also improves performance on Twitter sentiment classification applications and the perturbed GLUE benchmark datasets, where we introduce random errors into the text. © 2022 ACM.
"
10.1145/3471907,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85130755580&origin=inward,Article,SCOPUS_ID:85130755580,scopus,2022-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"model transformation development using automated requirements analysis, metamodel matching, and transformation by example","
AbstractView references

In this article, we address how the production of model transformations (MT) can be accelerated by automation of transformation synthesis from requirements, examples, and metamodels. We introduce a synthesis process based on metamodel matching, correspondence patterns between metamodels, and completeness and consistency analysis of matches. We describe how the limitations of metamodel matching can be addressed by combining matching with automated requirements analysis and model transformation by example (MTBE) techniques.We show that in practical examples a large percentage of required transformation functionality can usually be constructed automatically, thus potentially reducing development effort. We also evaluate the efficiency of synthesised transformations.Our novel contributions are:The concept of correspondence patterns between metamodels of a transformation.Requirements analysis of transformations using natural language processing (NLP) and machine learning (ML).Symbolic MTBE using ""predictive specification""to infer transformations from examples.Transformation generation in multiple MT languages and in Java, from an abstract intermediate language. © 2021 Association for Computing Machinery.
"
10.1145/3485465,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127430859&origin=inward,Article,SCOPUS_ID:85127430859,scopus,2022-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generating fast specialized simulators for stochastic reaction networks via partial evaluation,"
AbstractView references

Domain-specific modeling languages allow a clear separation between simulation model and simulator and, thus, facilitate the development of simulation models and add to the credibility of simulation results. Partial evaluation provides an effective means for efficiently executing models defined in such languages. However, it also implies some challenges of its own. We illustrate this and solutions based on a simple domain-specific language for biochemical reaction networks as well as on the network representation of the established BioNetGen language. We implement different approaches adopting the same simulation algorithms: one generic simulator that parses models at runtime and one generator that produces a simulator specialized to a given model based on partial evaluation and code generation. For the purpose of better understanding, we additionally generate intermediate variants, where only some parts are partially evaluated. Akin to profile-guided optimization, we use dynamic execution of the model to further optimize the simulators. The performance of the approaches is carefully benchmarked using representative models of small to large biochemical reaction networks. The generic simulator achieves a performance similar to state-of-the-art simulators in the domain, whereas the specialized simulator outperforms established simulation tools with a speedup of more than an order of magnitude. Technical limitations in regard to the size of the generated code are discussed and overcome using a combination of link-time optimization and code separation. A detailed performance study is undertaken, investigating how and where partial evaluation has the largest effect. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
10.1007/s12206-022-0326-0,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127292410&origin=inward,Article,SCOPUS_ID:85127292410,scopus,2022-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rotate vector reducer design using resnet-based model and integration of discretized optimization,"
AbstractView references

The author present an artificial intelligent (AI)-based deep generative model that demonstrate how to generate design options of mechanical systems, which are not only suitable for specific working conditions but also optimized for engineering performance. In current study, (1) a structural generative residual netowork (SG-Resnet) model is developed to establish the non-linear mapping between the working conditions and the external dimensions of the reducer, the main hyperparameters influencing the prediction ability and learning rate of the SG-Resnet are analyzed. (2) The mixed population non dominated sorting genetic algorithm-II (MP-NSGA-II) is proposed, and used to obtain pareto optimal solutions of the internal dimensions of the reducer. Experiments are performed to validate the positive effect of the structural generative model on the stiffness of the reducer. This research provides a novel method for reducer design and lays a solid foundation for the development of sequential engineering software for integrated rotate vector (RV) reducer. © 2022, The Korean Society of Mechanical Engineers and Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.1016/j.scico.2022.102777,S0167642322000107,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123779822&origin=inward,Article,SCOPUS_ID:85123779822,scopus,2022-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),formally verifying consistency of sequence diagrams for safety critical systems,"
                  UML interactions, aka sequence diagrams, are frequently used by engineers to describe expected scenarios of good or bad behaviors of systems under design, as they provide allegedly a simple enough syntax to express a quite large variety of behaviors. This paper uses them to express safety requirements for safety critical systems in an incremental way, where the scenarios are progressively refined after checking the consistency of the requirements. The semantics of these scenarios are expressed by transforming them into an intermediate semantic model amenable to formal verification. In this paper, we rely on the Clock Constraint Specification Language (CCSL) as the intermediate semantic language. In some sense, sequence diagrams and CCSL constraints both express a family of acceptable infinite traces that must include the behaviors given by the finite set of finite execution traces against which we validate. We compare these requirements to actual execution traces to prove the validity of our transformation. As to check the consistency of the sequence diagrams, we present two verification methods based on SMT and clock graphs respectively. The SMT based method relies on our analysis tool called MyCCSL to analyze CCSL constraints. The clock graph based method transforms CCSL constraints into a clock graph, and does the analysis through traversing the clock graph. Finally, these two methods are evaluated against real cases from the railway transit systems. The results show that the SMT based method provides accurate but slow analysis, while the clock graph based method dramatically increases the verification efficiency aiming at two kinds of typical inconsistencies found by the SMT based method.
               "
10.1145/3490099.3511119,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127781560&origin=inward,Conference Paper,SCOPUS_ID:85127781560,scopus,2022-03-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),investigating explainability of generative ai for code through scenario-based design,"
AbstractView references

What does it mean for a generative AI model to be explainable? The emergent discipline of explainable AI (XAI) has made great strides in helping people understand discriminative models. Less attention has been paid to generative models that produce artifacts, rather than decisions, as output. Meanwhile, generative AI (GenAI) technologies are maturing and being applied to application domains such as software engineering. Using scenario-based design and question-driven XAI design approaches, we explore users' explainability needs for GenAI in three software engineering use cases: natural language to code, code translation, and code auto-completion. We conducted 9 workshops with 43 software engineers in which real examples from state-of-the-art generative AI models were used to elicit users' explainability needs. Drawing from prior work, we also propose 4 types of XAI features for GenAI for code and gathered additional design ideas from participants. Our work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains. © 2022 Owner/Author.
"
10.1145/3490100.3516473,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127742767&origin=inward,Conference Paper,SCOPUS_ID:85127742767,scopus,2022-03-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),leveraging generative conversational ai to develop a creative learning environment for computational thinking,"
AbstractView references

We explore how generative conversational AI can assist students' learning, creative, and sensemaking process in a visual programming environment where users can create comics from code. The process of visualizing code in terms of comics involves mapping programming language (code) to natural language (story) and then to visual language (of comics). While this process requires users to brainstorm code examples, metaphors, and story ideas, the recent development in generative models introduces an exciting opportunity for learners to harness their creative superpower and researchers to advance our understanding of how generative conversational AI can augment our intelligence in creative learning contexts. We provide an overview of our system and discuss interaction scenarios to demonstrate ways we can partner with generative conversational AI in the context of learning computer programming. © 2022 Owner/Author.
"
10.1007/s10822-022-00448-3,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126762361&origin=inward,Article,SCOPUS_ID:85126762361,scopus,2022-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),affinity prediction using deep learning based on smiles input for d3r grand challenge 4,"
AbstractView references

Modern molecular docking comprises the prediction of pose and affinity. Prediction of docking poses is required for affinity prediction when three-dimensional coordinates of the ligand have not been provided. However, a large number of feature engineering is required for existing methods. In addition, there is a need for a robust model for the sequential combination of pose and affinity prediction due to the probabilistic deviation of the ligand position issue. We propose a pipeline using a bipartite graph neural network and transfer learning trained on a re-docking dataset. We evaluated our model on the released data from drug design data resource grand challenge 4 (D3R GC4). The two target protein data provided by the challenge have different patterns. The model outperformed the best participant by 9% on the BACE target protein from stage 2. Further, our model showed competitive performance on the CatS target protein. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG.
"
10.1007/s10676-022-09641-2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125472014&origin=inward,Article,SCOPUS_ID:85125472014,scopus,2022-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ethical responsibility and computational design: bespoke surgical tools as an instructive case study,"
AbstractView references

Computational design uses artificial intelligence (AI) to optimise designs towards user-determined goals. When combined with 3D printing, it is possible to develop and construct physical products in a wide range of geometries and materials and encapsulating a range of functionality, with minimal input from human designers. One potential application is the development of bespoke surgical tools, whereby computational design optimises a tool’s morphology for a specific patient’s anatomy and the requirements of the surgical procedure to improve surgical outcomes. This emerging application of AI and 3D printing provides an opportunity to examine whether new technologies affect the ethical responsibilities of those operating in high-consequence domains such as healthcare. This research draws on stakeholder interviews to identify how a range of different professions involved in the design, production, and adoption of computationally designed surgical tools, identify and attribute responsibility within the different stages of a computationally designed tool’s development and deployment. Those interviewed included surgeons and radiologists, fabricators experienced with 3D printing, computational designers, healthcare regulators, bioethicists, and patient advocates. Based on our findings, we identify additional responsibilities that surround the process of creating and using these tools. Additionally, the responsibilities of most professional stakeholders are not limited to individual stages of the tool design and deployment process, and the close collaboration between stakeholders at various stages of the process suggests that collective ethical responsibility may be appropriate in these cases. The role responsibilities of the stakeholders involved in developing the process to create computationally designed tools also change as the technology moves from research and development (R&D) to approved use. © 2022, The Author(s).
"
10.1007/s10664-021-10085-3,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123988141&origin=inward,Article,SCOPUS_ID:85123988141,scopus,2022-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),predicting the objective and priority of issue reports in software repositories,"
AbstractView references

Software repositories such as GitHub host a large number of software entities. Developers collaboratively discuss, implement, use, and share these entities. Proper documentation plays an important role in successful software management and maintenance. Users exploit Issue Tracking Systems, a facility of software repositories, to keep track of issue reports, to manage the workload and processes, and finally, to document the highlight of their team’s effort. An issue report is a rich source of collaboratively-curated software knowledge, and can contain a reported problem, a request for new features, or merely a question about the software product. As the number of these issues increases, it becomes harder to manage them manually. GitHub provides labels for tagging issues, as a means of issue management. However, about half of the issues in GitHub’s top 1000 repositories do not have any labels. In this work, we aim at automating the process of managing issue reports for software teams. We propose a two-stage approach to predict both the objective behind opening an issue and its priority level using feature engineering methods and state-of-the-art text classifiers. To the best of our knowledge, we are the first to fine-tune a Transformer for issue classification. We train and evaluate our models in both project-based and cross-project settings. The latter approach provides a generic prediction model applicable for any unseen software project or projects with little historical data. Our proposed approach can successfully predict the objective and priority level of issue reports with 82 % (fine-tuned RoBERTa) and 75 % (Random Forest) accuracy, respectively. Moreover, we conducted human labeling and evaluation on unlabeled issues from six unseen GitHub projects to assess the performance of the cross-project model on new data. The model achieves 90 % accuracy on the sample set. We measure inter-rater reliability and obtain an average Percent Agreement of 85.3 % and Randolph’s free-marginal Kappa of 0.71 that translate to a substantial agreement among labelers. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
"
10.1016/j.compeleceng.2022.107715,S0045790622000325,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123588536&origin=inward,Article,SCOPUS_ID:85123588536,scopus,2022-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),computer multimedia assisted language and literature teaching using heuristic hidden markov model and statistical language model,"
                  Computer technology has been used for decades in secondary education and foreign language preparation. Still, attempts to incorporate technology have presented educators with various challenges due to rapid progress in technology and occasional changes in language education methods. One way of improving student engagement is to provide connectivity throughout the teaching and learning process in a classroom, allowing students to improve their English language skills. The multimedia classroom allows students to interact with various texts, giving them a solid background intasks and material of mainstream college classes. This paper introduces a method to recognise large unconstrained handwritten text vocabulary using Heuristic Hidden Markov Model and Statistical Language model (HHMM-SLM).This provides statistical language models to be applied to enhance our system's performance. Numerous experiments with single and multiple writer data have been conducted. Language models have been used to improve system accuracy. The variable size Lexica is used (between 10,000 and 50,000 words). Further, a lexicon for English with a very precise width has been developed, which makes its contribution. Our approach is comprehensive and compared to other literature approaches to deal with the same issue.
               "
10.1016/j.cpc.2021.108246,S0010465521003581,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85120851362&origin=inward,Article,SCOPUS_ID:85120851362,scopus,2022-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),alpaca - a level-set based sharp-interface multiresolution solver for conservation laws,"
                  ALPACA is a simulation environment for simulating hyperbolic and (incompletely) parabolic conservation laws with multiple distinct and immiscible phases. As prominent example, consider the compressible Navier-Stokes equations (NSE). Solutions to these equations give insight and understanding of many important engineering applications. Numerical simulations of nonlinear parabolic systems of equations are very challenging for their complex nonlinear dynamics including the propagations of discontinuities such as shocks and phase interfaces. Accurate predictions require high temporal and spatial resolutions for such multi-scale problems. We utilize low dissipation high-resolution methods to capture the dynamics inside the separate phases. Their interaction is modeled by a sharp-interface level-set method with conservative interface-interaction. This allows to accurately locate the interface position and to easily prescribe arbitrary coupling conditions. We tackle the resulting immense computational loads by using a block-based multiresolution (MR) algorithm and adaptive local time stepping. The level-set treatment is integrated into the MR algorithm with little overhead by employing a smart tagging system and adaptive storage of the fluid data in the MR nodes. We embed these methods in a C++20 object-oriented modular framework using state-of-the-art programming paradigms. Furthermore, our implementation is capable to exploit the multiple levels of parallelism in modern high-performance computing (HPC) systems efficiently. We demonstrate the capabilities of our framework by simulating a variety of compressible multi-phase flow problems. Problem-sizes are of 
                        O
                        
                           (
                           
                              
                                 10
                              
                              
                                 10
                              
                           
                           )
                        
                      effective degree of freedom (DOFs). By the use of MR, we typically achieve memory and compute compressions of 
                        >
                        90
                        %
                     . We demonstrate near-optimal parallel performance for scaling runs using 
                        O
                        
                           (
                           
                              
                                 10
                              
                              
                                 4
                              
                           
                           )
                        
                      cores, regardless of the employed numerical models.
               
                  Program summary
                  
                     Program Title: ALPACA - Adaptive Level-set Parallel Code Alpaca
                  
                     CPC Library link to program files: 
                     https://doi.org/10.17632/5zr3sg83ct.1
                  
                  
                     Developer's repository link: 
                     https://gitlab.lrz.de/nanoshock/ALPACA
                  
                  
                     Licensing provisions: GPLv3
                  
                     Programming language: C++20
                  
                     Supplementary material: Code: Copy of the git repository, Videos: Air-helium shock bubble interaction (front and back view), Air-R22 shock bubble interaction, Three bubble shock interface interaction.
                  
                     Nature of problem: Numerical simulation of conservation laws such as the compressible Navier-Stokes equation with several interacting gaseous and liquid phases remains challenging even today. These flows often involve singularities such as shocks and interfaces as well as instabilities driven by their interaction. The inherent highly nonlinear dynamics of those systems leads to a broad range of temporal and spatial scales that have to be resolved. There exists a variety of mutually exclusive numerical models that are able to tackle these challenges. Those, however, are computationally expensive and require computational power that is offered only by large-scale state-of-the-art distributed-memory machines.
                  
                     Solution method: We have developed a modular simulation environment for conservation laws allowing exchange of numerical methods without loss of parallel performance. Computational efficiency is enhanced by employing multi-resolution schemes with adaptive local-time stepping. Our block-based implementation of these schemes is parallelized using the Message Passing Interface and exploits vectorization capabilities of the compute hardware. To simulate distinct and immiscible phases inside the computational domain, we utilize a sharp-interface level-set method. The level-set method allows to accurately locate the interface position and to easily prescribe arbitrary coupling conditions. The narrow-band approach reduces the computational load of the level-set method. We extend this method by a smart tagging system that exploits the block-based nature of the algorithm and further reduces the computational load. The simulation framework is written in modern C++20 and provides a Python interface for integration in Machine Learning and Uncertainty Quantification toolchains. We use parallel HDF5 in combination with XDMF to output field quantities. CMake is used as build system. We have completely annotated the source code using doxygen-style comments, allowing automated documentation generation in different formats. The source code is equipped with CI/CD-automated unit tests and an exhaustive integration test suite.
                  
                     Additional comments including restrictions and unusual features: ALPACA relies on open-source third-party libraries for input and testing. These are packaged as git submodules and automatically integrated. ALPACA is tested on Linux and macOS.
               "
10.1111/bjet.13165,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115223095&origin=inward,Article,SCOPUS_ID:85115223095,scopus,2022-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),extending the technology acceptance model of college learners' mobile-assisted language learning by incorporating psychological constructs,"
AbstractView references

Few studies have investigated the extension of the technology acceptance model (TAM) of mobile-assisted language learning (MALL) by incorporating psychological influence factors. We aimed to determine the factors affecting the continued adoption of MALL by college-age students of English as a foreign language (EFL). We extended the TAM by adding psychological constructs from action control theory and the concept of intrinsic motivation. Data from a large-scale survey of 557 Taiwanese college EFL students recruited through online convenience sampling were analysed through structural equation modelling. The results revealed that the significant predictors of behavioural intention were its antecedents. Three crucial psychological factors, namely nonpreoccupation, nonhesitation, and nonvolatility, significantly predicted behavioural intention. Perceived ubiquity value, tasks, and mobile self-efficacy were strong predictors of intrinsic motivation. Intrinsic motivation significantly predicted behavioural intention through perceived usefulness and perceived ease of use. Finally, perceived ease of use had a moderate effect on behavioural intention through perceived usefulness. The satisfactory explanatory power of the extended model was indicated by the explained variance (R2) of 80% for behavioural intention. This extended TAM may contribute to the long-term development of MALL. Practitioner notes What is already known about this topic Mobile technology has been widely adopted to enhance language learning. However, few studies have investigated individuals' adoption of mobile-assisted language learning (MALL). Factors affecting users' behavioural intention to accept an information system have been researched extensively. The technology acceptance model (TAM) has been extensively used to investigate users' acceptance of different technology in various contexts. What this paper adds This study extended the TAM by incorporating psychological factors to predict the behavioural intention of learners to continue using mobile technology in language learning. The three action-oriented psychological antecedents (nonpreoccupation, nonhesitation, and nonvolatility) used in the model significantly predicted EFL college learners' behavioural intention to continue using mobile devices for language learning. One major factor (intrinsic motivation) significantly affected continued behavioural intention to engage in MALL through perceived ease of use and perceived usefulness. Implications for practice and/or policy Teachers should incorporate various warm-ups, collaborative activities, and positive feedback from others to MALL tasks to reduce feelings of negativity for state-oriented learners and increase willingness to accept MALL for action-oriented learners. Teachers and developers of MALL systems can promote its acceptance by prioritising usable interfaces and enjoyable, challenging tasks in the design of MALL systems to increase learners' intrinsic motivation and perceptions of usefulness and ease of use. Technical guidance, clear and brief operation orientation sessions, and immediate support should be offered to assist learners in engaging in MALL tasks. © 2021 British Educational Research Association
"
10.1145/3523089.3523109,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131334646&origin=inward,Conference Paper,SCOPUS_ID:85131334646,scopus,2022-02-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an improved bp neural network-based quality evaluation model for chinese international education teaching courses,"
AbstractView references

For learners of Chinese as a second language, it is difficult to learn Chinese in the target language environment. With the large-scale applications of intelligent technology in Chinese international education, Chinese teaching is moving towards digitalization, networking and informatization, and computer technology is promoting the development of Chinese international education in a more convenient way. In order to effectively improve the teaching quality of Chinese international education, a scientific and reasonable evaluation system of course teaching quality needs to be formulated. This paper addresses the shortcomings of the traditional course teaching quality evaluation methods, introduces BP neural network to establish the teaching evaluation base model, and sets the corresponding course evaluation indicatores as the input quantity. Considering the problems of local optimum and low convergence efficiency of BP neural network, an improved BP neural network is proposed to optimize the initial weights and thresholds of BP neural network using the beetle antennae search algorithm, so as to obtain the improved teaching quality evaluation model of Chinese international education courses. Then, the performance test of the model is conducted in this paper. The test results show that the proposed teaching quality evaluation model can accurately evaluate the teaching quality of the curriculum and help improve the teaching courses of Chinese international education, which is an innovative attempt to combine computer technology with Chinese international education. Finally, this paper puts forward corresponding suggestions for the combination of Chinese international education and computer technology, in order to promote the further development of Chinese international education. © 2022 ACM.
"
10.1109/TSE.2020.2998503,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125458262&origin=inward,Article,SCOPUS_ID:85125458262,scopus,2022-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automatic generation of acceptance test cases from use case specifications: an nlp-based approach,"
AbstractView references

Acceptance testing is a validation activity performed to ensure the conformance of software systems with respect to their functional requirements. In safety critical systems, it plays a crucial role since it is enforced by software standards, which mandate that each requirement be validated by such testing in a clearly traceable manner. Test engineers need to identify all the representative test execution scenarios from requirements, determine the runtime conditions that trigger these scenarios, and finally provide the input data that satisfy these conditions. Given that requirements specifications are typically large and often provided in natural language (e.g., use case specifications), the generation of acceptance test cases tends to be expensive and error-prone. In this paper, we present Use Case Modeling for System-level, Acceptance Tests Generation (UMTG), an approach that supports the generation of executable, system-level, acceptance test cases from requirements specifications in natural language, with the goal of reducing the manual effort required to generate test cases and ensuring requirements coverage. More specifically, UMTG automates the generation of acceptance test cases based on use case specifications and a domain model for the system under test, which are commonly produced in many development environments. Unlike existing approaches, it does not impose strong restrictions on the expressiveness of use case specifications. We rely on recent advances in natural language processing to automatically identify test scenarios and to generate formal constraints that capture conditions triggering the execution of the scenarios, thus enabling the generation of test data. In two industrial case studies, UMTG automatically and correctly translated 95 percent of the use case specification steps into formal constraints required for test data generation; furthermore, it generated test cases that exercise not only all the test scenarios manually implemented by experts, but also some critical scenarios not previously considered. © 1976-2012 IEEE.
"
10.1002/jcc.26775,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118848461&origin=inward,Article,SCOPUS_ID:85118848461,scopus,2022-01-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),paracopasi: a package for parallel biochemical simulation and analysis,"
AbstractView references

Biochemical simuflation and analysis play a significant role in systems biology research. Numerous software tools have been developed to serve this area. Using these tools for completing tasks, for example, stochastic simulation, parameter fitting and optimization, usually requires sufficient computational power to make the duration of completion acceptable. COPASI is one of the most powerful tools for quantitative simulation and analysis targeted at biological systems. It supports systems biology markup language and covers multiple categories of tasks. This work develops an open source package ParaCopasi for parallel COPASI tasks and investigates its performance regarding accelerations. ParaCopasi can be installed on platforms equipped with multicore CPU to exploit the cores, scaling from desktop computers to large scale high-performance computing clusters. More cores bring more performance. The results show that the parallel efficiency has a positive correlation with the total workload. The parallel efficiency reaches a level of at least 95% for both homogeneous and heterogenous tasks when computational workload is adequate. An example is illustrated by applicating this package in parameter estimation to calibrate a biochemical kinetics model. © 2021 Wiley Periodicals LLC.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171421517&origin=inward,Conference Paper,SCOPUS_ID:85171421517,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated loinc standardization using pre-trained large language models,"
AbstractView references

Harmonization of local source concepts to standard clinical terminologies is a prerequisite for multi-center data aggregation and sharing. Challenges in automating the mapping process stem from the idiosyncratic source encoding schemes adopted by different health systems and the lack of large publicly available training data. In this study, we aim to develop a scalable and generalizable machine learning tool to facilitate standardizing laboratory observations to the Logical Observation Identifiers Names and Codes (LOINC). Specifically, we leverage the contextual embedding from pre-Trained T5 models and propose a two-stage fine-Tuning strategy based on contrastive learning to enable learning in a few-shot setting without manual feature engineering. Our method utilizes unlabeled general LOINC ontology and data augmentation to achieve high accuracy on retrieving the most relevant LOINC targets when limited amount of labeled data are available. We further show that our model generalizes well to unseen targets. Taken together, our approach shows great potential to reduce manual effort in LOINC standardization and can be easily extended to mapping other terminologies. © 2022 P.N. Argaw, E. Healey & I.S. Kohane.
"
10.1109/ICITBS55627.2022.00056,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170075914&origin=inward,Conference Paper,SCOPUS_ID:85170075914,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research on intelligent diagnosis model based on the medical knowledeg graph of multi-source data fusion,"
AbstractView references

Aiming at the problem of the shortage of online medical resources caused by the surge in the amount of online medical consultation and the ineffective use of a large amount of medical data, this paper proposes a comprehensive online intelligent consultation program. First, obtain heterogeneous data sources through multiple channels, use deep learning technology to build a medical knowledge graph, and store it as a knowledge repository then build an intelligent automatic consultation model and medical knowledge retrieval query model, use natural language processing technology to parse semantics for the question of users, and the answer is retrieved in the medical knowledge repository through the semantic query logic conversion, and finally an intelligent questioning model based on the medical knowledge graph covering the whole disease is implemented. This solution can not only meet the needs of patients for online consultation, reduce the workload of medical staff, but also provide clinical decision support services, improve the accuracy of intelligent diagnosis, and reduce the inaccuracy rate of expert decision-making and the cost of consultation. © 2022 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167567603&origin=inward,Conference Paper,SCOPUS_ID:85167567603,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),nanosmad - a satellite mission analysis and design tool for leo nano satellites,"
AbstractView references

Most of the Space Mission Analysis and Design (SMAD) tools available in the industry are geared towards small and large satellites. Nanosatellites and microsatellites below 50 kg use a very different design philosophy leveraging Commercial-Of-The-Shelf (COTS) components, minimal redundancy, higher risk, rapid development times and shorter mission durations. Consequently, the relationships for satellite mass, power, pointing accuracy, design life, and redundancy versus cost available in the literature from general SMAD tools are mostly valid only for satellite platforms of weight 100 kg and higher, built for 3+ mission design life. Here we would like to present NanoSMAD, a SMAD tool for Low Earth Orbiting (LEO) nano and microsatellite mission design that is built on a database containing about 150 Earth-orbiting satellites and subsystem components. The database was constructed based on a survey of commercially available LEO nano and microsatellite products. We have included component-level items with space heritage in our database. The analysis estimates relationships between parameters such as satellite mass, volume, power, sensor, and actuator type, pointing accuracy, transmit power, data rate and cost. These parameters can all be plotted against a choice variable such as cost or satellite mass, power etc. The tool enables users to provide input specifications for mass, volume, power, communication, and pointing requirements to arrive at a preliminary satellite design and to generate a Master Equipment List (MEL) for nano and microsatellite missions. The initial design and MEL can be customized further with more specific inputs or user choice of components from a drop-down menu. The model will then check for compatibility and provide feedback on the validity of customized designs thus avoids inconsistencies. Through this design process, users can arrive at a preliminary design and iterate to arrive at a customized design very quickly. The model is accessed using a web-based Graphical User Interface (GUI) which is built using Python language. We are currently working on providing an accompanying costing estimate for the hardware based on the generated MEL. Copyright © 2022 by the International Astronautical Federation (IAF). All rights reserved.
"
10.31399/asm.cp.istfa2022p0036,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165731443&origin=inward,Conference Paper,SCOPUS_ID:85165731443,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated labeling infrastructure for failure analysis,"
AbstractView references

The development of intelligent assistants helping Failure Analysis (FA) engineers in their daily work is essential to any digitalization strategy. In particular, these systems must solve various computer vision or natural language processing problems to select the most critical information from heterogeneous data, like images or texts, and present it to the users. Modern artificial intelligence (AI) techniques approach these tasks with machine learning (ML) methods. The latter, however, require large volumes of training data to create models to solve the required problems. In most cases, enterprise clouds store vast volumes of data captured while applying various FA methods. Nevertheless, this data is useless for ML training algorithms since it is stored in forms that can only be interpreted by highly-trained specialists. In this paper, we present an approach to embedding an annotation process in the everyday routines of FA engineers. Its services can easily be embedded in existing software solutions to (i) capture and store the semantics of each data piece in machine-readable form, as well as (ii) provide predictions of ML models trained on previously annotated data to simplify the annotation task. Preliminary experiments of the built prototype show that the extension of an image editor used by FA engineers with the services provided by the infrastructure can significantly simplify and speed up the annotation process. Copyright © 2022 ASM International® All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163342571&origin=inward,Conference Paper,SCOPUS_ID:85163342571,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the holon system: artificial general intelligence as work on command,"
AbstractView references

Recent interest in the Large Language Models of deep learning has led to widespread conjecture that artificial general intelligence (AGI) is thereby imminent. At the other end of the spectrum, it has also been claimed that general intelligence cannot exist at all. In this extended abstract, we argue that both of these perspectives are misconceived. We provide a pragmatic definition of general intelligence, grounded in fundamental business and engineering requirements. We explain why a deployed regression model (such as deep learning) cannot meet this criterion for generality of intelligence. We then proceed to describe the Holon system, designed and implemented to meet this criterion. © 2022 IWSSL. All Rights Reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163220075&origin=inward,Conference Paper,SCOPUS_ID:85163220075,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),peer: a comprehensive and multi-task benchmark for protein sequence understanding,"
AbstractView references

We are now witnessing significant progress of deep learning methods in a variety of tasks (or datasets) of proteins. However, there is a lack of a standard benchmark to evaluate the performance of different methods, which hinders the progress of deep learning in this field. In this paper, we propose such a benchmark called PEER, a comprehensive and multi-task benchmark for Protein sEquence undERstanding. PEER provides a set of diverse protein understanding tasks including protein function prediction, protein localization prediction, protein structure prediction, protein-protein interaction prediction, and protein-ligand interaction prediction. We evaluate different types of sequence-based methods for each task including traditional feature engineering approaches, different sequence encoding methods as well as large-scale pre-trained protein language models. In addition, we also investigate the performance of these methods under the multi-task learning setting. Experimental results show that large-scale pre-trained protein language models achieve the best performance for most individual tasks, and jointly training multiple tasks further boosts the performance. The datasets and source codes of this benchmark are all available at https://github.com/DeepGraphLearning/PEER_Benchmark. © 2022 Neural information processing systems foundation. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163186834&origin=inward,Conference Paper,SCOPUS_ID:85163186834,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),logigan: learning logical reasoning via adversarial pre-training,"
AbstractView references

We present LogiGAN, an unsupervised adversarial pre-training framework for improving logical reasoning abilities of language models. Upon automatic identification of logical reasoning phenomena in massive text corpus via detection heuristics, we train language models to predict the masked-out logical statements. Inspired by the facilitation effect of reflective thinking in human learning, we analogically simulate the learning-thinking process with an adversarial Generator-Verifier architecture to assist logic learning. LogiGAN implements a novel sequential GAN approach that (a) circumvents the non-differentiable challenge of the sequential GAN by leveraging the Generator as a sentence-level generative likelihood scorer with a learning objective of reaching scoring consensus with the Verifier; (b) is computationally feasible for large-scale pre-training with longer target length. Both base and large size language models pre-trained with LogiGAN demonstrate obvious performance improvement on 12 datasets requiring general reasoning abilities, revealing the fundamental role of logic in broad reasoning, as well as the effectiveness of LogiGAN. Ablation studies on LogiGAN components reveal the relative orthogonality between linguistic and logic abilities and suggest that reflective thinking's facilitation effect might also generalize to machine learning. © 2022 Neural information processing systems foundation. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162801769&origin=inward,Conference Paper,SCOPUS_ID:85162801769,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),diffusion-based molecule generation with informative prior bridges,"
AbstractView references

AI-based molecule generation provides a promising approach to a large area of biomedical sciences and engineering, such as antibody design, hydrolase engineering, or vaccine development. Because the molecules are governed by physical laws, a key challenge is to incorporate prior information into the training procedure to generate high-quality and realistic molecules. We propose a simple and novel approach to steer the training of diffusion-based generative models with physical and statistics prior information. This is achieved by constructing physically informed diffusion bridges, stochastic processes that guarantee to yield a given observation at the fixed terminal time. We develop a Lyapunov function based method to construct and determine bridges, and propose a number of proposals of informative prior bridges for both high-quality molecule generation and uniformity-promoted 3D point cloud generation. With comprehensive experiments, we show that our method provides a powerful approach to the 3D generation task, yielding molecule structures with better quality and stability scores and more uniformly distributed point clouds of high qualities. © 2022 Neural information processing systems foundation. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160698978&origin=inward,Conference Paper,SCOPUS_ID:85160698978,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),overview of the irse track at fire 2022: information retrieval in software engineering,"
AbstractView references

Code Comments increase the readability of the surrounding code if they highlight concepts that are not evident from the source code itself. Hence, evaluation of the quality of code comments is important to de-clutter large code bases and remove not useful comments. The Information Retrieval in Software Engineering (IRSE) track aims to develop solutions for automated evaluation of code comments. In this track, there is a binary classification task to classify comments as useful and not useful. The dataset consists of 9048 code comments and surrounding code snippet pairs extracted from open source github C based projects. Overall 34 experiments have been submitted by 11 teams from various universities and software companies. The submissions have been evaluated quantitatively using the F1-Score and qualitatively based on the type of features developed, the supervised learning model used and their corresponding hyper-parameters. The best performing architectures mostly have employed transformer architectures coupled with a software development related embedding space. © 2022 Copyright for this paper by its authors.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160681152&origin=inward,Conference Paper,SCOPUS_ID:85160681152,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using transformer-based pre-trained language model for automated evaluation of comments to aid software maintenance,"
AbstractView references

With the increasing number of software applications, evaluating code repositories is of paradigm importance for building elegant software systems. The quality of a code repository is dependent on the readability of the code and the easiness with which it can be understood by other developers. Code commenting is a key requirement to ensure that code is readable, reusable, and reproducible. While useful comments can help software developers write better software, irrelevant and ambiguous comments can be overwhelming and misguiding to developers. Automatic software maintenance can help evaluate code repository and associated comments to provide meaningful insight into code quality and also help improve code comprehension by flagging not useful comments and highlighting useful ones. This paper explores the task of identifying code comment usefulness using a pre-trained transformer-based language model. The data for evaluation has been sourced from FIRE 2022, December 9-13, 2022, Kolkata, India[1]. The paper tries to understand how pre-trained language models like BERT and GPT-2 trained on large text corpora including code repositories can help understand comment usefulness. Such a model can help identify the comment usefulness with a minimum requirement of feature engineering and thus be integrated into an automatic software maintenance system to generalize across code repositories. The proposed model achieves an F1-score 90.15% and accuracy of 91.21% in the test set with 90.12% precision and 90.47% recall. The paper also explores explainable AI techniques like LIME and Attention visualizer to understand how these transformer-based models are identifying comment usefulness and establishing a sense of trust for these black box language models to be used in building automated software maintenance technologies. © 2022 Copyright for this paper Forum for Information Retrieval Evaluation.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160290720&origin=inward,Conference Paper,SCOPUS_ID:85160290720,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),p2p: tuning pre-trained image models for point cloud analysis with point-to-pixel prompting,"
AbstractView references

Nowadays, pre-training big models on large-scale datasets has become a crucial topic in deep learning. The pre-trained models with high representation ability and transferability achieve a great success and dominate many downstream tasks in natural language processing and 2D vision. However, it is non-trivial to promote such a pretraining-tuning paradigm to the 3D vision, given the limited training data that are relatively inconvenient to collect. In this paper, we provide a new perspective of leveraging pre-trained 2D knowledge in 3D domain to tackle this problem, tuning pre-trained image models with the novel Point-to-Pixel prompting for point cloud analysis at a minor parameter cost. Following the principle of prompting engineering, we transform point clouds into colorful images with geometry-preserved projection and geometry-aware coloring to adapt to pre-trained image models, whose weights are kept frozen during the end-to-end optimization of point cloud analysis tasks. We conduct extensive experiments to demonstrate that cooperating with our proposed Point-to-Pixel Prompting, better pre-trained image model will lead to consistently better performance in 3D vision. Enjoying prosperous development from image pre-training field, our method attains 89.3% accuracy on the hardest setting of ScanObjectNN, surpassing conventional point cloud models with much fewer trainable parameters. Our framework also exhibits very competitive performance on ModelNet classification and ShapeNet Part Segmentation. Code is available at https://github.com/wangzy22/P2P. © 2022 Neural information processing systems foundation. All rights reserved.
"
10.1109/ICMERR56497.2022.10097820,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85153771644&origin=inward,Conference Paper,SCOPUS_ID:85153771644,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),no code ai: automatic generation of function block diagrams from documentation and associated heuristic for context-aware ml algorithm training,"
AbstractView references

Industrial process engineering and PLC program development have traditionally favored Function Block Diagram (FBD) programming over classical imperative style programming like the object oriented and functional programming paradigms. The increasing momentum in the adoption and trial of ideas now classified as 'No Code' or 'Low Code' alongside the mainstream success of statistical learning theory or the so-called machine learning is redefining the way in which we structure programs for the digital machine to execute. A principal focus of 'No Code' is deriving executable programs directly from a set of requirement documents or any other documentation that defines consumer or customer expectation. We present a method for generating Function Block Diagram (FBD) programs as either the intermediate or final artifact that can be executed by a target system from a set of requirement documents using a constrained selection algorithm that draws from the top line of an associated recommender system. The results presented demonstrate that this type of No-code generative model is a viable option for industrial process design. © 2022 IEEE.
"
10.1109/ICNGIS54955.2022.10079806,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85153082513&origin=inward,Conference Paper,SCOPUS_ID:85153082513,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),mental factor classification using computational model of consciousness,"
AbstractView references

Artificial Wisdom is advancement of Artificial Intelligence where wisdom should be recognized with the intelligence. It means the constructive behavior and values of humanity need to be the part of Artificial intelligence by incorporating wisdom. These can be demonstrates by simulating thought process and hence thinking ability of human beings is recognized as the consciousness. Currently researchers are working on thoughts and consciousness. These thoughts are coexisted with the particular mental factor. Abhidhamma model of ancient Indian literature are claimed 52 mental factors which are categorized in basic three classes such as Ethically Variable Factor, Unwholesome Factor and Beautiful Factor. Proposed model demonstrates the classification of the mental states. Dataset consists of 445 samples collected from various respondents by asking three questions. Preprocessing is performed by using the techniques of Natural language processing and Non-axiomatic logic. Convolutional Neural Network Machine learning technique applied to classify the mental factors. Performance of the proposed system is measured by applying statistical measures such as Accuracy, Precision, Specificity, Recall and F1-Score. Accuracy for small and large database is obtained as 86.92 percent and 93.02 percent respectively. © 2022 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85152935185&origin=inward,Conference Paper,SCOPUS_ID:85152935185,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),quill: query intent with large language models using retrieval augmentation and multi-stage distillation,"
AbstractView references

Large Language Models (LLMs) have shown impressive results on a variety of text understanding tasks. Search queries though pose a unique challenge, given their short-length and lack of nuance or context. Complicated feature engineering efforts do not always lead to downstream improvements as their performance benefits may be offset by increased complexity of knowledge distillation. Thus, in this paper we make the following contributions: (1) We demonstrate that Retrieval Augmentation of queries provides LLMs with valuable additional context enabling improved understanding. While Retrieval Augmentation typically increases latency of LMs (thus hurting distillation efficacy), (2) we provide a practical and effective way of distilling Retrieval Augmentation LLMs. Specifically, we use a novel two-stage distillation approach that allows us to carry over the gains of retrieval augmentation, without suffering the increased compute typically associated with it. (3) We demonstrate the benefits of the proposed approach (QUILL) on a billion-scale, real-world query understanding system resulting in huge gains. Via extensive experiments, including on public benchmarks, we believe this work offers a recipe for practical use of retrieval-augmented query understanding. © 2022 Association for Computational Linguistics.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85152144019&origin=inward,Conference Paper,SCOPUS_ID:85152144019,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),arguments to key points mapping with prompt-based learning,"
AbstractView references

Handling and digesting a huge amount of information in an efficient manner has been a long-term demand in modern society. Some solutions to map key points (short textual summaries capturing essential information and filtering redundancies) to a large number of arguments/opinions have been provided recently (Bar-Haim et al., 2020). To complement the full picture of the argument-to-keypoint mapping task, we mainly propose two approaches in this paper. The first approach is to incorporate prompt engineering for fine-tuning the pre-trained language models (PLMs). The second approach utilizes prompt-based learning in PLMs to generate intermediary texts, which are then combined with the original argument-keypoint pairs and fed as inputs to a classifier, thereby mapping them. Furthermore, we extend the experiments to cross/in-domain to conduct an in-depth analysis. In our evaluation, we find that i) using prompt engineering in a more direct way (Approach 1) can yield promising results and improve the performance; ii) Approach 2 performs considerably worse than Approach 1 due to the negation issue of the PLM. © ICNLSP 2022.All rights reserved
"
10.1109/TrustCom56396.2022.00205,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151752286&origin=inward,Conference Paper,SCOPUS_ID:85151752286,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),pre-training fine-tuning data enhancement method based on active learning,"
AbstractView references

With the development of Internet technology, the number of Internet users increases rapidly, and the amount of data generated on the Internet is very large every day. At the same time, with the development of storage technology and query technology, it is very easy to collect massive data, but the information value contained in these data is uneven, and most of them are unmarked. However, traditional supervised learning has a great demand for labeled samples. Faced with a large number of unlabeled samples, there is a problem of the lack of effective automatic labeling methods, and manual labeling costs are high. If the strategy of simple random sampling is used for annotation, it may lead to the selection of noisy information and waste of resources, and low-quality training data could also have an influence on the prediction accuracy of the model. Meanwhile, the training effect of traditional deep learning methods is very limited for small sample labeled training sets.This paper takes the text emotion analysis task in natural language processing as the background, selects IMDB film review data as the training set and test set, starts with the design of active learning algorithm based on clustering analysis, combined with the appropriate pre-training fine-tuning model, constructs a data enhancement method based on active learning. In the experiment, it is found that when the labeled training set is reduced by 90%, the prediction accuracy of the pre-training model is reduced by no more than 2%, which verifies the effectiveness of the data enhancement method combining active learning with the pre-training model. © 2022 IEEE.
"
10.1109/QRS57517.2022.00098,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151453328&origin=inward,Conference Paper,SCOPUS_ID:85151453328,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),codebert-nt: code naturalness via codebert,"
AbstractView references

Much of recent software-engineering research has investigated the naturalness of code, the fact that code, in small code snippets, is repetitive and can be predicted using statistical language models like n-gram. Although powerful, training such models on large code corpus can be tedious, time consuming and sensitive to code patterns (and practices) encountered during training. Consequently, these models are often trained on a small corpus and thus only estimate the language naturalness relative to a specific style of programming or type of project. To overcome these issues, we investigate the use of pre-trained generative language models to infer code naturalness. Pre-trained models are often built on big data, are easy to use in an out-of-the-box way and include powerful learning associations mechanisms. Our key idea is to quantify code naturalness through its predictability, by using state-of-the-art generative pre-trained language models. Thus, we suggest to infer naturalness by masking (omitting) code tokens, one at a time, of code-sequences, and checking the models' ability to predict them. We explore three different predictability metrics; a) measuring the number of exact matches of the predictions, b) computing the embedding similarity between the original and predicted code, i.e., similarity at the vector space, and c) computing the confidence of the model when doing the token completion task regardless of the outcome. We implement this workflow, named CODEBERT-NT, and evaluate its capability to prioritize buggy lines over non-buggy ones when ranking code based on its naturalness. Our results, on 2,510 buggy versions of 40 projects from the SmartShark dataset, show that CODEBERTNT outperforms both, random-uniform and complexity-based ranking techniques, and yields comparable results to the n-gram models. © 2022 IEEE.
"
10.1109/ICKECS56523.2022.10060389,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150678196&origin=inward,Conference Paper,SCOPUS_ID:85150678196,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),improve python's random forest algorithm in the intelligent construction of digital system in power grid enterprises,"
AbstractView references

Power grid enterprises play an important role in the development of national economy, with a large scale of management assets and high requirements for internal audit. In order to meet the increasing requirements of power grid enterprises for internal audit, the internal audit work on the one hand should actively use information means to carry out digital audit, on the other hand, it is necessary to cooperate with business activities to conduct industrial audit integration, so as to better play the function of internal audit. This paper aims to study the intelligent construction of the improved Python random forest algorithm in the digital system of power grid enterprises. This paper takes the context of digital transformation, First of all, focusing on the literature research of power supply service level, digital transformation and digital practice, Clarifying the research focus, direction, and content of this study, Secondly, around the concept of power supply service and enterprise digital transformation, To study the theoretical basis of service characteristics, service leading logic, service package, service quality measurement and so on, Laid the foundation for the status of power supply service level of A power grid enterprises, Then conduct research for A power grid enterprise customers and internal personnel, Summarize the problems existing in the power supply service, And to analyze the causes of the problems found in the investigation, Four reasons are identified, They are that the service platform operation lacks Internet thinking, the service evaluation mode is more conservative, the customer management mode is not scientific, and the employee allocation mode is not scientific, Then we proposed 10 strategies from the two aspects of optimizing existing services and developing value-added services, Including improving the operation level of the service platform, improving the business model of responding to customer needs, optimizing the power supply service feedback and evaluation system, narrowing the gap in regional power grid services, and actively connecting with digital government requirements, As well as the establishment of customer maintenance service system, to create a new energy vehicle service ecology, to provide comprehensive energy services, to explore the financial attributes of power commodities, to increase the administrative disclosure of agent power purchase business. Finally, the safeguard measures are put forward for the strategy implementation, and the corresponding measures are formulated from four aspects: enterprise organization guarantee, digital talent guarantee, compliance guarantee and system guarantee © 2022 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150336534&origin=inward,Conference Paper,SCOPUS_ID:85150336534,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),perceiver io: a general architecture for structured inputs &amp; outputs,"
AbstractView references

A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence. © 2022 ICLR 2022 - 10th International Conference on Learning Representationss. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149831728&origin=inward,Conference Paper,SCOPUS_ID:85149831728,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),uncertainty quantification with pre-trained language models: a large-scale empirical analysis,"
AbstractView references

Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline: (1) the choice and (2) the size of PLM, (3) the choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and many more. Although prior work has looked into some of these considerations, they usually draw conclusions based on a limited scope of empirical studies. There still lacks a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline. To fill this void, we compare a wide range of popular options for each consideration based on three prevalent NLP classification tasks and the setting of domain shift. In response, we recommend the following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if possible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal Loss for fine-tuning. © 2022 Association for Computational Linguistics.
"
10.1109/PDGC56933.2022.10053307,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149771404&origin=inward,Conference Paper,SCOPUS_ID:85149771404,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),machine learning-based malware detection using stacking of opcodes and bytecode sequences,"
AbstractView references

Malware detection is a complex problem. The commonly used signature-based technique cannot detect unknown or zero-day malware. Traditional machine learning-based methods can identify unknown malicious programs but require high domain expertise for feature engineering. This research study presents a new method of malware detection using the stacking of static opcode and bytecode features. At first, bytecode and opcode sequences of executables files are extracted using a disassembler. After that, an NLP technique, TF-IDF, is employed to vectorize the extracted feature map. The extracted features as opcodes and bytecodes are concatenated to combine a single feature map, which serves as input to train machine learning classifiers: SVM, k-NN, and Random Forest (RF). The RF classifier obtained the best accuracy, precision, recall, and F-score results as 98.47%, 98.60%,98.46%, and 98.47% even using small data counts of bench-marked Microsoft BIG data set. The accuracy of the best model is comparatively better than the accuracy of using bytecodes and opcodes as individual feature vectors. The empirical results signify that the proposed method is helpful to the security industries with the benefit of being less dependent on manual feature engineering and can reduce the burden on virus databases by handling large-scale malware. © 2022 IEEE.
"
10.1109/CRC55853.2022.10041228,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149337540&origin=inward,Conference Paper,SCOPUS_ID:85149337540,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),command pattern-based mof design and testing,"
AbstractView references

With the development of science and technology, the systems become more and more complex, and the difficulty of system development and maintenance also increases. To solve this problem, researchers propose MBSE(Model-Based Systems Engineering) and its standard system language SysML. At present, there are not many system modeling tools, and their scalability and quality need to be improved. This paper explores the design and testing of MOF(Meta Object Facility) based on the command pattern, trying to improve the expansibility and software quality of SysML modeling tools. Firstly, the MOF layer based on the command pattern is implemented to maintain the model data, then an existing modeling platform is refactored based on this base layer, and a large number of test codes are generated to ensure the reliability of the system. The experimental results show that the scalability and reliability of the system have been effectively improved. © 2022 IEEE.
"
10.1109/APSEC57359.2022.00090,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149177684&origin=inward,Conference Paper,SCOPUS_ID:85149177684,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an additional approach to pre-trained code model with multilingual natural languages,"
AbstractView references

Pre-trained language models have achieved many prominent results in natural language processing. Since software engineering widely includes many natural language documents, the application of pre-trained language models have received much attention in software engineering tasks. However, pretraining a large volume of source code requires a huge amount of computational resources and time. In this study, we propose an additional pre-training approach to a well-trained language model. Our initial results on mT5, multilingual T5 with an additional pretraining of Python code shows improved performance on multiple software engineering tasks including code generation, code summarisation, code repair, and error diagnosis. © 2022 IEEE.
"
10.1115/IMECE2022-90956,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85148695251&origin=inward,Conference Paper,SCOPUS_ID:85148695251,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a model-based approach for integrated variation management,"
AbstractView references

Variation management is a responsible task for product developers, which have to balance the ever-increasing quality demands and cost pressures, while considering the product design as well as the manufacturing and assembly process. These aspects have a direct impact on its subsequent success in the market. Therefore, a large number of different activities of variation management are necessary. In this area, a wide variety of mostly document-centered methods support product developers, which have individual interfaces. Thus, it is currently not possible to map the entire variation management process in a single model. Especially with regard to the increasing availability of large amounts of data, the potential of an integrated variation management cannot be exploited efficiently. For this reason, this paper presents a novel model-based approach for the development of a combined system and tolerancing model. This model contains the processes and activities of integrated variation management and links them with further system models and the corresponding data. The presented approach is a superordinate model for variation management as well as its processes and provides the modeling of individual views of different stakeholders. In addition, process- and program-specific solutions can be integrated into the model, which enables a cross-linking of the data beyond their interfaces. In this paper, the approach is realized using the systems modeling language (SysML). Copyright © 2022 by ASME.
"
10.1109/PuneCon55413.2022.10014939,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147544331&origin=inward,Conference Paper,SCOPUS_ID:85147544331,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),personality prediction with natural language processing using questionnaire responses,"
AbstractView references

As the modern IT revolution is booming at a rapid growth speed, organizations and recruiters are finding it increasingly challenging to select the ideal applicant from a large number of applicants with diverse skill sets and personalities. Hence, selecting a candidate with a suitable personality for respective job profiles is a very important and great challenge for the HR department nowadays. Out of various personality prediction methods available out there, Myers-Briggs Type Indicator or MBTI is famous and accurate for our purpose of creating a personality prediction system for selecting candidates based on their personality. This study took into account all sixteen MB-Model coordinates. A comparative study of Random Forest, Logistic Regression, SVM, XGBoost has been done to perform personality prediction, and accuracy and confusion matrix for performance measurement of the models. While using TF-IDF, for the personality categories like Introversion/Extroversion the accuracy is 80.46%, for Sensing/Intuition it is 88.70%, for Thinking/Feeling it is 81.21% and for Perceiving vs Judging it is 72.97% with the Logistic Regression algorithm. Using Count vectorization for tokenizing, the accuracy is 80.97% for Introversion/Extroversion, for Sensing/Intuition it is 88.93%, for Thinking/Feeling it is 77.92% and for Perceiving vs Judging it is 73.48% with XGBoost algorithm, which gave the best performance. © 2022 IEEE.
"
10.1109/TOCS56154.2022.10016201,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147251944&origin=inward,Conference Paper,SCOPUS_ID:85147251944,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),pivot-based unsupervised domain adaptation for pre-trained language model,"
AbstractView references

In the task of text classification, natural language processing technology provides an effective solution for automatically identifying text content classification, but labeled data is difficult to obtain in specific domains. To reduce manual labeling, some researchers have proposed unsupervised domain adaptation technology, which is a special transfer learning technology, transferring source domain models suitable for general knowledge to the target domain with less labeled data, to improve the generalization effect of the model in the target domain. However, the current unsupervised domain adaptation methods are mostly to fine-tune the pre-trained model directly using unsupervised data from the target domain. This method needs a large amount of unsupervised data as usual to improve the training effect. Therefore, this paper presents a pivot-based unsupervised domain adaptation method, which extracts and masks pivots from unsupervised data, fine-tunes the pre-trained language model, and finally validates the method using supervised training, compared with the method of directly using unsupervised data to fine-tune the original model. The pivot-based domain adaptation method effectively improves the efficiency of domain knowledge transfer for the specific domain. © 2022 IEEE.
"
10.1109/SIBCON56144.2022.10003001,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146839101&origin=inward,Conference Paper,SCOPUS_ID:85146839101,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),improving the radioelectronic device simulation quality by using a step recovery diode,"
AbstractView references

Computer design of radio electronic facilities and systems is currently the main tool for their creation by radio engineers. Diodes are widespread among the electronic component base used in the design. Modern physical layer models of diodes describe their operation with good accuracy, however, equivalent circuits models are used in radio engineering computer-aided design systems. In the vast majority of cases, these are simplified quasi-static models developed in the 70s of the last century. So, the dynamics of the diode operation is observed with a large error, and some aspects of transient processes are not modeled at all. In this paper we consider a refined non-quasi-static model of a diode with the dependence of the lifetime of nonequilibrium charge carriers on the forward current, the mathematical apparatus of which is expressed in the language of equivalent circuits. Therefore, it can be implemented directly by engineers. Using the dependence between the lifetime of nonequilibrium charge carriers and the forward current at a high level of injection in the non-quasistatic diode model, the modeling error of the output voltage of the push-pull pulse sharper does no more than 5%. The standard quasi-static model gives a significantly larger modeling error for both waveform and position. It is shown that the delay between the experimental and model curve is reduced by a factor of half. © 2022 IEEE.
"
10.1007/978-3-031-23515-3_7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146723344&origin=inward,Conference Paper,SCOPUS_ID:85146723344,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research directions in process modeling and mining using knowledge graphs and machine learning,"
AbstractView references

Services Computing has seen a dramatic rise in the last twenty years. The foundation for services provided by enterprises is business processes, so progress in the development of effective and efficient processes is of utmost importance. The design or modeling of business processes is a challenging task. Over the years many research and development efforts have paid dividends, including languages and notations like the Business Process Executing Language and the Business Process Modeling Notation, along with supporting methodologies and tools. Research in Semantic Web Services and Processes showed promise for the automation of services discovery and composition (orchestration/choreography). The current large-scale deployment of enterprise knowledge graphs by many organizations coupled with huge advancements in machine learning (particularly deep learning) provides new opportunities for advancing this automation forward. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145648889&origin=inward,Conference Paper,SCOPUS_ID:85145648889,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using breeze for modelling software architecture of component-based systems,"
AbstractView references

We present Breeze, a tool that aims at modelling software architecture with dynamic change to facilitate the design of static architecture model and support the architecture reconfiguration. Breeze is based on an architecture description language – Breeze/ADL, which adopts XML as the metalanguage. A key advantage of Breeze is that it yields a comprehensive model of the software system in the early stages of design processing, and employs graph transformation to capture the changes during both initial design and subsequent evolution. Breeze is scalable and can be applied to large, complex software systems as they evolve. © 2022 EUROSIS-ETI. All Rights Reserved.
"
10.1109/ICITSI56531.2022.9970920,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145438250&origin=inward,Conference Paper,SCOPUS_ID:85145438250,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),commonkads for knowledge based system development: a literature study,"
AbstractView references

COMMONKADS is a method for developing knowledge-based system. This method describes foundation, technique, modeling language and document structure for develop the knowledge-based system. COMMONKADS is people-oriented system development methodology, and this methodology is often used for developing organizational knowledge management system. COMMONKADS approach is divided based on context (organizational model, task model, agent model), concept (knowledge model) and artifact (design model). COMMONKADS have been used widely for knowledge-based system in several fields, such as COMMONKADS that integrated in tourism knowledge-based system, COMMONKADS for irrigation expert system, expertise model using COMMONKADS in manufactured company, COMMONKADS in energy management system and many more. Generally, there are eight strengths of COMMONKADS methodology for develop knowledge-based system. Its strength is flexible to use in any scope, represent knowledge (organizational, domain, task and inference knowledge), complete (representation, model, and form), powerful, accurate, comprehensive, represent KM process, systematic and effective. While the weakness of COMMONKADS methodology only three, there are don't have validation process and difficult to acquisition knowledge and use semi formal language, large data storage. Nevertheless, COMMONKADS is recommended methodology for develop knowledge-based system. © 2022 IEEE.
"
10.36680/j.itcon.2022.043,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144309918&origin=inward,Article,SCOPUS_ID:85144309918,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"artificial intelligence in construction asset management: a review of present status, challenges and future opportunities","
AbstractView references

The built environment is responsible for roughly 40% of global greenhouse emissions, making the sector a crucial factor for climate change and sustainability. Meanwhile, other sectors (like manufacturing) adopted Artificial Intelligence (AI) to solve complex, non-linear problems to reduce waste, inefficiency, and pollution. Therefore, many research efforts in the Architecture, Engineering, and Construction community have recently tried introducing AI into building asset management (AM) processes. Since AM encompasses a broad set of disciplines, an overview of several AI applications, current research gaps, and trends is needed. In this context, this study conducted the first state-of-the-art research on AI for building asset management. A total of 578 papers were analyzed with bibliometric tools to identify prominent institutions, topics, and journals. The quantitative analysis helped determine the most researched areas of AM and which AI techniques are applied. The areas were furtherly investigated by reading in-depth the 83 most relevant studies selected by screening the articles' abstracts identified in the bibliometric analysis. The results reveal many applications for Energy Management, Condition assessment, Risk management, and Project management areas. Finally, the literature review identified three main trends that can be a reference point for future studies made by practitioners or researchers: Digital Twin, Generative Adversarial Networks (with synthetic images) for data augmentation, and Deep Reinforcement Learning. © 2022 International Council for Research and Innovation in Building and Construction. All rights reserved.
"
10.1061/9780784484449.023,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144282821&origin=inward,Conference Paper,SCOPUS_ID:85144282821,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),polymer injection to remediate liquefaction induced foundation settlement: numerical simulation of shake table experiments,"
AbstractView references

A series of two large-scale shake table tests were performed to investigate the efficacy of the polymer injection technique in remediating liquefaction-induced foundation settlement. In these tests, foundation-ground system response was studied first without and subsequently with polymer injected into the liquefiable stratum, resulting in a set of unique and comprehensive data. Here, a set of preliminary class C1 numerical simulations of these tests are presented using a 3D two-phase (solid-fluid) fully coupled finite element model with parameters partially calibrated from earlier studies. The physical and numerical models indicate an extensive reduction in the liquefaction-induced foundation deformations due to the injection and associated stiffening of the ground. Comparisons are made between computed and recorded engineering demand parameters (EDPs), including excess pore pressure, soil acceleration, and foundation settlement. A reasonable match is observed to the physical response, and the calibrated model is further used to run an additional scenario with sloping ground conditions. Directions for further improvement of the model are then suggested. © 2022 Lifelines 2022: 1971 San Fernando Earthquake and Lifeline Infrastructure - Selected Papers from the Lifelines 2022 Conference. All rights reserved.
"
10.1088/1755-1315/1101/8/082015,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144217271&origin=inward,Conference Paper,SCOPUS_ID:85144217271,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),digital twin applications using the simultan data model and python,"
AbstractView references

Python is an open, general-purpose programming language that is used in many tools, libraries and APIs for Building Performance Simulations (BPS). Advantages of Python in the context of digital twins are the simple and powerful capabilities to generate input files, automate processes, import libraries in many languages and a large number of useful modules. However, in order to use BPS tools and libraries with real time data, a comprehensive data model is required in which all necessary data such as geometry, system engineering, databases, sensors, or simulation parameters for the different BPS are defined. Python in combination with SIMULTAN as a suitable open Building Information Modelling (BIM) data model allows an effective use of these tools and libraries to perform and automate analyses. This paper presents a Python module that integrates the SIMULTAN model in Python and enables almost seamless integration with minor adaptations to existing tools or modules. The import is achieved using simple text-based templates for the data types and their mapping in the data model. The data model, the definition of the data types and the use of this module is demonstrated by calculating the trend of the CO2 concentration in a zone of a digital twin using real time data. © Published under licence by IOP Publishing Ltd.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143897793&origin=inward,Conference Paper,SCOPUS_ID:85143897793,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),bigbio: a framework for data-centric biomedical natural language processing,"
AbstractView references

Training and evaluating language models increasingly requires the construction of meta-datasets - diverse collections of curated data with clear provenance. Natural language prompting has recently lead to improved zero-shot generalization by transforming existing, supervised datasets into a variety of novel instruction tuning tasks, highlighting the benefits of meta-dataset curation. While successful in general-domain text, translating these data-centric approaches to biomedical language modeling remains challenging, as labeled biomedical datasets are significantly underrepresented in popular data hubs. To address this challenge, we introduce BIGBIO a community library of 126+ biomedical NLP datasets, currently covering 13 task categories and 10+ languages. BIGBIO facilitates reproducible meta-dataset curation via programmatic access to datasets and their metadata, and is compatible with current platforms for prompt engineering and end-to-end few/zero shot language model evaluation. We discuss our process for task schema harmonization, data auditing, contribution guidelines, and outline two illustrative use cases: zero-shot evaluation of biomedical prompts and large-scale, multi-task learning. BIGBIO is an ongoing community effort and is available at this URL. © 2022 Neural information processing systems foundation. All rights reserved.
"
10.1109/SIST54437.2022.9945703,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143380773&origin=inward,Conference Paper,SCOPUS_ID:85143380773,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),computer simulation of intelligent control systems for high-precision cruise missiles,"
AbstractView references

This article discusses an intelligent flight control system for high-precision cruise missiles. In the field of space engineering and technologies, the most effective aspects of using computer programs and studying space analysis in a computer program are considered. Calculations and computer studies of precision guidance and guidance of high-speed cruise missiles are presented. Therefore, system of the control is set by the fuzzy logic method based on artificial intelligence. Computer modeling of intelligent control systems for high-precision cruise missiles. The cruise missile control system requires a large number of parameters and complex mathematical calculation models and physical problems to control the intelligent system. The article provides a simulator in a computer program to test high-speed cruise missiles. This simulator is written in Matlab editor R2015b in C ++ programming language. © 2022 IEEE.
"
10.1115/ICONE29-91909,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143197760&origin=inward,Conference Paper,SCOPUS_ID:85143197760,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),analysis of hidden fault in digital instrumentation and control system of nuclear power plant based on environmental model,"
AbstractView references

Nuclear safety is one of the key issues for a nuclear power plant (NPP). The instrumentation and control system (I&C system) plays critical roles for the safe and efficient operation of an NPP. Due to the large scale and complex logic of the I&C system, some hidden faults cannot be found by factory testing or on-site debugging and may cause system degradation or even system failure during NPP operation. These hidden faults may come from system design omissions, software programming errors, personnel operation failures, natural environment, inappropriate preventive maintenance, etc. Timely identification of these hidden faults is of great significance for improving the reliability of the I&C system. This paper takes the hidden faults of the I&C system as the research object, and carries out the analysis and investigation of the hidden faults based on the environmental model. After determining the key and important characteristics of the I&C system to be analyzed, this paper analyzes and classifies the triggering causes of the faults from three dimensions of specific application environment, specific application method and long-term dynamic operation. This paper takes the hidden fault ""Failure of Power Module of the DCS Cabinet"", ""Operation failure of KCS troubleshooting personnel"", and ""Communication abnormal of DCS internal network "" actually found in the I&C system of an NPP as cases to establish the mapping relationship between the hidden fault and the environmental model to verify the correctness of the built model. The work showed in this paper can contribute to improve the identification capability of hidden faults in I&C system of the NPPs, which has important engineering significance for improving the reliability of the I&C system. Copyright © 2022 by ASME.
"
10.1109/Metamaterials54993.2022.9920776,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142826178&origin=inward,Conference Paper,SCOPUS_ID:85142826178,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),representing linear equations and pulse propagation within waveguide junctions using petri-nets,"
AbstractView references

In this work, we explore and demonstrate how Petri-nets (PNs) can be exploited to represent linear equations. Such feature is then applied to graphically model the propagation of electromagnetic (EM) pulses within waveguide junctions. PNs have been used in a wide range of research and industrial scenarios to study dynamic systems such as designing asynchronous circuits and chemical engineering, here we exploit features of PNs to represent TEM square pulses in waveguide junctions. In this realm, as new approaches to computing with photonics may require modelling large systems that may need computationally intensive techniques, we discuss how PNs can provide a fast, efficient, and visual modelling of EM pulse propagation within a network of interconnected waveguides. This work represents a step towards allowing individuals from varying areas of expertise to contribute via a common visual language to the future of photonic computing systems and control. © 2022 IEEE.
"
10.1007/978-3-031-19775-8_23,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142733429&origin=inward,Conference Paper,SCOPUS_ID:85142733429,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),steex: steering counterfactual explanations with semantics,"
AbstractView references

As deep learning models are increasingly used in safety-critical applications, explainability and trustworthiness become major concerns. For simple images, such as low-resolution face portraits, synthesizing visual counterfactual explanations has recently been proposed as a way to uncover the decision mechanisms of a trained classification model. In this work, we address the problem of producing cotual explanations for high-quality images and complex scenes. Leveraging recent semantic-to-image models, we propose a new generative counterfactual explanation framework that produces plausible and sparse modifications which preserve the overall scene structure. Furthermore, we introduce the concept of “region-targeted counterfactual explanations”, and a corresponding framework, where users can guide the generation of counterfactuals by specifying a set of semantic regions of the query image the explanation must be about. Extensive experiments are conducted on challenging datasets including high-quality portraits (CelebAMask-HQ) and driving scenes (BDD100k). Code is available at: https://github.com/valeoai/STEEX. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.4324/9780429276866-3,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142553367&origin=inward,Book Chapter,SCOPUS_ID:85142553367,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),theorizing on the marginalization of boys and girls in caribbean schooling: recurring myths and emerging realities,"
AbstractView references

There are multiple marginalizations within Caribbean education systems. One frequently studied target is gender. Early work in the Caribbean brought to light the possibility of marginalized males within the education system. However, the broader claim of marginalized males in society led to a prolonged and distracting debate, which has subsequently limited further theoretical gaze and policy development. The fact is that both males and females can be marginalized within postcolonial elitist school systems but in different areas and to varying degrees. These outcomes might even be independent of existing patterns of inequity within the economic and social sectors. A key construct to better understanding marginalization processes in schooling is intersectionality. This changes the single-axis gender question of whether or not boys or girls are failing to which boys and which girls underperform. In this chapter, I first examine the debates about the perceived underperformance of males in Caribbean education systems, identifying multiple outcomes, recurring myths, and emerging evidence from large-scale assessment data. I also explore benchmarking data to show that many Caribbean males do underperform in reading and language arts, fail to complete secondary school, and access tertiary education in lower numbers. At the same time, many females are not accessing science, technology, engineering, and mathematics subjects in secondary school. There is little robust Caribbean theory to explain these patterns. I show that crafting sound equity policy requires robust evidence-informed theory. The current body of work on male academic achievement in the Caribbean does not always provide such evidence. To address future theorizing, I present a synthesis model of gendered performance in Caribbean schooling based on findings from case studies in the literature, empirical comparative analysis, and benchmarking data from large-scale assessments. © 2023 Taylor and Francis.
"
10.1115/DETC2022-90895,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142544048&origin=inward,Conference Paper,SCOPUS_ID:85142544048,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),natural language processing for content analysis of communication in collaborative design,"
AbstractView references

We address the problem of content analysis in text-based engineering design communication. Existing methods to characterize communication content in engineering design are manual or qualitative, which is tedious for large datasets. We formulate the characterization of communication messages as an intent classification task. We identify two intents—Intent 1 captures the presence and flow of information, Intent 2 captures specific topics about design parameters and objectives. We compare the predictive accuracy of convolutional LSTM, character-based convolutional LSTM, XLNet, and BERT models for the intent classification task. The results of our comparison show that the XLNet model predicts Intents 1 and 2 with 88% and 81% accuracy, respectively, on text data collected from 40 teams in a design experiment with university students. We analyze the differences in communication patterns between high- and low-performing teams. Time-series studies show that high-performing teams have more responsive communication and a higher consistency of information exchange. Copyright © 2022 by ASME.
"
10.1115/DETC2022-90366,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142499122&origin=inward,Conference Paper,SCOPUS_ID:85142499122,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative pre-trained transformers for biologically inspired design,"
AbstractView references

Biological systems in nature have evolved for millions of years to adapt and survive the environment. Many features they developed can be inspirational and beneficial for solving technical problems in modern industries. This leads to a specific form of design-by-analogy called bio-inspired design (BID). Although BID as a design method has been proven beneficial, the gap between biology and engineering continuously hinders designers from effectively applying the method. Therefore, we explore the recent advance of artificial intelligence (AI) for a computational approach to bridge the gap. This paper proposes a generative design approach based on the pre-trained language model (PLM) to automatically retrieve and map biological analogy and generate BID in the form of natural language. The latest generative pre-trained transformer, namely GPT-3, is used as the base PLM. Three types of design concept generators are identified and fine-tuned from the PLM according to the looseness of the problem space representation. Machine evaluators are also fine-tuned to assess the correlation between the domains within the generated BID concepts. The approach is then tested via a case study in which the fine-tuned models are applied to generate and evaluate light-weighted flying car concepts inspired by nature. The results show our approach can generate BID concepts with good performance. Copyright © 2022 by ASME.
"
10.1117/12.2646476,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141781962&origin=inward,Conference Paper,SCOPUS_ID:85141781962,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative design and digital manufacturing: using ai and robots to build lightweight instruments,"
AbstractView references

Digital Engineering technologies are transforming long-stagnant development processes by applying the tremendous advancements in Information Technology (IT) to classical engineering tasks such as design, analysis, and fabrication of space-flight instrument structures. Generative Design leverages developments in Artificial Intelligence (AI) and Cloud computing to enable a paradigm shift in the design process, allowing the engineer to focus on defining the requirements and objectives of the design while AI generates optimized designs which comply with the input requirements. Digital Manufacturing allows these complex lightweight designs to be efficiently manufactured by directly fabricating from the resulting 3D models. The development of these two Digital Engineering technologies realizes significant mass savings while simultaneously reducing structure development time from months to days. This paper describes the development of the Evolved Structures process applying these technologies to spaceflight optical instrument structures including an example demonstrating greater than 10x reduction in development time/cost and greater than 3x improvement in structural performance. © 2022 SPIE. All rights reserved.
"
10.5381/jot.2022.21.1.a5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141766053&origin=inward,Article,SCOPUS_ID:85141766053,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),language composition via kind-typed symbol tables,"
AbstractView references

The modularization of domain-specific modeling languages (DSMLs) fosters individual reuse of DSMLs in different contexts. Within this article, we discuss how it is possible to refer to model elements of other languages when composing different DSMLs. Related approaches usually rely on a DSML-agnostic language infrastructure that tests the compatibility of model elements via types encoded in Strings without any consistency checks. We propose the ""strongly kind-typed"" symbol table as an extension to the compiler approach to integrate the syntax of the languages using symbol tables that assign a symbol kind to each name definition. Our approach can be integrated into language workbenches that provide a symbol table infrastructure as part of a DSML implementation. The kind-typed symbol tables are integrated into the language workbench MontiCore. Strongly kind-typed symbol tables utilize the type system of the language workbench’s host language to ensure type consistency between the language-specific symbol table infrastructures during DSML composition, which ultimately supports DSML engineering in the large. © 2022, Journal of Object Technology. All Rights Reserved.
"
10.5381/jot.2022.21.4.a10,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141750054&origin=inward,Article,SCOPUS_ID:85141750054,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),model-driven engineering for complex event processing: a survey,"
AbstractView references

Complex Event Processing (CEP) is a powerful technology for analyzing and correlating large amounts of data coming from different application domains to automatically detect situations of interest (event patterns) in real time. However, extensive knowledge on CEP is required to be able to implement CEP applications. To alleviate this situation, in recent years, several works have proposed the use of Model-Driven Engineering (MDE) to facilitate the development of such CEP applications for domain experts. In this paper, we propose a systematic literature review of existing approaches, frameworks, systems and languages that integrate MDE with CEP, along with the application domains and maturity levels with which these proposals have been successfully adopted. Based on our findings, future research challenges in the CEP field are also discussed. © 2022, Journal of Object Technology. All Rights Reserved.
"
10.1007/978-3-031-10960-7_11,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141702779&origin=inward,Book Chapter,SCOPUS_ID:85141702779,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"creative ai, embodiment, and performance","
AbstractView references

In this chapter, we explore the relationship between creative AI, embodiment, and performance with reference to our artistic practice. The history of creative machines traces back to the automata of antiquity and featured at the dawn of computing. In the cognitive sciences, the study of human creativity has generally focused on creative thinking and the generation of novel and valuable ideas, rather than the role of embodiment in creative activity. The current renaissance of creative AI has seen remarkable advances in generative systems but has similarly shied away from the questions of embodiment. Our creative practice explores the possibility of creative AI in robotic systems and in turn, has highlighted the importance of embodiment in creative AI. As a result, we have shifted our focus from the development of computational systems as models of creative agents toward the realization of skillful performers able to facilitate the emergence of creative agency between humans and machines. The chapter outlines our inquiry through the lens of our arts-led research practice. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1109/CVPR52688.2022.01512,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141329844&origin=inward,Conference Paper,SCOPUS_ID:85141329844,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),visual abductive reasoning,
10.1109/RE54965.2022.00017,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141003015&origin=inward,Conference Paper,SCOPUS_ID:85141003015,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),mining user feedback for software engineering: use cases and reference architecture,"
AbstractView references

App reviews can provide valuable information about user needs but analyzing them manually is challenging due to their large quantity and noisy nature. To overcome this problem, a variety of app review mining techniques have been proposed. So far, however, research in this area has paid little attention to the software engineering use cases of the mining techniques. This limits the understanding of their usefulness, applications and desired future developments. We address this problem by elaborating a reference model relating app review mining techniques to specific software engineering activities. In this paper, we present a unified description of software engineering use cases for mining app reviews and define a reference architecture realizing these use cases through a combination of natural language processing and data mining techniques. The use cases provide a novel systematic exposition of the envisioned applications and benefits of app review mining for software engineers. The reference architecture synthesises the diversity of research to realise these benefits and provide a general framework guiding the development and evaluation of future research and tools. © 2022 IEEE.
"
10.1109/RE54965.2022.00012,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140961855&origin=inward,Conference Paper,SCOPUS_ID:85140961855,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automatic terminology extraction and ranking for feature modeling,"
AbstractView references

Requirements terminology defines and unifies key specialized and/or technical concepts of the software system, which is significant for understanding the application domain in requirements engineering (RE). However, manual terminology extraction from natural language requirements is laborious and expensive, especially with large scale requirements specifications. In this paper, we aim to employ natural language processing (NLP) techniques and machine learning (ML) algorithms to automatically extract and rank the requirements terms to support high-level feature modeling. To this end, we propose an automatic framework composed of noun phrase identification technique for requirements terms extraction and TextRank combined with semantic similarity for terms ranking. The final ranked terms are organized as a hierarchy, which can be used to help name elements when performing feature modeling. In the quantitative evaluation, our extraction method performs better than three baseline methods in recall with comparable precision. Moreover, our adapted TextRank algorithm can rank more relevant terms at the top positions in terms of average precision compared with most baselines. An illustrative example on the smart home domain further shows the usefulness of our framework in aiding elements naming during feature modeling. The research results suggest that proper adoption and adaption of NLP and ML techniques according to the characteristics of specific RE task could provide automation support for problem domain understanding. © 2022 IEEE.
"
10.1109/IJCNN55064.2022.9892426,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140790278&origin=inward,Conference Paper,SCOPUS_ID:85140790278,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),geometric analysis and metric learning of instruction embeddings,"
AbstractView references

Embeddings for instructions have been shown to be essential for software reverse engineering and automated program analysis. However, due to the complexity of dependencies and inherent variability of instructions, instruction embeddings using models that are successful for natural language processing may not be effective. In this paper, we perform geometric analysis of instruction embeddings at the token level and instruction family level, showing much greater variability and leading to degraded performance on intrinsic analyses. Then we propose to use metric learning to improve the relationships among instructions using triplet loss. Our results on a large dataset of instruction groups shows significant improvements. We also provide a theoretical analysis of the instruction embeddings by looking at the BERT components and characteristics of inner-product matrices for attention in the transformer blocks. The code will be available publicly after the paper is accepted for publication. © 2022 IEEE.
"
10.1109/ACCESS.2022.3215267,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140747029&origin=inward,Article,SCOPUS_ID:85140747029,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),malware detection by control-flow graph level representation learning with graph isomorphism network,"
AbstractView references

With society's increasing reliance on computer systems and network technology, the threat of malicious software grows more and more serious. In the field of information security, malware detection has been a key problem that academia and industry are committed to solving. Machine learning is an effective method for processing large-scale data, such as the Gradient Boosting Decision Tree (GBDT) and deep neural network technology. Although these types of detection methods can deal with cyber threats, most feature extraction methods are based on the statistical information features of portable executable (PE) files and thus lack the decompiled code and execution flow structure of the PE samples. Therefore, we propose a Control-Flow Graph (CFG)-and Graph Isomorphism Network (GIN)-based malware classification system. The feature vectors of CFG basic blocks are generated using the large-scale pre-trained language model MiniLM, which is beneficial for the GIN to further learn and compress the CFG-based representation, and classified with multi-layer perceptron. In addition, we evaluated the effectiveness of the representation under different dimensions and classifiers. To evaluate our method, we set up a CFG-based malware detection graph dataset from a PE file of the Blue Hexagon Open Dataset for Malware Analysis (BODMAS), which we call the Malware Geometric Binary Dataset (MGD-BINARY) and collected the experimental results of CFG representation in different dimensions and classifier settings. The evaluation results show that our proposal has proved an Accuracy metric of 0.99160 and achieved 0.99148 Area Under the Curve (AUC) results. © 2013 IEEE.
"
10.1016/j.csbj.2022.10.011,S2001037022004585,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140725533&origin=inward,Article,SCOPUS_ID:85140725533,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),treat: therapeutic rnas exploration inspired by artificial intelligence technology,"Recent advances in RNA engineering have enabled the development of RNA-based therapeutics for a broad spectrum of applications. Developing RNA therapeutics start with targeted RNA screening and move to the drug design and optimization. However, existing target screening tools ignore noncoding RNAs and their disease-relevant regulatory relationships. And designing therapeutic RNAs encounters high computational complexity of multi-objective optimization to overcome the immunogenicity, instability and inefficient translational production. To unlock the therapeutic potential of noncoding RNAs and enable one-stop screening and design of therapeutic RNAs, we have built the platform TREAT. It incorporates 43,087,953 regulatory relationships between coding and noncoding genes from 81 biological networks under different physiological conditions. TREAT introduces graph representation learning with Random Walk Diffusions to perform disease-relevant target screening, in addition to the commonly utilized Topological Degree and PageRank algorithms. Design and optimization of large RNAs or interfering RNAs are both available. To reduce the computational complexity of multi-objective optimization for large RNA, we stratified the features into local and global features. The local features are evaluated on the fixed-length or dynamic-length local bins, whereas the latter are inspired by AI language models of protein sequence. Then the global assessment is performed on refined candidates, thus reducing the enormous search space. Overall, TREAT is a one-stop platform for the screening and designing of therapeutic RNAs, with particular attention to noncoding RNAs and cutting-edge AI technology embedded, leading the progress of innovativetherapeutics for challenging diseases. TREAT is freely accessible at https://rna.org.cn/treat."
10.1007/978-3-031-16210-7_8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140451269&origin=inward,Conference Paper,SCOPUS_ID:85140451269,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),improving bert-based model for medical text classification with an optimization algorithm,"
AbstractView references

In the field of Natural Language Processing (NLP), automatic text classification is a classic topic that involves classifying textual material into predetermined categories based on its content. These models have been effectively applied to data containing a large number of dimensional features, some of which are inherently sparse. Machine learning and other statistical approaches, such as those used in medical text categorization, appear to be extremely successful for these tasks. However, much human work is still required to classify a large collection of training data. Recent research has shown the usefulness of pre-trained language models such as Bidirectional Encoder Representations from Transformers (BERT), all of which have demonstrated their ability to reduce the amount of work required for feature engineering. However, directly using the pre-trained BERT model in the classification task does not result in a statistically significant increase in performance. To improve the result of the BERT model, we propose an optimal deep learning model based on a BERT model and hyperparameter selection. The model consists of three steps: (1) processing medical text; (2) extracting medical text features using a BERT architecture; and (3) selecting hyperparameters for the Deep Learning model based on a Particle Swarm Optimization (PSO) algorithm. Finally, our approach uses a k-Nearest Neighbors algorithm (KNN) model to predict the matching response. Experiments conducted on the Hallmarks dataset have shown that the proposed method significantly increases the accuracy of the results. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-662-66146-8_3,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140429981&origin=inward,Conference Paper,SCOPUS_ID:85140429981,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),knowledge graph augmentation for increased question answering accuracy,"
AbstractView references

This research work presents a new augmentation model for knowledge graphs (KGs) that increases the accuracy of knowledge graph question answering (KGQA) systems. In the current situation, large KGs can represent millions of facts. However, the many nuances of human language mean that the answer to a given question cannot be found, or it is not possible to find always correct results. Frequently, this problem occurs because how the question is formulated does not fit with the information represented in the KG. Therefore, KGQA systems need to be improved to address this problem. We present a suite of augmentation techniques so that a wide variety of KGs can be automatically augmented, thus increasing the chances of finding the correct answer to a question. The first results from an extensive empirical study seem to be promising. © 2022, Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.1155/2022/7183207,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139886809&origin=inward,Article,SCOPUS_ID:85139886809,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a multimodel-based deep learning framework for short text multiclass classification with the imbalanced and extremely small data set,"
AbstractView references

Text classification plays an important role in many practical applications. In the real world, there are extremely small datasets. Most existing methods adopt pretrained neural network models to handle this kind of dataset. However, these methods are either difficult to deploy on mobile devices because of their large output size or cannot fully extract the deep semantic information between phrases and clauses. This paper proposes a multimodel-based deep learning framework for short-text multiclass classification with an imbalanced and extremely small dataset. Our framework mainly includes five layers: the encoder layer, the word-level LSTM network layer, the sentence-level LSTM network layer, the max-pooling layer, and the SoftMax layer. The encoder layer uses DistilBERT to obtain context-sensitive dynamic word vectors that are difficult to represent in traditional feature engineering methods. Since the transformer part of this layer is distilled, our framework is compressed. Then, we use the next two layers to extract deep semantic information. The output of the encoder layer is sent to a bidirectional LSTM network, and the feature matrix is extracted hierarchically through the LSTM at the word and sentence level to obtain the fine-grained semantic representation. After that, the max-pooling layer converts the feature matrix into a lower-dimensional matrix, preserving only the obvious features. Finally, the feature matrix is taken as the input of a fully connected SoftMax layer, which contains a function that can convert the predicted linear vector into the output value as the probability of the text in each classification. Extensive experiments on two public benchmarks demonstrate the effectiveness of our proposed approach on an extremely small dataset. It retains the state-of-the-art baseline performance in terms of precision, recall, accuracy, and F1 score, and through the model size, training time, and convergence epoch, we can conclude that our method can be deployed faster and lighter on mobile devices. © 2022 Jiajun Tong et al.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139388630&origin=inward,Conference Paper,SCOPUS_ID:85139388630,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),leveraging generative models to characterize the failure conditions of image classifiers,"
AbstractView references

We address in this work the question of identifying the failure conditions of a given image classifier. To do so, we exploit the capacity of producing controllable distributions of high quality image data made available by recent Generative Adversarial Networks (StyleGAN2): the failure conditions are expressed as directions of strong performance degradation in the generative model latent space. This strategy of analysis is used to discover corner cases that combine multiple sources of corruption, and to compare in more details the behavior of different classifiers. The directions of degradation can also be rendered visually by generating data for better interpretability. Some degradations such as image quality can affect all classes, whereas other ones such as shape are more class-specific. The approach is demonstrated on the MNIST dataset that has been completed by two sources of corruption: noise and blur, and shows a promising way to better understand and control the risks of exploiting Artificial Intelligence components for safety-critical applications. © 2022 Copyright for this paper by its authors.
"
10.1109/RE54965.2022.00011,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138923987&origin=inward,Conference Paper,SCOPUS_ID:85138923987,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated question answering for improved understanding of compliance requirements: a multi-document study,"
AbstractView references

Software systems are increasingly subject to regulatory compliance. Extracting compliance requirements from regulations is challenging. Ideally, locating compliance-related information in a regulation requires a joint effort from requirements engineers and legal experts, whose availability is limited. However, regulations are typically long documents spanning hundreds of pages, containing legal jargon, applying complicated natural language structures, and including cross-references, thus making their analysis effort-intensive. In this paper, we propose an automated question-answering (QA) approach that assists requirements engineers in finding the legal text passages relevant to compliance requirements. Our approach utilizes large-scale language models fine-tuned for QA, including BERT and three variants. We evaluate our approach on 107 question-answer pairs, manually curated by subject-matter experts, for four different European regulatory documents. Among these documents is the general data protection regulation (GDPR) - a major source for privacy-related requirements. Our empirical results show that, in $\approx 94$% of the cases, our approach finds the text passage containing the answer to a given question among the top five passages that our approach marks as most relevant. Further, our approach successfully demarcates, in the selected passage, the right answer with an average accuracy of $\approx$91%. © 2022 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138436811&origin=inward,Conference Paper,SCOPUS_ID:85138436811,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),response construct tagging: nlp-aided assessment for engineering education,"
AbstractView references

Recent advances in natural language processing (NLP) have greatly helped educational applications, for both teachers and students. In higher education, there is great potential to use NLP tools for advancing pedagogical research. In this paper, we focus on how NLP can help understand student experiences in engineering, thus facilitating engineering educators to carry out large scale analysis that is helpful for redesigning the curriculum. Here, we introduce a new task we call response construct tagging (RCT), in which student responses to tailored survey questions are automatically tagged for six constructs measuring transformative experiences and engineering identity of students. We experiment with state-of-the-art classification models for this task and investigate the effects of different sources of additional information. Our best model achieves an F1 score of 48. We further investigate multi-task training on the related task of sentiment classification, which improves our model’s performance to 55 F1. Finally, we provide a detailed qualitative analysis of model performance. © 2022 Association for Computational Linguistics.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138430205&origin=inward,Conference Paper,SCOPUS_ID:85138430205,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automatic construction of technology function matrix,"
AbstractView references

The Technology Function Matrix (TFM) is a typical patent analysis method that is widely used to detect high-value technology and to locate technical gaps in a specific field. Early TFM construction methods were either based on manual work or machine learning (ML) models. However ML-based models often require large-scale annotated datasets, which are labor-intensive and time-consuming. Therefore, there is a great practical need for low-cost and efficient construction of the TFM. In this paper, we propose a framework for automatically constructing a TFM that requires only a small amount of labeled data. First, we adopt a semi-supervised strategy that comprehensively uses the semantic dependency parser and the pre-trained language model to extract function and technology phrases. Second, a large-scale dictionary of upper and lower categories and synonyms is adopted to merge the related function and technology phrases. Finally, we build an interactive system to visualize the TFM construction process. Compared with traditional methods, our method can significantly improve the performance of technology and function phrase extraction. Furthermore, our system can help experts correct TFM construction results and analyze the current state of technology development in a certain field. © 2022 Copyright for this paper by its authors.
"
10.1080/17477778.2022.2122741,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138409542&origin=inward,Article,SCOPUS_ID:85138409542,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards an agent-based model using a hybrid conceptual modelling approach: a case study of relationship conflict within large enterprise system implementations,"
AbstractView references

An often-overlooked activity within the design and development of computational models for socio-technical systems, is the development of a comprehensive conceptual model defining the scope of the model with respect to actors, technical resources, environment and abstraction level. With specific reference to modelling large IT and IS implementations, this incurs the added challenges of dealing with qualitative information relating to project scope and implementation processes, along with quantitative and qualitative information regarding the social network of the project resources and emergent behaviours that result from interactions between them. Our approach involves a Multi-Paradigm Hybrid Study to develop a Cross-Disciplinary Hybrid Model of relationship conflict within an enterprise system implementation. We identify that Soft Systems Methodology, Social Network Analysis, Unified Modelling Language are complementary approaches from Operational Research, Social Sciences and Software Engineering, that provide a powerful combination of techniques to develop hybrid conceptual models of complex socio-technical systems. © 2022 The Operational Research Society.
"
10.1109/EMBC48229.2022.9871708,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138128219&origin=inward,Conference Paper,SCOPUS_ID:85138128219,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a streamable large-scale clinical eeg dataset for deep learning,"
AbstractView references

Deep Learning has revolutionized various fields, including Computer Vision, Natural Language Processing, as well as Biomedical research. Within the field of neuroscience, specifically in electrophysiological neuroimaging, researchers are starting to explore leveraging deep learning to make predictions on their data without extensive feature engineering. The availability of large-scale datasets is a crucial aspect of allowing the experimentation of Deep Learning models. We are publishing the first large-scale clinical EEG dataset that simplifies data access and management for Deep Learning. This dataset contains eyes-closed EEG data prepared from a collection of 1,574 juvenile participants from the Healthy Brain Network. We demonstrate a use case integrating this framework, and discuss why providing such neuroinformatics infrastructure to the community is critical for future scientific discoveries. © 2022 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137742759&origin=inward,Conference Paper,SCOPUS_ID:85137742759,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),utilizing cross-modal contrastive learning to improve item categorization bert model,"
AbstractView references

Item categorization (IC) is a core natural language processing (NLP) task in e-commerce. As a special text classification task, fine-tuning pre-trained models, e.g., BERT, has become a main stream solution. To improve IC performance further, other product metadata, e.g., product images, have been used. Although multimodal IC (MIC) systems show higher performance, expanding from processing text to more resource-demanding images brings large engineering impacts and hinders the deployment of such dual-input MIC systems. In this paper, we proposed a new way of using product images to improve text-only IC model: leveraging cross-modal signals between products’ titles and associated images to adapt BERT models in a self-supervised learning (SSL) way. Our experiments on the three genres in the public Amazon product dataset show that the proposed method generates improved prediction accuracy and macro-F1 values than simply using the original BERT. Moreover, the proposed method is able to keep using existing text-only IC inference implementation and shows a resource advantage than the deployment of a dual-input MIC system. © 2022 Association for Computational Linguistics.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137535762&origin=inward,Conference Paper,SCOPUS_ID:85137535762,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),what gpt knows about who is who,"
AbstractView references

Coreference resolution – which is a crucial task for understanding discourse and language at large – has yet to witness widespread benefits from large language models (LLMs). Moreover, coreference resolution systems largely rely on supervised labels, which are highly expensive and difficult to annotate, thus making it ripe for prompt engineering. In this paper, we introduce a QA-based prompt-engineering method and discern generative, pre-trained LLMs’ abilities and limitations toward the task of coreference resolution. Our experiments show that GPT-2 and GPT-Neo can return valid answers, but that their capabilities to identify coreferent mentions are limited and prompt-sensitive, leading to inconsistent results. © 2022 Association for Computational Linguistics.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137374292&origin=inward,Conference Paper,SCOPUS_ID:85137374292,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),masked measurement prediction: learning to jointly predict quantities and units from textual context,"
AbstractView references

Physical measurements constitute a large portion of numbers in academic papers, engineering reports, and web tables. Current benchmarks fall short of properly evaluating numeracy of pretrained language models on measurements, hindering research on developing new methods and applying them to numerical tasks. To that end, we introduce a novel task, Masked Measurement Prediction (MMP), where a model learns to reconstruct a number together with its associated unit given masked text. MMP is useful for both training new numerically informed models as well as evaluating numeracy of existing systems. To address this task, we introduce a new Generative Masked Measurement (GeMM) model that jointly learns to predict numbers along with their units. We perform fine-grained analyses comparing our model with various ablations and baselines. We use linear probing of traditional pretrained transformer models (RoBERTa) to show that they significantly underperform jointly trained number-unit models, highlighting the difficulty of this new task and the benefits of our proposed pre-training approach. We hope this framework accelerates progress towards building more robust numerical reasoning systems in the future. © Findings of the Association for Computational Linguistics: NAACL 2022 - Findings.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137355293&origin=inward,Conference Paper,SCOPUS_ID:85137355293,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on measuring social biases in prompt-based multi-task learning,"
AbstractView references

Large language models trained on a mixture of NLP tasks that are converted into a textto- text format using prompts, can generalize into novel forms of language and handle novel tasks. A large body of work within prompt engineering attempts to understand the effects of input forms and prompts in achieving superior performance. We consider an alternative measure and inquire whether the way in which an input is encoded affects social biases promoted in outputs. In this paper, we study T0, a large-scale multi-task text-to-text language model trained using prompt-based learning. We consider two different forms of semantically equivalent inputs: question-answer format and premise-hypothesis format. We use an existing bias benchmark for the former BBQ (Parrish et al., 2021) and create the first bias benchmark in natural language inference BBNLI with hand-written hypotheses while also converting each benchmark into the other form. The results on two benchmarks suggest that given two different formulations of essentially the same input, T0 conspicuously acts more biased in question answering form, which is seen during training, compared to premisehypothesis form which is unlike its training examples. Code and data are released under https://github.com/feyzaakyurek/bbnli. © Findings of the Association for Computational Linguistics: NAACL 2022 - Findings.
"
10.1109/CIEEC54735.2022.9846460,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137316443&origin=inward,Conference Paper,SCOPUS_ID:85137316443,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),power grid engineering data knowledge retrieval and graph construction technology,"
AbstractView references

The power grid infrastructure project has the characteristics of large scale, long period, and multiple subjects. Therefore, it is determined that a large amount of data with rich sources and complex formats will be generated in the whole process of power grid engineering from design, construction to acceptance. However, due to the lack of effective data extraction and sorting technical means, these data results have a single search method and related queries are difficult, and they cannot be displayed intuitively. This article takes the noise-containing multi-source heterogeneous infrastructure engineering data as the research object, and relies on natural language processing technology to carry out research on the information extraction model and method and the knowledge map construction technology. This method automatically analyzes the natural semantics contained in the text data of the power grid infrastructure project, excavates the valuable information contained therein, and then constructs a data knowledge graph to realize hierarchical storage, visual display and related information recommendation. © 2022 IEEE.
"
10.18293/SEKE2022-176,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137153155&origin=inward,Conference Paper,SCOPUS_ID:85137153155,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a node-merging based approach for generating istar models from user stories,"
AbstractView references

User story is a widely adopted requirement notation in agile development. Generally, user stories are written by customers or users in natural language with limited form to describe user's needs for the software system from their perspectives. However, since user stories are generally presented in a flat list, the relations derived from the user stories are difficult to capture. It reduces the understanding of the system as a whole. One solution to this problem is to build goal-oriented models that provide explicit relations among user stories. But extracting concepts and relationships from a large number of discrete user stories often take a lot of time for the agile development team. This paper proposes an iStar model generating approach based on node-merging from user stories. The method first extracts the iStar nodes from the semi-structured user stories, then uses a BERT (Bidirectional Encoder Representations from Transformers) model to measure the similarity between the nodes, and then nodes to be merged are identified and the edges between the iStar nodes are connected. Experiments are designed to illustrate the effectiveness of the proposed approach. © 2022 Knowledge Systems Institute Graduate School. All rights reserved.
"
10.1016/j.ifacol.2022.07.568,S2405896322009740,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137055433&origin=inward,Conference Paper,SCOPUS_ID:85137055433,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),processbert: a pre-trained language model for judging equivalence of variable definitions in process models*,"
                  Digital twins are expected to play a pivotal role in digital transformation. Although process informatics has attracted much attention, physical models are essential to realizing the digital twins. However, building a physical model of an industrial process takes much toil. We aim to facilitate the physical model building by developing an automated physical model building AI, named AutoPMoB, which performs five tasks: 1) retrieving documents about a target process from literature databases, 2) converting the format of each document to HTML format, 3) extracting information required for building a physical model from the documents, such as variables, equations, and experimental data, 4) judging the equivalence of the information extracted from different documents, and 5) reorganizing the information to output a desired physical model. This study focuses on task 4, especially judging the equivalence of variable definitions, i.e., whether two noun phrases represent the same variable. We created a large-scale corpus consisting of papers on chemical engineering, and built ProcessBERT, which is a domain-specific language model pre-trained on the corpus. We proposed a method for judging the equivalence of variable definitions based on ProcessBERT. When judging the equivalence, our proposed method first uses ProcessBERT to obtain the embeddings of the variable definitions. Then, the method calculates the cosine similarity between the embeddings. The method judges that the two definitions are equivalent when the similarity is larger than a threshold. Our proposed method judged the equivalence with higher accuracy than the method based on original BERT and SciBERT.
               "
10.1109/COMPSAC54236.2022.00237,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136954326&origin=inward,Conference Paper,SCOPUS_ID:85136954326,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),malware detection using attributed cfg generated by pre-trained language model with graph isomorphism network,"
AbstractView references

Traditional malware detection methods cannot keep up with the massive amount of newly created malware quickly and effectively. Machine learning is a promising method for the detection and classification of large-scale newly created malware according to the features of samples. The current research trend is to use machine learning technology, such as the Gradient Boosting Decision Tree (GBDT) and deep neural network technology, to learn newly created malware rapidly and accurately. We propose Control-Flow Graph (CFG)- and Graph Isomorphism Network (GIN)-based malware classification, where we first extract the CFG from portable executable (PE) files and use the large-scale pre-training language model MiniLM to generate the node features of CFG. The extracted CFG is compressed to a feature vector with GIN and classified with Multi-Layer Perceptron. To evaluate our approach, we made a CFG-based malware detection dataset from PE files of the Dike Dataset, which we call the Malware Geometric Dataset (MGD), and collected the results. The evaluation results show that our proposal demonstrated 0.9977 in the Area Under Curve metric and achieved a 97.44 % detection rate when the False Positive Rate was 0.1 %. © 2022 IEEE.
"
10.18178/wcse.2022.06.041,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136826570&origin=inward,Conference Paper,SCOPUS_ID:85136826570,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),alzheimer's disease detection through spontaneous speech using attention augmented convolutional neural network,"
AbstractView references

Alzheimer's disease (AD) is a neurodegenerative disease which affects patients' thinking, mood, and memory. Once diagnosed, it cannot be cured or reversed. Mild cognitive impairment (MCI) is the early stage of Alzheimer's disease, and medication at this stage can be used to slow down or even stop its development. A large number of studies have shown that AD can cause language barriers. There are significant symptoms in language, which can be used for early detection of AD. In this paper, convolutional neural network (CNN) is applied to the early diagnosis of AD. In addition, we introduced Convolutional Block Attention Module (CBAM) and incorporate CBAM into the CNN architecture to enhance the performance of the model. Experimental results show that the proposed model in this paper achieves 84.87% and 83.00% classification accuracy in long speech tracks and short speech tracks of the Alzheimer's Disease Recognition Competition, improved 5.07% and 9.00% compared to the baseline system. © 2022 WCSE. All Rights Reserved.
"
10.1016/j.caeai.2022.100081,S2666920X22000364,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136131222&origin=inward,Article,SCOPUS_ID:85136131222,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),machine learning based feedback on textual student answers in large courses,"Many engineering disciplines require problem-solving skills, which cannot be learned by memorization alone. Open-ended textual exercises allow students to acquire these skills. Students can learn from their mistakes when instructors provide individual feedback. However, grading these exercises is often a manual, repetitive, and time-consuming activity. The number of computer science students graduating per year has steadily increased over the last decade. This rise has led to large courses that cause a heavy workload for instructors, especially if they provide individual feedback to students. This article presents CoFee, a framework to generate and suggest computer-aided feedback for textual exercises based on machine learning. CoFee utilizes a segment-based grading concept, which links feedback to text segments. CoFee automates grading based on topic modeling and an assessment knowledge repository acquired during previous assessments. A language model builds an intermediate representation of the text segments. Hierarchical clustering identifies groups of similar text segments to reduce the grading overhead. We first demonstrated the CoFee framework in a small laboratory experiment in 2019, which showed that the grading overhead could be reduced by 85%. This experiment confirmed the feasibility of automating the grading process for problem-solving exercises. We then evaluated CoFee in a large course at the Technical University of Munich from 2019 to 2021, with up to 2, 200 enrolled students per course. We collected data from 34 exercises offered in each of these courses. On average, CoFee suggested feedback for 45% of the submissions. 92% (Positive Predictive Value) of these suggestions were precise and, therefore, accepted by the instructors."
10.1007/978-3-031-10542-5_6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135962110&origin=inward,Conference Paper,SCOPUS_ID:85135962110,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"fine-tuning gpt-2 to patch programs, is it worth it?","
AbstractView references

The application of Artificial Intelligence (AI) in the Software Engineering (SE) field is always a bit delayed compared to state-of-the-art research results. While the Generative Pre-trained Transformer (GPT-2) model was published in 2018, only a few recent works used it for SE tasks. One of such tasks is Automated Program Repair (APR), where the applied technique should find a fix to software bugs without human intervention. One problem emerges here: the creation of proper training data is resource-intensive and requires several hours of additional work from researchers. The sole reason for it is that training a model to repair programs automatically requires both the buggy program and the fixed one on large scale and presumably in an already pre-processed form. There are currently few such databases, so teaching and fine-tuning models is not an easy task. In this work, we wanted to investigate how the GPT-2 model performs when it is not fine-tuned for the APR task, compared to when it is fine-tuned. From previous work, we already know that the GPT-2 model can automatically generate patches for buggy programs, although the literature lacks studies where no fine-tuning has taken place. For the sake of the experiment we evaluated the GPT-2 model out-of-the-box and also fine-tuned it before the evaluation on 1559 JavaSript code snippets. Based on our results we can conclude that although the fine-tuned model was able to learn how to write syntactically correct source code almost on every attempt, the non-fine-tuned model lacked some of these positive features. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-981-19-2456-9_60,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135012506&origin=inward,Conference Paper,SCOPUS_ID:85135012506,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automatic scoring model of subjective questions based text similarity fusion model,"
AbstractView references

AI In this era, scene based translation and intelligent word segmentation are not new technologies. However, there is still no good solution for long and complex Chinese semantic analysis. The subjective question scoring still relies on the teacher's manual marking. However, there are a large number of examinations, and the manual marking work is huge. At present, the labor cost is getting higher and higher, the traditional manual marking method can't meet the demand The demand for automatic marking is increasingly strong in modern society. At present, the automatic marking technology of objective questions has been very mature and widely used. However, by reasons of the complexity and the difficulty of natural language processing technology in Chinese text, there are still many shortcomings in subjective questions marking, such as not considering the impact of semantics, word order and other issues on scoring accuracy. The automatic scoring technology of subjective questions is a complex technology, involving pattern recognition, machine learning, natural language processing and other technologies. Good results have been seen in the calculation method-based deep learning and machine learning. The rapid development of NLP technology has brought a new breakthrough for subjective question scoring. We integrate two deep learning models based on the Siamese Network through bagging to ensure the accuracy of the results, the text similarity matching model based on the birth networks and the score point recognition model based on the named entity recognition method respectively. Combining with the framework of deep learning, we use the simulated manual scoring method to extract and match the score point sequence of students’ answers with standard answers. The score recognition model effectively improves the efficiency of model calculation and long text keyword matching. The loss value of the final training score recognition model is about 0.9, and the accuracy is 80.54%. The accuracy of the training text similarity matching model is 86.99%, and the fusion model is single. The scoring time is less than 0.8s, and the accuracy is 83.43%. © 2022, The Author(s).
"
10.1155/2022/2424380,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134895733&origin=inward,Article,SCOPUS_ID:85134895733,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring the teaching mode of english audiovisual speaking in multimedia network environment,"
AbstractView references

Introducing multimedia network tools in English audiovisual teaching and building a new model of network-based multimedia teaching can make English audiovisual teaching more in line with students' cognitive thinking characteristics and processes. This can improve the overall efficiency of English teaching in schools. Computers have been widely used in language evaluation and speech recognition for language learning, and speech recognition technology is an important reflection of the level of language learning. The large amount of language signal data, complex pronunciation changes, and high dimensionality of pronunciation feature parameters in the language learning process make it difficult to identify pronunciation features. The computational volume of pronunciation evaluation and recognition is too large, which requires high hardware resources and software resources to realize high-speed processing of massive pronunciation signals. To address the problem of low recognition rate of English pronunciation, this study proposes a sound recognition algorithm based on adaptive particle swarm optimization (PSO) matching pursuit (MP) sparse decomposition. The algorithm firstly improves the parameter adaptive setting of PSO based on the particle and population evolution rate, establishes parameter adaptive PSO, and realizes the optimization of adaptive PSO optimized MP sparse decomposition. The continuous Gabor super-complete atomic set is constructed based on the continuous space search property of PSO to improve the optimal atomic matching of the evolutionary process. Finally, the recognition of English pronunciation is realized by the support vector machine (SVM) algorithm. The test results show that the misjudgement rate for different mispronunciations is less than 1% when the system is used to evaluate the English pronunciation level. It proves that the method can effectively detect the mispronunciation and has high evaluation accuracy. © 2022 Shunlan Wang.
"
10.1155/2022/4149492,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134434553&origin=inward,Article,SCOPUS_ID:85134434553,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),open university chinese language and literature teaching model based on nlp technology and mobile edge computing,"
AbstractView references

With the further improvement of comprehensive national strength, both cultural soft power and international status have increased significantly. The Chinese language has also become one of the languages eagerly studied in all countries in the world. However, the teaching resources of Chinese as a foreign language are far from enough to meet the rapid growth of learning needs. With the continuous improvement of the network environment of high-performance, low-latency, and high-bandwidth mobile edge computing, using computer network technology to assist Chinese language learning is an effective way to meet the needs. Under this circumstance, this paper proposed to design and build a Chinese language learning system based on NLP (natural language processing) technology. The built system modules could be roughly divided into three parts: the basic module of the system, the learning module, and the tool module combined with NLP technology. And, through the rational use of these three modules, it provided learners with the basic teaching of the Chinese language and also collected a large number of relevant documents on the Chinese language and culture through the system, which was more convenient for learners to learn. Research showed that the learning system basically met the learning needs. According to the data survey on the use of international students, more than 90% of the international students who have used the system believed that the content expressed by the learning system was clear and scientific. The results showed that the design of the Chinese learning system met the learning needs of learners. © 2022 Yan Liu.
"
10.23919/MIPRO55190.2022.9803560,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133938271&origin=inward,Conference Paper,SCOPUS_ID:85133938271,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),tool-supported teaching of uml diagrams in software engineering education - a systematic literature review,"
AbstractView references

There is hardly a university that does not offer a course in software engineering for computer scientists. Due to the expanding complexity of software systems and rapidly changing requirements, it has become increasingly difficult to teach students all the content they need for their professional careers in the industry or academia. Additionally, teaching modelling with modelling languages like UML is a sophisticated task for educators. Student-generated solutions may be visually different from a sample solution and still be correct. Regarding large software engineering courses, individual feedback for students is usually not possible or comes with a time delay. However, it would contribute to their learning success. Therefore, a rising number of software tools can be found to support this area of teaching. In this paper, a systematic literature review is presented. It follows the methodology of Kitchenham and provides an overview about the tools, that are used to support teaching of modelling in higher software engineering education. Alongside the functionalities and their differences, this literature review summarizes the difficulties, that motivated educators to develop these tools. © 2022 Croatian Society MIPRO.
"
10.1109/ICSE-SEIS55304.2022.9793869,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133698698&origin=inward,Conference Paper,SCOPUS_ID:85133698698,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),lowering barriers to application development with cloud-native domain-specific functions,"
AbstractView references

Creating and maintaining a modern, heterogeneous set of client applications remains an obstacle for many businesses and individu-als. While simple domain-specific graphical languages and libraries can empower a variety of users to create application behaviors and logic, using these languages to produce and maintain a set of heterogeneous client applications is a challenge. Primarily because each client typically requires the developers to both understand and embed the domain-specific logic. This is because application logic must be encoded to some extent in both the server and client sides. In this paper, we propose an alternative approach, which allows the specification of application logic to reside solely on the cloud. We have built a system where reusable application components can be assembled on the cloud in different logical chains and the client is largely decoupled from this logic and is solely concerned with how data is displayed and gathered from users of the application. In this way, the chaining of requests and responses is done by the cloud and the client side has no knowledge of the application logic. This means that the experts in the domain can build these modular cloud components, arrange them in various logical chains, generate a simple user interface to test the application, and later leave it to client-side developers to customize the presentation and gathering of data types to and from the user. An additional effect of our approach is that the client side developer is able to immediately see any changes they make, while executing the logic residing on the cloud. This further allows more novice programmers to perform these customizations, as they do not need to 'get the full application working' and are able to see the results of their code as they go, thereby lowering the obstacles to businesses and individuals to produce and maintain applications. Furthermore, this decoupling enables the quick generation and customization of a variety of application clients, ranging from web to mobile devices and personal assistants, while customizing one or more as needed. Creating new computer applications, distributing them to users, and maintaining them, is complex and time-consuming. Building even a simple application requires years of study. Furthermore, given the number of devices we have today (from smartphones to voice assistants), the task is further complicated and difficult even for experienced programmers. The world is undergoing a digital transformation, and computer applications are now part of our everyday existence and building them is a requirement for many organizations and businesses. How-ever, only companies and individuals with large budgets and skills can build these applications, a consequence of which is an increased digital gap between those with means and those without. We see a clear need to lower the barriers to building these applications so a more diverse set of voices are able to participate and shape our digital futures, as underrepresented groups are often left out of designing and building these platforms. Previous researchers have proposed approaches where applications can be easily specified by people without (or with few) technical skills. However, handling different device platforms and maintenance/distribution were neglected in these efforts. In this paper we propose an approach where the program is executed on the cloud, while the client only needs to display and gather data from users. A result of our approach is that generic front-end applications adapt to any changes to the program being executed on the cloud. Our programs work with end-user apps on any device, the application that the user sees automatically adapts to any changes. © 2022 IEEE.
"
10.1016/j.procir.2022.05.249,S2212827122006989,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133539488&origin=inward,Conference Paper,SCOPUS_ID:85133539488,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai based geometric similarity search supporting component reuse in engineering design,"Today, companies are faced with the challenge to develop and produce individualized products in the shortest possible time at very low cost in order to remain attractive under strong competitive pressure. For reasons of efficiency, products are therefore often developed in generations. Proven components are adopted in a new product generation and only some of the components are newly developed to meet new customer requirements. Many companies, therefore, have a large database of 3D CAD product models containing years of engineering experience. Nevertheless, it is often difficult to execute database queries to find which products or components already exist and could be reused or adapted for a new product generation or variant. As a result, many duplicates are created, which are associated with high effort and costs, and the risk of introducing design errors increases. Therefore, the aim of this paper is to develop an automated approach for geometric similarity search that also takes company-specific features of components into account. Machine learning methods are capable of automatically extracting relevant geometric features by learning a suitable representation of the corresponding 3D object. For this purpose, an autoencoder is developed which is trained to extract class-specific feature vectors. To improve the representativeness of those vectors for the similarity search, the architecture and hyperparameters of the autoencoder are optimized based on several experiments. Considering a real use case with a data set from the field of mechanical engineering, it is shown that geometrically similar CAD models can be found very quickly using the learned representation, and that better results are obtained than with conventional methods based on meta information, e.g. volume and bounding box. On the one hand, the fast finding of similar models encourages the reuse of existing solutions. On the other hand, standardization and, thus, economy of scale is promoted."
10.1145/3522664.3528592,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133467455&origin=inward,Conference Paper,SCOPUS_ID:85133467455,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),quality assurance of generative dialog models in an evolving conversational agent used for swedish language practice,"
AbstractView references

Due to the migration megatrend, efficient and effective second-language acquisition is vital. One proposed solution involves AI-enabled conversational agents for person-centered interactive language practice. We present results from ongoing action research targeting quality assurance of proprietary generative dialog models trained for virtual job interviews. The action team elicited a set of 38 requirements for which we designed corresponding automated test cases for 15 of particular interest to the evolving solution. Our results show that six of the test case designs can detect meaningful differences between candidate models. While quality assurance of natural language processing applications is complex, we provide initial steps toward an automated framework for machine learning model selection in the context of an evolving conversational agent. Future work will focus on model selection in an MLOps setting. © 2022 ACM.
"
10.1145/3510003.3510157,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133365193&origin=inward,Conference Paper,SCOPUS_ID:85133365193,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated handling of anaphoric ambiguity in requirements: a multi-solution study,"
AbstractView references

Ambiguity is a pervasive issue in natural-language requirements. A common source of ambiguity in requirements is when a pronoun is anaphoric. In requirements engineering, anaphoric ambiguity occurs when a pronoun can plausibly refer to different entities and thus be interpreted differently by different readers. In this paper, we develop an accurate and practical automated approach for handling anaphoric ambiguity in requirements, addressing both ambiguity detection and anaphora interpretation. In view of the multiple competing natural language processing (NLP) and machine learning (ML) technologies that one can utilize, we simultaneously pursue six alternative solutions, empirically assessing each using a col-lection of ˜1,350 industrial requirements. The alternative solution strategies that we consider are natural choices induced by the existing technologies; these choices frequently arise in other automation tasks involving natural-language requirements. A side-by-side em-pirical examination of these choices helps develop insights about the usefulness of different state-of-the-art NLP and ML technologies for addressing requirements engineering problems. For the ambigu-ity detection task, we observe that supervised ML outperforms both a large-scale language model, SpanBERT (a variant of BERT), as well as a solution assembled from off-the-shelf NLP coreference re-solvers. In contrast, for anaphora interpretation, SpanBERT yields the most accurate solution. In our evaluation, (1) the best solution for anaphoric ambiguity detection has an average precision of ˜60% and a recall of 100%, and (2) the best solution for anaphora interpretation (resolution) has an average success rate of ˜98%. © 2022 ACM.
"
10.1145/nnnnnnn.nnnnnnn,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133199310&origin=inward,Conference Paper,SCOPUS_ID:85133199310,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),semantic similarity metrics for evaluating source code summarization,"
AbstractView references

Source code summarization involves creating brief descriptions of source code in natural language. These descriptions are a key component of software documentation such as JavaDocs. Automatic code summarization is a prized target of software engineering research, due to the high value summaries have to programmers and the simultaneously high cost of writing and maintaining documentation by hand. Current work is almost all based on machine models trained via big data input. Large datasets of examples of code and summaries of that code are used to train an e.g. encoder-decoder neural model. Then the output predictions of the model are evaluated against a set of reference summaries. The input is code not seen by the model, and the prediction is compared to a reference. The means by which a prediction is compared to a reference is essentially word overlap, calculated via a metric such as BLEU or ROUGE. The problem with using word overlap is that not all words in a sentence have the same importance, and many words have synonyms. The result is that calculated similarity may not match the perceived similarity by human readers. In this paper, we conduct an experiment to measure the degree to which various word overlap metrics correlate to human-rated similarity of predicted and reference summaries. We evaluate alternatives based on current work in semantic similarity metrics and propose recommendations for evaluation of source code summarization. © 2022 ACM.
"
10.1145/3524610.3527892,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133166781&origin=inward,Conference Paper,SCOPUS_ID:85133166781,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on the cross-modal transfer from natural language to code through adapter modules,"
AbstractView references

Pre-trained neural Language Models (PTLM), such as CodeBERT, are recently used in software engineering as models pre-trained on large source code corpora. Their knowledge is transferred to downstream tasks (e.g. code clone detection) via fine-tuning. In natural language processing (NLP), other alternatives for transferring the knowledge of PTLMs are explored through using adapters, compact, parameter efficient modules inserted in the layers of the PTLM. Although adapters are known to facilitate adapting to many downstream tasks compared to fine-tuning the model that require retraining all of the models' parameters- which owes to the adapters' plug and play nature and being parameter efficient-their usage in software engineering is not explored. Here, we explore the knowledge transfer using adapters and based on the Naturalness Hypothesis proposed by Hindle et. al [12]. Thus, studying the bimodality of adapters for two tasks of cloze test and code clone detection, compared to their benchmarks from the CodeXGLUE platform. These adapters are trained using programming languages and are inserted in a PTLM that is pre-trained on English corpora (N-PTLM). Three programming languages, C/C++, Python, and Java, are studied along with extensive experiments on the best setup used for adapters. Improving the results of the N-PTLM confirms the success of the adapters in knowledge transfer to software engineering, which sometimes are in par with or exceed the results of a PTLM trained on source code; while being more efficient in terms of the number of parameters, memory usage, and inference time. Our results can open new directions to build smaller models for more software engineering tasks. We open source all the scripts and the trained adapters. © 2022 ACM.
"
10.1145/3524610.3527897,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133161254&origin=inward,Conference Paper,SCOPUS_ID:85133161254,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ptm4tag: sharpening tag recommendation of stack overflow posts with pre-trained models,"
AbstractView references

Stack Overflow is often viewed as one of the most influential Software Question & Answer (SQA) websites, containing millions of programming-related questions and answers. Tags play a critical role in efficiently structuring the contents in Stack Overflow and are vital to support a range of site operations, e.g., querying relevant contents. Poorly selected tags often introduce extra noise and redundancy, which raises problems like tag synonym and tag explosion. Thus, an automated tag recommendation technique that can accurately recommend high-quality tags is desired to alleviate the problems mentioned above. Inspired by the recent success of pre-trained language models (PTMs) in natural language processing (NLP), we present PTM4Tag, a tag recommendation framework for Stack Overflow posts that utilize PTMs with a triplet architecture, which models the components of a post, i.e., Title, Description, and Code with independent language models. To the best of our knowledge, this is the first work that leverages PTMs in the tag recommendation task of SQA sites. We comparatively evaluate the performance of PTM4Tag based on five popular pre-trained models: BERT, RoBERTa, ALBERT, CodeBERT, and BERTOverflow. Our results show that leveraging CodeBERT, a software engineering (SE) domain-specific PTM in PTM4Tag achieves the best performance among the five considered PTMs and outperforms the state-of-the-art Convolutional Neural Network-based approach by a large margin in terms of average Precision@k, Recall@k, and F1-score@k. We conduct an ablation study to quantify the contribution of a post's constituent components (Title, Description, and Code Snippets) to the performance of PTM4Tag. Our results show that Title is the most important in predicting the most relevant tags, and utilizing all the components achieves the best performance. © 2022 ACM.
"
10.1117/12.2623360,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132994782&origin=inward,Conference Paper,SCOPUS_ID:85132994782,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generation of mbse models from system requirements,"
AbstractView references

Cancellations of DoD acquisition programs have resulted in billions of dollars of losses annually, which reduces resources for new capabilities. One area identified is a lack of proper requirement scoping, which is prohibitively complex when tracing architectures for large systems. MBSE was developed to address these issues and provide a common framework to rapidly represent, convey, and synchronize information. This research explores the maturity of NLP and ML methodology needed to automatically generate and trace requirements in MBSE models, thus providing the tools to rapidly generate models from requirement documents and reducing the risk of program cancellation due to requirement scoping problems. © 2022 SPIE
"
10.1108/IJPCC-02-2022-0047,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132817415&origin=inward,Article,SCOPUS_ID:85132817415,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an optimized and efficient multiuser data sharing using the selection scheme design secure approach and federated learning in cloud environment,"
AbstractView references

Purpose: Until now, a lot of research has been done and applied to provide security and original data from one user to another, such as third-party auditing and several schemes for securing the data, such as the generation of the key with the help of encryption algorithms like Rivest–Shamir–Adleman and others. Here are some of the related works that have been done previously. Remote damage control resuscitation (RDCR) scheme by Yan et al. (2017) is proposed based on the minimum bandwidth. By enabling the third party to perform the verification of public integrity. Although it supports the repair management for the corrupt data and tries to recover the original data, in practicality it fails to do so, and thus it takes more computation and communication cost than our proposed system. In a paper by Chen et al. (2015), using broadcast encryption, an idea for cloud storage data sharing has been developed. This technique aims to accomplish both broadcast data and dynamic sharing, allowing users to join and leave a group without affecting the electronic press kit (EPK). In this case, the theoretical notion was true and new, but the system’s practicality and efficiency were not acceptable, and the system’s security was also jeopardised because it proposed adding a member without altering any keys. In this research, an identity-based encryption strategy for data sharing was investigated, as well as key management and metadata techniques to improve model security (Jiang and Guo, 2017). The forward and reverse ciphertext security is supplied here. However, it is more difficult to put into practice, and one of its limitations is that it can only be used for very large amounts of cloud storage. Here, it extends support for dynamic data modification by batch auditing. The important feature of the secure and efficient privacy preserving provable data possession in cloud storage scheme was to support every important feature which includes data dynamics, privacy preservation, batch auditing and blockers verification for an untrusted and an outsourced storage model (Pathare and Chouragadec, 2017). A homomorphic signature mechanism was devised to prevent the usage of the public key certificate, which was based on the new id. This signature system was shown to be resistant to the id attack on the random oracle model and the assault of forged message (Nayak and Tripathy, 2018; Lin et al., 2017). When storing data in a public cloud, one issue is that the data owner must give an enormous number of keys to the users in order for them to access the files. At this place, the knowledge assisted software engineering (KASE) plan was publicly unveiled for the first time. While sharing a huge number of documents, the data owner simply has to supply the specific key to the user, and the user only needs to provide the single trapdoor. Although the concept is innovative, the KASE technique does not apply to the increasingly common manufactured cloud. Cui et al. (2016) claim that as the amount of data grows, distribution management system (DMS) will be unable to handle it. As a result, various proven data possession (PDP) schemes have been developed, and practically all data lacks security. So, here in these certificates, PDP was introduced, which was based on bilinear pairing. Because of its feature of being robust as well as efficient, this is mostly applicable in DMS. The main purpose of this research is to design and implement a secure cloud infrastructure for sharing group data. This research provides an efficient and secure protocol for multiple user data in the cloud, allowing many users to easily share data. Design/methodology/approach: The methodology and contribution of this paper is given as follows. The major goal of this study is to design and implement a secure cloud infrastructure for sharing group data. This study provides an efficient and secure protocol for multiple user data in cloud, allowing several users to share data without difficulty. The primary purpose of this research is to design and implement a secure cloud infrastructure for sharing group data. This research develops an efficient and secure protocol for multiple user data in the cloud, allowing numerous users to exchange data without difficulty. Selection scheme design (SSD) comprises two algorithms; first algorithm is designed for limited users and algorithm 2 is redesigned for the multiple users. Further, the authors design SSD-security protocol which comprises a three-phase model, namely, Phase 1, Phase 2 and Phase 3. Phase 1 generates the parameters and distributes the private key, the second phase generates the general key for all the users that are available and third phase is designed to prevent the dishonest user to entertain in data sharing. Findings: Data sharing in cloud computing provides unlimited computational resources and storage to enterprise and individuals; moreover, cloud computing leads to several privacy and security concerns such as fault tolerance, reliability, confidentiality and data integrity. Furthermore, the key consensus mechanism is fundamental cryptographic primitive for secure communication; moreover, motivated by this phenomenon, the authors developed SSDmechanismwhich embraces the multiple users in the data-sharing model. Originality/value: Files shared in the cloud should be encrypted for security purpose; later these files are decrypted for the users to access the file. Furthermore, the key consensus process is a crucial cryptographic primitive for secure communication; additionally, the authors devised the SSD mechanism, which incorporates numerous users in the data-sharing model, as a result of this phenomena. For evaluation of the SSD method, the authors have considered the ideal environment of the system, that is, the authors have used java as a programming language and eclipse as the integrated drive electronics tool for the proposed model evaluation. Hardware configuration of the model is such that it is packed with 4 GB RAM and i7 processor, the authors have used the PBC library for the pairing operations (PBC Library, 2022). Furthermore, in the following section of this paper, the number of users is varied to compare with the existing methodology RDIC (Li et al., 2020). For the purposes of the SSD-security protocol, a prime number is chosen as the number of users in this work. © 2020, Emerald Publishing Limited.
"
10.1117/12.2635209,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132799641&origin=inward,Conference Paper,SCOPUS_ID:85132799641,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research on function analysis and architecture design method of satellite electrical power system based on sysml,"
AbstractView references

In the design of the satellite electrical power system (EPS), there is a large amount of complicated design information, which cannot be effectively tracked and correlated in the form of documents when it is iterated by different designers. In addition, there are risks of low communication efficiency and quality problems. Model-based system engineering (MBSE) is an effective solution to these problems. This paper adopts the MBSE method, based on the system modeling language (SysML), to analyze the functional requirements and architecture design of the satellite EPS, and at the same time to model and design the energy balance analysis. This will not only achieve efficient information expression and data transmission, but also provide a fundamental guarantee for the thoroughness of the EPS development process in satellite. This new EPS design mode has greatly improved the communication and efficiency of the design. © 2022 SPIE
"
10.1155/2022/6906587,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132519834&origin=inward,Article,SCOPUS_ID:85132519834,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),improving performance of automated essay scoring by using back-translation essays and adjusted scores,"
AbstractView references

Automated essay scoring plays an important role in judging students' language abilities in education. Traditional approaches use handcrafted features to score and are time consuming and complicated. Recently, neural network approaches have improved performance without any feature engineering. Unlike other natural language processing tasks, only a small number of datasets are publicly available for automated essay scoring, and the size of the dataset is not sufficiently large. Considering that the performance of a neural network is closely related to the size of the dataset, the lack of data limits the performance improvement of the automated essay scoring model. In this study, we proposed a method to increase the number of essay-score pairs using back translation and score adjustment and applied it to the Automated Student Assessment Prize dataset for augmentation. We evaluated the effectiveness of the augmented data using models from prior work. In addition, performance was evaluated in a model using long short-term memory, which is widely used for automated essay scoring. The performance was improved by using augmented data. © 2022 You-Jin Jong et al.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132495814&origin=inward,Conference Paper,SCOPUS_ID:85132495814,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),unsupervised corpus aware language model pre-training for dense passage retrieval,"
AbstractView references

Recent research demonstrates the effectiveness of using fine-tuned language models (LM) for dense retrieval. However, dense retrievers are hard to train, typically requiring heavily engineered fine-tuning pipelines to realize their full potential. In this paper, we identify and address two underlying problems of dense retrievers: i) fragility to training data noise and ii) requiring large batches to robustly learn the embedding space. We use the recently proposed Condenser pre-training architecture, which learns to condense information into the dense vector through LM pre-training. On top of it, we propose coCondenser, which adds an unsupervised corpus-level contrastive loss to warm up the passage embedding space. Experiments on MS-MARCO, Natural Question, and Trivia QA datasets show that coCondenser removes the need for heavy data engineering such as augmentation, synthesis, or filtering, and the need for large batch training. It shows comparable performance to RocketQA, a state-of-the-art, heavily engineered system, using simple small batch fine-tuning. © 2022 Association for Computational Linguistics.
"
10.1145/3510003.3510049,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132288505&origin=inward,Conference Paper,SCOPUS_ID:85132288505,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),multilingual training for software engineering,"
AbstractView references

Well-trained machine-learning models, which leverage large amounts of open-source software data, have now become an interesting approach to automating many software engineering tasks. Several SE tasks have all been subject to this approach, with performance gradually improving over the past several years with better models and training methods. More, and more diverse, clean, labeled data is better for training; but constructing good-quality datasets is time-consuming and challenging. Ways of augmenting the volume and diversity of clean, labeled data generally have wide applicability. For some languages (e.g., Ruby) labeled data is less abundant; in others (e.g., JavaScript) the available data maybe more focused on some application domains, and thus less diverse. As a way around such data bottlenecks, we present evidence suggesting that human-written code in different languages (which performs the same function), is rather similar, and particularly preserving of identifier naming patterns; we further present evidence suggesting that identifiers are a very important element of training data for software engineering tasks. We leverage this rather fortuitous phenomenon to find evidence that available multilingual training data (across different languages) can be used to amplify performance. We study this for 3 different tasks: code summarization, code retrieval, and function naming. We note that this data-augmenting approach is broadly compatible with different tasks, languages, and machine-learning models. © 2022 ACM.
"
10.1155/2022/3153845,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132031974&origin=inward,Article,SCOPUS_ID:85132031974,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an application of english reading mobile teaching model based on k -means algorithm,"
AbstractView references

The rapid development and maturity of emerging technologies such as mobile Internet and artificial intelligence have had a subversive impact on all aspects of teaching, and mobile learning has become the normal learning of students. Educational technology has developed from traditional technology and ordinary media technology to multimedia information network technology. Personal computers and wired networks have been more and more widely used. Wireless communication technology and mobile Internet technology of mobile computing devices will be popularized, and the trend from digital learning to mobility will be more and more obvious. This study uses the k-means algorithm to analyze the English reading performance, finds out the factors affecting the test performance, provides guidance for improving the pass rate of CET-4, and focuses on the rise of mobile learning under this background and its specific application in English teaching. The experimental results show that 39 students (55%) believe that mobile reading learning provides rich and real reading materials, can obtain real and authentic language input, and increase their interest in English reading. 31 students (44%) believed that the learning model was helpful to enhance autonomous learning ability. Therefore, one of the biggest advantages of this clustering algorithm is that it has high scalability and efficiency in the processing of large data sets. The mobile teaching model of English reading based on the k-means algorithm can more effectively promote the interaction of English reading teaching and improve students' English text reading comprehension ability than the traditional teaching model. In addition, it can promote students' active learning and cultivate students' awareness of autonomous learning and cooperation. © 2022 Changhong Peng.
"
10.1109/ACCESS.2022.3179712,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131734398&origin=inward,Article,SCOPUS_ID:85131734398,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an approach to develop collaborative virtual labs in modelica,"
AbstractView references

Virtual labs are valuable educational resources in control education, and are widely used in the process industry as tools for operator training and decision aid. In these application domains, virtual labs typically rely on the interactive simulation of large-scale hybrid-DAE models with components of different engineering domains, whose description can be greatly simplified by the use of the Modelica language. Existing free and commercial Modelica libraries of different domains can be used to describe these models. The Interactive Modelica library facilitates developing virtual labs based on Modelica models, using only Modelica. A new major release of the Interactive Modelica library is presented in this paper, whose most relevant feature is to facilitate the implementation of collaborative virtual labs written using only the Modelica language. This library can be used with the environment OpenModelica, facilitating the implementation of cooperative virtual labs using only open software. This type of virtual lab, which allows several students to interact cooperatively with the same model simulation run, is an effective tool in the context of collaborative learning methods. The efficient communication among the graphical user interfaces and the simulation model is a key issue. We developed a new communication protocol, a synchronization algorithm, and redesigned the Modelica classes of the library to make the communication completely transparent to virtual lab developers. The implementation of a collaborative virtual lab for process control education, based on a simplified version of the Tennessee Eastman process, is discussed. The Interactive Modelica library is freely distributed under Modelica License 2 and can be downloaded from http://www.euclides.dia.uned.es/Interactive. © 2013 IEEE.
"
10.1007/978-3-030-95473-4_1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85130792924&origin=inward,Book Chapter,SCOPUS_ID:85130792924,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"‘thinking born of curiosity, revolt, and change’","
AbstractView references

Environmental complexity and velocity have been dominant moderators in strategy and governance research. The underlying structure of environmental change has become increasingly distributed and discontinuous. The disintermediating nature of information technologies is challenging entrenched business models as well as logics of organizing. Centralized top-down strategy processes are rather anti-clocked unless dynamically and generatively intertwined in real time with bottom-up stimuli and data emanating from field operations in pursuit of prediction error minimization. Neither established logics of organizing nor prevalent corporate governance models are a match for ‘wholesale digital reinvention’ and continuous strategic renewal. In fact, akin to the AI evolution from a supervised to a deep learning logic, corporate governance is challenged to evolve from a supervised and rule-based (‘best practices’-centric) learning system to one of unsupervised (un)learning, i.e., self-organization. To survive, the firm must be conceived as one cross-hierarchically integrated inference machine. Organizing is generative prediction error minimization. Strategy is redefined as top-down/bottom-up predictions processing. “The search for knowledge is not nourished by certainty: it is nourished by a radical absence of certainty […] thinking born of curiosity, revolt, change” (Rovelli 2021). © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.23919/DATE54114.2022.9774656,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85130773282&origin=inward,Conference Paper,SCOPUS_ID:85130773282,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),aime: watermarking ai models by leveraging errors,"
AbstractView references

The recent evolution of deep neural networks (DNNs) has made running complex data analytics tasks, which range from natural language processing, object detection to autonomous cars, artificial intelligence (AI) warfare, cloud, healthcare, industrial robots, and edge devices feasible. The benefits of AI are indisputable. However, there are several concerns regarding the security of the deployed AI models, such as reverse engineering and Intellectual Property (IP) piracy. Accumulating a sufficiently large amount of data - building, training, improvement, and model deployment require immense human and computational power, making the process expensive. Therefore, it is of utmost importance to protect the model against IP infringement. We propose AIME, a novel watermarking framework that captures model inaccuracy during the training phase and converts it into the owner-specific unique signature. The watermark is embedded within the class mispredictions of the DNN model. Watermark extraction is performed when the model is queried by an owner-specific sequence of key inputs, and the signature is decoded from the sequence of model predictions. AIME works with negligible watermark embedding runtime overhead while preserving the accurate functionality of the DNN. We have performed a comprehensive evaluation of AIME, which models on MNIST, Fashion-MNIST, and CIFAR-10 dataset and corroborated its effectiveness, robustness, and performance. © 2022 EDAA.
"
10.1109/ICNS54818.2022.9771478,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85130698712&origin=inward,Conference Paper,SCOPUS_ID:85130698712,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),utilizing synthetic data for vv&amp;c of machine learning applications,"
AbstractView references

Machine Learning (ML) offers a revolutionary method to build algorithms from data. This data-driven methodology has proven to be very effective in progressing fields ranging from natural language processing to autonomy. Simultaneously, the technique has also opened significant challenges in system validation with important issues when used in safety-critical systems. If data forms function, how does one know the generated function is safe? Further, fundamental to the ML approach is the availability of vast amounts of data for training a ML model. For a variety of important applications, the availability of data is challenging.In this paper, we propose a unique computational paradigm which combines the inherent advantages of ML with algorithmic transformational methods. Using this technique, one can generate a large amount of synthetic data which preserve the ""signature""of the original dataset while building variations that aid the ML training process. Further, the nature of the algorithmic transformations offers insights into the validation and verification (V&V) scenario generation process. An example is provided of the methodology exercised in a maintenance application for the infrastructure delivering the safety critical Automatic Dependent Surveillance-Broadcast (ADS-B) services in the United States (US). © 2022 IEEE.
"
10.1109/MLKE55170.2022.00027,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85130014728&origin=inward,Conference Paper,SCOPUS_ID:85130014728,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an ai planning approach to factory production planning and scheduling,"
AbstractView references

The factory production planning and scheduling is an essential process for modern industry manufacturing, which includes production planning of workshop machines and transportation coordination among different locations. Generally, large-scale systems such as ERP and APS are used to complete the planning guidance. In this paper, we employ an AI planning approach to factory production planning and scheduling. The proposed approach models the factory production planning problem as a standard AI planning task expressed by the formal language of PDDL, and solves the task through a state-of-the-art planner which results in a complete production planning solution. The model simplifies the process of production planning and scheduling and can give valuable planning guidance for real production manufacture. © 2022 IEEE.
"
10.1155/2022/4250202,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129963896&origin=inward,Article,SCOPUS_ID:85129963896,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),holistic language teaching method in college english vocabulary teaching under big data and multimedia environment,"
AbstractView references

With the rapid advancement of informatization on a global scale, human society has entered a new era of information explosion and continuous data updating. In this situation, big data have become the future development trend of the world. In the context of big data, we must not only understand the world but also learn how to obtain information, and traditional teaching models can no longer meet the needs of teaching. Therefore, it is necessary to combine modern information technology, multimedia technology, and other means to assist the development of classroom teaching. At the same time, the holistic language teaching method plays an important role in improving students' comprehensive ability, comprehension, expression, and thinking innovation ability. Therefore, this article attempts to combine the teaching background of big data and multimedia and use their advantages in teaching to find effective teaching methods to promote students' vocabulary acquisition. This article uses questionnaire surveys and data analysis methods to understand the difficulties encountered by college students in English vocabulary learning and their views on the application of holistic language teaching methods in English vocabulary teaching. According to the survey results, due to factors such as large English vocabulary and complicated spelling rules, most interviewees think English vocabulary learning is more difficult. In addition, most of the interviewees believe that the application of the whole language teaching method in college English vocabulary teaching is very necessary and will get better teaching results, thereby improving the efficiency of college students in learning English vocabulary. © 2022 Xiaoxia Peng.
"
10.1109/ICIPTM54933.2022.9754092,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129451448&origin=inward,Conference Paper,SCOPUS_ID:85129451448,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),load balancing and parallel computation model for performance and accuracy over the cluster of nodes,"
AbstractView references

Cloud Computing can be online based network engineering which contributed with a rapid advancement at the progress of communication technological innovation by supplying assistance to clients of assorted conditions with aid from online computing sources. It's terms of hardware and software apps together side software growth testing and platforms applications because tools. Large-scale heterogeneous distributed computing surroundings give the assurance of usage of a huge quantity of computing tools in a comparatively low price. As a way to lessen the software development and setup onto such complicated surroundings, high speed parallel programming languages exist which have to be encouraged by complex operating techniques. There are numerous advantages for consumers in terms of cost and flexibility that come with Cloud computing's anticipated uptake. Building on well-established research in Internet solutions, networks and utility computing, virtualization et cetera Service-Oriented Architectures and the Internet of Services (IoS) have implications for a wide range of technological issues such as parallel computing and load balancing as well as high availability and scalability. Effective load balancing methods are essential to solving these issues. Since such systems' size and complexity make it impossible to concentrate job execution on a few select servers, a parallel distributed solution is required. Adaptive task load model is the name of the method wesuggest in our article for balancing the workload (ATLM). We developed an adaptive parallel distributed computing paradigm as a result of this (ADPM). While still maintaining the model's integrity, ADPM employs a more flexible synchronization approach to cut down on the amount of time synchronous operations use. As well as the ATLM load balancing technique, which solves the straggler issue caused by the performance disparity between nodes, ADPM also applies it to ensure model correctness. The results indicate that combining ADPM and ATLM improves training efficiency without compromising model correctness. © 2022 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128871557&origin=inward,Conference Paper,SCOPUS_ID:85128871557,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),gestural inputs as control interaction for generative human-ai co-creation,"
AbstractView references

While AI-powered generative systems offer new avenues for art-making, directing these algorithms remains a central challenge. Current methods for steering have focused on conventional interaction techniques (widgets, examples, etc.). This position paper argues that the intersection of user needs in creative contexts and algorithmic capabilities requires re-thinking our interactions with generative AI. We propose that rough gestural inputs, such as hand gestures or sketching, can enhance the experience of human-AI co-creation-even for text. First, the undetermined and ambiguous nature of gestural inputs corresponds to the purpose and the capabilities of generative systems. Second, rough gestural can be intuitive and expressive, facilitating iterative co-creation. We discuss design dimensions for inputs of artifact-creating systems, then characterize existing and proposed input interactions with those dimensions. We highlight how gestural inputs can expand the control interaction for generative systems by analyzing existing tools and describing speculative input designs. Our hope is that gestural inputs become actively studied and adopted to support user intentions and maximize the perceived efficacy of generative algorithms. © 2021 Copyright for this paper by its authors
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128858691&origin=inward,Conference Paper,SCOPUS_ID:85128858691,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),drinking chai with your (ai) programming partner: a design fiction about generative ai for software engineering,"
AbstractView references

Using design fiction, we develop a series of possible generative AI features and applications that could be developed in the future of humans' roles in software engineering. We use the fiction to highlight choices and value-tensions among these potential futures. © 2022 Copyright for this paper by its authors
"
10.1007/978-981-19-0390-8_114,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128752160&origin=inward,Conference Paper,SCOPUS_ID:85128752160,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a simulation framework for vrm requirement model,"
AbstractView references

Model simulation is a research method that can discover and improve system design problems through various experiments without the participation of the actual system. In the field of security critical systems and aeroelectronics, a series of security problems are often caused by large and complex system requirements. In this paper, a simulation framework for VRM formal requirement model is proposed to find and solve the problems that occur during the design phase of system requirements. The work includes: comprehensively analyzing each component of VRM model, relying on SysML model to build simulation sequence, executing simulation event propagation system constraints, and displaying system model status in real time. The framework solves the ambiguity of natural language requirements by using VRM formal requirement model, and designs some SysML modeling methods to help emulators build and refine simulation behaviors through case diagrams. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
"
10.1155/2022/9033421,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128748145&origin=inward,Article,SCOPUS_ID:85128748145,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the spoken english practice system based on computer english speech recognition technology,"
AbstractView references

Spoken English practice requires a combination of listening, speaking, reading, and writing, among which listening and speaking are the most difficult. In order to improve the speaking ability of the practitioner, the pronunciation of spoken English needs to be corrected in time. However, the workload of manual evaluation is too large, so it is necessary to combine intelligent methods for spoken language recognition. Based on the needs of spoken English pronunciation correction, this paper combines the computer English speech recognition technology to construct the spoken English recognition and correction model and combines the coding technology to study the English speech recognition technology. Moreover, this article constructs the spoken English practice system based on the actual needs of spoken English practice. Finally, this paper verifies the reliability of this system through experimental research, which provides a reliable means for the subsequent intelligent learning of spoken English. © 2022 Chi Gao.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128715774&origin=inward,Conference Paper,SCOPUS_ID:85128715774,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated requirement formalization using product design specifications,"
AbstractView references

Assuring the quality of complex and highly configurable software systems is a demanding and time-consuming process. Especially for safety-critical systems, extensive testing based on requirements is necessary. Methods for model-based test automation in agile software development offer the possibility to overcome these difficulties. However, it is still a major effort to create formal models from functional requirements in natural language on a large scale. In this paper, we present and evaluate automated support for the requirements formalization process to reduce cost and effort. We present a new approach based on Natural Language Processing (NLP) and textual similarity using requirements and product design specifications to generate human- and machine-readable models. The method is evaluated on an industrial use case from the railway domain. The recommended requirement models for the considered propulsion system show an average accuracy of more than 90% and an exact match of the entire models of about 55%. These results show that our approach can support the requirements formalization process, which can be further used for test case generation and execution, as well as for requirements and design verification. © 2022 Copyright for this paper by its authors
"
10.1016/j.procir.2022.05.266,S2212827122007156,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128666422&origin=inward,Conference Paper,SCOPUS_ID:85128666422,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),geometric variability in parametric 3d models: implications for engineering design,"Modern manufacturing companies operate in environments characterized by increasingly shorter development cycles and the need to develop highly customizable products at competitive prices. In this paper, we examine the role of parametric 3D modeling in the product development process, and highlight the importance of robustness, flexibility, and responsiveness to geometric variations, which are particularly relevant in the context of the Model-Based Enterprise (MBE). We discuss the often-inefficient parametric 3D modeling practices used in industry, their root causes and implications, and identify the detrimental effects of low-quality models on engineering design activities, specifically design changes during development, generative design algorithms, design optimization, simulation, product/part family configuration, AI-based parametric modeling, Model-Based System Engineering (MBSE), and parametric and adaptive encryption. Finally, we present future lines of research aimed at increasing the quality of parametric models."
10.1007/978-3-030-99336-8_7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128603370&origin=inward,Conference Paper,SCOPUS_ID:85128603370,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),verified security for the morello capability-enhanced prototype arm architecture,"
AbstractView references

Memory safety bugs continue to be a major source of security vulnerabilities in our critical infrastructure. The CHERI project has proposed extending conventional architectures with hardware-supported capabilities to enable fine-grained memory protection and scalable compartmentalisation, allowing historically memory-unsafe C and C++ to be adapted to deterministically mitigate large classes of vulnerabilities, while requiring only minor changes to existing system software sources. Arm is currently designing and building Morello, a CHERI-enabled prototype architecture, processor, SoC, and board, extending the high-performance Neoverse N1, to enable industrial evaluation of CHERI and pave the way for potential mass-market adoption. However, for such a major new security-oriented architecture feature, it is important to establish high confidence that it does provide the intended protections, and that cannot be done with conventional engineering techniques. In this paper we put the Morello architecture on a solid mathematical footing from the outset. We define the fundamental security property that Morello aims to provide, reachable capability monotonicity, and prove that the architecture definition satisfies it. This proof is mechanised in Isabelle/HOL, and applies to a translation of the official Arm specification of the Morello instruction-set architecture (ISA) into Isabelle. The main challenge is handling the complexity and scale of a production architecture: 62,000 lines of specification, translated to 210,000 lines of Isabelle. We do so by factoring the proof via a narrow abstraction capturing essential properties of arbitrary CHERI ISAs, expressed above a monadic intra-instruction semantics. We also develop a model-based test generator, which generates instruction-sequence tests that give good specification coverage, used in early testing of the Morello implementation and in Morello QEMU development, and we use Arm’s internal test suite to validate our model. This gives us machine-checked mathematical proofs of whole-ISA security properties of a full-scale industry architecture, at design-time. To the best of our knowledge, this is the first demonstration that that is feasible, and it significantly increases confidence in Morello. © 2022, The Author(s).
"
10.1109/ICSSIT53264.2022.9716516,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127308934&origin=inward,Conference Paper,SCOPUS_ID:85127308934,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),computer-based simulation data model for employment of business management talents in colleges: python implementations,"
AbstractView references

Computer-based simulation data model for the employment of business management talents in the colleges with Python implementations is studied in this paper. While improving the efficiency of education management, it is necessary to emphasize comprehensive thinking of the entire management process. Its basic idea is to use the created model to explain the changes of dependent variables with the changes of one or more independent variables, and find out the relationship between independent variables and the dependent variables through then testing the model. Besides this, the Python is adopted for the systematic implementations. Python is a pure object-oriented language that supports all object-oriented mechanisms, which makes it applicable to large-scale software project development. Using this tool, the designed model is validated through the further simulation. Compared with the other methods, our design model cam evaluate better. © 2022 IEEE
"
10.1145/3510003.3512766,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127255574&origin=inward,Conference Paper,SCOPUS_ID:85127255574,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),jucify: a step towards android code unification for enhanced static analysis,"
AbstractView references

Native code is now commonplace within Android app packages where it co-exists and interacts with Dex bytecode through the Java Native Interface to deliver rich app functionalities. Yet, state-of-the-art static analysis approaches have mostly overlooked the presence of such native code, which, however, may implement some key sensitive, or even malicious, parts of the app behavior. This limitation of the state of the art is a severe threat to validity in a large range of static analyses that do not have a complete view of the executable code in apps. To address this issue, we propose a new advance in the ambitious research direction of building a unified model of all code in Android apps. The JUCIFY approach presented in this paper is a significant step towards such a model, where we extract and merge call graphs of native code and bytecode to make the final model readily-usable by a common Android analysis framework: in our implementation, JUCIFY builds on the Soot internal intermediate representation. We performed empirical investigations to highlight how, without the unified model, a significant amount of Java methods called from the native code are 'unreachable' in apps' callgraphs, both in goodware and malware. Using JUCIFY, we were able to enable static analyzers to reveal cases where malware relied on native code to hide invocation of payment library code or of other sensitive code in the Android framework. Additionally, JUCIFY'S model enables state-of-the-art tools to achieve better precision and recall in detecting data leaks through native code. Finally, we show that by using JUCIFY we can find sensitive data leaks that pass through native code. © 2022 ACM.
"
10.1007/978-3-030-98464-9_5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127051029&origin=inward,Conference Paper,SCOPUS_ID:85127051029,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a zero-shot learning approach to classifying requirements: a preliminary study,"
AbstractView references

Context and motivation: Advances in Machine Learning (ML) and Deep Learning (DL) technologies have transformed the field of Natural Language Processing (NLP), making NLP more practical and accessible. Motivated by these exciting developments, Requirements Engineering (RE) researchers have been experimenting ML/DL based approaches for a range of RE tasks, such as requirements classification, requirements tracing, ambiguity detection, and modelling. Question/problem: Most of today’s ML/DL approaches are based on supervised learning techniques, meaning that they need to be trained using annotated datasets to learn how to assign a class label to examples from an application domain. This requirement poses an enormous challenge to RE researchers, as the lack of requirements datasets in general and annotated datasets in particular, makes it difficult for them to fully exploit the benefit of the advanced ML/DL technologies. Principal ideas/results: To address this challenge, this paper proposes a novel approach that employs the Zero-Shot Learning (ZSL) technique to perform requirements classification. We build several classification models using ZSL. We focus on the classification task because many RE tasks can be solved as classification problems by a large number of available ML/DL methods. In this preliminary study, we demonstrate our approach by classifying non-functional requirements (NFRs) into two categories: Usability and Security. ZSL supports learning without domain-specific training data, thus solving the lack of annotated datasets typical of RE. The study shows that our approach achieves an average of 82% recall and F-score. Contribution: This study demonstrates the potential of ZSL for requirements classification. The promising results of this study pave the way for further investigations and large-scale studies. An important implication is that it is possible to have very little or no training data to perform requirements classification. The proposed approach thus contributes to the solution of the long-standing problem of data shortage in RE. © 2022, Springer Nature Switzerland AG.
"
10.3390/s22020700,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122859430&origin=inward,Article,SCOPUS_ID:85122859430,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),neural collaborative filtering with ontologies for integrated recommendation systems,"
AbstractView references

Machine learning (ML) and especially deep learning (DL) with neural networks have demonstrated an amazing success in all sorts of AI problems, from computer vision to game playing, from natural language processing to speech and image recognition. In many ways, the approach of ML toward solving a class of problems is fundamentally different than the one followed in classical engineering, or with ontologies. While the latter rely on detailed domain knowledge and almost exhaustive search by means of static inference rules, ML adopts the view of collecting large datasets and processes this massive information through a generic learning algorithm that builds up tentative solutions. Combining the capabilities of ontology-based recommendation and ML-based techniques in a hybrid system is thus a natural and promising method to enhance semantic knowledge with statistical models. This merge could alleviate the burden of creating large, narrowly focused ontologies for complicated domains, by using probabilistic or generative models to enhance the predictions without attempting to provide a semantic support for them. In this paper, we present a novel hybrid recommendation system that blends a single architecture of classical knowledge-driven recommendations arising from a tailored ontology with recommendations generated by a data-driven approach, specifically with classifiers and a neural collaborative filtering. We show that bringing together these knowledge-driven and data-driven worlds provides some measurable improvement, enabling the transfer of semantic information to ML and, in the opposite direction, statistical knowledge to the ontology. Moreover, the novel proposed system enables the extraction of the reasoning recommendation results after updating the standard ontology with the new products and user behaviors, thus capturing the dynamic behavior of the environment of our interest. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
"
10.3390/electronics11010087,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121735802&origin=inward,Article,SCOPUS_ID:85121735802,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a vr-enabled chatbot supporting design and manufacturing of large and complex power transformers,"
AbstractView references

Virtual reality (VR) immersive technology allows users to experience enhanced reality using human–computer interfaces (HCI). Many systems have implemented VR with improved HCI to provide strategic market advantages for industry and engineering applications. An intelligent chatbot is a conversational system capable of natural language communication allowing users to ask questions and receive answers online to enhance customer services. This research develops and implements a system framework for a VR-enabled large industrial power transformer mass-customization chatbot. The research collected 1272 frequently asked questions (FAQs) from a power transformer manufacturers’ knowledge base that is used for question matching and answer retrieval. More than 1.2 million Wikipedia engineering pages were used to train a word-embedding model for natural language understanding of question intent. The complex engineering questions and answers are integrated with an immersive VR computer human interface. The system enables users to ask questions and receive explicit and detailed answers combined with 3D immersive images of industrial sized power transformer assemblies. The user interfaces can be projected into the VR headwear or computer screen and manipulated with a controller. The unique immersive VR consultation chatbot system is to support real-time design consultation for the design and manufacturing of complex power transformers. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
"
10.1016/j.anucene.2021.108754,S0306454921006307,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119509723&origin=inward,Article,SCOPUS_ID:85119509723,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),risk analysis virtual environment for dynamic event tree-based analyses,"
                  Conventional Event-Tree (ET) based methodologies are extensively used as tools to perform reliability and safety assessment of complex and critical engineering systems. One of the disadvantages of these methods is that timing/sequencing of events and system dynamics is not explicitly accounted for in the analysis. In order to overcome these limitations several techniques, also known as Dynamic Probabilistic Risk Assessment (DPRA), have been developed. Monte-Carlo (MC) and Dynamic Event Tree (DET) are two of the most widely used DPRA methodologies to perform safety assessment of Nuclear Power Plants (NPP). Since 2012, the Idaho National Laboratory (INL) is developing its own tool to perform Dynamic PRA: RAVEN (Risk Analysis and Virtual ENvironment). RAVEN has been designed in a high modular and pluggable way to enable easy integration of different programming languages (i.e., Python, C++) and coupling with other application including, among the others, several thermal–hydraulic and severe accident codes (e.g., RELAP5-3D, MELCOR, MAAP5, TRACE, etc.). RAVEN is aimed to provide a framework/container of capabilities for engineers and scientists to analyze the response of systems, physics and multi-physics, employing advanced numerical techniques and algorithms. Moreover, RAVEN models stochastic events, such as components failures, and performs uncertainty quantification (UQ). Such stochastic modeling is employed by using sampling strategies among which both MC and DET algorithms, which are going to be employed in this paper. In addition, RAVEN processes the large amount of data generated by sampling the physical models using data-mining based algorithms and risk assessment techniques. This paper provides an overview of the DET methodologies that have been deployed within the RAVEN framework, showing the potential of such techniques for the analysis of complex systems. A brief background of classical methodologies and their limitation is also reported and represent the motivation for the deployment of such dynamic technique. In addition, results from a pressurized water reactor loss of coolant accident scenario, using RELAP5-3D as physical model, are reported.
               "
10.1007/978-3-030-89511-2_18,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118787189&origin=inward,Conference Paper,SCOPUS_ID:85118787189,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),variant translation strategy of dough sculpture art based on deep neural network algorithm,"
AbstractView references

With the development of globalization and information diversification, translation plays a pivotal role in the exchanges of countries around the world; many translation theories have also emerged from this and become increasingly mature. As an efficient tool, machine translation can realize the equivalent conversion between different languages while retaining the original semantics, which has important practical significance. This article aims to study the translation strategy of dough art translation based on deep neural network algorithms. Based on the analysis of the characteristics of dough art, the main content of the translation and the neural machine translation model, it aims at the inability of neural machine translation technology knowledge in the existing language translation system. It makes good use of the current situation and existing problems, and proposes a new neural machine translation system model with a large amount of speech sequence information. Based on the theoretical basis of fully integrating people’s attention and thinking mechanism gru two-way translation language model, the stanford parser is used to analyze the language and syntax, and after obtaining relevant data and information about the part of speech and the order of sentences in the language. The language and encoder components integrated into the language translation model in the form of two-way encoding, using vector splicing methods and forms to jointly construct a vector background. Experiments have proved that adding a word sequence of information can greatly improve the translation model. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1007/978-3-030-85584-0_10,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85113822810&origin=inward,Book Chapter,SCOPUS_ID:85113822810,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),modeling and simulation of a swarm robot application using mbse method and multi-agent technology: monitoring oil spills,"
AbstractView references

Swarm Robotics is a new approach to the coordination of a large number of robots inspired by nature. This approach aims to design collective behaviors for many robots. Several researchers have tried to develop structured design methods, but unfortunately, these methods are still limited. Today, swarm robotics are used in many fields that include agriculture, medicine, industrial, etc. One of the most important fields that require swarm robots is surveillance. In this chapter, we present in the first section some methods of designing swarm robot systems by identifying swarm engineering based on a model (MBSE) and multi-agent simulation. Then, we study the energy problem of these robotic systems and the solution proposed by the researchers. In the second section, we present an application for detecting oil in the sea and cleaning it using swarm robots. We will model this application using the MBSE method with the SysML language. We will use the different diagrams of SysML to specify the system requirements and model the functions offered by the system. Finally, we will simulate the models on a multi-agent tool to identify the functional and structural architecture of the system. Throughout this approach, we check the transition from one step to another to ensure the consistency and continuity of the method. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1080/10400435.2021.1930283,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108616076&origin=inward,Article,SCOPUS_ID:85108616076,scopus,2022-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),evaluating user-personas as supplementary tools in aac intervention and clinician decision making,"
AbstractView references

Models used for the design and service delivery of Augmentative and Alternative Communication (AAC) systems are limited. There are no standardized protocols for gathering user requirements beyond clinical/diagnostic information relating to AAC access needs (i.e., physical and cognitive capabilities). Nonetheless, information on the social, cultural, and psychological aspects of technology orientation and use are important to understanding how an AAC system will complement the user’s lifestyle, personal goals, values, and activities. Persona development is a user-centered design method that creates descriptive user models of different segments of a user population. Personas describe users’ personal characteristics, and the ways in which they think, behave, and engage in activities (with or without technology). The objective of this study is to investigate the utility of user personas as a supplementary tool to aid SLPs in AAC assessment and service delivery. Three personas of individuals with Amyotrophic Lateral Sclerosis (ALS) were developed and validated in our prior research. Twelve SLPs engaged in mock AAC assessments of the three ALS case studies under two conditions: with or without the use of personas as a supplemental informational tool. By and large, there were no statistically significant differences between groups across objective measures; however, interview sessions with the SLP participants revealed benefits to using personas during clinical decision-making, particularly for training novice SLPs. Discussion also focuses on ways in which user personas can be adapted and improved to mitigate some of the challenges and risks identified. © 2021 RESNA.
"
10.1021/acssynbio.1c00157,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119582324&origin=inward,Article,SCOPUS_ID:85119582324,scopus,2021-12-17,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),virtual parts repository 2: model-driven design of genetic regulatory circuits,"
AbstractView references

Engineering genetic regulatory circuits is key to the creation of biological applications that are responsive to environmental changes. Computational models can assist in understanding especially large and complex circuits for which manual analysis is infeasible, permitting a model-driven design process. However, there are still few tools that offer the ability to simulate the system under design. One of the reasons for this is the lack of accessible model repositories or libraries that cater to the modular composition of models of synthetic systems. Here, we present the second version of the Virtual Parts Repository, a framework to facilitate the model-driven design of genetic regulatory circuits, which provides reusable, modular, and composable models. The new framework is service-oriented, easier to use in computational workflows, and provides several new features and access methods. New features include supporting hierarchical designs via a graph-based repository or compatible remote repositories, enriching existing designs, and using designs provided in Synthetic Biology Open Language documents to derive system-scale and hierarchical Systems Biology Markup Language models. We also present a reaction-based modeling abstraction inspired by rule-based modeling techniques to facilitate scalable and modular modeling of complex and large designs. This modeling abstraction enhances the modeling capability of the framework, for example, to incorporate design patterns such as roadblocking, distributed deployment of genetic circuits using plasmids, and cellular resource dependency. The framework and the modeling abstraction presented in this paper allow computational design tools to take advantage of computational simulations and ultimately help facilitate more predictable applications. © 2021 American Chemical Society
"
10.1016/j.oceaneng.2021.110102,S0029801821014268,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118327449&origin=inward,Article,SCOPUS_ID:85118327449,scopus,2021-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),wind-resistant performance and failure modes for a semi-submersible offshore platform during jacking closure,"
                  Currently, the construction process of large offshore platforms worldwide involves building the topside and hull separately and then having them integrated. The jacking closure scheme by overlapped support tower bearing the topside is a new integration method, which has a great advantage over other traditional methods in vertical bearing capacity. But its weak bearing capacity to lateral forces like wind load limits the jacking height. Moreover, few engineering examples and academic researches also cause insufficient understanding of it. Therefore, this paper, based on the first deep-water semi-submersible platform with the10,000-ton oil storage worldwide, i.e. “Deep Sea No. 1” energy station, aims to evaluate the wind-resistant performance during the jacking closure process. Firstly, the background and the jacking closure scheme of this project are introduced in detail. Secondly, the finite element models (FEMs) of the jacking system are established according to the overlapped characteristics of jacking towers and the corresponding failure criteria. Then, the wind-induced vibration response is simulated, and the static pushover is conducted to evaluate the ultimate bearing capacity and its influencing factors. The failure criteria are checked using ANSYS parameter design language. It is found that “the occurrence of tensile stress on the contact surface” is the first and main failure mode of the structure. Additionally, the results also reveal that the strand cables can not only improve the ultimate bearing capacity of the structure but also change its weak direction. However, the improvement is gradually weakened with the increase of jacking height. In contrast, the contribution of the bracing pipes to the structural bearing capacity is always significant. These findings can be used to develop a more reasonable closure plan.
               "
10.1038/s41540-021-00182-w,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107152103&origin=inward,Article,SCOPUS_ID:85107152103,scopus,2021-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),characteristics of mathematical modeling languages that facilitate model reuse in systems biology: a software engineering perspective,"
AbstractView references

Reuse of mathematical models becomes increasingly important in systems biology as research moves toward large, multi-scale models composed of heterogeneous subcomponents. Currently, many models are not easily reusable due to inflexible or confusing code, inappropriate languages, or insufficient documentation. Best practice suggestions rarely cover such low-level design aspects. This gap could be filled by software engineering, which addresses those same issues for software reuse. We show that languages can facilitate reusability by being modular, human-readable, hybrid (i.e., supporting multiple formalisms), open, declarative, and by supporting the graphical representation of models. Modelers should not only use such a language, but be aware of the features that make it desirable and know how to apply them effectively. For this reason, we compare existing suitable languages in detail and demonstrate their benefits for a modular model of the human cardiac conduction system written in Modelica. © 2021, The Author(s).
"
10.1007/s10462-021-09967-1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100910523&origin=inward,Article,SCOPUS_ID:85100910523,scopus,2021-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),application of deep learning algorithms in geotechnical engineering: a short critical review,"
AbstractView references

With the advent of big data era, deep learning (DL) has become an essential research subject in the field of artificial intelligence (AI). DL algorithms are characterized with powerful feature learning and expression capabilities compared with the traditional machine learning (ML) methods, which attracts worldwide researchers from different fields to its increasingly wide applications. Furthermore, in the field of geochnical engineering, DL has been widely adopted in various research topics, a comprehensive review summarizing its application is desirable. Consequently, this study presented the state of practice of DL in geotechnical engineering, and depicted the statistical trend of the published papers. Four major algorithms, including feedforward neural (FNN), recurrent neural network (RNN), convolutional neural network (CNN) and generative adversarial network (GAN) along with their geotechnical applications were elaborated. In addition, a thorough summary containing pubilished literatures, the corresponding reference cases, the adopted DL algorithms as well as the related geotechnical topics was compiled. Furthermore, the challenges and perspectives of future development of DL in geotechnical engineering were presented and discussed. © 2021, The Author(s), under exclusive licence to Springer Nature B.V. part of Springer Nature.
"
10.1186/s40537-020-00399-2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100438390&origin=inward,Article,SCOPUS_ID:85100438390,scopus,2021-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"array databases: concepts, standards, implementations","
AbstractView references

Multi-dimensional arrays (also known as raster data or gridded data) play a key role in many, if not all science and engineering domains where they typically represent spatio-temporal sensor, image, simulation output, or statistics “datacubes”. As classic database technology does not support arrays adequately, such data today are maintained mostly in silo solutions, with architectures that tend to erode and not keep up with the increasing requirements on performance and service quality. Array Database systems attempt to close this gap by providing declarative query support for flexible ad-hoc analytics on large n-D arrays, similar to what SQL offers on set-oriented data, XQuery on hierarchical data, and SPARQL and CIPHER on graph data. Today, Petascale Array Database installations exist, employing massive parallelism and distributed processing. Hence, questions arise about technology and standards available, usability, and overall maturity. Several papers have compared models and formalisms, and benchmarks have been undertaken as well, typically comparing two systems against each other. While each of these represent valuable research to the best of our knowledge there is no comprehensive survey combining model, query language, architecture, and practical usability, and performance aspects. The size of this comparison differentiates our study as well with 19 systems compared, four benchmarked to an extent and depth clearly exceeding previous papers in the field; for example, subsetting tests were designed in a way that systems cannot be tuned to specifically these queries. It is hoped that this gives a representative overview to all who want to immerse into the field as well as a clear guidance to those who need to choose the best suited datacube tool for their application. This article presents results of the Research Data Alliance (RDA) Array Database Assessment Working Group (ADA:WG), a subgroup of the Big Data Interest Group. It has elicited the state of the art in Array Databases, technically supported by IEEE GRSS and CODATA Germany, to answer the question: how can data scientists and engineers benefit from Array Database technology? As it turns out, Array Databases can offer significant advantages in terms of flexibility, functionality, extensibility, as well as performance and scalability—in total, the database approach of offering “datacubes” analysis-ready heralds a new level of service quality. Investigation shows that there is a lively ecosystem of technology with increasing uptake, and proven array analytics standards are in place. Consequently, such approaches have to be considered a serious option for datacube services in science, engineering and beyond. Tools, though, vary greatly in functionality and performance as it turns out. © 2021, The Author(s).
"
10.1109/TCDS.2020.3033963,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096129955&origin=inward,Article,SCOPUS_ID:85096129955,scopus,2021-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),modeling multiple language learning in a developmental cognitive architecture,"
AbstractView references

In this work, we model multiple natural language learning in a developmental neuroscience-inspired architecture. The artificial neural network with adaptive behavior exploited for language learning (ANNABELL) model, is a large-scale neural network, however, unlike most deep learning methods that solve natural language processing (NLP) tasks, it does not represent an empirical engineering solution for specific NLP problems; rather, its organization complies with findings from cognitive neuroscience, particularly the multicompartment working memory models. The system is appropriately trained to understand the level of cognitive development required for language acquisition and the robustness achieved in learning simultaneously four languages, using a corpus of text-based exchanges of developmental complexity. The selected languages, Greek, Italian and Albanian, besides English, differ significantly in structure and complexity. Initially, the system was validated in each language alone and was then compared with the open-ended cumulative training, in which languages are learned jointly, prior to querying with random language at random order. We aimed to assess if the model could learn the languages together to the same degree of skill as learning each apart. Moreover, we explored the generalization skill in multilingual context questions and the ability to elaborate a short text of preschool literature. We verified if the system could follow a dialogue coherently and cohesively, keeping track of its previous answers and recalling them in subsequent queries. The results show that the architecture developed broad language processing functionalities, with satisfactory performances in each language trained singularly, maintaining high accuracies when they are acquired cumulatively. © 2016 IEEE.
"
10.1145/3472883.3486995,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119280309&origin=inward,Conference Paper,SCOPUS_ID:85119280309,scopus,2021-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards reliable ai for source code understanding,"
AbstractView references

Cloud maturity and popularity have resulted in Open source software (OSS) proliferation. And, in turn, managing OSS code quality has become critical in ensuring sustainable Cloud growth. On this front, AI modeling has gained popularity in source code understanding tasks, promoted by the ready availability of large open codebases. However, we have been observing certain peculiarities with these black-boxes, motivating a call for their reliability to be verified before offsetting traditional code analysis. In this work, we highlight and organize different reliability issues affecting AI-for-code into three stages of an AI pipeline- data collection, model training, and prediction analysis. We highlight the need for concerted efforts from the research community to ensure credibility, accountability, and traceability for AI-for-code. For each stage, we discuss unique opportunities afforded by the source code and software engineering setting to improve AI reliability. © 2021 Association for Computing Machinery.
"
10.1007/s10664-021-09996-y,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112851625&origin=inward,Article,SCOPUS_ID:85112851625,scopus,2021-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),predicting unstable software benchmarks using static source code features,"
AbstractView references

Software benchmarks are only as good as the performance measurements they yield. Unstable benchmarks show high variability among repeated measurements, which causes uncertainty about the actual performance and complicates reliable change assessment. However, if a benchmark is stable or unstable only becomes evident after it has been executed and its results are available. In this paper, we introduce a machine-learning-based approach to predict a benchmark’s stability without having to execute it. Our approach relies on 58 statically-computed source code features, extracted for benchmark code and code called by a benchmark, related to (1) meta information, e.g., lines of code (LOC), (2) programming language elements, e.g., conditionals or loops, and (3) potentially performance-impacting standard library calls, e.g., file and network input/output (I/O). To assess our approach’s effectiveness, we perform a large-scale experiment on 4,461 Go benchmarks coming from 230 open-source software (OSS) projects. First, we assess the prediction performance of our machine learning models using 11 binary classification algorithms. We find that Random Forest performs best with good prediction performance from 0.79 to 0.90, and 0.43 to 0.68, in terms of AUC and MCC, respectively. Second, we perform feature importance analyses for individual features and feature categories. We find that 7 features related to meta-information, slice usage, nested loops, and synchronization application programming interfaces (APIs) are individually important for good predictions; and that the combination of all features of the called source code is paramount for our model, while the combination of features of the benchmark itself is less important. Our results show that although benchmark stability is affected by more than just the source code, we can effectively utilize machine learning models to predict whether a benchmark will be stable or not ahead of execution. This enables spending precious testing time on reliable benchmarks, supporting developers to identify unstable benchmarks during development, allowing unstable benchmarks to be repeated more often, estimating stability in scenarios where repeated benchmark execution is infeasible or impossible, and warning developers if new benchmarks or existing benchmarks executed in new environments will be unstable. © 2021, The Author(s).
"
10.3389/fphy.2021.738112,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119057226&origin=inward,Article,SCOPUS_ID:85119057226,scopus,2021-10-28,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),artificial intelligence for monte carlo simulation in medical physics,"
AbstractView references

Monte Carlo simulation of particle tracking in matter is the reference simulation method in the field of medical physics. It is heavily used in various applications such as 1) patient dose distribution estimation in different therapy modalities (radiotherapy, protontherapy or ion therapy) or for radio-protection investigations of ionizing radiation-based imaging systems (CT, nuclear imaging), 2) development of numerous imaging detectors, in X-ray imaging (conventional CT, dual-energy, multi-spectral, phase contrast …), nuclear imaging (PET, SPECT, Compton Camera) or even advanced specific imaging methods such as proton/ion imaging, or prompt-gamma emission distribution estimation in hadrontherapy monitoring. Monte Carlo simulation is a key tool both in academic research labs as well as industrial research and development services. Because of the very nature of the Monte Carlo method, involving iterative and stochastic estimation of numerous probability density functions, the computation time is high. Despite the continuous and significant progress on computer hardware and the (relative) easiness of using code parallelisms, the computation time is still an issue for highly demanding and complex simulations. Hence, since decades, Variance Reduction Techniques have been proposed to accelerate the processes in a specific configuration. In this article, we review the recent use of Artificial Intelligence methods for Monte Carlo simulation in medical physics and their main associated challenges. In the first section, the main principles of some neural networks architectures such as Convolutional Neural Networks or Generative Adversarial Network are briefly described together with a literature review of their applications in the domain of medical physics Monte Carlo simulations. In particular, we will focus on dose estimation with convolutional neural networks, dose denoising from low statistics Monte Carlo simulations, detector modelling and event selection with neural networks, generative networks for source and phase space modelling. The expected interests of those approaches are discussed. In the second section, we focus on the current challenges that still arise in this promising field. Copyright © 2021 Sarrut, Etxebeste, Muñoz, Krah and Létang.
"
10.1145/3495018.3495474,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126649441&origin=inward,Conference Paper,SCOPUS_ID:85126649441,scopus,2021-10-23,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),construction of mechanical control system based on artificial intelligence technology,"
AbstractView references

Since the concept of ""artificial intelligence""was put forward in 1956, it has been very successful both in application and promotion, and gradually changed its name to engineering technology. So far, it has been used for automatic translation, remote control, robot design and manufacturing, language and image analysis, genetic programming, automated factories, automated programming, aviation and aerospace applications, a large amount of information processing, information storage. The management and execution of complex or large-scale tasks that the human body can complete has great research value for artificial intelligence in these areas. The purpose of this article is to study the construction of a mechanical control system based on artificial intelligence technology. In this document, the motion and rotation equation models of the robotic arm are defined. According to the case in the report, calculate the maximum torque that each arm joint can bear, and use the calculated specific data parameters in the experiment and compare it in the literature. The experimental results are compared with the experimental results of this article. Through the research of the engine control system, based on a large amount of research and practice, this paper examines the threats and loopholes of the system in detail, and designs a system based on WINdow7. This paper uses the data obtained in the actual evaluation and practice of the system and the calculation results of the actual operation method to verify the rationality, feasibility and value of the artificial intelligence-based mechanical operating system. Experimental research shows that based on the artificial intelligence technology proposed in this paper, the specific rotation value of each joint is determined to be as high as 300 degrees through actual observation and practice. The data and the related formulas are calculated, and the results obtained are more reliable and true. It proved the feasibility and value of the mechanical control system specified in this article. © 2021 ACM.
"
10.1093/jamia/ocab126,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85116958963&origin=inward,Article,SCOPUS_ID:85116958963,scopus,2021-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),mt-clinical bert: scaling clinical information extraction with multitask learning,"
AbstractView references

Objective: Clinical notes contain an abundance of important, but not-readily accessible, information about patients. Systems that automatically extract this information rely on large amounts of training data of which there exists limited resources to create. Furthermore, they are developed disjointly, meaning that no information can be shared among task-specific systems. This bottleneck unnecessarily complicates practical application, reduces the performance capabilities of each individual solution, and associates the engineering debt of managing multiple information extraction systems. Materials and Methods: We address these challenges by developing Multitask-Clinical BERT: a single deep learning model that simultaneously performs 8 clinical tasks spanning entity extraction, personal health information identification, language entailment, and similarity by sharing representations among tasks. Results: We compare the performance of our multitasking information extraction system to state-of-the-art BERT sequential fine-tuning baselines. We observe a slight but consistent performance degradation in MT-Clinical BERT relative to sequential fine-tuning. Discussion: These results intuitively suggest that learning a general clinical text representation capable of supporting multiple tasks has the downside of losing the ability to exploit dataset or clinical note-specific properties when compared to a single, task-specific model. Conclusions: We find our single system performs competitively with all state-the-art task-specific systems while also benefiting from massive computational benefits at inference. © 2021 The Author(s).
"
10.1007/s00158-021-02953-9,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107949445&origin=inward,Article,SCOPUS_ID:85107949445,scopus,2021-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),integrating deep learning into cad/cae system: generative design and evaluation of 3d conceptual wheel,"
AbstractView references

Engineering design research integrating artificial intelligence (AI) into computer-aided design (CAD) and computer-aided engineering (CAE) is actively being conducted. This study proposes a deep learning-based CAD/CAE framework in the conceptual design phase that automatically generates 3D CAD designs and evaluates their engineering performance. The proposed framework comprises seven stages: (1) 2D generative design, (2) dimensionality reduction, (3) design of experiment in latent space, (4) CAD automation, (5) CAE automation, (6) transfer learning, and (7) visualization and analysis. The proposed framework is demonstrated through a road wheel design case study and indicates that AI can be practically incorporated into an end-use product design project. Engineers and industrial designers can jointly review a large number of generated 3D CAD models by using this framework along with the engineering performance results estimated by AI and find conceptual design candidates for the subsequent detailed design stage. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.4271/2021-26-0347,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85116837510&origin=inward,Conference Paper,SCOPUS_ID:85116837510,scopus,2021-09-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),model order reduction technique to aid control system design,"
AbstractView references

Design of real time active controls for structural dynamics problems requires a very precise mathematical model, to closely determine the system dynamic behavior, under virtual simulation. The finite element models can somehow be used as a mathematical model but due to complex shape/structure of the component, the size of discrete models resulting from finite element analysis is usually very large, causing the virtual simulation to be extremely computationally intensive and time consuming, also the boundary conditions applied are not very scalable, making the system deviate from its real dynamic behavior. Thus, this paper deals with the design of a Model Order Reduction technique, using orthogonal decomposition of system matrices, which can be used for creating accurate low-order dynamic model with scalable boundary conditions. The technique works in 3 phases namely, extraction of system matrices from software tools (such as ANSYS), Development of a second order reduced model using object-oriented programming language (such as MATLAB), and Deployment of model in form of state space matrices for model-based Design software (such as Simulink) This paper presents two case studies, first one done for a simplistic beam structure (cantilever) where the model is reduced using the MOR technique and the results (accelerations/displacements and mode frequencies) are validated with experimental and theoretical results. Second case study is done on an actual automotive component (Handle-bar) under an actual problem statement. © 2021 SAE International. All Rights Reserved.
"
10.1016/j.cose.2021.102372,S0167404821001966,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108874331&origin=inward,Article,SCOPUS_ID:85108874331,scopus,2021-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),phishing websites detection via cnn and multi-head self-attention on imbalanced datasets,"
                  Phishing websites belong to a social engineering attack where perpetrators fake legitimate websites to lure people to access so as to illegally acquire user’s identity, password, privacy and even properties. This attack imposes a great threat to people and becomes more and more severe. In order to identify phishing websites, many proposals have shown their merits. For example, the classical proposal CNN-LSTM received a very high precision by combining Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM) together. However, despite CNN achieved great success in AI area, LSTM still exists the biases issue since it always treats the later features much more important than the former ones. In the meanwhile, as the self-attention mechanism can discover the text’s inner dependency relationships, it has been widely applied to various tasks of deep learning-based Natural Language Processing (NLP). If we treat a URL as a text string, this mechanism can learn comprehensive URL representations. In order to improve the accuracy for phishing websites detection further, in this paper, we propose a novel Convolutional Neural Network (CNN) with self-attention named self-attention CNN for phishing Uniform Resource Locators (URLs) identification. Specifically, self-attention CNN first leverages Generative Adversarial Network (GAN) to generate phishing URLs so as to balance the datasets of legitimate and phishing URLs. Then it utilizes CNN and multi-head self-attention to construct our new classifier which is comprised of four blocks, namely the input block, the attention block, the feature block and the output block. Finally, the trained classifier can give a high-accuracy result for an unknown website URL. Overall thorough experiments indicate that self-attention CNN achieves 95.6% accuracy, which outperforms CNN-LSTM, single CNN and single LSTM by 1.4%, 4.6% and 2.1% respectively.
               "
10.1145/3468264.3473494,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85116200524&origin=inward,Conference Paper,SCOPUS_ID:85116200524,scopus,2021-08-20,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),does reusing pre-trained nlp model propagate bugs?,"
AbstractView references

In this digital era, the textual content has become a seemingly ubiquitous part of our life. Natural Language Processing (NLP) empowers machines to comprehend the intricacies of textual data and eases human-computer interaction. Advancement in language modeling, continual learning, availability of a large amount of linguistic data, and large-scale computational power have made it feasible to train models for downstream tasks related to text analysis, including safety-critical ones, e.g., medical, airlines, etc. Compared to other deep learning (DL) models, NLP-based models are widely reused for various tasks. However, the reuse of pre-trained models in a new setting is still a complex task due to the limitations of the training dataset, model structure, specification, usage, etc. With this motivation, we study BERT, a vastly used language model (LM), from the direction of reusing in the code. We mined 80 posts from Stack Overflow related to BERT and found 4 types of bugs observed in clients' code. Our results show that 13.75% are fairness, 28.75% are parameter, 15% are token, and 16.25% are version-related bugs. © 2021 Owner/Author.
"
10.1109/RTCSA52859.2021.00034,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85116649701&origin=inward,Conference Paper,SCOPUS_ID:85116649701,scopus,2021-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),work-in-progress abstract: revealing and analyzing architectural models in open-source ardupilot,"
AbstractView references

Building robust software can be considered a major challenge in current software engineering processes. This task is especially relevant for the code of cyber-physical systems (CPS) that interact with tangible data of the environment and make decisions that have an impact on the real world. The study of good practices of the architectural organization of such software systems is suitable to conduct on solutions with open-source code, which are developed by large communities of enthusiasts. Such a code bears a long history and has been tested many times on real devices in a real-world environment. The construction of various models using the program code allows us to understand stable architectural solutions, to present them in a graphical form; these solutions can be used in STEM centers when designing other systems, taking into account all the achievements of the communities. In addition, it is possible to propose methods for analyzing models to prove various properties of cyber-physical systems. In this paper, we analyze ArduPilot Mega (APM), an Arduino-compatible solution for building DIY driving and flying systems. The solution is based on a specially designed board with a controller and necessary peripherals, as well as a firmware code in a C++ -compatible dialect. Since there are many limitations associated with hardware, it is advisable to carry out a so-called co-modeling, taking into account both hardware and software sides. We consider modeling the interaction of equipment on connected pins and data transmission buses, the software part in the form of a class diagram for the solution. We then describe methods for analyzing the interactions between tasks running on the system through shared variables and evaluating the performance of the task scheduler. © 2021 IEEE.
"
10.1016/j.sysarc.2021.102089,S1383762121000746,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103381131&origin=inward,Article,SCOPUS_ID:85103381131,scopus,2021-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),robmex: ros-based modelling framework for end-users and experts,"Autonomous vehicles, such as drones, are gaining great popularity due to their usability and versatility. Nowadays, a significant number of them operate using open source software, such as Robot Operating System (ROS) and the de facto standard MAVLink communication protocol, as they are free and many of them are reusable enough that they can be deployed in various different vehicles. Although these technologies offer a wide variety of resources, using them requires a reasonable background of programming and system engineering. Often, this is not achievable by common drone end-users in the short-term, as they would need to acquire a considerably large amount of know-how before working on specific domains. However, a graphical Domain Specific Modelling Language (DSML) might provide a shortcut to design drone missions using already known concepts to the end-users (or, at least, ones easier to learn). Pursuing this shortcut, RoBMEX is presented as a top-down methodology based on a set of domain specific languages able to enhance the autonomy of ROS-based systems, by allowing the creation of missions graphically, and then generating automatically executable source codes conforming to the designed missions."
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124503630&origin=inward,Conference Paper,SCOPUS_ID:85124503630,scopus,2021-07-26,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a pilot study investigating stem learners' ability to decipher ai-generated video,"
AbstractView references

Artificial intelligence (AI) techniques such as Generative Neural Networks (GNNs) have resulted in remarkable breakthroughs such as the generation of hyper-realistic images, 3D geometries, and textual data. This work investigates the vulnerability of science, technology, engineering, and mathematics (STEM) learners to AI-generated misinformation in order to safeguard the public-availability of high-quality online STEM learning content. The COVID-19 pandemic has increased STEM learners' reliance on online learning content. Consequently, safeguarding the veracity of STEM learning content is critical to ensuring the safety and trust that both STEM educators and learners have in publicly-available STEM learning content. In this study, state-of-the-art AI algorithms are trained on a specific STEM context (i.e., climate change) using publicly-available data. STEM learners are then randomly presented with authentic and AI-manipulated STEM learning content and asked to judge the authenticity of the content. The authors introduce an approach that STEM educators can employ to understand correlations between STEM learning topics such as climate change, and students' susceptibility to AI-driven misinformation. The proposed approach has the potential to guide STEM educators as to the STEM topics that may be more difficult to teach (e.g., climate change), given students' susceptibility to AI-driven misinformation that promotes controversial viewpoints. In addition, the proposed approach may inform students themselves as to their susceptibility to AI-driven STEM misinformation so that they are more aware of AI's capabilities and how they could be utilized to alter their viewpoints on a STEM topic. © American Society for Engineering Education, 2021
"
10.23919/ANNSIM52504.2021.9552063,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85117367096&origin=inward,Conference Paper,SCOPUS_ID:85117367096,scopus,2021-07-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a tutorial introduction to colored petri nets framework for model-driven system design and engineering,"
AbstractView references

Colored Petri Nets (CPNs) extend the vocabulary of ordinary Petri Nets and add features that make them suitable for modeling large systems. CPNs combine the strengths of ordinary Petri Nets with the strengths of a high-level programming language. Petri Nets provide the primitives for process interaction, while the programming language provides the primitives for the definition of data types and the manipulations of data values. CPNs and the associated integrated development environment, CPN Tools, have been designed and developed with practical applications and ease of use in mind. This paper introduces the audience to the basic concepts of CPNs as well as CPN Tools. We illustrate the key ideas, underlying concepts, software tools, and modeling techniques, by means of numerous real-life examples that emphasize practical applications of CPNs and CPN Tools. Readers need no prior familiarity with Petri nets, system design and analysis, modeling, simulation, or any particular computer language. Our examples include the approach and use of CPN Tools for building and executing hierarchical CPN models, which is useful in the context of model-driven systems engineering for large systems. Our goal is to introduce these topics and encourage the reader to investigate the modeling approach and tools themselves. © 2021 SCS.
"
10.1145/3481127.3481258,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85120916380&origin=inward,Conference Paper,SCOPUS_ID:85120916380,scopus,2021-07-17,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design and implementation of the e-mall management system based on ssm,"
AbstractView references

The electronic mall is an electronic trading platform for large commodities built with e-commerce software. Its main function is to sell products to customers accurately and quickly through this platform. Online Marketing has become an irreplaceable and important means in product sales. This article designs and implements an electronic mall management system. It integrates functions such as product classification, product browsing, shopping cart function, order management, and announcement management. The system provides services for registered members, non-registered members, product managers, system administrators, and big data analysts. By using Spring framework's AOP (Aspect-Oriented Programming) features, Inversion of Control (Inversion of Control) features, Spring MVC model and MyBatis ORM features, using B/S architecture and MySQL database, this paper implements the system. This programming has high reconfigurability. This system provides functions such as product information management, user information management, shopping cart order management, announcement management, system log, and user authority management. At the same time, it is ensured that these functions will not be illegally operated by users without authority, so that the system has higher security. The system has now been deployed and used to meet the needs of registered members, product managers, system administrators, big data analysts and other users. This system provides an integrated solution for Online Marketing, which has high practical and commercial value. © 2021 ACM.
"
10.1145/3404835.3462942,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111696964&origin=inward,Conference Paper,SCOPUS_ID:85111696964,scopus,2021-07-11,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),answer complex questions: path ranker is all you need,"
AbstractView references

Currently, the most popular method for open-domain Question Answering (QA) adopts ""Retriever and Reader""pipeline, where the retriever extracts a list of candidate documents from a large set of documents followed by a ranker to rank the most relevant documents and the reader extracts answer from the candidates. Existing studies take the greedy strategy in the sense that they only use samples for ranking at the current hop, and ignore the global information across the whole documents. In this paper, we propose a purely rank-based framework Thinking Path Re-Ranker (TPRR), which is comprised of Thinking Path Ranker (TPR) for generating document sequences called ""a path""and External Path Reranker (EPR) for selecting the best path from candidate paths generated by TPR. Specifically, TPR leverages the scores of a dense model and conditional probabilities to score the full paths. Moreover, to further enhance the performance of the dense ranker in the iterative training, we propose a ""thinking""negatives selection method that the top-K candidates treated as negatives in the current hop are adjusted dynamically through supervised signals. After achieving multiple supporting paths through TPR, the EPR component which integrates several fine-grained training tasks for QA is used to select the best path for answer extraction. We have tested our proposed solution on the multi-hop dataset ""HotpotQA""with a full wiki set ting, and the results show that TPRR significantly outperforms the existing state-of-the-art models. Moreover, our method has won the first place in the HotpotQA official leaderboard since Feb 1, 2021 under the Fullwiki setting. Code is available at https://gitee.com/mindspore/mindspore/ tree/master/model_zoo/research/nlp/tprr. © 2021 ACM.
"
10.1063/5.0058211,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85109368543&origin=inward,Conference Paper,SCOPUS_ID:85109368543,scopus,2021-07-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),constructing and evaluating model for teaching grammar,"
AbstractView references

Encouraging to learn a sentence structure has got a great deal of basic consideration from educators. Sentence structure dominance is seen as a deciding angle to impart precisely in a composed structure, yet they frequently experience issues to procure it. Understudies may have essential information on principles or of sentence structures, yet a large portion of people despite everything commit errors just as blunders in their ensuing composition inspite of the fact that instructors have given satisfactory criticism. This idea pulls in the essayists to propose an elective model, called Create and Evaluate Model (CEM). Considerably, which involves five significant advances; forming sentences, examine audit, self-survey, educator's input and finishing up the idea. It is attested that CEM is one of the informative routes in constructing sentence structure that encourages to learn it adequately but additionally help them in building their own understanding, utilizing their insight in a worthy setting and building up their composing aptitude. The different language structure exercises utilizing CEM empowers them to have an option of people to coordinate the idea of punctuation into composing, become intuitive and community students, and hone high-request thinking abilities. This paper indicates that CEM properly applied in a language structure builds scores in punctuation tests and learning contribution. In such manner, the present article is planned to outline the idea and hypothetical system in constructing punctuation utilizing CEM, five significant strides to execute it, model, and its advantages. © 2021 Author(s).
"
10.1016/j.softx.2021.100711,S235271102100056X,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107075116&origin=inward,Article,SCOPUS_ID:85107075116,scopus,2021-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cnerator: a python application for the controlled stochastic generation of standard c source code,"The Big Code and Mining Software Repositories research lines analyze large amounts of source code to improve software engineering practices. Massive codebases are used to train machine learning models aimed at improving the software development process. One example is decompilation, where C code and its compiled binaries can be used to train machine learning models to improve decompilation. However, obtaining massive codebases of portable C code is not an easy task, since most applications use particular libraries, operating systems, or language extensions. In this paper, we present Cnerator, a Python application that provides the stochastic generation of large amounts of standard C code. It is highly configurable, allowing the user to specify the probability distributions of each language construct, properties of the generated code, and post-processing modifications of the output programs. Cnerator has been successfully used to generate code that, utilized to train machine learning models, has improved the performance of existing decompilers. It has also been used in the implementation of an infrastructure for the automatic extraction of code patterns."
10.1016/j.jmsy.2021.05.004,S0278612521001011,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85106942746&origin=inward,Article,SCOPUS_ID:85106942746,scopus,2021-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),construction method of shop-floor digital twin based on mbse,"
                  Digital twin (DT) technology is essential for achieving the fusion of virtual-real cyber-physical systems. Academics and companies have made great strides in the theoretical research and case studies of constructing the shop-floor digital twin (SDT), which is the premise of applying DT technology on the shop floor. A shop floor is a large complex system that involves many elements including people, machines, materials, methods, and the environment and processes, such as the technical flow, business process, logistics, and control flow. However, most of the developed cases lack a hierarchical, structured and modularized implementation framework for the development of an SDT system, which leads to problems such as a low reuse rate of the system blocks, lack of scalability, and high upgrade and maintenance costs. In response to these issues, we propose a construction method of the DT for the shop floor based on model-based systems engineering from the perspective of the system. In this method, a comprehensive DT model for the shop floor is gradually constructed by using system modeling language, the modeling method “MagicGrid,” and the “V model” of systems engineering. The model includes four dimensions of the shop-floor requirements, structure, behavior, and parameters, as well as three stages (the problem domain, solution domain, and implementation domain), and connects nine steps of the “V model,” including the system requirements, system architecture, subsystem implementation, subsystem integration, and system verification. Then, based on an example of a real NC machining shop floor, subsystems including a visualization system, synchronization system, and simulation system, are discussed. Finally, the functions of the integrated systems are verified based on the requirements, including the real-time synchronization of “man, machine, material, and method” and the transient simulation in real time. The numerical indicators of the integrated system are verified, including the model completeness and synchronization timeliness.
               "
10.1109/JCSSE53117.2021.9493829,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112351034&origin=inward,Conference Paper,SCOPUS_ID:85112351034,scopus,2021-06-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),classification of abusive thai language content in social media using deep learning,"
AbstractView references

This paper presents binomial and multinomial models for Thai language abusive speech classification in social media. While previous similar research focused on using traditional machine learning models for binomial classification, we showed that deep learning models have better performance. Our binomial and multinomial models achieved F1 scores of 0.8510 and 0.9067, respectively. These scores were significantly better than the machine learning models' respective best F1 scores of 0.7452 and 0.8090. While the bidirectional LSTM performed well, the DistilBERT had higher accuracy and recall. Moreover, the recall was especially higher for the 'figurative' class where certain words were more likely to have different meanings depending on context. © 2021 IEEE.
"
10.1109/ICAICA52286.2021.9498168,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85114556260&origin=inward,Conference Paper,SCOPUS_ID:85114556260,scopus,2021-06-28,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),computer agile development system with cluster computing architecture,"
AbstractView references

The traditional development of the waterfall type is a document-driven development method that requires detailed project planning and documentation. The deployment process, QA testing, and delivery process are rigorous. Not only does the integration cycle take a long time, but also the development cost is high and maintenance is not easy. The software architecture is a specialized advanced course offered by the Software Academy for Master of Engineering. By learning this course, students can master the architectural design knowledge in software project development and understand how a variety of non-functional requirements of a software system can be met. In this paper, we study the software agile development system based on cluster computing. The cluster model can deal with the challenges of the large data requirement, therefore, the processing efficiency will be improved. The proposed model uses the parallel computing model to construct the scenario of the cloud based software development pattern. Numerical and the theoretical verification is provided in the end to prove the performance of the model. © 2021 IEEE.
"
10.1109/SERP4IoT52556.2021.00008,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115161205&origin=inward,Conference Paper,SCOPUS_ID:85115161205,scopus,2021-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),model-driven development for esp-based iot systems,"
AbstractView references

The large variety of low-end devices and their programming environment increases the development complexity of embedded software for the Internet of Things. Therefore, developing IoT-based systems has been found to be a complex process. As a consequence, the development of these systems becomes more error-prone, time-consuming and costly. ESP (ESP8266 and ESP32) is one of the preferred micro-controllers for education, industrial projects, and prototyping. Arduino, a well-known firmware, is used in ESP which makes it easy-to-use. However, learning the device-specific configurations, using network features, and implementing IoT applications for ESP on the Arduino platform are still burdensome and time-consuming tasks. Raising the abstraction level can help to decrease the complexity and address this problem. To this end, in this paper, a model-driven approach is proposed for the development of Arduino-based programmable ESP micro-controllers. First, a meta-model is designed for the Arduino-based ESP micro-controllers. Based on this meta-model, a Domain-specific Language (DSL) is developed to graphically represent the domain models. To gain more functionality for the language, domain rules are defined as constraints. Also, partial system codes are automatically generated from the instance models. In this way, the development of IoT systems based on ESP modules is supported by code synthesis which increases the performance and reduces the number of errors. Finally, a motion-sensitive thief detector and a servo-motor based room temperature control system are implemented to evaluate the proposed DSL. © 2021 IEEE.
"
10.3390/COMPUTERS10060082,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85110213211&origin=inward,Article,SCOPUS_ID:85110213211,scopus,2021-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),uncertainty-aware deep learning-based cardiac arrhythmias classification model of electrocardiogram signals,"
AbstractView references

Deep Learning-based methods have emerged to be one of the most effective and practical solutions in a wide range of medical problems, including the diagnosis of cardiac arrhythmias. A critical step to a precocious diagnosis in many heart dysfunctions diseases starts with the accurate detection and classification of cardiac arrhythmias, which can be achieved via electrocardiograms (ECGs). Motivated by the desire to enhance conventional clinical methods in diagnosing cardiac arrhythmias, we introduce an uncertainty-aware deep learning-based predictive model design for accurate large-scale classification of cardiac arrhythmias successfully trained and evaluated using three benchmark medical datasets. In addition, considering that the quantification of uncertainty estimates is vital for clinical decision-making, our method incorporates a probabilistic approach to capture the model’s uncertainty using a Bayesian-based approximation method without introducing additional parameters or significant changes to the network’s architecture. Although many arrhythmias classification solutions with various ECG feature engineering techniques have been reported in the literature, the introduced AI-based probabilistic-enabled method in this paper outperforms the results of existing methods in outstanding multiclass classification results that manifest F1 scores of 98.62% and 96.73% with (MIT-BIH) dataset of 20 annotations, and 99.23% and 96.94% with (INCART) dataset of eight annotations, and 97.25% and 96.73% with (BIDMC) dataset of six annotations, for the deep ensemble and probabilistic mode, respectively. We demonstrate our method’s high-performing and statistical reliability results in numerical experiments on the language modeling using the gating mechanism of Recurrent Neural Networks. © 2021 by the author.
"
10.5381/JOT.2021.20.3.A10,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85109453358&origin=inward,Article,SCOPUS_ID:85109453358,scopus,2021-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automating model transformations for railway systems engineering,"
AbstractView references

Model-Based Systems Engineering (MBSE) enables system development and analysis on a suitable level of abstraction. In the context of railway systems engineering, system verification is of major importance as software failures can cause serious damage. At DB Netz AG, a railway infrastructure manager that operates large parts of the German railway system, the challenge of enabling both high-level system modelling and formal system verification is addressed by employing SysML, a widespread systems modelling language, and Event-B, a formal systems modelling language particularly suited for automated system verification. In the currently applied completely manual development process, engineers (i) create models using SysML, (ii) translate relevant parts of these models to Event-B for verification, (iii) possibly improve the Event-B models based on verification results, and finally (iv) reflect these improvements in the original SysML models. This process is both tedious and error-prone, clearly indicating a need for an increase in the level of automation. In this paper, we argue that steps (ii) and (iv) can be viewed as a coupled forward transformation and a backward synchronisation, respectively, as the SysML models cannot be completely reconstructed from their Event-B counterparts. Exploiting this observation, we demonstrate that steps (ii) and (iv) can be suitably automated using a bidirectional transformation (bx) language. With Triple Graph Grammars (TGGs) as a rule-based bx language, we establish a tool chain connecting the modelling tools used at DB Netz AG for SysML and Event-B. We show the feasibility of our automation solution by solving three representative case studies provided by DB Netz AG. Based on these case studies, we conduct a qualitative evaluation via semi-structured interviews with domain experts. © 2021. All Rights Reserved.
"
10.1016/j.compeleceng.2021.107195,S0045790621001932,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105359785&origin=inward,Article,SCOPUS_ID:85105359785,scopus,2021-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative adversarial networks based remaining useful life estimation for iiot,"
                  Artificial intelligence (AI) and Predictive Maintenance (PdM) become productive using IIoT-data with zero-downtime for maintenance in industries by estimating the remaining useful life (RUL). Most reported works consider training data availability with an equal number of normal and fault samples concerning different machine health conditions. However, practical scenarios have to deal with fault-data unavailability, resulting in an imbalanced training dataset. This problem can lead to inaccuracies with missed fault-prediction in RUL estimation approaches. This paper proposes a novel prognostics framework based on conditional generative adversarial network (CGAN) and deep gated recurrent unit (DGRU) network. The framework can generate multi-variate fault instances, solve data imbalance, and predict the RUL of complex systems with the least latency. We observed that the learning of fault samples using underlying noise distribution, data augmentation, and training DGRU improves the RUL prediction accuracy by at least 15% compared to reported imbalanced work on the C-MAPSS dataset.
               "
10.1002/mp.14849,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103911220&origin=inward,Article,SCOPUS_ID:85103911220,scopus,2021-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a roadmap for research in medical physics via academic medical centers: the divert model,"
AbstractView references

The field of medical physics has struggled with the role of research in recent years, as professional interests have dominated its growth toward clinical service. This article focuses on the subset of medical physics programs within academic medical centers and how a refocused academic mission within these centers should drive and support Discovery and Invention with Ventures and Engineering for Research Translation (DIVERT). A roadmap to a DIVERT-based scholarly research program is discussed here around the core building blocks of: (a) creativity in research and team building, (b) improved quality metrics to assess activity, (c) strategic partnerships and spinoff directions that extend capabilities, and (d) future directions driven by faculty-led initiatives. Within academia, it is the unique discoveries and inventions of faculty that lead to their recognition as scholars, and leads to financial support for their research programs and reconition of their intellectual contributions. Innovation must also be coupled to translation to demonstrate outcome successes. These ingredients are critical for research funding, and the two-decade growth in biomedical engineering research funding is an illustration of this, where technology invention has been the goal. This record can be contrasted with flat funding within radiation oncology and radiology, where a growing fraction of research is more procedure-based. However, some centers are leading the change of the definition of medical physics, by the inclusion or assimilation of researchers in fields such as biomedical engineering, machine learning, or data science, thereby widening the scope for new discoveries and inventions. New approaches to the assessment of research quality can help realize this model, revisiting the measures of success and impact. While research partnerships with large industry are productive, newer efforts that foster enterprise startups are changing how institutions see the benefits of the connection between academic innovation and affiliated startup company formation. This innovation-to-enterprise focus can help to cultivate a broader bandwidth of donor-to-investor networks. There are many predictions on future directions in medical physics, yet the actual inventive and discovery steps come from individual research faculty creativity. All success through a DIVERT model requires that faculty-led initiatives span the gap from invention to translation, with support from institutional leadership at all steps in the process. Institutional investment in faculty through endowments or clinical revenues will likely need to increase in the coming years due to the relative decreasing size of grants. Yet, radiology and radiation oncology are both high-revenue, translational fields, with the capacity to synergistically support clinical and research operations through large infrastructures that are mutually beneficial. These roadmap principles can provide a pathway for committed academic medical physics programs in scholarly leadership that will preserve medical physics as an active part of university academics. © 2021 American Association of Physicists in Medicine
"
10.1109/TNNLS.2020.3008037,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100259165&origin=inward,Article,SCOPUS_ID:85100259165,scopus,2021-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),hierarchical human-like deep neural networks for abstractive text summarization,"
AbstractView references

Developing an abstractive text summarization (ATS) system that is capable of generating concise, appropriate, and plausible summaries for the source documents is a long-term goal of artificial intelligence (AI). Recent advances in ATS are overwhelmingly contributed by deep learning techniques, which have taken the state-of-the-art of ATS to a new level. Despite the significant success of previous methods, generating high-quality and human-like abstractive summaries remains a challenge in practice. The human reading cognition, which is essential for reading comprehension and logical thinking, is still relatively new territory and underexplored in deep neural networks. In this article, we propose a novel Hierarchical Human-like deep neural network for ATS (HH-ATS), inspired by the process of how humans comprehend an article and write the corresponding summary. Specifically, HH-ATS is composed of three primary components (i.e., a knowledge-aware hierarchical attention module, a multitask learning module, and a dual discriminator generative adversarial network), which mimic the three stages of human reading cognition (i.e., rough reading, active reading, and postediting). Experimental results on two benchmark data sets (CNN/Daily Mail and Gigaword) demonstrate that HH-ATS consistently and substantially outperforms the compared methods. © 2012 IEEE.
"
10.1007/s11704-020-8426-4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100039497&origin=inward,Article,SCOPUS_ID:85100039497,scopus,2021-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an integrated pipeline model for biomedical entity alignment,"
AbstractView references

Biomedical entity alignment, composed of two sub-tasks: entity identification and entity-concept mapping, is of great research value in biomedical text mining while these techniques are widely used for name entity standardization, information retrieval, knowledge acquisition and ontology construction. Previous works made many efforts on feature engineering to employ feature-based models for entity identification and alignment. However, the models depended on subjective feature selection may suffer error propagation and are not able to utilize the hidden information. With rapid development in health-related research, researchers need an effective method to explore the large amount of available biomedical literatures. Therefore, we propose a two-stage entity alignment process, biomedical entity exploring model, to identify biomedical entities and align them to the knowledge base interactively. The model aims to automatically obtain semantic information for extracting biomedical entities and mining semantic relations through the standard biomedical knowledge base. The experiments show that the proposed method achieves better performance on entity alignment. The proposed model dramatically improves the F1 scores of the task by about 4.5% in entity identification and 2.5% in entity-concept mapping. © 2020, Higher Education Press.
"
10.1007/s10586-020-03170-7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089356704&origin=inward,Article,SCOPUS_ID:85089356704,scopus,2021-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),autowm: a novel domain-specific tool for universal multi-/many-core accelerations of the wrf cloud microphysics,"
AbstractView references

In large-scale atmospheric simulations, microphysics parameterization often takes a large portion of simulation time and usually consists of dozens of parameterization schemes. Performance optimizing these schemes one by one on different hardware platforms is tedious and error-prone even for skilled programmers. In this work, we propose AutoWM, a novel domain-specific tool for universal performance accelerations of the famous weather research and forecasting model (WRF) microphysics on multi-/many-core systems. The main idea of AutoWM is to reconstruct various schemes into compositions of common building blocks and optimize these building blocks instead of the schemes on target platforms for reusing. To achieve this goal, a light-weight domain-specific language, WML, is provided to describe different microphysics schemes so that the workflow information can be parsed and extracted easily. Experiments on the popular WRF single/double moments microphysics schemes show that AutoWM can automatically generate well optimized microphysics kernels on three multi- and many-core platforms including Intel Ivy Bridge, Intel Xeon Phi and Chinese homegrown SW26010, with the average floating-point efficiency reaching 47 % , 20 % and 10 % of the theoretical peak performance, respectively. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.
"
10.1145/3411763.3450391,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105820624&origin=inward,Conference Paper,SCOPUS_ID:85105820624,scopus,2021-05-08,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),buncho: ai supported story co-creation via unsupervised multitask learning to increasewriters' creativity in japanese,"
AbstractView references

Co-creation with artificial intelligence (AI) is an upcoming trend. However, less attention has been given to the construction of systems for Japanese novelists. In this study, we built ""BunCho"", an AI supported story co-creation system in Japanese. BunCho's AI is GPT-2 (an unsupervised multitask language model) trained using a large-scale dataset of Japanese web texts and novels. With BunCho, users can generate titles and synopses from keywords. Furthermore, we propose an interactive story co-creation AI system as a tabletop role-playing game. According to summative studies of writers (N=16) and readers (N=32), 69% writers enjoyed writing synopses with BunCho more than by themselves, and at least one of five common metrics were improved at objective evaluation, including creativity. In addition, 63% writers indicated that BunCho broadened their stories. BunCho showed paths to assist Japanese novelists in creating high-level and creative writing. © 2021 Owner/Author.
"
10.1145/3411763.3451720,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105794767&origin=inward,Conference Paper,SCOPUS_ID:85105794767,scopus,2021-05-08,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),learning and practicing logic circuits: development of a mobile-based learning prototype,"
AbstractView references

Nowadays, with the advent of electronic devices in everyday life, mobile devices can be utilized for learning purposes. When designing a mobile-based learning application, a large number of aspects should be taken into account. For the present paper, the following aspects are of special importance: first, it should be considered how to represent information; second, possible interactions between learner and system should be defined; third - and depending on the second aspect - it should be considered how real-time responses can be provided by the system. Moreover, psychological theories as for example the 4C/ID model and findings with respect to blended learning environments should be taken into account. In this paper, a mobile-based learning prototype concerning the learning topic ""logic circuit design""is presented which considers the mentioned aspects to support independent practice. The prototype includes four different representations: (i) code-based (Verilog hardware description language), (ii) graphical-based (gate-level view), (iii) Boolean function, and (iv) truth table for each gate. The proposed learning system divides the learning content into different sections to support independent practice in meaningful steps. Multiple representations are included in order to foster understanding and transfer. The resulting implications for future work are discussed. © 2021 ACM.
"
10.1145/3411764.3445735,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85106737895&origin=inward,Conference Paper,SCOPUS_ID:85106737895,scopus,2021-05-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),planning for natural language failures with the ai playbook,"
AbstractView references

Prototyping AI user experiences is challenging due in part to proba-bilistic AI models making it difcult to anticipate, test, and mitigate AI failures before deployment. In this work, we set out to support practitioners with early AI prototyping, with a focus on natural language (NL)-based technologies. Our interviews with 12 NL prac-titioners from a large technology company revealed that, in addition to challenges prototyping AI, prototyping was often not happen-ing at all or focused only on idealized scenarios due to a lack of tools and tight timelines. These fndings informed our design of the AI Playbook, an interactive and low-cost tool we developed to encourage proactive and systematic consideration of AI errors be-fore deployment. Our evaluation of the AI Playbook demonstrates its potential to 1) encourage product teams to prioritize both ideal and failure scenarios, 2) standardize the articulation of AI failures from a user experience perspective, and 3) act as a boundary object between user experience designers, data scientists, and engineers. © 2021 ACM.
"
10.1145/3411764.3445048,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85106715743&origin=inward,Conference Paper,SCOPUS_ID:85106715743,scopus,2021-05-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),nbsearch: semantic search and visual exploration of computational notebooks,"
AbstractView references

Code search is an important and frequent activity for developers using computational notebooks (e.g., Jupyter). The fexibility of notebooks brings challenges for efective code search, where classic search interfaces for traditional software code may be limited. In this paper, we propose, NBSearch, a novel system that supports semantic code search in notebook collections and interactive visual exploration of search results. NBSearch leverages advanced machine learning models to enable natural language search queries and intuitive visualizations to present complicated intra-and inter-notebook relationships in the returned results. We developed NB-Search through an iterative participatory design process with two experts from a large software company. We evaluated the models with a series of experiments and the whole system with a controlled user study. The results indicate the feasibility of our analytical pipeline and the efectiveness of NBSearch to support code search in large notebook collections. © 2021 ACM.
"
10.1145/3411764.3445226,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85106680425&origin=inward,Conference Paper,SCOPUS_ID:85106680425,scopus,2021-05-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring generative models with middle school students,"
AbstractView references

Applications of generative models such as Generative Adversarial Networks (GANs) have made their way to social media platforms that children frequently interact with. While GANs are associated with ethical implications pertaining to children, such as the generation of Deepfakes, there are negligible eforts to educate middle school children about generative AI. In this work, we present a generative models learning trajectory (LT), educational materials, and interactive activities for young learners with a focus on GANs, creation and application of machine-generated media, and its ethical implications. The activities were deployed in four online workshops with 72 students (grades 5-9).We found that these materials enabled children to gain an understanding of what generative models are, their technical components and potential applications, and benefts and harms, while refecting on their ethical implications. Learning from our fndings, we propose an improved learning trajectory for complex socio-technical systems. © 2021 ACM.
"
10.1145/3411764.3445219,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85106671513&origin=inward,Conference Paper,SCOPUS_ID:85106671513,scopus,2021-05-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai as social glue: uncovering the roles of deep generative ai during social music composition,"
AbstractView references

Recent advances in deep generative neural networks have made it possible for artificial intelligence to actively collaborate with human beings in co-creating novel content (e.g. music, art). While substantial research focuses on (individual) human-AI collaborations, comparatively less research examines how AI can play a role in human-human collaborations during co-creation. In a qualitative lab study, we observed 30 participants (15 pairs) compose a musical phrase in pairs, both with and without AI. Our findings reveal that AI may play important roles in influencing human social dynamics during creativity, including: 1) implicitly seeding a common ground at the start of collaboration, 2) acting as a psychological safety net in creative risk-taking, 3) providing a force for group progress, 4) mitigating interpersonal stalling and friction, and 5) altering users' collaborative and creative roles. This work contributes to the future of generative AI in social creativity by providing implications for how AI could enrich, impede, or alter creative social dynamics in the years to come. © 2021 ACM.
"
10.1109/CloudIntelligence52565.2021.00013,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115846203&origin=inward,Conference Paper,SCOPUS_ID:85115846203,scopus,2021-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),robust and transferable anomaly detection in log data using pre-trained language models,"
AbstractView references

Anomalies or failures in large computer systems, such as the cloud, have an impact on a large number of users that communicate, compute, and store information. Therefore, timely and accurate anomaly detection is necessary for reliability, security, safe operation, and mitigation of losses in these increasingly important systems. Recently, the evolution of the software industry opens up several problems that need to be tackled including (1) addressing the software evolution due software upgrades, and (2) solving the cold-start problem, where data from the system of interest is not available. In this paper, we propose a framework for anomaly detection in log data, as a major troubleshooting source of system information. To that end, we utilize pre-trained general-purpose language models to preserve the semantics of log messages and map them into log vector embeddings. The key idea is that these representations for the logs are robust and less invariant to changes in the logs, and therefore, result in a better generalization of the anomaly detection models. We perform several experiments on a cloud dataset evaluating different language models for obtaining numerical log representations such as BERT, GPT-2, and XL. The robustness is evaluated by gradually altering log messages, to simulate a change in semantics. Our results show that the proposed approach achieves high performance and robustness, which opens up possibilities for future research in this direction. © 2021 IEEE.
"
10.1109/ICSE43902.2021.00041,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85113421369&origin=inward,Conference Paper,SCOPUS_ID:85113421369,scopus,2021-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),studying the usage of text-to-text transfer transformer to support code-related tasks,"
AbstractView references

Deep learning (DL) techniques are gaining more and more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies in the Natural Language Processing (NLP) field have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of NLP tasks. The basic idea behind T5 is to first pre-train a model on a large and generic dataset using a self-supervised task (e.g., filling masked words in sentences). Once the model is pre-trained, it is fine-tuned on smaller and specialized datasets, each one related to a specific task (e.g., language translation, sentence classification). In this paper, we empirically investigate how the T5 model performs when pre-trained and fine-tuned to support code-related tasks. We pre-train a T5 model on a dataset composed of natural language English text and source code. Then, we fine-tune such a model by reusing datasets used in four previous works that used DL techniques to: (i) fix bugs, (ii) inject code mutants, (iii) generate assert statements, and (iv) generate code comments. We compared the performance of this single model with the results reported in the four original papers proposing DL-based solutions for those four tasks. We show that our T5 model, exploiting additional data for the self-supervised pre-training phase, can achieve performance improvements over the four baselines. © 2021 IEEE.
"
10.1109/ICSE43902.2021.00109,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85109572803&origin=inward,Conference Paper,SCOPUS_ID:85109572803,scopus,2021-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),infercode: self-supervised learning of code representations by predicting subtrees,"
AbstractView references

Learning code representations has found many uses in software engineering, such as code classification, code search, comment generation, and bug prediction, etc. Although representations of code in tokens, syntax trees, dependency graphs, paths in trees, or the combinations of their variants have been proposed, existing learning techniques have a major limitation that these models are often trained on datasets labeled for specific downstream tasks, and as such the code representations may not be suitable for other tasks. Even though some techniques generate representations from unlabeled code, they are far from being satisfactory when applied to the downstream tasks. To overcome the limitation, this paper proposes InferCode, which adapts the self-supervised learning idea from natural language processing to the abstract syntax trees (ASTs) of code. The novelty lies in the training of code representations by predicting subtrees automatically identified from the contexts of ASTs. With InferCode, subtrees in ASTs are treated as the labels for training the code representations without any human labelling effort or the overhead of expensive graph construction, and the trained representations are no longer tied to any specific downstream tasks or code units. We have trained an instance of InferCode model using Tree-Based Convolutional Neural Network (TBCNN) as the encoder of a large set of Java code. This pre-trained model can then be applied to downstream unsupervised tasks such as code clustering, code clone detection, cross-language code search, or be reused under a transfer learning scheme to continue training the model weights for supervised tasks such as code classification and method name prediction. Compared to prior techniques applied to the same downstream tasks, such as code2vec, code2seq, ASTNN, using our pre-trained InferCode model higher performance is achieved with a significant margin for most of the tasks, including those involving different programming languages. The implementation of InferCode and the trained embeddings are available at the link: https://github.com/bdqnghi/infercode. © 2021 IEEE.
"
10.1016/j.is.2019.101461,S0306437919305137,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075419202&origin=inward,Article,SCOPUS_ID:85075419202,scopus,2021-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a dsl for wsn software components coordination,"
                  Wireless Sensor Networks (WSNs) have become an integral part of urban scenarios. They are usually composed of a large number of devices. Developing systems for such networks is a hard task and often involves validation on simulation environments before deployment on real settings. Component-based development allows systems to be built from reusable, existing components that share a common interface. This paper proposes a domain specific language (DSL) for coordination of WSN software components. The language provides high-level composition primitives to promote a flexible coordination execution flow and interaction between them. We present the language specification as well as a case study of an in-network WSN data storage coordination. The current specification of the language generates code for the NS2 simulation environment. The case study shows that the language implements a flexible development model. Moreover, we analyze the code reusability promoted by the language and show that it reduces the programming effort in a component-based development framework.
               "
10.1109/EDUCON46332.2021.9453889,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112419714&origin=inward,Conference Paper,SCOPUS_ID:85112419714,scopus,2021-04-21,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),revisit of automated marking techniques for programming assignments,"
AbstractView references

Due to the popularity of the Computer science field many students study programming. With large numbers of student enrollments in undergraduate courses, assessing programming submissions is becoming an increasingly tedious task that requires high cognitive load, and considerable amount of time and effort. Programming assignments usually contain algorithmic implementations written in specific programming languages to assess students' logical thinking and problem-solving skills. Evaluators use either a test case-driven or source code analysis approach when evaluating programming assignments. Given that many marking rubrics and evaluation criteria provide partial marks for programs that are not syntactically correct, evaluators are required to analyze the source code during evaluations. This extra step adds additional burden on evaluators that consumes more time and effort. Hence, this research work attempts to study existing automatic source code analysis mechanisms, specifically, use of deep learning approaches in the domain of automatic assessments. Such knowledge may lead to creating novel automated marking models using past student data and apply deep learning techniques to implement automatic assessments of programming assignments irrespective of the computer language or the algorithm implemented. © 2021 IEEE.
"
10.1145/3460824.3460826,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122640265&origin=inward,Conference Paper,SCOPUS_ID:85122640265,scopus,2021-04-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the language of engineering: training a domain-specific word embedding model for engineering,"
AbstractView references

Since the introduction of Word2Vec in 2013, so-called word embeddings, dense vector representation of words that are supposed to capture their semantic meaning, have become a universally applied technique in a wide range of Natural Language Processing (NLP) tasks and domains. The vector representations they provide are learned on huge corpora of unlabeled text data. Due to the large amount of data and computing power that is necessary to train such embedding models, very often, pre-trained models are applied which have been trained on domain unspecific data like newspaper articles or Wikipedia entries. In this paper, we present a domain-specific embedding model that is trained exclusively on texts from the domain of engineering. We will show that such a domain-specific embeddings model performs better in different NLP tasks and can therefore help to improve NLP-based AI in the domain of Engineering. © 2021 ACM.
"
10.3390/en14082337,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85106227347&origin=inward,Article,SCOPUS_ID:85106227347,scopus,2021-04-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),virtual digital substation test system and interoperability assessments,"
AbstractView references

Interoperability testing and analysis tools provide a means for achieving and assuring the integrity of multivendor intelligent electronic devices (IEDs) data exchanges. However, the testing and analysis are very time consuming and error prone, and these problems worsen when a substation becomes large and complex during the engineering process, commission, replacement, maintenance, and extension. To address this challenge, this paper presents a virtual digital substation test system (VDSTS) with interoperability analysis tools for assessing and identifying the engineering challenges for the multiple-vendors digital substation. This VDSTS consists of three parts: (i) A virtual digital substation modelling for generating real-time digital substation primary plant operation and fault conditions, (ii) a standard IEC 61850-based substation protection, automation, and control (PAC) system architecture with multivendor IEDs and bay solutions, and (iii) multivendor Substation Configuration description Language (SCL) tools and in-house built data visualisation tool. The study focuses on the interoperability testing of sampled values (SV), generic object-oriented substation events (GOOSE), and manufacturing message specification (MMS) communication services, as defined in IEC 61850. The main issues identified are compatibility issues of SCL tools, protocol implementation issues, different information models, and application limitations. The outcomes will help utilities to reduce the risks associated with the general rollout of digital substations. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
"
10.1115/1.4049722,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105323701&origin=inward,Article,SCOPUS_ID:85105323701,scopus,2021-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),developing and implementing artificial intelligence-based classifier for requirements engineering,"
AbstractView references

In nuclear power plant (NPP) projects, requirements engineering manages the sheer volume of requirements, typically characterized by descriptive and nonharmonized requirements. Large projects may have tens of thousands to hundreds of thousands of requirements to be managed and fulfilled. Two main issues impede requirements analysis: tortuous requirements to be interpreted; and humans’ very limited ability to concentrate on a specific task. It has therefore been recognized that artificial intelligence (AI) algorithms have the potential to support designers’ decision making in classifying and allocating NPP requirements into predefined classes. This paper presents our work on developing an AI-based requirements classifier utilizing natural language processing (NLP) and supervised machine-learning (ML). In addition, the paper presents the integration of the classifier with the requirements management system. The focus is on the classification of nuclear power industry-specific requirements utilizing deep-learning-based NLP. Three classifiers are compared, and the corresponding results are presented. The results include predetermined requirement classes, manually gathered and classified data, a comparison of three models and their classification accuracies, microservice system architecture, and integration of the established classifier with the requirements management system. As the performance of the requirements classifier and related system has been successfully demonstrated, future AI-specific development and studies are suggested to focus on atomizing multiclass requirements, combining similar requirements into one, checking requirements syntax, and utilizing unsupervised learning for clustering. Furthermore, new and advantageous requirement classes and hierarchies are suggested for development while improving current datasets both quantitatively and qualitatively. Copyright © 2021 by ASME
"
10.1109/MMUL.2021.3075705,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105071007&origin=inward,Article,SCOPUS_ID:85105071007,scopus,2021-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sentiment-aware emoji insertion via sequence tagging,"
AbstractView references

Due to the booming popularity of online social networks, emojis have been widely used in online communication. As nonverbal language units, emojis help to convey emotions and express feelings. In this article, we focus on the sentiment-aware emoji insertion task, which predicts multiple emojis and their positions in a sentence conditioned on the plain texts and sentiment polarities. To facilitate future research in this field, we construct a large-scale emoji insertion corpus named MultiEmoji, which contains 420 000 English posts with at least one emoji per post. We formulate the insertion process as a sequence tagging task and apply a BERT-BiLSTM-CRF model to the insertion of emojis. Extensive experiments illustrate that our model outperforms existing methods by a large margin. © 2012 IEEE.
"
10.1145/3427669,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85104202642&origin=inward,Article,SCOPUS_ID:85104202642,scopus,2021-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),transbert: a three-stage pre-training technology for story-ending prediction,"
AbstractView references

Recent advances, such as GPT, BERT, and RoBERTa, have shown success in incorporating a pre-Trained transformer language model and fine-Tuning operations to improve downstream NLP systems. However, this framework still has some fundamental problems in effectively incorporating supervised knowledge from other related tasks. In this study, we investigate a transferable BERT (TransBERT) training framework, which can transfer not only general language knowledge from large-scale unlabeled data but also specific kinds of knowledge from various semantically related supervised tasks, for a target task. Particularly, we propose utilizing three kinds of transfer tasks, including natural language inference, sentiment classification, and next action prediction, to further train BERT based on a pre-Trained model. This enables the model to get a better initialization for the target task. We take story-ending prediction as the target task to conduct experiments. The final results of 96.0% and 95.0% accuracy on two versions of Story Cloze Test datasets dramatically outperform previous state-of-The-Art baseline methods. Several comparative experiments give some helpful suggestions on how to select transfer tasks to improve BERT. Furthermore, experiments on six English and three Chinese datasets show that TransBERT generalizes well to other tasks, languages, and pre-Trained models. © 2021 ACM.
"
10.1109/TGRS.2020.3010441,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103340930&origin=inward,Article,SCOPUS_ID:85103340930,scopus,2021-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rethinking cnn-based pansharpening: guided colorization of panchromatic images via gans,"
AbstractView references

Convolutional neural network (CNN)-based approaches have shown promising results in the pansharpening of the satellite images in recent years. However, they still exhibit limitations in producing high-quality pansharpening outputs. To that end, we propose a new self-supervised learning framework, where we treat pansharpening as a colorization problem, which brings an entirely novel perspective and solution to the problem compared with the existing methods that base their solution solely on producing a super-resolution version of the multispectral image. Whereas the CNN-based methods provide a reduced-resolution panchromatic image as the input to their model along with the reduced-resolution multispectral images and, hence, learn to increase their resolution together, we instead provide the grayscale transformed multispectral image as the input and train our model to learn the colorization of the grayscale input. We further address the fixed downscale ratio assumption during training, which does not generalize well to the full-resolution scenario. We introduce a noise injection into the training by randomly varying the downsampling ratios. Those two critical changes, along with the addition of adversarial training in the proposed PanColorization generative adversarial network (PanColorGAN) framework, help overcome the spatial-detail loss and blur problems that are observed in CNN-based pansharpening. The proposed approach outperforms the previous CNN-based and traditional methods, as demonstrated in our experiments. © 1980-2012 IEEE.
"
10.3390/electronics10070814,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103268004&origin=inward,Article,SCOPUS_ID:85103268004,scopus,2021-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),modular compilation for a hybrid non-causal modelling language,"
AbstractView references

Non-causal modelling is a powerful approach to modelling physical systems in a variety of domains from science and engineering. Non-causal modelling languages enable a high-level and modular approach to modelling. However, it is hard to compile non-causal languages modularly (in the sense of separate compilation). This causes difficulties when simulating large models for which code generation takes a long time, or structurally singular models in which parts of the model are allowed to change at runtime. In this work, we introduce a technique we call order-parametric differentiation to allow truly modular compilation. The idea is to generate (machine) code that can compute derivatives of any order of an expression as needed, thus allowing for ahead-of-time modular compilation of a hybrid non-causal language. We also develop a compilation scheme that enables using partial models as first-class objects in a seamless way and simulating them without the need for just-in-time compilation, even in the presence of structural dynamism. We present a performance evaluation of the scheme we used and study its shortcomings and possible improvements, demonstrating that it is a feasible complement to existing implementation techniques for cases where true modular compilation is a primary objective. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
"
10.1007/s12205-021-0170-2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100535633&origin=inward,Article,SCOPUS_ID:85100535633,scopus,2021-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),advanced stability analysis of the tunnels in jointed rock mass based on tsp and dem,"
AbstractView references

The discrete element method (DEM) can analyze the large deformation and large displacement of rock mass effectively, and it is widely used in underground engineering, slope engineering and other fields. However, due to the low accuracy of rock mass structural surface information acquisition, the application of discrete element method in the analysis of jointed rock mass stability is still deviated. In this paper, combined with the advantages of the tunnel seismic prediction (TSP) in obtaining discontinuous geological interface information and the discrete element method in the calculation and analysis of jointed rock mass stability, this paper proposes an advanced analysis method for jointed rock mass stability based on TSP and DEM. Compared to the traditional methods, the analysis results of the jointed rock mass stability are more reliable. Firstly, relying on the advanced detection system — Tunnel Seismic Prediction 203Plus, the unstructured rock mass structure information of the tunnel is obtained, and the spatial attitude of the discontinuous geological interface is further determined. Secondly, based on the Fish programming language, the non-continuous geological interface information can be expressed in the discrete unit software — 3D Distinct Element Code (3DEC). In this way, the excavation calculation model of the tunnels in jointed rock mass can be constructed. Finally, based on the DEM, the excavation of the tunnels in jointed rock mass can be simulated, analyze the stability of surrounding rock during the tunnel excavation process, and realize the stability analysis of surrounding rock stability of jointed rock mass. Based on the Huangjiazhuang Tunnel Project, this paper uses the above method to carry out on-site application. The results show that the location of the dangerous block is predicted to be consistent with the actual exposure of the tunnel surrounding rock based on TSP and DEM, which verify the accuracy and feasibility of this method, and the research results have practical guiding significance for the safe construction of the tunnels in jointed rock mass. © 2021, Korean Society of Civil Engineers.
"
10.1109/TKDE.2019.2941881,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072528498&origin=inward,Article,SCOPUS_ID:85072528498,scopus,2021-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),position-aware deep character-level ctr prediction for sponsored search,"
AbstractView references

Predicting the click-through rate of an advertisement is a critical component of online advertising platforms. In sponsored search, the click-through rate estimates the probability that a displayed advertisement is clicked by a user after she submits a query to the search engine. Commercial search engines typically rely on machine learning models trained with a large number of features to make such predictions. This inevitably requires a lot of engineering efforts to define, compute, and select the appropriate features. In this paper, we propose two novel approaches (one working at character level and the other working at word level) that use deep convolutional neural networks to predict the click-through rate of a query-advertisement pair. Specifically, the proposed architectures consider as input only the textual content appearing in a query-advertisement pair and the page position at which the advertisement appears on the search result page of the query, and produce as output a click-through rate prediction. By comparing the character-level model with the word-level model, we show that language representation can be learnt from scratch at character level when trained on enough data. Through extensive experiments using billions of query-advertisement pairs of a popular commercial search engine, we demonstrate that both approaches significantly outperform a baseline model built on well-selected text features and a state-of-the-art word2vec-based approach. We also show the importance of the position feature in the proposed approaches in improving the prediction accuracy. When combining the predictions of the deep models introduced in this study with the prediction of the model in production of the same commercial search engine, we significantly improve the accuracy and the calibration of the click-through rate prediction of the production system. We also show the potential of leveraging the CTR prediction of the proposed deep learning models for query-ad relevance modeling and query-ad matching tasks in sponsored search. © 1989-2012 IEEE.
"
10.1145/3412841.3442005,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85104989816&origin=inward,Conference Paper,SCOPUS_ID:85104989816,scopus,2021-03-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sector classification for crowd-based software requirements,"
AbstractView references

Requirements engineering (RE) is the process of defining, documenting, and maintaining software requirements. Crowd-based RE (CrowdRE) involves large scale user participation in requirements engineering tasks. It improves the quality of software requirements and helps in reducing the cost. Manual extraction of useful insights from a large body of unstructured, and noisy natural language data produced during CrowdRE is an expensive, error prone and time consuming task. Thus, automated techniques are required for processing the CrowdRE data. We focus on the problem of automatic classification of crowd-based software requirements into sectors. We propose three different approaches for sector classification of crowd-based software requirements. These approaches are based on supervised machine learning (ML) models, neural networks, and bidirectional encoder representations from transformers (BERT), respectively. We apply our classification approaches to a large-sized requirements document, i.e. a CrowdRE dataset with around 3000 crowd-generated requirements for smart home applications. To evaluate the quality of our classification algorithms we use the publicly available ground truth data for computing precision, recall, and F-score. We compare the performance of several classification algorithms and our detailed experiments indicate that these algorithms can be very useful for categorizing crowd-based requirements into sectors. © 2021 ACM.
"
10.1109/AEMCSE51986.2021.00217,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85114040527&origin=inward,Conference Paper,SCOPUS_ID:85114040527,scopus,2021-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),learning to slim deep networks with bandit channel pruning,"
AbstractView references

Recent years, deep neural network has achieved great success in machine vision, natural language processing, and reinforcement learning. While deploying these models on embedded devices and large clusters faces challenge in high energy consumption and low efficiency. In this paper, we propose an effective approach named Bandit Channel Pruning (BCP) to accelerate neural network by channel-level pruning.Inspired by autoML, we use Multi-Armed Bandit (MAB) method to explore and exploit the impact of each channel on model performance. Specifically, we use the loss value of model's output as penalty term to find the set of redundant channels. In addition, we prove that the change of this loss value can be used as criterion of channel redundant. We analyze the complexity of BCP and give the upper bound of search times.Our approach is validated with several deep neural networks, including VGGNet, ResNet56, ResNet110, on different image classification datasets. Extensive experiments on these models and datasets demonstrate the performance of this method is better than state-of-the-art channel pruning methods. © 2021 IEEE.
"
10.1016/j.future.2020.10.014,S0167739X20329915,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094807912&origin=inward,Article,SCOPUS_ID:85094807912,scopus,2021-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),easy and efficient agent-based simulations with the openabl language and compiler,"
                  Agent-based simulations represent an effective scientific tool, with numerous applications from social sciences to biology, which aims to emulate or predict complex phenomena through a set of simple rules performed by multiple agents. To simulate a large number of agents with complex models, practitioners have developed high-performance parallel implementations, often specialized for particular scenarios and target hardware. It is, however, difficult to obtain portable simulations, which achieve high performance and at the same time are easy to write and to reproduce on different hardware. This article gives a complete presentation of OpenABL, a domain-specific language and a compiler for agent-based simulations that enable users to achieve high-performance parallel and distributed agent simulations with a simple and portable programming environment. OpenABL is comprised of (1) an easy-to-program language, which relies on domain abstractions and explicitly exposes agent parallelism, synchronization and locality, (2) a source-to-source compiler, and (3) a set of pluggable compiler backends, which generate target code for multi-core CPUs, GPUs, and cloud-based systems. We evaluate OpenABL on simulations from different fields. In particular, our analysis includes predator–prey and keratinocyte, two complex simulations with multiple step functions, heterogeneous agent types, and dynamic creation and removal of agents. The results show that OpenABL-generated codes are portable to different platforms, perform similarly to manual target-specific implementations, and require significantly fewer lines of codes.
               "
10.1142/S0219876220500383,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85093940707&origin=inward,Article,SCOPUS_ID:85093940707,scopus,2021-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"localized lagrange multipliers mixed (u, p) formulation applied in wind turbine analysis","
AbstractView references

Fluid-structure analysis is frequently used to design offshore structures in a large range of engineering applications. In order to install wind turbines from the seashore, it is required that its towers must be attached at the sea floor or a system needs to be developed that allows the turbine to float. Therefore, the objective of this work is to develop a coupled structural finite element analysis using localized Lagrange multipliers (LLM) at idealized monopile wind turbines. This method enables us to model large structures like wind turbine towers submerged in the ocean and determine the influence of the fluid-structure interaction on the dynamic structural response of these equipments. In this work, the mixed formulation (u,p) developed takes into account the condition of irrotationality of the acoustic fluid. The classical LLM method is modified, thus, a new mixed-formulation for the interface frame has fluid pressure and solid displacement as degree of freedom. In other words, a coupling frame is introduced as fictitious porous material with the aim of non-symmetrical coupling between elastic solid and potential fluid. The solution to the non-matching meshes domain problem is also achieved with this methodology by several numerical strategies. The equations of solid and fluid domains are obtained by classical finite element method and their interaction is modeled with Localized Lagrange Multipliers. The solutions of numerical equations are obtained by direct form in a monolithic approach. The new modeling by using a porous material interface frame algorithm is verified through examples. © 2021 World Scientific Publishing Company.
"
10.1016/j.cpc.2020.107315,S0010465520301211,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083484642&origin=inward,Article,SCOPUS_ID:85083484642,scopus,2021-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),mxe: a package for simulating long-term diffusive mass transport phenomena in nanoscale systems,"
                  We present a package to simulate long-term diffusive mass transport in systems with atomic scale resolution. The implemented framework is based on a non-equilibrium statistical thermo-chemo-mechanical formulation of atomic systems where effective transport rates are computed using a kinematic diffusion law. Our implementation is built as an add-on to the Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS) code, it is compatible with other LAMMPS’ functionalities, and shows a good parallel scalability and efficiency. In applications involving diffusive mass transport, this framework is able to simulate problems of technological interest for exceedingly large time scales using an atomistic description, which are not reachable with the state-of-the-art molecular dynamics techniques. Several examples, involving complex diffusive behavior in materials, are investigated with the framework. We found good qualitative and quantitative comparison with known theories and models, with Monte Carlo methods, as well as with experimental results. Thus, our implementation can be used as a tool to understand diffusive behavior in materials where experimental characterization is difficult to perform.
               
                  Program summary
                  
                     Program Title: MXE package
                  
                     Program Files doi: 
                     http://dx.doi.org/10.17632/s2mhjb8hyk.1
                  
                  
                     Licensing provisions: GNU GPLv3
                  
                     Programming language: c++
                  
                     Supplementary material: The user manual and examples are provided along with the source code.
                  
                     External routines: MPI, LAMMPS, 12 December 2018 (http://lammps.sandia.gov/)
                  
                     Nature of problem: The simulation of diffusive mass transport in atomistic systems, involving vacancies, interstitials and solute atoms, is often challenging due to the exceedingly large time scale involved in these slow processes. Thus, molecular dynamics, a preferred technique to simulate atomistic systems, is not capable of accessing these long-term diffusive timescale, hindering its application to this kind of phenomena.
                  
                     Solution method: We present an implementation for simulating diffusive mass transport in atomic system. The methodology is based on two main pillars: (i) A non-equilibrium thermodynamic formulation of atomic system (Venturini et al., 2014); and (ii) Fokker–Planck master equation that encompasses the time evolution of the atomic molar fraction field in atomic systems (Ponga and Sun, 2018). The proposed implementation is built as a user package of the popular Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS). The implementation is flexible and robust, shows good parallel scalability and efficiency, and is compatible with all features available in LAMMPS.
                  
                     Additional comments including restrictions and unusual features: Unique features of the implementation involve the simulation of vacancy, interstitial, solutes and substitutional alloys for exceedingly large time scales.
                  The current implementation in this package is limited to the Embedded Atom Model potentials.
                  
                     References
                  
                  [1] G. Venturini, K. Wang, I. Romero, M. Ariza, M. Ortiz, Journal of the Mechanics and Physics of Solids 73 (2014) 242–268.
                  [2] M. Ponga, D. Sun, Modeling and Simulation in Materials Science and Engineering 26 (2018) 035014.
               "
10.1108/ECAM-04-2020-0256,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85087738573&origin=inward,Article,SCOPUS_ID:85087738573,scopus,2021-02-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative bim workspace for aec conceptual design automation: prototype development,"
AbstractView references

Purpose: The integration and automation of the whole design and implementation process have become a pivotal factor in construction projects. Problems of process integration, particularly at the conceptual design stage, often manifest through a number of significant areas, from design representation, cognition and translation to process fragmentation and loss of design integrity. Whilst building information modelling (BIM) applications can be used to support design automation, particularly through the modelling, amendment and management stages, they do not explicitly provide whole design integration. This is a significant challenge. However, advances in generative design now offer significant potential for enhancing the design experience to mitigate this challenge. Design/methodology/approach: The approach outlined in this paper specifically addresses BIM deficiencies at the conceptual design stage, where the core drivers and indicators of BIM and generative design are identified and mapped into a generative BIM (G-BIM) framework and subsequently embedded into a G-BIM prototype. This actively engages generative design methods into a single dynamic BIM environment to support the early conceptual design process. The developed prototype followed the CIFE “horseshoe” methodology of aligning theoretical research with scientific methods to procure architecture, construction and engineering (AEC)-based solutions. This G-BIM prototype was also tested and validated through a focus group workshop engaging five AEC domain experts. Findings: The G-BIM prototype presents a valuable set of rubrics to support the conceptual design stage using generative design. It benefits from the advanced features of BIM tools in relation to illustration and collaboration (coupled with BIM's parametric change management features). Research limitations/implications: This prototype has been evaluated through multiple projects and scenarios. However, additional test data is needed to further improve system veracity using conventional and non-standard real-life design settings (and contexts). This will be reported in later works. Originality/value: Originality and value rest with addressing the shortcomings of previous research on automation during the design process. It also addresses novel computational issues relating to the implementation of generative design systems, where, for example, instead of engaging static and formal description of the domain concepts, G-BIM actively enhances the applicability of BIM during the early design stages to generate optimised (and more purposeful) design solutions. © 2020, Emerald Publishing Limited.
"
10.3390/ijerph18041618,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100457538&origin=inward,Article,SCOPUS_ID:85100457538,scopus,2021-02-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),building a prevention system: infrastructure to strengthen health promotion outcomes,"
AbstractView references

Prevention systems improve the performance of health promotion interventions. This research describes the establishment of the Australian state government initiative, Healthy Together Victoria’s (HTV) macro infrastructure for the delivery of large-scale prevention interventions. Methods: This paper reports on findings of 31 semi-structured interviews about participants’ understanding of systems thinking and their reflections of the strengths and weaknesses of the HTV prevention system. A chronic disease prevention framework informed the coding that was used to create a causal loop diagram and a core feedback loop to illustrate the results. Results: Findings highlighted that HTV created a highly connected prevention system that included a sizeable workforce, significant funding and supportive leadership. Operating guidelines, additional professional development and real-time evaluation were significant gaps, which hindered systems practice. For inexperienced systems thinkers, these limitations encouraged them to implement programs, rather than interact with the seemingly ambiguous systems methods. Conclusions: HTV was an innovative attempt to strengthen health promotion infrastructure, creating a common language and shared understanding of prevention system requirements. However, the model was inadequate for HTV to achieve population-level reductions in chronic disease as system oversight was missing, as was an intervention delivery focus. Clarity was needed to define the systems practice that HTV was seeking to achieve. Importantly, the HTV prevention system needed to be understood as complex and adaptive, and not prioritized as individual parts. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
"
10.23919/DATE51398.2021.9474185,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108370840&origin=inward,Conference Paper,SCOPUS_ID:85108370840,scopus,2021-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),veridevops: automated protection and prevention to meet security requirements in devops,"
AbstractView references

Current software development practices are increasingly based on using both COTS and legacy components which make such systems prone to security vulnerabilities. The modern practice addressing ever changing conditions, DevOps, promotes frequent software deliveries, however, verification methods artifacts should be updated in a timely fashion to cope with the pace of the process. VeriDevOps, Horizon 2020 project, aims at providing a faster feedback loop for verifying the security requirements and other quality attributes of large scale cyber-physical systems. VeriDevOps focuses on optimizing the security verification activities, by automatically creating verifiable models directly from security requirements formulated in natural language, using these models to check security properties on design models and then generating artefacts such as, tests or monitors that can be used later in the DevOps process. The main drivers for these advances are: Natural Language Processing, a combined formal verification and model-based testing approach, and machine-learning-based security monitors. VeriDevOps is in its initial stage - the project started on 1.10.2020 and it will run for three years. In this paper we will present the major conceptual ideas behind the project approach as well as the organizational settings. © 2021 EDAA.
"
10.1111/soc4.12848,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100587021&origin=inward,Article,SCOPUS_ID:85100587021,scopus,2021-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),limiting labels: opportunities to learn and college readiness among english language learners,"
AbstractView references

Over a fifth of California's public schools' students have limited access to college and career pathways due to being labeled an “English-language learner (ELLs).” As reported by the California Department of Education, in the 2017–2018 school year, of the over 6.2 million students in California, nearly 1.3 million students are categorized as ELLs. The ELL label states students “whose difficulties in speaking, reading, writing, or understanding English may (my emphasis) limit his or her ability to (1) achieve in classrooms where English is the language of instruction and (2) access opportunities to fully participate in society.” Aligned with deficit thinking models, the US school system interprets “may” as “will” and makes decisions that negatively impact a large body of students across the country. The growing body of research reveals that many with the ELL label have been and continue to be intentionally underserved, limited access to postsecondary education, tracked into low-rigor and stigmatizing course pathways, and are deliberately unsupported to become college-ready, furthering the inequities in education and limiting options for quality k-12 educational experiences. The system becomes blinded by the label and ignores students with ELL labels' abilities and capacities to learn. So how are students with ELL labels supposed to become college-ready, let alone career-ready? For the purposes of this critical literature review, I will focus on exploring the existing structures that define “college-readiness” and the disparities created by the intentional tracking of students with the ELL label in comparison to their non-ELL peers. © 2021 John Wiley & Sons Ltd.
"
10.1002/adfm.202006245,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096964377&origin=inward,Article,SCOPUS_ID:85096964377,scopus,2021-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),surrogate model via artificial intelligence method for accelerating screening materials and performance prediction,"
AbstractView references

Predicting the performance of mechanical properties is an important and current issue in the field of engineering and materials science, but traditional experiments and modeling calculations often consume large amounts of time and resources. Therefore, it is imperative to use appropriate methods to accelerate the process of material selection and design. The artificial intelligence method, particularly deep learning models, has been verified as an effective and efficient method for handling computer vision and neural language problems. In this paper, a deep learning surrogate model (DLS) is proposed for predicting the mechanical performance of materials, that is, the maximum stress value under complex working conditions. The DLS can reproduce the finite element analysis model results with 98.79% accuracy. The results show that deep learning has great potential. This research also provides a new approach for material screening in practical engineering. © 2020 Wiley-VCH GmbH
"
10.1016/j.isatra.2020.08.012,S0019057820303359,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089745531&origin=inward,Article,SCOPUS_ID:85089745531,scopus,2021-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a hybrid of fem simulations and generative adversarial networks to classify faults in rotor-bearing systems,"
                  Condition monitoring of rotor-bearing systems using artificial intelligence has great significance to guarantee the reliability and security of mechanical systems. However, in engineering applications, AI model will fail to classify faults with insufficient fault samples owing to complex working condition. A hybrid fault classification approach is presented by combining finite element method (FEM) with generative adversarial networks (GANs) for rotor-bearing systems. Firstly, FEM simulations are employed to calculate simulation fault samples as additional sources of missing fault samples. Secondly, GANs is used to acquire abundant synthetic samples generated from the simulation and measurement samples, which aims to expand fault samples. Finally, the complete fault samples, including simulation, measurement and their corresponding synthetic samples, are utilized as training samples to train typical classifiers, and further to identify unknown faults. High classification accuracies for a rotor-bearing system using different kinds of artificial intelligent (AI) models are obtained, which demonstrates the effective of proposed method. It is noticed that the present idea can be guided to solve insufficient fault samples problem in more complex mechanical system with agreeable fault classification accuracy.
               "
10.1016/j.neucom.2020.08.078,S0925231220316027,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85095429875&origin=inward,Article,SCOPUS_ID:85095429875,scopus,2021-01-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),causality extraction based on self-attentive bilstm-crf with transferred embeddings,"
                  Causality extraction from natural language texts is a challenging open problem in artificial intelligence. Existing methods utilize patterns, constraints, and machine learning techniques to extract causality, heavily depending on domain knowledge and requiring considerable human effort and time for feature engineering. In this paper, we formulate causality extraction as a sequence labeling problem based on a novel causality tagging scheme. On this basis, we propose a neural causality extractor with the BiLSTM-CRF model as the backbone, named SCITE (Self-attentive BiLSTM-CRF wIth Transferred Embeddings), which can directly extract cause and effect without extracting candidate causal pairs and identifying their relations separately. To address the problem of data insufficiency, we transfer contextual string embeddings, also known as Flair embeddings, which are trained on a large corpus in our task. In addition, to improve the performance of causality extraction, we introduce a multihead self-attention mechanism into SCITE to learn the dependencies between causal words. We evaluate our method on a public dataset, and experimental results demonstrate that our method achieves significant and consistent improvement compared to baselines.
               "
10.1021/acssynbio.0c00554,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100053159&origin=inward,Article,SCOPUS_ID:85100053159,scopus,2021-01-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),flapjack: data management and analysis for genetic circuit characterization,"
AbstractView references

Characterization is fundamental to the design, build, test, learn (DBTL) cycle for engineering synthetic genetic circuits. Components must be described in such a way as to account for their behavior in a range of contexts. Measurements and associated metadata, including part composition, constitute the test phase of the DBTL cycle. These data may consist of measurements of thousands of circuits, measured in hundreds of conditions, in multiple assays potentially performed in different laboratories and using different techniques. In order to inform the learn phase this large volume of data must be filtered, collated, and analyzed. Characterization consists of using this data to parametrize models of component function in different contexts, and combining them to predict behaviors of novel circuits. Tools to store, organize, share, and analyze large volumes of measurement and metadata are therefore essential to linking the test phase to the build and learn phases, closing the loop of the DBTL cycle. Here we present such a system, implemented as a web app with a backend data registry and analysis engine. An interactive frontend provides powerful querying, plotting, and analysis tools, and we provide a REST API and Python package for full integration with external build and learn software. All measurements are associated with circuit part composition via SBOL (Synthetic Biology Open Language). We demonstrate our tool by characterizing a range of genetic components and circuits according to composition and context. ©
"
10.3390/app11020765,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099460604&origin=inward,Article,SCOPUS_ID:85099460604,scopus,2021-01-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),gaining insights into conceptual models: a graph‐theoretic querying approach,"
AbstractView references

Modern complex systems include products and services that comprise many intercon-nected pieces of integrated hardware and software, which are expected to serve humans interacting with them. As technology advances, expectations of a smooth, flawless system operation grow. Model‐based systems engineering, an approach based on conceptual models, copes with this chal-lenge. Models help construct formal system representations, visualize them, understand the design, simulate the system, and discover design flaws early on. Modeling tools can benefit tremendously from querying capabilities that enable gaining deep insights into system aspects that direct model observations do not reveal. Querying mechanisms can unveil and explain cause‐and‐effect phenom-ena, identify central components, and estimate impacts or risks associated with changes. Being con-nected networks of system elements, models can be effectively represented as graphs, to which queries are applied. Capitalizing on established graph‐theoretic algorithms to solve a large variety of problems can elevate the modeling experience to new levels. To utilize this rich set of capabilities, one must convert the model into a graph and store it in a graph database with no significant loss of information. Applying the appropriate algorithms and translating the query response back to the original intelligible and meaningful diagrammatic and textual model representation is most valua-ble. We present and demonstrate a querying approach of converting Object‐Process Methodology (OPM) ISO 19450 models into graphs, storing them in a Neo4J graph database, and performing queries that answer complex questions on various system aspects, providing key insights into the modeled system or phenomenon and helping to improve the system design. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147541511&origin=inward,Conference Paper,SCOPUS_ID:85147541511,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an integrated approach to formal analyze cyber physical systems,"
AbstractView references

Very large-scale systems now and in the future will be built by integrating existing systems from different providers to create „systems of systems‟ (SoS). Cyber Physical Systems (CPS) constitute an example of these complex systems, including computation, communication, and control. The complexity of CPS engineering arises when cyber capabilities communicate and coordinate with the physical capabilities (sensors/actuators). Several challenges should be overcome to design these systems, especially those related to their modeling and formal analysis. For this purpose, our doctoral research aims to propose a new approach for specifying CPS structures in a formal way facilitating their behaviors analysis. We opt to use the SySML model in order to bring the CPS designer closer to the theoretical models, based in this case on Maude language, which will serve as a formal basis for the analysis of these systems, in particular their security aspect, which currently constitutes a major challenge in all their application fields. © 2022 Copyright for this paper by its authors.
"
10.1007/978-3-030-81197-6_44,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138990964&origin=inward,Conference Paper,SCOPUS_ID:85138990964,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),revisiting tibetan word segmentation with neural networks,"
AbstractView references

Tibetan Word Segmentation is a basic and essential task in Tibetan Natural Language Processing workflow. Performance of TWS can directly affect many other downstream Tibetan NLP tasks since errors propagate in a multi-stage NLP pipeline. Traditionally the majority of researchers leverage linear statistical approaches to tackle Tibetan Word Segmentation, which often requires handcrafted linguistic feature engineering with great care. In this work, we propose a neural network architecture for Tibetan Word Segmentation, which is a stacked combination of CNN, Bi-LSTM and CRF. By using tagged data for supervised learning and unlabeled data for representation learning, with no involvement in feature engineering, our model can produce promising performance on the test set, surpassing our baseline models by a large margin, and indicating the effectiveness of the proposed neural model. © Springer Nature Switzerland AG 2021.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138358716&origin=inward,Conference Paper,SCOPUS_ID:85138358716,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),code to comment translation: a comparative study on model effectiveness &amp; errors,"
AbstractView references

Automated source code summarization is a popular software engineering research topic wherein machine translation models are employed to “translate” code snippets into relevant natural language descriptions. Most evaluations of such models are conducted using automatic reference-based metrics. However, given the relatively large semantic gap between programming languages and natural language, we argue that this line of research would benefit from a qualitative investigation into the various error modes of current state-of-the-art models. Therefore, in this work, we perform both a quantitative and qualitative comparison of three recently proposed source code summarization models. In our quantitative evaluation, we compare the models based on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics, and in our qualitative evaluation, we perform a manual open-coding of the most common errors committed by the models when compared to ground truth captions. Our investigation reveals new insights into the relationship between metric-based performance and model prediction errors grounded in an empirically derived error taxonomy that can be used to drive future research efforts. © 2021 Association for Computational Linguistics.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137961504&origin=inward,Conference Paper,SCOPUS_ID:85137961504,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on functional requirements for keyword-based query over heterogeneous databases on the web,"
AbstractView references

Context. A large amount of data is made available daily on the Web, but many databases cannot be accessed by conventional search engines, as they require proper access methods and specialised knowledge through their access languages. Focus. In the scenario of non-expert users to access databases, multiple database categories, and plural idioms, this work analyzes the functional requirements that need to be considered for keyword queries processing over data sources on the Web. The problem is still open and involves challenges such as query interpretation and access to databases. Method. The investigation is centered on the problem itself, which is portrayed by a set of functional issues, which together represent the challenges linked to the research field. Approach. This work introduces and systematically analyzes the functional requirements to the problem scope. Issues reported in the literature are refined and evolved to support the modeling of the problem views: functional responsibilities and their interactions by messaging between problem objects. Conclusions and Results. This paper contributes to characterize the problem, makes clearer its understanding and promotes the development of keyword-based query processing systems. A software engineering artifact is used to model the problem and make it more formal and precise. Further studies will refine such requirements and build (specialise) artifacts tailored to the solution space. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137685347&origin=inward,Conference Paper,SCOPUS_ID:85137685347,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),kilt: a benchmark for knowledge intensive language tasks,"
AbstractView references

Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the reuse of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at https://github.com/facebookresearch/KILT. © 2021 Association for Computational Linguistics.
"
10.3850/978-981-18-2016-8_561-cd,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135486055&origin=inward,Conference Paper,SCOPUS_ID:85135486055,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),flexibility of analysis through knowledge bases,"
AbstractView references

Model-based safety analysis (MBSA) offers multiple advantages for the stakeholders. The close link between a system description and the model that supports safety or dependability analysis allows automated model building and facilitates model reviews. Maintenance of the model benefits from a high-level system description with assumptions presented explicitly as an integral part of the model. Finally, the analysis process can utilize various properties of the system that are encapsulated in the components and their interactions. This paper focuses on the flexibility of analysis that does not require large remodeling or even building a new model when the question to be answered calls for additional or different system aspects to be considered. We show how knowledge bases built in the Figaro modeling language enable this flexibility on several examples from various application domains and for multiple specific system features. The modeling effort becomes to a large part decoupled from the analysis. An analyst building a model does not have to keep a specific analysis method in mind and might not even be aware of methods that will be required in the future. © ESREL 2021. Published by Research Publishing, Singapore.
"
10.3850/978-981-18-2016-8_671-cd,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135474167&origin=inward,Conference Paper,SCOPUS_ID:85135474167,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),efficient modeling of large markov chains models with altarica 3.0,"
AbstractView references

Markov chains are one of the modeling formalisms used in reliability engineering. Even if it is powerful, from a mathematical point of view, one of its big issue is the design of models for large scale systems. In fact, designing a Markov chain of a system with several components, each one may be in several states, is an important amount of job. There are no structural constructs to efficiently design such a model (e.g. composition, synchronization, etc.). In this publication, we present how the AltaRica 3.0 modeling language can be used to design efficiently large continuous time Markov chains. We consider an example of a system composed of combinations of series-parallel components, combining different states for components and different modes for parts of the system. We show that the design of the model is very efficient thanks to the advanced structural constructs of the AltaRica 3.0 modeling language. Finally, we use assessment tools available for AltaRica 3.0, e.g. the stochastic simulator, to evaluate the model of the system. © ESREL 2021. Published by Research Publishing, Singapore.
"
10.1061/9780784483893.018,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132572707&origin=inward,Conference Paper,SCOPUS_ID:85132572707,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),artificial-neural-network-based model for predicting heating and cooling loads on residential buildings,"
AbstractView references

A population growth of around 1,000 people per day is the main factor contributing to the increase in housing demands in Florida. Optimization of the energy performance of residential buildings is crucial for the reduction of greenhouse gas emissions and fossil fuel consumption. This optimization entails the designers to accurately predict the energy consumption of buildings right from the design stage. Traditional energy modeling techniques require a lot of expertise and tend to be time-consuming and lacking in terms of energy predictions. Artificial intelligence (AI) techniques have been recently used to predict the energy usage of buildings and to be an alternative solution to such engineering methods. This study developed an artificial neural network (ANN) model based on a large data set of more than 18,000 newly constructed single-family houses in Florida between the years 2009 and 2019 to predict the heating and cooling loads on single-family houses. An ANN model based on this data set is developed to predict the energy usage of detached residences. The ANN will help designers to make the right decisions during the conceptual design stage of residential projects, and to explore the different design options using generative design for energy optimization purposes. © 2021 Computing in Civil Engineering 2021 - Selected Papers from the ASCE International Conference on Computing in Civil Engineering 2021. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131926727&origin=inward,Conference Paper,SCOPUS_ID:85131926727,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sequence-to-sequence learning with latent neural grammars,"
AbstractView references

Sequence-to-sequence learning with neural networks has become the de facto standard for sequence prediction tasks. This approach typically models the local distribution over the next word with a powerful neural network that can condition on arbitrary context. While flexible and performant, these models often require large datasets for training and can fail spectacularly on benchmarks designed to test for compositional generalization. This work explores an alternative, hierarchical approach to sequence-to-sequence learning with quasi-synchronous grammars, where each node in the target tree is transduced by a node in the source tree. Both the source and target trees are treated as latent and induced during training. We develop a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering. We apply this latent neural grammar to various domains-a diagnostic language navigation task designed to test for compositional generalization (SCAN), style transfer, and small-scale machine translation-and find that it performs respectably compared to standard baselines. © 2021 Neural information processing systems foundation. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131144098&origin=inward,Conference Paper,SCOPUS_ID:85131144098,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a sysml-based holistic variability modelling of software and systems product lines,"
AbstractView references

Software and Systems Product Line (SSPL) engineering has shown capabilities to reduce costs and time to market. This is thanks to the creation and management of a common platform dedicated to develop a family of products. These latters can be a family of mobile phones, a family of different brake systems variants for automative needs, etc. Recently, more and more large-scale companies start to implement SSPL engineering in their domains by adopting Model-Based Systems Engineering (MBSE). Systems Product Line (PL) engineering is much broader than software PL engineering. Therefore, various aspects of variability (e.g. functional and quality attributes variabilities) have to be considered in MBSE. However, variability integrated in MBSE is still limited to functional variability. This paper contributes to enhance the SSPL modelling based on SysML by extending the SysML language. The principle aim is to include various aspects of variability. In fact, a holistic variability model is proposed to define the SysML extensions by means of the UML profiling mechanism. This permits to express variability constructs in different SysML modelling artifacts. We also present an application example namely the brake systems family extracted from Splot repository. We in fact, show how our SysML extensions are concretely used. © 2021, EasyChair. All rights reserved.
"
10.1109/ICISE-IE53922.2021.00356,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128331663&origin=inward,Conference Paper,SCOPUS_ID:85128331663,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the construction of a mixed university english teaching system based on artificial intelligence technology,"
AbstractView references

Artificial intelligence technology is widely used in English teaching, the application of artificial intelligence technology in the English teaching mainly adopts data real-time monitoring, data acquisition, data mining and other auxiliary English model system, auxiliary system of artificial intelligence application in English on the experiment platform are mainly composed of data acquisition, on the experimental platform for data integration analysis of English. The English experiment platform system has the characteristics of wide data range, large computation and high complexity. This paper mainly studies the practical role of artificial intelligence in college English teaching and discusses the application of artificial intelligence in English teaching. At the same time, through the systematic analysis of the current Al-assisted language learning software and system defects put forward relevant solutions, artificial intelligence technology is conducive to improve the application of intelligent human-machine system in English teaching, and provide technical support for the cultivation and development of English talents in colleges and universities. © 2021 IEEE
"
10.1109/iSPEC53008.2021.9735461,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128008983&origin=inward,Conference Paper,SCOPUS_ID:85128008983,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),unit commitment of power system with wind power and photovoltaic considering frequency safety constraint,"
AbstractView references

With the large-scale wind turbine and photovoltaic increase, the inertial response of the power system and ability of primary frequency modulation are reduced, which makes the frequency safety problem stand out gradually. It is particularly important to consider the frequency dynamic safety constraint in day-ahead generation scheduling. In this paper, a frequency safety constraint construction method considering the frequency nadir is proposed. Wind power and photovoltaic integrated inertial control are introduced to enable wind turbines and photovoltaic. Based on that, considering the frequency dynamic safety constraint with wind power and photovoltaic integrated inertial control of the unit commitment model is established. Then, the M language is used to dynamically control the frequency response model of multi-machines for Simulink, which is also embedded in the particle swarm optimization algorithm with changing weight for the model solution. The consequences of the IEEE24-bus system with wind power and photovoltaic indicate that model and method can assure the frequency nadir and frequency safety margin meeting requirement. © 2021 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127571107&origin=inward,Conference Paper,SCOPUS_ID:85127571107,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),inclusion of indigenous peoples in aerospace technologies.,"
AbstractView references

Currently, 68 indigenous peoples inhabit the Mexican territory, each one speaking their own native language. The advancement of the digital world, artificial intelligence, aerospace technology and access to big-data undoubtedly creates powerful new development opportunities for society, but it also has the potential to deepen existing inequality gaps. Something that characterizes Mexico are its indigenous peoples where a large part of the customs, wealth and traditions of the country are concentrated. The National Technological Institute of Mexico is found in every corner of the country through its 254 institutes where approximately 25% of them are in these areas. The Technological Institute of Milpa Alta of CDMX, in its impetus to contribute to closing the technological gap, signed a collaboration agreement with the Mexican Space Agency; where students put into practice their acquired knowledge so that the inequality gap is reduced, under a robust methodology workshops and training are given about new technologies such as the cansat model, robotic systems and software development, focused mainly on communities, where there is no access to information about science, technology and the aerospace sector. Because the aerospace sector is one of the most important and impressive in terms of the development of first-rate technologies, it offers the opportunity to accelerate the processes and mechanisms of growth between countries. To carry out all this evolution it is necessary to implement some of the 17 SDGs; 1- SDG 8 Decent work and economic growth. 2- SDG 10 Reduce inequalities. 3- SDG 17 Alliances to achieve the Goals. One of the advantages of indigenous peoples is the fusion of digital technologies with traditional knowledge, worldview, and indigenous priorities, offers a powerful opportunity to promote development with identity in the digital age. Supporting indigenous peoples’ access to new technologies is a priority in all regions, not only to close digital inclusion gaps, but also to move towards a more equitable 21st century. Copyright © 2021 by the International Astronautical Federation (IAF). All rights reserved.
"
10.1109/OCIT53463.2021.00029,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127387729&origin=inward,Conference Paper,SCOPUS_ID:85127387729,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),model-based test effort estimator - a case study,"
AbstractView references

As per the surveys conducted over a period of time, the top four reasons for failure of software projects are functionality issues, deadline misses, quality issues and budget overruns. Customer goodwill, which is extremely important for a software firm's continued operation, is greatly affected by the product quality. The two main strategies used to achieve quality are, improving the development processes and comprehensive testing of the product based on an accurate test plan. A realistic planning requires a dependable effort estimation of the testing activities. This work presents an automated mechanism for estimation of test effort of Object Oriented Systems (OOS) leveraging the widely accepted Class UML models. Also presented are the steps of estimation for a contemporary web-based case study project undertaken in a large public sector IT firm. The results obtained confirmed the validity and accuracy of this approach. © 2021 IEEE.
"
10.1109/CIS54983.2021.00128,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126913309&origin=inward,Conference Paper,SCOPUS_ID:85126913309,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),multi-turn response selection using business sequential relations in traffic field,"
AbstractView references

BERT-based models play an essential role and achieve significant results in many tasks of natural language processing (NLP), including dialog tasks. However, there is a limitation of BERT to handle long dialogs. The paper studies how pre-trained model handles long conversations as the input in the multi-turn response selection task. Given the lack of data with long conversations and current dialog tasks are based on engineering applications, it is necessary to collect large dataset with long text sequences. Meanwhile, collecting dialog dataset is a time-consuming work, practical industrial settings usually need more accurate. In this paper, we cite the Chinese Multi-Intention Dialogue (The CMID-Transportation) dataset of transportation customer service, and change it to adapt to response selection task in the dialog systems. To this end, we propose a business-level strategy and use truncation methods to address this problem on the corpus. The experimental results on this corpus show that our proposed approach is fit for the BERT-based model and brings better performance. © 2021 IEEE.
"
10.1109/ICCWS53234.2021.9702954,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126536048&origin=inward,Conference Paper,SCOPUS_ID:85126536048,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),efficient identification of race condition vulnerability in c code by abstract interpretation and value analysis,"
AbstractView references

The increased usage of information and communication technologies has changed the way industries look at things. This development of technology in terms of software utilization has resulted in various security vulnerabilities such as injection, data disclosure, authentication, and access control concerns. When working with concurrent applications, race conditions can trigger a number of these vulnerabilities. Formal approaches have been developed to detect the race condition vulnerability in the literature. Existing approaches for detecting race conditions have few drawbacks that includes static checkers' inability to analyze the uninterpreted programs, and minimal exploration of race condition types. Due to these weaknesses static checkers produce a large number of false alarms. This study proposes an algorithm AIT for analysis of uninterpreted programs based on a formal static analysis technique called Abstract Interpretation (AI). It also proposes the T2RC and Sync RC algorithms for race condition detection. The proposed approach is validated using Juliet and Data Race Bench data sets. The proposed method yields an average accuracy of 84% and produces very few false alarms. Additionally, it not only handles uninterpreted programs, but also analyzes for a wider range of Race Conditions thus performing better than other comparable approaches. © 2021 IEEE.
"
10.1109/ICBASE53849.2021.00121,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126484538&origin=inward,Conference Paper,SCOPUS_ID:85126484538,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deep learning for unsupervised neural machine translation,"
AbstractView references

With the breakthrough of deep learning techniques, many Natural Language Processing tasks have exploited the techniques to enhance performance. Neural Machine Translation, as a sub-field of NLP, also leverages deep learning methods to enhance performance. There are two main categories of NMT, the first one is supervised, and the other one is unsupervised. Supervised NMT uses labelled data and large parallel corpus for training, while unsupervised NMT only uses independent monolingual corpora for training the model. The latter one gives many conveniences in data collection but poses a great difficulty in engineering the architecture. Thus, in this paper, we give a comprehensive review of deep learning techniques for unsupervised neural machine translation.as illustrated by the portions given in this document. © 2021 IEEE.Allrights reserved
"
10.1007/978-3-030-93733-1_5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126179369&origin=inward,Conference Paper,SCOPUS_ID:85126179369,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a stacked bidirectional lstm model for classifying source codes built in mpls,"
AbstractView references

Over the years, programmers have improved their programming skills and can now write code in many different languages to solve problems. A lot of new code is being generated all over the world regularly. Since a programming problem can be solved in many different languages, it is quite difficult to identify the problem from the written source code. Therefore, a classification model is needed to help programmers identify the problems built (written/developed) in Multi-Programming Languages (MPLs). This classification model can help programmers learn better programming. However, source code classification models based on deep learning are still lacking in the field of programming education and software engineering. To address this gap, we propose a stacked Bidirectional Long Short-Term Memory (Bi-LSTM) neural network-based model for classifying source codes developed in MPLs. To accomplish this research, we collect a large number of real-world source codes from the Aizu Online Judge (AOJ) system. The proposed model is trained, validated, and tested on the AOJ dataset. Various hyperparameters are fine-tuned to improve the performance of the model. Based on the experimental results, the proposed model achieves an accuracy of about 93% and an F1-score of 89.24%. Moreover, the proposed model outperforms the state-of-the-art models in terms of other evaluation matrices such as precision (90.12%) and recall (89.48%). © 2021, Springer Nature Switzerland AG.
"
10.1109/ASE51524.2021.9678848,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125487834&origin=inward,Conference Paper,SCOPUS_ID:85125487834,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cross-lingual transfer learning framework for program analysis,"
AbstractView references

Deep learning-based techniques have been widely applied to program analysis tasks, in fields such as type inference, fault localization, and code summarization. Hitherto deep learning-based software engineering systems rely thoroughly on supervised learning approaches, which require laborious manual effort to collect and label a prohibitively large amount of data. However, most Turing-complete imperative languages share similar control- and data-flow structures, which make it possible to transfer knowledge learned from one language to another. In this paper, we propose a general cross-lingual transfer learning framework PLATO for program analysis by using a series of techniques that are general to different downstream tasks. PLATO allows Bert-based models to leverage prior knowledge learned from the labeled dataset of one language and transfer it to the others. We evaluate our approaches on several downstream tasks such as type inference and code summarization to demonstrate its feasibility. © 2021 IEEE.
"
10.1109/ASE51524.2021.9678582,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125461403&origin=inward,Conference Paper,SCOPUS_ID:85125461403,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a prediction model for software requirements change impact,"
AbstractView references

Software requirements Change Impact Analysis (CIA) is a pivotal process in requirements engineering (RE) since changes to requirements are inevitable. When a requirement change is requested, its impact on all software artefacts has to be investigated to accept or reject the request. Manually performed CIA in large-scale software development is time-consuming and error-prone so, automating this analysis can improve the process of requirements change management. The main goal of this research is to apply a combination of Machine Learning (ML) and Natural Language Processing (NLP) based approaches to develop a prediction model for forecasting the requirement change impact on other requirements in the specification document. The proposed prediction model will be evaluated using appropriate datasets for accuracy and performance. The resulting tool will support project managers to perform automated change impact analysis and make informed decisions on the acceptance or rejection of requirement change requests. © 2021 IEEE.
"
10.1109/ICECA52323.2021.9676097,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125379502&origin=inward,Conference Paper,SCOPUS_ID:85125379502,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design of java interactive system pattern based on computational thinking,"
AbstractView references

With the increasing complexity of data types, the workload of software development is also increasing, and the later maintenance of software becomes very difficult. According to statistics, the vulnerability related to complex data processing occupies a large proportion of software vulnerabilities. The existence of software vulnerability makes the software system vulnerable to attack and affects the reliability of the software system. In this paper, the upper computer system uses the HashMap structure to store the query data directly in the structure and reduce the mapping process from data to table objects. After that, the rationality of the system module is analyzed by building a complex network model, and the robustness of the system is improved by finding the key nodes. © 2021 IEEE.
"
10.1109/BigData52589.2021.9671514,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125305203&origin=inward,Conference Paper,SCOPUS_ID:85125305203,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),trustworthy knowledge graph population from texts for domain query answering,"
AbstractView references

Obtaining answers to domain-specific questions over large-scale unstructured (text) data is an important component of data analytics in many application domains. As manual question answering does not scale to large text corpora, it is common to use information extraction (IE) to preprocess the texts of interest prior to posing the questions. This is often done by transforming text corpora into the knowledge-graph (KG) triple format that is suitable for efficient processing of the user questions in graph-oriented data-intensive systems.In a number of real-life scenarios, trustworthiness of the answers obtained from domain-specific texts is vital for downstream decision making. In this paper we focus on one critical aspect of trustworthiness, which concerns aligning with the given domain vocabularies (ontologies) those KG triples that are obtained from the source texts via IE solutions. To address this problem, we introduce a scalable domain-independent text-to-KG approach that adapts to specific domains by using domain ontologies, without having to consult external triple repositories. Our IE solution builds on the power of neural-based learning models and leverages feature engineering to distinguish ontology-aligned data from generic data in the source texts. Our experimental results indicate that the proposed approach could be more dependable than a state-of-the-art IE baseline in constructing KGs that are suitable for trustworthy domain question answering on text data. © 2021 IEEE.
"
10.1117/12.2624898,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124806659&origin=inward,Conference Paper,SCOPUS_ID:85124806659,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),biomedical even trigger identification based on the gated unit neural network and word representation,"
AbstractView references

Biomedical event trigger extraction, as one of the sub-task of biomedical event extraction, plays an important role in biomedical research. A biomedical event trigger is a word or phrase that marks the emergence of a certain biomedical event. The recent works are usually based on the rule and the machine learning methods. However, the rule-based methods heavily rely on the concrete rules enumerated by the field expert and usually need a large amount of expert knowledge. The machine learning-based approaches usually utilize many handcraft features such as n-gram, lexicon, pose-tag and shortest dependency path. As a result, these methods based on machine learning can suffer from handcraft engineering with expensive time costs and the problem of generalization in the field transition. With the popularization of deep learning techniques, some effective frameworks in Natural Language Processing (NLP), such as adversarial training, self-attention mechanism, graph convolutional network, have been proposed to enhance the model performance for the NLP, especially the information extraction. As a task in the information extraction field, the frameworks mentioned above have been applied in the biomedical trigger identification subtask. This paper attempts to employ an external version of the recurrent neural network (RNN), i.e., bidirectional Gated Recurrent Unit (Bi-GRU) network, to extract the biomedical event trigger existing in the biological literature. Specifically, we first transform each token and entity label in the sentence to a word sequence with token index and an entity label sequence with entity label index. Subsequently, the above two sequences will be fed into the embedding layer to obtain the concatenated tensor between them. Moreover, we put the tensor into the Bi-GRU to generate the contextual encoding, which will be fed in a linear layer with an activation function to predict the probability distribution of the trigger. The final experiment on the MLEE dataset confirms that the proposed model can achieve comparable performance with an F-score of 78.82%. © 2021 SPIE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124474408&origin=inward,Conference Paper,SCOPUS_ID:85124474408,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design and research on safety analysis tool for avionics system,"
AbstractView references

With the continuous improvement of avionics system, the traditional method of safety analysis is difficult to guarantee the completeness of failure mode as it is too dependent on engineering experience. And in the process of system iterative design, due to the complexity of the system, the workload of safety analysis is too large, which increases the time and cost. Aiming at the above problems, an automatic safety analysis tool was designed, the safety data model was established based on SysML, automatic fault tree was built by using route tracing method, and the common mode analysis and zone safety analysis were carried out on the generated fault tree. The experimental results of a system show that the tool can realize the automatic modeling and analysis of the fault tree and improve the efficiency and competeness of the safety analysis. © 2021 32nd Congress of the International Council of the Aeronautical Sciences, ICAS 2021. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124459223&origin=inward,Conference Paper,SCOPUS_ID:85124459223,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),greener atm system architecture research based on complex system engineering method,"
AbstractView references

Greener aviation has gradually become a development trend of global air transportation industry, which is crucial for achieving sustainable development goals. The realization of greener aviation urgently requires innovation at the level of air traffic operation. Traditional air traffic management is mainly based on the premise of ensuring the safety of aircraft operations, with the goal of maximizing the utilization of airport and airspace service capabilities, paying less attention to the environmental impact induced by air traffic management strategy implementation. The future greener ATM system will focus on energy conservation and emission reduction. Through the implementation of new concepts, technologies and methods such as advanced surface management, continuous climb/descend operation, and collaborative decision-making, the airspace structure, flight trajectory and procedures will be optimized to realize the development requirements of greener aviation. The research on greener ATM system architecture is necessary to identify the characteristics of greener air traffic operation and to clarify the stakeholder requirements and system capability requirements for greener ATM system. It is also a prerequisite for the research of key technologies for greener ATM and avionic systems. As a typical complex system, the greener ATM system has the following characteristics: 1) multi-business and multi-node, involving the mutual cooperation and information exchange among multiple subsystems such as aircraft, satellites, and ground stakeholders; 2) each subsystem can play its own role and operate independently of other systems; 3) dynamic system which includes a large number of emergent behaviors; 4) central-radiation or distributed network topology which defines the connection among various nodes. Therefore, the analysis of this system requires the selection of appropriate system engineering methods, frameworks, and modeling processes to clearly and accurately reflect system functions and behaviors. The traditional architecture design approach is a document-centric system engineering method. This method extracts information from multifarious documents, causing divergence in understanding of different designers, and it requires too many iterative processes, which is difficult for information tracking and management. The model-based system engineering (MBSE) method avoids the vagueness and ambiguity caused by document via using the object-oriented, graphical and visualized system modeling language to describe the system, and has good consistency, verifiability and traceability. This paper proposes a greener ATM system architecture design and development process based on MBSE method. According to analysis of new concepts of air traffic operations for green aviation, areas of change and enhancement beyond existing air traffic operations are identified. Typical greener operation scenarios that implement the above concepts are constructed for long-haul and short-haul operations, and new requirements proposed by various stakeholders under these scenarios are analyzed. Based on the operation scenarios and requirements, the procedures of each flight phase and the information exchange pattern among the operational nodes are identified from the operation perspective, to establish the operational view models. The system composition and system functions of the greener ATM system are defined from the system perspective, and the mapping relationship between system functions and operational activities is created, to establish the system view models. These models include high-level operational concept diagrams, sequence diagrams, state-machine diagrams, etc. Through the development of these architecture models, the requirements of the ATM system supporting greener aviation operational concepts are clearly defined, which provides guidance for the subsequent research of key technologies. Compared with the traditional method, the MBSE-based greener air traffic management system development process has the following advantages: 1) the model decomposition process helps to explore more details in the system; 2) the model-based architecture analysis can guarantee the consistency in different views; 3) architecture analysis can predict and analyze the emergent behaviors of the system. © 2021 32nd Congress of the International Council of the Aeronautical Sciences, ICAS 2021. All rights reserved.
"
10.1109/ECICE52819.2021.9645649,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124242700&origin=inward,Conference Paper,SCOPUS_ID:85124242700,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),performance analysis of fine-tune transferred deep learning,"
AbstractView references

Nowadays, artificial intelligence (AI) and deep learning becomes the most important research issues. The history of AI technology started from machine learning, convolution neural networks (CNN), recurrent neural network (RNN), long short-term memory (LSTM), then developed to the latest technology of generative adversarial network (GAN). Deep learning is improving quickly. Data augmentation and transfer learning are currently advanced functions in the field of deep learning, which accelerates the whole training of the neural networks and improves model accuracy. This study implements these two functions and analyzes the performance in a deep learning model. Due to our performance study, researchers can make the most efficient AI model. To find a better deep learning model, we evaluate five deep learning strategies. In this way, we compare different learning effects and observe model accuracy. According to analysis charts and data tables, this study shows that transferred deep learning and fine-tune strategy not only increase the accuracy but also give insignificant overfitting. We also prove that fine-tune transfer learning significantly saves more time and costs in future tests and experiments. © 2021 IEEE.
"
10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00222,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124123569&origin=inward,Conference Paper,SCOPUS_ID:85124123569,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative adverserial analysis of phishing attacks on static and dynamic content of webpages,"
AbstractView references

In this paper, we studied the phishing problem from data analytics and AI-powered adversarial attacks perspectives. We evaluated several static and dynamic features that can be used as good models' predictors. Unlike the URL, which can be easily crafted to evade detection, the page static and dynamic content cannot be easily changed without changing what is presented to the potential victim. We evaluated several conventional and ensemble-based models and reported the best models and settings of models that showed high prediction accuracy. We then analyzed the feasibility of evading phishing classifiers by perturbing static and dynamic features using AI generative models then test both conventional and ensemble classifiers. Our results shows that the analysis of static and dynamic features of web pages has good potential in the area adversarial learning to generate phishing attacks. The results yield that it is more challenging to evade phishing classifiers relying on dynamic content features, which offers a good level of robustness against evasion tactics. © 2021 IEEE.
"
10.1109/MODELS-C53483.2021.00104,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124046606&origin=inward,Conference Paper,SCOPUS_ID:85124046606,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),heterogeneous model query optimisation,"
AbstractView references

With the growing size and complexity of software systems, the underlying models also grow in size proportionally. These large-scale models pose scalability issues for model-driven engineering technologies. These models can be persisted in various backend technologies (such as file systems, document and relational databases) and can be represented in different formats such as XMI and Flexmi. Several tailored high-level model management languages such as OCL and EOL enable developers to work on different backend technologies in a uniform way by shielding them from the complexities of different backends. On the contrary, performance with respect to execution time in tailored model management languages programs becomes one of the major scalability bottlenecks. In this work, we propose an architecture built on top of existing model query languages to facilitate query optimisation. The proposed approach will benefit from compile-time static analysis and automatic program rewriting to optimise queries operating over heterogeneous backend technologies. Optimisation strategies and performance will vary depending on the type of queries and the backend modelling technology. We expect to significantly improve performance (decrease in one order of magnitude of execution time) for model management programs, particularly over large-scale models. © 2021 IEEE.
"
10.1109/MODELS-C53483.2021.00019,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124040659&origin=inward,Conference Paper,SCOPUS_ID:85124040659,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards scalable validation of low-code system models: mapping evl to viatra patterns,"
AbstractView references

Adoption of low-code engineering in complex enterprise applications also increases the size of the underlying models. In such cases, the increasing complexity of the applications and the growing size of the underlying artefacts, various scalability challenges might arise for low-code platforms. Task-specific programming languages, such as OCL and EOL, are tailored to manage the underlying models. Existing model management languages have significant performance impact when it comes to complex queries operating over large-scale models reaching magnitudes of millions of elements in size. We propose an approach for automatically mapping expressions in Epsilon validation programs to VIATRA graph patterns to make the validation of large-scale low-code system models scalable by leveraging the incremental execution engine of VIATRA. Finally, we evaluate the performance of the proposed approach on large Java models of the Eclipse source code. Our results show performance speed-up up to 1481x compared to the sequential execution in Epsilon. © 2021 IEEE.
"
10.1109/CLEI53233.2021.9640221,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123873011&origin=inward,Conference Paper,SCOPUS_ID:85123873011,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),short-time prediction of dns queries using deep learning and pre-trained word embedding,"
AbstractView references

Word embeddings are widely used in natural language processing (NLP) to group semantically similar words but have been applied in other areas to find semantic similarity between entities. In this paper we create a vector embedding for Internet Domain Names (DNS) using a corpus of real anonymized DNS log queries from a large Internet Service Provider (ISP). We then use this embedding as a layer of a recurrent neural network (RNN) that works as a Language Model for the DNS queries generated by the users. We show that this RNN can be used to predict the next DNS query generated by a user with good accuracy (considering the size of the problem). Moreover, we show that training the same RNN without using the pre-trained vector model takes more time and is substantially less accurate. The results presented in this work can have practical applications in many engineering activities related to DNS architecture design. For example, latency reduction in address resolution, optimization of cache systems in recursive DNS servers, automatic filtering of inappropriate domains, and detecting anomalies in traffic. ©2021 IEEE
"
10.1109/IoTaIS53735.2021.9628587,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123794052&origin=inward,Conference Paper,SCOPUS_ID:85123794052,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),system configuration of human-in-the-loop simulation for level 3 autonomous vehicle using ipg carmaker,"
AbstractView references

The increasingly automated vehicles (AV) have increased the complexity of the testing methods and number of driven miles required to demonstrate the vehicle system's reliability. Most modern autonomous driving systems also used deep neural networks which requires a large amount of data to develop. Physical driving alone to collect driving data and test system's safety is no longer suitable for development of AV as this is costly, time consuming and could harm the road users if the safety system failed. This paper proposed human-in-the-loop simulation testing for evaluation of an autonomous vehicle using a 3D virtual vehicle driving platform that can be used for safety assessment of autonomous vehicle. The aim of this study is to establish human-computer interaction platform that can be used as safety testing for Level 3 autonomous vehicle whereby an emergency takeover is required during critical driving conditions. The proposed platform make use of IPG CarMaker to provide 3D virtual environment with accurate vehicle dynamics model, sensor model and environment model. We are able to interface the IPG CarMaker with Simulink and successfully developed a Simulink model that can interface a steering and pedal driving hardware with the virtual vehicle in the simulation. We can also collect driving data and simulation data from the IPG CarMaker as well as accessing the variable in the IPG CarMaker in real-time using Python. The recorded data can be used to train and fine-tune autonomous system based on machine learning. © 2021 IEEE.
"
10.1109/BigDIA53151.2021.9619686,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123471846&origin=inward,Conference Paper,SCOPUS_ID:85123471846,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),question answering-based socio-economic indicator extraction,"
AbstractView references

Socio-economic indicators are important statistical data used to monitor and evaluate the development of economy and society. They are of great value to policy makers of government, organizations and enterprises. Previous methods to measure socio-economic conditions are through data collecting or large-scale surveys by manpower. Over recent years, with the quick development of Internet, a huge amount of online text becomes available and contains clues of socio-economic indicators. As a result, rule-based methods and traditional machine learning methods are emerging to extract indicators more effectively but they require too much prior domain knowledge and feature engineering. Recently, deep learning approaches are dominating in Natural Language Processing (NLP) and formulating NLP tasks as machine reading comprehension (MRC) problems has been proven to be effective. Therefore, we propose a RoBERTa-based model that treats indicator extraction as a question answering (QA) task. By answering questions customized for indicator elements which are the components of the indicator, it transforms extracting indicator elements to identifying and extracting answer spans from the given contexts. Experiments show that it outperforms other deep learning baselines. © 2021 IEEE.
"
10.1109/RE51729.2021.00036,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123222563&origin=inward,Conference Paper,SCOPUS_ID:85123222563,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a pipeline for automating labeling to prediction in classification of nfrs,"
AbstractView references

Non-Functional Requirements (NFRs) focus on the operational constraints of the software system. Early detection of NFRs enables their incorporation into the architectural design at an initial stage, a practice obviously preferable to expensive refactoring at a later stage. Automated identification and classification of NFRs has therefore seen numerous efforts using rule-based, machine learning and deep learning-based approaches. One of the major challenges for such an automation is the manual effort that needs to be invested into labeling of training data. This is a concern for large software vendors who typically work on a variety of applications in diverse domains. We address this challenge by designing a pipeline that facilitates classification of NFRs using only a limited amount (~ 20% of an available new dataset) of labeled data for training. We (1) employed Snorkel to automatically label a dataset comprising NFRs from various Software Requirement Specification documents, (2) trained several classifiers using it, and (3) reused these pre-trained classifiers using a Transfer Learning approach to classify NFRs in industry-specific datasets. From among the various language model classifiers, the best results have been obtained for a BERT based classifier fine-tuned to learn the linguistic intricacies of three different domain-specific datasets from real-life projects. © 2021 IEEE.
"
10.1109/RE51729.2021.00060,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123207416&origin=inward,Conference Paper,SCOPUS_ID:85123207416,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),arf: automatic requirements formalisation tool,"
AbstractView references

Formal verification techniques enable the detection of complex quality issues within system specifications. However, the majority of system requirements are usually specified in natural language (NL). Manual formalisation of NL requirements is an error-prone and labour-intensive process requiring strong mathematical expertise, and can be infeasible for large numbers of requirements. Existing automatic formalisation techniques usually support heavily constrained natural language relying on requirement boilerplates or templates. In this paper, we introduce ARF: Automatic Requirements Formalisation Tool. ARF can automatically transform free-format natural language requirements into temporal logic based formal notations. This is achieved through two steps: 1) extraction of key requirement attributes into an intermediate representation (RCM: Requirement Capturing Model), and 2) transformation rules that convert requirements from the RCM format to formal notations. © 2021 IEEE.
"
10.1109/RE51729.2021.00032,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123187769&origin=inward,Conference Paper,SCOPUS_ID:85123187769,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rasaeco: requirements analysis of software for the aeco industry,"
AbstractView references

Digitalization is forging its path in the architecture, engineering, construction, operation (AECO) industry. This trend demands not only solutions for data governance but also sophisticated cyber-physical systems with a high variety of stakeholder background and very complex requirements. Existing approaches to general requirements engineering ignore the context of the AECO industry. This makes it harder for the software engineers usually lacking the knowledge of the industry context to elicit, analyze and structure the requirements and to effectively communicate with AECO professionals. To live up to that task, we present an approach and a tool for collecting AECO-specific software requirements with the aim to foster reuse and leverage domain knowledge. We introduce a common scenario space, propose a novel choice of an ubiquitous language well-suited for this particular industry and develop a systematic way to refine the scenario ontologies based on the exploration of the scenario space. The viability of our approach is demonstrated on an ontology of 20 practical scenarios from a large project aiming to develop a digital twin of a construction site. © 2021 IEEE.
"
10.1109/ICISCAE52414.2021.9590755,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123161955&origin=inward,Conference Paper,SCOPUS_ID:85123161955,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),interactive multimedia network teaching evaluation based on object segmentation algorithm,"
AbstractView references

With the deepening of teaching reform and the development of computer technology and application, multimedia network teaching has become the development direction of traditional teaching mode. In recent years, a large number of multimedia teaching software have emerged at home and abroad, most of which have relatively similar characteristics, such as audio/video interaction, sharing the whiteboard, teaching broadcast, with specific user roles and permission control, etc. The transmission and synchronization of multimedia stream is a research hotspot in real-time multimedia system. In this paper, the transmission protocol, delay model and synchronization strategy in multimedia stream network transmission are discussed deeply, and the calculation method of static buffer size of receiver is given. A new algorithm of dynamically changing playback rate according to the change of buffer is proposed to realize multimedia synchronization. On J2EE platform, the functions of distributed and developable network teaching system based on SOA architecture are realized. According to the requirements of unified modelling language UML in software engineering, the network teaching system was modelled, and various types of model description diagrams needed by the system were completed in Rational Rose, a UML development tool. According to the requirements of software engineering, the outline design and detailed design of the network teaching system based on UML and the concrete implementation in the object-oriented development platform are completed. It greatly improves the level and work efficiency of multimedia teaching and network teaching, and provides reliable technical support and strong technical support for teachers to carry out multimedia teaching activities smoothly in network classroom. © 2021 IEEE.
"
10.23919/MIPRO52101.2021.9596919,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123058268&origin=inward,Conference Paper,SCOPUS_ID:85123058268,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design and evaluation of a deep learning powered image regression sensor for control engineering applications,"
AbstractView references

This paper focuses on the study of Deep Learning based solutions in Computer Vision applications in Control Engineering. Machine Learning (ML) based Systems recently gained a lot of scientific traction, as the latest advances in complex fields like Computer Vision and Natural Language Processing proved their great capabilities in learning abstract and non-linear relationships. To demonstrate the power and operational capability of Machine-Learning algorithms within sensor-applications, an image regression model able to predict the angular position of a servo motor shaft, given only images as the sensors input will be designed and trained. The selected device is well known and used as a common actuator in many control applications. A large dataset consisting of 100.000 labelled images was necessary during training for the model to obtain its final prediction accuracy. The final model and its generalization capabilities underline the potential of Neural Networks to adapt themselves to engineering-tasks which are hardly achievable with traditional methods. The paper concludes with a statistical analysis of the trained sensor to gain some insights at the performance and behaviour of the derived model. © 2021 Croatian Society MIPRO.
"
10.1109/ICTC52510.2021.9621052,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122956638&origin=inward,Conference Paper,SCOPUS_ID:85122956638,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),network anomaly detection based on gan with scaling properties,"
AbstractView references

To protect the IT systems against network attacks in newly emerged network like 5G edge environments, the network intrusion detection system (IDS) has been widely used as the most important solution with effective defense methods. Most of IDS using machine learning have commonly employed the supervised learning approaches which surely need the labeled learning data. Also, in terms of the detection performance, the unsupervised learning method is generally not as good as the supervised learning method. Nevertheless, it is difficult to acquire the labeled network traffic data in real world. Therefore, in this paper, by employing the unsupervised learning, we propose network anomaly detector based on Generative Adversarial Network (GAN) with scaling properties. The detector consists of a property scaling module to improve the performance and anomaly detection module using GAN. For the effectiveness and feasibility of the system, we evaluated the performance using UNSW-NB15 dataset owing to limitation of obtaining real network traffic. In the future, we will apply the system to AI-based security platform to detect and predict the cyber threats in unlabeled network traffic of 5G edge network. © 2021 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122603149&origin=inward,Conference Paper,SCOPUS_ID:85122603149,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),what changes can large-scale language models bring? intensive study on hyperclova: billions-scale korean generative pretrained transformers,"
AbstractView references

GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications. © 2021 Association for Computational Linguistics
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121720003&origin=inward,Conference Paper,SCOPUS_ID:85121720003,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),enhancing abductive reasoning in design and engineering education via probabilistic knowledge; a case study in ai,"
AbstractView references

As we are moving into a knowledge-based economy, frameworks addressing the translational processes around value and impact permeate the development of educational curriculums in the design and engineering educational spectrum. In response to this approach, this paper presents an operational framework that explores how abductive reasoning, and its embodied probabilistic knowledge can bridge the gap between the challenges of accelerating technological development and current design and engineering educational practice. This is to enable students to locate, evaluate and work creatively with knowledge to generate new and improved solutions that can tackle uncertain and future real-world challenges, while delivering impact and value for society. In the process, we introduce probabilistic knowledge as the most adequate model to translate potentialities into impact and value. This repositioning enables practitioners to move beyond proving reality to a generative space aiming to transform it. In this context we present abductive reasoning as a fundamental approach to deal with directional and transformational potentialities to tackle uncertainties. © PDE 2021.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121677414&origin=inward,Conference Paper,SCOPUS_ID:85121677414,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an empirical study on data-driven requirements elicitation: reflections from nordic enterprises,"
AbstractView references

There is a plethora of digital data sources that may be exploited for collecting requirements for system development and evolution. In contrast to human sources, i.e. stakeholders, digital sources continuously generate data that is often not originally created for the purposes of requirements elicitation, e.g. on forums, microblogs, machine-generated trace logs, and sensor data. Streams of large volumes of data can be exploited to enable automation of a continuous requirements elicitation process using AI techniques that combine natural language or machine data processing, with machine learning. On the other hand, the complex characteristics of big data due to its size, lack of structure, high dynamics, and low predictability, present numerous challenges on the process of extracting requirements-related information that would be of a clear value for companies. The purpose of this interview study was to, from the practitioners' perspective, elicit their overall expectations and needs for a method for the elicitation of system requirements from digital data sources. Semi-structured interviews were conducted with several industrial experts from different business domains and the collected empirical data has been analyzed using thematic analysis. The results lead to the identification of a set of high-level requirements related to the method for the elicitation from digital data sources. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
10.22152/programming-journal.org/2021/5/3,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121128175&origin=inward,Article,SCOPUS_ID:85121128175,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),path-sensitive atomic commit: local coordination avoidance for distributed transactions,"
AbstractView references

Context Concurrent objects with asynchronous messaging are an increasingly popular way to struc-ture highly available, high performance, large-scale software systems. To ensure data-consistency and sup-port synchronization between objects such systems often use distributed transactions with Two-Phase Locking (2pl) for concurrency control and Two-Phase commit (2pc) as atomic commitment protocol. Inquiry In highly available, high-throughput systems, such as large banking infrastructure, however, 2pl becomes a bottleneck when objects are highly contended, when an object is queuing a lot of messages because of locking. Approach In this paper we introduce Path-Sensitive Atomic Commit (psac) to address this situation. We start from message handlers (or methods), which are decorated with pre-and post-conditions, describing their guards and effect. Knowledge This allows the psac lock mechanism to check whether the effect of two incoming messages at the same time are independent, and to avoid locking if this is the case. As a result, more messages are directly accepted or rejected, and higher overall throughput is obtained. Grounding We have implemented psac for a state machine-based DSL called Rebel, on top of a runtime based on the Akka actor framework. Our performance evaluation shows that psac exhibits the same scalability and latency characteristics as standard 2pl/2pc, and obtains up to 1.8 times median higher throughput in congested scenarios. Importance We believe psac is a step towards enabling organizations to build scalable distributed applica-tions, even if their consistency requirements are not embarrassingly parallel. ACM CCS 2012 Information systems → Distributed database transactions; Software and its engineering → Domain specific languages; State systems; Model-driven software engineering; Applied computing → Enterprise architectures; Event-driven architectures;. © Tim Soethout, Tijs van der Storm, and Jurgen J. Vinju.
"
10.1007/978-3-030-90963-5_30,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119888822&origin=inward,Conference Paper,SCOPUS_ID:85119888822,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"virtual control panel api: an artificial intelligence driven directive to allow programmers and users to create customizable, modular, and virtual control panels and systems to control iot devices via augmented reality","
AbstractView references

An incremental tide in IoT devices prompts a centralized control system where physical controls such as buttons and knobs can become virtual and centralized via augmented reality: especially headset AR. I conduct a literature mapping to identify gaps and create a front-end based virtual control panel API tool to allow programmers and end-users to rapidly create headset augmented reality-based virtual control systems for physical devices. I probe on how an artificial intelligence back-end aids in such a system in terms of panel creation and automation as well as panel usage and control scenarios. I delve into use cases of the virtual control panel API including mass usage and specialized scenarios. Furthermore, I specify the guidelines for a requirements elicitation study for such a system along with potential points of derivation. Next, I probe into implications and limitations of such a system in terms of artificial intelligence, usage, and precision. My work has implications for programmers and users of a virtual control system backed by artificial intelligence for the purpose of controlling physical IoT devices in mass usage as well as in specialized scenarios. © 2021, Springer Nature Switzerland AG.
"
10.1109/ACCESS.2021.3127344,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119421929&origin=inward,Article,SCOPUS_ID:85119421929,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),describing structure and complex interactions in multi-agent-based industrial cyber-physical systems,"
AbstractView references

The description of structure and complex interactions in Multi-agent-based Industrial Cyber-physical (MAS-ICPS) systems has been elusively addressed in the literature. Existing works, grounded on model-based engineering, have been successful at characterizing and solving system integration problems. However, they fail to describe accurately the collective and dynamic execution behaviour of large and complex industrial systems, particularly in more discrete production domains, such as: automotive, home appliances, aerospace, food and beverages, etc. In these domains, the execution flow diverts dynamically due to production disturbances, custom orders, fluctuations in demand in mixed model production, faults, quality-control and product rework, etc. These dynamic conditions require re-allocation and reconfiguration of production resources, redirection of production flows, re-scheduling of orders, etc. A meta-model for describing the structure and complex interactions in MAS-ICPS is defined in this paper. This contribution goes beyond the State-Of-The-Art (SOTA) as the proposed meta-model describes structure, as many other literature contributions, but also describes the execution behaviour of arbitrarily complex interactions. The previous is achieved with the introduction of general execution flow control operators in the meta-model. These operators cover, among other aspects, delegation of the execution flow and dynamic decision making. Additionally, the contribution also goes beyond the SOTA by including validation mechanisms for the models generated by the meta-model. Finally, the contribution adds to the current literature by providing a meta-model focusing on production execution and not just on describing the structural connectivity aspects of ICPSs. © 2013 IEEE.
"
10.1007/978-981-16-7512-6_6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119344897&origin=inward,Conference Paper,SCOPUS_ID:85119344897,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),semantic-aware deep neural attention network for machine translation detection,"
AbstractView references

Web crawling is an important way to collect a massive training corpus for building a high-quality machine translation system. However, a large amount of data collected comes from machine-translated texts rather than native speakers or professional translators, severely reducing the benefit of data scale. Traditional machine translation detection methods generally require human-crafted feature engineering and are difficult to distinguish the fine-grained semantic difference between real text and pseudo text from a modern neural machine translation system. To address this problem, we propose two semantic-aware models based on the deep neural network to automatically learn semantic features of text for monolingual scenarios and bilingual scenarios, respectively. Specifically, our models incorporate the global semantic from BERT and the local semantic from convolutional neural network together for monolingual detection and further explores the semantic consistency relationship for bilingual detection. The experimental results on the Chinese-English machine translation detection task show that our models achieve 83.12% F1 in the monolingual detection and 85.53% F1 in the bilingual detection respectively, which is better than the strong BERT baselines by 2.2–3.2%. © 2021, Springer Nature Singapore Pte Ltd.
"
10.1109/ACCESS.2021.3124628,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118617707&origin=inward,Article,SCOPUS_ID:85118617707,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),robust ensemble machine learning model for filtering phishing urls: expandable random gradient stacked voting classifier (erg-svc),"
AbstractView references

As cyber-attacks grow fast and complicated, the cybersecurity industry faces challenges to utilize state-of-the-art technology and strategies to battle the consistently present malicious threats. Phishing is a sort of social engineering attack produced technically and classified as identity theft and complicated attack vectors to steal information of internet users. In this perspective, our main objective of this study is to propose a unique, robust ensemble machine learning model architecture that provides the highest prediction accuracy with a low error rate while proposing few other robust machine learning models. Both supervised and unsupervised techniques were used for the detection process. For our experiments, seven classification algorithms, one clustering algorithm, two ensemble techniques, and two large standard legitimate datasets with 73,575 URLs and 100,000 URLs were used. Two test modes (percentage split, K-Fold cross-validation) were utilized for conducting experiments and final predictions. Mechanisms were developed to (I) identify the best N, which is the optimal heuristic-based threshold value for splitting words into subwords for each classifier, (II) tune hyperparameters for each classifier to specify the best parameter combination, (III) select prominent features using various feature selection techniques, (IV) propose a robust ensemble model (classifier) called the Expandable Random Gradient Stacked Voting Classifier (ERG-SVC) utilizing a voting classifier along with a model architecture, (V) analyze possible clusters of the dataset using k-means clustering, (VI) thoroughly analyze the gradient boost classifier (GB) with respect to utilizing the 'criterion' parameter with the Mean Absolute Error (MAE), Mean Squared Error (MSE), and Friendman_MSE, and(VII) propose a lightweight preprocessor to reduce computational cost and preprocessing time. Initial experiments were carried out with 46 features; the number of features was reduced to 22 after the experiments. The results show that the GB classifier outperformed with the least number of NLP based features by achieving a 98.118% prediction accuracy. Furthermore, our stacking ensemble model and proposed voting ensemble model (ERG-SVC) outperformed other tested approaches and yielded reliable prediction accuracy results in detecting malicious URLs at rates of 98.23% and 98.27%, respectively. © 2013 IEEE.
"
10.1155/2021/5006974,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118589832&origin=inward,Article,SCOPUS_ID:85118589832,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),applying deep learning technologies to evaluate the patent quality with the collaborative training,"
AbstractView references

As the country vigorously promotes the development of science and technology and tries to enhance independent innovation capabilities, more and more attention is paid on the protection of technology ownership. In recent years, China has developed rapidly in many scientific and technological fields, and the number of patent applications increased year by year. However, various patent quality problems including immature patent technology and low patent authorization rate appear. The indicators of patent quantification and quality evaluation are studied in this paper. First, we quantify the patent quality evaluation indicators and combine the content of the patent text to build a patent evaluation model. US patents with patent grade labels are used for training with multitask learning technology. Second, the evaluation model is transferred from the English patents to the Chinese patents, in which the active learning technology and transfer learning technology are used to minimize the work of manual labeling. Finally, a Chinese patent quality evaluation model based on collaborative training was designed and implemented. Methods used in this experiment have notably improved the prediction effect of the model and achieved a better migration effect. A large number of experimental results show that the Chinese patent quality evaluation model has achieved good evaluation results. This research uses deep learning and natural language processing technology to carry out research on patent quality evaluation models from different perspectives, to provide patent decision support for related companies, and to point out research directions for research institutions and patent inventors. © 2021 Xindong You et al.
"
10.12688/wellcomeopenres.16466.2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118577436&origin=inward,Article,SCOPUS_ID:85118577436,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"reproducible parallel inference and simulation of stochastic state space models using odin, dust, and mcstate","
AbstractView references

State space models, including compartmental models, are used to model physical, biological and social phenomena in a broad range of scientific fields. A common way of representing the underlying processes in these models is as a system of stochastic processes which can be simulated forwards in time. Inference of model parameters based on observed time-series data can then be performed using sequential Monte Carlo techniques. However, using these methods for routine inference problems can be made difficult due to various engineering considerations: allowing model design to change in response to new data and ideas, writing model code which is highly performant, and incorporating all of this with up-to-date statistical techniques. Here, we describe a suite of packages in the R programming language designed to streamline the design and deployment of state space models, targeted at infectious disease modellers but suitable for other domains. Users describe their model in a familiar domain-specific language, which is converted into parallelised C++ code. A fast, parallel, reproducible random number generator is then used to run large numbers of model simulations in an efficient manner. We also provide standard inference and prediction routines, though the model simulator can be used directly if these do not meet the user's needs. These packages provide guarantees on reproducibility and performance, allowing the user to focus on the model itself, rather than the underlying computation. The ability to automatically generate high-performance code that would be tedious and time-consuming to write and verify manually, particularly when adding further structure to compartments, is crucial for infectious disease modellers. Our packages have been critical to the development cycle of our ongoing real-time modelling efforts in the COVID-19 pandemic, and have the potential to do the same for models used in a number of different domains. © 2021 FitzJohn RG et al.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118238500&origin=inward,Conference Paper,SCOPUS_ID:85118238500,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),applying i∗ in conceptual modelling in machine learning,"
AbstractView references

The i∗ framework is a popular and well-equipped technique for capturing the organizational environment and requirements of a system. However, i∗ heavily depends on the designer experience to cope with the idiosyncrasy of each specific field. While the machine learning field would benefit from a requirements representation, its complexity makes it unfeasible to directly use i∗. The large number of constructs and nuances between elements puts a severe strain on the designer, leading to the creation of error-prone models. Therefore, in order to tackle this problem, we present an extension of i∗. Our proposal covers the main gaps between machine learning and conceptual modeling with the aim of creating a suitable baseline methodology for machine learning requirements engineering. The advantage of our proposal is that our language specifies the main elements involved in machine learning models and constrains their interactions, filtering invalid designs and thus reducing the burden of knowledge while making the process less error-prone. © 2021 CEUR-WS. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85117573564&origin=inward,Conference Paper,SCOPUS_ID:85117573564,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),requirement analysis with sysml for concept design of offshore wind farm grid connection,"
AbstractView references

Energy networks are continuously changing and expanding, e.g. due to the integration of renewable energies. For example, many offshore wind farms (OWF) have been built and are planned in the near future. This makes the development of reliable and cost-efficient grid connection systems important. High voltage direct current (HVDC) is a suitable technology for connecting large OWFs far away from shore. Many HVDC based grid connection topologies are possible. Therefore, a systematic development methodology for complex systems based on a holistic approach is required. As part of this research, Model Based Systems Engineering (MBSE) integrating the System Modeling Language (SysML) is applied for the development of a HVDC grid connection for OWFs. With this methodology it is possible to develop a detailed system model for a case study step by step, considering stakeholder objectives, constraints and requirements. SysML allows the creation of an abstract system model allowing an appropriate design in the early project phase while avoiding costly late changes. This paper focuses as a first step on the requirement engineering with a subsequent conceptual design of the grid connection system and provides a levelized cost of energy (LCOE) improved and efficient system concept as a result. © VDE VERLAG GMBH. Berlin. Offenbach
"
10.1155/2021/5935958,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85117371621&origin=inward,Article,SCOPUS_ID:85117371621,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sublemma-based neural machine translation,"
AbstractView references

Powerful deep learning approach frees us from feature engineering in many artificial intelligence tasks. The approach is able to extract efficient representations from the input data, if the data are large enough. Unfortunately, it is not always possible to collect large and quality data. For tasks in low-resource contexts, such as the Russian ← Vietnamese machine translation, insights into the data can compensate for their humble size. In this study of modelling Russian ← Vietnamese translation, we leverage the input Russian words by decomposing them into not only features but also subfeatures. First, we break down a Russian word into a set of linguistic features: part-of-speech, morphology, dependency labels, and lemma. Second, the lemma feature is further divided into subfeatures labelled with tags corresponding to their positions in the lemma. Being consistent with the source side, Vietnamese target sentences are represented as sequences of subtokens. Sublemma-based neural machine translation proves itself in our experiments on Russian-Vietnamese bilingual data collected from TED talks. Experiment results reveal that the proposed model outperforms the best available Russian ← Vietnamese model by 0.97 BLEU. In addition, automatic machine judgment on the experiment results is verified by human judgment. The proposed sublemma-based model provides an alternative to existing models when we build translation systems from an inflectionally rich language, such as Russian, Czech, or Bulgarian, in low-resource contexts. © 2021 Thien Nguyen et al.
"
10.1007/978-3-030-86062-2_37,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115820896&origin=inward,Conference Paper,SCOPUS_ID:85115820896,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),extracting interactive actor-based dataflow models from legacy c code,"
AbstractView references

Graphical actor-based models provide an abstract overview of the flow of data in a system. They are well-established for the model-driven engineering (MDE) of complex software systems and are supported by numerous commercial and academic tools, such as Simulink, LabVIEW or Ptolemy. In MDE, engineers concentrate on constructing and simulating such models, before application code (or at least a large fraction thereof) is synthesized automatically. However, a significant fraction of today’s legacy system has been coded directly, often using the C language. High-level models that give a quick, accurate overview of how components interact are often out of date or do not exist. This makes it challenging to maintain or extend legacy software, in particular for new team members. To address this problem, we here propose to reverse the classic synthesis path of MDE and to synthesize actor-based dataflow models automatically from source code. Here functions in the code get synthesized into nodes that represent actors manipulating data. Second, we propose to harness the modeling-pragmatic approach, which considers visual models not as static artefacts, but allows interactive, flexible views that also link back to textual descriptions. Thus we propose to synthesize actor models that can vary in level of detail and that allow navigation in the source code. To validate and evaluate our proposals, we implemented these concepts for C analysis in the open source, Eclipse-based KIELER project and conducted a small survey. © 2021, The Author(s).
"
10.1109/ACCESS.2021.3115659,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115803278&origin=inward,Article,SCOPUS_ID:85115803278,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),spacetransformers: language modeling for space systems,"
AbstractView references

The transformers architecture and transfer learning have radically modified the Natural Language Processing (NLP) landscape, enabling new applications in fields where open source labelled datasets are scarce. Space systems engineering is a field with limited access to large labelled corpora and a need for enhanced knowledge reuse of accumulated design data. Transformers models such as the Bidirectional Encoder Representations from Transformers (BERT) and the Robustly Optimised BERT Pretraining Approach (RoBERTa) are however trained on general corpora. To answer the need for domain-specific contextualised word embedding in the space field, we propose SpaceTransformers, a novel family of three models, SpaceBERT, SpaceRoBERTa and SpaceSciBERT, respectively further pre-trained from BERT, RoBERTa and SciBERT on our domain-specific corpus. We collect and label a new dataset of space systems concepts based on space standards. We fine-tune and compare our domain-specific models to their general counterparts on a domain-specific Concept Recognition (CR) task. Our study rightly demonstrates that the models further pre-trained on a space corpus outperform their respective baseline models in the Concept Recognition task, with SpaceRoBERTa achieving significant higher ranking overall. © 2013 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115694810&origin=inward,Conference Paper,SCOPUS_ID:85115694810,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),effective batching for recurrent neural network grammars,"
AbstractView references

As a language model that integrates traditional symbolic operations and flexible neural representations, recurrent neural network grammars (RNNGs) have attracted great attention from both scientific and engineering perspectives. However, RNNGs are known to be harder to scale due to the difficulty of batched training. In this paper, we propose effective batching for RNNGs, where every operation is computed in parallel with tensors across multiple sentences. Our PyTorch implementation effectively employs a GPU and achieves x6 speedup compared to the existing C++ DyNet implementation with model-independent auto-batching. Moreover, our batched RNNG also accelerates inference and achieves x20-150 speedup for beam search depending on beam sizes. Finally, we evaluate syntactic generalization performance of the scaled RNNG against the LSTM baseline, based on the large training data of 100M tokens from English Wikipedia and the broad-coverage targeted syntactic evaluation benchmark. © 2021 Association for Computational Linguistics
"
10.1007/978-3-030-86970-0_15,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115693890&origin=inward,Conference Paper,SCOPUS_ID:85115693890,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),systematic literature review on service oriented architecture modeling,"
AbstractView references

Context: With the recent trend of shifting from traditional architectures towards Service Oriented Architectures (SOA), an enterprise can create, choreograph new business functions, deploy and integrate multiple services that communicate with each other using service interfaces to pass messages from one service to another. So, to build an SOA that accommodate business scalability, and flexibility and facilitate ongoing and changing needs of business, an SOA modeling language is required. Objective: The purpose of this work is to determine the current state of the art in the field of SOA modeling shedding light on techniques have been used to model a SOA, its importance and domains where it is applied. Method: In order to fulfill the objective of the research, the method which we choose was a Systematic Literature Review (SLR). This served in collecting and structuring the information that exists in the field of SOA modeling. Results: Service oriented applications have been modeled with different modeling languages. Choosing a suitable modeling language is depending on the criteria for what is being modeled; if is it structural view point or behavioral aspects or even both. Conclusion: Summing up the results, it can be concluded firstly that modeling mitigates the complexity of huge and complicated data of service oriented applications. Secondly, the research that has been performed has shown that the Unified Modeling Language (UML), Business Process Modeling Notation (BPMN), Service Component Architecture (SCA) and Event-B are the most modeling languages used on large scale. © 2021, Springer Nature Switzerland AG.
"
10.1007/978-3-030-85347-1_20,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115244647&origin=inward,Conference Paper,SCOPUS_ID:85115244647,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a comparison of different source code representation methods for vulnerability prediction in python,"
AbstractView references

In the age of big data and machine learning, at a time when the techniques and methods of software development are evolving rapidly, a problem has arisen: programmers can no longer detect all the security flaws and vulnerabilities in their code manually. To overcome this problem, developers can now rely on automatic techniques, like machine learning based prediction models, to detect such issues. An inherent property of such approaches is that they work with numeric vectors (i.e., feature vectors) as inputs. Therefore, one needs to transform the source code into such feature vectors, often referred to as code embedding. A popular approach for code embedding is to adapt natural language processing techniques, like text representation, to automatically derive the necessary features from the source code. However, the suitability and comparison of different text representation techniques for solving Software Engineering (SE) problems is rarely studied systematically. In this paper, we present a comparative study on three popular text representation methods, word2vec, fastText, and BERT applied to the SE task of detecting vulnerabilities in Python code. Using a data mining approach, we collected a large volume of Python source code in both vulnerable and fixed forms that we embedded with word2vec, fastText, and BERT to vectors and used a Long Short-Term Memory network to train on them. Using the same LSTM architecture, we could compare the efficiency of the different embeddings in deriving meaningful feature vectors. Our findings show that all the text representation methods are suitable for code representation in this particular task, but the BERT model is the most promising as it is the least time consuming and the LSTM model based on it achieved the best overall accuracy (93.8%) in predicting Python source code vulnerabilities. © 2021, Springer Nature Switzerland AG.
"
10.1007/978-3-030-85347-1_26,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115244020&origin=inward,Conference Paper,SCOPUS_ID:85115244020,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),expressing structural temporal properties of safety critical hierarchical systems,"
AbstractView references

Software-intensive safety critical systems are becoming more and more widespread and are involved in many aspects of our daily lives. Since a failure of these systems could lead to unacceptable consequences, it is imperative to guarantee high safety standards. In practice, as a way to handle their increasing complexity, these systems are often modelled as hierarchical systems. To date, a good deal of work has focused on the definition and analysis of hierarchical modelling languages and on their integration within model-driven development frameworks. Less work, however, has been directed towards formalisms to effectively express, in a precise and rigorous way, relevant behavioural properties of such systems (e.g.: safety requirements). In this work, we propose a novel extension of classic Linear Temporal Logic (LTL) called Hierarchical Linear Temporal Logic (HLTL), designed to express, in a natural yet rigorous way, behavioural properties of hierarhical systems. The formalism we propose does not commit to any specific modelling language, and can be used to predicate over a large variety of hierarchical systems. © 2021, Springer Nature Switzerland AG.
"
10.5171/2021.265538,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85113753801&origin=inward,Article,SCOPUS_ID:85113753801,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),software engineering techniques for the extraction of ontology of historical geographic information,"
AbstractView references

Unorganized processing textual information in large files is time consuming and error-prone, especially if the texts are ununiform. Investigating past and historical data concerning geography and economics can be facilitated a lot when the data is stored in a data base or has a proper form. There was a lot of research concerning the transformation of textual data to an ontology in a methodical way. In this paper, a method is presented for the extraction of geographic and economic information from a historic Lexicon. The method is based on software engineering techniques and follows an iterative cycle. It results in a well-defined ontology specified using UML class diagrams which can be queried using Object Constraint Language. An evaluation of the model is presented. This research facilitates the comparison of various historical stages of development. Thus, it helps in assessing regional development, qualitative and structural changes in the regional economy, ownership, infrastructure as well as living standard transformation. Copyright © 2021. Piotr Kosiuczenko. Distributed under Creative Commons Attribution 4.0 International CC-BY 4.0
"
10.18293/DMSVIVA2021-011,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112744886&origin=inward,Conference Paper,SCOPUS_ID:85112744886,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ut-atd: universal transformer for anomalous trajectory detection by embedding trajectory information,"
AbstractView references

Due to the development of the transportation industry, a large amount of trajectory data is pouring into the Internet all the time. Based on these trajectory data, anomalous trajectory detection technology provides great support for traffic safety assurance and traffic risk prediction. Most existing anomalous trajectory detection methods are based on trajectory's physical characteristics or representation learning, and they achieve good performance in a few scenarios. But they still face the following problems. (1) The imperfect utilization of trajectory points. (2) The sparsity of trajectory data, which leads to generalization issues. (3) Longer model training time consumed, which can't adapt to the large amount of trajectory data generated every day. To solve the above problems, we propose a novel anomalous trajectory detection model based on Universal Transformer, called UT-ATD. UT-ATD captures the information of trajectory positions by learning trajectory embedding for classification. UT-ATD has a faster training speed, relatively few model parameters, and sufficient portability, which are ideal for the realistic scene requirements. Our model achieves state-of-the-art performance in most aspects, and its effectiveness is verified by a series of experiments on the real-world taxi trajectory dataset. © DMSVIVA 2021.All right reserved.
"
10.1155/2021/1874584,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112284471&origin=inward,Article,SCOPUS_ID:85112284471,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),evaluation model of college english multimedia teaching effect based on deep convolutional neural networks,"
AbstractView references

With the acceleration of global integration, the demand for English instruction is increasingly rising. On the other hand, Chinese English learners struggle to learn spoken English due to the limited English learning environment and teaching conditions in China. The advancement of artificial intelligence technology and the advancement of language teaching and learning techniques have ushered in a new era of language learning and teaching. Deep learning technology makes it possible to solve this problem. Speech recognition and assessment technology are at the heart of language learning, and speech recognition technology is the foundation. Because of the complex changes in speech pronunciation, a large amount of speech signal data, the high dimension of speech characteristic parameters, and a large amount of speech recognition and evaluation computation, the large volume of speech signal processing requires higher requirements of hardware and software resources and algorithms. However, traditional speech recognition algorithms, such as dynamic time-warped algorithms, hidden Markov models, and artificial neural networks, have their advantages and disadvantages. They have encountered unprecedented bottlenecks, so it is difficult to improve their accuracy and speed. To solve these problems, this paper focuses on evaluating the multimedia teaching effect of college English. A multilevel residual convolutional neural network algorithm for oral English pronunciation recognition is proposed based on a deep convolutional neural network. The experiments show that our algorithm can assist learners in identifying inconsistencies between their pronunciation and standard pronunciation and correcting pronunciation errors, resulting in improved oral English learning performance. © 2021 Limei Geng.
"
10.1007/978-3-030-77772-2_26,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112154845&origin=inward,Conference Paper,SCOPUS_ID:85112154845,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),klsi methods for human simultaneous interpretation and towards building a simultaneous machine translation system reflecting the klsi methods,"
AbstractView references

Simultaneous machine translation aims to maintain translation quality while minimizing the delay between reading input and incrementally producing the output. KL Simultaneous Interpreting (KLSI) is a set of methods developed to deliver non-revisable translation from English into Chinese with one second as the benchmark latency. To achieve this, it trains the human brain to stop thinking and execute commands instead. It has developed a range of formulaic techniques to be applied mechanically. This paper presents some of the key features and techniques of KLSI and explores its implications for machine simultaneous interpreting. The techniques include convergence, the concept of interpreting within three words heard at any given moment in time, co-texting, defaulting, and sequential translation techniques such as repeat, replace, reverse logic, and SAI (Skip, Add, Insert). Multiple English-to-Chinese examples and video recordings are listed in the paper to illustrate the KLSI features and techniques. In the second part of this paper, we describe computational methods related to KLMI techniques: wait-k policy with and without anticipation; word-based and phrase-based alignment and mapping; constrained context for neural machine translation. Commercial machine translation requires at least several gigabytes (or millions of words) of language pair documents for the training. It is not realistic to obtain enough parallel texts in English and Chinese reflecting KLSI techniques for training a neural machine translation system. However, a lot of parallel texts exist that do not reflect KLSI techniques. We propose to use a rule-based approach to modify available parallel texts using KLSI rules to generate a large enough KLSI-based corpus. The main contribution of this paper is to propose a novel rule-based approach―with the rules reflecting human interpretation traits—to revise training corpus to enable short latency and non-revisable machine translation. © 2021, Springer Nature Switzerland AG.
"
10.5220/0010549200830091,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111725733&origin=inward,Conference Paper,SCOPUS_ID:85111725733,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards the formal modeling methodology of wsn through the transformation of sysml into dspns,"
AbstractView references

When developing critical and complex systems, the requirement of the systems design verification is paramount. We address the problem of how to design these ones in order to satisfy their requirements. Wireless Sensor Networks (WSNs) are examples of such systems, which consist of a large amount of distributed and autonomous nodes. We aim to propose a Model-Based Systems Engineering specification and verification methodology for designing WSNs. The proposed approach uses SysML language to describe the WSNs requirements, behaviors and performance parameters. Then, it translates the SysML elements to a Deterministic Stochastic Petri Net (DSPNs) and integrates them into an analytic model. This allows designing WSNs and studying their behaviors and their performances, namely energy consumption. The current paper refines the first part of this project by transforming the activity diagram of SysML to a DSPN. To show the applicability of the mapping technique, a case study that presents a hierarchical WSN is used. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.
"
10.1109/ACCESS.2021.3100686,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111555920&origin=inward,Article,SCOPUS_ID:85111555920,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),unsupervised semantic mapping for healthcare data storage schema,"
AbstractView references

Data, information, and knowledge processing systems, in the domain of healthcare, are currently plagued by heterogeneity at various levels. Current solutions have focused on developing a standard-based, manual intervention mechanism, which requires a large number of human resources and necessitates the realignment of existing systems. State-of-the-art methodologies in the field of natural language processing and machine learning can help to partially automate this process, reducing the resource requirements and providing a relatively good multi-class-based classification algorithm. We present a novel methodology for bridging the gap between various healthcare data management solutions by leveraging the strength of transformer-based machine learning models, to create mappings between the data elements. Additionally, the annotated data, collected against five medical schemas and labeled by four annotators is made available for helping future researchers. Our results indicate, that for biased, dependent multi-class text classification, transformer-based models provide better results than linguistic and other classical models. In particular, the Robustly Optimized BERT Pretraining Approach (RoBERTa) provides the best schema matching performance by achieving a Cohen's kappa score of 0.47 and Matthews Correlation Coefficient (MCC) score of 0.48, with human-annotated data. © 2013 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85110412324&origin=inward,Conference Paper,SCOPUS_ID:85110412324,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),how data scientists improve generated code documentation in jupyter notebooks,"
AbstractView references

Generative AI models are capable of creating high-fidelity outputs, sometimes indistinguishable from what could be produced by human effort. However, some domains possess an objective bar of quality, and the probabilistic nature of generative models suggests that there may be imperfections or flaws in their output. In software engineering, for example, code produced by a generative model may not compile, or it may contain bugs or logical errors. Various models of human-AI interaction, such as mixed-initiative user interfaces, suggest that human effort ought to be applied to a generative model's outputs in order to improve its quality. We report results from a controlled experiment in which data scientists used multiple models-including a GNN-based generative model-to generate and subsequently edit documentation for data science code within Jupyter notebooks. In analyzing their edit-patterns, we discovered various ways that humans made improvements to the generated documentation, and speculate that such edit data could be used to train generative models to not only identify which parts of their output might require human attention, but also how those parts could be improved. ©2021 Copyright 2021 for this paper by its authors.
"
10.1016/j.procir.2021.05.015,S221282712100473X,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107868894&origin=inward,Conference Paper,SCOPUS_ID:85107868894,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using sysml to support impact analysis on structural dynamics simulation models,"Changes during the product development process pose a high risk of unexpected high implementation effort. To reduce this risk, the impact of engineering changes have to be estimated in advance before their implementation. However, in large systems, the numerous interdependencies often create challenges to the approximation of the implementation effort by inducing complex interdependencies which are often not easy to identify from the start. This paper therefore proposes a method to quickly determine change efforts of complex systems. To be able to estimate change efforts, the dependencies between system components have to be mapped together with their corresponding modeling effort (e.g. modeling time). The SysML (System Modeling Language) system model is a powerful tool to support this process as it offers the possibility of a very individual and versatile data storage. Furthermore, specific data (e.g. calculation time) can quickly be identified with algorithm such as “metachain navigation”. We propose in this paper to use a SysML system model for an advanced documentation of the product and construction structure to be carried out in parallel to the modeling process of a system simulation model (in this case finite-element- and multi-body-simulation). Assemblies and associated components are represented by the product structure, while the construction structure represents the force flow between two components. Furthermore, in SysML individual data, e.g. modeling time and calculation time, is stored in an instance that is connected to a specific component or assembly. As a demonstration example, the approach is shown on a structural dynamic analysis of a powertrain with seven subassemblies and over 100 components. Following the proposed method, the effort of a specific component change of the powertrain can be estimated quickly due to the stored data in the system model."
10.30880/ijie.2021.13.04.007,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107749133&origin=inward,Article,SCOPUS_ID:85107749133,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),categorizing natural language-based customer satisfaction: an implementation method using support vector machine and long short-term memory neural network,"
AbstractView references

Analyzing natural language-based Customer Satisfaction (CS) is a tedious process. This issue is practically true if one is to manually categorize large datasets. Fortunately, the advent of supervised machine learning techniques has paved the way toward the design of efficient categorization systems used for CS. This paper presents the feasibility of designing a text categorization model using two popular and robust algorithms – the Support Vector Machine (SVM) and Long Short-Term Memory (LSTM) Neural Network, in order to automatically categorize complaints, suggestions, feedbacks, and commendations. The study found that, in terms of training accuracy, SVM has best rating of 98.63% while LSTM has best rating of 99.32%. Such results mean that both SVM and LSTM algorithms are at par with each other in terms of training accuracy, but SVM is significantly faster than LSTM by approximately 35.47s. The training performance results of both algorithms are attributed on the limitations of the dataset size, high-dimensionality of both English and Tagalog languages, and applicability of the feature engineering techniques used. Interestingly, based on the results of actual implementation, both algorithms are found to be 100% effective in accurately predicting the correct CS categories. Hence, the extent of preference between the two algorithms boils down on the available dataset and the skill in optimizing these algorithms through feature engineering techniques and in implementing them toward actual text categorization applications. © Universiti Tun Hussein Onn Malaysia Publisher’s Office
"
10.1016/j.matpr.2020.10.559,S2214785320381827,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107330627&origin=inward,Conference Paper,SCOPUS_ID:85107330627,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),visualization of virtual environment through labview platform,"
                  Virtual instrumentation is the use of customizable software and modular measurement hardware to create user-defined measurement systems, called virtual instruments. The primary difference between hardware instrumentation and virtual instrumentation is that LabVIEW software is used to replace a large amount of hardware. The software enables complex and expensive hardware to be replaced by already purchased computer hardware. Virtual Instrumentation is established using LabVIEW. Laboratory Virtual Instrument Engineering Workbench (LabVIEW) is a system-design platform and development environment for a visual programming language from National Instruments. The graphical language is named “G”; not to be confused with G-code. Originally released for the Apple Macintosh in 1986, LabVIEW is commonly used for data acquisition, instrument control, and industrial automation on a variety of Operating Systems (OSs), including Microsoft Windows, various versions of Unix, Linux, and MacOS. Simple timer Circuit, Traffic Light Control System, Automotive Safety assurance, Speed, Direction control of DC motor and Library Database File Management are realized by implementing VI programs. Select switch, Looping concepts, Case and Flat Sequence structures, String Manipulation, Clusters, File IO systems and Arduino interfacing through DAQ are used to implement all Simulation Models. Software Developed timer circuit and Automotive Safety assurance overcome the accuracy problem of conventional circuits. Designed Library Management system enhances ease of tracking. Conventional control of DC motor relies on hardware accessories such as Field and Armature rheostats and it requires manual implementation, whereas Virtual instrumentation makes software based control and it improves the system reliability. Fetching of Real time data and manipulation addresses the flexibility issue of hardware based Traffic Light Control system.
               "
10.1007/978-3-030-72651-5_55,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107325246&origin=inward,Conference Paper,SCOPUS_ID:85107325246,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a knowledge management approach supporting model-based systems engineering,"
AbstractView references

Model-based Systems Engineering (MBSE) is a noval approach to support complex system development by formalizing system artifacts and development using models. Though MBSE models provide a completely structural formalisms about system development for system developers, such large of domain specific knowledge represented by models cannot be captured as what the developers expect. This leads to a big challenge when MBSE can be widely used for complex system development. In this paper, a knowledge management approach is proposed to support an intelligent question answering scenario when implementing MBSE in system lifecycle. We make use of the GOPPRR approach to support MBSE formalisms which are transformed to knowledge graph models. Then such models provide cues for intelligent question answers through reasoning. In the case study, we make use of an auto-braking system scenario to develop MBSE models and to implement the intelligent question answering. Finally, we find the availability of our approach is evaluated which the domain engineers enable to capture their domain knowledge more efficiently. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.1016/j.procs.2021.03.094,S1877050921007341,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85106671805&origin=inward,Conference Paper,SCOPUS_ID:85106671805,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),applying an mda-based approach for enhancing the validation of business process models,"Business process modeling is a key activity during the development of complex and large information systems, such as enterprise management systems. These systems deal with a wide number of business processes; thus, the modeling and validation of processes becomes a challenging task. This entails dealing with issues such as the precedence between tasks and activities within a process, as well as resources, roles and enterprise assets involved. Moreover, undetected mistakes in this phase will be propagated to the system design phase and consequently will have a negative effect in the final system quality. On the other hand, the scientific literature advocates the suitability of formal models to address some issues during the process modeling. However, the adoption of formal models leads to new problems because formal languages are difficult to understand and process stakeholders usually lack of knowledge about them. In that direction, the Model-Driven Architecture (MDA) paradigm includes specifications that may alleviate some difficulties in the adoption of formal languages. Hence, in this paper we introduce an approach which combines MDA-specifications and ontologies to support process modeling. These technologies have great acceptance between both software researchers and developers. The use of ontologies permits to semantically validate the models. Furthermore, the application of MDA-guidelines could facilitate the integration of BPMN, a graphical notation for describing business process models widely accepted among business analysts, with a formal language to automate the analysis of business process models."
10.1007/978-3-030-74814-2_2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105888551&origin=inward,Conference Paper,SCOPUS_ID:85105888551,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),application analysis of computer 3d modeling technology in ship design,"
AbstractView references

With the rapid development of computer technology, people’s life has become more and more convenient, and it is widely used in all fields. Three dimensional modeling of ship structure is a very important research direction in ship design. With the development of computer-aided three-dimensional modeling software, the realization of rapid and intelligent hull structure design is of great and practical significance, which can improve the efficiency of ship design, speed up the progress of shipbuilding, and enhance the overall competitiveness of the shipbuilding industry. The traditional modeling method of hull structure 3D modeling is to build a small part of the hull segment with the help of some professional 3D modeling software. The workload is very large, not only need to consume a lot of human resources and material resources, but also low efficiency, also need to use skilled design software personnel. In this paper, solidwoks 3D design software, as a secondary development technology, is used as the platform of graphic expression. In the design process, microsor file is used to store, visual basic language is used to program, and several problems of 3D modeling of ship structure are studied. The 3D modeling software system of hull structure is developed to realize the rapid design of hull structure and generate the 3D model and engineering drawings of the target ship. This paper introduces a method of ship design automation based on three-dimensional modeling technology. It aims to study the method of quickly building three-dimensional model of ship structure, provide a set of three-dimensional design software which is in line with design specifications, simple to use and complete in function for ship structure design, and provide a new method for other three-dimensional modeling problems in ship design. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
10.5381/jot.2021.20.1.a1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103620507&origin=inward,Article,SCOPUS_ID:85103620507,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),evolution of bad smells in labview graphical models,"
AbstractView references

Bad smells often indicate potential problems in software, which may lead to long-term challenges and expensive maintenance efforts. Although bad smells often occur in source code, bad smells also exist in representations of design descriptions and models. We have observed that many users of graphical modeling environments (e.g., LabVIEW) are systems engineers who may not be aware of core software engineering techniques, such as refactoring of bad smells. Systems engineers often focus on implementation correctness and may be unaware of how their designs affect long-term maintenance properties that may increase design smells. There exists a large body of research focused on analysing bad smells embedded in the source code of textual languages, but there has been limited research on bad smells in systems models of graphical languages. In this paper, we present a semi-automated approach for extracting design smells across versions of LabVIEW graphical models through user-defined queries. We describe example queries that highlight the emergence of design smells that we discovered from posts in the LabVIEW user’s forum. We then demonstrate the use of the example queries in understanding the evolution of seven bad smells we found in 81 LabVIEW models stored in 10 GitHub repositories. We analyze the evolution of these smells in order to understand the prevalence and introduction of bad smells, as well as the relationship between bad smells and the structural changes made to the models. Our results show that all of the models contain instances of at least one type of bad smell and the number of smells fluctuates as the size of a model increases. Furthermore, the majority of the structural changes across different versions of LabVIEW models involve the addition of new elements with a corresponding increase in the presence of design smells. This paper summarizes the need for better analysis of design smells in systems models and suggests an approach that may assist in improving the structure and quality of systems models developed in LabVIEW. © 2021, Journal of Object Technology. All rights reserved
"
10.1007/978-3-030-67731-2_34,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85101517307&origin=inward,Conference Paper,SCOPUS_ID:85101517307,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using process models to understand security standards,"
AbstractView references

Many industrial software development processes today have to comply with security standards such as the IEC 62443-4-1. These standards, written in natural language, are ambiguous and complex to understand. This is especially true for non-security experts. Security practitioners thus invest much effort into comprehending standards and, later, into introducing them to development teams. However, our experience in the industry shows that development practitioners might very well also read such standards, but nevertheless end up inviting experts for interpretation (or confirmation). Such a scenario is not in tune with current trends and needs of increasing velocity in continuous software engineering. In this paper, we propose a tool-supported approach to make security standards more precise and easier to understand for both non-security as well as security experts by applying process models. This approach emerges from a large industrial company and encompasses so far the IEC 62443-4–1 standard. We further present a case study with 16 industry practitioners showing how the approach improves communication between development and security compliance practitioners. © 2021, Springer Nature Switzerland AG.
"
10.1155/2021/6660928,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100958683&origin=inward,Article,SCOPUS_ID:85100958683,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),reliability analysis of a complex multistate system based on a cloud bayesian network,"
AbstractView references

This study focused on mixed uncertainty of the state information in each unit caused by a lack of data, complex structures, and insufficient understanding in a complex multistate system as well as common-cause failure between units. This study combined a cloud model, Bayesian network, and common-cause failure theory to expand a Bayesian network by incorporating cloud model theory. The cloud model and Bayesian network were combined to form a reliable cloud Bayesian network analysis method. First, the qualitative language for each unit state performance level in the multistate system was converted into quantitative values through the cloud, and cloud theory was then used to express the uncertainty of the probability of each state of the root node. Then, the β-factor method was used to analyze reliability digital characteristic values when there was common-cause failure between the system units and when each unit failed independently. The accuracy and feasibility of the method are demonstrated using an example of the steering hydraulic system of a pipelayer. This study solves the reliability analysis problem of mixed uncertainty in the state probability information of each unit in a multistate system under the condition of common-cause failure. The multistate system, mixed uncertainty of the state probability information of each unit, and common-cause failure between the units were integrated to provide new ideas and methods for reliability analysis to avoid large errors in engineering and provide guidance for actual engineering projects. © 2021 Jin-Zhang Jia et al.
"
10.1016/j.aei.2021.101256,S1474034621000112,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100385462&origin=inward,Article,SCOPUS_ID:85100385462,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a small samples training framework for deep learning-based automatic information extraction: case study of construction accident news reports analysis,"
                  Knowledge management is crucial for construction safety management. Widely collected and well-organized safety-related documents are recognized to be significant in raising the workers' security awareness and then to prevent hazards and accidents. To improve document processing efficiency, automatic information extraction plays an important role. However, currently, automatic information extraction modeling requires large scale training datasets. It is a big challenge for the engineering industry, especially for the fields which heavily rely on the experts’ knowledge. Limited data sources, and high time and labor costs make it not practical to establish a large-scale dataset. This work proposed a natural language data augmentation-based small samples training framework for automatic information extraction modeling. With the designed cross combination-based text data augmentation algorithm, the deep neural network can be employed to build up automatic information extraction models without large-scale raw data and manual annotations. Characters semantic coding is employed to avoid word segmentation and make sure that the framework can be utilized in different writing language systems. The BiLSTM-CRF model is adopted as the detection core to conduct character classification. Through a case study of two independent accident news report datasets analysis, the proposed framework has been validated. A reliable and robust automatic information extraction model can be established, even though with small samples training.
               "
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100317938&origin=inward,Conference Paper,SCOPUS_ID:85100317938,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),space launch system liftoff and separation dynamics analysis tool chain,"
AbstractView references

A flexible, hierarchical tool chain that is being applied to NASA’s Space Launch System (SLS) for critical dynamics phenomena is described. This tool chain, called CLVTOPS, is used to investigate lateral liftoff movement of the vehicle as it departs and clears the mobile launch tower and separation of the two solid rocket boosters without collision with the core stage and payload. The toolset’s architecture was configured to take advantage of a modern software engineering approach for maximum flexibility and utilization of open-source simulations and associated tools. As opposed to a “monolithic” approach, scripting languages were used to “bind” together a tool chain to configure and organize input data, execute and produce analysis results, and post-process these results to facilitate a rapid, iterative analysis process to quickly address issues and pursue alternatives with emphasis on analysis automation. Key capabilities in the tool chain include processing and mining of very large data sets, a wide range of graphical depictions, and high-fidelity, physics-based simulations. The paper begins with a problem description and the motivation for liftoff and separation dynamics analysis followed by a historical survey of dynamics analyses for previous NASA human-rated launch vehicles. Details of the tool chain and its components are then introduced and divided, first, into description of the scripting language architecture used to “bind” the simulation tools, programs, and scripts together and, second, the physics models and simulations. Representative analyses and data products for liftoff and booster separation dynamics are shown in order to provide in-depth insight into the tool chain’s capabilities. Supporting activities such as simulation tool chain verification, version archiving and data management, and training are addressed. The paper concludes with case examples on how the tool chain can be tailored to related aerospace dynamics analyses, both large and small. The flexibility and versatility of this tool chain in supporting analyses of such a diverse range of aerospace applications demonstrates the feasibility of applying these patterns and techniques for tool construction to other aerospace simulations. © 2021, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.
"
10.1016/j.compind.2020.103347,S0166361520305819,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096841392&origin=inward,Article,SCOPUS_ID:85096841392,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),portuguese word embeddings for the oil and gas industry: development and evaluation,"
                  Over the last decades, oil and gas companies have been facing a continuous increase of data collected in unstructured textual format. New disruptive technologies, such as natural language processing and machine learning, present an unprecedented opportunity to extract a wealth of valuable information within these documents. Word embedding models are one of the most fundamental units of natural language processing, enabling machine learning algorithms to achieve great generalization capabilities by providing meaningful representations of words, being able to capture syntactic and semantic features based on their context. However, the oil and gas domain-specific vocabulary represents a challenge to those algorithms, in which words may assume a completely different meaning from a common understanding. The Brazilian pre-salt is an important exploratory frontier for the oil and gas industry, with increasing attractiveness for international investments in exploration and production projects, and most of its documentation is in Portuguese. Moreover, Portuguese is one of the largest languages in terms of number of native speakers. Nonetheless, despite the importance of the petroleum sector of Portuguese speaking countries, specialized public corpora in this domain are scarce. This work proposes PetroVec, a representative set of word embedding models for the specific domain of oil and gas in Portuguese. We gathered an extensive collection of domain-related documents from leading institutions to build a large specialized oil and gas corpus in Portuguese, comprising more than 85 million tokens. To provide an intrinsic evaluation, assessing how well the models can encode domain semantics from the text, we created a semantic relatedness test set, comprising 1,500 word pairs labeled by selected experts in geoscience and petroleum engineering from both academia and industry. In addition, we performed an extrinsic quantitative evaluation on a downstream task of named entity recognition in geoscience, plus a set of qualitative analyses, and conducted a comparative evaluation against a public general-domain embedding model. The obtained results suggest that our domain-specific models outperformed the general model on their ability to represent specialized terminology. To the best of our knowledge, this is the first attempt to generate and evaluate word embedding models for the oil and gas domain in Portuguese. Finally, all the resources developed by this work are made available for public use, including the pre-trained specialized models, corpora, and validation datasets.
               "
10.1016/j.jss.2020.110815,S0164121220302144,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092430945&origin=inward,Article,SCOPUS_ID:85092430945,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),enabling consistency in view-based system development — the vitruvius approach,"During the development of large software-intensive systems, developers use several modeling languages and tools to describe a system from different viewpoints. Model-driven and view-based technologies have made it easier to define domain-specific languages and transformations. Nevertheless, using several languages leads to fragmentation of information, to redundancies in the system description, and eventually to inconsistencies. Inconsistencies have negative impacts on the system’s quality and are costly to fix. Often, there is no support for consistency management across multiple languages. Using a single language is no practicable solution either, as it is overly complex to define, use, and evolve such a language. View-based development is a suitable approach to deal with complex systems, and is widely used in other engineering disciplines. Still, we need to cope with the problems of fragmentation and consistency. In this paper, we present the Vitruvius approach for consistency in view-based modeling. We describe the approach by formalizing the notion of consistency, presenting languages for consistency preservation, and defining a model-driven development process. Furthermore, we show how existing models can be integrated. We have evaluated our approach at two case studies from component-based and embedded automotive software development, using our prototypical implementation based on the Eclipse Modeling Framework."
10.1016/j.cpc.2020.107572,S0010465520302745,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091933542&origin=inward,Article,SCOPUS_ID:85091933542,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a fluid simulation system based on the mps method,"
                  Fluid flow simulation is a highly active area with applications in a wide range of engineering problems and interactive systems. Meshless methods like the Moving Particle Semi-implicit (MPS) are a great alternative to deal efficiently with large deformations and free-surface flow. However, mesh-based approaches can achieve higher numerical precision than particle-based techniques with a performance cost. This paper presents a numerically stable and parallelized system that benefits from advances in the literature and parallel computing to obtain an adaptable MPS method. The proposed technique can simulate liquids using different approaches, such as two ways to calculate the particles’ pressure, turbulent flow, and multiphase interaction. The method is evaluated under traditional tests cases presenting comparable results to recent techniques. This work integrates the previously mentioned advances into a single solution, which can switch on improvements, such as better momentum conservation and less spurious pressure oscillations, through a graphical interface. The code is entirely open-source under the GPLv3 free software license. The GPU-accelerated code reached speedups ranging from 3 to 43 times, depending on the total number of particles. The simulation runs at one fps for a case with approximately 200,000 particles.
               
                  Program summary
                  
                     Program Title: Voxar MPS
                  
                     CPC Library link to program files: 
                     http://dx.doi.org/10.17632/49f6djvhjk.1
                  
                  
                     Licensing provisions: GNU General Public License version 3
                  
                     Programming language: C++ and CUDA
                  
                     Nature of problem: The Voxar MPS code has been developed to study the flow of incompressible fluids that requires high computational cost.
                  
                     Solution method: Voxar MPS is an implementation of the Moving Particle Semi-implicit, a Lagrangian meshless particle method for incompressible fluids.
               "
10.1007/978-3-030-47124-8_40,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85088453211&origin=inward,Book Chapter,SCOPUS_ID:85088453211,scopus,2021-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),zadehian paradigms shaping 21<sup>st</sup> century artificial intelligence,"
AbstractView references

Starting from the premise that Zadeh’s research heritage is irreducible to his 20th Century work, the paper aims to show that his Generalized Theory of Uncertainty is even more influential now, for 21st Century service-oriented engineering, than his papers on fuzzy sets were for the product-based industrial era. To mirror the whole architectonics of Zadeh’s work, the paper highlights the lasting puissance and evolution of 20th Century Zadehian paradigms. On this groundwork, two paradigmatic breakthroughs follow: (a) moving from ‘information is statistical in nature’ to ‘information is a generalized constraint’; (b) setting as target ‘achievement of NL-capability’. Next, two cardinal upshots: reshaping the relation between numbers and words and scaling down the importance of algorithmic paradigms. Both are needed to meet the challenge of modern artificial intelligence: interacting with living systems; emphasis is on model tractability (for efficiency) and on model interpretability (for user acceptance). © 2021, Springer Nature Switzerland AG.
"
10.1016/j.knosys.2020.106486,S0950705120306158,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092388406&origin=inward,Article,SCOPUS_ID:85092388406,scopus,2020-12-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),decab-lstm: deep contextualized attentional bidirectional lstm for cancer hallmark classification,"
                  The great number of online scientific publications on cancer research makes large scale data mining possible. The hallmarks or characteristics of cancer can be used to distinguish cancerous cells from normal cells. Therefore, it is extremely necessary to organize and categorize a sea of scientific articles into the corresponding hallmarks by predicting whether or not they contain the information of interest. In the past, many research works tended to employ traditional machine learning methods that characterize feature engineering. Deep learning-based methods have achieved state-of-the-art performance in a wide range of Natural Language Processing (NLP) tasks. However, there is only a limited number of work with a focus on deep learning techniques for the task of cancer hallmark text classification. To advance this task, a novel neural architecture DEep Contextualized Attentional Bidirectional LSTM (DECAB-LSTM) was proposed, capable of learning to attend to the valuable information in a sentence by introducing contextual attention mechanism. We also investigated the effect of a good word embedding for the cancer hallmark text classification. We trained our model on a benchmark dataset and reported the accuracy, f score, and AUC metrics. Compared to several baselines like Logistic regression, Support Vector Machines, Convolutional Neural Networks, fastText, etc., the proposed model have achieved state-of-the-art performance over baselines, demonstrating its great potential in the empirical application to cancer research.
               "
10.1109/ICECE51571.2020.9393048,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85104642496&origin=inward,Conference Paper,SCOPUS_ID:85104642496,scopus,2020-12-17,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),summarizing online product reviews in bengali based on similarity using sequence to sequence rnns,"
AbstractView references

By text summarization, large texts or documents can be concise while preserving the context of the main document. This research work is focused on summarizing online product reviews in Bengali Language. By using online user feedback, future buyers will be able to take decisions whether to buy the product or not. However, it is very difficult for a buyer to decide by reading manually all the reviews. Machine learning techniques are available nowadays for text summarization which is a promising solution for extracting information from those user reviews. There are many well-known summarization tools for English language but in Bengali, there are very few and insufficient tools for text summarization. In this paper, a model has been for abstract Bangla text summarization on online product reviews using a Recurrent Neural Network(RNN). Long ShortTerm Memory (LSTM) and Sequence-to-Sequence (Seq2Seq) based RNN has been applied here. Experimented results underscore that the training loss is reduced to 0.0034 and able to generate a frequent predictive summary from original texts or documents. This research work also includes a Word Mover's Distance(WMD) method which checks the similarity between human and machine summary. It is observed that, the applied WMD in this work for text similarity model has been able to achieve better performance than the Jaccard method. © 2020 IEEE.
"
10.1109/BigData50022.2020.9378100,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103833221&origin=inward,Conference Paper,SCOPUS_ID:85103833221,scopus,2020-12-10,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),assessing differences in large spatio-temporal climate datasets with a new python package,"
AbstractView references

Output data from modern Earth system model simulations are consuming increasingly massive amounts of storage resources, and storing these climate model data is not economically sustainable. Previous works have motivated lossy compression as a potential solution, which achieves greater compression ratios than lossless compression. This further reduction comes at the cost of a loss of information, and therefore, care must be taken to avoid introducing artifacts in the data that could affect scientific conclusions. In this paper we introduce a Python package designed to aid in the analysis of differences in large spatio-temporal datasets, such as those produced by global climate models. While the new package is agnostic to the source of the differences, our motivation is to enable climate scientists to more easily assess the effects of lossy data compression by visualizing and computing derived spatial-temporal quantities that compare lossily compressed datasets to the original dataset. Because Python is quickly becoming the tool of choice for scientific data analysis in the geoscience community, this new package makes use of the Python software stack in Pangeo (an active NSF-funded community platform for Big Data geoscience). Interoperability with other Pangeo software tools means that the new package easily integrates into climate scientists' post-processing and analysis workflows, which we hope will facilitate the adoption of lossy compression into the climate modeling community. © 2020 IEEE.
"
10.1109/ARGENCON49523.2020.9505355,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85114049671&origin=inward,Conference Paper,SCOPUS_ID:85114049671,scopus,2020-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),user stories identification in software's issues records using natural language processing,"
AbstractView references

Nowadays most of software development companies have adopted agile development methodologies, which suggest capturing requirements through user stories. The use of these good practices improves the organization of work teams and the quality of the resulting software product. However, user stories are too often poorly written in practice and exhibit inherent quality defects. In addition, it is common to find the user stories of a software project immersed in large volumes of issues request logs from software quality tracking systems, which makes difficult to process them later. In order to solve these defects and to formulate high quality requirements, a current trend is the application of computational linguistic techniques to identify and then process user stories. In this work, we present two recurrent neural network models that were developed for the identification of user stories in issue records from software quality tracking systems for further processing. © 2020 IEEE
"
10.1109/TASE49443.2020.00037,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105406422&origin=inward,Conference Paper,SCOPUS_ID:85105406422,scopus,2020-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),formally verifying sequence diagrams for safety critical systems,"
AbstractView references

UML interactions, aka sequence diagrams, are frequently used by engineers to describe expected scenarios of good or bad behaviors of systems under design, as they provide allegedly a simple enough syntax to express a quite large variety of behaviors. This paper uses them to express formal safety requirements for safety critical systems in an incremental way, where the scenarios are progressively refined after checking the consistency of the requirements. As before, the semantics of these scenarios are expressed by transforming them into an intermediate semantic model amenable to formal verification. We rely on the Clock Constraint Specification Language (CCSL) as the intermediate semantic language. An SMT-based analysis tool called MyCCSL is used to check consistency of the sequence diagrams. We compare these requirements against actual execution traces to prove the validity of our transformation. In some sense, sequence diagrams and CCSL constraints both express a family of acceptable infinite traces that must include the behaviors given by the finite set of finite execution traces against which we validate. Finally, the whole process is illustrated on partial requirements for a railway transit system. © 2020 IEEE.
"
10.1145/3439961.3439973,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85102887709&origin=inward,Conference Paper,SCOPUS_ID:85102887709,scopus,2020-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),evaluating the understandability and expressiveness of simulation executable models with professionals: obtaining perceptions from researchers and practitioners for improving quality of models,"
AbstractView references

Large-scale and complex systems exhibit (i) dynamic structures and behaviors, (ii) several components/systems involved and (iii) multiple interoperability links. Such technologies have exposed limitations and fragilities on traditional software specification languages (such as UML and SySML), since those languages were designed to document single (not multiple interoperating) systems, which can further compromise the quality of the final product. In this context, Executable Models (ExM) technology, such as simulation models, models@runtime and executable UML, match these requirements by supporting engineers with visualization of the systems structures (still at design-Time) and the ability to model their behaviors and interactions. However, we currently observe a decrease in the use of models and consequently ExM by software engineering professionals in the academy and industry and we claim that those professionals have not exhibited abilities to use ExM even in simpler scenarios. In this paper, we present the results of an exploratory study on the perceptions of those professionals regarding the use of ExM to solve problems in their current practice. 58 professionals were exposed to situations to solve problems using a specific type of ExM (DEVS simulation models), based on a survey research. Responses were quantitatively and qualitatively analyzed. Results reveal that executable languages still require advances to bring them even closer to the current software engineering practice and towards a larger adoption in the future. © 2020 ACM.
"
10.1186/s40537-020-00335-4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089177870&origin=inward,Article,SCOPUS_ID:85089177870,scopus,2020-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring the efficacy of transfer learning in mining image-based software artifacts,"
AbstractView references

Background: Transfer learning allows us to train deep architectures requiring a large number of learned parameters, even if the amount of available data is limited, by leveraging existing models previously trained for another task. In previous attempts to classify image-based software artifacts in the absence of big data, it was noted that standard off-the-shelf deep architectures such as VGG could not be utilized due to their large parameter space and therefore had to be replaced by customized architectures with fewer layers. This proves to be challenging to empirical software engineers who would like to make use of existing architectures without the need for customization. Findings: Here we explore the applicability of transfer learning utilizing models pre-trained on non-software engineering data applied to the problem of classifying software unified modeling language (UML) diagrams. Our experimental results show training reacts positively to transfer learning as related to sample size, even though the pre-trained model was not exposed to training instances from the software domain. We contrast the transferred network with other networks to show its advantage on different sized training sets, which indicates that transfer learning is equally effective to custom deep architectures in respect to classification accuracy when large amounts of training data is not available. Conclusion: Our findings suggest that transfer learning, even when based on models that do not contain software engineering artifacts, can provide a pathway for using off-the-shelf deep architectures without customization. This provides an alternative to practitioners who want to apply deep learning to image-based classification but do not have the expertise or comfort to define their own network architectures. © 2020, The Author(s).
"
10.1109/KSE50997.2020.9287650,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099597607&origin=inward,Conference Paper,SCOPUS_ID:85099597607,scopus,2020-11-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a bert-based hierarchical model for vietnamese aspect based sentiment analysis,"
AbstractView references

Aspect based sentiment analysis (ABSA) is the task of identifying sentiment polarity towards specific entities and their aspects mentioned in customers' reviews. This paper presents a new and effective hierarchical model using the pre-trained language model, Bidirectional Encoder Representations from Transformers (BERT). This model integrates the context information of the previous layer (i.e. entity type) into the prediction for the following layer (i.e. aspect type) and optimizes the global loss functions to capture the entire information from all layers. Experimental results on two public benchmark datasets in Vietnamese showed that the proposed model is superior to the existing ones. Specifically, the model achieved 84.23% and 82.06% in the F1_micro scores in detecting entities and their aspects on the domains of restaurants and hotels, respectively. In identifying aspect sentiment polarity, the model gained 71.3% and 74.69% in the F1_micro scores on the domains of restaurants and hotels, respectively. These results outperformed the best submission of the campaign by a large margin and gained a new state of the art. © 2020 IEEE.
"
10.1109/KSE50997.2020.9287406,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099535946&origin=inward,Conference Paper,SCOPUS_ID:85099535946,scopus,2020-11-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),from universal language model to downstream task: improving roberta-based vietnamese hate speech detection,"
AbstractView references

Natural language processing (NLP) is a fast-growing field of artificial intelligence. Since the Transformer [32] was introduced by Google in 2017, a large number of language models such as BERT, GPT, and ELMo have been inspired by this architecture. These models were trained on huge datasets and achieved state-of-the-art results on natural language understanding. However, fine-tuning a pre-trained language model on much smaller datasets for downstream tasks requires a carefully-designed pipeline to mitigate problems of the datasets such as lack of training data and imbalanced data. In this paper, we propose a pipeline to adapt the general-purpose RoBERTa language model to a specific text classification task: Vietnamese Hate Speech Detection. We first tune the PhoBERT1[9] on our dataset by re-training the model on the Masked Language Model (MLM) task; then, we employ its encoder for text classification. In order to preserve pre-trained weights while learning new feature representations, we further utilize different training techniques: Layer freezing, block-wise learning rate, and label smoothing. Our experiments proved that our proposed pipeline boosts the performance significantly, achieving a new state-of-the-art on Vietnamese Hate Speech Detection (HSD) campaign2 with 0.7221 F1 score. © 2020 IEEE.
"
10.1145/3368089.3417047,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85097147287&origin=inward,Conference Paper,SCOPUS_ID:85097147287,scopus,2020-11-08,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),frepa: an automated and formal approach to requirement modeling and analysis in aircraft control domain,"
AbstractView references

Formal methods are promising for modeling and analyzing system requirements. However, applying formal methods to large-scale industrial projects is a remaining challenge. The industrial engineers are suffering from the lack of automated engineering methodologies to effectively conduct precise requirement models, and rigorously validate and verify (V&V) the generated models. To tackle this challenge, in this paper, we present a systematic engineering approach, named Formal Requirement Engineering Platform in Aircraft (FREPA), for formal requirement modeling and V&V in the aerospace and aviation control domains. FREPA is an outcome of the seamless collaboration between the academy and industry over the last eight years. The main contributions of this paper include 1) an automated and systematic engineering approach FREPA to construct requirement models, validate and verify systems in the aerospace and aviation control domain, 2) a domain-specific modeling language AASRDL to describe the formal specification, and 3) a practical FREPA-based tool AeroReq which has been used by our industry partners. We have successfully adopted FREPA to seven real aerospace gesture control and two aviation engine control systems. The experimental results show that FREPA and the corresponding tool AeroReq significantly facilitate formal modeling and V&V in the industry. Moreover, we also discuss the experiences and lessons gained from using FREPA in aerospace and aviation projects. © 2020 ACM.
"
10.1145/3416506.3423580,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096999175&origin=inward,Conference Paper,SCOPUS_ID:85096999175,scopus,2020-11-08,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards demystifying dimensions of source code embeddings,"
AbstractView references

Source code representations are key in applying machine learning techniques for processing and analyzing programs. A popular approach in representing source code is neural source code embeddings that represents programs with high-dimensional vectors computed by training deep neural networks on a large volume of programs. Although successful, there is little known about the contents of these vectors and their characteristics. In this paper, we present our preliminary results towards better understanding the contents of code2vec neural source code embeddings. In particular, in a small case study, we use the code2vec embeddings to create binary SVM classifiers and compare their performance with the handcrafted features. Our results suggest that the handcrafted features can perform very close to the highly-dimensional code2vec embeddings, and the information gains are more evenly distributed in the code2vec embeddings compared to the handcrafted features. We also find that the code2vec embeddings are more resilient to the removal of dimensions with low information gains than the handcrafted features. We hope our results serve a stepping stone toward principled analysis and evaluation of these code representations. © 2020 ACM.
"
10.1109/CIS52066.2020.00086,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105302498&origin=inward,Conference Paper,SCOPUS_ID:85105302498,scopus,2020-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a high accuracy dns tunnel detection method without feature engineering,"
AbstractView references

Domain Name System (DNS) is a key protocol and service used on the Internet. It is responsible for converting domain names into IP addresses. DNS tunnel is a method of encoding data of other programs or protocols in DNS query and response. Previous studies usually need to extract a large number of features manually and train the classifier of DNS tunnel detection by feature engineering. In this paper, a new framework for DNS tunnel detection is proposed, which can automatically extract features, including long short-term memory (LSTM) language model with attention mechanism and gated recurrent unit (GRU) language model with attention mechanism. Finally, a single-level classifier based on a character-level convolutional neural network (Char-CNN) is proposed. The results show that the LSTM and GRU language models based on attention mechanism and the algorithm of character-level convolution neural network achieve high accuracy and near-zero false positives. © 2020 IEEE.
"
10.1145/3422817,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85097242385&origin=inward,Article,SCOPUS_ID:85097242385,scopus,2020-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on the construction of web ner model training tool based on distant supervision,"
AbstractView references

Named entity recognition (NER) is an important task in natural language understanding, as it extracts the key entities (person, organization, location, date, number, etc.) and objects (product, song, movie, activity name, etc.) mentioned in texts. However, existing natural language processing (NLP) tools (such as Stanford NER) recognize only general named entities or require annotated training examples and feature engineering for supervised model construction. Since not all languages or entities have public NER support, constructing a tool for NER model training is essential for low-resource language or entity information extraction. In this article, we study the problem of developing a tool to prepare training corpus from the Web with known seed entities for custom NER model training via distant supervision. The major challenge of automatic labeling lies in the long labeling time due to large corpus and seed entities as well as the concern to avoid false positive and false negative examples due to short and long seeds. To solve this problem, we adopt locality-sensitive hashing (LSH) for various length of seed entities. We conduct experiments on five types of entity recognition tasks, including Chinese person names, food names, locations, points of interest (POIs), and activity names to demonstrate the improvements with the proposed Web NER model construction tool. Because the training corpus is obtained by automatic labeling of the seed entity-related sentences, one could use either the entire corpus or the positive only sentences for model training. Based on the experimental results, we found the decision should depend on whether traditional linear chained conditional random fields (CRF) or deep neural network-based CRF is used for model training as well as the completeness of the provided seed list. © 2020 ACM.
"
10.1177/0037549720946107,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089682782&origin=inward,Article,SCOPUS_ID:85089682782,scopus,2020-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"dev-promela: modeling, verification, and validation of a video game by combining model-checking and simulation","
AbstractView references

Modeling, verifying, and validating are essential steps in order to build systems and software that do what designers expect. If formal verification, and especially model-checking, is a popular method for proving the correctness of properties, its efficiency depends on the accuracy of the used models and the quality of abstractions. As a consequence, applying verification techniques on large-scale complex software like video games is hard without strong assumptions and simplifications. Simulation models are generally more accurate than verification models, but it is often harder to verify them. Combined formalisms that take the benefits of both model-checking and discrete-event simulation represent a good deal between both of these families, although strong engineering expertise remains necessary to define the relevant tests and scenarios. This paper proposes an approach to build this kind of formalism through the example of DEv-PROMELA, which is built by combining Discrete-event System Specification formalism and PROMELA language. Then, it shows how combined formalisms can be used for modeling, verifying, and validating complex software like video games by using both formal-based and simulation-based verification and validation. © The Author(s) 2020.
"
10.1145/3417990.3420207,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096743616&origin=inward,Conference Paper,SCOPUS_ID:85096743616,scopus,2020-10-16,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),efficiently querying large-scale heterogeneous models,"
AbstractView references

With the increase in the complexity of software systems, the size and the complexity of underlying models also increases proportionally. In a low-code system, models can be stored in different backend technologies and can be represented in various formats. Tailored high-level query languages are used to query such heterogeneous models, but typically this has a significant impact on performance. Our main aim is to propose optimization strategies that can help to query large models in various formats efficiently. In this paper, we present an approach based on compile-time static analysis and specific query optimizers/translators to improve the performance of complex queries over large-scale heterogeneous models. The proposed approach aims to bring efficiency in terms of query execution time and memory footprint, when compared to the naive query execution for low-code platforms. © 2020 ACM.
"
10.1109/ICCASIT50869.2020.9368795,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103226267&origin=inward,Conference Paper,SCOPUS_ID:85103226267,scopus,2020-10-14,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),idcnn-crf-based domain named entity recognition method,"
AbstractView references

Entity recognition is the foundation of natural language processing. For traditional methods, a large number of manual annotations are required, and the accuracy of the model is low and the speed is slow. The word vector model cannot recognize unregistered words well. This paper proposes an entity recognition method based on character embedding, Iterated Dilated Convolutional Neural Networks (IDCNN) and Conditional Random Fields (CRF). Combining the characteristics of iterative dilation convolutional neural network GPU parallel operation and long-term and short-term memory, the ability of word vectors to express the meaning of unregistered words, and the excellent learning ability of conditional random fields for labeling rules, a character+IDCNN+CRF named entity is constructed Identify the model. Based on the corpus in the field of military equipment, the experiment shows that the method can distinguish the equipment name and organization name excellently in a certain dimension character vector. The F-1 value in the test corpus exceeds 94%. For military equipment domain entity recognition has a better effect, and the prediction speed has been greatly improved compared to before. © 2020 IEEE.
"
10.1145/3434581.3434599,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099343983&origin=inward,Conference Paper,SCOPUS_ID:85099343983,scopus,2020-10-14,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research on telemetry comparison system on manned spacecraft,"
AbstractView references

At present, the status of the integrated test, measurement, control and communication subsystem of manned spacecraft is becoming more and more complex. There are more and more equipment on the vehicle and the scale of equipment on the ground is also large. In a subsystem limited post personnel, the test personnel not only to the spacecraft parameters and instructions on the interpretation of the situation, but also to the ground equipment analysis of the engineering telemetry source code for comparison, which is not conducive to improve efficiency, and easy to miss. Therefore, it is urgent to design an engineering telemetry comparison system to improve the test efficiency and gradually realize the automation of the test process. In China, the comprehensive test of manned spacecraft has just started for several decades and is in the development stage. Manned spacecraft system is large, each subsystem in the comprehensive test needs a large number of ground testers, and automation degree is relatively low. In the existing spacecraft model test, the tester needs to interpret the engineering telemetry data transmitted down multiple channels in real time. In order to ensure the consistency of the engineering telemetry transmitted down each channel, it is necessary to compare the source code of the engineering telemetry data manually to judge the consistency and continuity of the data source code. In the face of manned spacecraft data downlink more and more, manual comparison of the source code has obviously not been able to adapt to the current growing number of types of missions. Engineering telemetry source code alignment requires a lot of manual time to interpret the source code packets, usually using BES and Ultra Edit software for source code interpretation. The large amount of data can only intercept a few packets for parsing and interpretation, and the full coverage of the test cycle cannot be achieved. Therefore, it is necessary to develop a system to realize the comparison of manned spacecraft engineering telemetry and gradually narrow the space technology gap with developed countries. © 2020 ACM.
"
10.1109/DASC50938.2020.9256753,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85097983935&origin=inward,Conference Paper,SCOPUS_ID:85097983935,scopus,2020-10-11,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),system architecture modeling for electronic systems using mathworks system composer and simulink,"
AbstractView references

Electronic system architectures have traditionally been documented as static block diagrams in tools such as Microsoft® Visio® or through a richer modeling approach such as Systems Modeling Language (SysML). These approaches did not fully meet the modeling needs for the Gulfstream authors, which led to an alternative approach. This paper introduces the Electronic System Architecture Modeling (eSAM) method, which leverages a new system architecture modeling tool called System Composer™. eSAM was created by the authors to define a standard method for applying the generic System Composer modeling constructs to build functional, physical, and logical architecture models of electronic systems. The eSAM methods are applied to an example avionics architecture to demonstrate capabilities needed for system modeling, collaborative OEM-supplier workflows, data management and ICD generation, systems integration activities, generation of system architecture deliverables for the avionics certification standards governed by SAE ARP4754A, and a Model-Based Design approach that connects a software function to its system-level ICD. System Composer is built on MATLAB® and Simulink® and leverages the modeling, analysis, and simulation capabilities of these well-established tools. System Composer adds additional capabilities for modeling integration between systems, filtering large models into manageable views, capturing important system and component properties, allocating between different descriptive architecture models, directly connecting system architecture models to software functional models, and flowing data down into specialized design tools. This paper summarizes desirable features in system architecture modeling tools, introduces the features and concepts of System Composer and describes application of the eSAM method. © 2020 IEEE.
"
10.1109/ICVEE50212.2020.9243175,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096641136&origin=inward,Conference Paper,SCOPUS_ID:85096641136,scopus,2020-10-03,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),what's in a caption?: leveraging caption pattern for predicting the popularity of social media posts,"
AbstractView references

In the past few years, social media has become an integral part of modern society. It also hassurfaced as an influential tool that helps a business or individual in gaining identity and reputation. Predicting the popularity of images before they are posted on social media thus may have a profound impact to reveal individual preference and public attention. However, an accurate prediction is a challenging task, mainly on account of factors that play a part in this. Previous studies, although achieve favourable results, overlook one unique characteristic of semantics in textual metadata, i.e., the language modeling, to better model the context information of a post. To that end, wepropose to exploit the language modeling features together with user profile and post metadata features. The language model features are extracted by utilizing the probability of word occurrence, while the user profile and post metadata features are provided as attributes by the original data source. Several state-of-the-art statistical modeling techniques are employed to investigate the performance of the proposed features on different estimation procedures. Experiments on a large-scale Flickr dataset demonstrate the benefits of the proposed features on predicting the popularity of social media posts. © 2020 IEEE.
"
10.1109/ICBASE51474.2020.00072,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105392189&origin=inward,Conference Paper,SCOPUS_ID:85105392189,scopus,2020-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),architectural facade recognition and generation through generative adversarial networks,"
AbstractView references

With the development of artificial intelligence technology, the ideas of machine learning have been introduced into the field of design in recent years. The research methods of 'AI + Architecture' have brought new ideas for solving traditional problems. Generative Adversarial Network (GAN) is a machine learning model for image generation. Pix2pix is an improved version of GAN, which is specially designed to learn and generate pairs of image data with similar characteristics. In this study, Pix2pix is applied to the recognition and generation of building facade. The purpose is to explore the feasibility of using image generation technology to achieve rapid recognition and generation of building facade based on pix2pix. This paper also discusses the application scenarios of this technology. The existing building façade datasets and the self-made Chinese traditional building datasets are used to test and verify that pix2pix under different types of datasets can nicely identify and generate facade images. Then we summarize a set of working methods based on GAN to realize the overall or local reconstruction design of the facade, so as to provide new ideas for the improvement of the efficiency of related industries and the expansion of teaching tools. © 2020 IEEE.
"
10.1109/ICECCS51672.2020.00011,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103486659&origin=inward,Conference Paper,SCOPUS_ID:85103486659,scopus,2020-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rl: a language for formal engineering,"
AbstractView references

Reflection is a notion that naturally emerges from philosophy, mathematics, and sciences. In short, reflection is the ability of an entity to alter its own behaviour. This paper suggests that reflection is crucial to the development of complex and trustworthy software systems. We present RL, a reflective computational model that aims to support the development of a large-scale framework for modelling and manipulating structured data. We give the formal semantics for this computational model. We also share preliminary work on a proof-of-concept implementation of RL and discuss future work. © 2020 IEEE.
"
10.1109/CLOUD49709.2020.00041,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099388799&origin=inward,Conference Paper,SCOPUS_ID:85099388799,scopus,2020-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),auto-generation of domain-specific systems: cloud-hosted devops for business,"
AbstractView references

The wide use of spreadsheet-based solutions for business processes illustrates the importance of giving business users simple mechanisms for specifying and managing their processes. However, spreadsheet-based solutions are hard to maintain, reuse, integrate, and scale. This paper describes an approach for supporting 'DevOps for business users' that enables business-level users to manage the full lifecycle of a large class of cloud-hosted business processes. The approach builds on DevOps for software engineering, but removes software engineers from the loop. Unlike general-purpose 'low code' business process management systems, the approach incorporates aspects of a processing domain (e.g., billing) to create a DevOps experience that business users can master easily. In the approach, business users follow an agile 'specify-check-generate-deploy' methodology, enabling them to rapidly and iteratively generate and operationalize cloud-hosted processing systems, with little or no assistance from IT staff. We demonstrate and evaluate the approach using a system built for the billing application area, developed at IBM, which provides technology support and maintenance services for numerous clients, each with different billing needs and logic. The paper describes the system, requirements, empirical evaluation of key components, and lessons learned. © 2020 IEEE.
"
10.1016/j.autcon.2020.103265,S092658051931341X,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85085244072&origin=inward,Article,SCOPUS_ID:85085244072,scopus,2020-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),text mining-based construction site accident classification using hybrid supervised machine learning,"
                  Safety is one key consideration in the monitoring of construction projects by engineers. Accidents in the project can potentially cause issues, such as workers' injury and progress delay, which lead to financial losses. Generally, accident narratives store all summaries and causes of the related events. Since documentations rapidly use large quantities of resources, the implementation of Artificial Intelligence (AI) begins to seek attention. Nevertheless, in current models, there are still drawbacks, such as weak learning performance and substantial error rate. In this regard, this study develops a hybrid model incorporating Gated Recurrent Unit (GRU) and Symbiotic Organisms Search (SOS), named Symbiotic Gated Recurrent Unit (SGRU). SOS searches the best parameters of GRU to ensure optimal performance. Furthermore, Natural Language Processing is applied to pre-process the text data prior classification process. The experimental result in this study showcases SGRU as the best classification model among other AI models. Therefore, SGRU shares the capability to aid the safety assessments of construction projects.
               "
10.4271/2020-28-0388,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85093836324&origin=inward,Conference Paper,SCOPUS_ID:85093836324,scopus,2020-09-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a study on topology optimization of aerospace and automobile components,"
AbstractView references

The advances in computer applications have been increasing in the recent years. The applications of Artificial Intelligence (AI) in the product design software, and the advancements in the Direct Digital Manufacturing (DDM) such as Additive Manufacturing have been leading the aerospace and automobile industries into the next level. AI assisted generative design helps the designers to reduce the weight of the structures without compensating the strength of the structures. The topology optimization is one of the subsets of the generative design which commonly used by the designers to design and redesign many components for weight reduction. This article studies the applications of topology optimization for an aerospace and an automobile components for design modification and weight reduction. The prototype of the optimized components are printed using FDM 3D printing for examining the shape optimization. The stress analyses of the components are analyzed using FEM. The results shows that significant weight reduction is noticed without compensating strength. An aerospace bracket and an automobile gear are taken for the topology optimization studies. © 2020SAE International. All Rights Reserved.
"
10.1145/3324884.3415289,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099202910&origin=inward,Conference Paper,SCOPUS_ID:85099202910,scopus,2020-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),gui2wire: rapid wireframing with a mined and large-scale gui repository using natural language requirements,"
AbstractView references

High-fidelity Graphical User Interface (GUI) prototyping is a well-established and suitable method for enabling fruitful discussions, clarification and refinement of requirements formulated by customers. GUI prototypes can help to reduce misunderstandings between customers and developers, which may occur due to the ambiguity comprised in informal Natural Language (NL). However, a disadvantage of employing high-fidelity GUI prototypes is their time-consuming and expensive development. Common GUI prototyping tools are based on combining individual GUI components or manually crafted templates. In this work, we present GUI2WiRe, a tool that enables users to retrieve GUI prototypes from a semiautomatically created large-scale GUI repository for mobile applications matching user requirements specified in Natural Language (NLR). We extract multiple text segments from the GUI hierarchy data and employ various Information Retrieval (IR) models and Automatic Query Expansion (AQE) techniques to achieve ad-hoc GUI retrieval from NLR. Retrieved GUI prototypes mined from applications can be inserted in the graphical editor of GUI2WiRe to rapidly create wireframes. GUI components are extracted automatically from the GUI screenshots and basic editing functionality is provided to the user. Finally, a preview of the application is created from the wireframe to allow interactive exploration of the current design. We evaluated the applied IR and AQE approaches for their effectiveness in terms of GUI retrieval relevance on a manually annotated collection of NLR and discuss our planned user studies. Video presentation of GUI2WiRe: https://youtu.be/2nN-Xr2Hk7I. © 2020 ACM.
"
10.1145/3324884.3416631,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85098140202&origin=inward,Conference Paper,SCOPUS_ID:85098140202,scopus,2020-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),trace-checking signal-based temporal properties: a model-driven approach,"
AbstractView references

Signal-based temporal properties (SBTPs) characterize the behavior of a system when its inputs and outputs are signals over time; they are very common for the requirements specification of cyber-physical systems. Although there exist several specification languages for expressing SBTPs, such languages either do not easily allow the specification of important types of properties (such as spike or oscillatory behaviors), or are not supported by (efficient) trace-checking procedures. In this paper, we propose SB-TemPsy, a novel model-driven trace-checking approach for SBTPs. SB-TemPsy provides (i) SB-TemPsy-DSL, a domain-specific language that allows the specification of SBTPs covering the most frequent requirement types in cyber-physical systems, and (ii) SB-TemPsy-Check, an efficient, model-driven trace-checking procedure. This procedure reduces the problem of checking an SB-TemPsy-DSL property over an execution trace to the problem of evaluating an Object Constraint Language constraint on a model of the execution trace. We evaluated our contributions by assessing the expressiveness of SB-TemPsy-DSL and the applicability of SB-TemPsy-Check using a representative industrial case study in the satellite domain. SB-TemPsy-DSL could express 97% of the requirements of our case study and SB-TemPsy-Check yielded a trace-checking verdict in 87% of the cases, with an average checking time of 48.7 s. From a practical standpoint and compared to state-of-the-art alternatives, our approach strikes a better trade-off between expressiveness and performance as it supports a large set of property types that can be checked, in most cases, within practical time limits. © 2020 ACM.
"
10.1109/AIRE51212.2020.00011,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096969725&origin=inward,Conference Paper,SCOPUS_ID:85096969725,scopus,2020-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),senet: a semantic web for supporting automation of software engineering tasks,"
AbstractView references

The use of Natural Language (NL) interfaces to allow devices and applications to respond to verbal commands or free-form textual queries is becoming increasingly prevalent in our society. To a large extent, their success in interpreting and responding to a request is dependent upon rich underlying ontologies and conceptual models that understand the technical or domain specific vocabulary of diverse users. The effective use of NL interfaces in the Software Engineering (SE) domains requires its own ontology models focusing upon software related terms and concepts. While many SE glossaries exist, they are often incomplete and tend to define the vocabulary for specific sub-fields without capturing associations between terms and phrases. This limits their usefulness for supporting NL-related tasks. In this paper we propose an approach for constructing and evolving a semantic network of software engineering concepts and phrases. Our approach starts with a set of existing SE glossaries, uses the existing glossary terms and explicitly defined associations as a starting point, uses machine learning-based techniques to dynamically identify and document additional associations between terms, leverages the network to interpret NL queries in the SE domain, and finally augments the resulting semantic network with feedback provided by users. We evaluate the viability of our approach within the sub-domain of Agile Software Development, focusing on requirements related queries, and show that the semantic network enhances the ability of an NL interface to correctly interpret and execute user queries. © 2020 IEEE.
"
10.34028/iajit/17/5/9,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85093898146&origin=inward,Article,SCOPUS_ID:85093898146,scopus,2020-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ontology-based transformation and verification of uml class model,"
AbstractView references

Software models describe structures, relationships and features of the software system. Especially, in Model Driven Engineering (MDE), they are considered as first-class elements instead of programming code and all software development activities move around these models. In MDE, programming code is automatically generated by the models and models’ defects can implicitly transfer to the code. These defects can harder to discover and rectify. Model verification is a promising solution to the problem. The Unified Modelling Language (UML) class model is an important part of UML and is used in both analysis and design. However, UML only provides graphical elements without any formal foundation. Therefore, verification of formal properties such as consistency, satisfiability and consequences are not possible in UML. This paper mainly focuses on ontology-based transformation and verification of the UML class model elements which have not been addressed in any existing verification methods e.g. xor association constraint, and dependencies relationships. We validate the scalability and effectiveness of the proposed solution using various UML class models. The empirical study shows that the proposed approach scales in the presence of the large and complex model. © 2020, Zarka Private University. All rights reserved.
"
10.1007/s10664-020-09856-1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089561236&origin=inward,Article,SCOPUS_ID:85089561236,scopus,2020-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),data-driven software design with constraint oriented multi-variate bandit optimization (combo),"
AbstractView references

Context: Software design in e-commerce can be improved with user data through controlled experiments (i.e. A/B tests) to better meet user needs. Machine learning-based algorithmic optimization techniques extends the approach to large number of variables to personalize software to different user needs. So far the optimization techniques has only been applied to optimize software of low complexity, such as colors and wordings of text. Objective: In this paper, we introduce the COMBO toolkit with capability to model optimization variables and their relationship constraints specified through an embedded domain-specific language. The toolkit generates personalized software configurations for users as they arrive in the system, and the configurations improve over time in in relation to some given metric. COMBO has several implementations of machine learning algorithms and constraint solvers to optimize the model with user data by software developers without deep optimization knowledge. Method: The toolkit was validated in a proof-of-concept by implementing two features that are relevant to Apptus, an e-commerce company that develops algorithms for web shops. The algorithmic performance was evaluated in simulations with realistic historic user data. Results: The validation shows that the toolkit approach can model and improve relatively complex features with many types of variables and constraints, without causing noticeable delays for users. Conclusions: We show that modeling software hierarchies in a formal model facilitates algorithmic optimization of more complex software. In this way, using COMBO, developers can make data-driven and personalized software products. © 2020, The Author(s).
"
10.1016/j.csl.2020.101079,S0885230820300127,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083084309&origin=inward,Article,SCOPUS_ID:85083084309,scopus,2020-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rap-net: recurrent attention pooling networks for dialogue response selection,"
                  The response selection has been an emerging research topic due to the growing interest in dialogue modeling, where the goal of the task is to select an appropriate response for continuing dialogues. To further push the end-to-end dialogue model toward real-world scenarios, the seventh Dialog System Technology Challenge (DSTC7) proposed a challenge track based on real chatlog datasets. The competition focuses on dialogue modeling with several advanced characteristics: (1) natural language diversity, (2) capability of precisely selecting a proper response from a large set of candidates or the scenario without any correct answer, and (3) knowledge grounding. This paper introduces recurrent attention pooling networks (RAP-Net), a novel framework for response selection, which can well estimate the relevance between the dialogue contexts and the candidates. The proposed RAP-Net is shown to be effective and can be generalize across different datasets and settings in the DSTC7 experiments.
               "
10.1007/s10270-019-00740-1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068819077&origin=inward,Article,SCOPUS_ID:85068819077,scopus,2020-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a verified catalogue of ocl optimisations,"
AbstractView references

OCL is widely used by model-driven engineering tools with different purposes like writing integrity constraints for meta-models, as a navigation language in model transformation languages or to define transformation specifications. Another scenario is the automatic generation of OCL code by a repair system. These generated expressions tend to be complex and unreadable due to the nature of the generative process. However, to be useful this code should be simple and resemble manually written code as much as possible when a developer must manually maintain it. There exists refactorings approaches for manually written OCL code, but there is no tool targeted to the optimisation of OCL expressions which have been automatically synthesised. Moreover, there is no available catalogue of OCL refactorings which can be integrated seamlessly into a tool. In this work, we contribute a set of refactorings intended to optimise OCL expressions, notably covering cases likely to arise in generated OCL code. We also contribute the implementation of these refactorings, built as a generic transformation catalogue using bentō, a transformation reuse tool for ATL. This makes it possible to specialise the catalogue for any OCL variant based on Ecore. Moreover, we propose a method to verify the correctness of the implemented catalogue based on translation validation and model finding. We describe the design and implementation of the catalogue and evaluate it by optimising a large amount of OCL expressions and proving the correctness of each optimisation execution. We also derive working implementations of the catalogue for ATL, EMF/OCL and SimpleOCL made available in a tool called BeautyOCL. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.1109/SOSE49046.2020.00018,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092770064&origin=inward,Conference Paper,SCOPUS_ID:85092770064,scopus,2020-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),pattern-based approach to modelling and verifying system security,"
AbstractView references

Security is one of the most important problems in the engineering of online service-oriented systems. The current best practice in security design is a pattern-oriented approach. A large number of security design patterns have been identified, categorised and documented in the literature. The design of a security solution for a system starts with identification of security requirements and selection of appropriate security design patterns; these are then composed together. It is crucial to verify that the composition of security design patterns is valid in the sense that it preserves the features, semantics and soundness of the patterns and correct in the sense that the security requirements are met by the design. This paper proposes a methodology that employs the algebraic specification language SOFIA to specify security design patterns and their compositions. The specifications are then translated into the Alloy formalism and their validity and correctness are verified using the Alloy model checker. A tool that translates SOFIA into Alloy is presented. A case study with the method and the tool is also reported. © 2020 IEEE.
"
10.4230/LIPIcs.CONCUR.2020.17,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091587138&origin=inward,Conference Paper,SCOPUS_ID:85091587138,scopus,2020-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),weighted transducers for robustness verification,"
AbstractView references

Automata theory provides us with fundamental notions such as languages, membership, emptiness and inclusion that in turn allow us to specify and verify properties of reactive systems in a useful manner. However, these notions all yield “yes”/“no” answers that sometimes fall short of being satisfactory answers when the models being analyzed are imperfect, and the observations made are prone to errors. To address this issue, a common engineering approach is not just to verify that a system satisfies a property, but whether it does so robustly. We present notions of robustness that place a metric on words, thus providing a natural notion of distance between words. Such a metric naturally leads to a topological neighborhood of words and languages, leading to quantitative and robust versions of the membership, emptiness and inclusion problems. More generally, we consider weighted transducers to model the cost of errors. Such a transducer models neighborhoods of words by providing the cost of rewriting a word into another. The main contribution of this work is to study robustness verification problems in the context of weighted transducers. We provide algorithms for solving the robust and quantitative versions of the membership and inclusion problems while providing useful motivating case studies including approximate pattern matching problems to detect clinically relevant events in a large type-1 diabetes dataset. © Emmanuel Filiot, Nicolas Mazzocchi, Jean-François Raskin, Sriram Sankaranarayanan, and Ashutosh Trivedi; licensed under Creative Commons License CC-BY 31st International Conference on Concurrency Theory (CONCUR 2020).
"
10.1088/1742-6596/1573/1/012005,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85093664830&origin=inward,Conference Paper,SCOPUS_ID:85093664830,scopus,2020-07-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),development of job sheet lathe machining practice based on animation video as interactive learning media,"
AbstractView references

The weakness of paper-based job sheet is that it is less effective in facilitating students' understanding of the explanation of images, and tends not to be stored by students for a long period of time as learning reference material so that it requires the development of job sheets to become interactive learning media that are in line with the demands of the development of the 4.0 industrial revolution. The objectives of this study are (1) to produce a job sheet for the practice of animation video-based lathe machining that can be accessed on Android SMK students, and (2) test the level of feasibility of the results of the development of animated video-based job sheets. This research method is Research and Development (R&D) with a 4-D model consisting of four stages, namely define, design, develop, and disseminate. The subject of the study consisted of material experts, media experts, and students of class XI machining techniques at Muhammadiyah Vocational School Prambanan. The instrument used was a questionnaire. While the data analysis technique uses quantitative descriptive analysis. The applications used in developing this job sheet include SSCNC (Swansoft CNC Simulator), bandicam, and wondershare filmora. The results of this study include: (1) producing a job sheet for video-based lathe machining practice with one subject of flat lathe work preparation; (2) the level of feasibility of animated videos is very feasible, it is known from the results of material expert validation which includes aspects of content, language, presentation, and benefits reaching an average value of 71.50 with a percentage of 89.50% in the very feasible category. The results of media expert validation covering aspects of appearance, ease of use, consistency, format, and graphics reached a value of 73 with a percentage of 91% in the very feasible category. The results of the small class test which included material aspects, language, graphics, and benefits obtained an average score of 62.31 with a percentage of 86% in the very feasible category and the results of the large class test obtained an average score of 59.55 with a percentage of 83% in the very feasible category. © Published under licence by IOP Publishing Ltd.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090213869&origin=inward,Article,SCOPUS_ID:85090213869,scopus,2020-07-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),asuring ripple effect of natural language requirements change for uls dynamic requirements,"
AbstractView references

Ultra Large Scales Systems (ULS) or Ecosystems are growing dramatically alongside their interactions and dependencies among other system components, change management needs new tactics. As consider one change or more in ULS requirements may result in a lot of side effects in other running or planned requirements that could be called ""Ripple Effect"". Different ULS elements are affected in this type of environments varying from RE workers, Change Requesters and Involved parties those can be called 'Crowed Sourcing""contributors in ULS environment. To challenge such problems, in this paper, we suggest a new methodology for requirements change evolution to able to measure the impact of several changes on ULS requirements which are represented by a Natural Language utilizing Similarity models. This paper reports on initial results of such an empirical study of Requirements change that led to ripple effects across an entire ULS environment, our case study around one of ecosystems for ERP with around 4480 stored requirement statements and closed to around 22 connected subsystems. We have used Natural Language Processing (NLP) and Similarity Models to support the model. © 2005 - ongoing JATIT & LLS.
"
10.1145/3387904.3389269,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091951300&origin=inward,Conference Paper,SCOPUS_ID:85091951300,scopus,2020-07-13,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),improving code search with co-attentive representation learning,"
AbstractView references

Searching and reusing existing code from a large-scale codebase,e.g, GitHub, can help developers complete a programming task efficiently. Recently, Gu et al. proposed a deep learning-based model(i.e., DeepCS), which significantly outperformed prior models. TheDeepCS embedded codebase and natural language queries intovectors by two LSTM (long and short-term memory) models separately, and returned developers the code with higher similarityto a code search query. However, such embedding method learnedtwo isolated representations for code and query but ignored theirinternal semantic correlations. As a result, the learned isolated representations of code and query may limit the effectiveness of codesearch.To address the aforementioned issue, we propose a co-attentiverepresentation learning model, i.e., Co-Attentive RepresentationLearning Code Search-CNN (CARLCS-CNN). CARLCS-CNN learnsinterdependent representations for the embedded code and querywith a co-attention mechanism. Generally, such mechanism learnsa correlation matrix between embedded code and query, and coattends their semantic relationship via row/column-wise max-pooling.In this way, the semantic correlation between code and query candirectly affect their individual representations. We evaluate the effectiveness of CARLCS-CNN on Gu et al.'s dataset with 10k queries.Experimental results show that the proposed CARLCS-CNN modelsignificantly outperforms DeepCS by 26.72% in terms of MRR (meanreciprocal rank). Additionally, CARLCS-CNN is five times fasterthan DeepCS in model training and four times in testing. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
10.1093/nar/gkaa325,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85088209750&origin=inward,Article,SCOPUS_ID:85088209750,scopus,2020-07-09,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),synthetic promoter design in escherichia coli based on a deep generative network,"
AbstractView references

Promoter design remains one of the most important considerations in metabolic engineering and synthetic biology applications. Theoretically, there are 450 possible sequences for a 50-nt promoter, of which naturally occurring promoters make up only a small subset. To explore the vast number of potential sequences, we report a novel AI-based framework for de novo promoter design in Escherichia coli. The model, which was guided by sequence features learned from natural promoters, could capture interactions between nucleotides at different positions and design novel synthetic promoters in silico. We combined a deep generative model that guides the search for artificial sequences with a predictive model to preselect the most promising promoters. The AI-designed promoters were optimized based on the promoter activity in E. coli and the predictive model. After two rounds of optimization, up to 70.8% of the AI-designed promoters were experimentally demonstrated to be functional, and few of them shared significant sequence similarity with the E. coli genome. Our work provided an end-to-end approach to the de novo design of novel promoter elements, indicating the potential to apply deep learning methods to de novo genetic element design. © 2020 The Author(s) 2020. Published by Oxford University Press on behalf of Nucleic Acids Research.
"
10.1109/INFOCOM41043.2020.9155462,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090297102&origin=inward,Conference Paper,SCOPUS_ID:85090297102,scopus,2020-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),systematic topology design for large-scale networks: a unified framework,"
AbstractView references

For modern large-scale networked systems, ranging from cloud to edge computing systems, the topology design has a significant impact on the system performance in terms of scalability, cost, latency, throughput, and fault-tolerance. These performance metrics may conflict with each other and design criteria often vary across different networks. To date, there has been little theoretic foundation on topology designs from a prescriptive perspective, indicating that the current status quo of the design process is more of an art than a science. In this paper, we advocate a novel unified framework to describe, generate, and analyze topology design in a systematic fashion. By reverse-engineering existing topology designs and developing a fine-grained decomposition method for topology design, we propose a general procedure that serves as a common language to describe topology design. By proposing general criteria for the procedure, we devise a top-down approach to generate topology models, based on which we can systematically construct and analyze new topologies. To validate our approach, we leverage concrete tools based on combinatorial design theory and propose a novel layered topology model. With quantitative performance analysis, we reveal the trade-offs among performance metrics and generate new topologies with various advantages for different large-scale networks. © 2020 IEEE.
"
10.1002/sys.21537,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084556002&origin=inward,Article,SCOPUS_ID:85084556002,scopus,2020-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring and managing the complexity of large infrastructure projects with network theory and model-based systems engineering—the example of radioactive waste disposal,"
AbstractView references

Given that model-based systems engineering (MBSE) captures the structure and behavior of an engineered system in an overarching system model, MBSE appears to be a promising approach to managing large infrastructure projects (LIPs). However, it is not apparent how to most appropriately organize the associated system model—and hence the infrastructure project itself. Furthermore, MBSE may today not be readily accepted by the civil engineering industry. In this research, a hypothetical project for the geological disposal of radioactive waste is taken as an example of an LIP and initial system models of the entire disposal project are created. Furthermore, a network representation of the project is generated and examined with network theory. Based on the results, different project organizations are synthesized and evaluated. Eventually, the initial system models are updated to accommodate the most suitable organization according to the network analysis results. In addition, the perception of, and attitude toward MBSE is assessed by means of a cross-sectional survey in a civil engineering company. The generation of system models of LIPs is found to be straightforward. Network theory is able to unveil the complex structure of LIPs in order to identify the most suitable way to organize them and the associated system models. The survey results suggest that MBSE may find broad acceptance in the civil engineering industry. © 2020 Wiley Periodicals LLC
"
10.1007/s10270-020-00794-6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083396373&origin=inward,Article,SCOPUS_ID:85083396373,scopus,2020-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),scalable model views over heterogeneous modeling technologies and resources,"
AbstractView references

When engineering complex systems, models are typically used to represent various systems aspects. These models are often heterogeneous in terms of modeling languages, provenance, number or scale. As a result, the information actually relevant to engineers is usually split into different kinds of interrelated models. To be useful in practice, these models need to be properly integrated to provide global views over the system. This has to be made possible even when those models are serialized or stored in different formats adapted to their respective nature and scalability needs. Model view approaches have been proposed to tackle this issue. They provide unification mechanisms to combine and query various different models in a transparent way. These views usually target specific engineering tasks such as system design, monitoring and evolution. In an industrial context, there can be many large-scale use cases where model views can be beneficial, in order to trace runtime and design-time data, for example. However, existing model view solutions are generally designed to work on top of one single modeling technology (even though model import/export capabilities are sometimes provided). Moreover, they mostly rely on in-memory constructs and low-level modeling APIs that have not been designed to scale in the context of large models stored in different kinds of data sources. This paper presents a general solution to efficiently support scalable model views over heterogeneous modeling resources possibly handled via different modeling technologies. To this intent, it describes our integration approach between a model view framework and various modeling technologies providing access to multiple types of modeling resources (e.g., in XML/XMI, CSV, databases). It also presents how queries on such model views can be executed efficiently by benefiting from the optimization of the different model technologies and underlying persistence backends. Our solution has been evaluated on a practical large-scale use case provided by the industry-driven European MegaM@Rt2 project that aims at implementing a runtime ↔ design time feedback loop. The corresponding EMF-based tooling support, modeling artifacts and reproducible benchmarks are all available online. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.1088/1757-899X/755/1/012076,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85087872597&origin=inward,Conference Paper,SCOPUS_ID:85087872597,scopus,2020-06-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),simcryogenics: a library to simulate and optimize cryoplant and cryodistribution dynamics,"
AbstractView references

In many fields of engineering, conception and operation teams need to perform simulations in order to design systems fulfilling the user requirements and to operate the systems efficiently. To simulate a cryogenic plant and its distribution to the end-users, a large number of commercial or homemade tools are nowadays available. However, there is a lack of available solutions for rapid dynamic simulations either for control with model-based design and for design optimization through parametric studies. This article presents the Simcryogenics library that has been developed at the CEA Cryogenic Engineering Department for several years. This library aims at generating model-based control schemes for cryogenic plants that are subject to high disturbances (such as the pulsed heat loads in fusion reactors or particle accelerators). The library is based on Simscape, the modelling language extension of the Matlab/Simulink software suite, which is very flexible and well documented. This paper introduces how Simcryogenics works, how to use it as well as it provides examples of applications such as the modelling of warm compression stations and cold boxes, the simulation of the cooling of superconducting magnets and RF cavities, the generation of control schemes. © Published under licence by IOP Publishing Ltd.
"
10.1145/3377812.3390806,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094110689&origin=inward,Conference Paper,SCOPUS_ID:85094110689,scopus,2020-06-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),open-vocabulary models for source code (extended abstract),"
AbstractView references

Statistical language modeling techniques have successfully beenapplied to large source code corpora, yielding a variety of newsoftware development tools, such as tools for code suggestion, improving readability, and API migration. A major issue with thesetechniques is that code introduces new vocabulary at a far higherrate than natural language, as new identifier names proliferate.Both large vocabularies and out-of-vocabulary issues severely affect Neural Language Models (NLMs) of source code, degradingtheir performance and rendering them unable to scale.In this paper, we address this issue by: 1) studying how variousmodelling choices impact the resulting vocabulary on a large-scalecorpus of 13,362 projects; 2) presenting an open vocabulary sourcecode NLM that can scale to such a corpus, 100 times larger than inprevious work, and outperforms the state of the art. To our knowledge, this is the largest NLM for code that has been reported. © 2020 Copyright held by the owner/author(s).
"
10.1145/3387940.3391477,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85093100717&origin=inward,Conference Paper,SCOPUS_ID:85093100717,scopus,2020-06-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),applying probabilistic models to c++ code on an industrial scale,"
AbstractView references

Machine learning approaches are widely applied to different research tasks of software engineering, but C/C++ code presents a challenge for these approaches because of its complex build system. However, C and C++ languages still remain two of the most popular programming languages, especially in industrial software, where a big amount of legacy code is still used. This fact prevents the application of recent advances in probabilistic modeling of source code to the C/C++ domain. We demonstrate that it is possible to at least partially overcome these difficulties by the use of a simple token-based representation of C/C++ code that can be used as a possible replacement for more precise representations. Enriched token representation is verified at a large scale to ensure that its precision is good enough to learn rules from. We consider two different tasks as an application of this representation: coding style detection and API usage anomaly detection. We apply simple probabilistic models to these tasks and demonstrate that even complex coding style rules and API usage patterns can be detected by the means of this representation. This paper provides a vision of how different research ML-based methods for software engineering could be applied to the domain of C/C++ languages and show how they can be applied to the source code of a large software company like Samsung. © 2020 ACM.
"
10.1145/3377813.3381354,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092549641&origin=inward,Conference Paper,SCOPUS_ID:85092549641,scopus,2020-06-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rule-based code generation in industrial automation: four large-scale case studies applying the cayenne method,"
AbstractView references

Software development for industrial automation applications is a growing market with high economic impact. Control engineers design and implement software for such systems using standardized programming languages (IEC 61131-3) and still require substantial manual work causing high engineering costs and potential quality issues. Methods for automatically generating control logic using knowledge extraction from formal requirements documents have been developed, but so far only been demonstrated in simplified lab settings. We have executed four case studies on large industrial plants with thousands of sensors and actuators for a rule-based control logic generation approach called CAYENNE to determine its practicability.We found that we can generate more than 70 percent of the required interlocking control logic with code generation rules that are applicable across different plants. This can lead to estimated overall development cost savings of up to 21 percent, which provides a promising outlook for methods in this class. © 2020 IEEE Computer Society. All rights reserved.
"
10.1145/3377811.3380332,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092154911&origin=inward,Conference Paper,SCOPUS_ID:85092154911,scopus,2020-06-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a novel approach to tracing safety requirements and state-based design models,"
AbstractView references

Traceability plays an essential role in assuring that software and systems are safe to use. Automated requirements traceability faces the low precision challenge due to a large number of false positives being returned and mingled with the true links. To overcome this challenge, we present a mutation-driven method built on the novel idea of proactively creating many seemingly correct tracing targets (i.e., mutants of a state machine diagram), and then exploiting model checking within process mining to automatically verify whether the safety requirement's properties hold in the mutants. A mutant is killed if its model checking fails; otherwise, it is survived. We leverage the underlying killed-survived distinction, and develop a correlation analysis procedure to identify the traceability links. Experimental evaluation results on two automotive systems with 27 safety requirements show considerable precision improvements compared with the state-of-the-art. © 2020 Association for Computing Machinery.
"
10.4018/978-1-7998-3591-2.ch004,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124249277&origin=inward,Book Chapter,SCOPUS_ID:85124249277,scopus,2020-06-26,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),heterogeneous large-scale distributed systems on machine learning,"
AbstractView references

Tensor flow is an interface for communicating AI calculations and a use for performing calculations like this. A calculation communicated using tensor flow can be done with virtually zero changes in a wide range of heterogeneous frameworks, ranging from cell phones, for example, telephones and tablets to massive scale-appropriate structures of many computers and a large number of computational gadgets, for example, GPU cards. The framework is adaptable and can be used to communicate a wide range of calculations, including the preparation and derivation of calculations for deep neural network models, and has been used to guide the analysis and send AI frameworks to more than twelve software engineering zones and different fields, including discourse recognition, sight of PCs, electronic technology, data recovery, everyday language handling, retrieval of spatial data, and discovery of device medication. This chapter demonstrates the tensor flow interface and the interface we worked with at Google. © 2020, IGI Global.
"
10.1007/s10515-020-00271-w,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083807732&origin=inward,Article,SCOPUS_ID:85083807732,scopus,2020-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),multi-criteria test cases selection for model transformations,"
AbstractView references

Model transformations play an important role in the evolution of systems in various fields such as healthcare, automotive and aerospace industry. Thus, it is important to check the correctness of model transformation programs. Several approaches have been proposed to generate test cases for model transformations based on different coverage criteria (e.g., statements, rules, metamodel elements, etc.). However, the execution of a large number of test cases during the evolution of transformation programs is time-consuming and may include a lot of overlap between the test cases. In this paper, we propose a test case selection approach for model transformations based on multi-objective search. We use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-offs between two conflicting objectives: (1) maximize the coverage of rules and (2) minimize the execution time of the selected test cases. We validated our approach on several evolution cases of medium and large ATLAS Transformation Language programs. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.
"
10.1007/s00521-019-04226-5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85066890830&origin=inward,Article,SCOPUS_ID:85066890830,scopus,2020-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),extraction of non-functional requirement using semantic similarity distance,"
AbstractView references

Functional and non-functional requirements are important equally in software development. Usually, the requirements are expressed in natural languages. The functional and non-functional requirements are written inter-mixed in software requirement document. The extraction of requirement from the software requirement document is a challenging task. Most of the recent studies adopted a supervised learning approach for the extraction of non-functional requirements. However, there is a drawback of supervised learning such as training of model and retrain if the domain changed. The proposed approach manipulates the textual semantic of functional requirements to identify the non-functional requirements. The semantic similarity is calculated based on co-occurrence of patterns in large human knowledge repositories of Wikipedia. This study finds the similarity distance between the popular indicator keywords and requirement statements to identify the type of non-functional requirement. The proposed approach is applied to PROMISE “NFR dataset.” The performance of the proposed approach is measured in terms of precision, recall and F-measure. Furthermore, the research applies three pre-processing approaches (traditional, part of speech tagging and word augmentation) to increase the performance of NFR extraction. The proposed approach outperforms the results of existing studies. © 2019, Springer-Verlag London Ltd., part of Springer Nature.
"
10.1109/ISCA45697.2020.00013,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091982123&origin=inward,Conference Paper,SCOPUS_ID:85091982123,scopus,2020-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),high-performance deep-learning coprocessor integrated into x86 soc with server-class cpus industrial product,"
AbstractView references

Demand for high performance deep learning (DL) inference in software applications is growing rapidly. DL workloads run on myriad platforms, including general purpose processors (CPU), system-on-chip (SoC) with accelerators, graphics processing units (GPU), and neural processing unit (NPU) addin cards. DL software engineers typically must choose between relatively slow general hardware (e.g., CPUs, SoCs) or relatively expensive, large, power-hungry hardware (e.g., GPUs, NPUs). This paper describes Centaur Technology's Ncore, the industry's first high-performance DL coprocessor technology integrated into an x86 SoC with server-class CPUs. Ncore's 4096 byte-wide SIMD architecture supports INT8, UINT8, INT16, and BF16 datatypes, with 20 tera-operations-per-second compute capability. Ncore shares the SoC ring bus for low-latency communication and work sharing with eight 64-bit x86 cores, offering flexible support for new and evolving models. The x86 SoC platform can further scale out performance via multiple sockets, systems, or third-party PCIe accelerators. Ncore's software stack automatically converts quantized models for Ncore consumption and leverages existing DL frameworks. In MLPerf's Inference v0.5 closed division benchmarks, Ncore achieves 1218 IPS throughput and 1.05ms latency on ResNet-50v1.5 and achieves lowest latency of all Mobilenet-V1 submissions (329 μs). Ncore yields 23x speedup over other x86 vendor percore throughput, while freeing its own x86 cores for other work. Ncore is the only integrated solution among the memory intensive neural machine translation (NMT) submissions. © 2020 IEEE.
"
10.1109/SP40000.2020.00055,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091589922&origin=inward,Conference Paper,SCOPUS_ID:85091589922,scopus,2020-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rigorous engineering for hardware security: formal modelling and proof in the cheri design and implementation process,"
AbstractView references

The root causes of many security vulnerabilities include a pernicious combination of two problems, often regarded as inescapable aspects of computing. First, the protection mechanisms provided by the mainstream processor architecture and C/C++ language abstractions, dating back to the 1970s and before, provide only coarse-grain virtual-memory-based protection. Second, mainstream system engineering relies almost exclusively on test-and-debug methods, with (at best) prose specifications. These methods have historically sufficed commercially for much of the computer industry, but they fail to prevent large numbers of exploitable bugs, and the security problems that this causes are becoming ever more acute.In this paper we show how more rigorous engineering methods can be applied to the development of a new security-enhanced processor architecture, with its accompanying hardware implementation and software stack. We use formal models of the complete instruction-set architecture (ISA) at the heart of the design and engineering process, both in lightweight ways that support and improve normal engineering practice - as documentation, in emulators used as a test oracle for hardware and for running software, and for test generation - and for formal verification. We formalise key intended security properties of the design, and establish that these hold with mechanised proof. This is for the same complete ISA models (complete enough to boot operating systems), without idealisation.We do this for CHERI, an architecture with hardware capabilities that supports fine-grained memory protection and scalable secure compartmentalisation, while offering a smooth adoption path for existing software. CHERI is a maturing research architecture, developed since 2010, with work now underway on an Arm industrial prototype to explore its possible adoption in mass-market commercial processors. The rigorous engineering work described here has been an integral part of its development to date, enabling more rapid and confident experimentation, and boosting confidence in the design. © 2020 IEEE.
"
10.1109/TSE.2018.2864159,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051386705&origin=inward,Article,SCOPUS_ID:85051386705,scopus,2020-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),observation-enhanced qos analysis of component-based systems,"
AbstractView references

We present a new method for the accurate analysis of the quality-of-service (QoS) properties of component-based systems. Our method takes as input a QoS property of interest and a high-level continuous-time Markov chain (CTMC) model of the analysed system, and refines this CTMC based on observations of the execution times of the system components. The refined CTMC can then be analysed with existing probabilistic model checkers to accurately predict the value of the QoS property. The paper describes the theoretical foundation underlying this model refinement, the tool we developed to automate it, and two case studies that apply our QoS analysis method to a service-based system implemented using public web services and to an IT support system at a large university, respectively. Our experiments show that traditional CTMC-based QoS analysis can produce highly inaccurate results and may lead to invalid engineering and business decisions. In contrast, our new method reduced QoS analysis errors by 84.4-89.6 percent for the service-based system and by 94.7-97 percent for the IT support system, significantly lowering the risk of such invalid decisions. © 1976-2012 IEEE.
"
10.1145/3334480.3383071,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090212415&origin=inward,Conference Paper,SCOPUS_ID:85090212415,scopus,2020-04-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),let's chance: playful probabilistic programming for children,"
AbstractView references

Probabilistic thinking has been one of the most powerful ideas in the history of science, and it is rapidly gaining even more relevance as it lies at the core of artificial intelligence (AI) systems and machine learning (ML) algorithms that are increasingly pervading our everyday lives. In this paper, we introduce Let's Chance - A novel computational microworld that extends the widely popular Scratch Programming Language with new types of code blocks and representations that make it accessible for children to encounter and tinker with the rich ideas and sophisticated concepts of probabilistic modeling and learning. Using the tool, children can imagine and code their own expressive, playful, and personally meaningful probabilistic projects, such as-generative art, music, or text; chance-based games and stories; interactive visualizations; and even advanced projects for making a computer learn from input data using simple Markov models of probabilistic learning, among many other creative possibilities. © 2020 Owner/Author.
"
10.1145/3334480.3382892,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090026045&origin=inward,Conference Paper,SCOPUS_ID:85090026045,scopus,2020-04-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ai-generated vs. human artworks. a perception bias towards artificial intelligence?,"
AbstractView references

Via generative adversarial networks (GANs), artificial intelligence (AI) has influenced many areas, especially the artistic field, as symbol of a human task. In human-computer interaction (HCI) studies, perception biases against AI, machines, or computers are generally cited. However, experimental evidence is still lacking. This paper presents a wide-scale experiment in which 565 participants are asked to evaluate paintings (which were created by humans or AI) on four dimensions: liking, perceived beauty, novelty, and meaning. A priming effect is evaluated using two between-subject conditions: Artworks presented as created by an AI, and artworks presented as created by a human artist. Finally, the paintings perceived as being drawn by human are evaluated significantly more highly than those perceived as being made by AI. Thus, using such a methodology and sample in an unprecedented way, the results show a negative bias of perception towards AI and a preference bias towards human systems. © 2020 Owner/Author.
"
10.1145/3313831.3376739,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091308437&origin=inward,Conference Paper,SCOPUS_ID:85091308437,scopus,2020-04-21,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),novice-ai music co-creation via ai-steering tools for deep generative models,"
AbstractView references

While generative deep neural networks (DNNs) have demonstrated their capacity for creating novel musical compositions, less attention has been paid to the challenges and potential of co-creating with these musical AIs, especially for novices. In a needfinding study with a widely used, interactive musical AI, we found that the AI can overwhelm users with the amount of musical content it generates, and frustrate them with its non-deterministic output. To better match co-creation needs, we developed AI-steering tools, consisting of Voice Lanes that restrict content generation to particular voices; Example-Based Sliders to control the similarity of generated content to an existing example; Semantic Sliders to nudge music generation in high-level directions (happy/sad, conventional/surprising); and Multiple Alternatives of generated content to audition and choose from. In a summative study (N=21), we discovered the tools not only increased users' trust, control, comprehension, and sense of collaboration with the AI, but also contributed to a greater sense of self-efficacy and ownership of the composition relative to the AI. © 2020 Owner/Author.
"
10.1145/3383219.3383240,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090822686&origin=inward,Conference Paper,SCOPUS_ID:85090822686,scopus,2020-04-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),mining decision-making processes in open source software development: a study of python enhancement proposals (peps) using email repositories,"
AbstractView references

Open source software (OSS) communities are often able to produce high quality software comparable to proprietary software. The success of an open source software development (OSSD) community is often attributed to the underlying governance model, and a key component of these models is the decision-making (DM) process. While there have been studies on the decision-making processes publicized by OSS communities (e.g., through published process diagrams), little has been done to study decision-making processes that can be extracted using a bottom-up, data-driven approach, which can then be used to assess whether the publicized processes conform to the extracted processes. To bridge this gap, we undertook a large-scale data-driven study to understand how decisions are made in an OSSD community, using the case study of Python Enhancement Proposals (PEPs), which embody decisions made during the evolution of the Python language. Our main contributions are: (a) the design and development of a framework using information retrieval and natural language processing techniques to analyze the Python email archives (comprising 1.48 million emails), and (b) the extraction of decision-making processes that reveal activities that are neither explicitly mentioned in documentation published by the Python community nor identified in prior research work. Our results provide insights into the actual decision-making process employed by the Python community. © 2020 ACM.
"
10.1109/AEMCSE50948.2020.00043,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85088655640&origin=inward,Conference Paper,SCOPUS_ID:85088655640,scopus,2020-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deep learning development review,"
AbstractView references

As a new branch of the machine learning, the nature of deep learning is to establish and simulate the neural network of human brain to analysis and learning. With the development of neural networks, the models are getting bigger and more complex, the network model is no longer a few layers, dozens or even hundreds of network models play a huge advantage. In recent years, various deep neural network models have achieved remarkable results in many fields, such as face recognition, voice recognition, natural language processing and so on. People called these large-scale neural networks 'deep learning'. This paper mainly reviews the development history of deep learning, and make a brief summary of the problems faced by deep learning at the end of the paper. © 2020 IEEE.
"
10.1007/s10009-019-00513-7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062696253&origin=inward,Article,SCOPUS_ID:85062696253,scopus,2020-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a formal approach to aadl model-based software engineering,"
AbstractView references

Formal methods have become a recommended practice in safety-critical software engineering. To be formally verified, a system should be specified with a specific formalism such as Petri nets, automata and process algebras, which requires a formal expertise and may become complex especially with large systems. In this paper, we report our experience in the formal verification of safety-critical real-time systems. We propose a formal mapping for a real-time task model using the LNT language, and we describe how it is used for the integration of a formal verification phase in an AADL model-based development process. We focus on real-time systems with event-driven tasks, asynchronous communication and preemptive fixed-priority scheduling. We provide a complete tool-chain for the automatic model transformation and formal verification of AADL models. Experimentation illustrates our results with the Flight control system and Line follower robot case studies. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.1109/TSE.2018.2859946,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050592485&origin=inward,Article,SCOPUS_ID:85050592485,scopus,2020-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a framework for temporal verification support in domain-specific modelling,"
AbstractView references

In Domain-Specific Modelling (DSM) the general goal is to provide Domain-Specific Modelling Languages (DSMLs) for domain users to model systems using concepts and notations they are familiar with, in their problem domain. Verifying whether a model satisfies a set of requirements is considered to be an important challenge in DSM, but is nevertheless mostly neglected. We present a solution in the form of ProMoBox, a framework that integrates the definition and verification of temporal properties in discrete-time behavioural DSMLs, whose semantics can be described as a schedule of graph rewrite rules. Thanks to the expressiveness of graph rewriting, this covers a very large class of problems. With ProMoBox, the domain user models not only the system with a DSML, but also its properties, input model, run-time state and output trace. A DSML is thus comprised of five sublanguages, which share domain-specific syntax, and are generated from a single metamodel. Generic transformations to and from a verification backbone ensure that both the language engineer and the domain user are shielded from underlying notations and techniques. We explicitly model the ProMoBox framework's process in the paper. Furthermore, we evaluate ProMoBox to assert that it supports the specification and verification of properties in a highly flexible and automated way. © 1976-2012 IEEE.
"
10.1109/TR.2019.2898351,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074283946&origin=inward,Article,SCOPUS_ID:85074283946,scopus,2020-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a template-based methodology for the specification and automated composition of performability models,"
AbstractView references

Dependability and performance analysis of modern systems is facing great challenges: their scale is growing, they are becoming massively distributed, interconnected, and evolving. Such complexity makes model-based assessment a difficult and time-consuming task. For the evaluation of large systems, reusable submodels are typically adopted as an effective way to address the complexity and to improve the maintainability of models. When using state-based models, a common approach is to define libraries of generic submodels, and then compose concrete instances by state sharing, following predefined 'patterns' that depend on the class of systems being modeled. However, such composition patterns are rarely formalized, or not even documented at all. In this paper, we address this problem using a model-driven approach, which combines a language to specify reusable submodels and composition patterns, and an automated composition algorithm. Clearly defining libraries of reusable submodels, together with patterns for their composition, allows complex models to be automatically assembled, based on a high-level description of the scenario to be evaluated. This paper provides a solution to this problem focusing on: formally defining the concept of model templates, defining a specification language for model templates, defining an automated instantiation and composition algorithm, and applying the approach to a case study of a large-scale distributed system. © 1963-2012 IEEE.
"
10.1016/j.cpc.2019.106966,S0010465519303169,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85073003832&origin=inward,Article,SCOPUS_ID:85073003832,scopus,2020-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),celeris base: an interactive and immersive boussinesq-type nearshore wave simulation software,"
                  We introduce our interactive and immersive coastal wave simulation software, Celeris Base, which is the successor to Celeris Advent. Celeris Base is an open source software developed in the Unity3D game engine and in C# language. It supports an interactive environment and allows users to view the simulations in a virtual reality headset. Celeris Base solves the same equations as Celeris Advent, the extended Boussinesq equations, using our hybrid finite volume–finite difference method. These equations are solved on the GPU using compute shaders, written in HLSL. Celeris Base has several new features such as 360°video capturing, geographic map overlays, built-in real-time gauge plotters, etc. It also improves the implementation of the sponge layer boundary condition by introducing new damping equations. Celeris Base is designed and implemented using the best software engineering practices in the hope that it will be a base for further developments of the Celeris software series by researchers around the globe. We validate Celeris Base against experimental results in this paper.
               
                  Program summary
                  
                     Program Title: Celeris Base
                  
                     Program Files doi: 
                     http://dx.doi.org/10.17632/jdx7tddcxz.1
                  
                  
                     Licensing provisions: MIT License
                  
                     Programming language: C#, HLSL
                  
                     Nature of problem: Celeris Advent enabled researchers and engineers for the first time to simulate nearshore waves with a Boussinesq-type model, faster than real-time and in an interactive environment. However, its development platform and implementation complexity hindered researchers from developing it further and made adding new features to the software a daunting task. The software used graphics shaders to solve scientific equations which could be confusing for many. The visualization environment was wired from scratch which made it very difficult to add features such as virtual reality.
                  
                     Solution method: A new software is developed completely from scratch following Celeris Advent, called Celeris Base. This software uses the same hybrid finite volume–finite difference scheme to solve the extended Boussinesq equations, but using a variant of shaders called compute shaders, removing possible barriers for other researchers to understand the code and develop it further. The software is developed in Unity3D, a popular game engine with a large and helpful community as well as thousands of ready to use plugins. Celeris Base is equipped with virtual reality and is the first nearshore simulation software to provide this feature.
               "
10.1145/3385032.3385039,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85082660268&origin=inward,Conference Paper,SCOPUS_ID:85082660268,scopus,2020-02-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),clustering glossary terms extracted from large-sized software requirements using fasttext,"
AbstractView references

Specialized terms used in the requirements document should be defined in a glossary.We propose a technique for automated extraction and clustering of glossary terms from large-sized requirements documents.We use text chunking combined withWordNet removal to extract candidate glossary terms. Next, we apply a state-of-the art neural word embeddings model for clustering glossary terms based on semantic similarity measures. Word embeddings are capable of capturing the context of a word and compute its semantic similarity relation with other words used in a document. Its use for clustering ensures that terms that are used in similar ways belong to the same cluster.We apply our technique to the CrowdRE dataset, which is a large-sized dataset with around 3000 crowd-generated requirements for smart home applications. To measure the effectiveness of our extraction and clustering technique we manually extract and cluster the glossary terms from CrowdRE dataset and use it for computing precision, recall and coverage. Results indicate that our approach can be very useful for extracting and clustering of glossary terms from a large body of requirements. © 2020 Association for Computing Machinery.
"
10.1051/e3sconf/202014501040,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079784087&origin=inward,Conference Paper,SCOPUS_ID:85079784087,scopus,2020-02-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research on the emotional cognitive evaluation model based on artificial neural network,"
AbstractView references

the neural network, fuzzy set theory and evolutionary algorithm in artificial intelligence are all intelligent information processing theories that follow the biological processing mode. These theories are realized by rational logical thinking mode without considering the role of human perceptual thinking in the information processing process, such as emotion and cognition. Among them, the neural network mainly imitates the function of the mental system of human, adopts the method from the bottom to the top, and processes the difficult language pattern information through a large number of complicated connections of neurons. Artificial neural network (Ann) is a cross research field of artificial intelligence and life science. This theory mainly imitates the information processing mechanism of organisms in nature and is mainly used in intelligent information processing systems that can adapt to long-term changes in the environment. Therefore, neural network has important application significance in the research of intelligence, robot and artificial emotion. © The Authors, published by EDP Sciences 2020.
"
10.1145/3377024.3377031,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079867676&origin=inward,Conference Paper,SCOPUS_ID:85079867676,scopus,2020-02-05,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"fast static analyses of software product lines - an example with more than 42,000 metrics","
AbstractView references

Context: Software metrics, as one form of static analyses, is a commonly used approach in software engineering in order to understand the state of a software system, in particular to identify potential areas prone to defects. Family-based techniques extract variability information from code artifacts in Software Product Lines (SPLs) to perform static analysis for all available variants. Many different types of metrics with numerous variants have been defined in literature. When counting all metrics including such variants, easily thousands of metrics can be defined. Computing all of them for large product lines can be an extremely expensive process in terms of performance and resource consumption. Objective: We address these performance and resource challenges while supporting customizable metric suites, which allow running both, single system and variability-aware code metrics. Method: In this paper, we introduce a partial parsing approach used for the efficient measurement of more than 42,000 code metric variations. The approach covers variability information and restricts parsing to the relevant parts of the Abstract Syntax Tree (AST). Conclusions: This partial parsing approach is designed to cover all relevant information to compute a broad variety of variability-aware code metrics on code artifacts containing annotation-based variability, e.g., realized with C-preprocessor statements. It allows for the flexible combination of single system and variability-aware metrics, which is not supported by existing tools. This is achieved by a novel representation of partially parsed product line code artifacts, which is tailored to the computation of the metrics. Our approach consumes considerably less resources, especially when computing many metric variants in parallel. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
10.1109/ICETCE48199.2020.9091744,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85085573691&origin=inward,Conference Paper,SCOPUS_ID:85085573691,scopus,2020-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),offline handwritten character classification of the same scriptural family languages by using transfer learning techniques,"
AbstractView references

Transfer Learning by using Convolutional Neural Network has shown its outstanding performance in large scale image classification. India is a multi-script and multilingual country. Out of all languages, Telugu and Kannada have shared almost similar structure characters. Offline character recognition of both handwriting characters is a challenging task. Different feature extraction models have been used in character recognition studies. Convolutional Neural Network is used as efficiently supervised feature vector extraction. Feature vectors from large scale pre-trained ImageNet or COCO were proved more efficient than other script datasets for better result. Fine-tuning model of transfer learning was used in the studies for comparison studies. © 2020 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85082541914&origin=inward,Article,SCOPUS_ID:85082541914,scopus,2020-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),development of an online printing press production and sales management system using software engineering workbenches,"
AbstractView references

This project primarily aimed to develop an Online Production and Sales Management System for a printing press company. It is an onlinebased application designed for the management of its business production and sales that is more efficient and synchronized than the current manual system. Specifically, it focused on the development of the following modules: (1) File Maintenance Module; (2) Material Inventory Management Module; (3) Job Order Management Module; (4) Billing Module; (5) Payment and Accounting Module; (6) Report Generation Module; (7) Security Module; and (8) User Management Module. The system minimized the company's production and sales endeavor for every transaction and processes they go through, reduced impediments, and upgraded the system into a new and improved one. It enabled handling a large amount of data and made the transaction processing faster and easier. The development of the system was guided by the two main methods in developing systems, namely: Rapid Application Development (RAD) and Model-Driven Development (MDD) methods. The tools used in the development are Microsoft Visual Studio 2010 Ultimate Edition as an integrated development environment (IDE), MS SQL Server 2008 as the database management system, C# 2010 as the programming language combine with AJAX, CSS, jQuery, HTML 5, and ASP.Net 4.0 as framework or platform. The implementation of these methodologies was aided by utilizing Software Engineering Workbenches (SEWs). These workbenches are integrated into Microsoft Visual Studio 2010 Ultimate Edition. Based on the evaluation of the system, the proposed system is highly acceptable by a respondent printing press management and employees, customers, and IT experts. In terms of usability, the proposed system is rated with an average mean of 6.4 described as ""Strongly Agree."" Thus, it was concluded that the developed system offers functions that can help improve the performance of a printing press company services and is highly recommended for its immediate utilization. © IJSTR 2020.
"
10.5220/0008990001160125,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173952729&origin=inward,Conference Paper,SCOPUS_ID:85173952729,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),graph-based model inspection tool for multi-disciplinary production systems engineering,"
AbstractView references

Background. In Production Systems Engineering (PSE), the planning of production systems involves domain experts from various domains, such as mechanical, electrical and software engineering collaborating and modeling their specific views on the system. These models, describing entire plants, can reach a large size (up to several GBs) with complex relationships and dependencies. Due to the size, ambiguous semantics and diverging views, consistency of data and the awareness of changes are challenging to track. Aim. In this paper we explore visualizations mechanisms for a model inspection tool to support consistency checking and the awareness of changes in multi-disciplinary PSE environments, as well has more efficient handing of AutomationML (AML) files. Method. We explore various visualization capabilities that are suitable for hierarchical structures common in PSE and identified requirements for a model-inspection tool for PSE purposes based on workshops with our company partner. A proof-of concept software prototype is developed based on the elicited requirements. Results. We evaluate the effectiveness of our Information Visualisation (InfoVis) approach in comparison to a standard modeling tool in PSE, the AutomationML Editor. The evaluation showed promising results for handling large-scale engineering models based on AML for the selected scenarios, but also areas for future improvement, such as more advanced capabilities. Conclusion. Although InfoVis was found useful in the evaluation context, in-depth analysis with domain experts from industry regarding usability and features remain for future work. © 2022 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138355043&origin=inward,Conference Paper,SCOPUS_ID:85138355043,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"impact of tokenization, pretraining task, and transformer depth on text ranking","
AbstractView references

This paper documents the University of Amsterdam's participation in the TREC 2020 Deep Learning Track. Rather than motivated by engineering the best scoring system, our work is motivated by our interest in analysis, informing our understanding of the opportunities and challenges of transformers for text ranking. Specifically, we focus on the passage retrieval task where we try to answer three of sets of questions. First, transformers use different tokenization than traditional IR approaches such as stemming and lemmatizing, leading to different document representations. What is the effect of modern preprocessing techniques on traditional retrieval algorithms? Our main observation is that the limited vocabulary of the BERT tokenizer is affecting many long-tail tokens, which leads to large gains in efficiency at the cost of a small decrease in effectiveness. Second, the effectiveness of transformers is a result of the self-supervised pre-training task promoting general language understanding, ignorant of the specific demands of ranking tasks. Can we make further correlate queries and relevant passages in the pre-training task? Our main observation is that there is a whole continuum between the original self-supervised training task of BERT and the final interaction ranker, and isolating ranking-aware pre-training tasks may leads to gains in efficiency (as these pretrained models can be reused for many tasks) as well as to gains in effectiveness (in particular when limited data on the target task is available). Third, transformers combine large sequence length with many layers, with unclear what this deep semantics adds in the context of ranking. How complex do the models need to be in order to perform well on this task? Our main observation is that the deep layers of BERT lead to some, but relatively modest, gains in performance, but that the exact role of the presumed superior language understanding for search is far from clear. © 2020 29th Text REtrieval Conference, TREC 2020 - Proceedings. All Rights Reserved.
"
10.14778/3415478.3415499,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135017705&origin=inward,Article,SCOPUS_ID:85135017705,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),spaqltools: a stochastic package query interface for scalable constrained optimization,"
AbstractView references

Everyone needs to make decisions under uncertainty and with limited resources, e.g., an investor who is building a stock portfolio subject to an investment budget and a bounded risk tolerance. Doing this with current technology is hard. There is a disconnect between software tools for data management, stochastic predictive modeling (e.g., simulation of future stock prices), and optimization; this leads to cumbersome analytical workflows. Moreover, current methods do not scale. To handle a broad class of uncertainty models, analysts approximate the original stochastic optimization problem by a large deterministic optimization problem that incorporates many “scenarios”, i.e., sample realizations of the uncertain data values. For large problems, a huge number of scenarios is required, often causing the solver to fail. We demonstrate sPaQL-TooLs, a system for in-database specification and scalable solution of constrained optimization problems. The key ingredients are (i) a database-oriented specification of constrained stochastic optimization problems as “stochastic package queries” (SPQs), (ii) use of a Monte Carlo database to incorporate stochastic predictive models, and (iii) a new SUMMARYSEARCH algorithm for scalably solving SPQs with approximation guarantees. In this demonstration, the attendees will experience first-hand the difficulty of manually constructing feasible and high-quality portfolios, using real-world stock market data. We will then demonstrate how SUMMARY-SEARCH can easily and efficiently help them find very good portfolios, while being orders of magnitude faster than prior methods. © VLDB Endowment. All rights reserved.
"
10.18420/inf2020_82,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127306445&origin=inward,Conference Paper,SCOPUS_ID:85127306445,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),citcom – citation recommendation,"
AbstractView references

Citation recommendation aims to predict references based on a given text. In this paper, we focus on predicting references using small passages instead of a whole document. Besides using a search engine as baseline, we introduce two further more advanced approaches that are based on neural networks. The first one aims to learn an alignment between a passage encoder and reference embeddings while using a feature engineering approach including a simple feed forward network. The second model takes advantage of BERT, a state-of-the-art language representation model, to generate context-sensitive passage embeddings. The predictions of the second model are based on inter-passage similarities between the given text and indexed sentences, each associated with a set of references. For training and evaluation of our models, we prepare a large dataset consisting of English papers from various scientific disciplines. © 2020 Gesellschaft fur Informatik (GI). All rights reserved.
"
10.1016/B978-0-12-820543-3.00010-9,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124929687&origin=inward,Book Chapter,SCOPUS_ID:85124929687,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"ai, autonomous machines and human awareness: towards shared machine-human contexts in medicine","
AbstractView references

Medical curricula trend to integrate clinical skills training and to create efficiencies in preclinical medical sciences, but the rapid emergence big data-intensive health care has led to initiating collaborations among data scientists, computer engineers, and medical educators that might generate novel educational high-technology platforms and innovative AI practice applications. The preprocessing of big data improves neural network feature recognition, improving the speed and accuracy of AI diagnostics and permitting chronic disease predictions. Applications of generative adversarial networks to create virtual patient phenotypes and image sets exposes medical learners to endless illness presentations, improving system-1 critical thinking for differential diagnosis development. AI offers great potential for education data managers working in support of medical educators and learners. These opportunities to build a shared context, in keeping with these themes of this book, include emerging data-driven AI applications for medical education and provider training include individual aptitude-based career advising, early identification of learners with academic difficulties, highly focused e-tutoring interventions, and natural language processing of standardized exam questions. © 2020 Elsevier Inc.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85117923344&origin=inward,Conference Paper,SCOPUS_ID:85117923344,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),puzzling machines: a challenge on learning from small data,"
AbstractView references

Deep neural models have repeatedly proved excellent at memorizing surface patterns from large datasets for various ML and NLP benchmarks. They struggle to achieve human-like thinking, however, because they lack the skill of iterative reasoning upon knowledge. To expose this problem in a new light, we introduce a challenge on learning from small data, PuzzLing Machines, which consists of Rosetta Stone puzzles from Linguistic Olympiads for high school students. These puzzles are carefully designed to contain only the minimal amount of parallel text necessary to deduce the form of unseen expressions. Solving them does not require external information (e.g., knowledge bases, visual signals) or linguistic expertise, but meta-linguistic awareness and deductive skills. Our challenge contains around 100 puzzles covering a wide range of linguistic phenomena from 81 languages. We show that both simple statistical algorithms and state-of-the-art deep neural models perform inadequately on this challenge, as expected. We hope that this benchmark, available at https://ukplab.github.io/PuzzLing-Machines/, inspires further efforts towards a new paradigm in NLP-one that is grounded in human-like reasoning and understanding. © 2020 Association for Computational Linguistics
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105980029&origin=inward,Conference Paper,SCOPUS_ID:85105980029,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deepvar: an end-to-end deep learning approach for genomic variant recognition in biomedical literature,"
AbstractView references

We consider the problem of Named Entity Recognition (NER) on biomedical scientific literature, and more specifically the genomic variants recognition in this work. Significant success has been achieved for NER on canonical tasks in recent years where large data sets are generally available. However, it remains a challenging problem on many domain-specific areas, especially the domains where only small gold annotations can be obtained. In addition, genomic variant entities exhibit diverse linguistic heterogeneity, differing much from those that have been characterized in existing canonical NER tasks. The state-of-the-art machine learning approaches heavily rely on arduous feature engineering to characterize those unique patterns. In this work, we present the first successful end-to-end deep learning approach to bridge the gap between generic NER algorithms and low-resource applications through genomic variants recognition. Our proposed model can result in promising performance without any handcrafted features or post-processing rules. Our extensive experiments and results may shed light on other similar low-resource NER applications. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85102028399&origin=inward,Article,SCOPUS_ID:85102028399,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a model of co-construction for curriculum and professional development in head start: the readiness through integrative science and engineering (rise) approach,"
AbstractView references

Background/Context: In the context of increasing accountability mandates in the preK-12 education system, the importance of professional development (PD) supports for early childhood educators is recognized. Education leaders emphasize the importance of partnering with teachers to inform the development of effective PD approaches. This partnering process is often referred to as co-construction. Co-construction with teachers is thought to be an essential element for ensuring that the learnings gained from any PD program are maintained once intensive supports are removed. However, guidance is scant concerning specific aspects of effective co-construction. Purpose of Study: In this article, we document the process of co-construction within the Readiness through Integrative Science and Engineering (RISE) curriculum and PD approach. In so doing, we hope to illuminate processes potentially at work within the ""black box"" of PD. Setting/Participants: The RISE project was implemented at two Head Start program sites that served a high proportion of dual language learning children and immigrant families in a large city in the northeastern United States. Participants were teachers and parent volunteers from these two programs. Research Design: Using grounded theory methods, qualitative data on implementation across key RISE contexts were analyzed. Data collected across three years included digital audio- and video-recorded interactions among participants, written documentation of meeting agendas, planning notes, and meeting notes. Results: Analyses resulted in the articulation of a three-step process: (1) setting the conditions for co-construction (establishing mutual respect and trust among partners, leveling roles and authority, and validating/naming partners' expertise); (2) establishing joint activities as the core process (setting shared goals and agenda, building relationships, and validating coconstructed products); and (3) observing outcomes of co-construction (shifts in attitudes and interactional roles, appropriation of RISE concepts, and integration of RISE components). Conclusions/Recommendations: The RISE model of co-construction comports with what others in the field have proposed about the importance of teacher input into their own professional learning, adding further dimensionality through systematic documentation and grounded theory analysis. We discuss how the RISE co-construction approach is similar to and distinct from other such efforts in the field of early childhood education, and we suggest future directions for research to document and test effective PD processes. © 2020 Teachers College, Columbia University. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85102015094&origin=inward,Conference Paper,SCOPUS_ID:85102015094,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the potential of using generative adversarial networks in military simulations,"
AbstractView references

The impact of Artificial Intelligence (AI) is significant and accelerating, but what does it currently do well, what can it do better, and how could one of its most capable implementations impact military simulation? Deep Neural Networks have shown remarkable capabilities and can be taught to evaluate a state such as game piece positions and select the next move or recognize patterns, but they have not been taught to learn (that is, to improve in an automated fashion from experience without having to be explicitly programmed). To develop a form of AI that could create original patterns, in 2014 Goodfellow et al. [1] introduced the Generative Adversarial Network (GAN). Initial research into GAN technology was primarily in the area of image creation but it has since been applied to fields such as nuclear physics, cosmology, and engineering design. As subtle patterns are pervasive in our world, the potential of GAN technology to create new instances is extensive. This paper provides an overview of GANs and associated on-going research. It then proposes several applications of GANs to military simulation including; refinement of data (e.g., generation of synthetic instances of surveillance imagery or target signatures), probability distribution creation, and invention of behaviors. © 2020 Simulation Innovation Workshop.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85101595163&origin=inward,Conference Paper,SCOPUS_ID:85101595163,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the application of system modelling language (sysml) in an aviation structure and maintenance system,"
AbstractView references

The aviation maintenance sector is an example of a large complex system since it integrates several systems and their sub-components that require frequent updates and maintenance. A survey of the literature shows that aviation maintenance documentation is preserved using an outdated paper-based approach, making the maintenance process more difficult for all stakeholders involved. In response, to ensure the accuracy of a system’s specifications and documentation, we propose a systemic approach to support the design process of complex systems. This study develops a modelling approach using Systems Modeling Language (SysML) as a way to document maintenance procedures for an important military aircraft - the EA-6B. Structural and behavioral aspects of the model are developed to examine the use of a model-based approach in aviation maintenance documentation. In addition, a demonstration of the documentation steps of the nose radome assembly of the EA-6B aircraft and different SysML diagrams are discussed. The proposed alternative to the current paper-based approach to record maintenance would serve as a roadmap for practitioners who intend to document the detail of aviation maintenance using a model-based methodology. Copyright, American Society for Engineering Management, 2020.
"
10.1007/978-3-030-66763-4_10,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85101308565&origin=inward,Conference Paper,SCOPUS_ID:85101308565,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),machine learning approaches for rapid pothole detection from 2d images,"
AbstractView references

Roads are inevitable parts of human civilisation, and construction of roads are considered under a Civil Engineering problem; but periodically these roads require maintenance and assessment, which is highly dependent on adequate and timely pavement condition data. Howbeit, in some cases, it has been found that the manual practice of collecting and analysing such data often leads to delay in reporting about the issues and fixing them on time. Also, repairing potholes is time consuming, and locating these manually is a huge task. We want to find out some mechanism which can identify the construction conditions as well as any kind of deformities on the road from the dashboard camera fitted into a car, and at the same time, can analyse the conditions of road surface and formation of potholes on the road. Optimization of manual pothole detection through automation has been a part of scientific research since long. Pothole identification has significantly been adapted in different screening and maintenance systems. But in our country, owing to the large number of road networks and wide variations in the nature of rural and urban road conditions, it is very difficult to identify potholes through an automated system. In this paper, we have looked into several methods of Computer Vision, like image processing techniques and object detection method so as to identify potholes from the video input stream to the system. But these techniques have been found to have different challenges like lighting conditions, interference in the line of vision on waterlogged roads, and inefficiency at night vision. Hence, furthermore, we have explored the viability of Deep Learning method for identifying the potholes from the processing of input video streams, and have also analysed the Convolutional Neural Networks approach of Deep Learning through a self-built CNN model. In this paper, the expediency of all the methods as well as their drawbacks have been discussed. © 2020, Springer Nature Switzerland AG.
"
10.1016/j.procir.2021.01.076,S2212827121001037,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85101070144&origin=inward,Conference Paper,SCOPUS_ID:85101070144,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),process-driven approach within the engineering domain by combining business process model and notation (bpmn) with process engines,"Digitization within the framework of Industry 4.0 is considered the biggest and fastest driver of change in history of manufacturing industry. While the size of a company is becoming less essential, the ability to adapt quickly to changing market conditions and new technologies is more important than ever. This trend particularly applies to the companies’ software landscapes, where individual sub-processes and services must be orchestrated, seamlessly integrated, and iteratively renewed according to the ever-increasing user requirements. However, inflexible, closed monolithic software applications as well as self-programmed stand-alone tools that are difficult to integrate are still predominant in the engineering domain. A complete reimplementation of existing, proprietary engineering tools and their integration into monolithic applications of large software providers is often not economically feasible, especially for small and medium-sized machinery and plant manufacturers. In this context, the so-called Process-Driven Approach (PDA) offers a sustainable and tool-neutral opportunity for process and tool orchestration, enabling an easy integration of individual software applications by consistent utilization of the separation of concerns principle. The PDA, originating from business informatics, is mainly based on the standardized and machine-executable visual modeling language Business Process Model and Notation (BPMN). Using the semantic enhancements found in version 2.0, BPMN is not just used to model the business processes but also to model and execute the integration processes between different systems. After the PDA has already been successfully applied to large-scale projects in business informatics, it is now being transferred to the engineering domain. As shown in this paper, PDA allows to orchestrate the different processes in engineering and to integrate the underlying software tools, such as e-mail or spreadsheet applications, engineering tools, or custom microservices, using standardized interfaces like REST API. In doing so, engineering processes can be made more transparent, monitored, and optimized by means of appropriate key figures. The concept is validated by a prototypical implementation of a minimum functional PDA architecture for the engineering domain."
10.1115/OMAE2020-18063,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100058079&origin=inward,Conference Paper,SCOPUS_ID:85100058079,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),software application based on subsea engineering design codes,"
AbstractView references

Development of software application for subsea engineering design and analysis is to a large extent based on codes and standards used in the offshore industry when considering subsea pipelines. In this paper a software is described which main purpose is to facilitate the design and analysis process and such that results and documentation are automatically generated to increase quality of documentation. Current scope is a standard calculation tool covering different aspects of design in compliance with relevant offshore codes. A modularization technique is used to divide the software system into multiple discrete and independent modules based on offshore codes, which are capable of carrying out task(s) independently. All modules in range operate from a project model that is accessed directly by other modules for analysis and performance prediction and allows design changes to flow through automatically to facilitate smooth communication and coordination between different design activities. All the modules have a number of common design features. The quality of an implementation of each offshore code in independent software modules is measured by defining the level of inter-dependability among modules and their interaction among them, and by defining the degree of intra-dependability within elements of a module. This modularization technique also includes other benefits, such as ease of maintenance and updates. The improvements are related to the objectives of a state-of-the-art procedure of performing engineering, design and analysis by use of offshore codes implemented in a software application. The application is developed in.NET C# language with MS Visual Studio Technology that provides a powerful graphical user interface well integrated in windows environment. Copyright © 2020 ASME
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099644982&origin=inward,Conference Paper,SCOPUS_ID:85099644982,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),calibrating structured output predictors for natural language processing,"
AbstractView references

We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. However, the output space of such structured prediction models is often too large to adapt binary or multi-class calibration methods directly. In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. Our proposed method can be used with any binary class calibration scheme and a neural network model. Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. We show that our method outperforms current calibration techniques for named-entity-recognition, part-of-speech and question answering. We also improve our model's performance from our decoding step across several tasks and benchmark datasets. Our method improves the calibration and model performance on out-of-domain test scenarios as well. © 2020 Association for Computational Linguistics
"
10.3844/jcssp.2020.1718.1730,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85098787725&origin=inward,Article,SCOPUS_ID:85098787725,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),conceptual software engineering applied to movie scripts and stories,"
AbstractView references

According to researchers, a large proportion of research software (software on which researchers rely) is fragile and the source of numerous problems that plague computational science. This study introduces another application of software engineering tools, conceptual modeling, which can be applied to other fields of research. One way to strengthen the relationship between software engineering and other fields is to develop a good way to perform conceptual modeling that is capable of addressing the peculiarities of these fields of study. This study concentrates on humanities and social sciences, which are usually considered “softer” and further away from abstractions and (abstract) machines. Specifically, we focus on conceptual modeling as a software engineering tool (e.g., UML) in the area of stories and movie scripts. Researchers in the humanities and social sciences might not use the same degree of formalization that engineers do, but they still find conceptual modeling useful. Current modeling techniques (e.g., UML) fail in this task because they are geared toward the creation of software systems. Similar Conceptual Modeling Language (e.g., ConML) has been proposed with the humanities and social sciences in mind and, as claimed, can be used to model “anything.” This study is a venture in this direction, where a software modeling technique, Thinging Machine (TM), is applied to movie scripts and stories. The paper presents a novel approach to developing diagrammatic static/dynamic models of movie scripts and stories. The TM model diagram serves as a neutral and independent representation for narrative discourse and can be used as a communication instrument among participants. It is based on the notion of thing/machine (or thimac). Things and events are defined as what can be created, released, transferred, received, accepted and processed. Machines are what create, process, release, transfer and receive things. The examples presented include examples from Propp’s model of fairytales; the railway children and an actual movie script seem to point to the viability of the approach. © 2020 Sabah Al-Fedaghi.
"
10.1007/978-3-030-63322-6_61,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85098178362&origin=inward,Conference Paper,SCOPUS_ID:85098178362,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),data harmonization for heterogeneous datasets in big data - a conceptual model,"
AbstractView references

Data comes from machines, transactions, and social media, which is gigantic and disparate in nature. About 80% of today’s data is unstructured, while the remaining percentage is semistructured and structured. It is a big challenge for management to make efficient decisions on run time and also to store heterogeneous nature of data by existing tools. Data Harmonization can be used to solve the heterogeneity problem; the idea of data harmonization is to provide a uniform representation and remove all forms of heterogeneity from the heterogeneous datasets. In recent studies, various models have been developed for integrating, mapping, and fusion of structured and semistructured datasets, but no such model has been developed for structured, semistructured, and unstructured datasets. Information extraction is used as a vital component to extract data from different textual datasets that information formats may comprise in different file formats, i.e., Excel, JSON, and text. For developing textual data harmonization model for heterogeneous datasets, comprises of structured, semistructured, and unstructured data based on phrases similarity techniques, it needs to be first preprocessed using Natural Language Processing and its techniques like Bag of Phrases, Parts of Speech and so on. Therefore this paper focuses on the conceptual data harmonization model based on text similarity technique, which will help to blend structured, semistructured, and unstructured data. The selected phrases from heterogeneous datasets will go through training and testing using Recurrent Neural Network. © 2020, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85097680558&origin=inward,Conference Paper,SCOPUS_ID:85097680558,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the knowledge transfer problem in systems engineering,"
AbstractView references

Systems Engineering (SE) processes are increasingly critical to achieving success in large scale complex engineered systems (LSCES). The International Council on Systems Engineering (INCOSE) has established that a 10:1 increase in the number of SE practitioners is needed to meet growing demand. Current SE processes are often highly technical and complex, presenting significant knowledge transfer barriers for potential SE practitioners. To achieve the goal of a 10:1 expansion in the number of SE practitioners, the next generation of knowledge transfer tools used in SE must be informed by communication practices. This paper examines current practices in SE and how they act as knowledge transfer tools. Three current practices are discussed: Systems Modeling Language (SysML) diagrams, requirements documentation, and Pattern Based Systems Engineering (PBSE). Communication challenges in these current practices are identified. Aspects of human communication that must be considered to create more effective SE knowledge transfer tools are discussed. By incorporating knowledge of communication from disciplines outside of engineering, SE practitioners may be able to develop more effective models that efficiently transfer system-level knowledge. © 2020 The MITRE Corporation. All Rights Reserved.
"
10.1007/978-3-030-62522-1_16,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85097368279&origin=inward,Conference Paper,SCOPUS_ID:85097368279,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),supporting collaborative modeling via natural language processing,"
AbstractView references

Engineering large-scale systems requires the collaboration among experts who use different modeling languages and create multiple models. Due to their independent creation and evolution, these models may exhibit discrepancies in terms of the domain concepts they represent. To help re-align the models without an explicit synchronization, we propose a technique that provides the modelers with suggested concepts that they may be interested in adding to their own models. The approach is modeling-language agnostic since it processes only the text in the models, such as the labels of elements and relationships. In this paper, we focus on determining the similarity of compound nouns, which are frequently used in conceptual models. We propose two algorithms, that make use of word embeddings and domain models, respectively. We report an early validation that assesses the effectiveness of our similarity algorithms against state-of-the-art machine learning algorithms with respect to human judgment. © 2020, Springer Nature Switzerland AG.
"
10.1007/978-3-030-58545-7_8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85097046740&origin=inward,Conference Paper,SCOPUS_ID:85097046740,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),soft expert reward learning for vision-and-language navigation,"
AbstractView references

Vision-and-Language Navigation (VLN) requires an agent to find a specified spot in an unseen environment by following natural language instructions. Dominant methods based on supervised learning clone expert’s behaviours and thus perform better on seen environments, while showing restricted performance on unseen ones. Reinforcement Learning (RL) based models show better generalisation ability but have issues as well, requiring large amount of manual reward engineering is one of which. In this paper, we introduce a Soft Expert Reward Learning (SERL) model to overcome the reward engineering designing and generalisation problems of the VLN task. Our proposed method consists of two complementary components: Soft Expert Distillation (SED) module encourages agents to behave like an expert as much as possible, but in a soft fashion; Self Perceiving (SP) module targets at pushing the agent towards the final destination as fast as possible. Empirically, we evaluate our model on the VLN seen, unseen and test splits and the model outperforms the state-of-the-art methods on most of the evaluation metrics. © 2020, Springer Nature Switzerland AG.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096619109&origin=inward,Conference Paper,SCOPUS_ID:85096619109,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),effort estimation in named entity tagging tasks,"
AbstractView references

Named Entity Recognition (NER) is an essential component of many Natural Language Processing pipelines. However, building these language dependent models requires large amounts of annotated data. Crowdsourcing emerged as a scalable solution to collect and enrich data in a more time-efficient manner. To manage these annotations at scale, it is important to predict completion timelines and compute fair pricing for workers in advance. To achieve these goals, we need to know how much effort will be taken to complete each task. In this paper, we investigate which variables influence the time spent on a named entity annotation task by a human. Our results are two-fold: first, the understanding of the effort-impacting factors which we divided into cognitive load and input length; and second, the performance of the prediction itself. On the latter, through model adaptation and feature engineering, we attained a Root Mean Squared Error (RMSE) of 25.68 words per minute with a Nearest Neighbors model. © European Language Resources Association (ELRA), licensed under CC-BY-NC
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096580602&origin=inward,Conference Paper,SCOPUS_ID:85096580602,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sensitive data detection and classification in spanish clinical text: experiments with bert,"
AbstractView references

Massive digital data processing provides a wide range of opportunities and benefits, but at the cost of endangering personal data privacy. Anonymisation consists in removing or replacing sensitive information from data, enabling its exploitation for different purposes while preserving the privacy of individuals. Over the years, a lot of automatic anonymisation systems have been proposed; however, depending on the type of data, the target language or the availability of training documents, the task remains challenging still. The emergence of novel deep-learning models during the last two years has brought large improvements to the state of the art in the field of Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018, and the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to conduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT to other algorithms. The experiments show that a simple BERT-based model with general-domain pre-training obtains highly competitive results without any domain specific feature engineering. © European Language Resources Association (ELRA), licensed under CC-BY-NC
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096578367&origin=inward,Conference Paper,SCOPUS_ID:85096578367,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sentence level human translation quality estimation with attention-based neural networks,"
AbstractView references

This paper explores the use of Deep Learning methods for automatic estimation of quality of human translations. Automatic estimation can provide useful feedback for translation teaching, examination and quality control. Conventional methods for solving this task rely on manually engineered features and external knowledge. This paper presents an end-to-end neural model without feature engineering, incorporating a cross attention mechanism to detect which parts in sentence pairs are most relevant for assessing quality. Another contribution concerns oprediction of fine-grained scores for measuring different aspects of translation quality, such as terminological accuracy or idiomatic writing. Empirical results on a large human annotated dataset show that the neural model outperforms feature-based methods significantly. The dataset and the tools are available. © European Language Resources Association (ELRA), licensed under CC-BY-NC
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096562536&origin=inward,Conference Paper,SCOPUS_ID:85096562536,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"progene - a large-scale, high-quality protein-gene annotated benchmark corpus","
AbstractView references

Genes and proteins are the fundamental entities of molecular genetics and deeper knowledge about their interactions constitutes a cornerstone for advancing precision medicine. We here introduce PROGENE (formerly called FSU-PRGE), a corpus that reflects our efforts to cope with this important class of named entities within the framework of a long-lasting large-scale annotation campaign at the Jena University Language & Information Engineering (JULIE) Lab. We partitioned the entire corpus into 11 subcorpora covering various biological domains to achieve an overall subdomain-independent corpus. It consists of 3,308 MEDLINE abstracts with over 36k sentences and more than 960k tokens annotated with nearly 60k named entity mentions. Two annotators strove for carefully assigning entity mentions to classes of genes/proteins as well as families/groups, complexes, variants and enumerations of those where genes and proteins are represented by a single class. The main purpose of the corpus is to provide a large body of consistent and reliable annotations for supervised training and evaluation of machine learning algorithms in this relevant domain. Furthermore, we provide an evaluation of two state-of-the-art baseline systems-BIOBERT and FLAIR-on the PROGENE corpus. We make the evaluation datasets and the trained models available as a benchmark to encourage comparable evaluations of new methods in the future. © European Language Resources Association (ELRA), licensed under CC-BY-NC
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096355625&origin=inward,Conference Paper,SCOPUS_ID:85096355625,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),bringing clouds down to earth: modeling arrowhead deployments via eclipse vorto,"
AbstractView references

The design and development of interconnected industrial production facilities, which integrate aspects of the Internet of Things (IoT) or, more specifically, the Industrial IoT (IIoT), often deals with complex scenarios involving dynamic System of Systems (SoS), resulting in immense development and deployment efforts. The Arrowhead community aims at delivering mechanisms and technologies to cope with such complex scenarios. In particular, the concept of local clouds constitutes a service-oriented architecture (SOA) framework for IIoT. Here, a central challenge is the conceptual modeling of such use-cases. SysML is widely established as a standardized modeling language and framework for large-scale systems engineering and, thus, for Arrowhead local cloud designs. However, SysML and its Arrowhead profile lack a canonical way to support actual platform modeling and device involvement in heavily distributed IIoT scenarios. The Eclipse Vorto project is ideal for filling this gap: it provides a modeling language for IoT devices, a set of modeling tools, and already existing reusable templates of device models. In this paper, we propose an approach to integrating Eclipse Vorto models into Arrowhead SysML models. We illustrate the concept with a realistic yet comprehensible industrial scenario and also present a prototype to emphasize the benefits of our novel integration platform. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85095968485&origin=inward,Conference Paper,SCOPUS_ID:85095968485,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),modular surrogate-based optimization framework for expensive computational simulations,"
AbstractView references

In practical applications, the use of computational modeling has been industry-wide adopted to speed up product development as well as reduce physical testing costs. Such models of complex or large systems are, however, often computationally expensive, hence solution times of hours or more are not uncommon. Additionally, as these models are typically evaluated using blackbox solvers, the direct study of relations between design parameters renders demanding in terms of computational time and provides poor engineering insight and understanding. To address this, a modular framework integrating computation automation with the use of surrogate-based modeling, optimization and visualization techniques is presented. The framework is built in the Python programming language. Its use is illustrated on a study of the side impact response of a car body using an artificial neural network as a surrogate together with the NSGA-III genetic algorithm for optimization. Copyright © 2020 for this paper by its authors.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85095776379&origin=inward,Conference Paper,SCOPUS_ID:85095776379,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),object-oriented modeling and performance evaluation of a pcm-based ventilation system,"
AbstractView references

The buildings sector is a large energy consumer, so improving the building block energy performance through implementing energy-efficient techniques would aid in attaining the energy and environmental goals. Among energy-saving technologies, Latent Thermal Energy Storage (LTES) systems have drawn great attention to be applied in buildings as they enable more efficient and cost-effective thermal management by reducing energy use or shifting peak loads. Particularly, Phase Change Material (PCM), as a storage medium in LTES systems, has received considerable attention in recent research studies and investigations. This study aims at modelling and assessment of a PCM-driven ventilation system. An energy storage system with four PCM stacks was modelled using Modelica language considering the physical and operational parameters. Subsequently, the influence of parameters on PCM temperature was evaluated by sensitivity analysis. Using collected data from a system experiment, unknown parameters for the first stack were estimated on the first 3 days and validated on the following 2 days through minimizing Root Mean Square Error (RMSE) of predicted PCM temperature to the measured temperature using a genetic algorithm. The estimated parameters were applied to the four stacks and developed ventilation system was then validated based on the 5-day measurements from experimental setup. The overall system performance was simulated and assessed and the PCM thermal energy storage capacity was evaluated. Results showed that the object-oriented Modelica model yields a sufficient accuracy in capturing the thermal behavior of the PCM-based energy storage system. For each stack, the RMSE of predicted PCM temperature compared to measurements are 0.277 °C, 0.332 °C, 0.332 °C and 0.410 °C respectively. © ECOS 2020.All right reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094681656&origin=inward,Conference Paper,SCOPUS_ID:85094681656,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cross-industry sectoral study: interactions and challenges of requirements engineering in the early phase of product development,"
AbstractView references

Product development is undergoing a profound change: Digitization, Industry 4.0 or the interconnected end-To-end product are trends that offer great innovation potential across industries, but strongly influence the development of new product generations in the product portfolio. In addition, many industry sectors such as automotive product development have in recent years greatly optimized common parts and platform strategies in order to achieve synergetic cost reductions. This raises the question of how conflicting requirements of customers and providers can be taken into account simultaneously in the early phase to guarantee an optimum combination between cost input and innovation potential. The aim of the cross-industry sectoral study in this paper was to gain a deeper understanding of the challenges of requirements engineering and the individual use of existing knowledge in early development phases. For this purpose, a workshop (consisting of five individual sessions with 8-16 participants each) was conducted by two independent moderators within the framework of an expert forum, the re:work Smart Requirements Engineering 2019. Building on the discussion of the workshop results, essential findings were consolidated with the participants at the end of each session and fields of action were synthesized. The survey clearly illustrated that regardless of company size or professional experience, the use of the reference system was considered a genuine success factor, which was unanimously not identified as ""creativity hindering"". Short iteration cycles, even in large companies, are necessary to validate the consideration and integration of customer, user and provider benefits. Furthermore, the creation of a uniform language, comprehensible structures and models is necessary. The respondents pointed out that functions are suitable as a link between properties and technical realization for evaluating the maturity level of a product specification. In addition, a consistent understanding of how to utilize the reference system can enable a structured product specification in the early phase of product development of new product generations. © Proceedings of the NordDesign 2020 Conference, NordDesign 2020. All rights reserved.
"
10.1007/978-3-030-60117-1_32,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094144617&origin=inward,Conference Paper,SCOPUS_ID:85094144617,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),safety analytics for ai systems,"
AbstractView references

Growing AI technologies are a threat to safety and security in systems due to its obscurity and uncertainty. This study introduces a prevailing Deep Learning model, Convolutional Neural Network (CNN) and it’s deep weaknesses through a simple case study of the CNN model based on Keras for handwriting recognition. It reveals that CNN algorithms don’t adapt well to changes. Adding new cases to the training data may improve accuracy, but not to the same level as before. Synthetic training data may improve the accuracy superficially because of the similarity of data distributions between generated data and original data. Prevailing ML models such as Generative Adversarial Networks (GAN) have their limitations such as similarity-addiction and modality collapse. They could be toxic to safety engineering without domain expertise. The study proposed four test strategies: 1) AI systems should be tested by the third parties, not the developers; 2) test datasets should be categorically different from training datasets; the test data should not be a part of the training data; the test data should be collected from independent sources to increase the “diversity” of data modality; 3) avoid fake data, or simulated data; and 4) don’t collect the data that are conveniently available, but actively collect disastrous event data, unexpected, or the worst scenarios that may destroy the model. The study also introduces a multidimensional checklist for AI safety analysis, including sensors, data and environments, default and recovery mode, system architectures, and human-system interaction. © 2020, Springer Nature Switzerland AG.
"
10.1007/978-3-030-60939-9_8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85093869319&origin=inward,Conference Paper,SCOPUS_ID:85093869319,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),dramsys4.0: a fast and cycle-accurate systemc/tlm-based dram simulator,"
AbstractView references

The simulation of DRAMs (Dynamic Random Access Memories) on system level requires highly accurate models due to their complex timing and power behavior. However, conventional cycle-accurate DRAM models often become the bottleneck for the overall simulation speed. A promising alternative are DRAM simulation models based on Transaction Level Modeling, which can be fast and accurate at the same time. In this paper we present DRAMSys4.0, which is, to the best of our knowledge, the fastest cycle-accurate open-source DRAM simulator and has a large range of functionalities. DRAMSys4.0 includes a novel simulator architecture that enables a fast adaptation to new DRAM standards using a Domain Specific Language. We present optimization techniques to achieve a high simulation speed while maintaining full temporal accuracy. Finally, we provide a detailed survey and comparison of the most prominent cycle-accurate open-source DRAM simulators with regard to their supported features, analysis capabilities and simulation speed. © 2020, Springer Nature Switzerland AG.
"
10.1007/978-3-030-60450-9_19,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85093116249&origin=inward,Conference Paper,SCOPUS_ID:85093116249,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),transbidifilter: knowledge embedding based on a bidirectional filter,"
AbstractView references

A large-scale knowledge base can support a large number of practical applications, such as intelligent search and intelligent question answering. As the completeness of the information in a knowledge base may have a direct impact on the quality of downstream applications, its automatic completion has become a crucial task for many researchers and practitioners. To address this challenge, the knowledge representation learning technology which represents entities and relations as low-dimensional dense real value vectors has been developed rapidly in recent years. Although researchers continue to improve knowledge representation learning models using an increasingly complex feature engineering, we find that the most advanced models can be outdone by simply considering interactions from entities to relations and that from relations to entities without requiring huge number of parameters. In this work, we present a knowledge embedding model based on a bidirectional filter called TransBidiFilter. By learning the global shared parameter set based on the traditional gate structure, TransBidiFilter captures the restriction rules from entities to relations and that from relations to entities respectively. It achieves better automatic completion ability by modifying the standard translation-based loss function. In doing so, though with much fewer discriminate parameters, TransBidiFilter performs better than state-of-the-art baselines of semantic discriminate models on most indicators on many datasets. © 2020, Springer Nature Switzerland AG.
"
10.1007/978-3-030-60457-8_31,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85093078085&origin=inward,Conference Paper,SCOPUS_ID:85093078085,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),measuring the semantic stability of word embedding,"
AbstractView references

The techniques of word embedding have a wide range of applications in natural language processing (NLP). However, recent studies have revealed that word embeddings have large amounts of instability, which affects the performance in downstream tasks and the applications in safety-critical fields such as medical diagnosis and financial analysis. Further researches have found that the popular metric of Nearest Neighbors Stability (NNS) is unreliable for qualitative conclusions on diachronic semantic matters, which means NNS cannot fully capture the semantic fluctuations of word vectors. To measure semantic stability more accurately, we propose a novel metric that combines the Nearest Senses Stability (NSS) and the Aligned Sense Stability (ASS). Moreover, previous studies on word embedding stability focus on static embedding models such as Word2vec and ignore the contextual embedding models such as Bert. In this work, we propose the SPIP metric based on Pairwise Inner Product (PIP) loss to extend the stability study to contextual embedding models. Finally, the experimental results demonstrate that CS and SPIP are effective in parameter configuration to minimize embedding instability without training downstream models, outperforming the state-of-the-art metric NNS. © 2020, Springer Nature Switzerland AG.
"
10.1007/978-3-030-60450-9_15,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85093074504&origin=inward,Conference Paper,SCOPUS_ID:85093074504,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),is pos tagging necessary or even helpful for neural dependency parsing?,"
AbstractView references

In the pre deep learning era, part-of-speech tags have been considered as indispensable ingredients for feature engineering in dependency parsing. But quite a few works focus on joint tagging and parsing models to avoid error propagation. In contrast, recent studies suggest that POS tagging becomes much less important or even useless for neural parsing, especially when using character-based word representations. Yet there are not enough investigations focusing on this issue, both empirically and linguistically. To answer this, we design and compare three typical multi-task learning framework, i.e., Share-Loose, Share-Tight, and Stack, for joint tagging and parsing based on the state-of-the-art biaffine parser. Considering that it is much cheaper to annotate POS tags than parse trees, we also investigate the utilization of large-scale heterogeneous POS tag data. We conduct experiments on both English and Chinese datasets, and the results clearly show that POS tagging (both homogeneous and heterogeneous) can still significantly improve parsing performance when using the Stack joint framework. We conduct detailed analysis and gain more insights from the linguistic aspect. © 2020, Springer Nature Switzerland AG.
"
10.18178/wcse.2020.02.011,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092402422&origin=inward,Conference Paper,SCOPUS_ID:85092402422,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),building large scale text corpus for joint word segmentation and part-of-speech tagging of myanmar language,"
AbstractView references

In Natural Language Processing (NLP), Word segmentation and Part-of-Speech (POS) tagging are fundamental tasks. The POS information is also necessary in NLP's preprocessing work applications such as machine translation (MT), information retrieval (IR), etc. Currently, there are many research efforts in word segmentation and POS tagging developed separately with different methods to get high performance and accuracy. Word segmentation and Part-of-speech tagging is one of the important actions in language processing. Against this, while numerous models are provided in different languages, few works have been performed for Myanmar language. This paper describes the building of Myanmar Corpus to use for joint word segmentation and part-of-speech tagging of Myanmar Language. In our research, the corpus contains 51207 sentences and 839161words. The corpus is created using 12 tags. To evaluate the accuracy of the corpus, HMM model is trained on different data size and testing is done with closed test and opened test. Results with 94% accuracy in the experiments show the appropriate efficiency of the built corpus. © WCSE 2020.
"
10.18293/SEKE2020-062,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090509513&origin=inward,Conference Paper,SCOPUS_ID:85090509513,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an ensemble approach to detect code comment inconsistencies using topic modeling,"
AbstractView references

In modern era, the size of software is increasing, as a result a large number of software developers are assigned into software projects. To have a better understanding about source codes these developers are highly dependent on code comments. However, comments and source codes are often inconsistent in a software project because keeping comments up-to-date is often neglected. Since these comments are written in natural language and consist of context related topics from source codes, manual inspection is needed to ensure the quality of the comment associated with the corresponding code. Existing approaches consider entire texts as feature, which fail to capture dominant topics to build the bridge between comments and its corresponding code. In this paper, an effective approach has been proposed to automatically extract dominant topics as well as to identify the consistency between a code snippet and its corresponding comment. This approach is evaluated with a benchmark dataset containing 2.8K Java code-comment pairs, which showed that proposed approach has achieved better performance with respect to the several evaluation metrics than the existing state-of-the-art Support Vector Machine on vector space model. © 2020 Knowledge Systems Institute Graduate School. All rights reserved.
"
10.1007/978-3-030-38712-9_4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089706672&origin=inward,Book Chapter,SCOPUS_ID:85089706672,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"trends, issues, and challenges in the domain of iot-based vehicular cloud network","
AbstractView references

Since the past few years, the vehicular network has gained significant attention because of its powerful potential applications such as traffic management, surveillance, and safety. The modern vehicles are equipped with smart sensors, actuators, and efficient communication devices such as GPS and embedded hardware. The vehicular network potential application outcomes are achieved by using vehicles onboard computational, communication, and storage capabilities with the help of cloud computing. Thus, the aim of the vehicular cloud network is to improve the traditional transportation system. The smart vehicle is equipped with smart devices such as computer on wheels, GPS devices, collision radars, and intelligent radio transceivers. The Internet of Things (IoT) and cloud computing have provided a solution to handle the increasing traffic congestion and vehicular safety. The cloud-based vehicular IoT network uses a number of software services which include sensor service, cloud service, and platform service. These services, when interacting with each other, provide a basic architecture to build traffic control and cloud-based vehicular data processing system. The IoT-based vehicular cloud network allows automobile manufacturers to innovate smart features into the vehicles with low cost, which also increases their market competitiveness. Automobile companies are utilizing cloud services from different cloud providers to support various service-level agreements. Since more and more vehicles are equipped with sensors that can access the Internet, vehicular services are combined with different cloud services to map, encapsulate, and aggregate the vehicular data to form the vehicular network platform. With the increasingly growing data in the cloud-based vehicular network, there are fundamental engineering challenges such as big data collection, data analysis for traffic management, real-time decision-making, and the ability to understand each other’s application formats and service-level agreement templates. The vehicular network has combined various technologies to handle these issues, such as machine learning, artificial intelligence, database management, and data mining. Similarly, cloud interoperability issues arise to support heterogeneous programming interfaces, programming languages, data models, and operating systems in an efficient and reliable manner. With the advancement in the mobile communication system, the vehicular cloud network can facilitate the scalable system with a reduced cost, efficient routing, resource sharing, and monitoring in a secure and efficient manner. The IoT-based vehicular cloud network is a complex system of interconnected sensors and communication tools and cloud platform. We can divide such a system into a number of subsystems and dimensions which include traffic management, data information processing, and service routing. The cloud-based vehicular network layered these services into cloud computing three distinct dimensions that include software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). In this chapter, we study the advances in the use of IoT in the transportation system via a cloud platform for developing an efficient vehicular cloud network. Thus, IoT and cloud computing in the automotive domain are studied. Similarly, the effectiveness of the vehicular network depends on its ability to handle large heterogeneous sensors and heterogeneous cloud platforms. Hence, the interoperability challenges at various cloud service levels and global standard issues are discussed. We also provided an analysis of how machine learning and blockchain can be applied to IoT-based vehicular cloud networks for self-learning and security mechanism, respectively. © 2020, Springer Nature Switzerland AG.
"
10.1007/978-3-030-47956-5_5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089613309&origin=inward,Book Chapter,SCOPUS_ID:85089613309,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),aimes: advanced computation and i/o methods for earth-system simulations,"
AbstractView references

Dealing with extreme scale earth system models is challenging from the computer science perspective, as the required computing power and storage capacity are steadily increasing. Scientists perform runs with growing resolution or aggregate results from many similar smaller-scale runs with slightly different initial conditions (the so-called ensemble runs). In the fifth Coupled Model Intercomparison Project (CMIP5), the produced datasets require more than three Petabytes of storage and the compute and storage requirements are increasing significantly for CMIP6. Climate scientists across the globe are developing next-generation models based on improved numerical formulation leading to grids that are discretized in alternative forms such as an icosahedral (geodesic) grid. The developers of these models face similar problems in scaling, maintaining and optimizing code. Performance portability and the maintainability of code are key concerns of scientists as, compared to industry projects, model code is continuously revised and extended to incorporate further levels of detail. This leads to a rapidly growing code base that is rarely refactored. However, code modernization is important to maintain productivity of the scientist working with the code and for utilizing performance provided by modern and future architectures. The need for performance optimization is motivated by the evolution of the parallel architecture landscape from homogeneous flat machines to heterogeneous combinations of processors with deep memory hierarchy. Notably, the rise of many-core, throughput-oriented accelerators, such as GPUs, requires non-trivial code changes at minimum and, even worse, may necessitate a substantial rewrite of the existing codebase. At the same time, the code complexity increases the difficulty for computer scientists and vendors to understand and optimize the code for a given system. Storing the products of climate predictions requires a large storage and archival system which is expensive. Often, scientists restrict the number of scientific variables and write interval to keep the costs balanced. Compression algorithms can reduce the costs significantly but can also increase the scientific yield of simulation runs. In the AIMES project, we addressed the key issues of programmability, computational efficiency and I/O limitations that are common in next-generation icosahedral earth-system models. The project focused on the separation of concerns between domain scientist, computational scientists, and computer scientists. The key outcomes of the project described in this article are the design of a model-independent Domain-Specific Language (DSL) to formulate scientific codes that can then be mapped to architecture specific code and the integration of a compression library for lossy compression schemes that allow scientists to specify the acceptable level of loss in precision according to various metrics. Additional research covered the exploration of third-party DSL solutions and the development of joint benchmarks (mini-applications) that represent the icosahedral models. The resulting prototypes were run on several architectures at different data centers. © The Author(s) 2020.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089529325&origin=inward,Conference Paper,SCOPUS_ID:85089529325,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),retrieving similar software from large-scale open-source repository by constructing representation of project description,"
AbstractView references

The rise of open source community has greatly promoted the development of software resource reuse in all phases of software process, such as requirements engineering, designing, coding, and testing. However, how to efficiently and accurately locate reusable resources on large-scale open source website remains to be solved. Presently, most open source websites provide text-matching-based searching mechanism while ignoring the semantic of project description. For enabling requirements engineers to find software that are similar to the one to be developed quickly at the very beginning of the project, we propose a searching framework based on constructing semantic embedding for software project with machine learning technique. In the proposed approach, both Type Distribution and Document Vector learnt through different neural network language models are used as project representations. Besides, we integrate searching results of different representations with a Ranking model. For evaluating our approach, we compare search results of different searching strategies manually using an evaluating system. Experimental results on a data set consisting of 24,896 projects show that the proposed searching framework, i.e., combining results derived from Inverted Index, Type Distribution and Document Vector, significantly superior to the text-matching-based one. Copyright © 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.
"
10.1007/978-3-030-52538-5_24,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089314043&origin=inward,Conference Paper,SCOPUS_ID:85089314043,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),philosophical approaches to smart education and smart cities,"
AbstractView references

The impact of technology affects the educational field in an extraordinary way. The effect of technology must be treated from the educational method itself as well as its horizon in the new paradigm of citizen in this smart environment. This work proposes a revision of the immediate future of education and citizenship, as determined by the exponential impact of technology, based on relevant issues of classic philosophy and specially along the history and didactic program of the Trivium (Grammar, Rhetoric and Logic). The paper analyses firstly the perspective of the transformation of current education towards a smart education. This transition is determined by the development of Artificial Intelligence (AI), and the reflection on the competences of the 21st century, as something intrinsically related to the issues of citizenship as well as smart cities; secondly, we review the strategic and methodological proposals in accordance with this transformation, based on the theory of generative learning and on computational and algorithmic thinking; thirdly, from the point of view of contents, we analyse the importance of digital skills and, as a fundamental element of these, programming skills. Our proposal is to recover and update the contents of the Trivium, as a renewing and revitalizing element of the methodology and contents of education in the 21st century. This proposal is based on the assumption of the epistemological unity of these disciplines and on the integral anthropological vision that supports them. Both ideas acquire special relevance in the current context marked by the impact of technology as a determining element of the medium (smart education), of the methodology (generative learning, computational thinking) and of the contents (digital skills and programming). © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2020.
"
10.1016/j.procs.2020.05.061,S1877050920314575,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089023699&origin=inward,Conference Paper,SCOPUS_ID:85089023699,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ontology based concept extraction and classification of ayurvedic documents,"Under the background of accelerating new industrial revolution, China’s higher engineering education is in urgent need of cultivating a group of innovative talents in engineering science and technology. In order to improve the quality of engineering education, China proposed the “new engineering research and practice project” in June 2017. This study attempted to adopt systematic review based on 207 projects of engineering superior universities in China’s new engineering research and practice project for text analysis, and discovered the common key issues such as mechanism issues, ability issues and discipline issues that are concerned in existing projects. The research findings are as follows: (1) The commonness of construction schemes is emerging. In terms of mechanism construction, the mechanism of collaborative education receives the most attention, while the talent ability pays more attention to the cultivation of soft ability; (2) The overall design of the projects are somewhat lacking innovation and features, and certain construction schemes are similar, failing to effectively combine the advantages of universities and disciplines to serve local industries and truly show its characteristics; (3) Most of the construction plans are aimed at the transformation of traditional disciplines, and there is little construction of corresponding disciplines for certain national key industries."
10.1007/978-3-030-51310-8_23,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85087532234&origin=inward,Conference Paper,SCOPUS_ID:85087532234,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),improving the community question retrieval performance using attention-based siamese lstm,"
AbstractView references

In this paper, we focus on the problem of question retrieval in community Question Answering (cQA) which aims to retrieve from the community archives the previous questions that are semantically equivalent to the new queries. The major challenges in this crucial task are the shortness of the questions as well as the word mismatch problem as users can formulate the same query using different wording. While numerous attempts have been made to address this problem, most existing methods relied on supervised models which significantly depend on large training data sets and manual feature engineering. Such methods are mostly constrained by their specificities that put aside the word order and ignore syntactic and semantic relationships. In this work, we rely on Neural Networks (NNs) which can learn rich dense representations of text data and enable the prediction of the textual similarity between the community questions. We propose a deep learning approach based on a Siamese architecture with LSTM networks, augmented with an attention mechanism. We test different similarity measures to predict the semantic similarity between the community questions. Experiments conducted on real cQA data sets in English and Arabic show that the performance of question retrieval is improved as compared to other competitive methods. © Springer Nature Switzerland AG 2020.
"
10.1007/978-3-030-49418-6_13,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85086316020&origin=inward,Conference Paper,SCOPUS_ID:85086316020,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),mining bpmn processes on github for tool validation and development,"
AbstractView references

Today, business process designers can choose from an increasing number of analysis tools to check their process model with respect to defects or flaws, before, e.g., deploying the model in a process engine. Answering questions about the tools’ effectiveness though is difficult, as their validation often lacks empirical evidence. In particular, for a modeling language like BPMN, where the process is the product, tools are validated by means of case studies or even artificial process examples. We here advocate instead an approach to systematically mine software repositories on GitHub.com for a large corpus of BPMN business process models and discuss how it can be used for tool validation and guiding tool development, using the example of the linting tool BPMNspector. © Springer Nature Switzerland AG 2020.
"
10.1007/978-3-030-45439-5_40,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083984239&origin=inward,Conference Paper,SCOPUS_ID:85083984239,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),diagnosing bert with retrieval heuristics,"
AbstractView references

Word embeddings, made widely popular in 2013 with the release of word2vec, have become a mainstay of NLP engineering pipelines. Recently, with the release of BERT, word embeddings have moved from the term-based embedding space to the contextual embedding space—each term is no longer represented by a single low-dimensional vector but instead each term and its context determine the vector weights. BERT’s setup and architecture have been shown to be general enough to be applicable to many natural language tasks. Importantly for Information Retrieval (IR), in contrast to prior deep learning solutions to IR problems which required significant tuning of neural net architectures and training regimes, “vanilla BERT” has been shown to outperform existing retrieval algorithms by a wide margin, including on tasks and corpora that have long resisted retrieval effectiveness gains over traditional IR baselines (such as Robust04). In this paper, we employ the recently proposed axiomatic dataset analysis technique—that is, we create diagnostic datasets that each fulfil a retrieval heuristic (both term matching and semantic-based)—to explore what BERT is able to learn. In contrast to our expectations, we find BERT, when applied to a recently released large-scale web corpus with ad-hoc topics, to not adhere to any of the explored axioms. At the same time, BERT outperforms the traditional query likelihood retrieval model by 40%. This means that the axiomatic approach to IR (and its extension of diagnostic datasets created for retrieval heuristics) may in its current form not be applicable to large-scale corpora. Additional—different—axioms are needed. © 2020, Springer Nature Switzerland AG.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85082726200&origin=inward,Conference Paper,SCOPUS_ID:85082726200,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards a catalog of privacy related concepts,"
AbstractView references

[Context and motivation] Data from software systems often captures a large amount of personal information and can be used for purposes other than initially intended. Therefore, the Requirements Engineering community has been recognizing the need for approaches to consider privacy concerns from the early activities of the software development process. [Question/problem] However, there is much confusion regarding privacy among people involved in Software Engineering because there is not a unified view of how to consider privacy in software development. [Principal ideas/results] Motivated by this situation, we conducted a Systematic Literature Review to investigate how modeling languages address privacy related concepts. As a result, we developed a catalog of privacy related concepts considered by modeling languages and a conceptual model to show how these concepts relate to each other. [Contribution] This paper contributes to the state of art by presenting a basis to standardize privacy in the Requirements Engineering field and help developers in understanding privacy. Copyright © 2020 for this paper by its authors.
"
10.1007/978-981-15-3281-8_1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85080864361&origin=inward,Conference Paper,SCOPUS_ID:85080864361,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"knowledge graph data management: models, methods, and systems","
AbstractView references

With the rise of artificial intelligence, knowledge graphs have been widely considered as a cornerstone of AI. In recent years, an increasing number of large-scale knowledge graphs have been constructed and published, by both academic and industrial communities, such as DBpedia, YAGO, Wikidata, Google Knowledge Graph, Microsoft Satori, Facebook Entity Graph, and others. In fact, a knowledge graph is essentially a large network of entities, their properties, semantic relationships between entities, and ontologies the entities conform to. Such kind of graph-based knowledge data has been posing a great challenge to the traditional data management theories and technologies. In this paper, we introduce the state-of-the-art research on knowledge graph data management, which includes knowledge graph data models, query languages, storage schemes, query processing, and reasoning. We will also describe the latest development trends of various database management systems for knowledge graphs. © Springer Nature Singapore Pte Ltd 2020.
"
10.1007/978-3-030-40760-5_3,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85080855645&origin=inward,Book Chapter,SCOPUS_ID:85080855645,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),basic structures of systems,"
AbstractView references

The essence of system science resides in the philosophy of holism. When talking about the state that system reaches optimal, it generally refers to global optimum of the whole system with respect to the objective features that are included in the intrinsic features of a system. The attainment of global optimum of a system must rely on the normal deployment of functions (features) of its subsystems. The word ‘system’ originates from Latin word syste¯ma meaning that a whole is made of several parts or members. Many different definitions had been made by scholars for system from different perspectives based on their particular research objectives. Let’s give some examples of them. “System is a pre-given set composed of elements and their normal behaviors”; “System is a well-organized wholeness”; “System is an entity made of connected materials and processes;” “System is a body composed of ordered elements/factors working towards a common goal” are some popular examples for the definition of system. The development of system theory and their applications in different field were mainly attributed to the contributions of scholars such as biologist Ludwig von Bertalanffy (1901–1972), Norbert Wiener (1894–1964), Ross Ashby (1903–1972), John Henry Holland (1929–2015), and Murray Gell-Mann (1929–2019). Bertalanffy pioneered the general system theory by introducing models, principles, and laws that apply to it. Wiener and Ashby used mathematics to study systems. Holland, Gell-Mann, and others proposed the term “complex adaptive system”. General system theory attempts to provide a definition that can capture common properties of various systems. The definition for general system is: a body composed of well-organized elements working toward attaining particular goals or features. This definition apparently includes 4 concepts and their relationships, namely system, element, structure, feature, relationships between elements, relationships between elements and structure, and relationships between system and external environment. The purpose of system theory is to investigate form, structure, and laws of general systems, to examine the common properties of those systems, to capture and illustrate their features using mathematical methods, and consequently to identify the mechanisms, rules, laws, principles, and mathematical models that can be applied to general systems. And the ultimate objective of learning system theory is to use the understanding on the system to better manage, control, renovate, or change the current system structures (natural or man-made systems) to align them with the needs of our civilized world. With better understanding on the system structures, we can introduce all kinds of interventions or policies to enable the systems of interest attain their optimal performance or outcomes. Moreover, by gaining better understanding on the dynamics of a system over time, decision/policy makers and practitioners can prevent policy-resistance (counter-intuitive behaviors). System theory is recognized a discipline that possesses both mathematical and logic characteristics. System theory proclaims that holism, connectedness, hierarchical structure, and dynamic equilibrium, time-dependence are common properties of all systems, which are both philosophy of system thinking and principles of using system approach. As a branch of scientific approaches, system theory helps identify the objective laws on how world is running and also offers human being a way of thinking the world. Therefore, system theory is also called system approach since it can represent concept, view, model, and mathematical methods as well. In Bertalanffy’s masterpiece titled “General System Theory; Foundations, Development, Applications”, he emphasized the concept of holism. System, as a organic body, is not mechanical combination or simple addition of its constituents but an organic combination of its elements working together towards a common goal. The system’s features are emerging behaviors, which can not be found in its individual elements or subsystems. By quoting Aristotle’s “A whole is greater than the sum of its part,” Bertalanffy opposed those mechanical philosophy that the wholeness (system behaviors) can be observed or inferred from the behavior of a particular element of the system. He also stated that each element of a system is in a particular location in the system hierarchy, which is also tightly coupled with other elements. The connectedness among system’s elements renders system integral and holistic. The “should-be” function of a system’s element will disappear once it is separated from the system structure. For example, having done the hand amputation due to traumatic injury, the removed ‘limb’ would never function as it should when it was an integral part of a person. The fundamental thoughts of system theory is to treat the object being investigated as a system and to analyze the structure, function, dynamic relationships between elements, system, and their environment. With better understanding on the dynamics, complexities, and uncertainties associated with the system, the ultimate goal is to find how it attain its optimal target and, consequently provide counterfactual analysis when interventions are needed to be implemented in the system. Systems are ubiquitous in the universe. From cosmos to the microscopic world, systems exist everywhere such as Milky Way, solar system, earth system, social system, transportation system, production system, human body system, bacterial system, cell system, and atom. The emergence of system theory brought profound changes on the way how people think about the world. In conventional research practices, Descartes’ philosophy of ‘reductionism’ had been dominating the academic fields. Under such influence, the general practice in research is to divide a complicated issue or object into multiple parts and investigate each part individually. Thereafter, the characteristics of those individual parts are then used to infer the behaviors of the original issue or object. The reductionisim approach focuses on local substructures or elements and abides by the unidirectional causal-effect determinism. Although this approach had proved valid for centuries within certain confined ranges and had served as the most popular way of thinking in mainstream research communities, it can only handle simple issues or objects without being able to capture the wholeness, dynamic interactions, and circular causalities of complicated objects (i.e., systems in the language of system theory). With accelerated development in economy, technology, and society, human beings with traditional analytical thinking became incompetent in dealing with issues/objects with thousands or even millions of variables connected/networked in various ways. However, the emergence of system theory, cybernetics, and informatics paved the way for human beings to drive the rapid advancement of modern science and technologies. The widespread applications of system theory have made it become the basis for developing new theories in handling complicated system in the fields of politics, economy, military, culture, science, and society, etc. Regarding the trend of system theory, the authors think it is moving towards the formation of unified framework that summarizes the achievements obtained from the empirical and theoretical research in different fields. System thinking ensued by system theory has become a very powerful force to overturn the ingrained singular causation thinking. For ease of studying system, many ways are used to categorize system: (1) natural systems and artificial systems (whether designed by human being or not); (2) natural systems, social systems, and thinking systems (according to research subject); (3) macro systems, mesa system, micro systems, and microscopic systems(scale of the systems); (4) simple systems, complex systems (in term of structure); (5) simple small systems, simple large systems, simple giant systems, and complex giant systems, etc.(scale and structure); (6) open systems, closed systems (whether there exists interaction with environment); (7) balanced systems (systems having equilibrium), non-equilibrium systems, near-equilibrium systems, and far-from-equilibrium systems (whether there exists equilibrium). © 2020, Springer Nature Switzerland AG.
"
10.1007/978-3-030-37873-8_2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078483442&origin=inward,Conference Paper,SCOPUS_ID:85078483442,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a model-based combination language for scheduling verification,"
AbstractView references

Cyber-Physical Systems (CPSs) are built upon discrete software and hardware components, as well as continuous physical components. Such heterogeneous systems involve numerous domains with competencies and expertise that go far beyond traditional software engineering: systems engineering. In this paper, we explore a model-based approach for systems engineering that advocates the composition of several heterogeneous artifacts (called views) into a sound and consistent system model. A model combination Language is proposed for this purpose. Thus, rather than trying to build the universal language able to capture all possible aspects of systems, the proposed language proposes to relate small subsets of languages in order to offer specific analysis capabilities while keeping a global consistency between all joined models. We demonstrate the interest of our approach through an industrial process based on Capella, which provides (among others) a large support for functional analysis from requirements to components deployment. Even though Capella is already quite expressive, it lacks support for schedulability analysis. AADL is also a language dedicated to system analysis. If it is backed with advanced schedulability tools, it lacks support for functional analysis. Thus, instead of proposing ways to add missing aspects in either Capella or AADL, we rather extract a relevant subset of both languages to build a view adequate for conducting schedulability analysis of Capella functional models. Finally, our combination language is generic enough to extract pertinent subsets of languages and combine them to build views for different experts. It also helps maintaining a global consistency between different modeling views. © Springer Nature Switzerland AG 2020.
"
10.1109/TII.2019.2930772,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078268647&origin=inward,Article,SCOPUS_ID:85078268647,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automatic generation of control flow from requirements for distributed smart grid automation control,"
AbstractView references

Smart grid is a cyber-physical system with a high level of complexity due to its decentralized infrastructure. IEC 61850 and IEC 61499 are two industrial standards that can address the challenges introduced by the smart grid on the substation automation level. Development of smart grid automation software is a very time-consuming process due to the need to address many requirements and a high degree of customization in every new substation, which limits the adoption of such smart grid technologies in digital substations. This article aims at addressing this limitation by applying a semiformal boilerplates (BPs) model of functional requirements originally presented in informal natural language. The BPs are then modeled formally in an ontology for model-driven engineering (MDE) model transformation. The contribution of this article is the development of the semiformal and formal BP representation in the form of ontology to formulate smart grid requirements and demonstrating how functional requirements can be translated to IEC 61499 control codes using MDE to autogenerate an IEC 61499 protection and control system with structure and control flow. The MDE framework augmented with the requirement models is illustrated in a case study from the International Council on Large Electric Systems representing different stages of modeling in the proposed framework. © 2005-2012 IEEE.
"
10.1007/978-981-15-0637-6_5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85077113635&origin=inward,Conference Paper,SCOPUS_ID:85077113635,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),formal modeling and analysis of probabilistic real-time systems,"
AbstractView references

This paper considers formal modeling and analysis of distributed timed and stochastic real-time systems. The approach is based on Stochastic Time Petri Nets (sTPN) which offer a readable yet powerful modeling language. sTPN are supported by special case tools which can ensure accuracy in the results by numerical methods and the enumeration of stochastic state classes. These techniques, though, can suffer of state explosion problems when facing large models. In this work, a reduction of sTPN onto the popular Uppaal model checkers is developed which permits both exhaustive non-deterministic analysis, which ignores stochastic aspects and it is useful for functional and temporal assessment of system behavior, and quantitative analysis through statistical model checking, useful for estimating by automated simulation runs probability measures of event occurrence. The paper provides the formal definition of sTPN and its embedding into Uppaal. A sensor network case study is used as a running example throughout the paper to demonstrate the practical applicability of the approach. © Springer Nature Singapore Pte Ltd. 2020.
"
10.1007/978-3-030-26574-8_10,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85070493007&origin=inward,Book Chapter,SCOPUS_ID:85070493007,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),nuts and bolts of extracting variability models from natural language requirements documents,"
AbstractView references

Natural language (NL) requirements documents are often ambiguous, and this is considered as a source of problems in the later interpretation of requirements. Ambiguity detection tools have been developed with the objective of improving the quality of requirement documents. However, defects as vagueness, optionality, weakness and multiplicity at requirements level can in some cases give an indication of possible variability, either in design and in implementation choices or configurability decisions. Variability information is actually the seed of the software engineering development practice aiming at building families of related systems, known as software product lines. Building on the results of previous analyses conducted on large and real word requirement documents, with QuARS NL analysis tool, we provide here a classification of the forms of ambiguity that indicate variation points, and we illustrate the practical aspects of the approach by means of a simple running example. To provide a more complete description of a line of software products, it is necessary to extrapolate, in addition to variability, also the common elements. To this end we propose here to take advantage of the capabilities of the REGICE tool to extract and cluster the glossary terms from the requirement documents. In summary, we introduce the combined application of two different NL processing tools to extract features and variability and use them to model a software product line. © 2020, Springer Nature Switzerland AG.
"
10.1002/sys.21499,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067385958&origin=inward,Article,SCOPUS_ID:85067385958,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),business process improvement using object-process methodology,"
AbstractView references

For decades, business process improvement (BPI) has been a persistent and expensive concern that spans across many industry sectors. We present OPM-BPI—a model-based method to improve business processes using ISO 19450—Object-Process Methodology (OPM). The approach compares favorably to state-of-the-art business process languages and approaches, such as Business Process Modeling Notation (BPMN). An aviation manufacturing company case study of safely removing a part from an aircraft and reinstalling it demonstrates the method. We show how using OPM-BPI enables removing a large portion of the supporting objects, and how related processes can be eliminated or merged, achieving considerable model simplification that represents a significantly improved, more effective and less wasteful business process. © 2019 Wiley Periodicals, Inc.
"
10.1007/s10270-019-00737-w,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067276496&origin=inward,Article,SCOPUS_ID:85067276496,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),lossless compaction of model execution traces,"
AbstractView references

Dynamic verification and validation (V&V) techniques are used to verify and validate the behavior of software systems early in the development process. In the context of model-driven engineering, such behaviors are usually defined using executable domain-specific modeling languages (xDSML). Many V&V techniques rely on execution traces to represent and analyze the behavior of executable models. Traces, however, tend to be overwhelmingly large, hindering effective and efficient analysis of their content. While there exist several trace metamodels to represent execution traces, most of them suffer from scalability problems. In this paper, we present a generic compact trace representation format called generic compact trace metamodel (CTM) that enables the construction and manipulation of compact execution traces of executable models. CTM is generic in the sense that it supports a wide range of xDSMLs. We evaluate CTM on traces obtained from real-world fUML models. Compared to existing trace metamodels, the results show a significant reduction in memory and disk consumption. Moreover, CTM offers a common structure with the aim to facilitate interoperability between existing trace analysis tools. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.1007/s00500-019-03942-3,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064094627&origin=inward,Article,SCOPUS_ID:85064094627,scopus,2020-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a variable-level automated defect identification model based on machine learning,"
AbstractView references

Static analysis tools, automatically detecting potential source code defects at an early phase during the software development process, are diffusely applied in safety-critical software fields. However, alarms reported by the tools need to be inspected manually by developers, which is inevitable and costly, whereas a large proportion of them are found to be false positives. Aiming at automatically classifying the reported alarms into true defects and false positives, we propose a defect identification model based on machine learning. We design a set of novel features at variable level, called variable characteristics, for building the classification model, which is more fine-grained than the existing traditional features. We select 13 base classifiers and two ensemble learning methods for model building based on our proposed approach, and the reported alarms classified as unactionable (false positives) are pruned for the purpose of mitigating the effort of manual inspection. In this paper, we firstly evaluate the approach on four open-source C projects, and the classification results show that the proposed model achieves high performance and reliability in practice. Then, we conduct a baseline experiment to evaluate the effectiveness of our proposed model in contrast to traditional features, indicating that features at variable level improve the performance significantly in defect identification. Additionally, we use machine learning techniques to rank the variable characteristics in order to identify the contribution of each feature to our proposed model. © 2019, The Author(s).
"
10.1109/ICITR49409.2019.9407794,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105456805&origin=inward,Conference Paper,SCOPUS_ID:85105456805,scopus,2019-12-10,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a review of query optimization techniques for complex event processing,"
AbstractView references

Complex Event Processing (CEP) Systems is a technology that is used in many modern fields of application such as finance and business analysis. It tracks and analyses data from large information streams pertaining to a string of related or non-related events in order to identify patterns and relations that could be used to derive useful connections among seemingly unrelated factors within its applications. CEP systems make use of pattern queries to match identified events within an event stream. However, due to the generalized nature of CEP query languages and the lack of general structure and semantics, it is difficult to write queries that function optimally to deliver the expected results within the required time frames. This issue is particularly of importance as CEP systems often deal with time sensitive data and hence require rapid processing in order to output useful information and hence, defines the importance and requirement for query optimization techniques that may be applied to CEP systems. This paper focuses on research publications related to the four main pattern query optimization techniques, namely, Multi-Query Optimization, Join Query Optimization, Nested Query Processing techniques and Query Rewriting as well as their applications within modern CEP systems. This study further aims to identify possible limitations within the four techniques mentioned previously and advise on possible measures that may be taken to further improve these techniques in order to offer greater efficiency and stability to pattern query processing within CEP systems. © 2019 IEEE.
"
10.1145/3368235.3368831,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85077340484&origin=inward,Conference Paper,SCOPUS_ID:85077340484,scopus,2019-12-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),envisioning slo-driven service selection in multi-cloud applications,"
AbstractView references

The current large selection of cloud instances that are functionally equivalent makes selecting the right cloud service a challenging decision. We envision a model driven engineering (MDE) approach to raise the level of abstraction for cloud service selection. One way to achieve this is through a domain specific language (DSL) for modelling the service level objectives (SLOs) and a brokerage system that utilises the SLO model to select services. However, this demands an understanding of the provider SLAs and the capabilities of the current cloud modelling languages (CMLs). This paper investigates the state-of-the-art for SLO support in both cloud providers SLAs and CMLs in order to identify the gaps for SLO support. We then outline research directions towards achieving the MDE-based cloud brokerage. © 2019 Association for Computing Machinery.
"
10.1109/CSCI49370.2019.00213,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084765180&origin=inward,Conference Paper,SCOPUS_ID:85084765180,scopus,2019-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a labview metamodel for automated analysis,"
AbstractView references

LabVIEW is a graphical environment used by hundreds of thousands of practitioners for developing applications that require fast access to hardware and test data. However, there is limited support for analyzing the LabVIEW models developed by its users, who are mainly traditional system engineers and may not be familiar with existing software engineering techniques. Large LabVIEW models often contain many loops and wires, which makes it very challenging to understand and manually analyze such models. This paper presents an extensible open-source metamodel for design analysis that captures essential relationships across elements in a LabVIEW BlockDiagram. The metamodel has been evaluated on several existing GitHub LabVIEW projects and it has been shown to capture all the dependencies and relationships in the models present in the projects. Finally, we demonstrate the usefulness of the metamodel in integrating LabVIEW with common model management tools. © 2019 IEEE.
"
10.1109/BigData47090.2019.9006518,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85081297051&origin=inward,Conference Paper,SCOPUS_ID:85081297051,scopus,2019-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),stratum: a bigdata-as-a-service for lifecycle management of iot analytics applications,"
AbstractView references

Smart Internet of Things (IoT) applications require real-time and robust predictive analytics, which are based on Machine Learning (ML) models. Building ML models from Big Data is not only time-consuming, but developers often lack the needed expertise for feature engineering, parameter tuning, and model selection. The proliferation of ML libraries and frameworks, data ingestion tools, stream and batch processing engines, visualization techniques, and the range of available hardware platforms further exacerbates the system design, rapid development, and deployment problems. Finally, resource constraints of IoT require that the execution of the analytics engine be distributed across the cloud-edge spectrum. To overcome these daunting challenges, we present Stratum, which is an event-driven Big Data-as-a-Service offering for IoT analytics lifecycle management. Stratum provides users with an intuitive, declarative mechanism based on the principles of model-driven engineering to specify the application and infrastructure requirements. It automates the deployment via generative programming principles. This paper highlights the problems that Stratum resolves, demonstrating its capabilities using real-world case studies. © 2019 IEEE.
"
10.1142/S0219720019500367,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079016565&origin=inward,Conference Paper,SCOPUS_ID:85079016565,scopus,2019-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),winbest-kit: biochemical reaction simulator that can define and customize algebraic equations and events as gui components,"
AbstractView references

We previously developed Windows-based Biochemical Engineering System analyzing Tool-KIT (WinBEST-KIT), a biochemical reaction simulator for analyzing large-scale and complicated biochemical reaction networks. One particularly notable feature is the ability for users to define original mathematical equations for representing unknown kinetic mechanisms and customize them as GUI components for representing reaction steps. Many simulators support System Biology Markup Language SBML; however, since the definition of the algebraic equations (AssignmentRule) and the events are made through an interface that is distinct from the definition of the reaction steps, there are tough works to define them. Accordingly, we have developed a new version of WinBEST-KIT that allows users to define the algebraic equations and the events through the same interface as those used in the definition of the reaction steps and customize them as GUI components appearing in the symbol selection area. The customized algebraic equations and events can thus be visually arranged at any time and any place. It also allows users to easily understand the roles of the algebraic equations and the events. We have also implemented other useful features, including importing/exporting of SBML format files, exporting to MATLAB, and merging the existing models into the model currently being created. The current version of WinBEST-KIT is freely available at http://winbest-kit.org/. © 2019 World Scientific Publishing Europe Ltd.
"
10.1109/DICTA47822.2019.8946083,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078694869&origin=inward,Conference Paper,SCOPUS_ID:85078694869,scopus,2019-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),corporate it-support help-desk process hybrid-automation solution with machine learning approach,"
AbstractView references

Comprehensive IT support teams in large scale organizations require more man power for handling engagement and requests of employees from different channels on a 24×7 basis. Automated email technical queries help desk is proposed to have instant real-time quick solutions and email categorisation. Email topic modelling with various machine learning, deep-learning approaches are compared with different features for a scalable, generalised solution along with sure-shot static rules. Email's title, body, attachment, OCR text, and some feature engineered custom features are given as input elements. XGBoost cascaded hierarchical models, Bi-LSTM model with word embeddings perform well showing 77.3 overall accuracy For the real world corporate email data set. By introducing the thresholding techniques, the overall automation system architecture provides 85.6 percentage of accuracy for real world corporate emails. Combination of quick fixes, static rules, ML categorization as a low cost inference solution reduces 81 percentage of the human effort in the process of automation and real time implementation. © 2019 IEEE.
"
10.1109/APSEC48747.2019.00053,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078220339&origin=inward,Conference Paper,SCOPUS_ID:85078220339,scopus,2019-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rbml: a refined behavior modeling language for safety-critical hybrid systems,"
AbstractView references

As a widely used modeling language, AADL (Architecture Analysis and Design Language) plays an important role in designing safety-critical systems. It provides abundant components for describing system architecture and supports the early prediction and repetitive analysis of performance-critical attributes. However, the approach used by AADL to describe the system behavior is based mainly on automata theory; thus, encountering the state space explosion problem when modeling and verifying large and complex systems is inevitable. Furthermore, due to the lack of means to describe the behavior details, it is also difficult for AADL to support the accurate analysis and verification of functional and non-functional requirements. In this paper, we propose a language called RBML that supports refined behavior modeling to compensate for the behavior modeling and verification deficiencies of AADL. This new language is based on AADL but extends the ability to detail various behaviors and allows SMT (Satisfiability Modulo Theories) solvers to verify the constructed refined behavior model, thus alleviating the state space explosion problem to some extent. Experiments on Baidu Apollo are presented to demonstrate the feasibility of our proposed approach. © 2019 IEEE.
"
10.1109/JSYST.2019.2897628,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075739140&origin=inward,Article,SCOPUS_ID:85075739140,scopus,2019-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a product line systems engineering process for variability identification and reduction,"
AbstractView references

Software product line engineering has attracted attention in the last two decades due to its promising capabilities to reduce costs and time to market through the reuse of requirements and components. In practice, developing system level product lines in a large-scale company is not an easy task as there may be thousands of variants and multiple disciplines involved. The manual reuse of legacy system models at domain engineering to build reusable system libraries and configurations of variants to derive target products can be infeasible. To tackle this challenge, a product line systems engineering process is proposed. Specifically, the process extends research in the system orthogonal variability model to support hierarchical variability modeling with formal definitions; utilizes systems engineering concepts and legacy system models to build the hierarchy for the variability model and to identify essential relations between variants; and finally, analyzes the identified relations to reduce the number of variation points. The process, which is automated by computational algorithms, is demonstrated through an illustrative example on generalized Rolls-Royce aircraft engine control systems. To evaluate the effectiveness of the process in the reduction of variation points, it is further applied to case studies in different engineering domains at different levels of complexity. Subjected to system model availability, reduction of 14%-40% in the number of variation points is demonstrated in the case studies. © 2007-2012 IEEE.
"
10.1016/j.mec.2019.e00089,S2214030118300348,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064166408&origin=inward,Article,SCOPUS_ID:85064166408,scopus,2019-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),impact framework: a python package for writing data analysis workflows to interpret microbial physiology,"Microorganisms can be genetically engineered to solve a range of challenges in diverse including health, environmental protection and sustainability. The natural complexity of biological systems makes this an iterative cycle, perturbing metabolism and making stepwise progress toward a desired phenotype through four major stages: design, build, test, and data interpretation. This cycle has been accelerated by advances in molecular biology (e.g. robust DNA synthesis and assembly techniques), liquid handling automation and scale-down characterization platforms, generating large heterogeneous data sets. Here, we present an extensible Python package for scientists and engineers working with large biological data sets to interpret, model, and visualize data: the IMPACT (Integrated Microbial Physiology: Analysis, Characterization and Translation) framework. Impact aims to ease the development of Python-based data analysis workflows for a range of stakeholders in the bioengineering process, offering open-source tools for data analysis, physiology characterization and translation to visualization. Using this framework, biologists and engineers can opt for reproducible and extensible programmatic data analysis workflows, mediating a bottleneck limiting the throughput of microbial engineering. The Impact framework is available at https://github.com/lmse/impact."
10.1007/s10270-019-00717-0,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060447074&origin=inward,Article,SCOPUS_ID:85060447074,scopus,2019-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),trade-off analysis for sysml models using decision points and csps,"
AbstractView references

The expected benefits of model-based systems engineering (MBSE) include assistance to the system designer in finding the set of optimal architectures and making trade-off analysis. Design objectives such as cost, performance and reliability are often conflicting. The SysML-based methods OOSEM and the ARCADIA method focus on the design and analysis of one alternative of the system. They freeze the topology and the execution platform before optimization starts. Further, their limitation quickly appears when a large number of alternatives were evaluated. The paper avoids these problems and improves trade-off analysis in a MBSE approach by combining the SysML modeling language and so-called “decision points.” An enhanced SysML model with decision points shows up alternatives for component redundancy and instance selection and allocation. The same SysML model is extended with constraints and objective functions using an optimization context and parametric diagrams. Then, a representation of a constraint satisfaction multi-criteria objective problem is generated and solved with a combination of solvers. A demonstrator implements the proposed approach into an Eclipse plug-in; it uses the Papyrus and CSP solvers, both are open-source tools. A case study illustrates the methodology: a mission controller for an Unmanned Aerial Vehicle that includes a stereoscopic camera sensor module. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.1007/s00521-018-3902-6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85057566027&origin=inward,Article,SCOPUS_ID:85057566027,scopus,2019-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research on prediction model of geotechnical parameters based on bp neural network,"
AbstractView references

With the vigorous development of the national economy, the pace and scale of urban construction have been unfolded at an unprecedented speed. A large number of construction projects have made the urban engineering geological exploration activities reach a considerable scale in depth and breadth. The survey results of these projects are very valuable information resources, which not only played an important role in urban planning and construction at that time, but also had high reuse value. Based on BP neural network theory, this paper uses engineering geological database as the research and development platform. Based on the theory of BP neural network and the engineering geological database as the research and development platform, this paper establishes the prediction of geotechnical parameters based on the analysis of the characteristics of geotechnical materials and the distribution of geotechnical sediments and geotechnical parameters. Based on the survey data and specific engineering information, the prediction model of the project was established, and the distribution of the stratum and the relevant geotechnical parameters were predicted. Based on the study of geotechnical properties and BP neural network, a new parameter prediction model is established. Taking the engineering geological database as the platform, using the programming language such as MATLAB, the preliminary research and construction of this prediction system were carried out. The results show that the generalization ability of the prediction model meets the requirements. © 2018, Springer-Verlag London Ltd., part of Springer Nature.
"
10.1007/s11227-018-2549-5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053038510&origin=inward,Article,SCOPUS_ID:85053038510,scopus,2019-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ginsoda: massive parallel integration of stiff ode systems on gpus,"
AbstractView references

Ordinary differential equations (ODEs) are a widespread formalism for the mathematical modeling of natural and engineering systems, whose analysis is generally performed by means of numerical integration methods. However, real-world models are often characterized by stiffness, a circumstance that can lead to prohibitive execution times. In such cases, the practical viability of many computational tools—e.g., sensitivity analysis—is hampered by the necessity to carry out a large number of simulations. In this work, we present ginSODA, a general-purpose black-box numerical integrator that distributes the calculations on graphics processing units, and allows to run massive numbers of numerical integrations of ODE systems characterized by stiffness. By leveraging symbolic differentiation, meta-programming techniques, and source code hashing, ginSODA automatically builds highly optimized binaries for the CUDA architecture, preventing code re-compilation and allowing to speed up the computation with respect to the sequential execution. ginSODA also provides a simplified Python interface, which allows to define a system of ODEs and the test to be performed in a few lines of code. According to our results, ginSODA provides up to a 25 × speedup with respect to the sequential execution. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.
"
10.1145/3338501.3357373,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075860536&origin=inward,Conference Paper,SCOPUS_ID:85075860536,scopus,2019-11-11,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),robust detection of obfuscated strings in android apps,"
AbstractView references

While string obfuscation is a common technique used by mobile developers to prevent reverse engineering of their apps, malware authors also often employ it to, for example, avoid detection by signature-based antivirus products. For this reason, robust techniques for detecting obfuscated strings in apps are an important step towards more effective means of combating obfuscated malware. In this paper, we discuss and empirically characterize four significant limitations of existing machine-learning approaches to string obfuscation detection, and propose a novel method to address these limitations. The key insight of our method is that discriminative classification methods, which try to fit a decision boundary based on a set of positive and negative samples, are inherently bound to generalize poorly when used for string obfuscation detection. Since many different string obfuscation techniques exist, both in the form of commercial tools and as custom implementations, it is close to impossible to construct a training set that is representative of all possible obfuscations. We instead propose a generative approach based on the Naive Bayes method. We first model the distribution of natural-language strings, using a large corpus of strings from 235 languages, and then base our classification on a measure of the confidence with which a language can be assigned to a string. Crucially, this allows us to completely eliminate the need for obfuscated training samples. In our experiments, this new method significantly outperformed both an n-gram based random forest classifier and an entropy-based classifier, in terms of accuracy and generalizability. © 2019 Association for Computing Machinery.
"
10.1186/s12909-019-1846-x,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074847986&origin=inward,Article,SCOPUS_ID:85074847986,scopus,2019-11-11,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),language of written medical educational materials for non-english speaking populations: an evaluation of a simplified bi-lingual approach,"
AbstractView references

Background: Debates have arisen in various non-English speaking countries over the chosen language of instruction in medical education, whether it has to be the English language or the mother tongue. English-based education supporters argue that English is the leading international language of medicine and research, and a crucial tool for Continuing Medical Education (CME), as well as for students who seek practice abroad. On the other hand, mother-tongue-based medical education supporters present it as a way to endorse communication and comprehension between medical practitioners and health care system users, to bridge the gap between practitioners and the paramedical staff, and to overcome linguistic dualism and the language thinking disparity while studying in another. This study aimed to evaluate one of the simplified bi-lingual approaches in terms of medical-educational-written texts for a non-English speaking population: Arabic speaking medical students in specific. Methods: 1546 Arabic-speaking-medical students from different countries participated in a one-step-interactive-experimental-online test. The test assessed participants' scientific comprehension of three distinct written paragraphs: The first paragraph used conventional mother tongue (Arabic), the second combined English terminology and simplified mother tongue (hybrid), and the third used an English excerpt (English). Two multiple-choice questions (First question in Arabic, second in English) followed each paragraph. Response time was communicated for each paragraph. Participants were asked to select their favorable method. Repeated Measures ANOVA models and Paired Samples t-Test were used for statistical analysis. Results: Participants scored a mean of [0.10] for the Arabic paragraph, [0.72] for the hybrid paragraph, and [0.24] for the English paragraph (P < 0.001). Results showed a significantly higher mean of points and correct answers within the fastest time for the hybrid paragraph [0.68] compared to the Arabic [0.08] and English [0.18] paragraphs (P < 0.001). Moreover, 50% of participants preferred the hybrid paragraph over the other two paragraphs. Conclusions: Taking into consideration the large number of participants and the statistically significant results, authors propose that simplified Arabic combined with English terminology may present a viable alternative method for medical-educational-written texts in Arabic-speaking population. © 2019 The Author(s).
"
10.1109/ICSRS48664.2019.8987732,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85080122470&origin=inward,Conference Paper,SCOPUS_ID:85080122470,scopus,2019-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"applying safecomp, a formal integrated system modeling framework, to the design of a steam generator controller","
AbstractView references

We previously introduced in [8] an integrated system of formal model called SafeComp framework that focuses on the implementation of a unified industrial process modeling using the graphic language of Hi-Graphs, a specific class of hypergraphs. This process takes into account that requirements can often be described using different formalisms and additionally provides functional views, taking into account the non-functional and dysfunctional at all stages of the system lifecycle to make the right choices/compromise in terms of software engineering, formal verification and assurance that the system meets the requirements, end-to-end. In this paper we show the application of this framework to explore the space of solutions when designing the control-command of the regulation of a steam generator and we also expose the results of this study. © 2019 IEEE.
"
10.1109/CAC48633.2019.8996449,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85080089604&origin=inward,Conference Paper,SCOPUS_ID:85080089604,scopus,2019-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the architecture of digital system model for systems-of-systems engineering,"
AbstractView references

The rapid increasing challenges of modeling and simulation for Systems-of-Systems Engineering (SoSE) is to deal with their large-scale and complexity of heterogeneous and inter-dependent constituent systems. Typically, the method of structural modeling provides a thorough understanding of the composition and operation of different elements, but loses sight of the dynamic characteristics of the systems or components. To make a comprehensive analysis of the System-of-Systems (SoS), the solution of behavior modeling will be more important to adapt to the evolving of the SoS, which focus on understanding the constituent systems and their relationships, and evaluating changes of the constituent system, and continuously responding to the new requirements and options of SoS. This paper discusses how to support the application of modeling and simulation (MS) based on the framework of SysML to describe the architecture and capability of SoS, and combined with the process and activities of model-based system engineering (MBSE). © 2019 IEEE.
"
10.1109/ASE.2019.00072,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078944075&origin=inward,Conference Paper,SCOPUS_ID:85078944075,scopus,2019-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),combining program analysis and statistical language model for code statement completion,"
AbstractView references

Automatic code completion helps improve developers' productivity in their programming tasks. A program contains instructions expressed via code statements, which are considered as the basic units of program execution. In this paper, we introduce AutoSC, which combines program analysis and the principle of software naturalness to fill in a partially completed statement. AutoSC benefits from the strengths of both directions, in which the completed code statement is both frequent and valid. AutoSC is first trained on a large code corpus to derive the templates of candidate statements. Then, it uses program analysis to validate and concretize the templates into syntactically and type-valid candidate statements. Finally, these candidates are ranked by using a language model trained on the lexical form of the source code in the code corpus. Our empirical evaluation on the large datasets of real-world projects shows that AutoSC achieves 38.9-41.3% top-1 accuracy and 48.2-50.1% top-5 accuracy in statement completion. It also outperforms a state-of-the-art approach from 9X-69X in top-1 accuracy. © 2019 IEEE.
"
10.1115/1.4044229,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072516810&origin=inward,Article,SCOPUS_ID:85072516810,scopus,2019-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deep generative design: integration of topology optimization and generative models,"
AbstractView references

Deep learning has recently been applied to various research areas of design optimization. This study presents the need and effectiveness of adopting deep learning for generative design (or design exploration) research area. This work proposes an artificial intelligent (AI)-based deep generative design framework that is capable of generating numerous design options which are not only aesthetic but also optimized for engineering performance. The proposed framework integrates topology optimization and generative models (e.g., generative adversarial networks (GANs)) in an iterative manner to explore new design options, thus generating a large number of designs starting from limited previous design data. In addition, anomaly detection can evaluate the novelty of generated designs, thus helping designers choose among design options. The 2D wheel design problem is applied as a case study for validation of the proposed framework. The framework manifests better aesthetics, diversity, and robustness of generated designs than previous generative design methods. Copyright © 2019 by ASME.
"
10.1016/j.scs.2019.101673,S221067071931323X,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85069610276&origin=inward,Article,SCOPUS_ID:85069610276,scopus,2019-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),development and application of linear ventilation and temperature models for indoor environmental prediction and hvac systems control,"
                  It has been of great importance to develop better control techniques for heating, ventilation and air conditioning (HVAC) systems to provide occupancy driven energy and comfort management due to the significance of building energy conservation. Following with our previous work, where low-dimensional linear ventilation model (LLVM) and artificial neural network (ANN) were incorporated to realize online control of indoor air quality (IAQ), we continued to expand the work on indoor thermal comfort (ITC) with low-dimensional linear temperature model (LLTM) and contribution ratio of indoor climate (CRI) to provide dependable support for HVAC online control. Two steps were required to be implemented, respectively considering pollutant and temperature responses. As a premise for control, the database was constructed by CFD, which was verified by the corresponding experiments. Linear ventilation model (LVM) and linear temperature model (LTM) could be well employed to expand the CFD database. Then, combined with satisfying ANN and CRI along with low-dimensional linear model (LLM) for database reconstruction, LLVM-based ANN and LLTM-based CRI could rapidly predict the distribution of indoor environmental parameters (e.g., CO2 concentration and temperature) within acceptable errors. Next, the comprehensive evaluation indices were defined to provide weighting factors for indoor environment (IAQ and ITC) and air conditioning energy. By using the current control strategy, the HVAC energy consumption resulting from ventilation and air conditioning loads could be significantly decreased up to 50% and 32% respectively. This study will further promote an intelligent control strategy to improve the deteriorated condition between occupant comfort and HVAC energy conservation.
               "
10.1016/j.infsof.2019.05.010,S0950584919301259,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85066478419&origin=inward,Article,SCOPUS_ID:85066478419,scopus,2019-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),scaling-up domain-specific modelling languages through modularity services,"
                  Context
                  Model-driven engineering (MDE) promotes the active use of models in all phases of software development. Even though models are at a high level of abstraction, large or complex systems still require building monolithic models that prove to be too big for their processing by existing tools, and too difficult to comprehend by users. While modularization techniques are well-known in programming languages, they are not the norm in MDE.
               
                  Objective
                  Our goal is to ease the modularization of models to allow their efficient processing by tools and facilitate their management by users.
               
                  Method
                  We propose five patterns that can be used to extend a modelling language with services related to modularization and scalability. Specifically, the patterns allow defining model fragmentation strategies, scoping and visibility rules, model indexing services, and scoped constraints. Once the patterns have been applied to the meta-model of a modelling language, we synthesize a customized modelling environment enriched with the defined services, which become applicable to both existing monolithic legacy models and new models.
               
                  Results
                  Our proposal is supported by a tool called EMF-Splitter, combined with the Hawk model indexer. Our experiments show that this tool improves the validation performance of large models. Moreover, the analysis of 224 meta-models from OMG standards, and a public repository with more than 300 meta-models, demonstrates the applicability of our patterns in practice.
               
                  Conclusions
                  Modularity mechanisms typically employed in programming IDEs can be successfully transferred to MDE, leading to more scalable and structured domain-specific modelling languages and environments.
               "
10.1145/3358501.3361235,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85077196942&origin=inward,Conference Paper,SCOPUS_ID:85077196942,scopus,2019-10-20,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),methodology to develop domain specific modeling languages,"
AbstractView references

Domain Specific Modeling Languages (DSML) significantly improve productivity in designing Computer Based System (CBS), by enabling them to be modeled at higher levels of abstraction. It is common for large and complex systems with distributed teams, to use DSMLs, to express and communicate designs of such systems uniformly, using a common language. DSMLs enable domain experts, with no or minimal software development background, to model solutions, using the language and terminologies used in their respective domains. Although, there are already a number of DSMLs available for modeling CBSs, their need is felt strongly across multiple domains, which still are not well supported with DSMLs. Developing a new DSML, however, is non trivial, as it requires (a) significant knowledge about the domain for which the DSML needs to be developed, as well as (b) skills to create new languages. In the current practice, DSMLs are developed by experts, who have substantial understanding of the domain of interest and strong background in computer science. One of the many challenges in the development of DSMLs, is the collection of domain knowledge and its utilization, based on which the abstract syntax, the backbone of the DSML is defined. There is a clear gap in the current state of art and practice, with respect to overcoming this challenge. We propose a methodology, which makes it easier for people with different backgrounds such as domain experts, solution architects, to contribute towards defining the abstract syntax of the DSML. The methodology outlines a set of steps to systematically capture knowledge about the domain of interest, and use that to arrive at the abstract syntax of the DSML. The key contribution of our work is in abstracting a CBS from a domain into a Domain Specific Machine, embodied in domain specific concepts. The methodology outlines, how the Domain Specific Machine, when coupled with guidelines from current practices of developing DSMLs, results in the definition of the abstract syntax of the intended DSML. We discuss our methodology in detail, in this paper. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
10.1016/j.eswa.2019.04.045,S0957417419302751,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065136874&origin=inward,Article,SCOPUS_ID:85065136874,scopus,2019-10-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),code similarity detection through control statement and program features,"
                  Software clone detection is an emerging research area in the field of software engineering. Software systems are subjected to continuous modifications in source code to improve the performance of the software, which may lead to code redundancy. Duplicate code/code clone is a piece of code reworked several times in software programs due to copy paste activity or reusability of existing software. Code clone is a prime subject in software evolution. Detection of software clones at the time of software evolution may improve the performance of software and reduce the maintenance cost and effort. This paper proposes metric based methods to detect code clones, as software clone is a universal problem in large scale programming environment. This paper introduces two metric based approaches to detect code clones by comparing (i) Control Statement Features (ii) Program Features like different types of statements, operators and operands. In order to demonstrate the effectiveness of the proposed approaches, extensive experiments are conducted on two datasets, C projects of Bellon's benchmark dataset and student lab programs (SLP).The methods efficiently identify similar functional clones. Proposed models only find similarity of whole programs but intelligent enough to highlight similar code segments across program files.
               "
10.1109/ISSE46696.2019.8984475,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85081091640&origin=inward,Conference Paper,SCOPUS_ID:85081091640,scopus,2019-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design and validation of cyber-physical systems through model abstraction<sup>∗</sup>,"
AbstractView references

Cyber Physical Systems (CPSs) are a category of systems of systems (SoS) that integrate physical and computational elements. Such systems have existed for some time, but rigorous design methodologies for CPSs are scarce. The typical design process of a CPS makes use of a large number of models to help answer the wide range of design questions that arise in development. In other words, the design process produces models that describe the various crucial aspects, scopes and system components on different level of detail and abstraction. These partial models can potentially be coupled for effective information propagation across the different modeling perspectives to increase design reliability. However, they are usually expressed in different specialized tools and programming languages as a consequence of the system heterogeneity and the preference of the numerous designers involved. This paper therefore proposes generating abstractions of detailed sub-models, and then embedding those abstractions into system-wide models formulated in other formalisms. © 2019 IEEE.
"
10.1109/EDOC.2019.00028,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078257165&origin=inward,Conference Paper,SCOPUS_ID:85078257165,scopus,2019-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),atlas: a framework for traceability links recovery combining information retrieval and semi-supervised techniques,"
AbstractView references

Current Model-Based Systems Engineering (MBSE) practices to design and implement complex systems require modeling and analysis based on many representations: structure, dynamics, safety, security, etc. This induces a large volume of overlapping heterogeneous artefacts which are subject to frequent changes during the project life cycle. In order to verify and validate systems requirements and ensure that models meet user's needs, MBSE techniques shall rely on consistent traceability management. In this paper, we investigate the benefits of Information Retrieval (IR) techniques and the latest advances in Natural Language Processing (NLP) approaches to suggest stakeholders with candidate semantic links generated from the processing of structured and unstructured contents. We illustrate our approach called ATLaS (Aggregation Trace Links Support) through an application on the design and analysis of a mobility service gathering several industrial partners. We provide an empirical evaluation regarding its limitations as part of an industrial MBSE process. Most importantly, we highlight how our method drastically reduces the false positive links generated compared to current IR techniques. The results obtained suggest a good synergy between the presented approach and MBSE techniques. © 2019 IEEE.
"
10.1109/ISMSIT.2019.8932734,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078032785&origin=inward,Conference Paper,SCOPUS_ID:85078032785,scopus,2019-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),classification and success investigation of biomedical data sets using supervised machine learning models,"
AbstractView references

Nowadays, information technologies are used in almost every field of Computer Science and Engineering. One of the most used areas is the health sector. With the use of digital hospital systems, patient data is now stored in a computerized environment, thereby creating biomedical data sets. These datasets, which are very large in size, are very difficult to analyze and interpret by a human. The machine learning algorithms are mainly used to analyze and interpreted these data sets. In this study, the performances of 5 machine learning algorithms have been compared by employing 5 different biomedical data sets and the results obtained were compared statistically. Results reveal that the KNN algorithm performs better for small biomedical data sets, whereas the ANN algorithm performs better for large data sets in terms of classification problem for the health sector. © 2019 IEEE.
"
10.1109/PHM-Qingdao46334.2019.8942926,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078029346&origin=inward,Conference Paper,SCOPUS_ID:85078029346,scopus,2019-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),fault diagnosis of rolling bearing based on time and frequency domain analysis and emd,"
AbstractView references

Prognostic and health management (PHM) technology is the use of a large amount of condition monitoring data and information, with the help of all kinds of fault model and artificial intelligence algorithms monitoring, diagnosis, prediction and management of the health status of the equipment technology, by predicting the problems and reliable working life, improving the safety of equipment, minimizing the fault effect, this article in rolling bearing, using Labview software construction time domain analysis program, the analysis of three kinds of condition from different perspective (35Hz12KN/37.5Hz11KN/40HZ10KN)under the rolling bearing, Finally, Matlab software was used for frequency domain analysis and empirical mode decomposition (EMD), and the inherent modal function and vibration signal spectrum were extracted to find out the fault characteristic frequency band, which provided a basis for bearing fault diagnosis under different loads. © 2019 IEEE.
"
10.1109/PHM-Qingdao46334.2019.8942874,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85077988045&origin=inward,Conference Paper,SCOPUS_ID:85077988045,scopus,2019-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),model-based system reliability analysis by using monte carlo methods,"
AbstractView references

increasement of integrity and complexity of aircraft systems, it is difficult to evaluate the impacts of the component failure modes on the systems. In this paper, a method for system reliability analysis of large and complex systems with multiple failure modes is proposed by combining the Monte Carlo (MC) method and model-based technology. The MATLAB/Simulink language is used to create the nominal model. And the model extension is obtained by injecting failure modes based on the nominal model. The extended system model is used to observe and analyze the behaviors and performances of the complex systems in the presence of different faults. Performance metrics are used to evaluate system effects caused by component failures. A procedure for system reliability evaluation based on the MC method is given, which can be applied to the reliability evaluation of a system. The method proposed is insensitive to the dimensionality of problems and can be used to the reliability evaluation of large and complex systems. The system response with fault injection can be analyzed to determine the effect of component failures or their combinations in system reliability analysis, which can avoid the dependence on the subjective judgment and experience of analysts. Furthermore, it can help improve the systems development. A case study is given to illustrate our proposed method. © 2019 IEEE.
"
10.3390/ijerph16193628,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072764196&origin=inward,Article,SCOPUS_ID:85072764196,scopus,2019-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ontology-based healthcare named entity recognition from twitter messages using a recurrent neural network approach,"
AbstractView references

Named Entity Recognition (NER) in the healthcare domain involves identifying and categorizing disease, drugs, and symptoms for biosurveillance, extracting their related properties and activities, and identifying adverse drug events appearing in texts. These tasks are important challenges in healthcare. Analyzing user messages in social media networks such as Twitter can provide opportunities to detect and manage public health events. Twitter provides a broad range of short messages that contain interesting information for information extraction. In this paper, we present a Health-Related Named Entity Recognition (HNER) task using healthcare-domain ontology that can recognize health-related entities from large numbers of user messages from Twitter. For this task, we employ a deep learning architecture which is based on a recurrent neural network (RNN) with little feature engineering. To achieve our goal, we collected a large number of Twitter messages containing health-related information, and detected biomedical entities from the Unified Medical Language System (UMLS). A bidirectional long short-term memory (BiLSTM) model learned rich context information, and a convolutional neural network (CNN) was used to produce character-level features. The conditional random field (CRF) model predicted a sequence of labels that corresponded to a sequence of inputs, and the Viterbi algorithm was used to detect health-related entities from Twitter messages. We provide comprehensive results giving valuable insights for identifying medical entities in Twitter for various applications. The BiLSTM-CRF model achieved a precision of 93.99%, recall of 73.31%, and F1-score of 81.77% for disease or syndrome HNER; a precision of 90.83%, recall of 81.98%, and F1-score of 87.52% for sign or symptom HNER; and a precision of 94.85%, recall of 73.47%, and F1-score of 84.51% for pharmacologic substance named entities. The ontology-based manual annotation results show that it is possible to perform high-quality annotation despite the complexity of medical terminology and the lack of context in tweets. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.
"
10.1109/ICAITI48442.2019.8982144,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85081054292&origin=inward,Conference Paper,SCOPUS_ID:85081054292,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),prediction of semantically correct bangla words using stupid backoff and word-embedding model,"
AbstractView references

Word prediction is an essential technique used in different text entry environment to facilitate error-free writing. It also used as a helping hand for people with different types of disabilities. Word prediction technique is available in different languages. But developing an optimized Bangla word predictor is still a great research challenge. To overcome the challenge we propose a hybrid method to predict Bangla words. The stupid backoff language model is used to detect the most-probable words that may fit into the previously typed sentence by calculating the word sequence frequency. The novelty of this work is that it can provide the semantically correct words as a suggestion. The Word-Embedding model is used to maintain the semantic context of the word. To test this approach, a large corpus is built consisting of almost 0.5 million data. We compared our approach with other well-established methods. The proposed methodology surpasses them by obtaining 83% accuracy. The approach is also computationally efficient as the running time is linear with the prediction length. © 2019 IEEE.
"
10.1109/REW.2019.00027,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85077974723&origin=inward,Conference Paper,SCOPUS_ID:85077974723,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),formal requirements and constraints modelling in form-l for the engineering of complex socio-technical systems,"
AbstractView references

Socio-technical systems combine behaviour and actions from human operators, physical processes (e.g., thermodynamic or electromagnetism), computing and data communication. One often uses expression 'cyber-physical system', or 'systems of systems' in the case of systems composed of socio-technical systems of their own that each have different stakeholders, owners and lifecycles, but that must cooperate in order to achieve what none can achieve independently. Complex systems are systems that require the cooperation and coordination of multiple individuals, multiple teams, multiple engineering disciplines and multiple stakeholders to be fully understood in all necessary aspects. This coordination must be ensured all along the lifetime of the system, from scoping studies that aim at determining the nature of the system needed, to deconstruction. For systems such as power plants or passenger aircrafts, this lifetime may cover several decades, during which the system needs to be operated, maintained, retrofitted and upgraded, by multiple successive generations. EDF has developed a FOrmal Requirements Modelling Language (FORM-L) to help address this issue for what concerns dynamic phenomena. This paper provides a brief introduction to FORM-L and its underlying methodology, illustrated with short examples. © 2019 IEEE.
"
10.1109/MODELS.2019.00-14,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076117499&origin=inward,Conference Paper,SCOPUS_ID:85076117499,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a model-based testing approach for cockpit display systems of avionics,"
AbstractView references

Avionics are highly critical systems that require extensive testing governed by international safety standards. Cockpit Display Systems (CDS) are an essential component of modern aircraft cockpits and display information from the user application (UA) using various widgets. A significant step in the testing of avionics is to evaluate whether these CDS are displaying the correct information. A common industrial practice is to manually test the information on these CDS by taking the aircraft into different scenarios during the simulation. Such testing is required very frequently and at various changes in the avionics. Given the large number of scenarios to test, manual testing of such behavior is a laborious activity. In this paper, we propose a model-based strategy for automated testing of the information displayed on CDS. Our testing approach focuses on evaluating that the information from the user applications is being displayed correctly on the CDS. For this purpose, we develop a profile for capturing the details of different widgets of the display screens using models. The profile is based on the ARINC 661 standard for Cockpit Display Systems. The expected behavior of the CDS visible on the screens of the aircraft is captured using constraints written in Object Constraint Language. We apply our approach on an industrial case study of a Primary Flight Display (PFD) developed for an aircraft. Our results showed that the proposed approach is able to automatically identify faults in the simulation of PFD. Based on the results, it is concluded that the proposed approach is useful in finding display faults on avionics CDS. © 2019 IEEE.
"
10.1109/MODELS.2019.000-1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076084659&origin=inward,Conference Paper,SCOPUS_ID:85076084659,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on-the-fly translation and execution of ocl-like queries on simulink models,"
AbstractView references

MATLAB/Simulink is a tool for dynamic system modelling widely used across industries such as aerospace and automotive. Model management languages such as OCL, ATL and the languages of the Epsilon platform enable the validation, model-to-model, model-to-text transformation of models but tend to focus on the Eclipse Modelling Framework (EMF), a de facto standard for domain specific modelling. As Simulink models are built on an entirely different technical stack, the current solution to manipulate them using such languages requires their transformation into an EMF-compatible representation. This approach is expensive as (a) the cost of the transformation can be crippling for large models, (b) it requires the synchronisation of the native Simulink model and its EMF counterpart, and (c) the EMF-representation may be an incomplete copy of the model potentially hampering model management operations. In this paper we propose an alternative approach that uses the MATLAB API to bridge Simulink models with existing model management languages that relies on the ""on-the-fly"" translation of model management language constructs into MATLAB/Simulink commands. Our approach not only eliminates the cost of the transformation and of the co-evolution of the EMF-compatible representation but also enables full access to all the aspects of Simulink models. We evaluate the performance of both approaches using a set of model validation constraints executed on a sample of the largest Simulink models available on GitHub. Our evaluation suggests that the on-the-fly translation approach can reduce the model validation time by up to 80%. © 2019 IEEE.
"
10.1109/MODELS-C.2019.00108,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075956606&origin=inward,Conference Paper,SCOPUS_ID:85075956606,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),teaching modelling literacy: an artificial intelligence approach,"
AbstractView references

In Model-Driven Engineering (MDE), models are used to build and analyze complex systems. In the last decades, different modelling formalisms have been proposed for supporting software development. However, their adoption and practice strongly rely on mastering essential modelling skills to develop a complete and coherent model-based system. Moreover, it is often difficult for novice modellers to get direct and timely feedback and recommendations on their modelling strategies and decisions, particularly in large classroom settings which hinders their learning. Certainly, there is an opportunity to apply Artificial Intelligence (AI) techniques to an MDE learning environment to empower the provisioning of automated and intelligent modelling advocacy. In this paper, we propose a framework called ModBud (a modelling buddy) to educate novice modellers about the art of abstraction. ModBud uses natural language processing (NLP) and machine learning (ML) to create modelling bots with the aim of improving the modelling skills of novice modellers and assisting other practitioners, too. These bots could be used to support teaching with automatic creation or grading of models and enhance learning beyond the traditional classroom-based MDE education with timely feedback and personalized tutoring. Research challenges for the proposed framework are discussed and a research roadmap is presented. © 2019 IEEE.
"
10.1109/MODELS-C.2019.00034,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075948863&origin=inward,Conference Paper,SCOPUS_ID:85075948863,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a case study of model-driven engineering for automated timetabling,"
AbstractView references

Large educational institutions such as universities need to efficiently allocate their staff and students into teaching and examination timetables, while respecting hard constraints and taking into account preferences. The timetables produced automatically by commercial products are generally unsatisfactory, and considerable manual modification is required before they can be used. The authors have designed and implemented an algorithm which produces good results, but the next challenge is to integrate it into real universities. This involves synchronizing with existing information systems, as well as presenting a user interface that can be used by timetabling staff to view and manipulate the generated timetables and to configure the algorithm. Given the limited number of developers available, the high productivity offered by model-driven engineering was deemed necessary. In this paper, the authors show how they have combined several transformation tools, persistence frameworks and model-driven UI approaches to deliver a first version of an integrated solution for automated timetabling. The authors identify areas of future work and places where the current state of the art could be further developed. © 2019 IEEE.
"
10.1109/MODELS-C.2019.00008,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075948856&origin=inward,Conference Paper,SCOPUS_ID:85075948856,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),querying automotive system models and safety artifacts with mmint and viatra,"
AbstractView references

In recent years, the automotive domain has increased its reliance on model based-software development. Models in the automotive domain have the qualities of being heterogenous, large and interconnected through traceability links. When introducing safety related artifacts, such as HAZOP, FTA, FMEA and safety cases, querying these collections of system models and safety artifacts becomes a complex activity. In this paper, we define generic requirements for querying megamodels and demonstrate how to run queries in our MMINT framework using the Viatra query engine. We apply our querying approach to the Lane Management System from the automotive domain through three different scenarios and compare it to an OCL-based one. © 2019 IEEE.
"
10.1109/MODELS-C.2019.00053,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075946216&origin=inward,Conference Paper,SCOPUS_ID:85075946216,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),firmware synthesis for ultra-thin iot devices based on model integration,"
AbstractView references

Developing firmware for ultra-thin Internet of Things (IoT) devices is challenging due to exceedingly limited hardware resources, increasing functional requirements, and rigorous time-to-market constraints prevalent in industry. Model-driven approaches are often used to tackle these challenges. In the IoT and embedded systems domains, highly specialized metamodels are employed in systems engineering, including the development of firmware. However, these metamodels exist in isolation, limiting the capabilities of model-driven activities. In this paper, we show how firmware synthesis for ultra-thin IoT devices can be enhanced by model integration which is realized by a novel unifying modeling language that aims at integrating the large number of dedicated metamodels. We demonstrate our approach with an industrial use case where we synthesize parts of the firmware for one of the sensor peripherals of an IoT device along with contracts enabling static code verification. © 2019 IEEE.
"
10.1109/MODELS-C.2019.00100,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075942599&origin=inward,Conference Paper,SCOPUS_ID:85075942599,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),improving solution reuse in automotive embedded applications using a pattern library based approach,"
AbstractView references

Given the rise in automotive application complexity and ever shorter development cycles, solution reuse is of great value. The reuse of proven solutions (e.g., Safety Mechanisms or architecture designs) for safety-critical applications is considered a good practice for increasing confidence in the system and cutting development cost and time, and is widely-spread in practice. However, reuse in safety-critical applications is mostly ad-hoc, with lack of process maturity or adequate tool support. Moreover, it is difficult to assess the quality or completeness of a reuse process, if there is no 'definition of done'. We defined a structured 'Pattern Library' approach for the reuse of Safety Mechanisms (fault avoidance / error detection and handling) in the automotive domain, elaborating a prototypical tool implementation for both Pattern Developer and User role. I pragmatically demonstrated the effectiveness of the approach via the evaluation of instantiations into a generic research CASE tool (AutoFOCUS3), as well as a domain-adequate automotive safety modeling framework (SAFE Framework), which subjectively demonstrate improvements in the maturity, adequacy and completeness of the reuse, as well as how the approach can be used to guide tool selection, development and/or extension. I also showcased how the approach can generally be applied to other system design reuse problems, via an instantiation into a large scale automotive functional & technical component library at a leading automotive supplier. These steps and results have been published. The next steps are to consolidate and formalize the instantiations and their results to provide academic rigor to the approach. © 2019 IEEE.
"
10.1109/MODELS-C.2019.00049,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075931548&origin=inward,Conference Paper,SCOPUS_ID:85075931548,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),executable modelling for highly parallel accelerators,"
AbstractView references

High-performance embedded computing is developing rapidly since applications in most domains require a large and increasing amount of computing power. On the hardware side, this requirement is met by the introduction of heterogeneous systems, with highly parallel accelerators that are designed to take care of the computation-heavy parts of an application. There is today a plethora of accelerator architectures, including GPUs, many-cores, FPGAs, and domain-specific architectures such as AI accelerators. They all have their own programming models, which are typically complex, low-level, and involve explicit parallelism. This yields error-prone software that puts the functional safety at risk, unacceptable for safety-critical embedded applications. In this position paper we argue that high-level executable modelling languages tailored for parallel computing can help in the software design for high performance embedded applications. In particular, we consider the data-parallel model to be a suitable candidate, since it allows very abstract parallel algorithm specifications free from race conditions. Moreover, we promote the Action Language for fUML (and thereby fUML) as suitable host language. © 2019 IEEE.
"
10.1109/MODELS-C.2019.00029,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075929785&origin=inward,Conference Paper,SCOPUS_ID:85075929785,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on artificial intelligence for simulation and design space exploration in gas turbine design,"
AbstractView references

Gas turbine design is a process that requires designing many interrelated subsystems, e.g., performance, secondary air system, air compression, or combustion. Subsystem models are created by various engineering design tools. During the design process there exists an extraordinary amount of generated data resulting from created models, simulation, and engine field tests. This data can be leveraged by artificial intelligence techniques such as machine learning to help accelerate the exploration of the large design spaces existing in the complex system of a gas engine. This paper presents a vision and road map of integrating such AIs and preliminary ideas on relevant AI models for such use cases. We explore increasing the realistic nature of existing simulations, approximating simulations to avoid excess computation, and cumulative effect modeling. © 2019 IEEE.
"
10.1109/MODELS-C.2019.00065,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075927773&origin=inward,Conference Paper,SCOPUS_ID:85075927773,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),position paper: knowledge sharing and distances in collaborative modeling,"
AbstractView references

To develop systems effectively, a shared system understanding is required. Collaborative modeling is one way to capture this shared understanding. Increasingly, in large systems engineering projects different distances lead to social barriers between stakeholders. These barriers affect the quantity and quality of knowledge that is shared between stakeholders, thus reducing the quality of the resulting product. While it has been proposed to limit modeling activities to co-located teams, this might not always be possible or feasible. We argue that, despite the technological advances in collaborative modeling, effective collaboration can only be achieved if we understand how to account for social barriers. We propose to study, in depth, how these barriers affect modeling, and how their effects can be reduced. By understanding the effects of social barriers and accounting for them, we can maximize the benefits of collaborative modeling. © 2019 IEEE.
"
10.1109/MODELS-C.2019.00052,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075921584&origin=inward,Conference Paper,SCOPUS_ID:85075921584,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),simulation of model execution for embedded systems,"
AbstractView references

In automotive and robotics, simulation is an indispensable tool for testing and validation. A simulator is able to test a system under varying conditions at low cost in a non-safety-critical environment. Furthermore, in the development process of a new vehicle, the first prototype is mostly produced after a long design phase, which can take up to several years. Before the prototype is available, tests can only be performed in a simulation. To make the simulation results reliable, both the system and its environment need to be modeled as realistic as possible. As modern vehicles include a large amount of software, the execution of vehicle software needs to be simulated with respect to the underlying E/E infrastructure. In this paper, we present a simulation framework for the execution of vehicle control models deployed in a vehicle network. Furthermore, the execution simulator is embedded into a vehicle simulator, making it possible to validate the vehicle software functionality under the given hardware conditions. © 2019 IEEE.
"
10.1109/MODELS-C.2019.00079,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075918465&origin=inward,Conference Paper,SCOPUS_ID:85075918465,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using the scade toolchain to generate requirements-based test cases for an adaptive cruise control system,"
AbstractView references

In the last years, model-driven engineering has gained a lot of traction, especially in industrial domains, such as automotive or avionics. Various tools which support model-driven engineering, e.g. SCADE or MATLAB/Simulink, have developed over the years in fully fledged integrated development environments, with strong capabilities for the modeling of complex software systems. Model-driven engineering tools are mature enough so that the model created with them are amenable to formal analysis for the purpose of verification and validation. Acceptance testing is a validation method by which a system is tested extensively against legal and customer requirements, before it is allowed in series production. Due to the inherent complexity of automotive systems, large requirements catalogues have become usual in this domain. Checking that a complex automotive software system conforms to an extensive requirements catalogue is a task which cannot be managed manually anymore. In this paper, we design a workflow for test engineers to construct test cases from formalized requirements and examine the quality of tests via mutant testing within the SCADE toolchain. We construct an academic case study based on a prototypical adaptive cruise control system and evaluate our workflow on it. We report on results and lessons learned. © 2019 IEEE.
"
10.1109/ETFA.2019.8869006,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074192597&origin=inward,Conference Paper,SCOPUS_ID:85074192597,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),extending the formal process description towards consistency in product/ion-aware modeling,"
AbstractView references

In discrete manufacturing, basic and detail engineering workgroups collaborate to design a cyber-physical production system. Product/ion-aware modeling recognizes requirements coming from the product and from the production process for designing a production resource. These requirements imply consistency dependencies between product, production process, and resource (PPR) model elements. Unfortunately, there is only limited support for modeling consistency dependencies between PPR model elements and for defining abstract types in addition to concrete instances of PPR model elements. In this paper, we build on the PPR modeling capabilities of the VDI/VDE 3682 guideline, the Formal Process Description (FPD). We propose extensions for representing the refinement of types and instances as well as consistency dependencies between PPR model elements. We evaluate the FPD language extensions in a feasibility study with domain experts at a large production system engineering company for discrete manufacturing. The main result is that the domain experts found the extended FPD useful and usable for representing PPR consistency as a foundation for making design decisions. © 2019 IEEE.
"
10.3390/app9183658,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072406763&origin=inward,Article,SCOPUS_ID:85072406763,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),information extraction from electronic medical records using multitask recurrent neural network with contextual word embedding,"
AbstractView references

Clinical named entity recognition is an essential task for humans to analyze large-scale electronic medical records efficiently. Traditional rule-based solutions need considerable human effort to build rules and dictionaries; machine learning-based solutions need laborious feature engineering. For the moment, deep learning solutions like Long Short-term Memory with Conditional Random Field (LSTM-CRF) achieved considerable performance in many datasets. In this paper, we developed a multitask attention-based bidirectional LSTM-CRF (Att-biLSTM-CRF) model with pretrained Embeddings from Language Models (ELMo) in order to achieve better performance. In the multitask system, an additional task named entity discovery was designed to enhance the model's perception of unknown entities. Experiments were conducted on the 2010 Informatics for Integrating Biology & the Bedside/Veterans Affairs (I2B2/VA) dataset. Experimental results show that our model outperforms the state-of-the-art solution both on the single model and ensemble model. Our work proposes an approach to improve the recall in the clinical named entity recognition task based on the multitask mechanism. © 2019 by the authors.
"
10.1109/MC.2019.2924546,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85071551672&origin=inward,Article,SCOPUS_ID:85071551672,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generative adversarial networks in ai-enabled safety-critical systems: friend or foe?,"
AbstractView references

Generative adversarial networks can be exploited to launch attacks against detection systems that rely on artificial intelligence (AI). To build effective cyberphysical systems that are operationally robust and socially accepted, we must expend significant effort to develop novel AI-based safety-critical systems. © 1970-2012 IEEE.
"
10.1002/cae.22119,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068818960&origin=inward,Article,SCOPUS_ID:85068818960,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),from modeling to virtual laboratory development of a continuous binary distillation column for engineering education using matlab and labview,"
AbstractView references

The utilization of computers in all aspects and scales of chemical engineering, from fundamental understanding to solution of large-scale optimizations, is spreading quickly, hand-in-hand with access to data science and information technology. It is straightforward to involve computers and virtual laboratory tools into engineering education. This study presents the development of an interactive virtual laboratory method for a continuous binary distillation process. The three main steps, namely, mathematical modeling, model calibration, and graphical user interface (GUI) development are described and discussed in this paper. The model takes into account the specific needs of the simulator (fast run time) and limitations from the measurement system (calibration considerations). Then, it is validated based on temperature and concentration data from a lab/pilot scale continuous rectification column. The separation of an ethanol–water mixture is considered, the classroom example of distillation, which is, on the other hand, an azeotrope-forming nonideal mixture. The objective of this paper, beyond the virtual laboratory coding description, is to provide useful guidelines for a user-friendly and yet realistic simulator development for commonly available experimental devices in chemical engineering education. To fulfill this aim, MATLAB and LabVIEW, two routinely used programming languages by chemical and process engineers, are used. © 2019 Wiley Periodicals, Inc.
"
10.1177/1063293X19855115,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068359820&origin=inward,Article,SCOPUS_ID:85068359820,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),narrowing the set of complex systems’ possible design solutions derived from the set-based concurrent engineering approach,"
AbstractView references

Nowadays, complex systems are dominating our contemporary as well as professional lives. These modern technical products are considered as mechatronic systems which incorporate mechanics with electronics, software, and control in various domains mainly transport, medicine, and robotics. The development of these modern technical products is thus so tough. Hence, mechatronics’ critical challenges are to be not only well understood but also supported by practical models and tools in order to overcome this difficulty. Moreover, using the traditional design method which is point based is inefficient as it leads to a huge decrease in the innovation potential through limiting the design space to few solutions, an important increase in the cost of the product as well as production delay due to the great number of iterations. Some product development practices have shifted from using the “fixed-point design” approach to the “set-based design” one. Indeed, the set-based concurrent engineering widely considers a set of possible solutions and then shrinks the number of possibilities in order to converge toward a final solution. Yet, since this approach is too difficult to be put into action, a small number of industries in the field of mechatronics use the set-based concurrent engineering concept. Accordingly, this work aims to develop a novel method for the purpose of facilitating the development of a complex product based on the set-based concurrent engineering method and its implementation in an industrial setting by developing an algorithm to find the set of possible solutions to the design problem and narrow this set to merge toward the final solution. Finally, this algorithm is implemented using “Python” language. © The Author(s) 2019.
"
10.1016/j.jpdc.2019.04.019,S0743731518305690,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065618798&origin=inward,Article,SCOPUS_ID:85065618798,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),large-scale performance of a dsl-based multi-block structured-mesh application for direct numerical simulation,"
                  SBLI (Shock-wave/Boundary-layer Interaction) is a large-scale Computational Fluid Dynamics (CFD) application, developed over 20 years at the University of Southampton and extensively used within the UK Turbulence Consortium. It is capable of performing Direct Numerical Simulations (DNS) or Large Eddy Simulation (LES) of shock-wave/boundary-layer interaction problems over highly detailed multi-block structured mesh geometries. SBLI presents major challenges in data organization and movement that need to be overcome for continued high performance on emerging massively parallel hardware platforms. In this paper we present research in achieving this goal through the OPS embedded domain-specific language. OPS targets the domain of multi-block structured mesh applications. It provides an API embedded in C/C++ and Fortran and makes use of automatic code generation and compilation to produce executables capable of running on a range of parallel hardware systems. The core functionality of SBLI is captured using a new framework called OpenSBLI which enables a developer to declare the partial differential equations using Einstein notation and then automatically carryout discretization and generation of OPS (C/C++) API code. OPS is then used to automatically generate a wide range of parallel implementations. Using this multi-layered abstractions approach we demonstrate how new opportunities for further optimizations can be gained, such as fine-tuning the computation intensity and reducing data movement and apply them automatically. Performance results demonstrate there is no performance loss due to the high-level development strategy with OPS and OpenSBLI, with performance matching or exceeding the hand-tuned original code on all CPU nodes tested. The data movement optimizations provide over 3
                        ×
                      speedups on CPU nodes, while GPUs provide 5
                        ×
                      speedups over the best performing CPU node. The OPS generated parallel code also demonstrates excellent scalability on nearly 100K cores on a Cray XC30 (ARCHER at EPCC) and on over 4K GPUs on a CrayXK7 (Titan at ORNL).
               "
10.1007/s13740-019-00103-5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064937003&origin=inward,Article,SCOPUS_ID:85064937003,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a multi-criteria evaluation approach for selecting a sensitive business process modeling language for knowledge management,"
AbstractView references

In an organizational context, the modeling and representation of sensitive business processes (SBPs) have become an effective way of managing and developing organization’s knowledge that needs to be capitalized. These processes are characterized by a high complexity and dynamism in their execution, high number of critical activities with intensive acquisition, sharing, storage and (re)use of very specific crucial knowledge, large diversity of heterogeneous knowledge and information sources, high number of knowledge conversion actions and high degree of collaboration among experts. Thus, the specification of a precise conceptualization for SBPs, and the selection of an appropriate SBP modeling language that adequately characterizes and integrates all their relevant dimensions are of prime importance. We aim at improving the localization and identification of the crucial knowledge that are mobilized by and created by these processes. This paper proposes a multi-criteria-based approach for evaluating and comparing currently widely used modeling languages (process oriented and knowledge oriented) for the representation of SBPs, taking into account their specific modeling requirements. We consider guiding and justifying the choice of the most suitable SBP modeling language for knowledge identification purposes. The different modeling languages are originally assessed according to the ontological completeness of SBP modeling aspects (i.e., the coverage of the functional, organizational, behavioral, informational, intentional and knowledge dimensions) based on the BPM4KI meta-model (as an SBP specification based on core ontologies). Secondly, they are evaluated according to several key requirement indicators (e.g., understandability, expressibility, complexity, level of adoption, tools availability and extendibility). Results show that none of the studied languages, individually, satisfies all the SBP modeling requirements. In this study, we choose the better one positioned nowadays, BPMN 2.0, as the best suited standard for SBP representation. Moreover, we develop a valid extension of BPMN 2.0 «BPMN4SBP» for integrating and implementing all relevant SBP modeling dimensions (the six BPM4KI dimensions), exploring the dynamic, interaction and knowledge aspects. The extension is then used to illustrate the representation of SBPs in a real case study in the healthcare domain. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.1016/j.fusengdes.2018.11.016,S0920379618307221,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85057237376&origin=inward,Article,SCOPUS_ID:85057237376,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),systems engineering approach in support to the breeding blanket design,"
                  Nowadays the Systems Engineering (SE) methodology is applied in several fields of engineering and it represents a powerful interdisciplinary means to enable the realisation of complex systems taking into account the customer and Stakeholder´s needs. Also in the fusion community, this theme is becoming increasingly pressing and the implementation of the SE approach, from the early stage of design, is now a must. Indeed, within the framework of EUROfusion activities, SE method has been selected for capturing the system and interface requirements and for their management and verification with particular focus to the Breeding Blanket (BB) System of the European Demonstration Fusion Power Reactor (DEMO). Specifically, various levels of functions and requirements have been elicited and a development of the BB SE model, set-up using the Systems Modelling Language (SySML), has been performed. This paper describes the advantages of applying a SE approach to the BB design considering, in particular, the BB requirement development and management and the definition of the interfaces between the BB and the major interconnected systems, including Remote Maintenance, Balance of Plant, Vacuum Vessel attachment, Heating and Current Drive and Fuelling Lines Systems. An effective application of SE technique to the pre-conceptual design phase of the BB is also provided in this paper.
               "
10.1109/JSYST.2018.2876836,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056183083&origin=inward,Article,SCOPUS_ID:85056183083,scopus,2019-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),conceptualization of a system-of-systems in the defense domain: an experience report in the brazilian scenario,"
AbstractView references

National sovereignty and protection require a diversity of interdependent systems that jointly provide a large infrastructure for the national security, making possible a continuous monitoring and control. These systems assure the confidential information exchange while providing more complex functionalities when working together and forming alliances known as Systems-of-Systems (SoS). This paper reports an experience in the Brazilian defense scenario, externalizing the acquired knowledge in the form of lessons learned during the conduction of a real, strategic project called SisGAAz (Blue Amazon Management System), which has its main goal to develop the Brazilian navy management SoS. In particular, we focus on reporting our experience in the architectural design of this SoS as a quality driver in our project. We also raise challenges that were overcome, and also others that must still be faced. The results communicated herein contribute to deliver a panorama of the Brazilian state of the practice about SoS engineering. Such results are important, as they report the current situation and gaps to be bridged by both academics and practitioners, not only in Brazil but also worldwide, especially in those developing countries that are also living and implementing such technological revolution. © 2019 IEEE.
"
10.1007/s10664-019-09717-6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065306542&origin=inward,Article,SCOPUS_ID:85065306542,scopus,2019-08-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),aspectocl: using aspects to ease maintenance of evolving constraint specification,"
AbstractView references

Constraints play an important role in Model-Driven Software Engineering. Industrial systems commonly exhibit cross-cutting behaviors in design artifacts. Aspect-orientation is a well-established approach to deal with cross-cutting behaviors and has been successfully used for programming and design languages. In model-driven software engineering, the presence of cross-cutting constraints makes it difficult to maintain constraints defined on the models of large-scale industrial systems. In this work, we improve our previous work on AspectOCL, which is an extension of OCL that allows modeling of cross-cutting constraints. We provide the abstract and concrete syntax of the language. We add support for new constructs such as composite aspects and invariant specification on a package. We also provide tool support for writing cross-cutting constraints using AspectOCL. To evaluate AspectOCL, we apply it on benchmark case studies from the OCL repository. The results show that by separating the cross-cutting constraints, the number of constructs in the constraint specifications can be reduced to a large amount. AspectOCL reduces the maintenance effort by up to 55% in one case study. To explore the impact on maintenance time and accuracy, we also perform a controlled experiment with 90 student subjects. The results show that AspectOCL has a small magnitude of improvement in terms of maintenance time when compared to OCL, whereas modifications to OCL specification are more accurate. The post-experiment survey indicates that the majority of subjects favored AspectOCL, but faced challenges in applying aspect-orientation to constraint specification due to a lack of prior exposure. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.
"
10.1145/3338906.3338920,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85071946902&origin=inward,Conference Paper,SCOPUS_ID:85071946902,scopus,2019-08-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generating automated and online test oracles for simulink models with continuous and uncertain behaviors,"
AbstractView references

Test automation requires automated oracles to assess test outputs. For cyber physical systems (CPS), oracles, in addition to be automated, should ensure some key objectives: (i) they should check test outputs in an online manner to stop expensive test executions as soon as a failure is detected; (ii) they should handle time- and magnitude-continuous CPS behaviors; (iii) they should provide a quantitative degree of satisfaction or failure measure instead of binary pass/fail outputs; and (iv) they should be able to handle uncertainties due to CPS interactions with the environment. We propose an automated approach to translate CPS requirements specified in a logic-based language into test oracles specified in Simulink - a widely-used development and simulation language for CPS. Our approach achieves the objectives noted above through the identification of a fragment of Signal First Order logic (SFOL) to specify requirements, the definition of a quantitative semantics for this fragment and a sound translation of the fragment into Simulink. The results from applying our approach on 11 industrial case studies show that: (i) our requirements language can express all the 98 requirements of our case studies; (ii) the time and effort required by our approach are acceptable, showing potentials for the adoption of our work in practice, and (iii) for large models, our approach can dramatically reduce the test execution time compared to when test outputs are checked in an offline manner. © 2019 ACM.
"
10.1088/1757-899X/563/5/052075,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85071857262&origin=inward,Conference Paper,SCOPUS_ID:85071857262,scopus,2019-08-09,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an automatic test case generation method based on sysml activity diagram,"
AbstractView references

Model-Based Systems Engineering (MBSE) is a new method to engineer the modern large, complex systems. MBSE has a good guarantee for system reliability and it can improve the efficiency of system development. So, MBSE has received extensive attention from academia and industry. SysML is an auxiliary language for MBSE. SysML activity diagram can model the behavior of the system. In this paper, we use SysML activity diagram to generate test case. There are two main things in our work. One is that we have designed an automated SysML activity diagram test case generation method. Second, we develop a test case generation tool to support related applications in the industry. © Published under licence by IOP Publishing Ltd.
"
10.3389/frobt.2019.00062,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85088480545&origin=inward,Article,SCOPUS_ID:85088480545,scopus,2019-08-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),natural language processing in large-scale neural models for medical screenings,"
AbstractView references

Many medical screenings used for the diagnosis of neurological, psychological or language and speech disorders access the language and speech processing system. Specifically, patients are asked to fulfill a task (perception) and then requested to give answers verbally or by writing (production). To analyze cognitive or higher-level linguistic impairments or disorders it is thus expected that specific parts of the language and speech processing system of patients are working correctly or that verbal instructions are replaced by pictures (avoiding auditory perception) or oral answers by pointing (avoiding speech articulation). The first goal of this paper is to propose a large-scale neural model which comprises cognitive and lexical levels of the human neural system, and which is able to simulate the human behavior occurring in medical screenings. The second goal of this paper is to relate (microscopic) neural deficits introduced into the model to corresponding (macroscopic) behavioral deficits resulting from the model simulations. The Neural Engineering Framework and the Semantic Pointer Architecture are used to develop the large-scale neural model. Parts of two medical screenings are simulated: (1) a screening of word naming for the detection of developmental problems in lexical storage and lexical retrieval; and (2) a screening of cognitive abilities for the detection of mild cognitive impairment and early dementia. Both screenings include cognitive, language, and speech processing, and for both screenings the same model is simulated with and without neural deficits (physiological case vs. pathological case). While the simulation of both screenings results in the expected normal behavior in the physiological case, the simulations clearly show a deviation of behavior, e.g., an increase in errors in the pathological case. Moreover, specific types of neural dysfunctions resulting from different types of neural defects lead to differences in the type and strength of the observed behavioral deficits. © Copyright © 2019 Stille, Bekolay, Blouw and Kröger.
"
10.1109/TASE.2019.00-22,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076962118&origin=inward,Conference Paper,SCOPUS_ID:85076962118,scopus,2019-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),abac requirements engineering for database applications,"
AbstractView references

We show how complex privacy requirements can be represented and processed by an extended model of Attribute Based Access Control (ABAC), working with a simple database applications pattern. During application model development, most likely based on UML (e.g. Use Case, Class Diagrams), the analyst and possibly the end user specifies ABAC permissions, and then verifies their effect by running queries on the target data. The ABAC model supports positive and negative permissions, 'break glass' overrides of negative permissions, and message/alert generation. The permissions combining algorithms are based on relational database optimisation, and permissions processing is implemented by query modification, producing structurally-optimised queries in an SQL-like language; the queries can then be processed by many database and big data systems. The method and models have been implemented in a prototype Privacy Preferences Tool in collaboration with a large medical records development, and we discuss experiences with focus group evaluations of this tool. © 2019 IEEE.
"
10.1016/j.softx.2019.100357,S2352711019302134,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074753758&origin=inward,Article,SCOPUS_ID:85074753758,scopus,2019-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),practical software reliability engineering with the software failure and reliability assessment tool (sfrat),"Many large software projects struggle to achieve their reliability targets. Software reliability growth models quantify reliability from failure data collected during software testing. This paper presents the Software Failure and Reliability Assessment Tool (SFRAT), which implements several software reliability growth models as a free and open source application. The open source nature of the tool enables users to integrate the methods into their organization’s workflow and researchers to contribute additional statistical methods. The tool is presented in the context of a NASA project."
10.1109/INDIN41052.2019.8972322,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074213175&origin=inward,Conference Paper,SCOPUS_ID:85074213175,scopus,2019-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),quality risks in the data exchange process for collaborative cpps engineering,"
AbstractView references

The realization of a cyber-physical production system (CPPS) requires suitable methods and tools for the exchange and integration of engineering data between collaborating disciplines. Unfortunately, the description languages used in a CPPS Engineering (CPPSE) organization to describe discipline-specific views are not necessarily well suited for high-quality data exchange between workgroups. In this paper, we identify technical debt and risks regarding CPPSE description languages for data exchange using the VDI 3695 guideline as best practice. We report on effects and likely causes of the quality risks identified in a case study at a large CPPSE company. Based on data from workshops and semi-structured interviews with 28 domain experts from 12 workgroups, we propose a preliminary model relating causes and effects as foundation for analyzing and managing risks in the CPPSE data exchange process. © 2019 IEEE.
"
10.1109/ISI.2019.8823152,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072980110&origin=inward,Conference Paper,SCOPUS_ID:85072980110,scopus,2019-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),no-doubt: attack attribution based on threat intelligence reports,"
AbstractView references

The task of attack attribution, i.e., identifying the entity responsible for an attack, is complicated and usually requires the involvement of an experienced security expert. Prior attempts to automate attack attribution apply various machine learning techniques on features extracted from the malware's code and behavior in order to identify other similar malware whose authors are known. However, the same malware can be reused by multiple actors, and the actor who performed an attack using a malware might differ from the malware's author. Moreover, information collected during an incident may contain many clues about the identity of the attacker in addition to the malware used. In this paper, we propose a method of attack attribution based on textual analysis of threat intelligence reports, using state of the art algorithms and models from the fields of machine learning and natural language processing (NLP). We have developed a new text representation algorithm which captures the context of the words and requires minimal feature engineering. Our approach relies on vector space representation of incident reports derived from a small collection of labeled reports and a large corpus of general security literature. Both datasets have been made available to the research community. Experimental results show that the proposed representation can attribute attacks more accurately than the baselines' representations. In addition, we show how the proposed approach can be used to identify novel previously unseen threat actors and identify similarities between known threat actors. © 2019 IEEE.
"
10.1177/1548512917725620,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85066972117&origin=inward,Article,SCOPUS_ID:85066972117,scopus,2019-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),framing cyber warfare: an analyst’s perspective,"
AbstractView references

Understanding how new phenomenon impacts complex systems requires the capacity to examine and characterize how the system might respond. A soft-systems approach to analyzing cyber warfare can be flexible enough to deal with the inherent complexity and rates of change and still support a more rigorous representation. This paper presents high-level models that use distinct lenses to provide an understanding of cyber implications in the system. These reference models are intended to allow a consistent approach to be applied across conventional and cyber warfare domains and provide a common language to underpin debate and discussion. © The Author(s) 2017.
"
10.1109/TCBB.2018.2858794,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050375411&origin=inward,Article,SCOPUS_ID:85050375411,scopus,2019-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),pareto optimization of combinatorial mutagenesis libraries,"
AbstractView references

In order to increase the hit rate of discovering diverse, beneficial protein variants via high-throughput screening, we have developed a computational method to optimize combinatorial mutagenesis libraries for overall enrichment in two distinct properties of interest. Given scoring functions for evaluating individual variants, POCoM (Pareto Optimal Combinatorial Mutagenesis) scores entire libraries in terms of averages over their constituent members, and designs optimal libraries as sets of mutations whose combinations make the best trade-offs between average scores. This represents the first general-purpose method to directly design combinatorial libraries for multiple objectives characterizing their constituent members. Despite being rigorous in mapping out the Pareto frontier, it is also very fast even for very large libraries (e.g., designing 30 mutation, billion-member libraries in only hours). We here instantiate POCoM with scores based on a target's protein structure and its homologs' sequences, enabling the design of libraries containing variants balancing these two important yet quite different types of information. We demonstrate POCoM's generality and power in case study applications to green fluorescent protein, cytochrome P450, and β-lactamase. Analysis of the POCoM library designs provides insights into the trade-offs between structure- and sequence-based scores, as well as the impacts of experimental constraints on library designs. POCoM libraries incorporate mutations that have previously been found favorable experimentally, while diversifying the contexts in which these mutations are situated and maintaining overall variant quality. © 2004-2012 IEEE.
"
10.1145/3307681.3326604,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85069213305&origin=inward,Conference Paper,SCOPUS_ID:85069213305,scopus,2019-06-17,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),better late than never: an n-variant framework of verification for java source code on cpu × gpu hybrid platform,"
AbstractView references

A method of detecting malicious intrusions and runtime faults in software is proposed, which replicates untrusted computations onto two diverse but often co-located instruction architectures: CPU and GPU. Divergence between the replicated computations signals an intrusion or fault, such as a zero-day exploit. A prototype implementation for Java demonstrates that the approach is realizable in practice, and can successfully detect exploitation of Java VM and runtime system vulnerabilities even when the vulnerabilities are not known in advance to defenders. To achieve acceptable performance, it is shown that GPU parallelism can be leveraged to rapidly validate CPU computations that would otherwise exhibit unacceptable performance if executed on GPU alone. The resulting system detects anomalies in CPU computations on a short delay, during which the GPU replica quickly validates many CPU computation fragments in parallel in order to catch up with the CPU computation. Significant differences between the CPU and GPU computational models lead to high natural diversity between the replicas, affording detection of large exploit classes without laborious manual diversification of the code. © 2019 Association for Computing Machinery.
"
10.1007/s10664-018-9641-6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052622088&origin=inward,Article,SCOPUS_ID:85052622088,scopus,2019-06-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),empowering ocl research: a large-scale corpus of open-source data from github,"
AbstractView references

Model-driven engineering (MDE) enables the rise in abstraction during development in software and system design. In particular, meta-models become a central artifact in the process, and are supported by various other artifacts such as editors and transformation. In order to define constraints, invariants, and queries on model-driven artifacts, a generic language has been developed: the Object Constraint Language (OCL). In literature, many studies into OCL have been performed on small collections of data, mostly originating from a single source (e.g., OMG standards). As such, generalization of results beyond the data studied is often mentioned as a threat to validity. Creation of a benchmark dataset has already been identified as a key enabler to address the generalization threat. To facilitate further empirical studies in the field of OCL, we present the first large-scale dataset of 103262 OCL expression, systematically extracted from 671 GitHub repositories. In particular, our dataset has extracted these expressions from various types of files (a.o. metamodels and model-to-text transformations). In this work we showcase a variety of different studies performed using our dataset, and describe several other types that could be performed. We extend previous work with data and experiments regarding OCL in model-to-text (mtl) transformations. © 2018, The Author(s).
"
10.1186/s12859-019-2843-0,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067187226&origin=inward,Article,SCOPUS_ID:85067187226,scopus,2019-06-11,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the kendrick modelling platform: language abstractions and tools for epidemiology,"
AbstractView references

Background: Mathematical and computational models are widely used to study the transmission, pathogenicity, and propagation of infectious diseases. Unfortunately, complex mathematical models are difficult to define, reuse and reproduce because they are composed of several concerns that are intertwined. The problem is even worse for computational models because the epidemiological concerns are also intertwined with low-level implementation details that are not easily accessible to non-computing scientists. Our goal is to make compartmental epidemiological models easier to define, reuse and reproduce by facilitating implementation of different simulation approaches with only very little programming knowledge. Results: We achieve our goal through the definition of a domain-specific language (DSL), Kendrick, that relies on a very general mathematical definition of epidemiological concerns as stochastic automata that are combined using tensor-algebra operators. A very large class of epidemiological concerns, including multi-species, spatial concerns, control policies, sex or age structures, are supported and can be defined independently of each other and combined into models to be simulated by different methods. Implementing models does not require sophisticated programming skills any more. The various concerns involved within a model can be changed independently of the others as well as reused within other models. They are not plagued by low-level implementation details. Conclusions: Kendrick is one of the few DSLs for epidemiological modelling that does not burden its users with implementation details or required sophisticated programming skills. It is also currently the only language for epidemiology modelling that supports modularity through clear separation of concerns hence fostering reproducibility and reuse of models and simulations. Future work includes extending Kendrick to support non-compartmental models and improving its interoperability with existing complementary tools. © 2019 The Author(s).
"
10.1109/DT.2019.8813373,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072535680&origin=inward,Conference Paper,SCOPUS_ID:85072535680,scopus,2019-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),toward generation of dependability assessment models for industrial control system,"
AbstractView references

This article focuses on the development of a tool-based approach for the assessment of industrial control IT systems. The originality of the approach relies in two main points. First of all, the underlying formal models for dependability assessment must cover dynamic behavior of the IT architectures to take into account reparation, reconfiguration and modes in the life cycle of the architecture. Secondly, these formal models must be automatically established and hidden to the architecture designers to reduce time consumption when dealing with a large amount of candidate architectures evaluated during the engineering phase. This work is a first step towards such an objective by defining a structured UML (Unified Modelling Language) modelling framework for identifying and structuring the key objects of an architecture with regard to dependability. © 2019 IEEE.
"
10.1002/smr.2144,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067873971&origin=inward,Conference Paper,SCOPUS_ID:85067873971,scopus,2019-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the parallel agile process: applying parallel processing techniques to software engineering,"
AbstractView references

For the last 4 years, we have been experimenting with the parallel agile (PA) approach. PA achieves significant schedule compression by leveraging parallelism; large teams of developers can independently and concurrently develop scenarios from initial concept through code. This paper summarizes our experience in defining and evolving PA by applying it to four representative emergent-technology applications: location-based advertising, picture sharing, bad driver reporting, and a VR/AR game project. In comparison with the mainstream architected agile process that we had been using on similar systems, the PA process has consistently achieved significant speedups in system development while simultaneously reducing defects. PA uses storyboards and prototypes to define both sunny-day and rainy-day scenarios, defines requirements for each use case, and decomposes each use case into a conceptual model-view-controller (MVC) pattern. PA also uses code generation from UML models to rapidly construct a domain-driven microservice architecture at the inception of a project. This microservice architecture is then used to enable prototype code to interact with a live database during requirements definition. PA then uses automatic test case generation from the same UML model. The paper summarizes the overall challenge of software schedule compression, identifies managed parallel development as generally the most powerful but least-practiced strategy for schedule compression, and summarizes the key elements required to support parallelism. It then summarizes the key techniques for scaling up PA, using a previous million-line command and control project as an example. We have used MS-degree graduate student projects to gather productivity data because the university environment has afforded us the opportunity to explore massively parallel development over an extended time period. We are now working with a large company to modernize their main legacy system using the PA methods. The prospective looks good as many of the techniques used (eg, domain models, use case analysis, and MVC decomposition) have been proven to be effective in industry on a wide range of projects for multiple decades. The new techniques (eg, executable domain models, and visual modeling of sprint plans) should also serve to make integration and project management work better on large development efforts. © 2018 John Wiley & Sons, Ltd.
"
10.1016/j.compind.2019.02.014,S0166361518306547,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062460785&origin=inward,Article,SCOPUS_ID:85062460785,scopus,2019-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),specifying a modelling language for pss engineering – a development method and an operational tool,"
                  Although the literature is full of research on the transition of industry towards Product-Service Systems (PSS), the question of how to effectively support PSS engineering is poorly addressed. The compelling need for decision support throughout the various stages of the engineering process is particularly challenging due to the inherent complexity of PSS. In this sense, visualisation and modelling at large have been put forth as promising means for supporting PSS engineering. This paper proposes a method for specifying a modelling language for PSS engineering, putting together PSS domain-specific knowledge and modelling concepts inherited from conceptual modelling and model-based engineering. It relies on a recursive transformation process of the underlying PSS meta-model using knowledge from case studies and the literature. The method has proven to be a practical means for gradual enrichment of the modelling language leading to successful experimentations in the industrial context.
               "
10.1016/j.cpc.2019.01.016,S0010465519300335,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061639309&origin=inward,Article,SCOPUS_ID:85061639309,scopus,2019-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a library for wall-modelled large-eddy simulation based on openfoam technology,"
                  This work presents a feature-rich open-source library for wall-modelled large-eddy simulation (WMLES), which is a turbulence modelling approach that reduces the computational cost of standard (wall-resolved) LES by introducing special treatment of the inner region of turbulent boundary layers (TBLs). The library is based on OpenFOAM and enhances the general-purpose LES solvers provided by this software with state-of-the-art wall modelling capability. The included wall models belong to the class of wall-stress models that account for the under-resolved turbulent structures by predicting and enforcing the correct local value of the wall shear stress. A review of this approach is given, followed by a detailed description of the library, discussing its functionality and extensible design. The included wall-stress models are presented, based on both algebraic and ordinary differential equations. To demonstrate the capabilities of the library, it was used for WMLES of turbulent channel flow and the flow over a backward-facing step (BFS). For each flow, a systematic simulation campaign was performed, in order to find a combination of numerical schemes, grid resolution and wall model type that would yield a good predictive accuracy for both the mean velocity field in the outer layer of the TBLs and the mean wall shear stress. The best result, 
                        ≈
                        1
                        %
                      error in the above quantities, was achieved for channel flow using a mildly dissipative second-order accurate scheme for the convective fluxes applied on an isotropic grid with 
                        27
                        
                        000
                      cells per 
                        
                           
                              δ
                           
                           
                              3
                           
                        
                     -cube, where 
                        δ
                      is the channel half-height. In the case of flow over a BFS, this combination led to the best agreement with experimental data. An algebraic model based on Spalding’s law of the wall was found to perform well for both flows. On the other hand, the tested more complicated models, which incorporate the pressure gradient in the wall shear stress prediction, led to less accurate results.
               
                  Program Summary
                  
                     Program Title: libWallModelledLES
                  
                     Program Files doi: 
                     http://dx.doi.org/10.17632/m8dnsnp4nd.1
                  
                  
                     Licensing provisions: GPLv3
                  
                     Programming language: C++
                  
                     Nature of problem: Large-eddy simulation (LES) is a scale-resolving turbulence modelling approach providing a high level of predictive accuracy. However, LES of high Reynolds number wall-bounded flows is prohibitively computationally expensive due to the need for resolving the inner region of turbulent boundary layers (TBLs) [1]. This inhibits the application of LES to many industrially relevant flows [2] and prompts for the development of novel modelling techniques that would modify the LES approach in a way that allows it to retain its accuracy (at least away from walls) yet significantly lower its computational cost. Solution method: Wall-modelled LES (WMLES) is an approach that is based on complementing LES with special near-wall modelling that allows to leave the inner layer of TBLs unresolved by the computational grid. Many types of wall models have been proposed [1,3], commonly tested within the framework of in-house research codes. Here, an open-source library implementing several wall models is presented. The library is based on OpenFOAM, which is currently the most widely used general-purpose open-source software for computational fluid dynamics. The developed library can be directly applied to both academic and industrial flow cases, leading to a wider adoption of wall modelling and better understanding of its strengths and limitations.
                  [1] J. Larsson, S. Kawai, J. Bodart, and I. Bermejo-Moreno. Large eddy simulation with modeled wall-stress: recent progress and future directions. Mechanical Engineering Reviews, 3(1):1-23, 2016.
                  [2] J. Slotnick, A. Khodadoust, J. Alonso, D. Darmofal, W. Gropp, E. Lurie, D. Mavriplis. CFD vision 2030 study: A path to revolutionary computational aerosciences, Tech. rep., NASA, 2014.
                  [3] S. T. Bose and G. I. Park. Wall-modeled large-eddy simulation for complex turbulent flows. Annual Review of Fluid Mechanics, 50(1):535–561, 2018.
               "
10.1007/s10462-019-09688-6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061188066&origin=inward,Article,SCOPUS_ID:85061188066,scopus,2019-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),arabic named entity recognition via deep co-learning,"
AbstractView references

Named entity recognition (NER) is an important natural language processing (NLP) task with many applications. We tackle the problem of Arabic NER using deep learning based on Arabic word embeddings that capture syntactic and semantic relationships between words. Deep learning has been shown to perform significantly better than other approaches for various NLP tasks including NER. However, deep-learning models also require a significantly large amount of training data, which is highly lacking in the case of the Arabic language. To remedy this, we adopt the semi-supervised co-training approach to the realm of deep learning, which we refer to as deep co-learning. Our deep co-learning approach makes use of a small amount of labeled data, which is augmented with partially labeled data that is automatically generated from Wikipedia. Our approach relies only on word embeddings as features and does not involve any additional feature engineering. Nonetheless, when tested on three different Arabic NER benchmarks, our approach consistently outperforms state-of-the-art Arabic NER approaches, including ones that employ carefully-crafted NLP features. It also consistently outperforms various baselines including purely-supervised deep-learning approaches as well as semi-supervised ones that make use of only unlabeled data such as self-learning and the traditional co-training approach. © 2019, Springer Nature B.V.
"
10.1007/s10270-017-0641-6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85035344860&origin=inward,Article,SCOPUS_ID:85035344860,scopus,2019-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),enabling automated requirements reuse and configuration,"
AbstractView references

A system product line (PL) often has a large number of reusable and configurable requirements, which in practice are organized hierarchically based on the architecture of the PL. However, the current literature lacks approaches that can help practitioners to systematically and automatically develop structured and configuration-ready PL requirements repositories. In the context of product line engineering and model-based engineering, automatic requirements structuring can benefit from models. Such a structured PL requirements repository can greatly facilitate the development of product-specific requirements repository, the product configuration at the requirements level, and the smooth transition to downstream product configuration phases (e.g., at the architecture design phase). In this paper, we propose a methodology with tool support, named as Zen-ReqConfig, to tackle the above challenge. Zen-ReqConfig is built on existing model-based technologies, natural language processing, and similarity measure techniques. It automatically devises a hierarchical structure for a PL requirements repository, automatically identifies variabilities in textual requirements, and facilitates the configuration of products at the requirements level, based on two types of variability modeling techniques [i.e., cardinality-based feature modeling (CBFM) and a UML-based variability modeling methodology (named as SimPL)]. We evaluated Zen-ReqConfig with five case studies. Results show that Zen-ReqConfig can achieve a better performance based on the character-based similarity measure Jaro than the term-based similarity measure Jaccard. With Jaro, Zen-ReqConfig can allocate textual requirements with high precision and recall, both over 95% on average and identify variabilities in textual requirements with high precision (over 97% on average) and recall (over 94% on average). Zen-ReqConfig achieved very good time performance: with less than a second for generating a hierarchical structure and less than 2 s on average for allocating a requirement. When comparing SimPL and CBFM, no practically significant difference was observed, and they both performed well when integrated with Zen-ReqConfig. © 2017, Springer-Verlag GmbH Germany, part of Springer Nature.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85071379733&origin=inward,Conference Paper,SCOPUS_ID:85071379733,scopus,2019-05-18,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"automated design techniques for new nuclear power plant design: knowledge based engineering, generative design and optimisation","
AbstractView references

Ensuring a nuclear power plant is designed on time can be crucial to minimising delays and delivering the project on budget. (ETI, 2018) found that many of today's large nuclear reactors normally start construction without a completed final design which can have a major impact on total capital cost. Quick and efficient design is therefore crucial to designing new power plants. This paper assesses the concept of speeding up the nuclear power plant design process using Automated Design (AD) techniques through branches of Artificial Intelligence (AI) such as Knowledge Based Engineering (KBE) and Generative Design. KBE is “the process of capturing and use (and reuse) of product and process engineering knowledge by automating parts of the design process”. Knowledge Based Engineering can be applied at all stages of the design process, from analysing concept designs, to speeding up detailed design tasks and can be applied to components, products or systems. This wide definition creates many applications of KBE. Many of today's engineering design programs offer some form of KBE capability and there are even dedicated applications for KBE. While KBE has been utilised in a variety of industries such as: automotive, aerospace, chemical process plant, oil and gas, ship and submarine design. It has been highlighted that the technique has had little use within nuclear engineering and design academia. This paper reviews different types of KBE and presents possible directions for research utilising KBE in nuclear. It also gives examples of where KBE can be useful in Nuclear Engineering and Design. Copyright © 2018-2019 by JSME
"
10.1109/COMSNETS.2019.8711054,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85066467087&origin=inward,Conference Paper,SCOPUS_ID:85066467087,scopus,2019-05-09,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a text data augmentation approach for improving the performance of cnn,"
AbstractView references

Deep learning is an emerging research area in the field of machine learning and its fascinating accuracy has attracted many researchers to apply it in various domains, including computer vision and natural language processing. Traditional machine learning approaches require to invest a good amount of time in feature engineering and related tasks. Deep learning, on the other hand, does not require to define features explicitly; instead, it aims to learn different representations from data automatically. However, it needs large enriched corpus to train deep classification models properly. Overfitting is another challenging issue, which needs enriched corpus. In this paper, we propose a data augmentation approach, which combines n-grams and LDA techniques to identify class-specific phrases to enrich the underlying corpus. We have evaluated the performance of the convolutional neural network on both original and augmented corpus, and it is found that the augmented corpus has lower variance and better validation accuracy in comparison to the original corpus. The proposed data augmentation approach seems very useful for the domains with small data corpus to train deep learning models. © 2019 IEEE.
"
10.1145/3290605.3300677,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067615092&origin=inward,Conference Paper,SCOPUS_ID:85067615092,scopus,2019-05-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a is for artificial intelligence the impact of artificial intelligence activities on young children’s perceptions of robots,"
AbstractView references

We developed a novel early childhood artificial intelligence (AI) platform, PopBots, where preschool children train and interact with social robots to learn three AI concepts: knowledge-based systems, supervised machine learning, and generative AI. We evaluated how much children learned by using AI assessments we developed for each activity. The median score on the cumulative assessment was 70% and children understood knowledge-based systems the best. Then, we analyzed the impact of the activities on children’s perceptions of robots. Younger children came to see robots as toys that were smarter than them, but their older counterparts saw them more as people that were not as smart as them. Children who performed worse on the AI assessments believed that robots were like toys that were not as smart as them, however children who did better on the assessments saw robots as people who were smarter than them. We believe early AI education can empower children to understand the AI devices that are increasingly in their lives. © 2019 Association for Computing Machinery.
"
10.1145/3290607.3299059,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067311841&origin=inward,Conference Paper,SCOPUS_ID:85067311841,scopus,2019-05-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),identifying the intersections: user experience + research scientist collaboration in a generative machine learning interface,"
AbstractView references

Creative generative machine learning interfaces are stronger when multiple actors bearing different points of view actively contribute to them. User experience (UX) research and design involvement in the creation of machine learning (ML) models help ML research scientists to more effectively identify human needs that ML models will fulfill. The People and AI Research (PAIR) group within Google developed a novel program method in which UXers are embedded into an ML research group for three months to provide a human-centered perspective on the creation of ML models. The first full-time cohort of UXers were embedded in a team of ML research scientists focused on deep generative models to assist in music composition. Here, we discuss the structure and goals of the program, challenges we faced during execution, and insights gained as a result of the process. We offer practical suggestions for how to foster communication between UX and ML research teams and recommended UX design processes for building creative generative machine learning interfaces. © 2019 Copyright held by the owner/author(s). ACM
"
10.3390/s19102320,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067299742&origin=inward,Article,SCOPUS_ID:85067299742,scopus,2019-05-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),raman distributed temperature sensor with optical dynamic difference compensation and visual localization technology for tunnel fire detection,"
AbstractView references

The field of tunnel fire detection requires a Raman distributed temperature sensor (RDTS) with high-accuracy and visual localization. A novel temperature demodulation method to improve the temperature measurement accuracy of the RDTS systems is presented. This method is based on the optical dynamic difference compensation algorithm, which can eliminate the optical power fluctuation. In addition, the visual localization technology is presented by using the longitudinal lining model (LLM) of a three-dimensional (3D) temperature display, which enhances the engineering application of RDTS in tunnel fire detection. Experimental results indicate that the temperature measurement accuracy is optimized from 7.0 °C to 1.9 °C at the sensing distance of 18.27 km by using the presented method. We provide a solution for temperature field monitoring as well as fire visual localization of the tunnel through RDTS systems. © 2019 by the author. Licensee MDPI, Basel, Switzerland.
"
10.1109/MiSE.2019.00021,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074891966&origin=inward,Conference Paper,SCOPUS_ID:85074891966,scopus,2019-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"domain-specific languages for the design, deployment and manipulation of heterogeneous databases","
AbstractView references

The need for levels of availability and scalability beyond those supported by relational databases has led to the emergence of a new generation of purpose-specific databases grouped under the term NoSQL. In general, NoSQL databases are designed with horizontal scalability as a primary concern and deliver increased availability and fault tolerance at a cost of temporary inconsistency and reduced durability of data. To balance the requirements for data consistency and availability, organisations increasingly migrate towards hybrid data persistence architectures comprising both relational and NoSQL databases. The consensus is that this trend will only become stronger in the future; critical data will continue to be stored in ACID (largely relational) databases while non-critical data will be progressively migrated to high-availability NoSQL databases. Designing and deploying a hybrid data persistence architecture that involves a combination of relational and NoSQL databases is a complex, technically challenging and error-prone task. In this paper we outline a model-based methodology developed in the context of the EC-funded H2020 TYPHON project for designing, developing, querying and evolving such scalable architectures for persistence, analytics and monitoring of large volumes of hybrid (relational, graph-based, document-based, natural language, etc.) data, in a systematic and disciplined manner. © 2019 IEEE.
"
10.1109/MSR.2019.00063,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072343960&origin=inward,Conference Paper,SCOPUS_ID:85072343960,scopus,2019-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated software vulnerability assessment with concept drift,"
AbstractView references

Software Engineering researchers are increasingly using Natural Language Processing (NLP) techniques to automate Software Vulnerabilities (SVs) assessment using the descriptions in public repositories. However, the existing NLP-based approaches suffer from concept drift. This problem is caused by a lack of proper treatment of new (out-of-vocabulary) terms for the evaluation of unseen SVs over time. To perform automated SVs assessment with concept drift using SVs' descriptions, we propose a systematic approach that combines both character and word features. The proposed approach is used to predict seven Vulnerability Characteristics (VCs). The optimal model of each VC is selected using our customized time-based cross-validation method from a list of eight NLP representations and six well-known Machine Learning models. We have used the proposed approach to conduct large-scale experiments on more than 100,000 SVs in the National Vulnerability Database (NVD). The results show that our approach can effectively tackle the concept drift issue of the SVs' descriptions reported from 2000 to 2018 in NVD even without retraining the model. In addition, our approach performs competitively compared to the existing word-only method. We also investigate how to build compact concept-drift-aware models with much fewer features and give some recommendations on the choice of classifiers and NLP representations for SVs assessment. © 2019 IEEE.
"
10.1109/ICSE.2019.00086,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072281754&origin=inward,Conference Paper,SCOPUS_ID:85072281754,scopus,2019-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a novel neural source code representation based on abstract syntax tree,"
AbstractView references

Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classification and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches. © 2019 IEEE.
"
10.1109/ECBIOS.2019.8807891,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072028343&origin=inward,Conference Paper,SCOPUS_ID:85072028343,scopus,2019-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),implementation of an e-learning platform in hybrid clouds,"
AbstractView references

The provision of computing facilities and services has been revolutionized by cloud computing where virtual resources that are scalable are increasingly offered as services over the internet. Starting out as provision of Software as a Service (SaaS), cloud computing has evolved to Platform as a Service (PaaS) whereby scalable, large-scale computing resources such as data centers are offered as services to Infrastructure as a Service (IaaS) where a complete computing infrastructure and computing resources are integrated as a service to clients. The application model of hybrid clouds including public clouds and private clouds is current mainstream of cloud technology with various advantages such as elastic computing resource, load balance, geo-replication (like those from public cloud), safety, privacy and cost effectiveness (like those from private cloud). Based on previous work on cloud-based electronic design automation for integrated circuit design, implementation of an e-learning platform in hybrid clouds using Eucalyptus private cloud and Openshift public cloud is performed in this study. There are eight virtual machines in private cloud and the same number of virtual machines in public cloud. Each virtual machine in the hybrid clouds is running with Linux operating system, Nginx HTTP server, Wordpress CMS, and shared MySQL database. In hybrid clouds outside sixteen virtual machines, an additional Nginx reverse proxy is running for load balancing due to mass visiting request. In this study, e-learning platform is developed with using Wordpress CMS and PHP language to realize various functions such user identification, WYSIWYG course authoring, video audio learning material with text, online quiz, and discussion. The Eucalyptus, Openshift, Nginx, Wordpress, and MySQL utilized in this study are all free open source codes that much improvements of the e-learning platform in hybrid clouds is very possible for future works. © 2019 IEEE.
"
10.1145/3314940,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065797925&origin=inward,Article,SCOPUS_ID:85065797925,scopus,2019-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a sense annotated corpus for all-words urdu word sense disambiguation,"
AbstractView references

Word Sense Disambiguation (WSD) aims to automatically predict the correct sense of a word used in a given context. All human languages exhibit word sense ambiguity, and resolving this ambiguity can be difficult. Standard benchmark resources are required to develop, compare, and evaluate WSD techniques. These are available for many languages, but not for Urdu, despite this being a language with more than 300 million speakers and large volumes of text available digitally. To fill this gap, this study proposes a novel benchmark corpus for the Urdu All-Words WSD task. The corpus contains 5,042 words of Urdu running text in which all ambiguous words (856 instances) are manually tagged with senses from the Urdu Lughat dictionary. A range of baseline WSD models based on n-gram are applied to the corpus, and the best performance (accuracy of 57.71%) is achieved using word 4-gram. The corpus is freely available to the research community to encourage further WSD research in Urdu. © 2019 Association for Computing Machinery.
"
10.1016/j.engappai.2019.02.019,S0952197619300430,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062901276&origin=inward,Article,SCOPUS_ID:85062901276,scopus,2019-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a new hierarchical approach to requirement analysis of problems in automated planning,"
                  The use of Knowledge Engineering (KE) processes to analyze and configure domains in automated planning is becoming more appealing since it was noticed that this issue could make a difference to solve real problems. The contrast between a generic domain independent approach, taken as canonical in AI, and alternative processes that include knowledge engineering – eventually adding specific knowledge – has been discussed by Computer and Engineering communities. A big impact has been noticed mainly in the early phase of requirement analysis when KE approach is normally introduced. Requirement analysis is responsible for carrying out the Knowledge modeling of both problem and work domains, which is a key issue to guide different planner algorithms to come out with efficient solutions. Also, there is the scalability issue that appear in most real problems. To face that, hierarchical methods played an important hole in the history of planning and inspired several solutions since the proposal of NONLIN in the 70’s. Since then, the idea of associating hierarchical relational nets with partial ordered actions has prevailed when large systems were considered. However, there is still a gap between the hierarchical approach and the state of art of requirements analysis to allow features anticipated by KE approach to really appear in the requirements of a planning process. This paper proposes a pathway to solve this gap starting with requirements elicitation represented first in the conventional semi-formal (diagrammatic) language – UML – that is translated to Hierarchical Petri Nets (HPNs) by a new enhanced algorithm. The proposed process was installed in a software tool – developed by one of the authors – that analyzes the performance of the KE planning model: itSIMPLE (Integrated Tools Software Interface for Modeling Planning Environment). This tool was initially designed to use classic Place/Transition nets and an old version of UML (2.1). It is now enhanced to use UML 2.4 and a hierarchical Petri Net extension, also developed by the authors. Realistic examples illustrate the process which is now being applied to larger problems related to the manufacturing of car sequencing domain, one of challenge of ROADEF 2005 (French Operations Research & Decision Support Society). Finally, we consider the possibility to introduce another approach to the KE process by using KAOS (Keep All Object Satisfied) to make the planning design more accurate.
               "
10.1016/j.jss.2019.01.068,S0164121219300214,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061007557&origin=inward,Article,SCOPUS_ID:85061007557,scopus,2019-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),developing a model-driven reengineering approach for migrating pl/sql triggers to java: a practical experience,"
                  Model-driven software engineering (MDE) techniques are not only useful in forward engineering scenarios, but can also be successfully applied to evolve existing systems. RAD (Rapid Application Development) platforms emerged in the nineties, but the success of modern software technologies motivated that a large number of enterprises tackled the migration of their RAD applications, such as Oracle Forms. Our research group has collaborated with a software company in developing a solution to migrate PL/SQL monolithic code on Forms triggers and program units to Java code separated in several tiers.
                  Our research focused on the model-driven reengineering process applied to develop the migration tool for the conversion of PL/SQL code to Java. Legacy code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In this paper, we propose a software process to implement a model-driven re-engineering. This process integrates a TDD-like approach to incrementally develop model transformations with three kinds of validations for the generated code. The implementation and validation of the re-engineering approach are explained in detail, as well as the evaluation of some issues related with the application of MDE.
               "
10.1007/s10270-017-0606-9,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021753284&origin=inward,Article,SCOPUS_ID:85021753284,scopus,2019-04-04,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),stress-testing remote model querying apis for relational and graph-based stores,"
AbstractView references

Recent research in scalable model-driven engineering now allows very large models to be stored and queried. Due to their size, rather than transferring such models over the network in their entirety, it is typically more efficient to access them remotely using networked services (e.g. model repositories, model indexes). Little attention has been paid so far to the nature of these services, and whether they remain responsive with an increasing number of concurrent clients. This paper extends a previous empirical study on the impact of certain key decisions on the scalability of concurrent model queries on two domains, using an Eclipse Connected Data Objects model repository, four configurations of the Hawk model index and a Neo4j-based configuration of the NeoEMF model store. The study evaluates the impact of the network protocol, the API design, the caching layer, the query language and the type of database and analyses the reasons for their varying levels of performance. The design of the API was shown to make a bigger difference compared to the network protocol (HTTP/TCP) used. Where available, the query-specific indexed and derived attributes in Hawk outperformed the comprehensive generic caching in CDO. Finally, the results illustrate the still ongoing evolution of graph databases: two tools using different versions of the same backend had very different performance, with one slower than CDO and the other faster than it. © 2017, The Author(s).
"
10.1109/SYSCON.2019.8836741,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85073163845&origin=inward,Conference Paper,SCOPUS_ID:85073163845,scopus,2019-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a survey on systems engineering methodologies for large multi-energy cyber-physical systems,"
AbstractView references

Today's large distributed energy cyber-physical systems such as power networks with multiple production units are becoming more and more complex due to the increasing share of renewables. They are characterized by long-lived lifecycles that can even be eternal such as electric grids where design and operational phases can overlap. These systems exhibit dynamic configurations and involve several interacting disciplines and manifold stakeholders that can, at any time, take part in the system or leave it. A pressing need has emerged for means to test a large number of scenarios all along the system design, operation and maintenance phases. Doing so requires the ability to model the system behavior and perform simulation on each of its facets using accurate tools for the purpose of automated testing, verification and validation. Existing industrial engineering design practices are becoming obsolete and do not have the means to follow the growing complexity of such multi-disciplinary and multi-stakeholder systems. For this matter, we have explored systems engineering (SE) practices among research communities and tool editors. Design methodologies found in literature are generally based on the functional breakdown of requirements and use general modeling languages for representing the system behavior. They are limited to finite state machines representation with a wide gap regarding the physical aspects that are neglected or at best developed in a separate corner. A survey on existing engineering methodologies is presented in this work. The main common missing aspects of these practices are identified and emphasized. A focus on formal approaches for system design and especially for automatic verification and validation processes is also introduced. Finally, an outlook of the main concepts that we chose to focus on in future works concerning the engineering of multi-energy systems is presented in this paper. © 2019 IEEE.
"
10.1145/3328433.3328435,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072798781&origin=inward,Conference Paper,SCOPUS_ID:85072798781,scopus,2019-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the issue of monorepo and polyrepo in large enterprises,"
AbstractView references

Product and engineering teams' speed of producing highquality results is critical to ensuring enterprise competitiveness. Additionally, one can observe an increase in IT systems complexity driven by the adoption of service-oriented architecture, micro-services, and serverless. Therefore, many large enterprises benefit from a mono-repository for source code management because of the improved team cognition that results from eroding barriers between teams and from influencing enhanced teamwork quality. This paper, first, reviews the characteristics of a multi-repositories structure, a monorepository structure, and a hybrid model. Second, it discusses why some manage source code in a multi-repositories structure, either by choice or because of the organic evolution of large enterprises. Third, it reviews how mono-repositories in large teams, beyond the technical arguments, can drive high efficiency and enhanced product quality through improved team cognition. © 2019 Association for Computing Machinery.
"
10.1145/3314058.3318167,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068780365&origin=inward,Conference Paper,SCOPUS_ID:85068780365,scopus,2019-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),threatzoom: neural network for automated vulnerability mitigation,"
AbstractView references

Increasing the variety and quantity of cyber threats becoming the evident that traditional human-in-loop approaches are no longer sufficient to keep systems safe. To address this momentous moot point, forward-thinking pioneers propose new cyber security strategy using automation to build a more efficient and cheaper defense. Associating large number of unpatchable CVEs (vulnerability descriptions) generated everyday to appropriate CWE (weakness) and CAPEC (attack pattern) can be used to automatically infer the expected impact and corresponding mitigation course of actions for that new CVE. Routinely, adversary exploits a vulnerability to trigger a cyber attack where this vulnerability results from a product or system weakness. Hence, finding a common system weakness associated with a vulnerability within a particular product can help to identifying the software, system, or architecture flaw and the potential attack impacts. This identification leads to prevent, detect, and mitigate those flaws. On the other hand, after recognizing the cause and the effect of a vulnerability, discovering the procedural-oriented description of the attack to create behavioral observables for detection and mitigation is necessary that can be derived from CAPEC and ATTCK. Mapping the CWE to CAPEC and ATTCK which provides pre-TTP and post-TTP respectively where TTP stands for Tactics, Techniques, and Procedures. Having all CWE, CAPEC, and ATTCK in one hand enables us to find corresponding mitigation for each one. On the other hand, extracting threat actions provided by each of these concepts leads to find another type of mitigation coming from Critical Security Controls (CSC). In this proposal, the target is to do mapping all the way from CVE to CAPEC and ATTCk automatically using machine learning, deep learning, and natural language processing and find the appropriate mitigation for each one and then find a proper patch as course of action defense. So far, we have introduced a neural network model which successfully classifies CVE to CWE automatically and as working on a deep learning model to classify CWEs to CAPEC. © 2019 Copyright held by the owner/author(s).
"
10.1016/j.jbi.2019.103133,S1532046419300516,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062369499&origin=inward,Article,SCOPUS_ID:85062369499,scopus,2019-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),incorporating dictionaries into deep neural networks for the chinese clinical named entity recognition,"Clinical named entity recognition aims to identify and classify clinical terms such as diseases, symptoms, treatments, exams, and body parts in electronic health records, which is a fundamental and crucial task for clinical and translational research. In recent years, deep neural networks have achieved significant success in named entity recognition and many other natural language processing tasks. Most of these algorithms are trained end to end, and can automatically learn features from large scale labeled datasets. However, these data-driven methods typically lack the capability of processing rare or unseen entities. Previous statistical methods and feature engineering practice have demonstrated that human knowledge can provide valuable information for handling rare and unseen cases. In this paper, we propose a new model which combines data-driven deep learning approaches and knowledge-driven dictionary approaches. Specifically, we incorporate dictionaries into deep neural networks. In addition, two different architectures that extend the bi-directional long short-term memory neural network and five different feature representation schemes are also proposed to handle the task. Computational results on the CCKS-2017 Task 2 benchmark dataset show that the proposed method achieves the highly competitive performance compared with the state-of-the-art deep learning methods."
10.3837/tiis.2019.03.034,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065581983&origin=inward,Article,SCOPUS_ID:85065581983,scopus,2019-03-31,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an xpdl-based workflow control-structure and data-sequence analyzer,"
AbstractView references

A workflow process (or business process) management system helps to define, execute, monitor and manage workflow models deployed on a workflow-supported enterprise, and the system is compartmentalized into a modeling subsystem and an enacting subsystem, in general. The modeling subsystem’s functionality is to discover and analyze workflow models via a theoretical modeling methodology like ICN, to graphically define them via a graphical representation notation like BPMN, and to systematically deploy those graphically defined models onto the enacting subsystem by transforming into their textual models represented by a standardized workflow process definition language like XPDL. Before deploying those defined workflow models, it is very important to inspect its syntactical correctness as well as its structural properness to minimize the loss of effectiveness and the depreciation of efficiency in managing the corresponding workflow models. In this paper, we are particularly interested in verifying very large-scale and massively parallel workflow models, and so we need a sophisticated analyzer to automatically analyze those specialized and complex styles of workflow models. One of the sophisticated analyzers devised in this paper is able to analyze not only the structural complexity but also the data-sequence complexity, especially. The structural complexity is based upon combinational usages of those control-structure constructs such as subprocesses, exclusive-OR, parallel-AND and iterative-LOOP primitives with preserving matched pairing and proper nesting properties, whereas the data-sequence complexity is based upon combinational usages of those relevant data repositories such as data definition sequences and data use sequences. Through the devised and implemented analyzer in this paper, we are able eventually to achieve the systematic verifications of the syntactical correctness as well as the effective validation of the structural properness on those complicate and large-scale styles of workflow models. As an experimental study, we apply the implemented analyzer to an exemplary large-scale and massively parallel workflow process model, the Large Bank Transaction Workflow Process Model, and show the structuralcomplexity analysis results via a series of operational screens captured from the implemented analyzer. © 2019 KSII.
"
10.1007/s00766-018-0300-7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048494693&origin=inward,Article,SCOPUS_ID:85048494693,scopus,2019-03-13,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an empirical study on the use of i* by non-technical stakeholders: the case of strategic dependency diagrams,"
AbstractView references

Early phases of information systems engineering include the understanding of the enterprise’s context and the construction of models at different levels of decomposition, required to design the system architecture. These time-consuming activities are usually conducted by relatively large teams, composed of groups of non-technical stakeholders playing mostly an informative role (i.e. not involved in documentation and even less in modelling), led by few experienced technical consultants performing most of the documenting and modelling effort. This paper evaluates the ability of non-technical stakeholders to create strategic dependency diagrams written with the i* language in the design of the context model of a system architecture, and find out which difficulties they may encounter and what the quality of the models they build is. A case study involving non-technical stakeholders from 11 organizational areas in an Ecuadorian university held under the supervision and coordination of the two authors acting as consultants. The non-technical stakeholders identified the majority of the dependencies that should appear in the case study’s context model, although they experienced some difficulties in declaring the type of dependency, representing such dependencies graphically and applying the description guidelines provided in the training. Managers were observed to make more mistakes than other more operational roles. From the observations of these results, a set of methodological advices were compiled for their use in future, similar endeavours. It is concluded that non-technical stakeholders can take an active role in the construction of the context model. This conclusion is relevant for both researchers and practitioners involved in technology transfer actions with use of i*. © 2018, Springer-Verlag London Ltd., part of Springer Nature.
"
10.1109/AERO.2019.8742082,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068345246&origin=inward,Conference Paper,SCOPUS_ID:85068345246,scopus,2019-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),artificial intelligence for the early design phases of space missions,"
AbstractView references

Recent introduction of data mining methods has led to a paradigm shift in the way we can analyze space data. This paper demonstrates that Artificial Intelligence (AI), and especially the field of Knowledge Representation and Reasoning (KRR), could also be successfully employed at the start of the space mission life cycle via an Expert System (ES) used as a Design Engineering Assistant (DEA). An ES is an AI-based agent used to solve complex problems in particular fields. There are many examples of ES being successfully implemented in the aeronautical, agricultural, legal or medical fields. Applied to space mission design, and in particular, in the context of concurrent engineering sessions, an ES could serve as a knowledge engine and support the generation of the initial design inputs, provide easy and quick access to previous design decisions or push to explore new design options. Integrated to the User design environment, the DEA could become an active assistant following the design iterations and flagging model inconsistencies. Today, for space missions design, experts apply methods of concurrent engineering and Model-Based System Engineering, relying both on their implicit knowledge (i.e., past experiences, network) and on available explicit knowledge (i.e., past reports, publications, data sheets). The former knowledge type represents still the most significant amount of data, mostly unstructured, non-digital or digital data of various legacy formats. Searching for information through this data is highly time-consuming. A solution is to convert this data into structured data to be stored into a Knowledge Graph (KG) that can be traversed by an inference engine to provide reasoning and deductions on its nodes. Knowledge is extracted from the KG via a User Interface (UI) and a query engine providing reliable and relevant knowledge summaries to the Human experts. The DEA project aims to enhance the productivity of experts by providing them with new insights into a large amount of data accumulated in the field of space mission design. Natural Language Processing (NLP), Machine Learning (ML), Knowledge Management (KM) and Human-Machine Interaction (HMI) methods are leveraged to develop the DEA. Building the knowledge base manually is subjective, time-consuming, laborious and error bound. This is why the knowledge base generation and population rely on Ontology Learning (OL) methods. This OL approach follows a modified model of the Ontology Layer Cake. This paper describes the approach and the parameters used for the qualitative trade-off for the selection of the software to be adopted in the architecture of the ES. The study also displays the first results of the multi-word extraction and highlights the importance of Word Sense Disambiguation for the identification of synonyms in the context. This paper includes the detailed software architecture of both front and back-ends, as well as the tool requirements. Both architectures and requirements were refined after a set of interviews with experts from the European Space Agency. The paper finally presents the preliminary strategy to quantify and mitigate uncertainties within the ES. © 2019 IEEE.
"
10.1109/JSYST.2018.2793665,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041525786&origin=inward,Article,SCOPUS_ID:85041525786,scopus,2019-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),model checking techniques applied to satellite operational mode management,"
AbstractView references

Satellites are nowadays complex systems and can be considered as components of larger mission-level systems of systems. The increasing complexity of space mission objectives is actually complicating the requirement engineering process. It is generally understood that space system engineers should translate system-level requirements (elaborated in natural language) into verifiable models, which can expose the design issues before the satellite manufacturing phase. This paper shows how the verification of complex system requirements can be performed via model checking. More specifically, a methodology is proposed which exploits the flexibility provided by the calculus of communicating systems to model complex system concurrent parts and their mutual interactions for verifying analytically their correctness, completeness, and consistency as prescribed by the system requirements. The proposed methodology is applied to the verification of a real satellite operational mode management specification. An abstraction reduction technique based on the selective mu-calculus logic is used to address the computational issues in model checking. It allows capturing and analyzing the parts of a satellite involved in the verification of a specific set of its system-level properties. © 2018 IEEE
"
10.1016/j.is.2017.11.005,S0306437917300108,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85036526363&origin=inward,Article,SCOPUS_ID:85036526363,scopus,2019-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),medit4cep-cpn: an approach for complex event processing modeling by prioritized colored petri nets,"Complex Event Processing (CEP) is an event-based technology that allows us to process and correlate large data streams in order to promptly detect meaningful events or situations and respond to them appropriately. CEP implementations rely on the so-called Event Processing Languages (EPLs), which are used to implement the specific event types and event patterns to be detected for a particular application domain. To spare domain experts this implementation, the MEdit4CEP approach provides them with a graphical modeling editor for CEP domain, event pattern and action definition. From these graphical models, the editor automatically generates a corresponding Esper EPL code. Nevertheless, the generated code is syntactically but not semantically validated. To address this problem, MEdit4CEP is extended in this paper by Prioritized Colored Petri Net (PCPN) formalism, resulting in the MEdit4CEP-CPN approach. This approach provides both a novel PCPN domain-specific modeling language and a graphical editor. By using model transformations, event pattern models can be automatically transformed into PCPN models, and then into the corresponding PCPN code executable by CPN Tools. In addition, by using PCPNs we can compare the expected output with the actual output and can even conduct a quantitative analysis of the scenarios of interest. To illustrate our approach, we have conducted an air quality level detection case study and we show how this novel approach facilitates the modeling, simulation, analysis and semantic validation of complex event-based systems."
10.1145/3287324.3287400,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064403090&origin=inward,Conference Paper,SCOPUS_ID:85064403090,scopus,2019-02-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),investigating the impact of group size on non-programming exercises in cs education courses full paper,"
AbstractView references

Computer science (CS) courses are taught with increasing emphasis on group work and with non-programming exercises facilitating peer-based learning, computational thinking, and problem solving. However, relatively little work has been done to investigate the interaction of group work and non-programming exercises because collaborative, non-programming work is usually open-ended and requires analysis of unstructured, natural language responses. In this paper, we consider collaborative, non-programming work consisting of online wiki text from 236 groups in nine different CS1 and higher-level courses at a large Midwestern university. Our investigation uses analysis tools with natural language processing (NLP) and statistical analysis components. First, NLP uses IBM Watson Personality Insights to automatically convert students' collaborative wiki text into a Big Five model. This model is useful as a quality metric on group work since Big Five factors such as Openness and Conscientiousness are strongly related to both academic performance and learning. Then, statistical analysis generates regression models on group size and each Big Five trait that make up the factors. Our results show that increasing group size has a significant impact on collaborative, non-programming work in CS1 courses, but not for such work in higher-level courses. Furthermore, increasing group size can have either a positive or negative impact on the Big Five traits. These findings imply the feasibility of using such tools to automatically assess the quality of non-programming group exercises and offer evidence for effective group sizes. © 2019 Association for Computing Machinery.
"
10.1145/3302333.3302350,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123043289&origin=inward,Conference Paper,SCOPUS_ID:85123043289,scopus,2019-02-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),variability modeling of service robots: experiences and challenges,"
AbstractView references

Sensing, planning, controlling, and reasoning, are human-like capabilities that can be articially replicated in an autonomous robot. Such a robot implements data structures and algorithms devised on a large spectrum of theories, from probability theory, mechanics, and control theory to ethology, economy, and cognitive sciences. Software plays a key role in the development of robotic systems, as it is the medium to embody intelligence in the machine. During the last years, however, software development is increasingly becoming the bottleneck of robotic systems engineering due to three factors: (a) the software development is mostly based on community efforts and it is not coordinated by key stakeholders; (b) robotic technologies are characterized by a high variability that makes reuse of software a challenging practice; and (c) robotics developers are usually not specically trained in software engineering. In this paper, we illustrate our experiences from EU, academic, and industrial projects in identifying, modeling, and managing variability in the domain of service robots. We hope to raise awareness for the specic variability challenges in robotics software engineering and to inspire other researchers to advance thiseld. © 2019 ACM.
"
10.1145/3302913,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062334164&origin=inward,Article,SCOPUS_ID:85062334164,scopus,2019-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),handling massive n-gram datasets efficiently,"
AbstractView references

Two fundamental problems concern the handling of large n-gram language models: indexing, that is, compressing the n-grams and associated satellite values without compromising their retrieval speed, and estimation, that is, computing the probability distribution of the n-grams extracted from a large textual source. Performing these two tasks efficiently is vital for several applications in the fields of Information Retrieval, Natural Language Processing, and Machine Learning, such as auto-completion in search engines and machine translation. Regarding the problem of indexing, we describe compressed, exact, and lossless data structures that simultaneously achieve high space reductions and no time degradation with respect to the state-of-the-art solutions and related software packages. In particular, we present a compressed trie data structure in which each word of an n-gram following a context of fixed length k, that is, its preceding k words, is encoded as an integer whose value is proportional to the number of words that follow such context. Since the number of words following a given context is typically very small in natural languages, we lower the space of representation to compression levels that were never achieved before, allowing the indexing of billions of strings. Despite the significant savings in space, our technique introduces a negligible penalty at query time. Specifically, the most space-efficient competitors in the literature, which are both quantized and lossy, do not take less than our trie data structure and are up to 5 times slower. Conversely, our trie is as fast as the fastest competitor but also retains an advantage of up to 65% in absolute space. Regarding the problem of estimation, we present a novel algorithm for estimating modified Kneser-Ney language models that have emerged as the de-facto choice for language modeling in both academia and industry thanks to their relatively low perplexity performance. Estimating such models from large textual sources poses the challenge of devising algorithms that make a parsimonious use of the disk. The state-of-the-art algorithm uses three sorting steps in external memory: we show an improved construction that requires only one sorting step by exploiting the properties of the extracted n-gram strings. With an extensive experimental analysis performed on billions of n-grams, we show an average improvement of 4.5 times on the total runtime of the previous approach. © 2019 Copyright held by the owner/author(s).
"
10.1109/TBCAS.2018.2880012,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056322384&origin=inward,Article,SCOPUS_ID:85056322384,scopus,2019-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),neuromorphic lif row-by-row multiconvolution processor for fpga,"
AbstractView references

Deep Learning algorithms have become state-of-the-art methods for multiple fields, including computer vision, speech recognition, natural language processing, and audio recognition, among others. In image vision, convolutional neural networks (CNN) stand out. This kind of network is expensive in terms of computational resources due to the large number of operations required to process a frame. In recent years, several frame-based chip solutions to deploy CNN for real time have been developed. Despite the good results in power and accuracy given by these solutions, the number of operations is still high, due the complexity of the current network models. However, it is possible to reduce the number of operations using different computer vision techniques other than frame-based, e.g., neuromorphic event-based techniques. There exist several neuromorphic vision sensors whose pixels detect changes in luminosity. Inspired in the leaky integrate-and-fire (LIF) neuron, we propose in this manuscript an event-based field-programmable gate array (FPGA) multiconvolution system. Its main novelty is the combination of a memory arbiter for efficient memory access to allow row-by-row kernel processing. This system is able to convolve 64 filters across multiple kernel sizes, from 1 × 1 to 7 × 7, with latencies of 1.3 \mus and 9.01 \mus, respectively, generating a continuous flow of output events. The proposed architecture will easily fit spike-based CNNs. © 2007-2012 IEEE.
"
10.1016/j.micpro.2018.10.007,S014193311830348X,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056196400&origin=inward,Article,SCOPUS_ID:85056196400,scopus,2019-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),memory management of safety-critical hard real-time systems designed in systemj,"
                  SystemJ is a programming language based on the Globally Asynchronous Locally Synchronous (GALS) Model of Computation (MoC) used to design safety critical hard real-time systems. SystemJ uses the Java programming language as the “host” language, for carrying out data computations, because Java provides clearly defined operational semantics, type and memory safety in the form of the Garbage Collector(GC), which help with formal functional verification. The same GC, which helps in functional verification, makes Worst Case Reaction Time (WCRT)
                        1
                     
                     
                        1
                        Similar to Worst Case Execution Time.
                        
                      analysis challenging. Any WCRT analysis framework for GALS programs needs to consider the operations performed by the host language. It has been shown that the worst case time estimates for garbage collection cycles are in seconds, whereas the program’s WCRT itself is in micro-seconds. These pessimistic estimates render the WCRT analysis framework ineffective. In order to overcome this problem, we develop a compiler assisted memory management technique for applications written in SystemJ. The SystemJ MoC plays the central role in the proposed technique. The SystemJ MoC allows clearly demarcating the state boundaries of the program, which in turn allows us to partition the heap, at compile time, into two distinct areas: (1) the memory area called the permanent heap, which holds objects that are alive throughout the life time of the application, and (2) the memory area used to hold all other objects, called the transient heap. The size of these memory areas are bounded statically. Furthermore, the memory allocation and reclaim procedures are simple load and pointer reset operations, respectively, which are guaranteed to complete within a bounded number of clock-cycles, thereby alleviating the need for large pessimistic WCRT bounds obtained due to the GC. Experimental results also show that the proposed approach is approximately three times faster, in terms of memory allocation times as compared to standard real-time GC approaches.
               "
10.1016/j.cpc.2018.08.016,S0010465518303126,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053819760&origin=inward,Article,SCOPUS_ID:85053819760,scopus,2019-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),gpusphase—a shared memory caching implementation for 2d sph using cuda (new version announcement),"
                  Smoothed particle hydrodynamics (SPH) is a meshless Lagrangian method that has been successfully applied to computational fluid dynamics (CFD), solid mechanics and many other multi-physics problems. gpuSPHASE is a graphics processing unit (GPU) accelerated solver for 2D SPH simulations that is optimized for relatively small numbers of particles but long physical simulation times as required to solve transport phenomena in process engineering applications. The software aims at a low latency execution pipeline with thousands of iterations per second on contemporary GPU accelerator cards, while using symplectic time integration for long term stability of the simulations. For this, a novel caching algorithm for CUDA shared memory is proposed and implemented. gpuSPHASE is validated against SPHERIC test cases and the performance is evaluated and compared to state of the art implementations of the SPH method.
               
                  New version program summary
                  
                     Program Title: gpuSPHASE
                  
                     Program Files doi: 
                     http://dx.doi.org/10.17632/8vpwwh8th5.2
                  
                  
                     Licensing provisions: GPLv3
                  
                     Programming language: CUDA C++
                  
                     Journal reference of previous version: Comput. Phys. Comm., 213 (2017), 165–180
                  
                     Does the new version supersede the previous version?: Yes
                  
                     Reasons for the new version: The software is in active development and thus several new features have been added since the initial release. Those improvements are related to accuracy, maintainability and platform independence.
                  
                     Summary of revisions: 
                     
                        
                           •
                           Build process: The previous distribution was relatively complicated to build due to the external dependencies of the software. This process has been optimized such that CMake is used for all platforms to build the software. For performance reasons several decisions have to be taken at compile time, which can be configured through the CMake configuration. External dependencies are platform independently retrieved and included using CMake features.
                        
                        
                           •
                           Code: While optimized for performance, zero-cost decorators were added to improve the readability of the code. Furthermore, a weak form of type punning was used previously, which was entirely replaced by a custom defined type to eliminate the possibility of undefined behavior.
                        
                        
                           •
                           Floating point precision: As described in the literature and in the original version of the manuscript, 32 bit floating point precision is generally sufficient for SPH simulations of water [1,2]. While not optimized for the opportunistic caching strategy, 64 bit floating point precision has been added. Due to the significant performance loss of consumer grade GPUs for FP64 computations the performance may be considerably worse. FP64 precision can be used to check if FP32 is sufficient and thus detect simulations where FP64 is required. This feature does not influence the already implemented cell relative position calculations that allow for large computational domains without any loss in accuracy [2]. Improvements can be observed e.g. in the Poiseuille flow test case released with DualSPHysics 4.2 [3] with the configuration of 
                                 ν
                                 =
                                 1
                                 
                                    
                                       0
                                    
                                    
                                       −
                                       6
                                    
                                 
                               [m
                                 
                                    
                                    
                                       2
                                    
                                 
                              .s
                                 
                                    
                                    
                                       −
                                       1
                                    
                                 
                              ], 
                                 ρ
                                 =
                                 1000
                               [kg.m
                                 
                                    
                                    
                                       −
                                       3
                                    
                                 
                              ] and a body force of 
                                 
                                    
                                       F
                                    
                                    
                                       x
                                    
                                 
                                 =
                                 1
                                 
                                    
                                       0
                                    
                                    
                                       −
                                       4
                                    
                                 
                               [m.s
                                 
                                    
                                    
                                       −
                                       2
                                    
                                 
                              ], as shown in Fig. 1.
                        
                     
                     
                        
                           •
                           Particle shifting: The diffusion based particle shifting method proposed by Skillen et al. (2013) [4] is implemented. As suggested by Lind et al. (2012) [5], the particle concentration is augmented with the factor 
                                 
                                    
                                       f
                                    
                                    
                                       i
                                       j
                                    
                                 
                               to prevent pairing instability for free surface flows [6]. The improvements are best observed with Poiseuille flow configuration of 
                                 ν
                                 =
                                 1
                                 
                                    
                                       0
                                    
                                    
                                       −
                                       2
                                    
                                 
                               [m
                                 
                                    
                                    
                                       2
                                    
                                 
                              .s
                                 
                                    
                                    
                                       −
                                       1
                                    
                                 
                              ], 
                                 ρ
                                 =
                                 1
                               [kg.m
                                 
                                    
                                    
                                       −
                                       3
                                    
                                 
                              ] and a body force of 
                                 
                                    
                                       F
                                    
                                    
                                       x
                                    
                                 
                                 =
                                 1
                                 
                                    
                                       0
                                    
                                    
                                       −
                                       1
                                    
                                 
                               [m.s
                                 
                                    
                                    
                                       −
                                       2
                                    
                                 
                              ] [7]. This test case does not converge with gpuSPHASE using single or double precision due to the problems described by [8]. Enabling particle shifting allows to execute the simulation until 
                                 t
                                 >
                                 100
                              , as shown in Fig. 2.
                        
                     
                     
                        
                           •
                           Advective-diffusion and heat transfer: Heat transfer is modeled based on the temperature 
                                 T
                               and the thermal conductivity 
                                 k
                               as 
                                 d
                                 T
                                 ∕
                                 d
                                 t
                                 =
                                 
                                    
                                       ρ
                                    
                                    
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       c
                                    
                                    
                                       p
                                    
                                    
                                       −
                                       1
                                    
                                 
                                 k
                                 
                                    
                                       ∇
                                    
                                    
                                       2
                                    
                                 
                                 T
                              . The SPH discretization is implemented according to Alshaer et al. (2017) [9] and successfully verified [10]. The mass transfer of concentration 
                                 C
                               is modeled using the advective-diffusion equation 
                                 d
                                 C
                                 ∕
                                 d
                                 t
                                 =
                                 D
                                 
                                    
                                       ∇
                                    
                                    
                                       2
                                    
                                 
                                 C
                                 −
                                 ∇
                                 ⋅
                                 
                                    (
                                    v
                                    C
                                    )
                                 
                              . The model is implemented according to the discretization of Aristodemo et al. [11] and successfully verified [12].
                        
                     
                     Nature of problem: gpuSPHASE aims at free surface fluid dynamics simulations of long running physical phenomena that must be calculated in the order of real-time. Due to the long physical time and thus the requirement of millions to billions of iterations, the code is restricted to 2D. This restriction allows for optimizations that would not be possible for more generic approaches [13].
                  
                     Solution method: gpuSPHASE is a 2D SPH solver for CUDA capable devices that is optimized for the computation of real-time simulations. Compile time definitions allow to optimize the code for fast execution of the CUDA kernel functions. Symplectic time integration and correction methods enable stable simulations of long physical time problems.
                  
                     References 
                     
                        
                           [1]
                           K. Szewc, Granular Matter 19 (2016) 3.
                        
                        
                           [2]
                           D. Winkler, M. Meister, M. Rezavand, W. Rauch, Computer Physics Communications 213 (2017) 165–180.
                        
                        
                           [3]
                           A. J. C. Crespo, J. M. Domínguez, B. D. Rogers, M. Gómez-Gesteira, S. Longshaw, R. B. Canelas, R. Vacondio, A. Barreiro, O. García-Feal, Computer Physics Communications 187 (2015) 204–216.
                        
                        
                           [4]
                           A. Skillen, S. Lind, P. K. Stansby, B. D. Rogers, Computer Methods in Applied Mechanics and Engineering 265 (2013) 163–173.
                        
                        
                           [5]
                           S. Lind, R. Xu, P. Stansby, B. Rogers, Journal of Computational Physics 231 (4) (2012) 1499–1523.
                        
                        
                           [6]
                           J. J. Monaghan, Journal of Computational Physics 159 (2) (2000) 290–311.
                        
                        
                           [7]
                           S. Adami, X. Y. Hu, N. Adams, Journal of Computational Physics 231 (21) (2012) 7057–7075.
                        
                        
                           [8]
                           M. Basa, N. J. Quinlan, M. Lastiwka, International Journal for Numerical Methods in Fluids 60 (10) (2008) 1127–1148.
                        
                        
                           [9]
                           A. Alshaer, B. Rogers, L. Li, Computational Materials Science 127 (2017) 161–179.
                        
                        
                           [10]
                           M. Rezavand, D. Winkler, W. Rauch, in: Proceedings of the 13th International SPHERIC Workshop (2018) 371–378.
                        
                        
                           [11]
                           F. Aristodemo, I. Federico, P. Veltri, A. Panizzo, Environmental Fluid Mechanics 10 (4) (2010) 451–470.
                        
                        
                           [12]
                           M. Rezavand, D. Winkler, W. Rauch, in: Proceedings of the 12th International SPHERIC Workshop (2017) 426–433.
                        
                        
                           [13]
                           D. Winkler, M. Rezavand, W. Rauch, Computer Physics Communications 225 (2018) 140–148.
                        
                     
                  
               "
10.1109/BIBM.2018.8621282,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062529380&origin=inward,Conference Paper,SCOPUS_ID:85062529380,scopus,2019-01-21,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deep medical entity recognition for swedish and spanish,"
AbstractView references

Clinical texts, although challenging to process, are rich in valuable information, and named entity recognition is an important element in any system designed to extract relevant information from such texts. Recently, improved performance for named entity recognition has been achieved through deep learning methods, and here, a recurrent neural network is evaluated for medical named entity recognition in clinical texts in two different languages, Spanish and Swedish. An important factor for any machine learning model is the input representation, how the features are preprocessed and presented to the model. Therefore, a number of different embeddings derived from large corpora of clinical texts, and several combination strategies for embeddings have been evaluated for this task. Combining a bidirectional LSTM with embeddings derived from words and lemmas gave an improvement in performance with over three points in average F-measure over using only shallow learning methods for both languages, while at the same time reducing the dependency on external resources and feature engineering, showing this approach to be suitable for medical named entity recognition. An average F-measure of 74.87 is obtained for Spanish using lemma embeddings and of 76.04 for Swedish when concatenated lemma and word embeddings are used. © 2018 IEEE.
"
10.1145/3293880.3294102,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061199480&origin=inward,Conference Paper,SCOPUS_ID:85061199480,scopus,2019-01-14,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),formally verified big step semantics out of x86-64 binaries,"
AbstractView references

This paper presents a methodology for generating formally proven equivalence theorems between decompiled x86-64 machine code and big step semantics. These proofs are built on top of two additional contributions. First, a robust and tested formal x86-64 machine model containing small step semantics for 1625 instructions. Second, a decompilation-into-logic methodology supporting both x86-64 assembly and machine code at large scale. This work enables black-box binary verification, i.e., formal verification of a binary where source code is unavailable. As such, it can be applied to safety-critical systems that consist of legacy components, or components whose source code is unavailable due to proprietary reasons. The methodology minimizes the trusted code base by leveraging machine-learned semantics to build a formal machine model. We apply the methodology to several case studies, including binaries that heavily rely on the SSE2 floating-point instruction set, and binaries that are obtained by compiling code that is obtained by inlining assembly into C code. © 2019 ACM.
"
10.1109/NICS.2018.8606875,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061902291&origin=inward,Conference Paper,SCOPUS_ID:85061902291,scopus,2019-01-09,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),integrating grammatical features into cnn model for emotion classification,"
AbstractView references

Emotion analysis is currently an attractive research topic in data mining and natural language processing. Along with the development of technology, people are also gradually evolving to post their emotional thinking on social media. Emotional information is useful for various aspects of business such as advertisement. Automatically classifying user emotions therefore becomes very important. In this paper we firstly formulate this problem under Convolutional Neural Network (CNN) framework. Actually language to express emotions is very diverse that make deep learning techniques such as CNN are ineffective in feature learning when the training data is not large enough. To solve this problem, we propose to use predefined grammatical patterns, which contain potential emotional information, to extract external features and integrate them into the CNN model. Our experiment are performed on two datasets, the ISEAR 1 1 http://affective-sciences.org/home/research/materials-and-onlineresearch/research-material/ (International Survey On Emotion Antecedents And Reactions) dataset and the Vietnamese emotion dataset. The experimental results show that the proposed model is very effective in comparison with previous studies. © 2018 IEEE.
"
10.5220/0007484104020409,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173552302&origin=inward,Conference Paper,SCOPUS_ID:85173552302,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards model-driven verification of robot control code using abstract syntax trees in production systems engineering,"
AbstractView references

Context. In Production Systems, software components are often tightly connected to defined hardware device typeslikerobots. Differenttypesofrobots,evenfromthesamevendor,oftenusevendor-specificprogramming languages. Therefore, the exchange of devices or device types, e.g., during system evolution, is challenging and needs new or adapted control software and repeated verification and validation process steps, even if the softwarebehaviorremainsunchanged. Modelsaimatsupportingtheseverificationandvalidationtasksduring system evolution. Objective. This position paper aims at providing a verification and validation process approach with models for supporting automation systems maintenance and evolution processes. For evaluation purposes, we report on a feasibility study with a focus on two selected robot types in the context of Production Systems Engineering (PSE). Method. We use the Abstract Syntax Tree (AST) concept as a foundation for generatingmodelsasthebasisforhuman-basedverificationandvalidation. BasedontwogeneratedAST variants, related to old and new software control code, human experts can compare the behavior of the expected system to verify and validate the code. Results. First results showed the feasibility of the AST concept to support human-based verification and validation in the context of PSE maintenance projects on a structural level. Conclusion. Althoughthehuman-basedverificationandvalidationprocessisfeasibleandpromisingon a structural level, the complexity of AST for large-scale models needs to be addressed by tool support to overcome complexity levels of the production system and limitations of human-based verification and validation. © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.
"
10.5220/0007310702480255,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173496653&origin=inward,Conference Paper,SCOPUS_ID:85173496653,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using fuml combined with a dsml: an implementation using papyrus uml/sysml modeler,"
AbstractView references

The definition of standards is an efficient way to ensure a consensus on concepts and usages for a given domain. Unified modeling language (UML) and System modeling language (SysML) are standards: UML provides a large set of concepts enabling the specification of software-oriented systems meanwhile SysML specializes UML for system engineering. Both languages have an accurate semantics, this is especially true for UML which has a subset of objects (classes, composite structures, activities and state machines) that can be executed. Executable UML is driven by the following standards: Foundational subset for executable UML (fUML), precise semantics of UML composite structure (PSCS) and precise semantics of UML state machines (PSSM). From the UML based standard contributor standpoint, there is a great temptation to conclude that system architects can use these standards easily to model complex systems, and run simulations. Unfortunately, in practice, this assumption hardly ever holds. Indeed, these standards are built to be generic enough to apply for the largest possible set of systems. This implies that their scopes are wider than what is required by a user to work in its domain. This problem is solved by using and specializing (if required) a subset of these generic languages to match the needs. This paper shows how to reuse the power of UML, SysML, fUML, PSCS and PSSM efficiently, with a customized version of Papyrus dedicated to system architecture design. © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.
"
10.5220/0007397503370344,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173492346&origin=inward,Conference Paper,SCOPUS_ID:85173492346,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a language-oriented approach for the maintenance of megamodel-based complex systems,"
AbstractView references

Model Driven Engineering (MDE) provides the concept of a runtime megamodel to represent the dynamic structure of a given system, to which it is causally connected. A system changes at runtime therefore implies frequent and dynamic changes of its related megamodel. In a previous work we have proposed to automate change management through a runtime megamodel evolution management approach. In such an approach, a megamodel manipulation, a kind of programming in-the-large activity, is considered as a mega-program which is modified throughout Global Operation Models (GOMs). Then we proposed a safe execution of GOMs as the solution for megamodel consistency preserving during evolution. In this work, we propose LAMEME, a domain-specific language for the management and the evolution of megamodels, and its axiomatic semantics. LAMEME gives the possibility to express an evolving megamodel as a mega-program and therefore defines a framework that supports our previously proposed approach. © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.
"
10.2118/198391-MS,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139380657&origin=inward,Conference Paper,SCOPUS_ID:85139380657,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),technological opportunities to increase efficiency of primary recovery of high viscous oil,"
AbstractView references

Oil reservoir of East-Messoyakha field formed of continental deposition environment is characterized by large degree of uncertainty and heterogeneity, observed even at a short distance from the wells. Other complicating factors are high viscous oil (110 centipoise), the presence of an active aquifer and a gas cap. In conditions of high uncertainties, as the geological concept of the field becomes more complex, the involvement of hard-to-recover reserves could be achieved by development of well drilling and well completion. Initially, there was information about the reservoir as a homogeneous reservoir with a thick gas cap and rim of highly viscous oil: the optimal row horizontal well pattern was chosen after calculations. Further studying of the reservoir clarified the idea of reservoir compartmentalization: the reservoir was divided into 3 cyclothems. For draining the maximum oil-saturated thickness from all of cyclothems, it was decided to drill low-angle well through them. With the next update of the conceptual model, theory of multiple-contact reservoir was supposed: development scheme was changed to primary drilling on cyclothem C with parallel study of the overlying objects. Based on the accumulated experience, Workflow was made. By a number of geological and technical criterias the design of well and completion type could be chosen. Besides, development scheme of the neighbor field analog is planned using this workflow. Using workflow allows choosing the optimal development system for a field with large lateral and vertical variability. Implemented development approaches that have changed in the course of a change in the geological concept can serve as experience in the initial assessment of the analogue-field development scheme. Copyright © 2019, Society of Petroleum Engineers.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85120944065&origin=inward,Conference Paper,SCOPUS_ID:85120944065,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),arabic named entity recognition: whatworks and what's next,"
AbstractView references

This paper presents the winning solution to the Arabic Named Entity Recognition challenge run by Topcoder.com. The proposed model integrates various tailored techniques together, including representation learning, feature engineering, sequence labeling, and ensemble learning. The final model achieves a test F1 score of 75:82% on the AQMAR dataset and outperforms baselines by a large margin. Detailed analyses are conducted to reveal both its strengths and limitations. Specifically, we observe that (1) representation learning modules can significantly boost the performance but requires a proper pre-processing and (2) the resulting embedding can be further enhanced with feature engineering due to the limited size of the training data. All implementations and pre-trained models are made public. © ACL 2019.All right reserved.
"
10.1017/9781108654555.004,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099192199&origin=inward,Book Chapter,SCOPUS_ID:85099192199,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),computing education: literature review and voices from the field,"
AbstractView references

Since the creation of the Logo language in 1967, Computing Education (CEd) slowly penetrated educational systems worldwide. Since the mid-2000s, however, there has been a pronounced growth of Science, Technology, Engineering, and Mathematics (STEM) education, with CEd at the forefront. This unprecedented growth has generated competing rationales for the teaching of CEd. It has also generated many model and blueprints for large-scale implementation. In this chapter, based on 14 interviews with leading experts in the field and a literature review, we start by describing four rationales for CEd: labor market needs, computational thinking, computational literacy, and equity of participation. Then we analyze systemic obstacles and possible pathways for sustainable large-scale implementation: equity, broadening participation, scaling, assessment, quality of implementation, curriculum, and teacher development. In particular, the interview data points to the need of scaling “with depth” and allowing for multiple models of implementation to coexist. The data also highlight the need for programs focus on pluralistic views of Computing, exploring new tools, populations, and contexts. © Cambridge University Press 2019.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089685552&origin=inward,Conference Paper,SCOPUS_ID:85089685552,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"a metamodeling approach to teaching conceptual modeling ""at large""","
AbstractView references

In the authors' university there is a challenge, with respect to Conceptual Modeling topics, of bridging the gap between bachelor-level studies and research work. At bachelor-level, Conceptual Modeling is subordinated to Software Engineering topics consequently making extensive use of software design standards. However, at doctoral level or in project-based work, modeling methods must be scientifically framed within wider-scoped paradigms-Design Science, Enterprise Modeling etc. In order to bridge this gap, we developed a teaching artifact to present Conceptual Modeling as a standalone discipline that can produce its own artifacts, driven by requirements in a variety of domains. The teaching artifact is an ""agile modeling method"" that is iteratively implemented by students. The key takeaway revelation for students is that a modeling language is a knowledge schema that can be tailored and migrated for specific purposes just like a database schema, to accommodate an application domain and its modeling requirements. © ISD 2019. All rights reserved.
"
10.2118/197538-ms,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85088200860&origin=inward,Conference Paper,SCOPUS_ID:85088200860,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),digital twins and industry 4.0: videogamers will staff and manage industrial projects in the near future,"
AbstractView references

Executive Summary: The next time you are tempted to scold your son or daughter for spending too many hours playing videogames, think twice: they may be training to be the best workers of the 21st century and even replace your position… Collaborative Work Environments (CWE) combined with Telepresence and Mixed Reality technologies are revolutionizing the design, engineering and building large petrochemical projects. This paper provides an overview of the technologies and describes how the design, implementation and control processes in these projects can be performed more safely and accurately at lower cost. Over three decades ago, businesses experienced a leap in performance, code reusability and maintainability when their information technologies moved from numbered line to object-oriented programming (OOP). We are now poised at the cusp of another quantum change in efficiency as a result of technology. In this new era data travels from “cradle to grave.” From design, construction or assembly, to use, service and final dismantling of refineries and industrial facilities, the physical world of discrete elements will have an accurate digital equivalent. Thanks to powerful computing and Big Data warehousing, complex structures with millions of individual parts can now be tracked and displayed like intelligent LEGO® structures. The vision is that by adopting an open, agree-upon, and already-existing platform for technical communication among different software vendors, huge improvements in efficiency results from enabling a platform or “communal space” that interacts seamlessly with remote presence tools, and a global talent pool working side-by-side with local workers and designers in a virtual fashion. The technology for these real time virtual worlds is already commonplace in online video gaming. Together with the ability to log the activities in this parallel virtual but completely accurate digital world using Big Data, an exhaustive register from initial design through construction, operation and eventual dismantling may be used for detailed analysis, training and automation of preventive maintenance and safety, resulting in lower costs from improved efficiency and better management and enhanced safety. Extending this model and common language of data communication to include various industries, such as engineering, construction, aviation and military operations provides economies of scale in the adoption of an open, global and flexible platform for use by all, but without restricting innovation or compromising security. 3D provides spatial information as in existing CAD systems, adding the time element '4D' incorporates project management and logistics. The next logical step includes cost and supplier information for informed complete life-cycle management of the equipment, project or facility, or '5D'. Uniquely tagging each object is '6D' for real-time RFID asset management and Facilities Management using Big Data enhanced prognosis of maintenance and avoidance of failures, and finally '7D' for artificially intelligent, self-managing and self-repairing systems, on the way to sentient systems of machines. © 2019, Society of Petroleum Engineers
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85086433167&origin=inward,Conference Paper,SCOPUS_ID:85086433167,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sipl: towards a comprehensive development environment for delta-oriented model-based software product lines,"
AbstractView references

Model-based development has become a widely-used approach to implement software, especially for embedded systems. Such systems must often be delivered to customers in a large number of variants. This need is addressed by the concept of a model-based software product line (MBSPL). Implementation methodologies for MBSPLs must be able to specify variability in models and to generate models as instances of an MBSPL. Delta modeling (DM) is a transformational approach to implement variability in the solution space. Products are generated by applying one or several deltas onto a core product. While the basic concepts of DM for MBSPLs are well-understood essential development tasks such as implementing and debugging of deltas of models as well as managing entire networks of deltas are barely supported by appropriate tools. This deficiency is addressed by SiPL, a generic development environment for delta-oriented MBSPLs which can be easily adapted to common modeling languages. SiPL offers a range of generic functionalities for creating deltas, analyzing dependencies and other relations between deltas, refactorings of deltas, and assessing their quality by metrics. © MBEES 2017.All right reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85086252418&origin=inward,Conference Paper,SCOPUS_ID:85086252418,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),segmentation of argumentative texts with contextualisedword representations,"
AbstractView references

The segmentation of argumentative units is an important subtask of argument mining, which is frequently addressed at a coarse granularity, usually assuming argumentative units to be no smaller than sentences. Approaches focusing at the clause-level granularity, typically address the task as sequence labeling at the token level, aiming to classify whether a token begins, is inside, or is outside of an argumentative unit. Most approaches exploit highly engineered, manually constructed features, and algorithms typically used in sequential tagging - such as Conditional Random Fields, while more recent approaches try to exploit manually constructed features in the context of deep neural networks. In this context, we examined to what extend recent advances in sequential labelling allow to reduce the need for highly sophisticated, manually constructed features, and whether limiting features to embeddings, pre-trained on large corpora is a promising approach. Evaluation results suggest the examined models and approaches can exhibit comparable performance, minimising the need for feature engineering. © ACL 2019.All right reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084523516&origin=inward,Article,SCOPUS_ID:85084523516,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),facilitating automated compliance checking of processes in the safety-critical context,"
AbstractView references

In some domains, the applicable safety standards prescribe processrelated requirements. Essential pieces of evidence for compliance assessment with such standard are the compliance justifications of the process plans used to engineer systems. These justifications should show that the process plans are produced in accordance with the prescribed requirements. However, providing the required evidence may be time-consuming and error-prone since safety standards are large, natural language-based documents with hundreds of requirements. Besides, a company may have many safety-critical-related processes to be examined. In this paper, we propose a novel approach that combines process modeling and compliance checking capabilities. Our approach aims at facilitating the analysis required to conclude whether the model of a process plan corresponds to a model with compliant states. Hitherto, our proposed methodology has been evaluated with academic examples that show the potential benefits of its use. © 2019 Universitatsbibliothek TU Berlin.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084297898&origin=inward,Conference Paper,SCOPUS_ID:85084297898,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),neural cross-lingual relation extraction based on bilingual word embedding mapping,"
AbstractView references

Relation extraction (RE) seeks to detect and classify semantic relationships between entities, which provides useful information for many NLP applications. Since the state-of-the-art RE models require large amounts of manually annotated data and language-specific resources to achieve high accuracy, it is very challenging to transfer an RE model of a resource-rich language to a resource-poor language. In this paper, we propose a new approach for cross-lingual RE model transfer based on bilingual word embedding mapping. It projects word embeddings from a target language to a source language, so that a well-trained source-language neural network RE model can be directly applied to the target language. Experiment results show that the proposed approach achieves very good performance for a number of target languages on both in-house and open datasets, using a small bilingual dictionary with only 1K word pairs. © 2019 Association for Computational Linguistics
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083945629&origin=inward,Conference Paper,SCOPUS_ID:85083945629,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),development of an online exam platform for the programming language course: ontology-based approach,"
AbstractView references

As the programming competence has become one of the most fundamental skills for civil engineers, undergraduate students in many universities and colleges are now required to take a programming language course such as C++ or Java. Researchers have pointed out that the best way to expedite the learning process for students taking the programming language course is to ask them to write codes by themselves. However, plagiarism always exists among students' source codes and the course instructor does not have an effective means to verify whether a student truly understand the programming concepts or not. Use of the ontology for the programming language concepts may help resolve the above problem. In fact, the interaction between the instructor and the students is a knowledge exchange process. In the Web Ontology Language (OWL) definition, the ontology consists of three components: individuals, properties, and classes. The relationships of these components and the characteristics of properties make the ontology be able to represent complicated concepts. The ontology also includes a reasoning mechanism which can automatically verify the rationality and consistency of the model and do the classification. This study constructs an ontology model to capture the concepts of the C++ programming language. With the ontology model, similar concepts of the programming language could be combined together by the OWL reasoner to generate a new question. This model can assist in creating an online exam platform that can contain a large number of question templates and generate a question dynamically. Preventing plagiarism can also be achieved by generating a unique set of questions for each student, i.e., changing the parameters or other status of a question. The instructor can concentrate more on the learning process of a student, while students' real learning performances can be evaluated by using this online exam platform. © Nottingham University Press
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85081625568&origin=inward,Conference Paper,SCOPUS_ID:85081625568,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),giant: the 1-billion annotated synthetic bibliographic-reference-string dataset for deep citation parsing,"
AbstractView references

Extracting and parsing reference strings from research articles is a challenging task. State-of-the-art tools like GROBID apply rather simple machine learning models such as conditional random fields (CRF). Recent research has shown a high potential of deep-learning for reference string parsing. The challenge with deep learning is, however, that the training step requires enormous amounts of labelled data - which does not exist for reference string parsing. Creating such a large dataset manually, through human labor, seems hardly feasible. Therefore, we created GIANT. GIANT is a large dataset with 991,411,100 XML labeled reference strings. The strings were automatically created based on 677,000 entries from CrossRef, 1,500 citation styles in the citation-style language, and the citation processor citeproc-js. GIANT can be used to train machine learning models, particularly deep learning models, for citation parsing. While we have not yet tested GIANT for training such models, we hypothesise that the dataset will be able to significantly improve the accuracy of citation parsing. The dataset and code to create it, are freely available at https://github.com/BeelGroup/. Copyright © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85080640117&origin=inward,Conference Paper,SCOPUS_ID:85080640117,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),language models as knowledge bases?,"
AbstractView references

Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA. © 2019 Association for Computational Linguistics
"
10.1017/dsi.2019.325,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079784330&origin=inward,Conference Paper,SCOPUS_ID:85079784330,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),can ethics enhance creative design activity?,"
AbstractView references

This study exhibits that there exists generative ethics in which ethical thinking will allow us to change our perspectives to consider the artifacts' social effects in design, thereby enhancing our activity to generate novel and practical design ideas. As an example of practicing the generative ethics, a case for addressing the increasing requirement to properly introduce “artificial intelligence” (AI) systems in society was considered. We applied the ethical design theory to promote the practice of ethical AI design by engineers. To achieve this, we implemented a creativity support tool that can be used based on the knowledge base of AI ethics. To confirm the validity of the theory and the tool, we conducted user experiments in which the AI research students had to consider the effects of their own research projects with using the tool. We could confirm that the tool actually induced the users to consider social impacts. Some students revealed in response to the questionnaire that the experiment provided them with an opportunity to reconsider their own research theme. In this study, the ethical design theory and tool will be briefly reviewed, and the experiments will be discussed. © 2019 Design Society. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079674216&origin=inward,Conference Paper,SCOPUS_ID:85079674216,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),dynamic modelling and development of a reliable control strategy of organic rankine cycle power systems for waste heat recovery on heavy-duty vehicles,"
AbstractView references

Organic Rankine cycle is a promising technology for waste heat recovery on heavy-duty vehicles. However, the waste heat from internal combustion engines of heavy-duty vehicles is characterized by large fluctuations in mass flow rate and temperature, which may lead to failures of the organic Rankine cycle system. Therefore, it is of crucial importance to develop a suitable control strategy ensuring safe operation and high performance of the organic Rankine cycle system. The objective of the present paper is to develop a simple and robust proportional integral derivative controller for an organic Rankine cycle system recovering the waste heat on a long-haul truck. The exhaust data considers 45 minutes operation of a 450 hp diesel engine of a long-haul truck. A dynamic model was developed in the object-oriented language, Modelica. The dynamic response of the organic Rankine cycle unit for open loop operation was analysed to develop the input and output data for the tuning of the controller. Two different control strategies ensuring the superheat at the expander inlet was developed: i) manipulation of the pump speed and an engine exhaust gas valve, and ii) manipulation of the pump speed and a valve located at the inlet of the expander. The controllability and performances of the controllers were evaluated using the exhaust data. The results indicate that both of the control strategies ensure at all times the superheat at the expander inlet. The second control strategy results in 10.8% more net power output and 12.7% higher thermal efficiency compared with the organic Rankine cycle unit employing the first control strategy. Furthermore, the efficiency of the truck engine is increased by 2.34% for organic Rankine cycle unit employing the first controller and 3.82% employing the second controller. © ECOS 2019 - Proceedings of the 32nd International Conference on Efficiency, Cost, Optimization, Simulation and Environmental Impact of Energy Systems. All rights reserved.
"
10.1287/isre.2019.0843,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079276079&origin=inward,Article,SCOPUS_ID:85079276079,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using technology to persuade: visual representation technologies and consensus seeking in virtual teams,"
AbstractView references

Although Fogg’s ideas of persuasive technologies are widely accepted, few attempts have been made to test his ideas, particularly in a team context. In this article, we (1) theoretically extend Fogg’s ideas by identifying contexts in which virtual teams are more likely to use persuasive technologies; (2) empirically measure technology visualness, a factor that likely makes technologies more or less persuasive; and (3) assess the association between the use of persuasive technologies, judgment shifts, and forecast performance in a real-world virtual team context. We identify visual representation technologies (VRTs) as a class of technologies used by virtual teams to select, transform, and present data in a rich visual format. We propose that such technologies play a persuasive, as well as diagnostic, role in virtual team decisions. Over a three-year period, we examine the daily chat room discussions and decisions of a virtual team that makes smog forecasts with large economic and health consequences. We supplement regression models of field data with an experiment, interviews with team members, and analyses of imagery processing and group cohesion in team language use. Experiment results show that, relative to non-VRTs, the use of a VRT in a forecasting task increases imagery processing. Field data results show that team members increase their use of VRTs during chat room discussions when initial team consensus is low and the environment is more exacting. Greater use of VRTs in team discussions relates to greater shifts in the initial to final consensus forecasts of the team and greater odds of the team shifting its forecast policy to issue a smog alert. Increased use of VRTs is associated with lower forecast bias but is not significantly associated with forecast accuracy. VRT use is also associated with greater imagery processing and increased group cohesion, as shown through language use. Copyright: © 2019 INFORMS
"
10.7559/citarj.v11i2.664,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078819999&origin=inward,Article,SCOPUS_ID:85078819999,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),wandering machines narrativity in generative art,"
AbstractView references

The development of automatic narrative systems has been largely driven by the engineering tendency to anthropomorphize the machine logic so they can ‘tell stories’ similar to how humans do. From the artists’ perspective, however, the experimentation with their media is often more important than the (plausibility of) storytelling, and it often unfolds in non-verbal events that have a potential to generate diverse narratives through the experience of the audience. We discuss the emergence of the creative practices that enrich the poetic repertoire of new media art by playfully utilizing the machine flaws, irregularities, errors and systemic technical imperfections thus revealing the human biases and fallacies entangled with technology. One of the implications of these practices is that if AI research opens up a broader space in which a machine could achieve its own authorial voice, our concept and understanding of the narrative would need to be reconsidered. © 2019, Universidade Catolica Portuguesa. All rights reserved.
"
10.1109/ACCESS.2019.2950884,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078565768&origin=inward,Article,SCOPUS_ID:85078565768,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a model driven reverse engineering framework for generating high level uml models from java source code,"
AbstractView references

Legacy systems are large applications which are significant in performing daily organizational operations and cannot be upgraded easily especially in the absence of architectural and design documentation. Software modernization is an emerging field of software engineering, which transforms the legacy systems into new one according to the specified requirements of stakeholders. It mainly deals with improving the architecture, features, rules and data sources of existing system. It always remained a challenging task to achieve high-level representation of legacy systems. In order to achieve this high-level representation, Reverse Engineering (RE) plays an integral role. The issues of traditional RE are overcome with the help of Model Driven Reverse Engineering (MDRE) such as it generates model-based view of the legacy systems, which is comprehensible and easy to understand for practitioners. MDRE is an active research area but it provides limited tool support to extract and model both structural and behavioral aspects of legacy systems. In this paper, a novel MDRE framework named as 'Source to Model Framework (Src2MoF)'is proposed to generate Unified Modeling Language (UML) structural (class) and behavioral (activity) diagrams from the Java source code. Src2MoF is based on the principles of Model Driven Engineering (MDE), which use models as first-class citizens alleviating the complexity of software systems. The contributions of this paper are as follows; first, an Intermediate Model Discoverer (IMD) is developed using source code parser to get the intermediate representation of the system from the existing Java code. Second, an open source transformation engine named 'UML model generator' is implemented using Java, which takes these intermediate models as input, and produce high-level UML models of the subject legacy system. Finally, the two benchmark case studies are presented to depict the relevance and usability of Src2MoF. © 2013 IEEE.
"
10.1109/ICNTE44896.2019.8946095,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078119490&origin=inward,Conference Paper,SCOPUS_ID:85078119490,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),study of acoustic correlates between prosodic features and emotions in marathi language,"
AbstractView references

For designing intelligible and natural Text-to-Speech systems (TTS) researchers all over the world are studying relation between prosodic features and emotions in various languages. The objective of this research is to study acoustic correlates between prosodic features including duration, F0 and intensity for vowels and various emotions in Marathi language. Also, these prosodic features are analyzed for vowels in words non-final and final syllables. Pause durations occurring between words for various emotions are also studied. Marathi sentences spoken by a female speaker are recorded in five emotion styles: anger, fear, happy, sadness and neutral. For every sentence vowel mean duration, mean F0 and mean intensity are calculated. Long pauses, mid-pauses and short pauses for all the five emotions are analyzed. The ANOVA analysis for F0 and intensity with p<0.001 indicate that these parameters can be used as cue for various emotion styles for Marathi language. Short vowel duration and large number of short pauses are observed for angry emotion whereas long vowel duration and short, mid and long pauses are observed equally for happy emotion. Above observations related to prosodic features and pauses would be useful to model Marathi TTS system. © 2019 IEEE.
"
10.1007/978-3-030-35653-8_13,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85077067138&origin=inward,Conference Paper,SCOPUS_ID:85077067138,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ocl2psql: an ocl-to-sql code-generator for model-driven engineering,"
AbstractView references

The Object Constraint Language (OCL) is a textual, declarative language typically used as part of the UML standard for specifying constraints and queries on models. Several attempts have been proposed in the past for translating OCL expressions into SQL queries in the context of Model Driven Engineering (MDE). To cope with OCL expressions that include iterators, previous attempts resorted to imperative features (loops, cursors) of SQL, with the consequent loss of efficiency. In this paper, we define (and implement) a novel mapping from OCL to SQL that covers (possibly nested) iterators, without resorting to imperative, non-declarative features of SQL. We show with a preliminary benchmark that our mapping generates SQL queries that can be efficiently executed on mid- and large-size SQL databases. © 2019, Springer Nature Switzerland AG.
"
10.1007/978-3-030-35699-6_26,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076924985&origin=inward,Conference Paper,SCOPUS_ID:85076924985,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),neural semantic parsing with anonymization for command understanding in general-purpose service robots,"
AbstractView references

Service robots are envisioned to undertake a wide range of tasks at the request of users. Semantic parsing is one way to convert natural language commands given to these robots into executable representations. Methods for creating semantic parsers, however, rely either on large amounts of data or on engineered lexical features and parsing rules, which has limited their application in robotics. To address this challenge, we propose an approach that leverages neural semantic parsing methods in combination with contextual word embeddings to enable the training of a semantic parser with little data and without domain specific parser engineering. Key to our approach is the use of an anonymized target representation which is more easily learned by the parser. In most cases, this simplified representation can trivially be transformed into an executable format, and in others the parse can be completed through further interaction with the user. We evaluate this approach in the context of the RoboCup@Home General Purpose Service Robot task, where we have collected a corpus of paraphrased versions of commands from the standardized command generator. Our results show that neural semantic parsers can predict the logical form of unseen commands with 89% accuracy. We release our data and the details of our models to encourage further development from the RoboCup and service robotics communities. © 2019, Springer Nature Switzerland AG.
"
10.1007/978-3-030-36027-6_9,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076693061&origin=inward,Conference Paper,SCOPUS_ID:85076693061,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards an architecture proposal for federation of distributed des simulators,"
AbstractView references

The simulation of large and complex Discrete Event Systems (DESs) increasingly imposes more demanding and urgent requirements on two aspects accepted as critical: (1) Intensive use of models of the simulated system that can be exploited in all phases of its life cycle where simulation can be used, and methodologies for these purposes; (2) Adaptation of simulation techniques to HPC infrastructures, as a method to improve simulation efficiency and to have scalable simulation environments. This paper proposes a Model Driven Engineering approach (MDE) based on Petri Nets (PNs) as formal model. This approach proposes a domain specific language based on modular PNs from which efficient distributed simulation code is generated in an automatic way. The distributed simulator is constructed over generic simulation engines of PNs, each one containing a data structure representing a piece of net and its simulation state. The simulation engine is called simbot and versions of it are available for different platforms. The proposed architecture allows, in an efficient way, a dynamic load balancing of the simulation work because the moving of PN pieces can be realized by moving a small number of integers representing the subnet and its state. © Springer Nature Switzerland AG 2019.
"
10.1007/978-3-030-33716-2_1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076192462&origin=inward,Conference Paper,SCOPUS_ID:85076192462,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),gdpr modelling for log-based compliance checking,"
AbstractView references

Since the entry into force of the General Data Protection Regulation (GDPR), public and private organizations face unprecedented challenges to ensure compliance with new data protection rules. To help its implementation, academics and technologists proposed innovative solutions leading to what is known today as privacy engineering. Among the main goals of these solutions are to enable compliant data processing by controllers and to increase trust in compliance by data subjects. While data protection by design (Article 25 of GDPR) constitutes a keystone of the regulation, many legacy systems are not designed and implemented with this concept in mind, but still process large quantities of personal data. Consequently, there is a need for “after design” ways to check compliance and remediate to data protection issues. In this paper, we propose to monitor and check the compliance of legacy systems through their logs. In order to make it possible, we modelled a core subset of the GDPR in the Prolog language. The approach we followed produced an operational model of the GDPR which eases the interactions with standard operational models of Information Technology (IT) systems. Different dimensions required to properly address data protection obligations have been covered, and in particular time-related properties such as retention time. The logic-based GDPR model has also been kept as close as possible to the legal wording to allow a Data Protection Officer to explore the model in case of need. Finally, even if we don’t have a completed tool yet, we created a proof-of-concept framework to use the GDPR model to detect data protection compliance violations by monitoring the IT system logs. © IFIP International Federation for Information Processing 2019.
"
10.5277/e-Inf190106,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075466565&origin=inward,Article,SCOPUS_ID:85075466565,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),measuring goal-oriented requirements language actor stability,"
AbstractView references

Background: Goal models describe interests, preferences, intentions, desired goals and strategies of intervening stakeholders during the early requirements engineering stage. When capturing the requirements of real-world systems such as socio-technical systems, the produced goal models evolve quickly to become large and complex. Hence, gaining a sufficient level of understanding of such goal models, to perform maintenance tasks, becomes more challenging. Metric-based approaches have shown good potential in improving software designs and making them more understandable and easier to maintain. Aim: In this paper, we propose a novel metric to measure GRL (Goal-oriented Requirements Language) ""actor stability"" that provides a quantitative indicator of the actor maintainability. Method: We first, validate the proposed metric theoretically then empirically using a case study of a GRL model describing the fostering of the relationship between the university and its alumni. Results: The proposed actor stability metric is found to have significant negative correlation with the maintenance effort of GRL models. Conclusions: Our results show that the proposed metric is a good indicator of GRL actors' stability. © 2019 Wroclaw University of Science and Technology. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074864658&origin=inward,Conference Paper,SCOPUS_ID:85074864658,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),large-scale traffic simulation for smart city planning with mars,"
AbstractView references

Understanding individual mobility in larger cities is an important success factor for future smart cities. Related simulation scenarios incorporate enormous numbers of agents, with the disadvantage of long run times. In order to provide large-scale and multimodal traffic simulations, we developed MARS V3. Adapting the Modeling and Simulation as a Service (MSaaS) paradigm, a seamless workflow can be provided to the modeling community. An integrated domain-specific language allows model descriptions without a technical overhead. For this study, selected parts of an individual-based traffic model of the City of Hamburg, Germany, were taken as an example. The entire workflow from model development, open data integration, simulation, and result analysis will be described and evaluated. Performance was measured for local and cloud-based simulation execution for up to one million agents. First results show that this concept can be utilized for building decision support systems for smart cities in the near future. © 2019 Society for Modeling & Simulation International (SCS).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074450301&origin=inward,Conference Paper,SCOPUS_ID:85074450301,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),glimpse to the future. technological trends in the shipbuilding cad world,"
AbstractView references

The ship design has varied extensively throughout history. Over the past forty years, CAD/CAM/CIM, from now they will be referred just as CAD, tools have progressed from mainframe computers to tablets, from independent programs to fully-integrated software packages, and from large shipyards to companies of all sizes. We are living a continuous and fast technology evolution, maybe this evolution goes faster than our capacity to assimilate what we can do with it, but the potential is clear and the future will be for those who identifies the right technology with the right application. In the information era, we are literally swimming in an ocean of structured and not structured data and thanks to the evolution in the Telecommunications Technologies, all that information can be used from everywhere. But information means nothing without the capability to analyse, extract conclusions and learn from it, which is way the technologies like treatment of Big Data and the Artificial Intelligence are crucial. Imagine how these technologies shall allow to engage the design of a part or any concept by applying rules which will facilitate the design significantly, how the integration of the validation of the structural models by the Classification Societies will be linked directly by cloud applications. Imagine all the benefits of this two simple examples that can be implemented thanks to the potential of these technologies. The way we work with CAD tools is also changing thanks to the ubiquitous access to the information and the different hardware available to explode that information: Augmented Reality, Virtual Reality or Mixed Reality. But not only the way we work, but also the way we interact with CAD tools is changing, with technologies like natural language processes that allows to have a direct conversation with the applications. The concepts that are absolutely clear from now to the future in shipbuilding is the use of Data Centric model and the concept of Digital Twin, a real and effective synchronization between what we design, what we construct, by covering the complete life cycle of the product thanks to technologies like IoT and RFID. Nowadays it is unimaginable to work without using CAD in shipbuilding: ease of design with design rules embedded, speed of design, use and reuse of information, etc. It is expected that in the future CAD tools will advance further and allow greater information management through new improvements. There are several scenarios of improvements explained in this paper for the next years. Some of these improvements may seem unrealistic in the short term, but reality often exceeds expectations in any field, and probably more in technology. © 2019: The Royal Institution of Naval Architects.
"
10.5220/0008559104620469,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074281514&origin=inward,Conference Paper,SCOPUS_ID:85074281514,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a domain specific language for web-based gis,"
AbstractView references

Geographic information systems (GIS) manage entities with a spatial component (typically, in the form of a point, line, or polygon defined according to a known geographic coordinate system), and provide specific operations to process them, and specific interfaces to visualize them. Although different GIS may provide a completely different set of functionalities, they all share a common set of concepts, architecture, and technologies. Therefore, in the development of GIS there are typically large parts of the system that can be very repetitive. In this paper, we present a domain-specific language to develop GIS from high-level declarative specifications. We present a metamodel for web-based GIS, and a domain-specific language based on that metamodel. We also present a usage example that shows how the language would be used in a real scenario. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.
"
10.1117/12.2513222,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85073910523&origin=inward,Conference Paper,SCOPUS_ID:85073910523,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),analog-to-digital conversion and model based engineering,"
AbstractView references

Pervasive and ubiquitous computing is now universal. We now have intelligent and interconnected kitchens. Cars are interconnected networks on wheels. Smart highways will be here before the flying car. In the near future, the Internet-of-Things (IoT) will provide inter-connectivity for all things electronic. In 1971, the first 4 bit microprocessor was introduced. Nearly 50 years later, the microprocessor is driving global, world-wide connectivity to everything. More fundamental than pervasive and ubiquitous computing is the underlying technology of digital communications. Shannon proved in 1948, that digital communications provide for an arbitrarily small number of errors regardless of the distance between sender and receiver. Once data is in a digital form, it can be transmitted or copied an arbitrarily large number of times without error. Digital is a universal language which can transmit and store information for millennia to come. Digital is defined as quantized, discrete time. In this paper, we review sampling theory to convert analog to discrete time and discrete time to digital. We will illustrate the importance of understanding analog-to-digital conversion for model based engineering. © 2019 SPIE.
"
10.1007/978-3-030-30985-5_17,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85073693773&origin=inward,Book Chapter,SCOPUS_ID:85073693773,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),quod: an nlp tool to improve the quality of business process descriptions,"
AbstractView references

[Context and Motivation] In real-world organisations, business processes (BPs) are often described by means of natural language (NL) documents. Indeed, although semi-formal graphical notations exist to model BPs, most of the legacy process knowledge—when not tacit—is still conveyed through textual procedures or operational manuals, in which the BPs are specified. This is particularly true for public administrations (PAs), in which a large variety of BPs exist (e.g., definition of tenders, front-desk support) that have to be understood and put into practice by civil servants. [Question/problem] Incorrect understanding of the BP descriptions in PAs may cause delays in the delivery of services to citizens, or, in some cases, incorrect execution of the BPs. [Principal idea/results] In this paper, we present the development of an NLP-based tool named QuOD (Quality Analyser for Official Documents), oriented to detect linguistic defects in BP descriptions and to provide recommendations for improvements. [Contribution] QuOD is the first tool that addresses the problem of identifying NL defects in BP descriptions of PAs. The tool is available online at http://narwhal.it/quod/index.html. © 2019, Springer Nature Switzerland AG.
"
10.1007/978-3-030-30985-5_29,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85073691766&origin=inward,Book Chapter,SCOPUS_ID:85073691766,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),modelling of railway signalling system requirements by controlled natural languages: a case study,"
AbstractView references

The railway sector has been a source of inspiration for generations of researchers challenged to develop models and tools to analyze safety and reliability. Threats were coming mainly from within, due to occasionally faults in hardware components. With the advent of smart trains, the railway industry is venturing into cybersecurity and the railway sector will become more and more compelled to protect assets from threats against information & communication technology. We discuss this revolution at large, while speculating that instruments developed for security requirements engineering can then come in support of in the railway sector. And we explore the use of one of them: the Controlled Natural Language for Data Sharing Agreement (CNL4DSA). We use it to formalize a few exemplifying signal management system requirements. Since CNL4DSA enables the automatic generation of enforceable access control policies, our exercise is preparatory to implementing the security-by design principle in railway signalling management engineering. © 2019, Springer Nature Switzerland AG.
"
10.1007/978-3-030-29736-7_25,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072984165&origin=inward,Conference Paper,SCOPUS_ID:85072984165,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automatic text difficulty estimation using embeddings and neural networks,"
AbstractView references

Text difficulty, also called reading difficulty, refers to the complexity of texts on a language level. For many educational applications, such as learning resource recommendation systems, the text difficulty of text is highly relevant information. However, manual annotation of text difficulty is very expensive and not feasible for large collections of texts. For this reason, many approaches to automatic text difficulty estimation have been proposed in the past. All text difficulty estimation models published thus far have one thing in common: they rely on manually engineered feature sets. This is problematic as features are tailored to a specific type of text and do not generalize well to other types and languages. To alleviate this problem we propose a novel approach using neural networks and embeddings to the task of text difficulty classification. Our approach distinguishes between 5 reading levels which correspond to non-overlapping age groups ranging from ages 7 to 16. It performs comparably to existing state-of-the-art approaches in terms of accuracy and Pearson correlation coefficient while being easier and cheaper to adapt to new types of text. © 2019, Springer Nature Switzerland AG.
"
10.12989/sss.2019.23.4.393,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072637324&origin=inward,Article,SCOPUS_ID:85072637324,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),faults detection and identification for gas turbine using dnn and llm,"
AbstractView references

Applying more features gives us better accuracy in modeling; however, increasing the inputs causes the curse of dimensions. In this paper, a new structure has been proposed for fault detecting and identifying (FDI) of high-dimensional systems. This structure consist of two structure. The first part includes Auto-Encoders (AE) as Deep Neural Networks (DNNs) to produce feature engineering process and summarize the features. The second part consists of the Local Model Networks (LMNs) with LOcally LInear MOdel Tree (LOLIMOT) algorithm to model outputs (multiple models). The fault detection is based on these multiple models. Hence the residuals generated by comparing the system output and multiple models have been used to alarm the faults. To show the effectiveness of the proposed structure, it is tested on single-shaft industrial gas turbine prototype model. Finally, a brief comparison between the simulated results and several related works is presented and the well performance of the proposed structure has been illustrated. Copyright © 2019 Techno-Press, Ltd.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85071474120&origin=inward,Conference Paper,SCOPUS_ID:85071474120,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an integer programming based two-stage decomposition method (iptdm) for satellite tt&amp;c resource scheduling problem,"
AbstractView references

Satellite TT&C (Telemetry, Tracking and Control) resource scheduling problem (STRSP) is to allocate TT&C resources to satellites, and determine when to give support in the limited visible time interval. The Satellite Control Networks (SCN) of China provides TT&C support for more than one hundred satellites. With the increasing number of satellites, STRSP is of great practical importance, but is computationally challenging. In this research, we consider a STRSP that includes satellites of three orbital heights, namely Low Earth Orbit (LEO), Medium Earth Orbit (MEO) and Geostationary Earth Orbit (GEO). In order to promote the problem-solving speed, an integer programming based two-stage decomposition method (IPTDM) is proposed. According to the characteristics of LEO, the tasks of LEO are firstly scheduled through a 0-1 integer programming model. Secondly, the time interval of each antenna for MEO and GEO which is occupied in LEO is cut, then the final solution is obtained by solving integer programming for MEO and GEO. As shown in the experimental results from five small instances and three large instances, the two-stage method is effective to solve the STRSP which contains 80 satellites, while the unified integer programming just solves the problem with 20 satellites. © 2019 Proceedings of the 8th International Conference on Logistics and Systems Engineering 2018. All rights reserved.
"
10.18293/SEKE2019-041,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85071365418&origin=inward,Conference Paper,SCOPUS_ID:85071365418,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),pat approach to architecture behavioural verification,"
AbstractView references

Software architecture design plays a vital role in software development, as it gives an overview of how the software system should be constructed and executed at runtime. The verification of software architecture design is hence important but it is an error-prone task that heavily relies on knowledge and experience of the software architect, especially for a large software system that its behaviour is complex. Automated verification can be a solution to this problem, however, the specification language must be expressive enough to describe the behaviour of different design entities. This paper presents an enhancement of an architecture description language supported by PAT. The enhancement aims to improve the expressiveness of the language, in order to support the automated behaviour verification of software architecture design. With this enhancement, different behaviour of specific component and connector can be thoroughly checked and traced. The implementation of this enhancement is presented to demonstrate how the standard model checking engine such as PAT can be extended to support an architecture description language. We evaluated our approach with a case study and the result is presented. © 2019 Knowledge Systems Institute Graduate School. All rights reserved.
"
10.18293/SEKE2019-050,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85071359227&origin=inward,Conference Paper,SCOPUS_ID:85071359227,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards machine learning for learnability of mdd tools,"
AbstractView references

Learning how to build software systems using new tools can be a daunting task to anyone new to the job. This is especially true of tools that provide a large number of functionalities and views on the system under development, such as IDES for Model-Driven Development (MDD). Applying Machine Learning (ML) techniques can help in this state of affairs by pointing out to appropriate next actions to rookie or even intermediate developers. AutoFOCUS3 (AF3) is a mature MDD tool we are building in-house and for which we provide regular tutorials to new users. These users come from both the academia (e.g, students/professors) and the industry (e.g. managers/software engineers). Nonetheless, AF3 remains a complex tool and we have found there is a need to speedup the learning curve of the tool for students that attend our tutorials - or alternatively and more importantly for others that simply download the tool and attempt using it without human supervision. In this paper, we describe a machine learning-based recommendation system named MAGNET for aiding beginner and intermediate users of AF3 in learning the tool. We describe how we have gathered data and trained an ML model to suggest new commands, how a recommender system was integrated in the AF3, experiments we have run thus far, and the future directions of our work. © 2019 Knowledge Systems Institute Graduate School. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85070942223&origin=inward,Conference Paper,SCOPUS_ID:85070942223,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),management of engineering and safety knowledge along reactor lifetime,"
AbstractView references

Complexity, safety relevance and long life time make nuclear power plants heavily dependent on information and skilled people. The knowledge accumulated during design, construction and operation is a critical asset that must be preserved for continuous improvement and future modifications. This paper proposes an approach that supports the safety and reliability of nuclear power plants based on cross-disciplinary methods and tools to create, maintain and exploit a structured knowledge basis for long-term operation, maintenance and up-grade. It proposes advanced modelling and justification methods to capture design information that currently often remains incompletely documented. In particular, it aims at making safety properties, dependencies and reasons behind design solutions understandable to all stakeholders. As a step towards its vision of model-based knowledge repositories, it extends traditional system descriptions with assumptions, operational constraints, design rationales and safety justifications. The knowledge base also links design solutions to requirements in regulations and standards. In addition, advanced plant models enable use of computer tools e.g. for automated dependency analysis and simulation. Focusing on design models for plant operation, the approach works in a multidisciplinary fashion and looks at reactor islands as socio-technical systems. Within the context of knowledge management and organizational learning, it integrates process and plant engineering, instrumentation and control system design and human factors engineering. On the basis of the current state-of-the-art and industrial needs, a common framework is defined. The general principles are refined towards more formal modelling languages and software tools. Finally, the framework and its implementations are tested in industrial case studies, and the experiences are used to refine the outputs of the project. © 2018 Westinghouse Electric Company LLC All Rights Reserved
"
10.3389/fbioe.2019.00156,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85069982905&origin=inward,Article,SCOPUS_ID:85069982905,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),semnet: using local features to navigate the biomedical concept graph,"
AbstractView references

Literature-Based Discovery (LBD) aims to connect scientists across silos by assembling models of the literature to reveal previously hidden connections. Unfortunately, LBD systems have been unable to achieve user adoption on a large scale. This work develops opens source software in Python to convert a database of semantic predications of all of PubMed's 27.9 million indexed abstracts into a semantic inference network and biomedical concept graph in Neo4j. The developed software, called SemNet, queries a modified version of the publicly available SemMedDB and computes feature vectors on source-target pairs. Each unique United Medical Language System (UMLS) concept is represented as a node and each predication as an edge. Each node is assigned one of 132 node labels (e.g., Amino Acid, Peptide, or Protein (AAPP); Gene or Genome (GG); etc.) and each edge is labeled with one of 58 predications (e.g. treats, causes, inhibits, etc.). SemNet computes a single feature value for each metapath, or sequence of node types, between a source node and user-specified target node(s). Several different types of metapath-based features (count, degree weighted path count, and HeteSim metric) are computed and vectorized. SemNet employs an unsupervised learning algorithm for rank aggregation (ULARA) to rank identified source nodes that are most relevant to the user-specified target nodes(s). Statistical analysis of correlation among identified source nodes or resultant literature network features are used to identify patterns that can guide future research. Analysis of high residual nodes is used to compare and contrast SemNet rankings between different targets of interest. An example SemNet use case is presented to assess ""the differential impact of smoking on cognition in males and females"" using the following target nodes: nicotine, learning, memory, tetrahydrocannabinol (THC), cigarette smoke, X chromosome, and Y chromosome. Detailed rankings are discussed. Overall results suggest a hypothesis where smoking negatively impacts cognition to a greater extent in females, but smoking has stronger cardiovascular impacts inmales. In summary, SemNet provides an adoptable method for efficient LBD of PubMed that extends beyond omics-only relationships to true multi-scalar connections that can provide actionable insight for predictive medicine, research prioritization, and clinical care. © 2019 Sedler and Mitchell.
"
10.1007/978-3-030-23522-2_8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85069682890&origin=inward,Conference Paper,SCOPUS_ID:85069682890,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design research of new energy imagery transformation based on verb semantics,"
AbstractView references

At present, due to large-scale development of non-renewable energy, its reserves are getting smaller and smaller and the use of fossil energy causes serious pollution. In order to cope with the dual pressure of energy and environmental protection, the products powered by new energy will become future directions. It is an urgent problem to be studied that how to apply new energy imagery to product design. The research is to perceive the new energy based on the study of verb semantics and transform its imagery into a perceptible design model. It is carried out in three parts: The extraction of “new energy” verb and the establishment of the word bank, the establishment of a gene bank and the extraction and transformation of new energy design elements. Perceived imagery from the perspective of the verb which analogies to the study of adjectives. Combined with card classification, correlation analysis, and other methods, a new energy word library and image library of verbs are formed and finally transformed into a perception and experience model reflecting new energy. The use of verb thinking can help designers create the immersive design to better convey the design in a variety of ways while enabling users to mobilize multiple senses to perceive new energy products, increase usability and experience, and arouse users’ emotional resonance. © Springer Nature Switzerland AG 2019.
"
10.1007/978-3-030-20618-5_4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85069161639&origin=inward,Conference Paper,SCOPUS_ID:85069161639,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),logic based look-ahead for the execution of multi-perspective declarative processes,"
AbstractView references

In declarative process models all the activities which do not violate a constraint of the process model can be executed. Consequently, the number of viable paths is large. In turn, when considering multiple perspectives during execution, i.e., constraints on resources and data values, it may happen that the execution of activities or the change of data values may result in the non-executability of crucial activities. Execution engines for single-perspective declarative process models have been extensively discussed in research where, among others look-ahead functionality has been investigated. Execution approaches for multi-perspective declarative models that involve constraints on data and resources, however, are less mature. In this paper, we introduce a logic based look-ahead approach for the execution of multi-perspective declarative processes. We use the look-ahead for simulating a fixed number of execution steps with regard to the existing trace and the choice of the next step. The look-ahead allows for estimating all consequences and effects of certain decisions at any time of process execution. We develop an algorithm for trace generation and checking traces using the logic language Alloy. We extensively evaluate our approach by means of a practical example and give some advice for further optimizations. © 2019, Springer Nature Switzerland AG.
"
10.1007/978-3-030-22888-0_4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068253503&origin=inward,Conference Paper,SCOPUS_ID:85068253503,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated support to capture creative requirements via requirements reuse,"
AbstractView references

Increasingly competitive software industry, where multiple systems serve the same application domain and compete for customers, favors software with creative features. To promote software creativity, research has proposed multi-day workshops with experienced facilitators, and semi-automated tools to provide a limited support for creative thinking. Such approach is either time-consuming and demands substantial involvement from analysts with creative abilities, or useful only for existing large-scale software with a rich issue tracking system. In this paper, we present a novel framework, useful for both new and existing systems, providing an end-to-end automation to support creativity. In particular, the framework reuses freely available requirements for similar software, leverages state-of-the-art natural language processing and machine learning techniques, and generates candidate creative requirements. We apply the framework on three application domains: Antivirus, Web Browser, and File Sharing, and further report a human subject evaluation. The results demonstrate our framework’s ability to generate creative features and provoke innovative thinking among developers with various experience levels. © 2019, Springer Nature Switzerland AG.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068078920&origin=inward,Conference Paper,SCOPUS_ID:85068078920,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),supporting the development of cyber-physical systems with natural language processing: a report,"
AbstractView references

Software has become the driving force for innovations in any technical system that observes the environment with different sensors and influence it by controlling a number of actuators; nowadays called Cyber-Physical System (CPS). The development of such systems is inherently inter-disciplinary and often contains a number of independent subsystems. Due to this diversity, the majority of development information is expressed in natural language artifacts of all kinds. In this paper, we report on recent results that our group has developed to support engineers of CPSs in working with the large amount of information expressed in natural language. We cover the topics of automatic knowledge extraction, expert systems, and automatic requirements classification. Furthermore, we envision that natural language processing will be a key component to connect requirements with simulation models and to explain tool-based decisions. We see both areas as promising for supporting engineers of CPSs in the future. Copyright ©2019 by the paper’s authors.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068033480&origin=inward,Conference Paper,SCOPUS_ID:85068033480,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an overview of user feedback classification approaches,"
AbstractView references

Online user feedback about software products is a promising source of user requirements. To allow scaling analyses to large amounts of user feedback, research on Crowd-based Requirements Engineering (CrowdRE) seeks to tailor natural language processing (NLP) techniques to Requirements Engineering (RE). Various frameworks have been proposed, but it remains largely unclear why particular NLP techniques are better suited for CrowdRE than others, which makes it hard to make a well-founded choice for a technique. We found that CrowdRE research most often uses machine learning (ML) and has so far applied twelve clusters of ML algorithms and seven clusters of ML features. The prevalent algorithm–feature pair is Naïve Bayes with Bag of Words – Term Frequency (BOW-TF), followed by Support Vector Machines (SVM) with BOW-TF. An initial comparison of the reported precision and recall suggests that classifications in RE need further improvement. Our research presents a preliminary overview of the current landscape of automated classification techniques for RE whose results may inspire researchers to apply new strategies to advance research in this field, or to include ML models they had not considered previously in their benchmarks. Copyright © 2019 by the paper’s authors.
"
10.1007/978-3-030-22734-0_17,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067608483&origin=inward,Conference Paper,SCOPUS_ID:85067608483,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),xscan: an integrated tool for understanding open source community-based scientific code,"
AbstractView references

Many scientific communities have adopted community-based models that integrate multiple components to simulate whole system dynamics. The community software projects’ complexity, stems from the integration of multiple individual software components that were developed under different application requirements and various machine architectures, has become a challenge for effective software system understanding and continuous software development. The paper presents an integrated software toolkit called X-ray Software Scanner (in abbreviation, XScan) for a better understanding of large-scale community-based scientific codes. Our software tool provides support to quickly summarize the overall information of scientific codes, including the number of lines of code, programming languages, external library dependencies, as well as architecture-dependent parallel software features. The XScan toolkit also realizes a static software analysis component to collect detailed structural information and provides an interactive visualization and analysis of the functions. We use a large-scale community-based Earth System Model to demonstrate the workflow, functions and visualization of the toolkit. We also discuss the application of advanced graph analytics techniques to assist software modular design and component refactoring. © 2019, Springer Nature Switzerland AG.
"
10.1007/978-3-030-21290-2_42,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067344889&origin=inward,Conference Paper,SCOPUS_ID:85067344889,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),behavior-derived variability analysis: mining views for comparison and evaluation,"
AbstractView references

The large variety of computerized solutions (software and information systems) calls for a systematic approach to their comparison and evaluation. Different methods have been proposed over the years for analyzing the similarity and variability of systems. These methods get artifacts, such as requirements, design models, or code, of different systems (commonly in the same domain), identify and calculate their similarities, and represent the variability in models, such as feature diagrams. Most methods rely on implementation considerations of the input systems and generate outcomes based on predefined, fixed strategies of comparison (referred to as variability views). In this paper, we introduce an approach for mining relevant views for comparison and evaluation, based on the input artifacts. Particularly, we equip SOVA – a Semantic and Ontological Variability Analysis method – with data mining techniques in order to identify relevant views that highlight variability or similarity of the input artifacts (natural language requirement documents). The comparison is done using entropy and Rand index measures. The method and its outcomes are evaluated on a case of three photo sharing applications. © 2019, Springer Nature Switzerland AG.
"
10.5220/0007768004810488,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067121182&origin=inward,Conference Paper,SCOPUS_ID:85067121182,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),approach to testing many state machine models in education,"
AbstractView references

In state machine modeling education, the effort required by instructors to test a large number of learner-created models should be reduced to concentrate on the following feedback activity. Although there are several methods for validating and verifying a state machine model, a considerable problem for instructors is the lack of tools to test multiple models at once. This study proposes a preliminary approach and a support tool to efficiently and promptly test multiple state machine models. A basic approach to solving this problem is creating test cases and then testing multiple state machine models simultaneously using these test cases. To reduce the instructors’ testing effort, the proposed approach includes three new concepts: (1) a logger extension to capture simulated data generated by an existing state machine simulation tool called SMart-Learning; (2) a method for creating test cases based on these logs; and (3) a feature to test many models using these test cases. As a result of a preliminary evaluation, it was confirmed that the proposed approach could be useful to test many answer models efficiently. © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved
"
10.1007/978-3-030-18553-4_7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85066882396&origin=inward,Conference Paper,SCOPUS_ID:85066882396,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),development of creative potential of students of engineering specialties within the framework of mathematical and information disciplines,"
AbstractView references

Trends in the development of the industry require specialists capable of designing smart networks, as well as those who can model “smart environments” for various types of tasks, develop technological and infrastructure requirements for these environments throughout the entire life cycle. This fact requires the development of the ability of students of engineering specialties of higher education to creativity in the conditions of technologies of big data. Future professionals who will work in the new digital reality must have the following qualities: to be able to identify complex systems and work with them; understand technology, processes and market situation. For this, it is necessary to have the skills of non-standard solutions, which can be formed within the framework of mathematical disciplines. Creative skills are formed during the study of mathematics in the re-water description of the processes of objective reality in the language of formulas. When constructing models, the choice of a variant of the mathematical description expresses a creative attitude to the process. The specialist of the new format should distinguish a constant desire to eliminate all types of losses, which can be achieved only if it is involved in the process of optimizing the business, as well as maximum customer orientation. These capabilities are provided by large data technologies. The training of future engineers in the principle positions of working with large data makes it possible to use the selected technologies as a basis for further processes of constructing mathematical models in their subject area. Thus, the developed methodology for using the technology of big data for modeling processes and systems, allows to form the competence of future specialists on the basis of developing the creative abilities of students and to be a source of inspiration for the creation of new technical products. © 2019, Springer Nature Switzerland AG.
"
10.1007/978-3-030-17938-0_44,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065848816&origin=inward,Conference Paper,SCOPUS_ID:85065848816,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),graph model for the identification of multi-target drug information for culinary herbs,"
AbstractView references

Drug discovery strategies based on natural products are re-emerging as a promising approach. Due to its multi-target therapeutic properties, natural compounds in herbs produce greater levels of efficacy with fewer adverse effects and toxicity than monotherapies using synthetic compounds. However, the study of these medicinal herbs featuring multi-components and multi-targets requires an understanding of complex relationships, which is one of the fundamental goals in the discovery of drugs using natural products. Relational database systems such as the MySQL and Oracle store data in multiple tables, which are less efficient when data such as the one from natural compounds contain many relationships requiring several joins of large tables. Recently, there has been a noticeable shift in paradigm to NoSQL databases, especially graph databases, which was developed to natively represent complex high throughput dynamic relations. In this paper, we demonstrate the feasibility of using a graph-based database to capture the dynamic biological relationships of natural plant products by comparing the performance of MySQL and one of the most widely used NoSQL graph databases called Neo4j. Using this approach we have developed a graph database HerbMicrobeDB (HbMDB), and integrated herbal drug information, herb-targets, metabolic pathways, gut-microbial interactions and bacterial-genome information, from several existing resources. This NoSQL database contains 1,975,863 nodes, 3,548,314 properties and 2,511,747 edges. While probing the database and testing complex query execution performance of MySQL versus Neo4j, the latter outperformed MySQL and exhibited a very fast response for complex queries, whereas MySQL displayed latent or unfinished responses for complex queries with multiple-join statements. We discuss information convergence of pharmacochemistry, bioactivities, drug targets, and interaction networks for 24 culinary herbs and human gut microbiome. It is seen that all the herbs studied contain compounds capable of targeting a minimum of 55 enzymes and a maximum of 250 enzymes involved in biochemical pathways important in disease pathology. © 2019, Springer Nature Switzerland AG.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064979636&origin=inward,Conference Paper,SCOPUS_ID:85064979636,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),dexter - data extraction &amp; entity recognition for low resource datasets,"
AbstractView references

Extraction of key information such as named entities, key phrases, and numbers is critical for several banking and financial processes. Banks and Financial Institutions resort to the use of automation tools to reduce the human effort required for these processes. Training a system to extract key datapoints reliably and efficiently from text requires large labeled datasets. However, openly available datasets in the financial sector have limited labeled data. In our paper, we address the issues in developing a data extraction system for low resource datasets. We experiment with a Bi-directional long short-term memory (Bi-LSTM) model which works well on low resource datasets. We introduce a novel domain-specific Bi-LSTM layer, which allows us to add domain-specific knowledge into the neural architecture. We observed that transfer learning from out-of-domain dataset boosts the accuracy on several extraction tasks. We create three new low resource financial datasets and demonstrate that our model consistently achieves a high degree of accuracy on these datasets. Furthermore, our model outperforms the reported state of the art results on the Financial NER dataset and achieves F1 of 87.48. Our experiments consistently show that transfer learning combined with domain-specific knowledge engineering improves entity recognition in a low resource setting. Copyright held by the author(s).
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064964178&origin=inward,Conference Paper,SCOPUS_ID:85064964178,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),building knowledge base through deep learning relation extraction and wikidata,"
AbstractView references

Many AI agent tasks require domain specific knowledge graph (KG) that is compact and complete. We present a methodology to build domain specific KG by merging output from deep learning-based relation extraction from free text and existing knowledge database such as Wikidata. We first form a static KG by traversing knowledge database constrained by domain keywords. Very large high-quality training data set is then generated automatically by matching Common Crawl data with relation keywords extracted from knowledge database. We describe the training data generation process in detail and subsequent experiments with deep learning approaches to relation extraction. The resulting model is used to generate new triples from free text corpus and create a dynamic KG. The static and dynamic KGs are then merged into a new KB satisfying the requirement of specific knowledge-oriented AI tasks such as question answering, chatting, or intelligent retrieval. The proposed methodology can be easily transferred to other domains or languages. Copyright held by the author(s).
"
10.5441/002/edbt.2019.64,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064955582&origin=inward,Conference Paper,SCOPUS_ID:85064955582,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),improving named entity recognition using deep learning with human in the loop,"
AbstractView references

Named Entity Recognition (NER) is a challenging problem in Natural Language Processing (NLP). Deep Learning techniques have been extensively applied in NER tasks because they require little feature engineering and are free from language-specific resources, learning important features from word or character embeddings trained on large amounts of data. However, these techniques are data-hungry and require a massive amount of training data. This work proposes Human NERD (stands for Human Named Entity Recognition with Deep learning) which addresses this problem by including humans in the loop. Human NERD is an interactive framework to assist the user in NER classification tasks from creating a massive dataset to building/maintaining a deep learning NER model. Human NERD framework allows the rapid verification of automatic named entity recognition and the correction of errors. It takes into account user corrections, and the deep learning model learns and builds upon these actions. The interface allows for rapid correction using drag and drop user actions. We present various demonstration scenarios using a real world data set. © 2019 Copyright held by the owner/author(s).
"
10.1007/978-3-030-17465-1_14,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064532126&origin=inward,Conference Paper,SCOPUS_ID:85064532126,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),stochy: automated verification and synthesis of stochastic processes,"
AbstractView references

StocHy is a software tool for the quantitative analysis of discrete-time stochastic hybrid systems (shs). StocHy accepts a high-level description of stochastic models and constructs an equivalent shs model. The tool allows to (i) simulate the shs evolution over a given time horizon; and to automatically construct formal abstractions of the shs. Abstractions are then employed for (ii) formal verification or (iii) control (policy, strategy) synthesis. StocHy allows for modular modelling, and has separate simulation, verification and synthesis engines, which are implemented as independent libraries. This allows for libraries to be easily used and for extensions to be easily built. The tool is implemented in c++ and employs manipulations based on vector calculus, the use of sparse matrices, the symbolic construction of probabilistic kernels, and multi-threading. Experiments show StocHy ’s markedly improved performance when compared to existing abstraction-based approaches: in particular, StocHy beats state-of-the-art tools in terms of precision (abstraction error) and computational effort, and finally attains scalability to large-sized models (12 continuous dimensions). StocHy is available at www.gitlab.com/natchi92/StocHy. Data or code related to this paper is available at: [31]. © 2019, The Author(s).
"
10.1016/bs.pbr.2019.03.016,S0079612319300469,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063761931&origin=inward,Book Chapter,SCOPUS_ID:85063761931,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on the nature and evolution of the human mind,"
                  Organisms are faced during their lives with an immense variety of environmental challenges and organism specific problems, for which they have to find adequate solutions in order to survive. Problem solving, in other words, is an essential dynamic survival mechanism, evolved to cope with disturbances in the ecological equilibrium. With the evolution of sensory systems as adaptations to specialized environments, the capacity to process large amounts of sensory information increased and, with that, the power to create more complex models of reality. The object of this review is to present current perspectives on the organization and evolution of the human brain and to examine some of the design principles and operational modes that underlie its information processing capacity. Furthermore, the neural correlates of mind—the set of cognitive faculties involved in perceiving, remembering, reasoning and deciding—will be explored. It will be argued that in primates, and especially humans, the complexity of the neural circuitry of the cerebral cortex is the neural correlate of higher cognitive functions, including mind-like properties and consciousness.
               "
10.1109/ACCESS.2019.2895956,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062917603&origin=inward,Article,SCOPUS_ID:85062917603,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an integrated cloud cae simulation system for industrial service applications,"
AbstractView references

The cloud-based computer-aided engineering (CAE) technology expands the application scope of CAE, solves the problem of uneven distribution, and improves the efficiency of the simulation resources. Consequently, various industries are turning to cloud CAE technology to design their products. However, to utilize this technology fully, we must overcome problems, such as impracticality, inability to handle complex analysis objects, poor data transmission, and compatibility, and poor industrial applicability. Based on the ASP.NET, VB.NET, and ANSYS Parametric Design Language, this paper establishes an integrated cloud CAE simulation system for industrial service applications. The system: 1) connects the dispatching manager through the network and 2) clusters the servers according to their load and usage before dispatch, thereby enabling users to access the ANSYS software remotely. The model can be built either by inputting the dimensions of the parametric model through the page or by uploading a three-dimensional computer-aided design (CAD) model that meets the format requirements. The material parameters and the boundary conditions also input through the page. Thus, clients can input their own parameters and analyze the server CAD. Finally, the CAE analysis results can be invoked by a page operation, saving meaningful results on the server to be accessed directly by users. The proposed simulation system serves the needs of users both quickly and efficiently. In the large-scale industrial application trials, enterprises responded favorably to the system. In particular, it properly solves the CAE analysis problems of small- and medium-sized machinery and equipment enterprises. © 2013 IEEE.
"
10.1155/2019/5182629,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062373059&origin=inward,Article,SCOPUS_ID:85062373059,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),numerical modeling and experimental characterization of elastomeric pads bonded in a conical spring under multiaxial loads and pre-compression,"
AbstractView references

Elastomeric components are widely used in the engineering field since their mechanical properties can vary according to a specific condition, enabling their applications under large deformations and multiaxial loading. In this context, the present study seeks to investigate the main challenges involved in the finite element hyperelasticity simulation of rubber-like material components under different cases of multiaxial loading and precompression. The complex geometry of a conical rubber spring was chosen to deal with several deformation modes; this component is in the suspension system placed between the frame and the axle for railway vehicles. The framework of this study provides the correlation between axial and radial stiffness under precompression obtained by experimental tests in prototypes and virtual modeling obtained through a curve fitting procedure. Since the material approaches incompressibility, different shape functions were adopted to describe the fields of pressure and displacements according to the finite element hybrid formulation. The material parameters were accurately adjusted through an optimization algorithm implemented in Python program language which calibrates the finite element model according to the prototype test data. However, as an initial guess, the proper constitutive model and its parameters were first defined based only on the uniaxial tensile test data, since this test is easy to perform and well understood. The validation of the simulation results in comparison with the experimental data demonstrated that care should be given when the same component is subjected to different multiaxial loading cases. © 2019 Debora Francisco Lalo et al.
"
10.1109/ACCESS.2018.2889557,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85058997791&origin=inward,Article,SCOPUS_ID:85058997791,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),scoreboard architectural pattern and integration of emotion recognition results,"
AbstractView references

This paper proposes a new design pattern, named Scoreboard, dedicated for applications solving complex, multi-stage, non-deterministic problems. The pattern provides a computational framework for the design and implementation of systems that integrate a large number of diverse specialized modules that may vary in accuracy, solution level, and modality. The Scoreboard is an extension of Blackboard design pattern and comes under behavioral type. The pattern allows for an integration of multimodal results, employing early, and/or late fusion paradigms. Additionally, it provides a framework for the evaluation of the modules, dealing with inconsistency and low accuracy. In this paper, the Scoreboard design pattern is described with the standard meta-data model, followed by a sample implementation. This paper also provides the evaluation results based on experiments and a case study. The evaluation results confirmed the robustness, modularization, ease of integration, efficiency, and adaptability of the solutions with the Scoreboard pattern in comparison with the Blackboard pattern and 'no pattern' condition. This paper provides also a case study of Scoreboard application in an integration of emotion recognition results. There are certain complex problems in modern software engineering which require multi-stage, multi-party, multi-modal solutions, and non-deterministic control strategies. Among those are natural language processing, image processing, and emotion recognition, to name just a few. The proposed Scoreboard pattern might be used in the software addressing the problems, especially in research systems that explore large solution spaces and require runtime decisions on execution order. © 2013 IEEE.
"
10.1016/j.eswa.2018.08.009,S0957417418305177,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052130680&origin=inward,Article,SCOPUS_ID:85052130680,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),knowledge-oriented convolutional neural network for causal relation extraction from natural language texts,"
                  Causal relation extraction is a challenging yet very important task for Natural Language Processing (NLP). There are many existing approaches developed to tackle this task, either rule-based (non-statistical) or machine-learning-based (statistical) method. For rule-based method, extensive manual work is required to construct handcrafted patterns, however, the precision and recall are low due to the complexity of causal relation expressions in natural language. For machine-learning-based method, current approaches either rely on sophisticated feature engineering which is error-prone, or rely on large amount of labeled data which is impractical for causal relation extraction problem. To address the above issues, we propose a Knowledge-oriented Convolutional Neural Network (K-CNN) for causal relation extraction in this paper. K-CNN consists of a knowledge-oriented channel that incorporates human prior knowledge to capture the linguistic clues of causal relationship, and a data-oriented channel that learns other important features of causal relation from the data. The convolutional filters in knowledge-oriented channel are automatically generated from lexical knowledge bases such as WordNet and FrameNet. We propose filter selection and clustering techniques to reduce dimensionality and improve the performance of K-CNN. Furthermore, additional semantic features that are useful for identifying causal relations are created. Three datasets have been used to evaluate the ability of K-CNN to effectively extract causal relation from texts, and the model outperforms current state-of-art models for relation extraction.
               "
10.1007/978-981-10-8848-3_40,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85049065964&origin=inward,Conference Paper,SCOPUS_ID:85049065964,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),trusted operating system-based model-driven development of secure web applications,"
AbstractView references

This paper adds security engineering into an object-oriented model-driven software development for real-life Web applications. In this paper, we use mining patterns in Web applications. This research paper proposes a unified modeling language-based secure software maintenance procedure. The proposed method is applied for maintaining a large-scale software product and real-life product-line products. After modeling, we can implement and run this Web application, on SPF-based trusted operating systems. As we know, reverse engineering of old software is focused on the understanding of legacy program code without having proper software documentation. The extracted design information was used to implement a new version of the software program written in C++. For secure designing of Web applications, this paper proposes system security performance model for trusted operating system. For re-engineering and re-implementation process of Web applications, this paper proposes the model-driven round-trip engineering approach. © Springer Nature Singapore Pte Ltd 2019.
"
10.1007/978-3-319-91186-1_23,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85047942283&origin=inward,Conference Paper,SCOPUS_ID:85047942283,scopus,2019-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the semantic models of arctic zone legal acts visualization for express content analysis,"
AbstractView references

Currently, large amounts of data are available in text form. However, due to the characteristic features of the text in natural languages, the development of fully automatic methods for analyzing the semantics of texts is a difficult task. This paper describes the composition, structure and some areas of application of the developed technologies of semantic analysis and visualization of semantic models of text documents. Also, methods for visual express content analysis of documents are described. These methods are part of the technology for visualizing semantic models of text documents and implemented as independent software tools. To demonstrate the main features of the technology, the experience of using the visualization of semantic document models for visual express content analysis of legal acts regulating the development of spatially-distributed systems of various levels and analysis of the results is described in detail. The final part of the paper identifies some promising areas of application of the developed technologies, as well as determines the main directions for further work and the possibilities to expand the functionality of the methods of visual express content analysis of text documents. © 2019, Springer International Publishing AG, part of Springer Nature.
"
10.1109/CIST.2018.8596422,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061433884&origin=inward,Conference Paper,SCOPUS_ID:85061433884,scopus,2018-12-28,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),bpmn profile for collaborative business process,"
AbstractView references

Business process collaboration is an imminent need when it comes to evolving an organization or expanding its system to keep up with the pace of technology. In a collaborative environment, business processes become more complex as organizations find themselves in front of large systems that need to share their resources and connect their businesses. Thus, the modeling of these processes becomes a tedious work for the designers. This article will represent a collaborative business process modeling language. The latter will be an extension of the Business Process Model and Notation (BPMN) a standard for business process modeling. © 2018 IEEE.
"
10.1145/3330089.3330113,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068802526&origin=inward,Conference Paper,SCOPUS_ID:85068802526,scopus,2018-12-26,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a hybrid approach for amazigh-english machine translation,"
AbstractView references

In this paper, we present our hybrid methodology for building a bidirectional Amazigh-English machine translation. The architecture of the proposed system is based on both Interlingua-Based Machine Translation (IBMT) and Statistical-Based Machine Translation (SBMT) approaches. Amazigh is a less-resourced language. It does not have parallel corpora with enough size. So, using statistical approach for such language will not be a good choice, because this approach requires large parallel corpora to well train probabilistic models, and to ensure a translation of good quality. Since we dispose of an Amazigh IBMT based deconverter, we thought, firstly, to use it in building an Amazigh-English parallel corpus. This latter have been exploited to train the necessary models in SBMT. © 2018 Association for Computing Machinery.
"
10.1109/QUATIC.2018.00049,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061319695&origin=inward,Conference Paper,SCOPUS_ID:85061319695,scopus,2018-12-26,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a reactive and model-based approach for developing internet-of-things systems,"
AbstractView references

Software has a longstanding association with a state of crisis considering its success rate. The explosion of Internet-connected devices - Internet-of-Things - adds to the complexity of software systems. The particular characteristics of these systems, such as its large-scale and heterogeneity, pose increasingly new challenges. Model-based approaches have been widely used as a mechanism to abstract low-level programming details and processes. By using such approaches, and leveraging concepts as reactive design, visual notations, and live programming, we believe to be able to reduce the complexity of creating, operate/monitor and evolve such systems. The main objective of this Ph.D. is to delve into the software engineering practices for developing IoT systems and systems of systems, exploiting models as a suitable abstraction, expecting to reduce the complexity of managing most of the software development lifecycle that targets IoT systems and to develop the prototype that will aid on the validation of such approach. © 2018 IEEE.
"
10.1109/IAEAC.2018.8577597,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060392880&origin=inward,Conference Paper,SCOPUS_ID:85060392880,scopus,2018-12-14,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),research on chinese sentence similarity calculation method based on multiple features,"
AbstractView references

Chinese sentence similarity calculation is widely used in the field of natural language processing, and the similarity calculation method is also varied. This paper introduces the similarity calculation method based on shallow Information, based on the similarity of part of speech, and also analyzes their calculation principles and calculation methods. Their advantages and disadvantages are given. At the same time, it puts forward the idea of using deep learning thinking to automatically learn the essential information of data from large-scale text data. Using the word2vec model, sentence processing is reduced to word vectors. It also proposes a method for calculating the similarity of shallow features, parts of speech features, and features of word vectors. The experimental results show that compared with the conventional method, the accuracy of the algorithm in calculating the sentence similarity is higher. © 2018 IEEE.
"
10.1080/15325008.2018.1531444,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85059079067&origin=inward,Article,SCOPUS_ID:85059079067,scopus,2018-12-14,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),risk-based reserve coordinative unit commitment for a large-scale wind-storage system,"
AbstractView references

The increasingly high integration of wind power into power systems will further increase the power uncertainty and make it difficult to calculate the requirement of the reserve. In this article, a risk-based reserve optimization approach is proposed to quantitatively evaluate the reserve requirement of a large-scale wind-storage system. Conditional value-at risk (CVaR) is adapted to calculate the risk reserve managing the uncertainty of wind generation; this risk reserve achieves the optimal risk without sacrificing system reliability. An energy storage system (ESS) is employed to undertake the role of reserve transfer (RT) by cooperating with wind generation and conventional thermal units in the decision-making of unit commitment (UC); their cooperation and coordination can be achieved by building a bilevel optimization model with a day-ahead risk-constrained unit commitment model and a real-time risk reserve adjustment model. By using the duality principle and the big-M method, the formulations are converted into a mixed integer linear programing problem (MILP) that is solved using a column and constraint generation algorithm (C&CG). The model is tested on the six-bus system and the IEEE 118-bus system. The simulation results show that the ESS can transfer the reserve capacity of the thermal generator and improve the ability to accommodate the uncertainty of wind power generation, thereby demonstrating the effectiveness of the proposed methodology. © 2018, Copyright © Taylor & Francis Group, LLC.
"
10.1109/KSE.2018.8573418,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060395178&origin=inward,Conference Paper,SCOPUS_ID:85060395178,scopus,2018-12-11,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),integrating word embeddings into ibm word alignment models,"
AbstractView references

Word alignment models are used to generate word-aligned parallel text which is used in statistical machine translation systems. Currently, the most popular word alignment models are IBM models which have been widely applied in a large number of translation systems. The parameters of IBM models are estimated by using Maximum Likelihood principle, i.e. by counting the co-occurrence of words in the parallel text. This way of parameter estimation leads to the 'ambiguity' problem when some words stand together in many sentence pairs but each of them is not translation of any other. Additionally, this method requires large amount of training data to achieve good results. However, parallel text which is used to train the IBM models is usually limited for low-resource languages. In this work, we try to solve these two problems by adding semantic information to the models. Our semantic information is derived from word embeddings which only need monolingual data to train. We deploy evaluation on a language pair that has great differences in grammar structure, English-Vietnamese. Even with this challenged task, our proposed models gain significant improvements in word alignment result and help increasing translation quality. © 2018 IEEE.
"
10.1109/DCABES.2018.00084,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060291580&origin=inward,Conference Paper,SCOPUS_ID:85060291580,scopus,2018-12-10,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),specification and design of cyber physical systems based on system of systems engineering approach,"
AbstractView references

Cyber-physical Systems of Systems (CPSoS) are large complex systems where physical elements interact with and are controlled by a large number of distributed and net-worked computing elements and human users. A SoS is an integration of a finite number of constituent systems which are independent and operable, and which are networked together for a period of time to achieve a certain higher goal. In order to specify and model such kind of systems, we need develop specification and modeling methods which would be capable to encompass the systems of systems (SoS) specific properties of cyber physical systems. In this paper, we propose a new paradigm for specifying and modeling cyber physical systems based on system-of-systems approach. We propose an approach to support specification and modeling cyber physical systems based on systems of systems engineering by integrating AADL, Modelicalml and other modeling language. On the basis of the hierarchical concept of industrial CPS system, a hierarchical design scheme of industrial CPSoS system based on OPC UA heterogeneous data integration processing is proposed. This paper will also use AADL for modeling CPS on three levels: 1). robot on the unit level; 2) workshops of smart factory on the system level 3) intellectual factory on the SOS level. For the physical aspect of cyber physical system, this paper will propose a method to combine modelical, Simulink and AADL model to model a unit robot which can interaction with real environment. © 2018 IEEE.
"
10.1109/ICCKE.2018.8566605,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060290433&origin=inward,Conference Paper,SCOPUS_ID:85060290433,scopus,2018-12-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),word embedding in small corpora: a case study in quran,"
AbstractView references

Text is a complex set of words to carry the meaning and representations of words is the first step to perform linguistic processing and text comprehension. So far, many researches have been done on the semantic representations of words using neural networks in various areas of natural language processing using large text corpus from general domain. In the meantime, some efforts have been done to apply deep learning methods to represent the words of small corpus supporting the hypothesis that the bigger corpora doesn't necessarily provide better results in words representation. In this research the capability of word2vec for learning semantic representation of words in small corpus is investigated. Here, we consider Skip-gram and CBOW learning models with different values of hyper parameters. Two new data sets have been created to evaluate the model's performance on the small domain-specific Quranic corpus. First and second datasets are used to test the words categorization and word pairwise similarity respectively. Our results demonstrate that the best performance for skip-gram occurs with 30 numbers of iterations when the dimension is set to 7. © 2018 IEEE.
"
10.1109/MEMCOD.2018.8557005,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060277725&origin=inward,Conference Paper,SCOPUS_ID:85060277725,scopus,2018-12-03,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),hierarchical behavior annex: towards an aadl functional specification extension,"
AbstractView references

AADL is a modeling language to design and analyze embedded real-time systems and is widely used to model safety-critical systems. AADL describes the system models hierarchically through components such as systems, processes, and threads, etc. The Behavioral Annex is a supplement of AADL in terms of functional behavior. It enables modeling component and component interaction behavior in a state-machine-based annex sublanguage. At present, there is no mechanism to represent hierarchical automata in the behavioral annex. However, this is a very important feature because industrial complex systems are always described with concurrent and composite states. Although we can model a system with AADL's own hierarchical description capabilities, it will result in a large amount of threads. In actual development, a refinement process is always needed before system synthesis, in which several threads may be combined into one thread that has concurrent and composite states. This paper proposes a hierarchical extension of the AADL behavioral annex which is named HBA (Hierarchical Behavior Annex). First, the formal syntax of HBA is given, and then we formally define the semantics of HBA. We propose a meta-model of HBA and implement its textual and graphical editor in the OSATE environment. Finally, an industrial case study is given to validate the approach. © 2018 IEEE.
"
10.1007/s11761-018-0245-1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055923167&origin=inward,Conference Paper,SCOPUS_ID:85055923167,scopus,2018-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a review and future directions of soa-based software architecture modeling approaches for system of systems,"
AbstractView references

Software architecture is a software system’s earliest set of design decisions that are critical for the quality of the system desired by the stakeholders. The architecture makes it easier to reason about and manage change during different phases of complex software life cycle. The modeling of software architecture for System of Systems (SoS) is a challenging task because of a system’s complexity arising from an integration of heterogeneous, distributed, managerially and operationally independent systems collaborating to achieve global missions. SoS is essentially dynamic and evolutionary by design requiring suitable architectural patterns to deal with runtime volatility. Service-oriented architecture offers several architectural features to these complex systems; these include, interoperability, loose coupling, abstraction and the provision of dynamic services based on standard interfaces and protocols. There is some research work available that provides critical analysis of current software architecture modeling approaches for SoS. However, none of them outlines the important characteristics of SoS or provides detailed analysis of current service-oriented architecture modeling approaches to model those characteristics. This article addresses this research gap and provides a taxonomy of software architecture modeling approaches, comparing and contrasting them using criteria critical for realization of SoS. Additionally, research gaps are identified, and future directions are outlined for building software architecture for SoS to model and reason about architecture quality in a more efficient way in service-oriented paradigm. © 2018, Springer-Verlag London Ltd., part of Springer Nature.
"
10.1016/j.cl.2017.12.001,S1477842417300301,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85047773102&origin=inward,Article,SCOPUS_ID:85047773102,scopus,2018-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),improving formal analysis of state machines with particular emphasis on and-cross transitions,"
                  In this paper, we present an approach to formally encode state machines expressed in Umple for symbolic verification. We illustrate this with a real-world modeling example that encodes and analyzes and-cross transitions. This paper discusses a formal description of our approach to represent state machine systems under analysis (SSUAs); a systematic approach to certifying that SSUAs are deterministic; and an evaluation of performance (memory usage and execution time) on the case study.
               
                  Method
                  We describe a formalization of state machines in Umple that enables their translation to model checking tools and also to code that is consistent with this. We present three alternative modeling solutions for a sample problem and a solution based on the use of and-cross transitions. State machine models corresponding to these solutions are represented in Umple, a model-oriented programming language. These are automatically transformed to SMV, the input language of the nuXmv (or NuSMV) model checker. By cleanly separating concerns, we systematically integrate components of hierarchical state systems as opposed to the traditional flattening approach, yet manage the complexity introduced by concurrency and and-crossing. We then compose and verify a set of requirements (e.g., correctness, safety, liveliness, etc.) on the resulting systems of all the modeling approaches to empirically compare the different modeling alternatives with the use of and-cross transitions.
               
                  Results
                  We can encode and formally analyse complex state machines with and-cross transition(s). We observed a large reduction in the number of required transitions for encoding the SSUA, as opposed to the alternative approaches. We asserted that solutions derived from the approaches are identical behavior-wise even though each approach models the SSUA differently. Each of the approaches yielded the same result for potentially conflicting pairs which is a false positive (i.e., the SSUAs are deterministic). We observe that each approach maintains the same global state-space irrespective of the variations in their number of transitions. Furthermore, we observe that it is untrue that a more abstract method applied to an SSUA outperforms its less abstract counterpart whenever parameters (such as execution time, memory usage and the number of Binary Decision Diagrams - BDDs) are the factors under consideration.
               
                  Contributions
                  A systematic approach to encode state machines with and-cross transitions (including unusual transitions). An enhanced but fully automated approach to discovering nondeterminism in state machines even in the presence of unbounded domains and multiple and-cross transitions within the same enclosing orthogonal state. An empirical study of the impact of abstraction on some performance parameters. We also present an extended formalization of Umple state machines.
               "
10.1109/ICACCI.2018.8554371,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060024010&origin=inward,Conference Paper,SCOPUS_ID:85060024010,scopus,2018-11-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a hybrid approach to rumour detection in microblogging platforms,"
AbstractView references

Microblogging platforms facilitate fast and frequent communication among very large numbers of users. Rumours, especially in times of crisis, tend to spread quickly, causing confusion, and impair people's ability to make decisions. Hence, it is of utmost importance to automatically detect a rumour as soon as possible. Using the PHEME dataset that contains rumours and non-rumours pertaining to five major events, we have developed a rumour detection system that classifies posts from Twitter, a popular microblogging website. We have first analyzed and ranked a number of content-based and user-based features. Some content-based features are derived using natural language processing techniques. We then trained multiple machine learning models (Naive Bayes, Random Forests and Support Vector Machines) using different combinations of the features. Finally, we compared the performance of these models. The performance of the models on one such event resulted in 78% accuracy. © 2018 IEEE.
"
10.1109/HONET.2018.8551480,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85059986936&origin=inward,Conference Paper,SCOPUS_ID:85059986936,scopus,2018-11-28,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automatic formal verification of digital components of iots using cbmc,"
AbstractView references

These days, internet of things (IoT) are being widely used in many safety-critical domains, like healthcare and transportation. Thus, their functional correctness is very important. However, simulation based analysis is based on sampling methods and thus their results are not complete and cannot be termed as accurate. Formal verification has been recently proposed to verify the digital components of IoT devices and thus overcome the incompleteness issues of simulation. However, formal verification process requires manual development of a formal model of the given circuit and its desired properties. Moreover, the verification of the relationship between the formally specified model and its properties sometimes also requires manual interventions. These manual efforts can be quite cumbersome while verifying large systems and thus make formal verification of IoT devices somewhat infeasible for industrial usage. To overcome these limitations, we present a tool chain to automatically formally verify digital components of IoT devices, which are usually expressed in the Verilog language. The proposed methodology primarily leverages upon the strong verification support for the C language. The idea is to convert the given Verilog code and its properties to C language and use bounded model checking to verify the obtained C code. The formally verified C code is then converted back to Verilog to facilitate circuit design steps i.e., synthesis, timing analysis etc., and thus continue with the regular digital system design flow. For illustration, we present the verification of several widely used components of IoT devices, including an ALU and a 64-bit processor, which are fairly complex and to the best of our knowledge have never been formally verified automatically before. © 2018 IEEE.
"
10.1109/SysEng.2018.8544424,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85059977525&origin=inward,Conference Paper,SCOPUS_ID:85059977525,scopus,2018-11-26,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),from models of structures to structures of models,"
AbstractView references

The complexity of industrial systems is steadily increasing. To face this complexity, the different engineering disciplines are designing models. hese models are complex as they reflect the complexity of systems under study. Therefore, they need to be structured. In this article we study structural constructs of modeling languages used in systems engineering. We introduce for that purpose a small domain specific language, the so-called S2ML for System Structure Modeling Language. We show that a large class of actual modeling languages can be (re)constructed by plugging their underlying mathematical framework into S2ML. © 2018 IEEE.
"
10.1109/SysEng.2018.8544402,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85059975986&origin=inward,Conference Paper,SCOPUS_ID:85059975986,scopus,2018-11-26,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sysml executable model of an energy-efficient house and trade-off analysis,"
AbstractView references

With the growing complexity of energy efficient buildings, the methods of modeling and simulating such structures must account for monitoring several thousand design parameters across multiple diverse domains. As a result, modeling tools are now very specific to their respective domains and are growing more and more incongruous with each other. This calls for a way to integrate multiple modeling tools in the effort to create a single, large model capable of encapsulating data from multiple, different models. Thus, in this paper, different methods to perform an integration with Systems Modeling Language (SysML) and a simulation tool were identified, described and evaluated. Then, a new method was developed and discussed. Finally, the new method was demonstrated by developing a SysML executable model of a simple two-room house. Using the Functional Mock-up Interface (FMI) standard, the SysML model is integrated with a Modelica model, and a simulation is run in Simulink. Finally, a tradeoff analysis was performed for the purpose of design space exploration. © 2018 IEEE.
"
10.1109/ICTC.2018.8539447,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85059442005&origin=inward,Conference Paper,SCOPUS_ID:85059442005,scopus,2018-11-16,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),machine learning based fast reading algorithm for future ict based education,"
AbstractView references

With the development of ICT and Big Data, a new education paradigm has been attracting attention by utilizing techniques to efficiently grasp large amount of articles and fairy tales and so on. For example, even in the same event and subject, new version-type articles or fairy tales are pouring out of myriad times and regions. This paper proposes machine learning based fast reading algorithm to identify elements of an important story that are handed down in spite of temporal and spatial differences using the version of 72 similar folk tales of the folk tales ""Red Hat"" which exist in Europe, Asia, Africa etc. To do this, we analyze the factors depending on the existence of various versions in a decision tree and conduct research using R language tree and caret package. Through the evaluation of the analytical model, we confirmed the existence of the unchanging core elements of traditional talks, which are handed down to the constraints of time and space, and the possibility of a model that intuitively understands them. The result of this study is expected to be used as a new educational field for ICT - based computing thinking (CT). © 2018 IEEE.
"
10.1109/ICSCEE.2018.8538421,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85059421516&origin=inward,Conference Paper,SCOPUS_ID:85059421516,scopus,2018-11-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a multi-layered annotation scheme and computational model for co-learning semantic and prosodic structures of chinese discourse,"
AbstractView references

This paper presents a novel annotation scheme of Chinese discourse structures to model the complex interactions among grammar, semantics and phonology. The scheme mainly contains three layers, i.e., grammatical, semantic and prosodic layers. Within each layer, the representations of dependency relations, rhetorical structure, information structure, topic chain, prosodic boundaries and stress distributions are specified. Based on the scheme, a large scale corpus of transcribed speech data is constructed and annotated. We further propose a machine learning methodology to learn from the annotated corpus a computational representation of the internal structure of each layer and the interactions across different layers. Specifically, we employ the Recursive Neural Network (RNN) to model the fine-grained structure in natural language information, through learning a distributed representation of the structural units. The proposed annotation scheme and machine learning methodology to expected to underpin more effective and intelligent speech engineering and understanding technologies of the future. © 2018 IEEE.
"
10.1088/1755-1315/189/3/032047,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056747536&origin=inward,Conference Paper,SCOPUS_ID:85056747536,scopus,2018-11-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),development of two explosive candidate reference materials: certification of metallic impurities in llm-105 and fox-7,"
AbstractView references

Explosive reference materials are significant to traceability analysis, quality control and measurement of energetic materials applied in modern weapon system. In the field of engineering-oriented application, LLM-105 and FOX-7 were proved that they had a promising prospect in used extensively as insensitive high energetic ingredients in munitions. Certification of metallic impurities in LLM-105 and FOX-7 was achieved by the microwave digestion-assistant inductively coupled plasma-mass spectrometry method. The contents of metallic impurities, including Na, K, Ca, Cr, Ni, Fe, Mg, Al, Co, Cu and Zn, in several LLM-105 and FOX-7 samples were accurately detected under the optimal conditions. And the detected results suggested that recrystallization was an effective method for high-purity explosive reference materials. This study could be helpful for the development of LLM-105 and FOX-7 candidate reference materials. © Published under licence by IOP Publishing Ltd.
"
10.1016/j.sysarc.2018.09.007,S1383762118302455,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053702722&origin=inward,Article,SCOPUS_ID:85053702722,scopus,2018-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a survey on modeling and model-driven engineering practices in the embedded software industry,"
                  Software-intensive embedded systems have become an essential aspect of our lives. To cope with its growing complexity, modeling and model-driven engineering (MDE) are widely used for analysis, design, implementation, and testing of these systems. Since a large variety of software modeling practices is used in the domain of embedded software, it is important to understand and characterize the-state-of-the-practices and also the benefits, challenges and consequences of using software modeling approaches in this domain. The goal of this study is to investigate those practices in the embedded software engineering projects by identifying to what degree, why and how software modeling and MDE are used. To achieve this objective, we designed and conducted an online survey. Opinions of 627 practicing embedded software engineers from 27 different countries are included in the survey. The survey results reveal important and interesting findings about the state of software modeling and MDE practices in the worldwide embedded software industry. Among the results: (1) Different modeling approaches (from informal sketches to formalized models) are widely used in the embedded software industry with different needs and all of the usages could be effective depending on the various modeling characteristics; (2) The majority of participants use UML; and the second most frequently selected response is “Sketch/No formal modeling language”, which shows the wide-spread informal usage of modeling; (3) In model-driven approaches, it is not so important to have a graphical syntax to represent the model (as in UML) and depending on the type of target embedded industrial sector, modeling stakeholders prefer models, which can be represented in a format that is readable by a machine (as in DSL); (4) Sequence diagrams and state-machines are the two most popular diagram types; (5) Top motivations for adopting MDE are: cost savings, achieving shorter development time, reusability and quality improvement. The survey results will shed light on the state of software modeling and MDE practices and provide practical benefits to embedded software professionals (e.g., practitioners, researchers and also educators).
               "
10.1016/j.robot.2018.07.006,S0921889017306280,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052875451&origin=inward,Article,SCOPUS_ID:85052875451,scopus,2018-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks,"
                  Linking human whole-body motion and natural language is of great interest for the generation of semantic representations of observed human behaviors as well as for the generation of robot behaviors based on natural language input. While there has been a large body of research in this area, most approaches that exist today require a symbolic representation of motions (e.g. in the form of motion primitives), which have to be defined a-priori or require complex segmentation algorithms. In contrast, recent advances in the field of neural networks and especially deep learning have demonstrated that sub-symbolic representations that can be learned end-to-end usually outperform more traditional approaches, for applications such as machine translation. In this paper we propose a generative model that learns a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks (RNNs) and sequence-to-sequence learning. Our approach does not require any segmentation or manual feature engineering and learns a distributed representation, which is shared for all motions and descriptions. We evaluate our approach on 2 846 human whole-body motions and 6 187 natural language descriptions thereof from the KIT Motion-Language Dataset. Our results clearly demonstrate the effectiveness of the proposed model: We show that our model generates a wide variety of realistic motions only from descriptions thereof in form of a single sentence. Conversely, our model is also capable of generating correct and detailed natural language descriptions from human motions.
               "
10.1016/j.compind.2018.06.002,S0166361517305213,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051242928&origin=inward,Article,SCOPUS_ID:85051242928,scopus,2018-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cbg-framework: a bottom-up model-based approach for collaborative business process management,"
                  Nowadays most existing products and services are the result of the collaboration of a large number of companies that form a value chain known as Supply Chain (SC). Then individual Business Process Management (BPM) requires a holistic vision that incorporates an inter organizational view that supports SC decision making. This study proposes a novel idea trying to address collaborative BP modelling problem with a new perspective, a bottom-up approach, reusing process models that each organization may have created with a different modelling language. Collaborative Business Generation (CBG) Framework, following Model-Driven Engineering (MDE) paradigm, includes a meta model, a method, a set of model transformations and a support tool to create collaborative BP models from individual ones, maintaining privacy and autonomy in decision making. This paper presents main CBG-Framework elements as well as a real world case study for early validation.
               "
10.1007/s10009-018-0491-8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85045481909&origin=inward,Article,SCOPUS_ID:85045481909,scopus,2018-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),slicing atl model transformations for scalable deductive verification and fault localization,"
AbstractView references

Model-driven engineering (MDE) is increasingly accepted in industry as an effective approach for managing the full life cycle of software development. In MDE, software models are manipulated, evolved and translated by model transformations (MT), up to code generation. Automatic deductive verification techniques have been proposed to guarantee that transformations satisfy correctness requirements (encoded as transformation contracts). However, to be transferable to industry, these techniques need to be scalable and provide the user with easily accessible feedback. In MT-specific languages like ATL, we are able to infer static trace information (i.e., mappings among types of generated elements and rules that potentially generate these types). In this paper, we show that this information can be used to decompose the MT contract and, for each sub-contract, slice the MT to the only rules that may be responsible for fulfilling it. Based on this contribution, we design a fault localization approach for MT, and a technique to significantly enhance scalability when verifying large MTs against a large number of contracts. We implement both these algorithms as extensions of the VeriATL verification system, and we show by experimentation that they increase its industry readiness. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.
"
10.1109/CLUSTER.2018.00068,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85057267930&origin=inward,Conference Paper,SCOPUS_ID:85057267930,scopus,2018-10-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),challenges in developing mpi fault-tolerant fortran applications,"
AbstractView references

Powerful high performance computing systems of the future are expected to have higher failure rates than current systems. As a result, HPC applications running on such future systems are more likely to encounter a system failure than on today's machines. Application fault tolerance is therefore becoming more important to avoid costly waste of resources associated with rerunning failed applications. The MPI 3.1 standard does not address the issue of MPI process failures. Checkpoint/restart is commonly used to add fault tolerance to MPI applications. However, there can be complicated issues impacting an MPI application's ability to correctly and efficiently write checkpoint files, particularly if Fortran I/O statements are used. Moreover, it may be inefficient restart a large number MPI processes from a checkpoint. Several MPI fault tolerance libraries, such as ULFM, are being developed to enabl MPI programs to recover from MPI process failures. This can circumvent much of the overhead of an application restart, including rescheduling, launching, initializing, and reading checkpoint data. Each library uses a different approach to recovery from MPI process failures. Unfortunately, some of the proposed recovery models are incompatible with Fortran. This paper intends to help Fortran MPI application developers avoid problems when developing fault-tolerant MPI applications. © 2018 IEEE.
"
10.1145/3236024.3236046,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85058341895&origin=inward,Conference Paper,SCOPUS_ID:85058341895,scopus,2018-10-26,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"model transformation languages under a magnifying glass: a controlled experiment with xtend, atl, and qvt","
AbstractView references

In Model-Driven Software Development, models are automatically processed to support the creation, build, and execution of systems. A large variety of dedicated model-transformation languages exists, promising to efficiently realize the automated processing of models. To investigate the actual benefit of using such specialized languages, we performed a large-scale controlled experiment in which over 78 subjects solve 231 individual tasks using three languages. The experiment sheds light on commonalities and differences between model transformation languages (ATL, QVT-O) and on benefits of using them in common development tasks (comprehension, change, and creation) against a modern general-purpose language (Xtend). Our results show no statistically significant benefit of using a dedicated transformation language over a modern general-purpose language. However, we were able to identify several aspects of transformation programming where domain-specific transformation languages do appear to help, including copying objects, context identification, and conditioning the computation on types. © 2018 Association for Computing Machinery.
"
10.1145/3239372.3239403,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056906027&origin=inward,Conference Paper,SCOPUS_ID:85056906027,scopus,2018-10-14,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),visualizations of evolving graphical models in the context of model review,"
AbstractView references

Code reviewing is well recognized as a valuable software engineering practice for improving software quality. Today a large variety of tools exist that support code reviewing and are widely adopted in open source and commercial software projects. They commonly support developers in manually inspecting code changes, providing feedback on and discussing these code changes, as well as tracking the review history. As source code is usually text-based, code reviewing tools also only support text-based artifacts. Hence, code changes are visualized textually and review comments are attached to text passages. This renders them unsuitable for reviewing graphical models, which are visualized graphically in diagrams instead of textually and hence require graphical change visualizations as well as annotation capabilities on the diagram level. Consequently, developers currently have to switch back and forth between code reviewing tools and comparison tools for graphical models to relate reviewer comments to model changes. Furthermore, adding and discussing reviewer comments on the diagram level is simply not possible. To improve this situation, we propose a set of coordinated visualizations of reviewing-relevant information for graphical models including model changes, diagram changes, review comments, and review history. The proposed visualizations have been implemented in a prototype tool called Mervin supporting the reviewing of graphical UML models developed with Eclipse Papyrus. Using this prototype, the proposed visualizations have been evaluated in a user study concerning effectiveness. The evaluation results show that the proposed visualizations can improve the review process of graphical models in terms of issue detection. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
10.1145/3239372.3239408,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056878573&origin=inward,Conference Paper,SCOPUS_ID:85056878573,scopus,2018-10-14,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards scalable model views on heterogeneous model resources,"
AbstractView references

When engineering complex systems, models are used to represent various systems aspects. These models are often heterogeneous in terms of modeling language, provenance, number or scale. They can be notably managed by different persistence frameworks adapted to their nature. As a result, the information relevant to engineers is usually split into several interrelated models. To be useful in practice, these models need to be integrated together to provide global views over the system under study. Model view approaches have been proposed to tackle such an issue. They provide an unification mechanism to combine and query heterogeneous models in a transparent way. These views usually target specific engineering tasks such as system design, monitoring, evolution, etc. In our present context, the MegaM@Rt2 industrially-supported European initiative defines a set of large-scale use cases where model views can be beneficial for tracing runtime and design time data. However, existing model view solutions mostly rely on in-memory constructs and low-level modeling APIs that have not been designed to scale in the context of large models stored in different kinds of sources. This paper presents the current status of our work towards a general solution to efficiently support scalable model views on heterogeneous model resources. It describes our integration approach between model view and model persistence frameworks. This notably implies the refinement of the view framework for the construction of large views from multiple model storage solutions. This also requires to study how parts of queries can be computed on the contributing models rather than on the view. Our solution has been benchmarked on a practical large-scale use case from the MegaM@Rt2 project, implementing a runtime - design time feedback loop. The corresponding EMF-based tooling support and modeling resources are fully available online. © 2018 Association for Computing Machinery.
"
10.1145/3239372.3239388,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056871758&origin=inward,Conference Paper,SCOPUS_ID:85056871758,scopus,2018-10-14,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),highly-optimizing and multi-target compiler for embedded system models :c++ compiler toolchain for the component and connector language embeddedmontiarc,"
AbstractView references

Component and Connector (C&C) models, with their corresponding code generators, are widely used by large automotive manufacturers to develop new software functions for embedded systems interacting with their environment; C&C example applications are engine control, remote parking pilots, and traffic sign assistance. This paper presents a complete toolchain to design and compile C&C models to highly-optimized code running on multiple targets including x86/x64, ARM and WebAssembly. One of our contributions are algebraic and threading optimizations to increase execution speed for computationally expensive tasks. A further contribution is an extensive case study with over 50 experiments. This case study compares the runtime speed of the generated code using different compilers and mathematical libraries. These experiments showed that programs produced by our compiler are at least two times faster than the ones compiled by MATLAB/Simulink for machine learning applications such as image clustering for object detection. Additionally, our compiler toolchain provides a complete model-based testing framework and plug-in points for middleware integration. We make all materials including models and toolchains electronically available for inspection and further research. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
10.1088/1742-6596/1098/1/012009,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056458760&origin=inward,Conference Paper,SCOPUS_ID:85056458760,scopus,2018-10-11,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design and implementation of object oriented large-scale finite element visual system,"
AbstractView references

As to the large scale visualized demands of Engineering finite element calculation system, a highly open visual software named HAJIF-PrePost is designed and implemented. In the architecture design, the hierarchy structure design of framework is detailed based on the object-oriented design technologies. In the pre-processing, Fem model data was documented and variable memory was managed by Boost which is similarly C++ standard lib. And the method of node-face correlativity and Depth-Buffer mechanism of OpenGL were used to improve visibility efficiency for Fem model. In the post-processing, the contour drawing, deforming and animation generation is design and implemented, then add rear query function. The practical application shows that it has a practical performance, user experience, and extensibility. © Published under licence by IOP Publishing Ltd.
"
10.1108/ICS-12-2017-0087,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056827054&origin=inward,Article,SCOPUS_ID:85056827054,scopus,2018-10-08,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),applying the physics of notation to the evaluation of a security and privacy requirements engineering methodology,"
AbstractView references

Purpose: The purpose of this study is the analysis of a security and privacy requirements engineering methodology. Such methodologies are considered an important part of systems’ development process when they contain and process a large amount of critical information, and thus need to remain secure and ensure privacy. Design/methodology/approach: These methodologies provide techniques, methods and norms for tackling security and privacy issues in information systems. In this process, the utilisation of effective, clear and understandable modelling languages with sufficient notation is of utmost importance, as the produced models are used not only among IT experts or among security specialists but also for communication among various stakeholders, in business environments or among novices in an academic environment. Findings: The qualitative analysis revealed a partial satisfaction of these principles. Originality/value: This paper evaluates the effectiveness of a security and privacy requirements engineering methodology, namely, Secure Tropos, on the nine principles of the theory of notation. © 2018, Emerald Publishing Limited.
"
10.1108/DTA-01-2018-0007,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052600461&origin=inward,Article,SCOPUS_ID:85052600461,scopus,2018-10-04,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards semantically-aided domain specific business process modeling,"
AbstractView references

Purpose: Domain-specific process modeling has been proposed in the literature as a solution to several problems in business process management. The problems arise when using only the generic Business Process Model and Notation (BPMN) standard for modeling. This language includes domain ambiguity and difficult long-term model evolution. Domain-specific modeling involves developing concept definitions, domain-specific processes and eventually industry-standard BPMN models. This entails a multi-layered modeling approach, where any of these artifacts can be modified by various stakeholders and changes done by one person may influence models used by others. There is therefore a need for tool support to keep track of changes done and their potential impacts. The paper aims to discuss these issues. Design/methodology/approach: The authors use a multi-context systems-based approach to infer the impacts that changes may cause in the models; and alsothe authors incrementally map components of business process models to ontologies. Findings: Advantages of the framework include: identifying conflicts/inconsistencies across different business modeling layers; expressing rich information on the relations between two layers; calculating the impact of changes taking place in one layer to the rest of the layers; and selecting incrementally the most appropriate semantic models on which the transformations can be based. Research limitations/implications: The authors consider this work as one of the foundational bricks that will enable further advances toward the governance of multi-layer business process modeling systems. Extensive usability tests would enable to further confirm the findings of the paper. Practical implications: The approach described here should improve the maintainability, reuse and clarity of business process models and in extension improve data governance in large organizations. The approaches described here should improve the maintainability, reuse and clarity of business process models. This can improve data governance in large organizations and for large collections of processes by aiding various stakeholders to understand problems with process evolutions, changes and inconsistencies with business goals. Originality/value: This paper fulfills an identified gap to enabling semantically aided domain–specific process modeling. © 2018, Emerald Publishing Limited.
"
10.1016/j.ascom.2018.09.004,S2213133718300325,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053837360&origin=inward,Article,SCOPUS_ID:85053837360,scopus,2018-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),galaxy detection and identification using deep learning and data augmentation,"
                  We present a method for automatic detection and classification of galaxies which includes a novel data-augmentation procedure to make trained models more robust against the data taken from different instruments and contrast-stretching functions. This method is shown as part of AstroCV, a growing open source computer vision repository for processing and analyzing big astronomical datasets, including high performance Python and C++ algorithms used in the areas of image processing and computer vision.
                  The underlying models were trained using convolutional neural networks and deep learning techniques, which provide better results than methods based on manual feature engineering and SVMs in most of the cases where training datasets are large. The detection and classification methods were trained end-to-end using public datasets such as the Sloan Digital Sky Survey (SDSS), the Galaxy Zoo, and private datasets such as the Next Generation Virgo (NGVS) and Fornax (NGFS) surveys.
                  Training results are strongly bound to the conversion method from raw FITS data for each band into a 3-channel color image. Therefore, we propose data augmentation for the training using 5 conversion methods. This greatly improves the overall galaxy detection and classification for images produced from different instruments, bands and data reduction procedures.
                  The detection and classification methods were trained using the deep learning framework DARKNET and the real-time object detection system YOLO. These methods are implemented in C language and CUDA platform, and makes intensive use of graphical processing units (GPU). Using a single high-end Nvidia GPU card, it can process a SDSS image in 50 ms and a DECam image in less than 3 s.
                  We provide the open source code, documentation, pre-trained networks, python tutorials, and how to train your own datasets, which can be found in the AstroCV repository. https://github.com/astroCV/astroCV.
               "
10.1109/SERA.2018.8477223,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055846320&origin=inward,Conference Paper,SCOPUS_ID:85055846320,scopus,2018-09-28,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),feature model synthesis from language-independent functional descriptions,"
AbstractView references

Software product lines (SPLs) identify and manage the commonalities and variability, called features, among the variants of products in a given domain. This reuse technique improves productivity factors such as reducing costs and time to market while enabling the derivation of particular applications that meet customers' needs by reusing the domains' artifacts. In this paper, we tackle the problem of SPL extraction from language-independent functional descriptions of existing product variants. Our contribution consists in synthesizing the SPL feature model from possibly incomplete requirements (use case diagrams, scenarios and functional requirements) of the product variants. To validate our approach, we applied it on five case studies: ArgoUML-SPL (small-scale system), Mobile Media-SPL and Messaging-SPL (medium scale systems), Health complaint-SPL and Crisis management-SPL (large scale systems). For this purpose, we used several releases from these domains as the considered product variants. Then, we applied our approach and evaluated its efficiency through measurements. © 2018 IEEE.
"
10.1016/j.compfluid.2018.06.005,S0045793018302950,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048936380&origin=inward,Article,SCOPUS_ID:85048936380,scopus,2018-09-15,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),domain-specific acceleration and auto-parallelization of legacy scientific code in fortran 77 using source-to-source compilation,"Massively parallel accelerators such as GPGPUs, manycores and FPGAs represent a powerful and affordable tool for scientists who look to speed up simulations of complex systems. However, porting code to such devices requires a detailed understanding of heterogeneous programming tools and effective strategies for parallelization. In this paper we present a source to source compilation approach with whole-program analysis to automatically transform single-threaded FORTRAN 77 legacy code into OpenCL-accelerated programs with parallelized kernels. The main contributions of our work are: (1) whole-source refactoring to allow any subroutine in the code to be offloaded to an accelerator. (2) Minimization of the data transfer between the host and the accelerator by eliminating redundant transfers. (3) Pragmatic auto-parallelization of the code to be offloaded to the accelerator by identification of parallelizable maps and reductions. We have validated the code transformation performance of the compiler on the NIST FORTRAN 78 test suite and several real-world codes: the Large Eddy Simulator for Urban Flows, a high-resolution turbulent flow model; the shallow water component of the ocean model Gmodel; the Linear Baroclinic Model, an atmospheric climate model and Flexpart-WRF, a particle dispersion simulator. The automatic parallelization component has been tested on as 2-D Shallow Water model (2DSW) and on the Large Eddy Simulator for Urban Flows (UFLES) and produces a complete OpenCL-enabled code base. The fully OpenCL-accelerated versions of the 2DSW and the UFLES are resp. 9x and 20x faster on GPU than the original code on CPU, in both cases this is the same performance as manually ported code."
10.1109/RAM.2018.8463007,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054135353&origin=inward,Conference Paper,SCOPUS_ID:85054135353,scopus,2018-09-11,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),modeling and valuation of contractual ram requirements using domain-specific languages,"
AbstractView references

Tender documents for supply or servicing of large and complex technical systems often stipulate contractual RAM requirements that may entail substantial costs for the contractor in case of non-conformance. Assessment of the potential financial impact due to these non-conformance costs before submission of a bid, as well as during contract commitment, is essential and should ideally be done in an efficient and transparent manner. Today, this is frequently not the case because the technical, contractual and commercial details of RAM requirements are commonly dealt with by different parties, who are specialists only for their domains and each of them use their own (occasionally simplifying or error-prone) techniques and approaches. In this paper, we present a novel approach that models contractual RAM requirements integrated with technical RAM aspects of a system, both specified in high-level domain languages. Our approach allows stakeholders such as bid managers and technical project managers to perform comprehensive analyses of the financial implications of different technical or contractual alternatives, and to gain a better understanding of different constraints and causes for costs, thus providing improved support for decisions. It is efficient and flexible due to the use of language engineering technologies as well as contract formalization approaches from the financial domain. Based on the high-level domain-specific languages (DSLs), appropriate computational models are automatically generated and the resulting financial risk evaluated and visualized. We illustrate our approach with a (hypothetical) example of a technical project. Last but not least we discuss variation points and generalization possibilities. © 2018 IEEE.
"
10.1109/JCSSE.2018.8457331,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85057736514&origin=inward,Conference Paper,SCOPUS_ID:85057736514,scopus,2018-09-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),enhance machine reading comprehension on multiple sentence questions with gated and dense coreference information,"
AbstractView references

Machine reading comprehension (MC) is one of the most important problems in natural language processing. Most of the previous works rely heavily on features engineering and handcrafting techniques. Since the release of SQuAD, a large-scale MC dataset, many deep learning models have been proposed. However, these models are limited by the soft attention mechanism only relied on keywords that appears in a question. Therefore, the performance is always poor in a question that needs to infer an answer from multiple sentences, which cannot depend on keywords in a question. In this paper, we propose a deep learning model that incorporates coreference information to improve the prediction performance especially on multiple sentence question. We also propose the bi-directional answering technique that can help the model avoid a local maxima of the single directional answering method in a traditional model. The results have shown that our approach outperforms the baseline in terms of F1 and Exact Match (EM). © 2018 IEEE.
"
10.1002/cae.22017,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053824805&origin=inward,Article,SCOPUS_ID:85053824805,scopus,2018-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),vrcdea-tcs: 3d virtual reality cooperative drawing educational application with textual chatting system,"
AbstractView references

Virtual reality (VR) products have noticeably improved in terms of quality and price in the last few years which made them more popular than ever. A number of wearable VR glasses are commercially available like Google cardboard. Moreover, leap motion is a small sensor that is connected to a USB port of a personal computer (PC) and allows the user to draw three dimensional (3D) shapes by tracking the movements of hands and fingers. On the other hand, Unity is a game engine which can be used to build 3D Games and VR applications using Java or C# programming languages. In this work, we present a novel 3D Virtual Reality Cooperative Drawing Educational Application with Textual Chatting System (VRCDEA-TCS), which is a drawing application (app) that allows engineering students to interact together via text messages in addition to the ability to draw 3D models either individually or cooperatively whereas each student is capable of working remotely in a common 3D drawing area with the options of adding or removing parts to the design. Interestingly, the proposed app uses Google cardboard and leap motion sensor to create a VR drawing area and works on both windows PCs and Android smart phones. To this end, the app has shown a reliable and solid performance after being used by a large number of students of the school of engineering at the University of Jordan in which their responses were very positive and encouraging toward the features and characteristics included in our work. © 2018 Wiley Periodicals, Inc.
"
10.1109/TCSS.2018.2859189,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052652949&origin=inward,Article,SCOPUS_ID:85052652949,scopus,2018-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),extreme-scale dynamic exploration of a distributed agent-based model with the emews framework,"
AbstractView references

Agent-based models (ABMs) integrate the multiple scales of behavior and data to produce higher order dynamic phenomena and are increasingly used in the study of important social complex systems in biomedicine, socioeconomics, and ecology/resource management. However, the development, validation, and use of ABMs are hampered by the need to execute very large numbers of simulations in order to identify their behavioral properties, a challenge accentuated by the computational cost of running realistic, large-scale, potentially distributed ABM simulations. In this paper, we describe the Extreme-scale Model Exploration with Swift (EMEWS) framework that is capable of efficiently composing and executing large ensembles of simulations and other 'black box' scientific applications while integrating model exploration (ME) algorithms developed with the use of widely available third-party libraries written in popular languages, such as R and Python. EMEWS combines novel stateful tasks with traditional run-to-completion many-task computing and solves many problems relevant to high-performance workflows, including scaling to very large numbers (millions) of tasks, maintaining state and locality information, and enabling effective multiple-language problem solving. We present the high-level programming model of the EMEWS framework and demonstrate how it is used to integrate an active learning ME algorithm to dynamically and efficiently characterize the parameter space of a large and complex, distributed message passing interface agent-based infectious disease model. © 2014 IEEE.
"
10.1016/j.cl.2018.02.002,S1477842417301690,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85043506483&origin=inward,Article,SCOPUS_ID:85043506483,scopus,2018-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated modelling assistance by integrating heterogeneous information sources,"
                  Model-Driven Engineering (MDE) uses models as its main assets in the software development process. The structure of a model is described through a meta-model. Even though modelling and meta-modelling are recurrent activities in MDE and a vast amount of MDE tools exist nowadays, they are tasks typically performed in an unassisted way. Usually, these tools cannot extract useful knowledge available in heterogeneous information sources like XML, RDF, CSV or other models and meta-models.
                  We propose an approach to provide modelling and meta-modelling assistance. The approach gathers heterogeneous information sources in various technological spaces, and represents them uniformly in a common data model. This enables their uniform querying, by means of an extensible mechanism, which can make use of services, e.g., for synonym search and word sense analysis. The query results can then be easily incorporated into the (meta-)model being built. The approach has been realized in the Extremo tool, developed as an Eclipse plugin.
                  
                     Extremo has been validated in the context of two domains – production systems and process modelling – taking into account a large and complex industrial standard for classification and product description. Further validation results indicate that the integration of Extremo in various modelling environments can be achieved with low effort, and that the tool is able to handle information from most existing technological spaces.
               "
10.1007/s11277-018-5241-4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041796008&origin=inward,Article,SCOPUS_ID:85041796008,scopus,2018-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),from act to utterance: a research on linguistic act convergence,"
AbstractView references

This paper presents an act–utterance convergence between children and their guardians (an umbrella term for the adults who serve as children’s interaction partner in this paper). Act and utterance are two different carriers of intention. Some convergence patterns exist in act–utterance interaction between young children and their guardians. Guardians tend to respond to their children by describing what the children are doing, reflecting this convergence. This research has built the A–U Assimilation Model to assimilate act and utterance and narrow the gap of expression between act and utterance while keeping their meanings unchanged. The unified expression framework in this model makes the act and utterance comparable. The research analyzed the convergence between act and utterance from children’s preverbal period to the age of 60 months by adopting a large-scale corpus. And the analysis of the convergence between act and utterance is conducted by the Doc2Vec in a neural network. The experiment results show that the degree of guardian-led act–utterance convergence increases rapidly during the period that children begin to learn to speak and decrease gradually as children become more proficient language learners and users. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.
"
10.1145/3230833.3232799,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055265185&origin=inward,Conference Paper,SCOPUS_ID:85055265185,scopus,2018-08-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a meta language for threat modeling and attack simulations,"
AbstractView references

Attack simulations may be used to assess the cyber security of systems. In such simulations, the steps taken by an attacker in order to compromise sensitive system assets are traced, and a time estimate may be computed from the initial step to the compromise of assets of interest. Attack graphs constitute a suitable formalism for the modeling of attack steps and their dependencies, allowing the subsequent simulation. To avoid the costly proposition of building new attack graphs for each system of a given type, domain-specific attack languages may be used. These languages codify the generic attack logic of the considered domain, thus facilitating the modeling, or instantiation, of a specific system in the domain. Examples of possible cyber security domains suitable for domain-specific attack languages are generic types such as cloud systems or embedded systems but may also be highly specialized kinds, e.g. Ubuntu installations; the objects of interest as well as the attack logic will differ significantly between such domains. In this paper, we present the Meta Attack Language (MAL), which may be used to design domain-specific attack languages such as the aforementioned. The MAL provides a formalism that allows the semi-automated generation as well as the efficient computation of very large attack graphs. We declare the formal background to MAL, define its syntax and semantics, exemplify its use with a small domain-specific language and instance model, and report on the computational performance. © 2018 Association for Computing Machinery.
"
10.1109/ICSA-C.2018.00043,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052597700&origin=inward,Conference Paper,SCOPUS_ID:85052597700,scopus,2018-08-09,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),development of a functional safety software layer for the control of an electric in-wheel motor based powertrain,"
AbstractView references

This paper describes the development of a software layer which monitors the functional behavior of an electric in-wheel motor controller. Due to the large amount of software, sensors and actuators present in such a powertrain system, the risk of E/E failures that cause hazardous situations needs to be considered. To this end, a software safety layer is developed which detects and controls safety goal violations during runtime. This is realized using a model-based design methodology in accordance with ISO 26262 part 6: Product Development on the Software Level. This paper describes the steps taken in the design and implementation of this functional safety monitoring layer, from requirements modelling in SysML to a MATLAB Simulink model suitable for production code generation. © 2018 IEEE.
"
10.1109/IPDPS.2018.00056,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052232792&origin=inward,Conference Paper,SCOPUS_ID:85052232792,scopus,2018-08-03,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),babelflow: an embedded domain specific language for parallel analysis and visualization,"
AbstractView references

The rapid growth in simulation data requires large-scale parallel implementations of scientific analysis and visualization algorithms, both to produce results within an acceptable timeframe and to enable in situ deployment. However, efficient and scalable implementations, especially of more complex analysis approaches, require not only advanced algorithms, but also an in-depth knowledge of the underlying runtime. Furthermore, different machine configurations and different applications may favor different runtimes, i.e., MPI vs Charm++ vs Legion, etc., and different hardware architectures. This diversity makes developing and maintaining a broadly applicable analysis software infrastructure challenging. We address some of these problems by explicitly separating the implementation of individual tasks of an algorithm from the dataflow connecting these tasks. In particular, we present an embedded domain specific language (EDSL) to describe algorithms using a new task graph abstraction. This task graph is then executed on top of one of several available runtimes (MPI, Charm++, Legion) using a thin layer of library calls. We demonstrate the flexibility and performance of this approach using three different large scale analysis and visualization use cases, i.e., topological analysis, rendering and compositing dataflow, and image registration of large microscopy scans. Despite the unavoidable overheads of a generic solution, our approach demonstrates performance portability at scale, and, in some cases, outperforms hand-optimized implementations. © 2018 IEEE.
"
10.1016/j.ejor.2017.04.039,S0377221717303867,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85044500687&origin=inward,Article,SCOPUS_ID:85044500687,scopus,2018-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),structured democratic dialogue: an application of a mathematical problem structuring method to facilitate reforms with local authorities in cyprus,"
                  This paper reports on a Community Operational Research (Community OR) project consisting of ten applications of a problem structuring method (PSM) with the Local Government Authorities of Cyprus. The PSM, Structured Democratic Dialogue Process (SDDP), is a systemic methodology that sits somewhere between Soft OR and traditional OR methods. It uses natural language constructs to support stakeholders explore similarity and influence relations between their distinct observations, and directed graphs to illustrate and communicate the consensus results. Matrix operations that take place behind the scenes make it possible for people from all walks of life to deal with complex societal problems without needing to master systems science. The application of the SDDP methodology in the case of the Local Government Authorities of Cyprus created the trust and the momentum necessary to achieve large-scale consensus and facilitate envisioned societal reforms. SDDP may have value for Community OR more broadly because of its emphasis on meaningful stakeholder and community participation.
               "
10.1051/matecconf/201818402018,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054564365&origin=inward,Conference Paper,SCOPUS_ID:85054564365,scopus,2018-07-31,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"comparison between proportional, integral, derivative controller and fuzzy logic approaches on controlling quarter car suspension system","
AbstractView references

Developing and constantly changing technologies, efforts to achieve maximum efficiency with minimum fuel consumption, as well as the development of comfort and safety systems, have become very essential topic in car manufacturing and design. Whereas comfort and security were not given a high importance in the first produced cars, they are indispensable elements of today's automobiles. Since public transportation uses road in large scale, the need for safety and repose is also increasing. Nowadays, vehicles have better security and comfort systems, which react very quickly to all kinds of loads and different cases of driving (braking, acceleration, high speed, cornering), where the tires can keep the road at its best, utilizing an advanced suspension system. In this study, a quarter-car model was fulfilled using LabVIEW (Laboratory Virtual Instrumentation Engineering Workbench) software. The control of this model has been realized by applying two different controllers. PID (proportional, integral, derivative) controller which is a common and conventional control method and the Fuzzy Logic controller which is considered as an expert system that is becoming more and more widely used. In both control approaches, controlling the suspension system was achieved successfully. However; It has been determined that controlling the system using Fuzzy Logic controller gave better dynamic response than applying the PID controller for the quarter car suspension model that has been used in the direction of this study. © The Authors, published by EDP Sciences, 2018.
"
10.1109/MERCon.2018.8421939,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051488705&origin=inward,Conference Paper,SCOPUS_ID:85051488705,scopus,2018-07-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),transliteration and byte pair encoding to improve tamil to sinhala neural machine translation,"
AbstractView references

Neural Machine Translation (NMT) is the current state-of-the-art machine translation technique. However, applicability of NMT for language pairs that have high morphological variations is still debatable. Lack of language resources, especially a sufficiently large parallel corpus causes additional issues, which leads to very poor translation performance, when NMT is applied to languages with high morphological variations. In this paper, we present three techniques to improve domain-specific NMT performance of the under-resourced language pair Sinhala and Tamil that have high morphological variations. Out of these three techniques, transliteration is a novel approach to improve domain-specific NMT performance for language pairs such as Sinhala and Tamil that share a common grammatical structure and have moderate lexical similarity. We built the first transliteration system for Sinhala to English and Tamil to English, which provided an accuracy of 99.6%, when tested with the parallel corpus we used for NMT training. The other technique we employed is Byte Pair Encoding (BPE), which is a technique that has been used to achieve open vocabulary translation with a fixed vocabulary of subword symbols. Our experiments show that while the translation based on independent BPE models and pure transliteration perform moderately, integrating transliteration to build a joint BPE model for the aforementioned language pair increases the translation quality by 1.68 BLEU score. © 2018 IEEE.
"
10.1109/ICSA.2018.00025,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051103640&origin=inward,Conference Paper,SCOPUS_ID:85051103640,scopus,2018-07-20,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),infrastructure-as-code for data-intensive architectures: a model-driven development approach,"
AbstractView references

As part of the DevOps tactics, Infrastructure-as-Code (IaC) provides the ability to create, configure, and manage complex infrastructures by means of executable code. Writing IaC, however, is not an easy task, since it requires blending different infrastructure programming languages and abstractions, each specialized on a particular aspect of infrastructure creation, configuration, and management. Moreover, the more the architectures become large and complex (e.g. Data-Intensive or Microservice-based architectures), the more dire the need of IaC becomes. The goal of this paper is to exploit Model-Driven Engineering (MDE) to create language-agnostic models that are then automatically transformed into IaC. We focus on the domain of Data-Intensive Applications as these typically exploit complex infrastructures which demand sophisticated and fine-grained configuration and re-configuration - we show that, through our approach, called DICER, it is possible to create complex IaC with significant amounts of time savings, both in IaC design as well as deployment and re-deployment times. © 2018 IEEE.
"
10.1109/EuroSP.2018.00039,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050745134&origin=inward,Conference Paper,SCOPUS_ID:85050745134,scopus,2018-07-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chainsmith: automatically learning the semantics of malicious campaigns by mining threat intelligence reports,"
AbstractView references

Modern cyber attacks consist of a series of steps and are generally part of larger campaigns. Large-scale field data provides a quantitative measurement of these campaigns. On the other hand, security practitioners extract and report qualitative campaign characteristics manually. Linking the two sources provides new insights about attacker strategies from measurements. However, this is a time-consuming task because qualitative measurements are generally reported in natural language and are not machine-readable. We propose an approach to bridge measurement data with manual analysis. We borrow the idea from threat intelligence: we define campaigns using a 4-stage model, and describe each stage using IOCs (indicators of compromise), e.g. URLs and IP addresses. We train a multi-class classifier to extract IOCs and further categorize them into different stages. We implement these ideas in a system called ChainSmith. Our system can achieve 91.9% precision and 97.8% recall in extracting IOCs, and can determine the campaign roles for 86.2% of IOCs with 78.2% precision and 80.7% recall. We run ChainSmith on 14,155 online security articles, from which we collect 24,653 IOCs. The semantic roles allow us to link manual attack analysis with large scale field measurements. In particular, we study the effectiveness of different persuasion techniques used on enticing user to download the payloads. We find that the campaign usually starts from social engineering and 'missing codec' ruse is a common persuasion technique that generates the most suspicious downloads each day. © 2018 IEEE.
"
10.1145/3282308.3282318,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061284518&origin=inward,Conference Paper,SCOPUS_ID:85061284518,scopus,2018-07-04,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),patterns for discussing and modelling variability in business processes,"
AbstractView references

Expressing variability in graphical process models can lead to large and complicated models even for expressing rather simple situations. However, expressing variability in process models is important in many processes. During the execution of a process, it is not uncommon that knowledge workers can decide for additional steps, change the execution order or skip a task. In this paper, we propose a set of business process variability patterns to express those situations. When communicating with business experts, the patterns can be used as building blocks which serve as placeholder for syntactically well-defined (but more complicated) model fragments in a formal language such as BPMN or CMMN. Those building bloc ks can be used for communication and later be transformed into a formal modelling language. We believe that the patterns can be useful for supporting the communication between process analysts and stakeholders who are not familiar with formal process modeling languages. If necessary, the informal models that are created in a lightweight modeling language can later be transformed into more formal models. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
"
10.1109/APSEC.2018.00027,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85066815090&origin=inward,Conference Paper,SCOPUS_ID:85066815090,scopus,2018-07-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),scalable verification framework for c program,"
AbstractView references

Software verification has been well applied in safety critical areas and has shown the ability to provide better quality assurance for modern software. However, as lines of code and complexity of software systems increase, the scalability of verification becomes a challenge. In this paper, we present an automatic software verification framework TSV to address the scalability issues: (i) the extended structural abstraction and property-guided program slicing to solve large-scale program verification problem, saving time and memory without losing accuracy; (ii) automatically select different verification methods according to the program and property context to improve the verification efficiency. For evaluation, we compare TSV's different configurations with existing C program verifiers based on open benchmarks. We found that TSV with auto-selection performs better than with bounded model checking only or with extended structural abstraction only. What's more, TSV with auto selection achieves a better balance of accuracy, time and memory consumption. © 2018 IEEE.
"
10.1109/ITMC.2018.8691271,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065037574&origin=inward,Conference Paper,SCOPUS_ID:85065037574,scopus,2018-07-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the internet of things: overview of the essential elements and the new enabling technology 6lowpan,"
AbstractView references

The Internet has been a great success over the past twenty years, growing from a small academic network into a global one. It is regularly used by over 4 billion people worldwide. The innovation of the World Wide Web (WWW) model, the hypertext transfer protocol (HTTP) and the hypertext markup language (HTML) led to an exponential growth of the objects and users connected to the Internet. This paper provides an overview of the new technology called »The Internet of Things» (IoT), which is considered to be the next major big opportunity and challenge for the Internet engineering community, users of technology, companies, and society. The paper begins by presenting the purpose of the Internet and how a large number of users and objects have joined it since the time of its conception. The next section provides a definition of the IoT with an emphasis on the basic elements required in the IoT environment, related trends, and the key issues. Also we examine the introduction of IPv6 in the IoT architecture through the 6LowPAN networking technology adaptation layer. This allows IPv6 packets to be carried within small link layer frames, such as those defined by IEEE 802.15.4. As a result, 6LowPAN permits the use of an end-to-end IP-based infrastructure which led to take full advantage of more than thirty years of IP technology development, facilitating open standards and interoperability as largely demonstrated through the daily use of the Internet. © 2018 IEEE.
"
10.1109/BigData.2018.8622512,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062594168&origin=inward,Conference Paper,SCOPUS_ID:85062594168,scopus,2018-07-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),il-net: using expert knowledge to guide the design of furcated neural networks,"
AbstractView references

Deep neural networks (DNN) excel at extracting patterns. Through representation learning and automated feature engineering on large datasets, such models have been highly successful in computer vision and natural language applications. Designing optimal network architectures from a principled or rational approach, however, has been less than successful, with the best successful approaches utilizing an additional machine learning algorithm to tune the network hyperparameters. This is despite that, in many technical fields, there exist established domain knowledge and understanding about the subject matter. In this work, we develop a novel furcated neural network architecture that utilizes domain knowledge as high-level design principles of the network. We demonstrate proof-of-concept by developing IL-Net, a furcated network for predicting the properties of ionic liquids, which is a class of complex multi-chemical entities. Compared to existing state-of-the-art approaches, we show that furcated networks can improve model accuracy by approximately ~20-35%, without using additional labeled data. Lastly, we distill two key design principles for furcated networks that can be adapted to other domains. © 2018 IEEE.
"
10.1109/IEMCON.2018.8615096,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062073815&origin=inward,Conference Paper,SCOPUS_ID:85062073815,scopus,2018-07-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),business process models to web services generation: a systematic literature review,"
AbstractView references

Business process automation is complex activity especially while dealing with large and composite processes. To simplify the automation process, the business requirements are frequently model and verified in early stages. In this context, Business Process Modelling Notation (BPMN) is a renowned language particularly used for the modelling of business processes. Subsequently, the BPMN models are transformed to target models for further verification and deployment. Therefore, in this article, a Systematic Literature Review (SLR) is performed to investigate BPMN features, SoaML constructs used for services specification and tools for service generation. Consequently, 30 studies published during 2009-2018 are selected and analysed. This leads to identify 12 leading BPMN modelling constructs in the context of service generation. Moreover, 4 model transformations and 8 service generation techniques are identified. Furthermore, 6 BPMN modelling, 10 transformations, 5 service generation and 2 testing tools are presented. Finally, a comparative analysis of BPMN modelling tools is performed. It is concluded that business process automation became easy with the help of model representation and these models can be used for services generation. The finds of the article are highly beneficial for the researchers and practitioners of the domain. © 2018 IEEE.
"
10.1007/s00354-018-0038-2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050667358&origin=inward,Article,SCOPUS_ID:85050667358,scopus,2018-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generation of reversible c++ code for optimistic parallel discrete event simulation,"
AbstractView references

The reversible execution of C/C++ code has been a target of research and engineering for more than a decade as reversible computation has become a central notion in large-scale parallel discrete event simulation (PDES). The simulation models that are implemented for PDES are of increasing complexity and size and require various language features to support abstraction, encapsulation, and composition when building a simulation model. In this paper, we focus on parallel simulation models that are written with user-defined C++ abstractions and abstractions of the C++ Standard Library. We present an approach based on incremental state saving for establishing reversibility of C++ and an evaluation for a kinetic Monte-Carlo simulation implemented in C++. Although a significant runtime overhead is introduced with our technique, it is an enormous win that it allows using the entire C++ language, and has that code automatically transformed into reversible code to enable parallel execution with the Rensselaer’s optimistic simulation system (ROSS). © 2018, Ohmsha, Ltd. and Springer Japan KK, part of Springer Nature.
"
10.1109/ICAIBD.2018.8396176,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050187276&origin=inward,Conference Paper,SCOPUS_ID:85050187276,scopus,2018-06-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an improved deep neural network model for job matching,"
AbstractView references

Job matching which benefit job seekers, employees and employers is very important today. In this work, a deep neural network model is proposed to predict an employee's future career details, which includes position name, salary and company scale based on the online resume data. Like most NLP tasks, the input features are multi-field, non-sparse, discrete and categorical, while their dependencies are mostly unknown. Previous works were mostly focused on engineering, which resulted in a large feature space and heavy computation. To solve this task, we use embedding layers to explore feature interactions and merge two automatically learned features extracted from the resumes. Experimental results on over 70,000 real-word online resumes show that our model outperforms shallow models, like SVM and Random Forests, in effectiveness and accuracy. © 2018 IEEE.
"
10.1145/3182393,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050850495&origin=inward,Article,SCOPUS_ID:85050850495,scopus,2018-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a computational architecture for coupling heterogeneous numerical models and computing coupled derivatives,"
AbstractView references

One of the challenges in computational modeling is coupling models to solve multidisciplinary problems. Flow-based computational frameworks alleviate part of the challenge through a modular approach, where data flows from component to component. However, existing flow-based frameworks are inefficient when coupled derivatives are needed for optimization. To address this, we develop the modular analysis and unified derivatives (MAUD) architecture. MAUD formulates the multidisciplinary model as a nonlinear system of equations, which leads to a linear equation that unifies all methods for computing derivatives. This enables flow-based frameworks that use the MAUD architecture to provide a common interface for the chain rule, adjoint method, coupled adjoint method, and hybrid methods; MAUD automatically uses the appropriate method for the problem. A hierarchical, matrix-free approach enables modern solution techniques such as Newton-Krylov solvers to be used within this monolithic formulation without computational overhead. Two demonstration problems are solved using a Python implementation of MAUD: a nanosatellite optimization with more than 2 million unknowns and 25,000 design variables, and an aircraft optimization involving over 6,000 design variables and 23,000 constraints. MAUD is now implemented in the open source framework OpenMDAO, which has been used to solve aircraft, satellite, wind turbine, and turbofan engine design problems. 2018 Copyright is held by the owner/author(s). © 2018 ACM. All rights reserved.
"
10.1177/0954406217718858,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85045441062&origin=inward,Article,SCOPUS_ID:85045441062,scopus,2018-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a semantic model for axiomatic systems design,"
AbstractView references

Design of large-scale engineering systems such as an automobile, satellite, or airplane is a process to satisfy requirements by making various decisions. Design axioms provide system designers with a theoretical background to make right decisions. However, the axiomatic systems design is still hard to be implemented in the real word due to its informal representation for both the human and machine, and few researches focus on formalizing concepts of this process. In order to define axiomatic systems design models to be both user-understandable and machine-readable, this paper combines axiomatic design process with the Semantic Web technology and proposes an axiomatic design semantic representation model, called axiomatic design ontology, which organizes customers’ requirements, functional requirements, design parameters, and design solutions. The class of concepts elements and their semantic relationships are defined by the Web Ontology Language (OWL2). Rules for identifying functional couplings (the Independence Axiom) and selecting the optimal design solution (the Information Axiom) are formally represented and encoded with the Semantic Web Rule Language, which enhances the reasoning capability of the axiomatic design ontology. A framework for capturing systems design semantic information based on the axiomatic design ontology, and aligning it with domain-specific ontologies according to the semantic mapping approach has been developed, by which elaborated design information is captured and shared. Finally, a case study of systems design of a satellite solar wing subsystem is given to demonstrate the proposed axiomatic design ontology-based systems design approach. © 2017, IMechE 2017.
"
10.1007/s10664-017-9543-z,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85029748378&origin=inward,Article,SCOPUS_ID:85029748378,scopus,2018-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),general methods for software architecture recovery: a potential approach and its evaluation,"
AbstractView references

Software architecture is a critical artefact in the software lifecycle. It is a system blueprint for construction, it aids in planning teaming and division of work, and it aids in reasoning about system properties. But architecture documentation is seldom created and, even when it is initially created, it is seldom maintained. For these reasons organisations often feel the need to recover legacy architectures, for example, as part of planning for evolution or cloud migration. But there is no existing general architecture recovery approach nor tool that can be applied to any type of system, under any condition. We will show that one way of achieving such generality is to apply systematic code inspection following a Grounded Theory (GT) approach. Though relatively costly and human-intensive, a GT-based approach has several merits, for example: (a) it is general by design; (b) it can be partially automated; (c) it yields evidence-based results rooted of the system being examined. This article presents one theoretical formulation of a general architecture recovery method–called REM–and reports on the evaluation of REM in the context of a large architecture recovery campaign performed for the European Space Agency. Our results illustrate some intriguing properties and opportunities of GT-based architecture recovery approaches and point out lessons learned and venues for further research. © 2017, Springer Science+Business Media, LLC.
"
10.1145/3196321.3196343,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051633907&origin=inward,Conference Paper,SCOPUS_ID:85051633907,scopus,2018-05-28,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),hierarchical abstraction of execution traces for program comprehension,"
AbstractView references

Understanding the dynamic behavior of a software system is one of the most important and time-consuming tasks for today's software maintainers. In practice, understanding the inner workings of software requires studying the source code and documentation and inserting logging code in order to map high-level descriptions of the program behavior with low-level implementation, i.e., the source code. Unfortunately, for large codebases and large log files, such cognitive mapping can be quite challenging. To bridge the cognitive gap between the source code and detailed models of program behavior, we propose a fully automatic approach to present a semantic abstraction with different levels of functional granularity from full execution traces. Our approach builds multi-level abstractions and identifies frequent behaviors at each level based on a number of execution traces, and then, it labels phases within individual execution traces according to the identified major functional behaviors of the system. To validate our approach, we conducted a case study on a large-scale subject program, Javac, to demonstrate the effectiveness of the mining result. Furthermore, the results of a user study demonstrate that our approach is capable of presenting users a high-level comprehensible abstraction of execution behavior. Based on a real world subject program the participants in our user study were able to achieve a mean accuracy of 70%. © 2018 ACM.
"
10.1145/3196558.3196565,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051240612&origin=inward,Conference Paper,SCOPUS_ID:85051240612,scopus,2018-05-28,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),engineering safety in swarm robotics,"
AbstractView references

Robotics, artificial intelligence, and the Internet-of-Things are driving current research and development for the technology sector. Robotic and multi-robot systems are becoming pervasive and more and more lives rely on their proper functioning in transportation, medical systems, personal robotics, and manufacturing. Assuring the security and safety of these systems is of primary importance to guarantee the real-world applicability of current research, and we argue that it should be an integral part of system design. Current software standards for safety and security for critical systems (e.g. industrial and aerospace) are not directly applicable to the large distributed systems that are envisioned for the near future. In this paper, we propose to address safety and security of swarm robotics systems at the programming language level. We propose to extend the Buzz multi-robot scripting language with constructs and code analysis that allow the verification of safety and security during development. We believe that detecting and correcting issues with what are inherently emergent systems - i.e. where collective behavior might not be immediately apparent from a single robot's code - -during development would allow for a more effective advancement of swarm robotics. © 2018 ACM.
"
10.1145/3183440.3183448,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85049695716&origin=inward,Conference Paper,SCOPUS_ID:85049695716,scopus,2018-05-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),interactive model mining from embedded legacy software,"
AbstractView references

Model mining from software systems can be very helpful for program comprehension. The few existing approaches for extracting high level models from code-when applied to real-world systems written in C-deliver too detailed and complex models that cannot be understood by humans. In my Ph.D. project, I propose an approach that complements fully-Automatic model mining approaches with user interaction to get understandable models. The evaluation of this approach includes a controlled experiment with a large number of experts, in order to assess the effectiveness of the interactively mined models for understanding complex legacy software. © 2018 ACM.
"
10.1145/3180155.3180231,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85049395416&origin=inward,Conference Paper,SCOPUS_ID:85049395416,scopus,2018-05-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automatically finding bugs in a commercial cyber-physical system development tool chain with slforge,"
AbstractView references

Cyber-physical system (CPS) development tool chains are widely used in the design, simulation, and verification of CPS data-flow models. Commercial CPS tool chains such as MathWorks' Simulink generate artifacts such as code binaries that are widely deployed in embedded systems. Hardening such tool chains by testing is crucial since formally verifying them is currently infeasible. Existing differential testing frameworks such as CyFuzz can not generate models rich in language features, partly because these tool chains do not leverage the available informal Simulink specifications. Furthermore, no study of existing Simulink models is available, which could guide CyFuzz to generate realistic models. To address these shortcomings, we created the first large collection of public Simulink models and used the collected models' properties to guide random model generation. To further guide model generation we systematically collected semi-formal Simulink specifications. In our experiments on several hundred models, the resulting SLforge generator was more effective and efficient than the state-of-the-art tool CyFuzz. SLforge also found 8 new confirmed bugs in Simulink. © 2018 ACM.
"
10.1109/ICST.2018.00014,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048413411&origin=inward,Conference Paper,SCOPUS_ID:85048413411,scopus,2018-05-25,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),characterizing defective configuration scripts used for continuous deployment,"
AbstractView references

In software engineering, validation and verification (V&V) resources are limited and characterization of defective software source files can help in efficiently allocating V&V resources. Similar to software source files, defects occur in the scripts used to automatically manage configurations and software deployment infrastructure, often known as infrastructure as code (IaC) scripts. Defects in IaC scripts can have dire consequences, for example, creating large-scale system outages. Identifying the characteristics of defective IaC scripts can help in mitigating these defects by allocating V&V efforts efficiently based upon these characteristics. The objective of this paper is to help software practitioners to prioritize validation and verification efforts for infrastructure as code (IaC) scripts by identifying the characteristics of defective IaC scripts. Researchers have previously extracted text features to characterize defective software source files written in general purpose programming languages. We investigate if text features can be used to identify properties that characterize defective IaC scripts. We use two text mining techniques to extract text features from IaC scripts: The bag-of-words technique, and the term frequency-inverse document frequency (TF-IDF) technique. Using the extracted features and applying grounded theory, we characterize defective IaC scripts. We also use the text features to build defect prediction models with tuned statistical learners. We mine open source repositories from Mozilla, Openstack, and Wikimedia Commons, to construct three case studies and evaluate our methodology. We identify three properties that characterize defective IaC scripts: Filesystem operations, infrastructure provisioning, and managing user accounts. Using the bag-of-word technique, we observe a median F-Measure of 0.74, 0.71, and 0.73, respectively, for Mozilla, Openstack, and Wikimedia Commons. Using the TF-IDF technique, we observe a median F-Measure of 0.72, 0.74, and 0.70, respectively, for Mozilla, Openstack, and Wikimedia Commons. © 2018 IEEE.
"
10.1109/DINWC.2018.8357002,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048378189&origin=inward,Conference Paper,SCOPUS_ID:85048378189,scopus,2018-05-09,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a study of new techniques for learning management system to accelerate language acquisition using structural visual models,"
AbstractView references

Language skills are the basis for communication and thinking. Language acquisition is a complex process which includes a large number of different parameters. Therefore, the study and improvement of language learning and teaching require creative collaboration between experts from different domains. We propose to transfer knowledge about the structure of the language from the verbal to the visual form, thereby creating the opportunity to use them as an indicative basis for planning, managing, controlling and correcting the training of primary language skills both by the teacher and by the student himself. The proposed method allows to develop the ability to organize sentences to convey meaning by means of Visual Models and to describe the sequential steps to choose the most effective ways for working with audio to improve listening skills. The aim of our study is to put principles for building a new effective system using new methods for acquiring language skills. © 2018 IEEE.
"
10.1016/j.scico.2017.12.007,S0167642317302927,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042858862&origin=inward,Article,SCOPUS_ID:85042858862,scopus,2018-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards the systematic analysis of non-functional properties in model-based engineering for real-time embedded systems,"The real-time scheduling theory provides analytical methods to assess the temporal predictability of embedded systems. Nevertheless, their use is limited in a Model-Based Systems Engineering approach. In fact, the large number of applicability conditions makes the use of real-time scheduling analysis tedious and error-prone. Key issues are left to the engineers: when to apply a real-time scheduling analysis? What to do with the analysis results? This article presents an approach to systematize and then automate the analysis of non-functional properties in Model-Based Systems Engineering. First, preconditions and postconditions define the applicability of an analysis. In addition, contracts specify the analysis interfaces, thereby enabling to reason about the analysis process. We present a proof-of-concept implementation of our approach using a combination of constraint languages (REAL for run-time analysis) and specification languages (Alloy for describing interfaces and reasoning about them). This approach is experimented on architectural models written with the Architecture Analysis and Design Language (AADL)."
10.1007/s10270-017-0585-x,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85012154527&origin=inward,Article,SCOPUS_ID:85012154527,scopus,2018-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using uml/marte to support performance tuning and stress testing in real-time systems,"
AbstractView references

Real-time embedded systems (RTESs) operating in safety-critical domains have to satisfy strict performance requirements in terms of task deadlines, response time, and CPU usage. Two of the main factors affecting the satisfaction of these requirements are the configuration parameters regulating how the system interacts with hardware devices, and the external events triggering the system tasks. In particular, it is necessary to carefully tune the parameters in order to ensure a satisfactory trade-off between responsiveness and usage of computational resources, and also to stress test the system with worst-case inputs likely to violate the requirements. Performance tuning and stress testing are usually manual, time-consuming, and error-prone processes, because the system parameters and input values range in a large domain, and their impact over performance is hard to predict without executing the system. In this paper, we provide an approach, based on UML/MARTE, to support the generation of system configurations predicted to achieve a satisfactory trade-off between response time and CPU usage, and stress test cases that push the system tasks to violate their deadlines. First, we devise a conceptual model that specifies the abstractions required for analyzing task deadlines, response time, and CPU usage, and provide a mapping between these abstractions and UML/MARTE. Then, we prune the UML/MARTE metamodel to only contain a purpose-specific subset of entities needed to support performance tuning and stress testing. The pruned version is a supertype of UML/MARTE, which ensures that all instances of the pruned metamodel are also instances of UML/MARTE. Finally, we cast the generation of configurations and stress test cases as two constrained optimization problems (COPs) over our conceptual model. The input data for these COPs in automatically generated via a model-to-text (M2T) transformation from models specified in the pruned UML/MARTE metamodel to the Optimization Programming Language. We validate our approach in a safety-critical RTES from the maritime and energy domain, showing that (1) our conceptual model can be applied in an industrial setting with reasonable effort, and (2) the optimization problems effectively identify configurations predicted to minimize response time and CPU usage, and stress test cases that maximize deadline misses. Based on our experience, we highlight challenges and potential issues to be aware of when using UML/MARTE to support performance tuning and stress testing in an industrial context. © 2017, Springer-Verlag Berlin Heidelberg.
"
10.1145/3184558.3190665,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85085170401&origin=inward,Conference Paper,SCOPUS_ID:85085170401,scopus,2018-04-23,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),panel on cognitive service engineering,"
AbstractView references

Cognitive services and conversational digital assistants are emerging as the engine that powers natural interactions between humans, software services, devices and ""things"" - supported by advances in AI and human computations. Not surprisingly, many large and small tech companies are rushing to occupy this space by providing platforms for building cognitive services and conversational bots. Digital assistants interact in a natural way (through text or voice) with both software and humans to get information and perform actions, from checking the weather to booking a restaurants and a cab ride, managing cloud resources, answering simple scientific questions, and preparing a decaf latte using IoT enabled coffee machines. User requests or tasks are often expressed in natural language, an interaction ensues to clarify the intent and the details, and the answer is sought - or the appropriate service or device is invoked - based on the cognitive service understanding. While the potential of this new wave of services is exciting, it also brings significant challenges: we are far away from the comfort of developing deterministic software that responds to API calls by invoking other APIs. Now we have to understand, guess, explore options, take decisions based on probabilistic models over a large set of possible intents and services, all while engaging with users. Doing so brings a large set of engineering challenges related to the development, training, tuning and evolution of such services. This panel will discuss such challenges and identify interesting opportunities for research as well as promising trends. © 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License.
"
10.1145/3170427.3170629,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052024645&origin=inward,Conference Paper,SCOPUS_ID:85052024645,scopus,2018-04-20,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),secret lives of data publics: mixed reality smart city interfaces,"
AbstractView references

Conventional smart city design processes tend to focus on instrumental planning for city systems or novel services for humans. Interacting with data produced by the new services and restructured systems entailed by these processes is commonly done via interfaces like civic dashboards, leading to a critique that data-driven urbanism is bound by the rules and constraints of dashboard design [1]. Informed citizens are expected to engage with new urban information flows through the logic of dashboard interfaces. What datastreams are left off the dashboard of engaged urban experience? What design opportunities arise when dashboard visualizations are moved into the domain of mixed reality? In this two-day workshop, participants will construct prototype mixed reality interfaces for engaging the informational layer of the built urban environment. Using the Unity game engine and the Microsoft HoloLens, participants will focus on generative design in the space of data-driven interfaces, addressing issues of data access, civic agency, and privacy in the context of smart cities. Specific attention will be paid to interfaces that facilitate harmonious coexistence between humans and non-human systems (AI, IoT, etc.). © 2018 Copyright is held by the owner/author(s).
"
10.1145/3173574.3173851,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85046957508&origin=inward,Conference Paper,SCOPUS_ID:85046957508,scopus,2018-04-20,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),emotional dialogue generation using image-grounded language models,"
AbstractView references

Computer-based conversational agents are becoming ubiquitous. However, for these systems to be engaging and valuable to the user, they must be able to express emotion, in addition to providing informative responses. Humans rely on much more than language during conversations; visual information is key to providing context. We present the first example of an image-grounded conversational agent using visual sentiment, facial expression and scene features. We show that key qualities of the generated dialogue can be manipulated by the features used for training the agent. We evaluate our model on a large and very challenging real-world dataset of conversations from social media (Twitter). The image-grounding leads to significantly more informative, emotional and specific responses, and the exact qualities can be tuned depending on the image features used. Furthermore, our model improves the objective quality of dialogue responses when evaluated on standard natural language metrics. © 2018 ACM.
"
10.1109/CFIS.2018.8336657,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050496477&origin=inward,Conference Paper,SCOPUS_ID:85050496477,scopus,2018-04-11,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),faults detecting of high-dimension gas turbine by stacking dnn and llm,"
AbstractView references

Increasing the input dimension cause curse of dimensions which make the inefficiency of methods, especially in analysis and interpretation practically. In this paper, a new two-part structure for fault diagnosing and identifying (FDI) of high-dimension systems has been presented. The first part of which consists of Auto-Encoder (AE) as Deep Neural Networks (DNNs) to produce features engineering process and summarize the features, and the second part Local Models Networks (LMNs) with LOcal LInear MOdel Tree (LOLIMOT) algorithm to model outputs. Then the residual which generated by comparing output of system and obtained models in each condition is used to alarm faults. Standard laboratory Gas Turbine data is the case study for this structure. Finally, by comparing the simulated results with the several reliable works, the effectiveness of this proposed structure is well illustrated. © 2018 IEEE.
"
10.1016/j.entcs.2018.03.008,S1571066118300100,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85045283174&origin=inward,Article,SCOPUS_ID:85045283174,scopus,2018-04-10,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"chromar, a rule-based language of parameterised objects","Modelling in biology becomes necessary when systems are complex but the more complex the systems are the harder the models become to read. The most common ways of writing models are by writing reactions on discrete, typed objects (e.g. molecules of different species), or writing rate equations for the populations of such species. One problem (1) with those approaches is that the number of species and reactions is often so large that the model cannot be realistically enumerated. Another problem (2) is that the number of species and reactions is fixed, whereas biology often grows new compartments which means new reactions and species. Here we develop an extension to the representation of reactions where the objects carry variables that are defined by their type (for example objects of type Leaf all have a Mass variable). The dynamics are defined by rules about types, which means they work for all objects of that type. This compact representation solves problem 1. If we think of the object variables as the analogue of reaction/rate equation species, creating a new object of some type means we are also creating new species (solving problem 2). We also developed an embedding of Chromar in the programming language Haskell and showed its applicability to two examples. Having a more compact representation can help make models a tool for knowledge representation and exchange instead of just a simulation input. Embedding Chromar in a general purpose programming language lifts some of the constraints of modelling languages while still maintaining the naturalness of a domain-specific language."
10.1145/3167132.3167282,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050572274&origin=inward,Conference Paper,SCOPUS_ID:85050572274,scopus,2018-04-09,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an ocarina extension for aadl formal semantics generation,"
AbstractView references

The formal verification has become a recommended practice in safety-critical software engineering. The hand-written of the formal specification requires a formal expertise and may become complex, especially with large systems. In such context, the automatic generation of the formal specification seems helpful and rewarding, particularly for reused and generic mapping such as hardware representations and real-time features. In this paper, we aim to formally verify real-time systems designed by AADL language. We propose an extension AADL2LNT of the Ocarina tool suite allowing the automatic generation of an LNT specification to draw a gateway for the CADP formal analysis toolbox. © 2018 ACM.
"
10.1145/3185046,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053511194&origin=inward,Article,SCOPUS_ID:85053511194,scopus,2018-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a modeling language for conceptual design of systems integration solutions,"
AbstractView references

Systems integration'connecting software systems for cross-functional work'is a significant concern in many large organizations, which continue to maintain hundreds, if not thousands, of independently evolving software systems. Current approaches in this space remain ad hoc, and closely tied to technology platforms. Following a design science approach, and via multiple design-evaluate cycles, we develop Systems Integration Requirements Engineering Modeling Language (SIRE-ML) to address this problem. SIRE-ML builds on the foundation of coordination theory, and incorporates important semantic information about the systems integration domain. The article develops constructs in SIRE-ML, and a merge algorithm that allows both functional managers and integration professionals to contribute to building a systems integration solution. Integration models built with SIRE-ML provide benefits such as ensuring coverage and minimizing ambiguity, and can be used to drive implementation with different platforms such as middleware, services, and distributed objects. We evaluate SIRE-ML for ontological expressiveness and report findings about applicability check with an expert panel. The article discusses implications for future research such as tool building and empirical evaluation, as well as implications for practice. © 2018 ACM 2158-656X/2018/09-ART8 $15.00
"
10.1080/19397038.2018.1439121,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042234836&origin=inward,Article,SCOPUS_ID:85042234836,scopus,2018-03-04,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),implementation of a software platform to support an eco-design methodology within a manufacturing firm,"
AbstractView references

The paper aims to explore the implementation of an eco-design methodology and the related software platform (G.EN.ESI–Green ENgineering dESIgn) within technical departments of a manufacturing firm. The G.EN.ESI eco-design methodology is based on the life cycle thinking concept and the software platform is conceived as a set of inter-operable software tools able to efficiently exchange data among them and with the traditional design systems (i.e. CAD, PDM and PLM). A multinational company, designing and producing household appliances, adopted the proposed methodology and related software platform for redesigning two cooker hood models with the aim to improve their environmental performances. Design and engineering departments evaluated the methodology and platform impact on the product development process, as well as the platform inter-operability with traditional design tools. The results indicate that methodology and software platform satisfy the requirements of the enterprise in terms of: (i) degree of expertise and training requirement on this subject, (ii) low impact in a consolidated design process and, (ii) good level of inter-operability among heterogeneous tools. However, the testing results highlight the necessity of a further platform optimisations in terms of software integration (single workbench made by integrated software tools with the same graphical user interface). © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.
"
10.1371/journal.pcbi.1005897,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85044772320&origin=inward,Article,SCOPUS_ID:85044772320,scopus,2018-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),fluctuating finite element analysis (ffea): a continuum mechanics software tool for mesoscale simulation of biomolecules,"
AbstractView references

Fluctuating Finite Element Analysis (FFEA) is a software package designed to perform continuum mechanics simulations of proteins and other globular macromolecules. It combines conventional finite element methods with stochastic thermal noise, and is appropriate for simulations of large proteins and protein complexes at the mesoscale (length-scales in the range of 5 nm to 1 μm), where there is currently a paucity of modelling tools. It requires 3D volumetric information as input, which can be low resolution structural information such as cryo-electron tomography (cryo-ET) maps or much higher resolution atomistic co-ordinates from which volumetric information can be extracted. In this article we introduce our open source software package for performing FFEA simulations which we have released under a GPLv3 license. The software package includes a C ++ implementation of FFEA, together with tools to assist the user to set up the system from Electron Microscopy Data Bank (EMDB) or Protein Data Bank (PDB) data files. We also provide a PyMOL plugin to perform basic visualisation and additional Python tools for the analysis of FFEA simulation trajectories. This manuscript provides a basic background to the FFEA method, describing the implementation of the core mechanical model and how intermolecular interactions and the solvent environment are included within this framework. We provide prospective FFEA users with a practical overview of how to set up an FFEA simulation with reference to our publicly available online tutorials and manuals that accompany this first release of the package. © 2018 Solernou et al.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042773984&origin=inward,Article,SCOPUS_ID:85042773984,scopus,2018-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the ganfather: the man who's given machines the gift of imagination,"
AbstractView references

By pitting neural networks against one another, Ian Goodfellow has created a powerful AI tool. The AI tool developed by him is called generative adversarial network (GAN). The technique has sparked huge excitement in the field of machine learning and turned its creator into an AI celebrity. The goal of GANs is to give machines something akin to an imagination. In the future, computers will get much better at feasting on raw data and working out what they need to learn from it without being told.
"
10.1007/s00006-018-0827-1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041800714&origin=inward,Article,SCOPUS_ID:85041800714,scopus,2018-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an extended implementation framework for geometric algebra operations on systems of coordinate frames of arbitrary signature,"
AbstractView references

There is a steadily increasing interest in applying Geometric Algebra (GA) in diverse fields of science and engineering. Consequently, we need better software implementations to accommodate such increasing demands that widely vary in their possible uses and goals. For large-scale complex applications having many integrating parts, such as Big Data and Geographical Information Systems, we should expect the need for integrating several GAs to solve a given problem. Even within the context of a single GA space, we often need several interdependent systems of coordinates to efficiently model and solve the problem at hand. Future GA software implementations must take such important issues into account in order to scale, extend, and integrate with existing software systems, in addition to developing new ones, based on the powerful language of GA. This work attempts to provide GA software developers with a self-contained description of an extended framework for performing linear operations on GA multivectors within systems of interdependent coordinate frames of arbitrary metric. The work explains the mathematics and algorithms behind this extended framework and discusses some of its implementation schemes and use cases. If properly implemented, the extended framework can significantly reduce the memory requirements for implementing Geometric Algebras with larger dimensions, especially for systems based on the symbolic processing of multivector scalar coefficients. © 2018, Springer International Publishing AG, part of Springer Nature.
"
10.1145/3183895.3183898,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85045107568&origin=inward,Conference Paper,SCOPUS_ID:85045107568,scopus,2018-02-24,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),dsmodels: a little language for dynamical systems,"
AbstractView references

Dynamical systems are used to model a variety of time-dependent systems. Visualizations of dynamical systems can display a large amount of information in a single image, but generating these images requires both mathematical and programming expertise. dsmodels is a domain-specific language (DSL) for visualizing twodimensional dynamical systems. dsmodels speeds up the process of visualizing a dynamical system by providing primitives to capture models, encapsulate features, and depict the overall behavior of the system. dsmodels can also simulate dynamical systems to compute attractors and their basis of attraction, allowing for rapid prototyping or informal analysis. We present dsmodels using a case study of population models. © 2018 Association for Computing Machinery.
"
10.1007/s10462-016-9521-7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84991821725&origin=inward,Article,SCOPUS_ID:84991821725,scopus,2018-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a metamodeling approach for the identification of organizational smells in multi-agent systems: application to aspecs,"
AbstractView references

Software Quality is one of the most important subjects in the Process Development Software, especially in large and complex systems. Much effort has been devoted to the development of techniques and concepts to improve software quality over the years. We are especially interested on smells, which represent anomalies or flaws in the design/code that can have serious consequences in maintenance or future development of the systems. These techniques have a strong development in the Object Oriented paradigm, however, very few studies were conducted in the agent oriented paradigm. In this paper we focus on the detection of design smells applied to multi-agent systems models based on the organizational approach, named Organizational Design Smells (ODS). Early and automatic detection of these ODS allows reducing the costs and development times, while increasing the final product’s quality. To achieve this objective, validation rules were defined based in the EVL language. The approach is illustrated with two examples, their validation rules, and the refactoring solutions proposed. © 2016, Springer Science+Business Media Dordrecht.
"
10.1007/s00170-016-9338-1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84983394789&origin=inward,Article,SCOPUS_ID:84983394789,scopus,2018-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),design and development of a cnc machining process knowledge base using cloud technology,"
AbstractView references

Nowadays, computer numerical control (CNC) machine tool undertakes more processing tasks than other common machine tools because of its highly automated machining ability and high performance. However, due to the lack of intelligence in machining process planning, machining procedure of products mostly depends on process planners rather than CNC machine tools. To make product quality less dependable on process planner’s ability and improve the efficiency of process planning in order to fulfill changeable market, this paper presents an approach to design and develop CNC machining process knowledge base using cloud technology. The general standard STEP-NC is mapped to web ontology language (OWL) to describe machining process-related knowledge in a readable and comprehensible way. This mapping relation also makes knowledge suitable for storage in HBase. Through this ontology model, descriptive and logical knowledge can be collected. Hadoop platform is used in this approach to provide the NoSQL database HBase for large-scale knowledge storage and MapReduce programming model for large-scale knowledge processing. Taking advantage of MapReduce, knowledge query engine and reasoning engine can be developed. Users can submit task and resource descriptive files to the cloud through CNC controller and get machining process solutions from knowledge base. Evaluation mechanism is also adopted to filter low-quality knoweldge. © 2016, Springer-Verlag London.
"
10.1007/978-3-319-78807-4_8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145739623&origin=inward,Book Chapter,SCOPUS_ID:85145739623,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),growing informal justice (from the inside-out),"
AbstractView references

In this chapter I argue that the development and proliferation of ADR in Europe for providing high-quality procedures and outcomes needs to be addressed from the inside-out. ADR, at its best, can contribute to access to justice and has the potential to be a model of dispute resolution that embraces users notions of a just and fair procedure. ADR models then have to be designed to reflect values and ethical standards that go hand in hand with users attitudes. Does ADR (need to) create its own norms of fairness, justice and language? Thinking about justice, fairness, trust and legitimacy, questions guiding our future inquiries could include: do our traditional values and roles within our justice system have to be reconsidered? Do we need new measures and tools to create appropriate protection for actors and users in these new and rapidly growing spaces? How can we best understand these areas of little regulation and large complexity that cannot fully be captured by traditional methods, models and language? © 2018, The Author(s).
"
10.2514/6.2018-1396,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141612840&origin=inward,Conference Paper,SCOPUS_ID:85141612840,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),scenario challenges for next generation aviation technology demonstrations,"
AbstractView references

The Federal Aviation Administration’s (FAA) Next Generation Air Transportation System (NextGen) program is a long-term modernization and transformation of the current National Airspace System (NAS) into a more efficient and coordinated decision-making system. In order to test the key concepts and methods proposed by FAA and affiliated standard organizations such as RTCA (Radio Technical Commission for Aeronautics), Modeling and Simulation (M&S)-based demonstration of the solutions are employed in the early stages for the verification and validation of concepts and their technologies. With the help of model-based and simulation-based engineering, large-scale systems integration and demonstrations take place seamlessly. This demands for common understanding of simulation scenarios, allowing for cross-platform interoperability. A major challenge facing the development and testing of the demonstrations is the lack of standard scenario definition language bridging the gap between the operational team (controllers, pilots, and dispatchers) and the engineering community. This paper will tackle aviation scenario generation challenges using the recently proposed Aviation Scenario Definition Language (ASDL). ASDL aims at providing a standard scenario specification that leads to a common mechanism for verifying and executing aviation scenarios, effective sharing of scenarios among various simulation environments, improve the consistency among different stakeholders and entities and enable the reuse of scenario specifications. Based on Domain-Specific Language (DSL) design methodologies, current version of ASDL provides a well-structured definition language to define various flight operations scenarios. In this paper, a number of scenarios that have been implemented in previous FAA funded projects, including scenarios of 4 Dimensional trajectory operations as well as unmanned aerial system operations will be presented and an equivalent ASDL generated scenario script will be provided to show the streamline process of defining and validating aviation scenarios, using a formal language. © 2018, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.
"
10.21437/SLTU.2018-43,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132052897&origin=inward,Conference Paper,SCOPUS_ID:85132052897,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),building speech recognition systems for language documentation: the coedl endangered language pipeline and inference system (elpis),"
AbstractView references

Machine learning has revolutionised speech technologies for major world languages, but these technologies have generally not been available for the roughly 4,000 languages with populations of fewer than 10,000 speakers. This paper describes the development of Elpis, a pipeline which language documentation workers with minimal computational experience can use to build their own speech recognition models, resulting in models being built for 16 languages from the Asia-Pacific region. Elpis puts machine learning speech technologies within reach of people working with languages with scarce data, in a scalable way. This is impactful since it enables language communities to cross the digital divide, and speeds up language documentation. Complete automation of the process is not feasible for languages with small quantities of data and potentially large vocabularies. Hence our goal is not full automation, but rather to make a practical and effective workflow that integrates machine learning technologies. © SLTU 2018. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119987253&origin=inward,Conference Paper,SCOPUS_ID:85119987253,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),feature engineering for second language acquisition modeling,"
AbstractView references

Knowledge tracing serves as a keystone in delivering personalized education. However, few works attempted to model students' knowledge state in the setting of Second Language Acquisition. The Duolingo Shared Task on Second Language Acquisition Modeling (Settles et al., 2018) provides students' trace data that we extensively analyze and engineer features from for the task of predicting whether a student will correctly solve a vocabulary exercise. Our analyses of students' learning traces reveal that factors like exercise format and engagement impact their exercise performance to a large extent. Overall, we extracted 23 different features as input to a Gradient Tree Boosting framework, which resulted in an AUC score of between 0.80 and 0.82 on the official test set. © 2018 Association for Computational Linguistics
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065792493&origin=inward,Conference Paper,SCOPUS_ID:85065792493,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a generic framework for the analysis of heterogeneous legacy software systems,"
AbstractView references

The reverse engineering of legacy systems is a process that involves analysis and understanding of the given systems. Some people believe in-depth knowledge of the system is a prerequisite for its analysis, whereas others, ourselves included, argue that only specific knowledge is required on a per-project basis. To give support for the latter approach, we propose a generic framework that employs the techniques of non-determinism and abstraction to enable us to build tooling for analyzing large systems. As part of the framework, we introduce an extensible imperative procedural language called KERNEL which can be used for constructing an abstract representation of the control flow and data flow of the system. To illustrate its use, we show how such framework can be instantiated to build a use-def graph for a large industrial legacy COBOL and JCL system. We have implemented our framework in a model-driven fashion to facilitate development of relevant tools. The resulting GELATO tool set can be used within the Eclipse environment. © 2018 CEUR-WS. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065321563&origin=inward,Conference Paper,SCOPUS_ID:85065321563,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a model-driven software architecture for ultra-cold gas experiments in space,"
AbstractView references

Developing software for large and complex experiments is a challenging task. It must incorporate many requirements from different domains, all with their own conceptions about the overall systems. An additional level of complexity is added if the experiment is conducted autonomously during a sounding rocket flight. Without a proper software architecture and development techniques, achieving and maintaining a high code quality is a very cumbersome task. This paper describes the architecture and the model-driven development approach we used to implement the control software of the experiments in the MAIUS-1 mission (matter-wave interferometry in microgravity). In this mission, the software had to handle around 150 experiments in six minutes autonomously and adapt to changes in the control flow according to real-time data from the experiment. The MAIUS-1 mission was the first mission to create Bose-Einstein condensates in space and conduct other experiments with ultra-cold gases on a sounding rocket. Besides the scientific goals in the area of quantum-optics, other important objectives of the mission were the miniaturization and further development of laser systems, vacuum components, optical sensors, and other related technologies. To fulfil these goals, new experimental hardware has been created which had to be integrated and tested with the software of the experiment computer. The custom-made hardware and the considerable number of domains involved brought up many challenges for the software engineering. To face all these challenges of developing software with this high complexity, we chose to follow a model-driven software development approach. Several domain-specific languages (DSLs) accompanied with specialized tools were created to allow the physicists and electronic engineers to describe system components and the experiments in a domain-specific way. These descriptions were then automatically transformed in C++ code for the flight software. This way we could actively incorporate all the domains involved in conducting the experiment directly in building the flight software without compromising the software quality. We created a versatile software platform not only for the MAIUS-1 mission but also for upcoming missions with similar experiments and hardware. With our approach we were able to generate around 84% of the source code for the final flight software from the domain-specific models. Besides the improvement of the development process, the code generation made a significant contribution to the overall software quality as almost all manual coding of error-prone boilerplate code could be mitigated. © 2018 International Astronautical Federation, IAF. All rights reserved.
"
10.3166/I2M.17.375-391,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063648823&origin=inward,Article,SCOPUS_ID:85063648823,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),application of model predictive control for the optimization of thermo-hygrometric comfort and energy consumption of buildings,"
AbstractView references

The use of tools of simulation in every field of engineering is in the last years widely spreading. Lot of them can be used and a large amount of simulators can be found on the market in order to perform every kind of analysis and prediction. In the field of building/plant system, tools based on white, grey and black box approaches are often used as a function of accuracy and reliability. Several tools were developed according to mathematical models and transient analysis in order to perform Building Energy Simulations. The lumped capacitance models have a potential in terms of both data reliability and low computational cost. The Resistance-Capacitance models can be realized with different orders to improve the dynamic thermal behavior of building and coupled with model-based design tools. Dymola with Modelica language can provide a useful tool for engineers to design a thermohygrometric comfort model optimizing the energy consumptions. The paper describes a calculation method developed with the aid of an outdoor test cell, based on a second order Lumped parameters model coupled with a hygrometric model and a Model Predictive Control thanks to a library for real time control and management of energy consumptions and thermal comfort. © 2018 Lavoisier.
"
10.3233/978-1-61499-900-3-559,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063389616&origin=inward,Conference Paper,SCOPUS_ID:85063389616,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),context sampling strategies for generating linked data graph embeddings,"
AbstractView references

Linked data is a data publishing method that can be used to connect any kind of globally available data into a single multigraph. This kind of graph provides enormous opportunities for machine learning and data mining techniques to train models with large heterogenous types of data and find new relationships. Both are however strongly dependent on the engineering of high quality features and therefore requires knowledge of the domain. Recent advances in the field of representation learning has led to significant progress in automating the feature engineering process. Neural word embedding techniques from the natural language processing domain have been used to learn representations of graph nodes and subsequently applied to linked data nodes. In contrast to natural language where sentences serve as natural boundary for the context of a word, in a graph - boundaries are not clearly defined and multiple context sampling strategies exist. Applying different context sampling strategies on graph nodes result in different context sentences and subsequently different features. In this work, we explore two different context sampling strategies: predicate removal from random walks as well as breadth first search based sampling and compare them to the state of the art based on random walks. The quality of the generated features is evaluated indirectly by measuring the performance of machine learning models on a classification task across multiple data sets. Furthermore, we explore the effect of generating embeddings only for the entities that have to be classified and their neighbors, instead of generating embeddings for every node in a possibly large RDF graph. The results suggest that for classification of same typed entities the inclusion of predicates in the sampled walks for generating embeddings is of little use and can be omitted without losing classification accuracy. Results also show that the in-degree and out-degree of the entities may be useful hint for selecting the optimal sampling technique. © 2018 The authors and IOS Press. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063105061&origin=inward,Conference Paper,SCOPUS_ID:85063105061,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),implementing the mdetools'18 challenge with thingml,"
AbstractView references

This paper presents a ThingML implementation of the MDETools'18 challenge. ThingML is a textual modeling language that implements a sub-set of the UML (components and state-machines) and complements it with a first-class action language. ThingML also comes with a set of compilers targeting a large variety of platforms and programming languages. Using ThingML, we have been able to 1) fully model the MDETools'18 challenge, 2) automatically compile this specification it to two distinct programming languages, and 3) successfully execute the resulting programs and achieve reasonable results in the simulation. © 2018 CEUR-WS. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063097036&origin=inward,Conference Paper,SCOPUS_ID:85063097036,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),delta-driven collaborative modeling,"
AbstractView references

Model-Driven Engineering has already become a significant means in software development activities, which is well-suited to design and develop large-scale software systems. Developing and maintaining large-scale model-driven software systems entails a need for collaborative modeling by a large number of software designers and developers. As the first-class entities of the model-driven engineering paradigm, software models are constantly changed during development and maintenance. Collaborative modeling support for frequently synchronizing model changes between collaborators is required. Thereby, a solid change representation support for model changes plays an essential role for collaborative modeling systems. This paper applies a meta-model generic, operation-based and textual difference language to existing domain-specific modeling tool UML Designer and demonstrates a collaborative modeling application - CoMo. It is validated for UML activity diagrams. © 2018 CEUR-WS. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062382326&origin=inward,Article,SCOPUS_ID:85062382326,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),clinical document classification using labeled and unlabeled data across hospitals,"
AbstractView references

Reviewing radiology reports in emergency departments is an essential but laborious task. Timely follow-up of patients with abnormal cases in their radiology reports may dramatically affect the patient's outcome, especially if they have been discharged with a different initial diagnosis. Machine learning approaches have been devised to expedite the process and detect the cases that demand instant follow up. However, these approaches require a large amount of labeled data to train reliable predictive models. Preparing such a large dataset, which needs to be manually annotated by health professionals, is costly and time-consuming. This paper investigates a semi-supervised transfer learning framework for radiology report classification across three hospitals. The main goal is to leverage both vastly available clinical unlabeled data and already learned knowledge in order to improve a learning model where limited labeled data is available. Our experimental findings show that (1) convolutional neural networks (CNNs), while being independent of any problem-specific feature engineering, achieve significantly higher effectiveness compared to conventional supervised learning approaches, (2) leveraging unlabeled data in training a CNN-based classifier reduces the dependency on labeled data by more than 50% to reach the same performance of a fully supervised CNN, and (3) transferring the knowledge gained from available labeled data in an external source hospital significantly improves the performance of a semi-supervised CNN model over their fully supervised counterparts in a target hospital.
"
10.25018/0236-1493-2018-3-0-208-217,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061788158&origin=inward,Article,SCOPUS_ID:85061788158,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),application of the object-oriented analysis method to creating an automated open pit mine planning and design system,"
AbstractView references

Available integrated systems connected with mining and geology are reviewed. The most efficient methods and approaches are identified with a view to making rational decisions on creating an automated system of open pit mine planning and design using the object-oriented analysis and design and programming using unified modeling language UML. The methodology of creating a complex integral dynamic system as a set of interconnected models and their conversion to a real programming product, as well as the international experience of this methodology application are analyzed. The discussed design processes allow modeling and program support of large and complex dynamic systems using technologies which enable thorough understanding of the behavior of correlation between artifacts of each iteration based on the architecture and modeling, and on the more comprehensive approach to software engineering. The article illustrates application of the object-oriented analysis methods to creating an automated open pit mine planning and design system. © 2018, Publishing house Mining book. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060782885&origin=inward,Conference Paper,SCOPUS_ID:85060782885,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),yamtl solution to the ttc 2018 social media case,"
AbstractView references

Software models raise the level of abstraction of software artefacts involved in the design, implementation and testing phases of software systems. Such models may be used to automate many of the tasks involved in them, where queries play an important role. Moreover, some of those models may be inferred automatically from existing software artefacts, e.g., by means of reverse engineering, yielding potentially very large models (VLMs). Technology to analyse VLMs e ciently enables the application of model-driven software development in industry and is the subject of study in the TTC 2018 Social Media Case. YAMTL is both a model transformation (MT) language that is available as an internal DSL of Xtend and a companion MT engine that can be used from any JVM application and that supports incremental execution of MT. In this paper, we present the YAMTL solution to the social media case and discuss its performance, scalability and memory usage w.r.t. the reference solution. The YAMTL solution was deemed to be the most scalable solution at the TTC 2018. Copyright © by the author(s).
"
10.5220/0006933701020109,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85059034476&origin=inward,Conference Paper,SCOPUS_ID:85059034476,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),predicting violent behavior using language agnostic models,"
AbstractView references

Groups advocating violence have caused significant destruction to individuals and societies. To combat this, governmental and non-governmental organizations must quickly identify violent groups and limit their exposure. While some groups are well-known for their violence, smaller, less recognized groups are difficult to classify. However, using texts from these groups, we may be able to identify them. This paper applies text analysis techniques to differentiate violent and non-violent groups using discourses from various value-motivated groups. Significantly, the algorithms are constructed to be language-agnostic. The results show that deep learning models outperform traditional models. Our models achieve high accuracy when fairly trained only on data from other groups. Additionally, the results indicate that the models achieve better performance by removing groups with a large amount of documents that can bias the classification. This study shows promise in using scalable, language-independent techniques to effectively identify violent value-motivated groups. Copyright 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved
"
10.5220/0007239101270138,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85059000370&origin=inward,Conference Paper,SCOPUS_ID:85059000370,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cross-domain &amp; in-domain sentiment analysis with memory-based deep neural networks,"
AbstractView references

Cross-domain sentiment classifiers aim to predict the polarity, namely the sentiment orientation of target text documents, by reusing a knowledge model learned from a different source domain. Distinct domains are typically heterogeneous in language, so that transfer learning techniques are advisable to support knowledge transfer from source to target. Distributed word representations are able to capture hidden word relationships without supervision, even across domains. Deep neural networks with memory (MemDNN) have recently achieved the state-of-the-art performance in several NLP tasks, including cross-domain sentiment classification of large-scale data. The contribution of this work is the massive experimentations of novel outstanding MemDNN architectures, such as Gated Recurrent Unit (GRU) and Differentiable Neural Computer (DNC) both in cross-domain and in-domain sentiment classification by using the GloVe word embeddings. As far as we know, only GRU neural networks have been applied in cross-domain sentiment classification. Sentiment classifiers based on these deep learning architectures are also assessed from the viewpoint of scalability and accuracy by gradually increasing the training set size, and showing also the effect of fine-tuning, an explicit transfer learning mechanism, on cross-domain tasks. This work shows that MemDNN based classifiers improve the state-of-the-art on Amazon Reviews corpus with reference to document-level cross-domain sentiment classification. On the same corpus, DNC outperforms previous approaches in the analysis of a very large in-domain configuration in both binary and fine-grained document sentiment classification. Finally, DNC achieves accuracy comparable with the state-of-the-art approaches on the Stanford Sentiment Treebank dataset in both binary and fine-grained single-sentence sentiment classification. Copyright 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85058793542&origin=inward,Conference Paper,SCOPUS_ID:85058793542,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),application of high performance computing in modeling giant fields of saudi arabia,"
AbstractView references

High resolution reservoir modeling has been a valuable tool for reservoir simulations of the giant fields managed by Saudi Aramco; the world leader in high performance reservoir simulation studies utilizing state-of-the-art computer facilities. Three of its current computing clusters used for reservoir simulations are in the list of top 500 1 computing platforms in the world. The computing center in Saudi Aramco currently hosts systems with computing power over 300 teraflops dedicated for reservoir simulation activities. This is complemented with over 750 terabytes of data storage. POWERS 2 , the Saudi Aramco in-house developed parallel simulator, is used for most simulation studies on these computing platforms. It has been ported to many parallel systems, such as the Thinking Machine, IBM Nighthawk, Linux clusters, Blue Gene, etc. The simulation environment is being continuously enhanced to support studies containing complex wells, coupled surface facility, etc. The algorithm complexity and memory contentions in the underlying software and hardware present challenges in efficient use of parallel computing platforms. In this paper we review various benchmark efforts conducted in Saudi Aramco to determine performance of the latest computational hardware and software tools for computations of large simulation models. We study strength and limitations of emerging architectures, such as graphics processing units (GPUs), to solve various important computation and communication intensive building blocks in POWERS. © Copyright 2011, Society of Petroleum Engineers
"
10.1007/978-3-030-04070-3_35,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85058513882&origin=inward,Conference Paper,SCOPUS_ID:85058513882,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),semvec: semantic features word vectors based deep learning for improved text classification,"
AbstractView references

Semantic word representation is a core building block in many deep learning systems. Most word representation techniques are based on words angle/distance, word analogies and statistical information. However, popular models ignore word morphology by representing each word with a distinct vector. This limits their ability to represent rare words in languages with large vocabulary. This paper proposes a dynamic model, named SemVec, for representing words as a vector of both domain and semantic features. Based on the problem domain, semantic features can be added or removed to generate an enriched word representation with domain knowledge. The proposed method is evaluated on adverse drug events (ADR) tweets/text classification. Results show that SemVec improves the precision of ADR detection by 15.28% over other state-of-the-art deep learning methods with a comparable recall score. © 2018, Springer Nature Switzerland AG.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85058054624&origin=inward,Conference Paper,SCOPUS_ID:85058054624,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),vertical domain text classification: towards understanding it tickets using deep neural networks,"
AbstractView references

It is challenging to directly apply text classification models without much feature engineering on domain-specific use cases, and expect the state of art performance. Much more so when the number of classes is large. Convolutional Neural Network (CNN or ConvNet) has attracted much in text mining due to its effectiveness in automatic feature extraction from text. In this paper, we compare traditional and deep learning approaches for automatic categorization of IT tickets in a real world production ticketing system. Experimental results demonstrate the good potential of CNN models in our task. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
"
10.3233/978-1-61499-898-3-956,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85057987478&origin=inward,Conference Paper,SCOPUS_ID:85057987478,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),management of complex product data using incremental semantic validation,"
AbstractView references

Management of semantically complex product data is one of the challenging problems tightly connected with emerging concurrent engineering environments and next generation product data management systems (PDM). Although ACID principles (Atomicity, Consistency, Isolation, and Durability) are widely recognized and recommended for any information system, it is hard to guarantee the consistency of the product data. Such data are usually driven by formal models in EXPRESS language being part of the STEP standard (ISO 10303). To be consistent and unambiguously interpretable by computer programs the data must satisfy syntactic and semantic rules defined by the standards. Available PDM systems are rather limited in maintaining the data consistency. Complete semantic validation requires extremely high costs, often exceeding the processing time of individual transactions. Periodic validation or validation on user demand is possible, but at a high risk of losing data that become useless in case of rule violation. In the paper an effective incremental method for semantic validation of product data is presented. It is guaranteed that the final data revision is consistent if only the original revision was consistent and the spot rules were not violated. Static analysis of the model specifications is applied and a dependency graph is formed. The dependency graph enables to identify the spot rules and the data that should be inspected against the rules. Computational experiments prove the effectiveness of the method in conformity to complex large-scale product data managed under an innovative platform PDMhub. © 2018 The authors and IOS Press.
"
10.18293/SEKE2018-119,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056887576&origin=inward,Conference Paper,SCOPUS_ID:85056887576,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),svega: answering natural language questions over knowledge base with semantic matching,"
AbstractView references

Nowadays, more and more large scale knowledge bases are available for public access. Although these knowledge bases have their inherent access interfaces, such as SPARQL, they are generally unfriendly to end users. An intuitive way to bridge the gap between users and knowledge bases is to enable users to ask questions with natural language interface and return desired answers directly. Here the challenge is how to discover the query intention of users. Another challenge is how to obtain accurate answers from knowledge bases. In this paper, we model the query intention with a graph based on an entity-driven method. Consequently, the core problem of natural language question answering can be treated as subgraph matching over knowledge bases. For a query graph, there is a huge number of candidate mappings in a knowledge base, including ambiguities. Thus, a semantic vector is proposed to address disambiguation by evaluating the semantic similarity between edges in a query graph and paths in a knowledge base. By this way, our system can extract accurate answers directly without any offline work. Extensive experiments over the series of QALD challenges show the effectiveness of our system Svega in terms of recall and precision against other state-of-The-Art systems. © 2018 Universitat zu Koln. All rights reserved.
"
10.18293/SEKE2018-012,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056876703&origin=inward,Conference Paper,SCOPUS_ID:85056876703,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),ontology-based software architectural pattern recognition and reasoning,"
AbstractView references

Designing software architecture is a knowledgeintensive task that typically involves textual and diagrammatic notation. Using these kinds of notation is often inconsistent, misleading, and ambiguous. Ontology representation is, therefore, a suitable approach, as it can semantically define architectural design model that can be automatically verified through reasoning. However, a large-scale software system is usually complex and applies more than one architectural styles with various behavioral patterns. Therefore, the scalability of automated verification for a complex software architecture design is a challenge. We propose an approach that helps to formally define complex architectural design model and automate different verifications such as consistency checking, architectural styles recognition, and behavioral sequence inference. Ontology Web Language (OWL) is used to semantically define basic architectural elements and architectural styles, while a set of rules defined in Semantic Web Rule Language (SWRL) helps to capture behavioral pattern according to style. We evaluated the scalability of our approach. The result shows that different levels of complexity in architectural design model has a minor impact on the verification performance. © 2018 Universitat zu Koln. All rights reserved.
"
10.18293/SEKE2018-191,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056867026&origin=inward,Conference Paper,SCOPUS_ID:85056867026,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),improving code summarization by combining deep learning and empirical knowledge,"
AbstractView references

Code summaries are human-readable text that describes the functionality of code blocks. Software developers use code summaries to understand the specification of API while code retrieve system relies on code summaries for effective code search. However, code summaries are often written by software developers. Writing good code summaries usually requires great effort. It could be helpful if developers use automatic code summarization system to generate code summaries. Recently, some works have applied deep learning methods to generate code summaries for code snippets. However, those deep learning methods treat code snippets as streams of text tokens while ignoring the inherent code structure information. In this paper, we propose a novel code summarization method named the CDEModel (Code summarization by Deep learning and Empirical knowledge) that combines inherent code structure information with deep learning models. The CDE-Model proposes several empirical strategies to transform code snippets to refined code representation and feeds them into an encoder-decoder neural network for text generation. We conduct large-scale experiments on 1500 popular Java projects on GitHub1 with 396,184 pairs of code snippets and summaries. Experimental results show that the quality of code summaries generated by our CDE-Model is better than other two methods. To the best of our knowledge, this paper is the first to combine code structure information with deep learning. © 2018 Universitat zu Koln. All rights reserved.
"
10.18293/SEKE2018-030,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056862354&origin=inward,Conference Paper,SCOPUS_ID:85056862354,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a model-based approach for build avoidance,"
AbstractView references

In large software systems, we frequently encounter change scenarios which require long build times. In many cases, it would suffice to build only a subset of the dependent build components to generate sound build results. Current approaches for change-specific identification of affected build components rely on knowledge about the language-specific propagation of changes, which renders them inapplicable to multi-language systems. In this paper, we present a model-based approach to derive the affected build components for a change scenario using an existing change propagation approach. This way, we make the advantages of a set of change-specific dependencies also accessible to those members of the development team who are less knowledgeable about the build process. Our approach enables the use of change-specific dependencies in multi-language software systems and shortens build times. We implemented our approach in a productive build environment to show the feasibility and practicability in a user study. © 2018 Universitat zu Koln. All rights reserved.
"
10.18293/SEKE2018-023,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056838018&origin=inward,Conference Paper,SCOPUS_ID:85056838018,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards formal modeling and verification of probabilistic connectors in coq,"
AbstractView references

The coordination language Reo has played an important role in organizing the interactions among different components in large-scale distributed applications. A probabilistic extension on classical Reo is necessary to deal with the uncertainty of the real world. In this paper we developed a framework in Coq for formalizing probabilistic connectors and reasoning about their probabilistic properties. Different types of probabilistic channels are characterized by the relations on their input and output timed data distribution streams. More complex probabilistic connectors can be further constructed based on the probabilistic channels and composition operators. Within such a framework, properties under analysis and refinement / equivalence relations between probabilistic connectors can be naturally established as theorems and proved using tactics in Coq. © 2018 Universitat zu Koln. All rights reserved.
"
10.1007/978-3-030-02450-5_6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056834920&origin=inward,Conference Paper,SCOPUS_ID:85056834920,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deductive verification of hybrid control systems modeled in simulink with keymaera x,"
AbstractView references

Hybrid control systems are, due to their ever-increasing complexity, more and more developed in model-driven design languages like Simulink. At the same time, they are often used in safety-critical applications like automotive or medical systems. Ensuring the correctness of Simulink models is challenging, as their semantics is only informally defined. There exist some approaches to formalize the Simulink semantics, however, most of them are restricted to a discrete subset. To overcome this problem, we present an approach to map the informally defined execution semantics of hybrid Simulink models into the formally well-defined semantics of differential dynamic logic (). In doing so, we provide a formal foundation for Simulink, and we enable deductive formal verification of hybrid Simulink models with an interactive theorem prover for hybrid systems, namely KeYmaera X. Our approach supports a large subset of Simulink, including time-discrete and time-continuous blocks, and generates compact and comprehensible models fully-automatically. We show the applicability of our approach with a temperature control system and an industrial case study of a multi-object distance warner. © Springer Nature Switzerland AG 2018.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056721814&origin=inward,Conference Paper,SCOPUS_ID:85056721814,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),opencal simulation of the 1992 tessina landslide,"
AbstractView references

OpenCAL is a scientific software library developed for the simulation of 2D/3D complex dynamical systems on multi/many-core systems. A MPI preliminary extension also allows for the execution on cluster of many-core devices. The library provides the Extended Cellular Automata paradigm as a Domain-Specific Language for modeling complex systems on structured grids. Here we briefly describe the software library and show a first application regarding the implementation of a simple but effective landslide simulation model, namely the SciddicaT extended cellular automaton. The application to a real case of study, namely the 1992 Tessina landslide (Italy), is also shown. Computational results achieved on an Intel Xeon E5-2650 socket, a Nvidia Tesla K40 compute dedicated many-core device and a Nvidia GeForce GTX 980 GPU are reported. © Institute of Information Science. All rights reserved.
"
10.1007/978-3-030-03421-4_8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056468677&origin=inward,Conference Paper,SCOPUS_ID:85056468677,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),programming safe robotics systems: challenges and advances,"
AbstractView references

A significant challenge for large-scale deployment of autonomous mobile robots is to program them with formal guarantees and high assurance of correct operation. Our approach towards enabling safe programming of robotics system consists of two parts: (1) a programming language for implementing, specifying, and compositionally (assume-guarantee) testing the high-level reactive robotics software; (2) a runtime assurance system to ensure that the assumptions used during design-time testing of high-level software hold at runtime. Combining high-level programming language and its systematic testing with runtime enforcement helps us bridge the gap between software testing that makes assumptions about the low-level controllers and the physical world, and the actual execution of the software on a real robotic platform in the physical world. We implement our approach in, a programming framework for building safe robotics systems. This paper introduces the toolchain and describes how it addresses the unique challenges involved in programming safety-critical robots. © Springer Nature Switzerland AG 2018.
"
10.2514/6.2018-2572,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056190232&origin=inward,Conference Paper,SCOPUS_ID:85056190232,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),leveraging mbse for esa ground segment engineering: starting with the euclid mission,"
AbstractView references

The development of a mission operations ground segment is a complex systems engineering activity that follows multiple applicable standards and best practices. The approach currently in place to perform this activity is very document-centric i.e. based on a large number of documented deliverables and document reviews. However, documents can easily suffer from redundant and often inconsistent contents as the system development lifecycle progresses. Thus the execution of ground segment engineering tasks is rendered unnecessarily complex, time-consuming and prone to human error or oversight. In this paper, we describe the approach taken by the European Space Agency to overcome the above problems in ground segment engineering by adopting a Model-Based System Engineering (MBSE) paradigm. Our approach to MBSE is a bottom-up one that is conceptualized around the need of the ground segment system engineer. This means, the complexity of MBSE, in particular the underlying data model and associated language, is abstracted as much as possible. In this paper we discuss our paperless operations ground segment engineering framework that is attempting to implement the paradigm. © 2018, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055478039&origin=inward,Conference Paper,SCOPUS_ID:85055478039,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sls model based design: a navigation perspective,"
AbstractView references

The SLS Program has implemented a Model-based Design (MBD) and Modelbased Requirements approach for managing component design information and system requirements. This approach differs from previous large-scale design efforts at Marshall Space Flight Center where design documentation alone conveyed information required for vehicle design and analysis and where extensive requirements sets were used to scope and constrain the design. The SLS Navigation Team is responsible for the Program-controlled Design Math Models (DMMs) which describe and represent the performance of the Inertial Navigation System (INS) and the Rate Gyro Assemblies (RGAs) used by Guidance, Navigation, and Controls (GN&C). The SLS Navigation Team is also responsible for navigation algorithms. The navigation algorithms are delivered for implementation on the flight hardware as a DMM. For the SLS Block 1B design, the additional GPS Receiver hardware model is managed as a DMM at the vehicle design level. This paper describes the models, and discusses the processes and methods used to engineer, design, and coordinate engineering trades and performance assessments using SLS practices as applied to the GN&C system, with a particular focus on the navigation components. © 2018 Univelt Inc. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055327652&origin=inward,Conference Paper,SCOPUS_ID:85055327652,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards a framework for executable systems modeling: an executable systems modeling language (esysml),"
AbstractView references

The Systems Modeling Language (SysML), which is the de-facto modeling standard in the systems engineering community, consists of a number of independently derived methodologies (i.e. state charts, activity diagrams etc.) which have been co-opted into a single modeling framework. This and the lack of an overarching meta-model that specifies relationships and rules governing the various language constructs precludes their uniform application across diagram types. This has resulted in a large unwieldy and at best semi-formal language specification, with adverse implications for interoperability of modeling tools and model execution. This paper presents an executable language that re-factors the SysML language schema and offers an equivalent textual syntax for model specification in tandem with the existing graphical syntax. This is aimed at supporting the development of time based simulation models useful for decision support and architecture verification and validation in systems engineering. © 2018 Society for Modeling & Simulation International (SCS).
"
10.1007/978-3-030-00801-7_2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054852289&origin=inward,Conference Paper,SCOPUS_ID:85054852289,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),run-time analysis of temporal constrained objects,"
AbstractView references

The programming paradigm of constrained objects is a declarative variant of the object-oriented paradigm wherein objects define the structure of a system and declarative constraints (rather than imperative methods) define its behavior. Constrained objects have many uses in the engineering domain and computation in this paradigm is essentially constraint solving. This paper is concerned with an extension of constrained objects called temporal constrained objects, which are especially appropriate for modeling dynamical systems. The main extensions are series variables and metric temporal operators to declaratively specify time-varying behavior. The language TCOB exemplifies this paradigm and the execution of TCOB programs consists of constraint solving within a time-based simulation framework. One of the challenges in TCOB is identifying errors owing both to the complexity of programs and the underlying constraint solving methods. We address this problem by extracting a run-time trace of the execution of a TCOB program and providing an analysis of the cause of error. The run-time trace also serves as a basis, in many cases, for constructing a finite-state machine which in turn can be used for ‘model-checking’ properties of the system. The paper also presents abstraction techniques for dealing with simulations that result in large state spaces. © Springer Nature Switzerland AG 2018.
"
10.1007/978-3-030-01042-3_10,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054791923&origin=inward,Conference Paper,SCOPUS_ID:85054791923,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),textual user requirements notation,"
AbstractView references

The User Requirements Notation (URN) is a requirements engineering standard published by the International Telecommunication Union that combines goal and scenario modeling in support of the elicitation, specification, analysis, and validation of requirements. The URN standard focuses on a graphical notation. This paper introduces a textual notation for URN called TURN (Textual User Requirements Notation). The main objective of TURN is to support the modeling of very large URN specifications where thousands of separate goal graphs or scenarios become unwieldy to navigate. In addition, the entering of large specifications in graphical tools has proven tedious, as the modeler must be concerned with layout issues that are unrelated to the information that is attempted to be modeled. In general, TURN offers an alternative input medium for URN specifications which aims to be easier, faster, and more scalable. Xtext is the defacto standard for the specification of textual metamodel-based software languages. To validate the feasibility of TURN, it is specified as an Xtext grammar, resulting in a metamodel tailored to TURN and covering a large subset of URN. The differences between the URN standard and TURN are elaborated, a multi-phased model-to-model transformation from TURN to URN is described, and conformance to URN is demonstrated with a rather exhaustive set of test cases for TURN specifications and their transformations. © Springer Nature Switzerland AG 2018.
"
10.1007/978-3-030-01461-2_6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054194838&origin=inward,Book Chapter,SCOPUS_ID:85054194838,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),overview: system architecture virtual integration based on an aadl model,"
AbstractView references

Many large scale embedded systems are safety-critical systems and are becoming increasingly complex. They are designed and developed by a worldwide network of enterprises and companies and often use multiple distributed models with little or late integration. System Architecture Virtual Integration (SAVI) is an effective way to improve system quality and reduce cost. It enables the model-driven virtual integration of complex systems across multiple development environments. It aims to find defects earlier in the development process, thus saving time. Architecture Analysis and Design Language (AADL), as a standard architecture modelling language, supports SAVI virtual integration process and can be a central and integrated model of integration. This paper gives an overview of SAVI virtual integration based on an AADL model. The integration can be performed using model transformation that transforms heterogeneous models into an AADL model, or using the model bus through which various annotated architecture models can interoperate. The focus of SAVI is to integrate and analyze systems, and then build. So, AADL-based non-functional properties analysis approaches are presented. The tool for these methods has been implemented to demonstrate feasibility and applicability. © Springer Nature Switzerland AG 2018.
"
10.21278/idc.2018.0146,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053343594&origin=inward,Conference Paper,SCOPUS_ID:85053343594,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),implementation of systems modeling language (sysml) in consideration of the consens approach,"
AbstractView references

The following paper, which is based on the design research methodology (DRM) according to Blessing and Chakrabarti, will first make a literature survey related to MBSE approaches in general. Based on the literature survey this paper will demonstrate the combination of SysML and MBSE in an industrial context by means of Cameo Systems as a MBSE-Modeler. Especially at the use of modeling tools, like Cameo Systems Modeler, users often have to create a completely new model draft. © 2018 Faculty of Mechanical Engineering and Naval Architecture. All Rights Reserved.
"
10.1007/978-3-319-96983-1_36,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052967604&origin=inward,Conference Paper,SCOPUS_ID:85052967604,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),openabl: a domain-specific language for parallel and distributed agent-based simulations,"
AbstractView references

Agent-based simulations are becoming widespread among scientists from different areas, who use them to model increasingly complex problems. To cope with the growing computational complexity, parallel and distributed implementations have been developed for a wide range of platforms. However, it is difficult to have simulations that are portable to different platforms while still achieving high performance. We present OpenABL, a domain-specific language for portable, high-performance, parallel agent modeling. It comprises an easy-to-program language that relies on high-level abstractions for programmability and explicitly exploits agent parallelism to deliver high performance. A source-to-source compiler translates the input code to a high-level intermediate representation exposing parallelism, locality and synchronization, and, thanks to an architecture based on pluggable backends, generates target code for multi-core CPUs, GPUs, large clusters and cloud systems. OpenABL has been evaluated on six applications from various fields such as ecology, animation, and social sciences. The generated code scales to large clusters and performs similarly to hand-written target-specific code, while requiring significantly fewer lines of codes. © 2018, Springer International Publishing AG, part of Springer Nature.
"
10.1117/12.2312242,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052620762&origin=inward,Conference Paper,SCOPUS_ID:85052620762,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the preliminary design of the g-clef spectrograph instrument device control system,"
AbstractView references

The Giant Magellan Telescope (GMT)-Consortium Large Earth Finder (G-CLEF) is a fiber-fed, precision radial velocity optical echelle spectrograph. The preliminary software design incorporates a hierarchical, multi-level state machine. At the lowest level, the state machine utilizes GMT-provided frameworks to communicate with the hardware. At higher levels of abstraction, the design makes extensive use of State Chart Extensible Markup Language (SCXML) representations to define the operation of the instrument. The functionality of the design can be validated by executing these representations. The incorporation of an interpreter to directly execute the SCXML as a component of the control system is being investigated. The approaches used to develop the preliminary software design concept are described, the use and utility of SCXML for instrument control is discussed, and the application of the preliminary design to a subset of G-CLEF subsystems is demonstrated. © 2018 SPIE.
"
10.5220/0006732207190730,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052023298&origin=inward,Conference Paper,SCOPUS_ID:85052023298,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),integration of hawk for model metrics in the measure platform,"
AbstractView references

The MEASURE project aims to integrate metrics across all phases of the software development lifecycle into a single decision support platform. For the earlier phases, metrics can be derived from models. Industrial use of model-driven engineering produces large model repositories, and high-performance querying is key to keep their metrics up to date. This paper presents an integration between the MEASURE metrics platform and the Hawk model indexing tool. Hawk was improved in several ways, such as adding support for the new Modelio metamodelling framework, or allowing Hawk servers to be provisioned through configuration files rather than through its web services. MEASURE and Hawk were then combined successfully to extract metrics from Modelio models of various domains, and Hawk was able to index and efficiently answer queries about the 2GB collection of models used by Softeam to develop Modelio. Copyright © 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved
"
10.5220/0006605804460453,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052022737&origin=inward,Conference Paper,SCOPUS_ID:85052022737,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),exploring dsl evolutionary patterns in practice a study of dsl evolution in a large-scale industrial dsl repository,"
AbstractView references

Model-driven engineering is used in the design of systems to (a.o.) enable analysis early in the design process. For instance, by using domain-specific languages, enabling engineers to model systems in terms of their domain, rather then encoding them into general purpose modeling languages. Domain-specific languages, like classical software, evolve over time. When domain languages evolve, they may trigger co-evolution of models, model-to-model transformations, editors (both graphical and textual), and other artifacts that depend on the domain-specific language. This co-evolution can be tedious and very costly. In literature, various approaches are proposed towards automated co-evolution. However, these approaches do not reach full automation. Several other studies have shown that there are theoretical limitations to the level of automation that can be achieved in certain scenarios. For several scenarios full automation can never be achieved. We wish to gain insight to which extent practically occurring scenarios can be automated. To gain this insight, in this paper, we investigate on a large-scale industrial repository, which (co-)evolutionary scenarios occur in practice, and compare them with the various scenarios and their theoretical automatability. We then assess whether practically occurring scenarios can be fully automated. Copyright © 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.
"
10.5220/0006733906320641,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052015962&origin=inward,Conference Paper,SCOPUS_ID:85052015962,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),recent advances towards the industrial application of model-driven engineering for assurance of safety-critical systems,"
AbstractView references

Safety-critical systems are typically subject to assurance processes as way to ensure that they do not pose undue risks to people, property, or the environment, usually in compliance with assurance standards. The planning, execution, and management of assurance processes can be a complex activity in practice because of issues in the application of the standards, the large amount of information to handle, and the need for providing convincing justifications of assurance adequacy, among other difficulties. As a solution, many authors have argued that the use of Model-Driven Engineering principles and techniques can facilitate and improve assurance of safety-critical systems. This paper presents some of the latest advances that have been and are being made towards the use of these principles and techniques in industry. Although models have been used for assurance of safety-critical systems for many years, e.g. to specify safety cases, it has only been recently when the full potential of Model-Driven Engineering has started to be more widely exploited. This includes aspects such as the specification of metamodels and domain specific languages for assurance, the extension and application of UML, and the use of model transformations. Copyright © 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved
"
10.1007/978-3-319-97304-3_17,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051930808&origin=inward,Conference Paper,SCOPUS_ID:85051930808,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),phonologically aware bilstm model for mongolian phrase break prediction with attention mechanism,"
AbstractView references

Phrase break prediction is the first and most important component in increasing naturalness and intelligibility of text-to-speech (TTS) systems. Most works rely on language specific resources, large annotated corpus and feature engineering to perform well. However, phrase break prediction from text for Mongolian speech synthesis is still a great challenge because the data sparse problem due to the scarcity of resources. In this paper, we introduce a Bidirectional Long Short-Term Memory (BiLSTM) model with attention mechanism which uses the position-based enhanced phonological representations, word embeddings and character embeddings to achieve state of the art performance. The position-based enhanced phonological representations, derived from a separately BiLSTM model, are comprised of phoneme and syllable embeddings which take along position information. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word or phonological component. To handle Out-of-Vocabulary (OOV) problem, we incorporated word, phonological and character embeddings together as inputs to the model. Experimental results show the proposed method significantly outperforms the systems which only used the word embeddings by successfully leveraging position-based phonologically information and attention mechanism. © Springer Nature Switzerland AG 2018.
"
10.2200/S00861ED1V01Y201806AIM039,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051535921&origin=inward,Article,SCOPUS_ID:85051535921,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),adversarial machine learning,"
AbstractView references

The increasing abundance of large high-quality datasets, combined with significant technical advances over the last several decades have made machine learning into a major tool employed across a broad array of tasks including vision, language, finance, and security. However, success has been accompanied with important new challenges: many applications of machine learning are adversarial in nature. Some are adversarial because they are safety critical, such as autonomous driving. An adversary in these applications can be a malicious party aimed at causing congestion or accidents, or may even model unusual situations that expose vulnerabilities in the prediction engine. Other applications are adversarial because their task and/or the data they use are. For example, an important class of problems in security involves detection, such as malware, spam, and intrusion detection. The use of machine learning for detecting malicious entities creates an incentive among adversaries to evade detection by changing their behavior or the content of malicius objects they develop. The field of adversarial machine learning has emerged to study vulnerabilities of machine learning approaches in adversarial settings and to develop techniques to make learning robust to adversarial manipulation. This book provides a technical overview of this field. After reviewing machine learning concepts and approaches, as well as common use cases of these in adversarial settings, we present a general categorization of attacks on machine learning. We then address two major categories of attacks and associated defenses: decision-time attacks, in which an adversary changes the nature of instances seen by a learned model at the time of prediction in order to cause errors, and poisoning or training time attacks, in which the actual training dataset is maliciously modified. In our final chapter devoted to technical content, we discuss recent techniques for attacks on deep learning, as well as approaches for improving robustness of deep neural networks. We conclude with a discussion of several important issues in the area of adversarial learning that in our view warrant further research. Given the increasing interest in the area of adversarial machine learning, we hope this book provides readers with the tools necessary to successfully engage in research and practice of machine learning in adversarial settings. Copyright © 2018 by Morgan & Claypool.
"
10.2514/1.I010601,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051215587&origin=inward,Article,SCOPUS_ID:85051215587,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),review of formal agile methods as cost-effective airworthiness certification processes,"
AbstractView references

SAFETY-CRITICAL software systems are part of our daily life and any error in these systems can result in catastrophic consequences, with the worst-case scenario being loss of human life. Therefore, a rigorous safety certification process is needed to prove the correctness and reliability of such systems. The purpose of the certification process is to ensure that the system to be used in a specific environment under specific conditions is safe. To make these software systems trustworthy and more reliable, the Federal Aviation Administration (FAA) is imposing safety requirements on the development and verification of airborne avionic systems as stated in the DO-178 [1] (“Software Considerations in Airborne Systems and Equipment Certification”) guidance document developed by the Radio Technical Commission for Aeronautics, Inc. and the European Organisation for Civil Aviation Equipment (EUROCAE; a nonprofit organization providing a European forum for resolving technical problems with electronic equipment for air transport). The latest DO-178C guidance includes modern technologies and methodologies necessary to achieve a more reliable and safe system within a constrained time and cost. The DO-178C guidance document also describes various guidelines to engineer (design, specify, develop, test, and deploy) a software component and all associated equipment with a certain level of safety that complies with the FAA airworthiness requirements [2]. Although these standards define the requirements for a process to remain compliant when used to develop a safety-critical system, the standards do not specify which process to use. Thus, software developers can use any preferred processes if they meets the objectives and safety standards of DO-178C, which in our case leave us with an option to then use Agile, formal methods, and model-based development (MBD) [3,4] with all their associated advantages in the certification and development process. The agile software development process is a set of practices and methods that are based on the values and principles expressed in the agile manifesto established on 17 February 2001 [5]. These methods take an iterative and lean development approach that emphasizes the rapid development of a minimum viable product and frequent releases of the software, producing high-quality code and reducing process overhead and direct involvement of the customer in the development process. Agile is significantly accepted in the industry but not as widely accepted in safety-critical systems development because of its undisciplined nature when it comes to documentation and the lack of rigorous verification and validation techniques. One study suggests that using the Agile process as a standalone method to develop a safety-critical system has proven to be a failure because of the quality control mechanisms used by Agile such as informal reviews and pair programming, which have not assured developers or authorities that the product is safe [6]. However, the study also suggests that using Agile can deliberately reduce the complexity on the aircraft development process and evolutionary technology used [7]. The study suggests using project awareness to handle the process complexity and open-source tools to start processes from the second half of the development life cycle earlier in order to detect and react to possible blockers as soon as possible. Scaling agile methods for large systems is hard and, in handling this issue, the study suggests using virtual simulation and testing that will reduce system complexity, save the budget, and get early feedback about the system. However, critical systems need upfront design, exhaustive documentation, and continuous integration, which is practically challenging when using Agile. Therefore, the aviation industry avoids applying Agile in the development process. © 2018 American Institute of Aeronautics and Astronautics Inc.. All rights reserved.
"
10.1016/B978-0-444-64241-7.50363-3,B9780444642417503633,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050641476&origin=inward,Book Chapter,SCOPUS_ID:85050641476,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),next generation multi-scale process systems engineering framework,"
                  The IDAES PSE framework represents a new approach for the design and optimization of innovative steady state and dynamic processes by integrating an extensible, equation-oriented process model library with the Pyomo algebraic modeling language. Built specifically to enable rigorous large-scale mathematical optimization, the framework includes capabilities for conceptual design, steady state and dynamic optimization, multi-scale modeling, uncertainty quantification, and the automated development of thermodynamic, physical property, and kinetic submodels from experimental data.
               "
10.1007/978-3-319-94580-4_13,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85049366157&origin=inward,Conference Paper,SCOPUS_ID:85049366157,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cooperative agents for discovering pareto-optimal classifiers under dynamic costs,"
AbstractView references

In contrast to passive classifiers that use all available input feature values to assign class labels to instances, active classifiers determine the features on which to base the classification. Motivated by the tradeoff between the cost of classification errors and the cost of obtaining additional information, active classifiers are widely used for diagnostic applications in domains such as in medicine, engineering, finance, and natural language processing. This paper extends the extant literature on active classifiers to applications where cost of obtaining additional information may vary over instances to be classified and over time. We show that this entails training a set of classifiers that grows exponentially with the number of features and propose an efficient way to discover models in the cost-accuracy Pareto optimal frontier. Our method is based on a set of cooperative agents. The incremental contributions of agents to a coalition is used as a surrogate measure to guide a heuristic search for models. Empirical results based on controlled experiments indicate that our approach can identify Pareto-optimal active classifiers under dynamic costs even in domains that involve a large number of input features. © 2018, Springer International Publishing AG, part of Springer Nature.
"
10.1007/978-3-319-91764-1_6,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85049013123&origin=inward,Conference Paper,SCOPUS_ID:85049013123,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),verification of the consistency of time-aware cyber-physical processes,"
AbstractView references

Cyber-physical systems (CPS) represent an emerging type of distributed systems that integrate a multitude of physical elements and software applications into large networks of interconnected components. Ensuring that such systems meet their timing requirements is essential, especially with time-sensitive applications. To deal with this, suitable ways to specify and verify distributed CPS applications including their timing requirements are needed. Current CPS modeling solutions specify CPS as inter-organizational processes using existing process modeling languages. However, the existing process modeling languages mostly focus on web-based workflow and are not directly compatible with CPS. Modeling processes in CPS requires the consideration of cyber elements, physical elements, and their non-functional properties such as time-related and physical properties. Given an inter-organizational CPS processes model with considering structural and non-functional properties, implicit conflicts may arise. To deal with this issue, we propose an approach for modeling and verifying inter-organizational cyber-physical processes associated with temporal properties. To do that, we provide an extended version of BPMN that supports CPS concepts and properties. Then, we define a set of transformation rules to automatically transform the inter-organizational processes model into a constraint satisfaction model. Thereafter, we analyze the generated model to check its consistency. © Springer International Publishing AG, part of Springer Nature 2018.
"
10.1007/978-3-319-92058-0_63,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85049009928&origin=inward,Conference Paper,SCOPUS_ID:85049009928,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),fuzzy clustering ensemble for prioritized sampling based on average and rough patterns,"
AbstractView references

This paper uses fuzzy clustering to extend a previous prioritized sampling proposal. In many big data problems, modeling an individual object such as a large engineering plant can be a tedious process requiring up to a month of analysis. A solution is to model as many representative objects as possible to represent the entire population. A new object can then use a model (or combination of models) from previously analyzed objects that best matches its characteristics. Since the modeling process can continue indefinitely adding models over time, we prioritize the sampling based on the ability of objects to represent as many characteristics as possible. The approach is demonstrated with a large set of weather stations to create a ranked sample based on hourly and monthly variations of important weather parameters, such as temperature, solar radiation, wind speed, and humidity. The weather patterns are represented using a combination of average and rough patterns to capture the essence of the distribution. The weather stations are grouped using Fuzzy C-Means and the objects with the largest fuzzy memberships are used as the representatives of each cluster. The weather stations representing a combination of different clustering schemes are then ranked based on the number of weather patterns they represent. © 2018, Springer International Publishing AG, part of Springer Nature.
"
10.1007/978-3-319-92997-2_3,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048865217&origin=inward,Conference Paper,SCOPUS_ID:85048865217,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),optimising ocl synthesized code,"
AbstractView references

OCL is a important element of many Model-Driven Engineering tools, used for different purposes like writing integrity constraints, as navigation language in model transformation languages or to define transformation specifications. There are refactorings approaches for manually written OCL code, but there is not any tool for the simplification of OCL expressions which have been automatically synthesized (e.g., by a repair system). These generated expressions tend to be complex and unreadable due to the nature of the generative process. However, to be useful this code should be as simple and resemble manually written code as much as possible. In this work we contribute a set of refactorings intended to optimise OCL expressions, notably covering cases likely to arise in generated OCL code. We also contribute the implementation of these refactorings, built as a generic transformation component using bentō, a transformation reuse tool for ATL, so that it is possible to specialise the component for any OCL variant based on Ecore. We describe the design and implementation of the component and evaluate it by simplifying a large amount of OCL expressions generated automatically showing promising results. Moreover, we derive implementations for ATL, EMF/OCL and SimpleOCL. © 2018, Springer International Publishing AG, part of Springer Nature.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85045417653&origin=inward,Conference Paper,SCOPUS_ID:85045417653,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the interactive narrator tool: effective requirements exploration and discussion through visualization,"
AbstractView references

Requirements visualization can contribute to requirements comprehension through the creation of conceptual models. However, these models can become hard to read and current tool support is minimal. Applying the right visualization mechanisms can help construct models that are more readable. To such extent, we present the Interactive Narrator tool: a web application that helps practitioners analyze software requirements at an abstract level. Interactive Narrator uses Natural Language Processing to derive conceptual models from user stories, which are then translated into an interactive network diagram with zooming and filtering capabilities. Interactive Narrator facilitates discussion and aims to accelerate the understanding of large sets of software requirements. Copyright 2018 for this paper by its authors.
"
10.1007/978-3-319-77935-5_14,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85045350374&origin=inward,Conference Paper,SCOPUS_ID:85045350374,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),static value analysis of python programs by abstract interpretation,"
AbstractView references

We propose a static analysis by abstract interpretation for a significant subset of Python to infer variable values, run-time errors, and uncaught exceptions. Python is a high-level language with dynamic typing, a class-based object system, complex control structures such as generators, and a large library of builtin objects. This makes static reasoning on Python programs challenging. The control flow is highly dependent on the type of values, which we thus infer accurately. As Python lacks a formal specification, we first present a concrete collecting semantics of reachable program states. We then propose a non-relational flow-sensitive type and value analysis based on simple abstract domains for each type, and handle non-local control such as exceptions through continuations. We show how to infer relational numeric invariants by leveraging the type information we gather. Finally, we propose a relational abstraction of generators to count the number of available elements and prove that no StopIteration exception is raised. Our prototype implementation is heavily in development; it does not support some Python features, such as recursion nor the compile builtin, and it handles only a small part of the builtin objects and standard library. Nevertheless, we are able to present preliminary experimental results on analyzing actual, if small, Python code from a benchmarking application and a regression test suite. © 2018, Springer International Publishing AG, part of Springer Nature.
"
10.1007/978-3-319-76941-7_14,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85044443516&origin=inward,Conference Paper,SCOPUS_ID:85044443516,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),attention-based neural text segmentation,"
AbstractView references

Text segmentation plays an important role in various Natural Language Processing (NLP) tasks like summarization, context understanding, document indexing and document noise removal. Previous methods for this task require manual feature engineering, huge memory requirements and large execution times. To the best of our knowledge, this paper is the first one to present a novel supervised neural approach for text segmentation. Specifically, we propose an attention-based bidirectional LSTM model where sentence embeddings are learned using CNNs and the segments are predicted based on contextual information. This model can automatically handle variable sized context information. Compared to the existing competitive baselines, the proposed model shows a performance improvement of ∼ 7% in WinDiff score on three benchmark datasets. © Springer International Publishing AG, part of Springer Nature 2018.
"
10.1007/978-3-319-75477-2_31,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85044428543&origin=inward,Conference Paper,SCOPUS_ID:85044428543,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deep learning approach for arabic named entity recognition,"
AbstractView references

Inspired by recent work in Deep Learning that have achieved excellent performance on difficult problems such as computer vision and speech recognition, we introduce a simple and fast model for Arabic named entity recognition based on Deep Neural Networks (DNNs). Named Entity Recognition (NER) is the task of classifying or labelling atomic elements in the text into categories such as Person, Location or Organization. The unique characteristics and the complexity of the Arabic language make the extraction of named entities a challenging task. Most state-of-the-art systems use a combination of various Machine Learning algorithms or rely on handcrafted engineering features and the output of other NLP tasks such as part-of-speech (POS) tagging, text chunking, prefixes and suffixes as well as a large gazetteer. In this paper, we present an Arabic NER system based on DNNs that automatically learns features from data. The experimental results show that our approach outperforms the model based on Conditional Random Fields by 12.36 points in F-measure. Moreover, our model outperforms the state-of-the-art by 5.18 points in Precision and gets very close results in F-measure. Most importantly, our system can be easily extended to recognize other named entities without any additional rules or handcrafted engineering features. © Springer International Publishing AG, part of Springer Nature 2018.
"
10.1007/978-3-319-74730-9_10,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042682091&origin=inward,Conference Paper,SCOPUS_ID:85042682091,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"models, more models, and then a lot more","
AbstractView references

With increased adoption of Model-Driven Engineering, the number of related artefacts in use, such as models, metamodels and transformations, greatly increases. To confirm this, we present quantitative evidence from both academia — in terms of repositories and datasets — and industry — in terms of large domain-specific language ecosystems. To be able to tackle this dimension of scalability in MDE, we propose to treat the artefacts as data, and apply various techniques — ranging from information retrieval to machine learning — to analyse and manage those artefacts in a holistic, scalable and efficient way. © Springer International Publishing AG 2018.
"
10.1007/978-3-319-74730-9_21,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042678320&origin=inward,Conference Paper,SCOPUS_ID:85042678320,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards integration of context-based and scenario-based development,"
AbstractView references

In scenario-based models of reactive systems complex specifications are divided into artifacts corresponding to separate aspects of overall system behavior, as they may appear, e.g., in a robot’s requirements document or user specifications. The advantages of scenario-based development include intuitiveness and clarity, the ability to execute or simulate specifications of early prototypes and of final systems, and the ability to verify the specification for early detection of conflicts, omissions, and errors. In this position paper we discuss two issues that emerge when applying scenario-based development in complex cases: (a) simple scenarios become unwieldy when subjected to a growing number of conditions, exceptions and refinements, and (b) it is hard to understand and maintain a large ‘flat’ specification, consisting of an unorganized list of independently-specified scenarios, simple as they may individually be. We address these issues by basing certain facets of scenario design on context, an increasingly popular foundational consideration in software engineering. We first show how one can incorporate context into the graphical language of live sequence charts (LSC) using existing LSC idioms. We then outline two other possibilities: (i) enriching the LSC language, or (ii) embedding LSCs within hierarchical state machines, namely, statecharts. We believe that this research can contribute to the broader goals of developing complex and powerful reactive systems in intuitive and robust ways. © Springer International Publishing AG 2018.
"
10.1145/3158669,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042487711&origin=inward,Article,SCOPUS_ID:85042487711,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),scalable cloning on large-scale gpu platforms with application to time-stepped simulations on grids,"
AbstractView references

Cloning is a technique to efficiently simulate a tree of multiple what-if scenarios that are unraveled during the course of a base simulation. However, cloned execution is highly challenging to realize on large, distributed memory computing platforms, due to the dynamic nature of the computational load across clones, and due to the complex dependencies spanning the clone tree. We present the conceptual simulation framework, algorithmic foundations, and runtime interface of CloneX, a new system we designed for scalable simulation cloning. It efficiently and dynamically creates whole logical copies of a dynamic tree of simulations across a large parallel system without full physical duplication of computation and memory. The performance of a prototype implementation executed on up to 1,024 graphical processing units of a supercomputing system has been evaluated with three benchmarks—heat diffusion, forest fire, and disease propagation models—delivering a speed up of over two orders of magnitude compared to replicated runs. The results demonstrate a significantly faster and scalable way to execute many what-if scenario ensembles of large simulations via cloning using the CloneX interface. 2018 Copyright is held by the owner,author's.
"
10.1016/bs.pmch.2017.12.003,S0079646817300243,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042402496&origin=inward,Book Chapter,SCOPUS_ID:85042402496,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),big data in drug discovery,"
                  Interpretation of Big Data in the drug discovery community should enhance project timelines and reduce clinical attrition through improved early decision making. The issues we encounter start with the sheer volume of data and how we first ingest it before building an infrastructure to house it to make use of the data in an efficient and productive way. There are many problems associated with the data itself including general reproducibility, but often, it is the context surrounding an experiment that is critical to success. Help, in the form of artificial intelligence (AI), is required to understand and translate the context. On the back of natural language processing pipelines, AI is also used to prospectively generate new hypotheses by linking data together. We explain Big Data from the context of biology, chemistry and clinical trials, showcasing some of the impressive public domain sources and initiatives now available for interrogation.
               "
10.1016/j.jcde.2017.11.004,S2288430017300350,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042076094&origin=inward,Article,SCOPUS_ID:85042076094,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),interdisciplinary semantic model for managing the design of a steam-assisted gravity drainage tooling system,"Complex engineering systems often require extensive coordination between different expert areas in order to avoid costly design iterations and rework. Cyber-physics system (CPS) engineering methods could provide valuable insights to help model these interactions and optimize the design of such systems. In this work, steam assisted gravity drainage (SAGD), a complex oil extraction process that requires deep understanding of several physical-chemical phenomena, is examined whereby the complexities and interdependencies of the system are explored. Based on an established unified feature modeling scheme, a software modeling framework is proposed to manage the design process of the production tools used for SAGD oil extraction. Applying CPS methods to unify complex phenomenon and engineering models, the proposed CPS model combines effective simulation with embedded knowledge of completion tooling design in order to optimize reservoir performance. The system design is expressed using graphical diagrams of the unified modelling language (UML) convention. To demonstrate the capability of this system, a distributed research group is described, and their activities coordinated using the described CPS model."
10.1007/978-3-319-72817-9_14,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041538684&origin=inward,Conference Paper,SCOPUS_ID:85041538684,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),evaluation of a security and privacy requirements methodology using the physics of notation,"
AbstractView references

Security and Privacy Requirements Methodologies are considered an important part of the development process of systems, especially for the ones that contain and process a large amount of critical information and inevitably needs to remain secure and thus, ensuring privacy. These methodologies provide techniques, methods, and norms for tackling security and privacy issues in Information Systems. In this process, the utilisation of effective, clear and understandable modelling languages with sufficient notation is of utmost importance, since the produced models are used not only among IT experts or among security specialists, but also for communication among various stakeholders, in business environments or among novices in an academic environment. This paper evaluates the effectiveness of a Security and Privacy Requirements Engineering methodology, namely Secure Tropos on the nine principles of the Theory of Notation. Our qualitative analysis revealed a partial satisfaction of these principles. © Springer International Publishing AG 2018.
"
10.1007/978-981-10-7796-8_14,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041098265&origin=inward,Conference Paper,SCOPUS_ID:85041098265,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),constructing a goal model from requirements descriptions based on extraction rules,"
AbstractView references

A goal model, which is one of the common requirements models, has advantages of formalizing and visualizing results of requirements analysis. The model regards a requirement as a goal, and the root goal that is achieved by system execution should be decomposed to precondition goals. Current systems are large and complexed, so that there are a lot of requirements to be implemented. Therefore it is difficult to extract all goals and construct an elaborated goal model manually. In this paper we propose a process to support constructing goal models from requirements descriptions written in a natural language. In the proposed process, extraction rules are used to extract goals from requirements descriptions and then to construct a goal model from the goals. To evaluate our process, we applied the process to two system descriptions to construct goal models. The results show that the proposed process extracted appropriate goals and successfully assembled these goals in a goal hierarchy. We also report preliminary results of automating the proposed process. © Springer Nature Singapore Pte Ltd. 2018.
"
10.1587/transinf.2017SWP0005,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85040253504&origin=inward,Conference Paper,SCOPUS_ID:85040253504,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a joint neural model for fine-grained named entity classification of wikipedia articles,"
AbstractView references

This paper addresses the task of assigning labels of fine-grained named entity (NE) types to Wikipedia articles. Information of NE types are useful when extracting knowledge of NEs from natural language text. It is common to apply an approach based on supervised machine learning to named entity classification. However, in a setting of classifying into fine-grained types, one big challenge is how to alleviate the data sparseness problem since one may obtain far fewer instances for each fine-grained types. To address this problem, we propose two methods. First, we introduce a multi-task learning framework, in which NE type classifiers are all jointly trained with a neural network. The neural network has a hidden layer, where we expect that effective combinations of input features are learned across different NE types. Second, we propose to extend the input feature set by exploiting the hyperlink structure of Wikipedia. While most of previous studies are focusing on engineering features from the articles’ contents, we observe that the information of the contexts the article is mentioned can also be a useful clue for NE type classification. Concretely, we propose to learn article vectors (i.e. entity embeddings) from Wikipedia’s hyperlink structure using a Skip-gram model. Then we incorporate the learned article vectors into the input feature set for NE type classification. To conduct large-scale practical experiments, we created a new dataset containing over 22,000 manually labeled articles. With the dataset, we empirically show that both of our ideas gained their own statistically significant improvement separately in classification accuracy. Moreover, we show that our proposed methods are particularly effective in labeling infrequent NE types. We’ve made the learned article vectors publicly available. The labeled dataset is available if one contacts the authors. Copyright © 2018 The Institute of Electronics, Information and Communication Engineers
"
10.5277/e-Inf180101,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85039871546&origin=inward,Article,SCOPUS_ID:85039871546,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a graphical modelling editor for starsoc design flow tool based on model driven engineering approach,"
AbstractView references

Background: Due to the increasing complexity of embedded systems, system designers use higher levels of abstraction in order to model and analyse system performances. STARSoC (Synthesis Tool for Adaptive and Reconfigurable System-on-Chip) is a tool for hardware/software co-design and the synthesis of System-on-Chip (SoC) starting from a high level model using the StreamsC textual language. The process behaviour is described in the C syntax language, whereas the architecture is defined with a small set of annotation directives. Therefore, these specifications bring together a large number of details which increase their complexity. However, graphical modelling is better suited for visualizing system architecture. Objectives: In this paper, the authors propose a graphical modelling editor for STARSoC design tool which allows models to be constructed quickly and legibly. Its intent is to assist designers in building their models in terms of the UML Component-like Diagram, and in the automatic translation of the drawn model into StreamsC specification. Methods: To achieve this goal, the Model-Driven Engineering (MDE) approach and well-known frameworks and tools on the Eclipse platform were employed. Conclusion: Our results indicate that the use of the Model-Driven Engineering (MDE) approach reduces the complexity of embedded system design, and it is sufficiently flexible to incorporate new design needs.
"
10.1007/978-3-319-69832-8_7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85033668728&origin=inward,Conference Paper,SCOPUS_ID:85033668728,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),hlogo: a haskell stm-based parallel variant of netlogo,"
AbstractView references

Agent-based Modeling and Simulation (ABMS) has become a quite popular approach among researchers in the community, mainly due to its simplicity, expressiveness and wide applicability. However, in most cases, ABMS tools demonstrate reduced performance, especially when dealing with large experiments. This paper presents HLogo, a parallel variant of the NetLogo ABMS framework, that aims to increase the performance of simulations by utilizing Software Transactional Memory and multi-core CPUs, while maintaining the user friendliness of NetLogo. HLogo is implemented as a Domain Specific Language embedded in the functional language Haskell, which means that it also inherits Haskell’s features, such as strong static typing, a module system and a vast collection of programming libraries. © Springer International Publishing AG 2018.
"
10.1007/978-3-319-67459-9_40,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85029814427&origin=inward,Conference Paper,SCOPUS_ID:85029814427,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),flipping out in japan: engineering the academic english classroom for innovation,"
AbstractView references

Inspired by Simon Sinek’s TED Talk, “How Great Leaders Inspire Action” (2009) and Carol Dweck’s Mindset (2006), George Couros’s book, The Innovator’s Mindset (2015) builds idea of a “flipped classroom”. Couros’s thinking resonated with an American engineer, partnering with a seasoned language teacher. The routine class, Academic English, required for all third year Electrical/Electronic Engineering students presents a context for creative improvisation. We are “flipping” the classroom. Standard Academic English is simple. Right? The true case is that is not a simple matter at all. Too much vocabulary for too long a time period is stultifying and hypnotic. A 90-minute class once each week is not the best setting, either. The problem is how to teach a substantial amount of technical vocabulary to a large number of Japanese “false beginners,” who have studied English since age twelve, yet who evidently do not wish to speak or be seen to understand spoken English. This situation led to the experimental “flipped” classroom, in which homework is done outside of the class while class time is devoted to laboratory/workshop activities in teams. The investigator roles are Professor and Language Facilitator (Principle Investigator). In this work, we set up the experiment of a new vocabulary learning strategy (VLS), “flipped classroom” combined with Middle-Up-Down2 management style and the SECI3 model of dynamic knowledge creation. Via an intensive 15-week course, students are being immersed in an international-style conference setting, where they must regularly participate in mini-poster presentations on various technical topics. Our research shows that the topic of vocabulary learning strategies (VLSs4) focused on evaluating the efficacies of existing techniques rather than creating new techniques. Furthermore, previous VLS studies largely aimed at word recall rather than on context mastery. In this study, we specifically ask if the use of e-VLS comprised of mini-poster sessions and centered on a core set of engineering vocabulary can have a significant impact on English listening and speaking skills. © Springer International Publishing AG 2018.
"
10.1109/TETC.2017.2731984,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85029188711&origin=inward,Article,SCOPUS_ID:85029188711,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"pads: design and implementation of a cloud-based, immersive learning environment for distributed systems algorithms","
AbstractView references

As distributed systems become more complex, understanding the underlying algorithms that make these systems work becomes even harder. Traditional learning modalities based on didactic teaching and theoretical proofs alone are no longer sufficient for a holistic understanding of these algorithms. Instead, an environment that promotes an immersive, hands-on learning of distributed systems algorithms is needed to complement existing teaching modalities. Such an environment must be flexible to support the learning of a variety of algorithms. The environment should also support extensibility and reuse since many of these algorithms share several common traits with each other while differing only in some aspects. Finally, it must also allow students to experiment with large-scale deployments in a variety of operating environments. To address these concerns, we use the principles of software product lines and model-driven engineering, and adopt the cloud platform to design an immersive learning environment called the Playground of Algorithms for Distributed Systems (PADS). A prototype implementation of PADS is described to showcase use cases involving BitTorrent Peer-to-Peer file sharing, ZooKeeper-based coordination, and Paxos-based consensus, which show the benefits of rapid deployment of the distributed systems algorithms. Results from a preliminary user study are also presented. © 2013 IEEE.
"
10.1007/s11265-017-1226-x,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85011798124&origin=inward,Article,SCOPUS_ID:85011798124,scopus,2018-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),efficient system-level hardware synthesis of dataflow programs using shared memory based fifo: hevc decoder case study,"
AbstractView references

The purpose of this paper is to raise the level of abstraction in the design of embedded systems to the system-level. A novel design flow was proposed that enables an efficient hardware implementation of video processing applications described using a Domain-Specific Language (DSL) for dataflow programming. Despite the huge advancements in High-Level Synthesis (HLS) for Field-Programmable Gate Arrays (FPGAs), designers are still required to have detailed knowledge about coding techniques and the targeted architecture to achieve efficient solutions. Moreover, the main downside of the High-Level Synthesis (HLS) tools is the lack of the entire system consideration. As a remedy, in this work, we propose a design flow that combines a dataflow compiler for generating C-based High-Level Synthesis (HLS) descriptions from a dataflow description and a C-to-gate synthesizer for generating Register Transfer Level (RTL) descriptions. The challenge of implementing the communication channels of dataflow programs relying on Model of Computations (MoC) in Field-Programmable Gate Array (FPGA) is the minimization of the communication overhead. In this issue, we introduced a new interface synthesis approach that maps the large amounts of data that multimedia and image processing applications process, to shared memories on the Field-Programmable Gate Array (FPGA). This leads to a tremendous decrease in the latency and an increase in the throughput. These results were demonstrated upon the hardware synthesis of the emerging High-Efficiency Video Coding (HEVC) standard. Simulation results showed that the proposed implementation has increased throughput by a 5.2× speedup and reduced latency by a 3.8× speedup compared to a state-of-the-art implementation. © 2017, Springer Science+Business Media New York.
"
10.1109/FIE.2017.8190608,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85043299791&origin=inward,Conference Paper,SCOPUS_ID:85043299791,scopus,2017-12-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),leveraging institutional data to understand student perceptions of teaching in large engineering classes,"
AbstractView references

A global push to pursue careers in engineering has led to an increase in enrollment in engineering programs. However, rising student populations have led institutions to make compromises in order to effectively manage existing resources and rising costs, such as resorting to large classes despite evidence that they may be detrimental to student learning. Recognizing that large classes are both a necessity for institutions and a challenge for the instructors who teach them, we seek ways to help faculty create effective learning environments despite the difficulties posed by this setting. Developing an effective learning environment requires instructors to reflect and consider input from various sources, including students. A source of data for student input are student perceptions of teaching surveys. This paper used the MUSIC Model of Academic Motivation as basis to characterize qualitative data from student surveys with respect to two of the five MUSIC dimensions: Success and Caring. We allowed categorical variables to emerge from qualitative data and investigated how quantitative results from the student evaluation (e.g., Did the instructor present the material clearly?) varied across categories. The manually-analyzed text data were also used to explore text analytics as a qualitative analysis technique for course evaluation surveys. © 2017 IEEE.
"
10.1109/IESC.2017.8167478,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85046534095&origin=inward,Conference Paper,SCOPUS_ID:85046534095,scopus,2017-12-05,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cempl: a new domain-specific language for rapid modeling of cross-energy systems,"
AbstractView references

The extensive involvement of energy conversion and storage technologies is becoming increasingly important in the context of energy transition. A sufficient modeling and simulation of large-scale and coherent cross-energy systems is essential to enable decision makers to put the right course. This process implies growing complexity in terms of physical and market economy modeling on the device-level, and more important, the ensemble playing and actual integration into energy networks. Currently, the underlying process of software development with general-purpose programming languages is very time consuming and inherents subtasks that recur in multiple industry and research projects. Based on the concepts of Language-Oriented Programming (LOP), we provide a high-level Domain-Specific Language (DSL) for energy experts. The Cross Energy Management Programming Language (CEMPL) acts as a well-defined communication interface between experts in the fields of renewable energy and computer science and provides a simple and clear syntax. An intrinsic language support for modularization and hierarchically component design promotes reusability and favors automatic device instantiation. The requirement to define manifold objectives is achieved by the paradigm of declarative programming. This paper describes CEMPL, its embedding in a simulation environment and exemplary fields of application. © 2017 IEEE.
"
10.1109/ICCKE.2017.8167924,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85046455415&origin=inward,Conference Paper,SCOPUS_ID:85046455415,scopus,2017-12-05,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),sentiment analysis on twitter using mcdiarmid tree algorithm,"
AbstractView references

In recent years advent of social networking services has created large amounts of data. Microblogging website is a kind of social network in which users share short messages with others. One of the most popular microblogging services is Twitter. Every day millions of people post their opinions and sentiments in this microblog. Due to the large numbers of tweets, finding new approaches to discover and summarize the general overview of a specific topic has become a new challenge. Twitter messages are generated constantly and arrive at high speed and follow data stream model; hence, to predict the sentiment on Twitter we must apply algorithms which can do this in real time and under limited time. Hoeffding tree algorithm is the most popular tool in mining data streams. For this tree algorithm the Hoeffding's bound is utilized to find the smallest amount of instances required in a node to choose a splitting attribute. Replacing the MacDiarmid's bound in Hoeffding tree algorithm, we obtain McDiarmid tree algorithm which is employed in this paper. The accuracy from the McDiarmid tree for sentiment analysis on Twitter is very close to that from the Hoeffding tree; however, the process time of the former has considerably decreased. © 2017 IEEE.
"
10.1109/ECAI.2017.8166457,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85043303627&origin=inward,Conference Paper,SCOPUS_ID:85043303627,scopus,2017-12-04,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),intruder monitoring system for local networks using python,"
AbstractView references

Nowadays, Information Security specialists focus on the development of complex solutions for monitoring large enterprise networks. This paper is intended to document a security solution developed within Python programming language created for the use in LANs. Based on Open-Source software and constructed to run on low-cost hardware, this program is designed to run ad-hoc or on demand scanning across a network and notify the owner through alarms in case of intrusions. This article will additionally analyze the program's integration with related concepts like CIA (Confidentiality, Integrity, and Availability) of Information Security, Defense in Depth Model and Cyber Kill Chain. © 2017 IEEE.
"
10.1088/1757-899X/263/4/042005,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85037814183&origin=inward,Conference Paper,SCOPUS_ID:85037814183,scopus,2017-12-03,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),real time text analysis,"
AbstractView references

This paper aims to illustrate real time analysis of large scale data. For practical implementation we are performing sentiment analysis on live Twitter feeds for each individual tweet. To analyze sentiments we will train our data model on sentiWordNet, a polarity assigned wordNet sample by Princeton University. Our main objective will be to efficiency analyze large scale data on the fly using distributed computation. Apache Spark and Apache Hadoop eco system is used as distributed computation platform with Java as development language. © Published under licence by IOP Publishing Ltd.
"
10.1016/j.infsof.2017.07.006,S0950584916303287,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85025631316&origin=inward,Article,SCOPUS_ID:85025631316,scopus,2017-12-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),developing software systems to big data platform based on mapreduce model: an approach based on model driven engineering,"
                  
                     Context: The need to analyze a large volume and variety of data for the purpose of extracting information has been promoting investments in Big Data, e.g., for storage, analysis and, more recently, methodologies and approaches for software system development for Big Data platforms. The application of software engineering for Big Data is recent and emerging, so in the literature we find a number of challenges and opportunities related to Big Data, but few practical approaches.
               
                  Objective
                  In this paper, we propose a practical approach based on MDE (Model Driven Engineering) to support the semi-automated development of software systems for Big Data platform that use MapReduce model.
               
                  Method
                  The proposed approach consists of framework, process, metamodels, visual Alf, transformation definitions written in ATL and Eclipse IDE plug-in. The proposed framework uses concepts of MDE, Weaving and software development based on Y. Our proposed process guides the use of our approach. A graphical notation and extended metamodel for Alf (i.e. visual Alf) assign executable behavior for UML or DSLs. An Eclipse IDE plug-in implements our approach.
               
                  Results
                  We show the applicability of the proposed approach through an illustrative example.
               
                  Conclusion
                  Our approach brings a contribution because the development of software systems is assisted by models which preserves the business logic and adds Big Data features throughout the development process.
               "
10.1109/ITOEC.2017.8122512,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85043394959&origin=inward,Conference Paper,SCOPUS_ID:85043394959,scopus,2017-11-27,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),building an unequal spacing fractal probability model for layered and jointed rock mass and numerical implementation,"
AbstractView references

As the joint spacing of layered and jointed rock masses has the characteristic of discontinuous distribution, building a network model reflecting the actual distribution of joint spacing will be the research basis for calculating and analyzing the mechanical behavior of layered and jointed rock masses. A tunnel under construction in Chongqing was chosen as the background for the engineering research. The joint spacing was measured on site by scanline examination to analyze the fractal distribution pattern of the joint spacing from the perspective of fractal geometry. This enabled us to derive the fractal dimension D, which is capable of reflecting the distribution of the spacing, and the fractal distribution probability density function. We performed a Monte Carlo random simulation employing MATLAB for programming. The joint spacing simulation data obtained in this way was used to construct a three-dimensional network model of the layered and joint rock masses through the use of distinct element code (3DEC) and the internal fish programming language. The research results showed that the joint spacing of layered and jointed rock masses had a better self-similar characteristic and that the theory of fractal geometry could be used to obtain a good description of the joint spacing distribution. It was also found that the fractal distribution of joint spacing contains more spacing distribution information than the negative exponential distribution, because it more closely resembled the actual distribution. The numerical model that was built based on the fractal distribution of the unequal spacing of the layered and jointed rock mass using the Monte Carlo method had optimal statistical similarity with the actual data; thus, it offered a practical method for building a numerical model of layered and jointed rock masses. These research findings laid the foundation for subsequent research on the extent to which joint spacing distribution affects the metamorphism of wall rock, as well as changes in the locations and distribution of plastic regions when excavating a tunnel in layered and jointed rock masses. © 2017 IEEE.
"
10.1109/KSE.2017.8119435,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85043704450&origin=inward,Conference Paper,SCOPUS_ID:85043704450,scopus,2017-11-22,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),on the usage of character distribution for the detection of web attacks,"
AbstractView references

Character distribution has been extensively used in literature to build models for the detection of web attacks. This paper explores that character distribution models should be built at attribute level in order to achieve a reasonable accuracy. However, attaching detection models to every single attribute leads to high memory and time complexities, which make attribute-specific models less practical. To remove these barriers, a simple yet effective solution has been proposed. In more details, by exploiting the language function of characters, character distribution can be reduced in size and rearranged in an intentional manner so that both time and memory complexities are reduced significantly. Detection models that use minimized and rearranged character distribution are, therefore, highly efficient and practical, especially suitable to large, high-traffic web applications. © 2017 IEEE.
"
10.1109/ASE.2017.8115678,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041438717&origin=inward,Conference Paper,SCOPUS_ID:85041438717,scopus,2017-11-20,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a language model for statements of software code,"
AbstractView references

Building language models for source code enables a large set of improvements on traditional software engineering tasks. One promising application is automatic code completion. State-of-the-art techniques capture code regularities at token level with lexical information. Such language models are more suitable for predicting short token sequences, but become less effective with respect to long statement level predictions. In this paper, we have proposed PCC to optimize the token-level based language modeling. Specifically, PCC introduced an intermediate representation (IR) for source code, which puts tokens into groups using lexeme and variable relative order. In this way, PCC is able to handle long token sequences, i.e., group sequences, to suggest a complete statement with the precise synthesizer. Further more, PCC employed a fuzzy matching technique which combined genetic and longest common subsequence algorithms to make the prediction more accurate. We have implemented a code completion plugin for Eclipse and evaluated it on open-source Java projects. The results have demonstrated the potential of PCC in generating precise long statement level predictions. In 30%-60% of the cases, it can correctly suggest the complete statement with only six candidates, and 40%-90% of the cases with ten candidates. © 2017 IEEE.
"
10.1109/VLHCC.2017.8103479,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85040961745&origin=inward,Conference Paper,SCOPUS_ID:85040961745,scopus,2017-11-09,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),investigating uni-stroke gesture input for diagram editors on large wall-mounted touchscreens,"
AbstractView references

Nowadays, touch-input devices are widely available. The use of such touch input devices, e.g., large wall-mounted touchscreens in (team) meeting rooms appear appropiate and desirable. Thus, team meetings can change from one-man-presentation-shows towards collaborative and interactive developing of plans and processes via designing and creating its corresponding diagrams. In this paper, we present an approach for interacting with diagrams using uni-stroke touch gestures. We focus on large wall-mounted touchscreens and present an editor design for such environments. In order to validate the usability of our approach we report on the results of a user study with a diagram editor for Business Process Modeling Networks. © 2017 IEEE.
"
10.1109/MODELS.2017.35,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85040540829&origin=inward,Conference Paper,SCOPUS_ID:85040540829,scopus,2017-11-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),symbolic execution for realizability-checking of scenario-based specifications,"
AbstractView references

Scenario-based specification with the Scenario Modeling Language (SML) is an intuitive approach for formally specifying the behavior of reactive systems. SML is close to how humans conceive and communicate requirements, yet SML is executable and simulation and formal realizability checking can find specification flaws early. The realizability checking complexity is, however, exponential in the number of scenarios and variables. Therefore algorithms relying on explicit-state exploration do not scale and, especially when specifications have message parameters and variables over large domains, fail to unfold their potential. In this paper, we present a technique for the symbolic execution of SML specifications that interprets integer message parameters and variables symbolically. It can be used for symbolic realizability checking and interactive symbolic simulation. We implemented the technique in ScenarioTools. Evaluation shows drastic performance improvements over the explicit-state approachfor a range of examples. Moreover, symbolic checking produces more concise counter examples, which eases the comprehension of specification flaws. © 2017 IEEE.
"
10.1371/journal.pcbi.1005851,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85036476747&origin=inward,Article,SCOPUS_ID:85036476747,scopus,2017-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),strawberry: fast and accurate genome-guided transcript reconstruction and quantification from rna-seq,"
AbstractView references

We propose a novel method and software tool, Strawberry, for transcript reconstruction and quantification from RNA-Seq data under the guidance of genome alignment and independent of gene annotation. Strawberry consists of two modules: assembly and quantification. The novelty of Strawberry is that the two modules use different optimization frameworks but utilize the same data graph structure, which allows a highly efficient, expandable and accurate algorithm for dealing large data. The assembly module parses aligned reads into splicing graphs, and uses network flow algorithms to select the most likely transcripts. The quantification module uses a latent class model to assign read counts from the nodes of splicing graphs to transcripts. Strawberry simultaneously estimates the transcript abundances and corrects for sequencing bias through an EM algorithm. Based on simulations, Strawberry outperforms Cufflinks and StringTie in terms of both assembly and quantification accuracies. Under the evaluation of a real data set, the estimated transcript expression by Strawberry has the highest correlation with Nanostring probe counts, an independent experiment measure for transcript expression. Availability: Strawberry is written in C++14, and is available as open source software at https://github.com/ruolin/strawberry under the MIT license. © 2017 Liu, Dickerson.
"
10.1051/0004-6361/201730852,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85032699656&origin=inward,Article,SCOPUS_ID:85032699656,scopus,2017-11-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cosmological exploitation of cosmic void statistics: new numerical tools in the cosmobolognalib to extract cosmological constraints from the void size function,"
AbstractView references

Context. We present new numerical tools to analyse cosmic void catalogues, implemented inside the CosmoBolognaLib, a large set of open source C++/Python numerical libraries. Aims. The CosmoBolognaLib provides a common numerical environment for cosmological calculations. This work extends these libraries by adding new algorithms for cosmological analyses of cosmic voids, covering the existing gap between theory and observations. Methods. We implemented new methods to model the size function of cosmic voids, in both observed and simulated samples of dark matter and biased tracers. Moreover, we provide new numerical tools to construct unambiguous void catalogues. The latter are designed to be independent of the void finder, in order to allow a high versatility in comparing independent results. Results. The implemented open source software is available at the GitHub repository of the CosmoBolognaLib. We also provide a full doxygen documentation and some example codes that explain how to use these libraries. © ESO, 2017.
"
10.1145/3133956.3134015,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041449815&origin=inward,Conference Paper,SCOPUS_ID:85041449815,scopus,2017-10-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deeplog: anomaly detection and diagnosis from system logs through deep learning,"
AbstractView references

Anomaly detection is a critical step towards building a secure and trustworthy system. The primary purpose of a system log is to record system states and signi.cant events at various critical points to help debug system failures and perform root cause analysis. Such log data is universally available in nearly all computer systems. Log data is an important and valuable resource for understanding system status and performance issues; therefore, the various system logs are naturally excellent source of information for online monitoring and anomaly detection. We propose DeepLog, a deep neural network model utilizing Long Short-Term Memory (LSTM), to model a system log as a natural language sequence. This allows DeepLog to automatically learn log patterns from normal execution, and detect anomalies when log pa.erns deviate from the model trained from log data under normal execution. In addition, we demonstrate how to incrementally update the DeepLog model in an online fashion so that it can adapt to new log pa.erns over time. Furthermore, DeepLog constructs work.ows from the underlying system log so that once an anomaly is detected, users can diagnose the detected anomaly and perform root cause analysis effectively. Extensive experimental evaluations over large log data have shown that DeepLog has outperformed other existing log-based anomaly detection methods based on traditional data mining methodologies. © 2017 author(s).
"
10.1109/SysEng.2017.8088273,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85040117422&origin=inward,Conference Paper,SCOPUS_ID:85040117422,scopus,2017-10-26,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),model-based requirements engineering: architecting for system requirements with stakeholders in mind,"
AbstractView references

Specifying system requirements (SysReqs) is a critical activity in complex systems development. The SysReqs and emerging architecture are constructed through gradual and iterative transition from the problem domain and operational stakeholder requirements to the conceptual solution domain. They later constitute the basis for functional requirements elaborating, concept formation, technology selection, function-to-form allocation, and asset utilization. Only rarely can stakeholder requirements (SHRs) readily translate to SysReqs. Systems engineers must therefore elicit, analyze, and evolve the SysReqs, as these will radically affect the system's performance, robustness, endurance, and appeal. Model-Based Systems Engineering (MBSE) provides a framework for effective and consistent systems engineering and architecting. MBSE relies on modeling languages, such as Object-Process Methodology (OPM). OPM is a holistic MBSE paradigm and language for complex systems and processes, standardized as ISO 19450, which relies on the principle of minimal universal ontology. In this paper, we propose a model-based requirement engineering (MBRE) approach to facilitate the transition from SHRs to SysReqs, and from SysReqs to system architecture specification. We demonstrate the applicability of this framework in architecting a robotic baggage loading system for a leading international airport. © 2017 IEEE.
"
10.1109/AUTEST.2017.8080506,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85038585545&origin=inward,Conference Paper,SCOPUS_ID:85038585545,scopus,2017-10-23,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"integrated modeling and simulation framework expedite operational software testing, integration and verification for system of systems platform","
AbstractView references

Modeling and Simulation (M&S) has become an essential tool in the development, testing and verification of operational software in a complex multi-domain, multi-threaded heterogeneous system of systems environment. The complex systems of today encompass mix of hardware sub-systems (with varying degree of capabilities), software environments (comprising a plethora of development environments, operating systems and programming languages) and peripherals (sensors, effectors, actuators etc.), realization of which requires massive investments (human, capital, tools etc.) and carries a huge risk to cost, schedule and requirements compliance. These risks are alleviated to a large extent with a well formulated test architecture and integrated simulation environment with high fidelity systems modeling framework, where majority of system compliance metrics can be verified in a digital domain. © 2017 IEEE.
"
10.1145/3136014.3136026,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85037140429&origin=inward,Conference Paper,SCOPUS_ID:85037140429,scopus,2017-10-23,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),structural model subtyping with ocl constraints,"
AbstractView references

In model-driven engineering (MDE), models abstract the relevant features of software artefacts and model management operations, including model transformations, act on them automating large tasks of the development process. Flexible reuse of such operations is an important factor to improve productivity when developing and maintaining MDE solutions. In this work, we revisit the traditional notion of object subtyping based on subsumption, discarded by other approaches to model subtyping. We refine a type system for object-oriented programming, with multiple inheritance, to support model types in order to analyse its advantages and limitations with respect to reuse in MDE. Specifically, we extend type expressions with referential constraints and with OCL constraints. Our approach has been validated with a tool that extracts model types from (EMF) metamodels, paired with their OCL constraints, automatically and that exploits the extended subtyping relation to reuse model management operations. We show that structural model subtyping is expressive enough to support variants of model subtyping, including multiple, partial and dynamic model subtyping. The tool has received the ACM badge ""Artifacts Evaluated - Functional"". © 2017 Association for Computing Machinery.
"
10.1103/PhysRevE.96.042905,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85032855778&origin=inward,Article,SCOPUS_ID:85032855778,scopus,2017-10-13,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),compliant contact versus rigid contact: a comparison in the context of granular dynamics,"
AbstractView references

We summarize and numerically compare two approaches for modeling and simulating the dynamics of dry granular matter. The first one, the discrete-element method via penalty (DEM-P), is commonly used in the soft matter physics and geomechanics communities; it can be traced back to the work of Cundall and Strack [P. Cundall, Proc. Symp. ISRM, Nancy, France 1, 129 (1971); P. Cundall and O. Strack, Geotechnique 29, 47 (1979)GTNQA80016-850510.1680/geot.1979.29.1.47]. The second approach, the discrete-element method via complementarity (DEM-C), considers the grains perfectly rigid and enforces nonpenetration via complementarity conditions; it is commonly used in robotics and computer graphics applications and had two strong promoters in Moreau and Jean [J. J. Moreau, in Nonsmooth Mechanics and Applications, edited by J. J. Moreau and P. D. Panagiotopoulos (Springer, Berlin, 1988), pp. 1-82; J. J. Moreau and M. Jean, Proceedings of the Third Biennial Joint Conference on Engineering Systems and Analysis, Montpellier, France, 1996, pp. 201-208]. The DEM-P and DEM-C are manifestly unlike each other: They use different (i) approaches to model the frictional contact problem, (ii) sets of model parameters to capture the physics of interest, and (iii) classes of numerical methods to solve the differential equations that govern the dynamics of the granular material. Herein, we report numerical results for five experiments: shock wave propagation, cone penetration, direct shear, triaxial loading, and hopper flow, which we use to compare the DEM-P and DEM-C solutions. This exercise helps us reach two conclusions. First, both the DEM-P and DEM-C are predictive, i.e., they predict well the macroscale emergent behavior by capturing the dynamics at the microscale. Second, there are classes of problems for which one of the methods has an advantage. Unlike the DEM-P, the DEM-C cannot capture shock-wave propagation through granular media. However, the DEM-C is proficient at handling arbitrary grain geometries and solves, at large integration step sizes, smaller problems, i.e., containing thousands of elements, very effectively. The DEM-P vs DEM-C comparison is carried out using a public-domain, open-source software package; the models used are available online. © 2017 American Physical Society.
"
10.1186/s12859-017-1857-8,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85031011134&origin=inward,Article,SCOPUS_ID:85031011134,scopus,2017-10-13,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a method for named entity normalization in biomedical articles: application to diseases and plants,"
AbstractView references

Background: In biomedical articles, a named entity recognition (NER) technique that identifies entity names from texts is an important element for extracting biological knowledge from articles. After NER is applied to articles, the next step is to normalize the identified names into standard concepts (i.e., disease names are mapped to the National Library of Medicine's Medical Subject Headings disease terms). In biomedical articles, many entity normalization methods rely on domain-specific dictionaries for resolving synonyms and abbreviations. However, the dictionaries are not comprehensive except for some entities such as genes. In recent years, biomedical articles have accumulated rapidly, and neural network-based algorithms that incorporate a large amount of unlabeled data have shown considerable success in several natural language processing problems. Results: In this study, we propose an approach for normalizing biological entities, such as disease names and plant names, by using word embeddings to represent semantic spaces. For diseases, training data from the National Center for Biotechnology Information (NCBI) disease corpus and unlabeled data from PubMed abstracts were used to construct word representations. For plants, a training corpus that we manually constructed and unlabeled PubMed abstracts were used to represent word vectors. We showed that the proposed approach performed better than the use of only the training corpus or only the unlabeled data and showed that the normalization accuracy was improved by using our model even when the dictionaries were not comprehensive. We obtained F-scores of 0.808 and 0.690 for normalizing the NCBI disease corpus and manually constructed plant corpus, respectively. We further evaluated our approach using a data set in the disease normalization task of the BioCreative V challenge. When only the disease corpus was used as a dictionary, our approach significantly outperformed the best system of the task. Conclusions: The proposed approach shows robust performance for normalizing biological entities. The manually constructed plant corpus and the proposed model are available at http://gcancer.org/plant and http://gcancer.org/normalization , respectively. © 2017 The Author(s).
"
10.1109/ACCESS.2017.2760060,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85030777898&origin=inward,Article,SCOPUS_ID:85030777898,scopus,2017-10-04,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cognitive behaviors modeling using uml profile: design and experience,"
AbstractView references

To achieve model reuse in combat effectiveness simulation systems development, cognitive decision behaviors are usually implemented using a scripting language, which is separate from the programming language used to implement simulation models. Therefore, it is desirable to establish a much better grounding for cognitive behaviors modeling. In the context of domain specific modeling, metamodeling from scratch for designing such a scripting language poses some limitations, among which is the issue of integrating various models that are represented by various customized languages with different syntax and semantics, together with a large expenditure of designing, implementing, and maintaining these languages and their supporting resources. Instead, UML profile-based metamodeling is adopted, as a lightweight extension to capture the cognitive domain specific concepts, relationships, and constraints. Moreover, a unifying framework is proposed to guide the cognitive domain specific profiles design. Upon this framework, the development process is shown through constructing an anti-submarine tactical profile in combat effectiveness simulation systems domain and the feasibility of the domain specific language is illustrated with an armed escort scenario. © 2013 IEEE.
"
10.21629/JSEE.2017.05.09,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85040170453&origin=inward,Article,SCOPUS_ID:85040170453,scopus,2017-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),model architecture-oriented combat system effectiveness simulation based on mde,"
AbstractView references

Combat system effectiveness simulation (CSES) is a special type of complex system simulation. Three non-functional requirements (NFRs), i.e. model composability, domain specific modeling, and model evolvability, are gaining higher priority from CSES users when evaluating different modeling methodologies for CSES. Traditional CSES modeling methodologies are either domain-neutral (lack of domain characteristics consideration and limited support for model composability) or domain-oriented (lack of openness and evolvability) and fall short of the three NFRs. Inspired by the concept of architecture in systems engineering and software engineering fields, we extend it into a concept of model architecture for complex simulation systems, and propose a model architecture-oriented modeling methodology in which the model architecture plays a central role in achieving the three NFRs. Various model-driven engineering (MDE) approaches and technologies, including simulation modeling platform (SMP), unified modeling language (UML), domain specific modeling (DSM), eclipse modeling framework (EMF), graphical modeling framework (GMF), and so forth, are applied where possible in representing the CSES model architecture and its components' behaviors from physical and cognitive domain aspects. A prototype CSES system, called weapon effectiveness simulation system (WESS), and a non-trivial air-combat simulation example are presented to demonstrate the methodology. © 1990-2011 Beijing Institute of Aerospace Information.
"
10.4230/LIPIcs.TIME.2017.12,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85030718327&origin=inward,Conference Paper,SCOPUS_ID:85030718327,scopus,2017-10-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),time expressions recognition with word vectors and neural networks,"
AbstractView references

This work re-examines the widely addressed problem of the recognition and interpretation of time expressions, and suggests an approach based on distributed representations and artificial neural networks. Artificial neural networks allow us to build highly generic models, but the large variety of hyperparameters makes it difficult to determine the best configuration. In this work we study the behavior of different models by varying the number of layers, sizes and normalization techniques. We also analyze the behavior of distributed representations in the temporal domain, where we find interesting properties regarding order and granularity. The experiments were conducted mainly for Spanish, although this does not affect the approach, given its generic nature. This work aims to be a starting point towards processing temporality in texts via word vectors and neural networks, without the need of any kind of feature engineering.
"
10.1145/3127041.3131362,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85036476407&origin=inward,Conference Paper,SCOPUS_ID:85036476407,scopus,2017-09-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),formal verification of complex systems: model-based and data-driven methods,"
AbstractView references

Two known shortcomings of standard techniques in formal veri-cation are the limited capability to provide system-level assertions, and the scalability to large, complex models, such as those needed in Cyber-Physical Systems (CPS) applications. Leveraging data, which nowadays is becoming ever more accessible, has the potential to mitigate such limitations. However, this leads to a lack of formal proofs that are needed for modern safety-critical systems. is contribution presents a research initiative that addresses these shortcomings by bringing model-based techniques and data-driven methods together, which can help pushing the envelope of existing algorithms and tools in formal verication and thus expanding their applicability to complex engineering systems, such as CPS. In the rst part of the contribution, we discuss a new, formal, measurement-driven and model-based automated technique, for the quantitative verication of physical systems with partly unknown dynamics. We formulate this setup as a data-driven Bayesian inference problem, formally embedded within a quantitative, model-based verication procedure. We argue that the approach can be applied to complex physical systems that are key for CPS applications, dealing with spatially continuous variables, evolving under complex dynamics, driven by external inputs, and accessed under noisy measurements. In the second part of the contribution, we concentrate on systems represented by models that evolve under probabilistic and heterogeneous (continuous/discrete - that is “hybrid” - as well as nonlinear) dynamics. Such stochastic hybrid models (also known as SHS) are a natural mathematical framework for CPS. With focus on model-based verication procedures, we provide algorithms for quantitative model checking of temporal specications on SHS with formal guarantees. is is aained via the development of formal abstraction techniques that are based on quantitative approximations. eory is complemented by algorithms, all packaged in soware tools that are available to users, and which are applied here in the domain of Smart Energy. © 2017 Copyright is held by the owner/author(s).
"
10.1109/REW.2017.72,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85034665514&origin=inward,Conference Paper,SCOPUS_ID:85034665514,scopus,2017-09-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),reqdl: a requirements description language to support requirements traces generation,"
AbstractView references

It is important to manage traceability between requirements and other artifacts, including stakeholders, sources, and system development elements. Requirements are often expressed independently from those artifacts using textual and model approaches. This fact makes hard and tedious the inference of trace links between them. This paper proposes a new Domain Specific Language (DSL), called REQDL, for describing requirements, and at the same time, capturing bi-directional traceability data, which concerns especially SYSML modeling elements. Using REQDL expressions, we aim at assisting the traceability operation by applying a model-to-model transformation of both REQDL and SYSML constructs. The main result is the generation of requirements trace models, which incorporate intention and viewpoint concepts in conformance with a predefined trace metamodel. We use a car cooling system to illustrate the paper contributions. © 2017 IEEE.
"
10.1109/REW.2017.82,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85034642132&origin=inward,Conference Paper,SCOPUS_ID:85034642132,scopus,2017-09-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards aligning multi-concern models via nlp,"
AbstractView references

The design of large-scale complex systems requires their analysis from multiple perspectives, often through the use of requirements models. Diversely located experts with different backgrounds (e.g., safety, security, performance) create such models using different requirements modeling languages. One open challenge is how to align these models such that they cover the same parts of the domain. We propose a technique based on natural language processing (NLP) that analyzes several models included in a project and provides suggestions to modelers based on what is represented in the models that analyze other concerns. Unlike techniques based on meta-model alignment, ours is flexible and language agnostic. We report the results of a focus group session in which experts from the air traffic management domain discussed our approach. © 2017 IEEE.
"
10.1109/REW.2017.38,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85034641436&origin=inward,Conference Paper,SCOPUS_ID:85034641436,scopus,2017-09-29,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),seamless integration of multirequirements in complex systems,"
AbstractView references

Requirements are the keystone of complex systems development. In order to reduce inconsistencies, requirements analysis is an important issue of systems engineering. In this context, there is a need for conciliating views of several stakeholders from different domains and for tracing these requirements from specification to realization. The computerization of analysis, with the help of a clearly defined semantics linked to a non-specialist readable language, should lead to overcome this major issue. Several works already go into this direction. The most popular ones are dealing with natural language, easily understandable but with few semantics. Other approaches propose more formal notations, with stronger semantics but then being less affordable by stakeholders. In this paper, we propose a preliminary work that should drive us to define a language dedicated to requirements which combine the best of both worlds in order to ease requirements analysis throughout the system lifecycle. © 2017 IEEE.
"
10.1109/SEAA.2017.29,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85034442203&origin=inward,Conference Paper,SCOPUS_ID:85034442203,scopus,2017-09-26,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),rapid construction of co-simulations of cyber-physical systems in hla using a dsl,"
AbstractView references

The development of cyber-physical systems (CPSs) is a multi-disciplinary process. A model-based approach during the design of a system is important for making design decisions during the exploration of alternatives. However, all disciplines use different modelling tools and techniques, which makes the integration of these models difficult and time-consuming. The use of the High Level Architecture (HLA) simplifies this problem, but still requires quite an effort to implement. Our work focuses on minimising the effort required to construct co-simulations. We have created a Domain Specific Language (DSL) to define a system design consisting of different types of models. We demonstrate how this DSL can be used to experiment with alternative designs of the system quickly. The DSL allows us to build virtual prototypes of CPSs without the large overhead of constructing the co-simulation. © 2017 IEEE.
"
10.1109/SEAA.2017.55,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85034422237&origin=inward,Conference Paper,SCOPUS_ID:85034422237,scopus,2017-09-26,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),dissecting design effort and drawing effort in uml modeling,"
AbstractView references

One argument in the discussion about the adoption of UML in industry is the supposedly large effort it takes to do modeling. Our study explores how the creation of UML models can be understood to consist of different cognitive activities: (i) designing: thinking about the design (ideation, key-design decision making), (ii) notation expression: expressing a design in a modeling notation and (iii) layouting: the spatial organization of model elements in a diagram. We explain that these different subactivities relate to different short-term and long-term benefits of modeling. In this study we present two controlled experiments with a total of 100 subjects creating models for a small system. In these experiments we focus on software models as represented through UML class diagram. Our results show that at least 56% of the effort spent on creating a class model is actually due to designing. Notation expression is around 41% of the model creation effort and layouting is in the order of 3%. This finding suggests that a significant part of creating models is devoted to design thinking about the problem. © 2017 IEEE.
"
10.1109/JCSSE.2017.8025938,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85031726669&origin=inward,Conference Paper,SCOPUS_ID:85031726669,scopus,2017-09-05,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),extracting uml class diagrams from software requirements in thai using nlp,"
AbstractView references

In software development, requirements, normally written in natural language, are documents that specify what users want in software products. Software developers then analyze these requirements to create domain models represented in UML diagrams in an attempt to comprehend what users need in the software products. These domain models are usually converted into design models and finally carried over into classes in source code. Thus, domain models have an impact on the final software products. However, creating correct domain models can be difficult when software developers are not skilled. Moreover, even for skilled developers, when requirements are large, wading through all requirements to create domain models can take times and might result in errors. Therefore, researchers have studied various approaches to apply natural language processing techniques to transform requirements written in natural language into UML diagrams. Those researches focus on requirements written in English. This paper proposes an approach to process requirements written in Thai to extract UML class diagrams using natural language processing techniques. The UML class diagram extraction is based on transformation rules that identify classes and attributes from requirements. The results are evaluated with recall and precision using truth values created by humans. Future works include identifying operations and relationships from requirements to complete class diagram extraction. Our research should benefit Thai software developers by reducing time in requirement analysis and also helping novice software developers to create correct domain models represented in UML class diagram. © 2017 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85032591174&origin=inward,Article,SCOPUS_ID:85032591174,scopus,2017-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),six emerging aerospace capabilities to watch,"
AbstractView references

Emerging manufacturing capabilities are helping the aerospace industry advance a faster, more powerful future. Emerging design tools, such as generative design, leverage artificial intelligence (AI) along with real-world data from the factory floor, the field and supply chain to produce parts more affordably, more quickly and at lower weights, critical for the aerospace industry. Cognitive Assistants, ever smarter, eventually will be able to do more and will be able to help get the job done in manufacturing plants. With human augmentation and augmented reality, super-strong humans now work in manufacturing plants. Encased in exoskeletons, workers become much stronger and can safely move heavy equipment they couldn't lift unaided. Designer materials are being designed and constructed at the atomic level using Integrated Computational Materials Engineering (ICME) to have certain critical properties. intelligent Machines will work alongside people in factories and help in smaller works as well. Transformative Computing can offer the best of the computer and human worlds, much faster but with human capabilities.
"
10.1016/j.jsr.2017.06.016,S0022437516304522,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85024399928&origin=inward,Article,SCOPUS_ID:85024399928,scopus,2017-09-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"work-related injuries among commercial janitors in washington state, comparisons by gender","
                  Introduction
                  We analyzed workers' compensation (WC) data to identify characteristics related to workers' compensation claim outcomes among janitorial service workers in Washington State.
               
                  Method
                  We analyzed WC data from the Washington State Department of Labor & Industries (L&I) State Fund (SF) from January 1, 2003 through December 31, 2013, for janitorial service workers employed in the National Occupational Research Agenda (NORA) Services Sector. We constructed multivariable models to identify factors associated with higher medical costs and increased time lost from work.
               
                  Results
                  There were 2,390 janitorial service compensable claims available for analysis. There were significant differences in injury type and other factors by gender, age, and language preference. Linguistic minority status was associated with longer time loss and higher median medical costs. Women were estimated to account for 35% of janitorial service workers but made up 55% of the compensable claims in this study.
               
                  Conclusions
                  Janitorial service workers comprise a large vulnerable occupational group in the U.S. workforce. Identifying differences by injury type and potential inequitable outcomes by gender and language is important to ensuring equal treatment in the workers' compensation process.
               
                  Practical applications
                  There were significant differences in injury and individual characteristics between men and women in this study. Women had twice the estimated rate of injury to men, and were more likely to require Spanish language materials. Improving communication for training and knowledge about the workers' compensation system appear to be high priorities in this population of injured janitorial service workers.
               "
10.1145/3106237.3117771,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85030785829&origin=inward,Conference Paper,SCOPUS_ID:85030785829,scopus,2017-08-21,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),automated identification of security issues from commit messages and bug reports,"
AbstractView references

The number of vulnerabilities in open source libraries is increasing rapidly. However, the majority of them do not go through public disclosure. These unidentified vulnerabilities put developers' products at risk of being hacked since they are increasingly relying on open source libraries to assemble and build software quickly. To find unidentified vulnerabilities in open source libraries and secure modern software development, we describe an efficient automatic vulnerability identification system geared towards tracking large-scale projects in real time using natural language processing and machine learning techniques. Built upon the latent information underlying commit messages and bug reports in open source projects using GitHub, JIRA, and Bugzilla, our K-fold stacking classiffer achieves promising results on vulnerability identification. Compared to the state of the art SVM-based classiffer in prior work on vulnerability identification in commit messages, we improve precision by 54.55% while maintaining the same recall rate. For bug reports, we achieve a much higher precision of 0.70 and recall rate of 0.71 compared to existing work. Moreover, observations fromrunning the trained model at SourceClear in production for over 3 months has shown 0.83 precision, 0.74 recall rate, and detected 349 hidden vulnerabilities, proving the efffectiveness and generality of the proposed approach. © 2017 Association for Computing Machinery.
"
10.1145/3106237.3106252,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85030770145&origin=inward,Conference Paper,SCOPUS_ID:85030770145,scopus,2017-08-21,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),is there a mismatch between real-world feature models and product-line research?,"
AbstractView references

Feature modeling has emerged as the de-facto standard to compactly capture the variability of a software product line. Multiple feature modeling languages have been proposed that evolved over the last decades to manage industrial-size product lines. However, less expressive languages, solely permitting require and exclude constraints, are permanently and carelessly used in product-line research. We address the problem whether those less expressive languages are sufficient for industrial product lines. We developed an algorithm to eliminate complex cross-tree constraints in a feature model, enabling the combination of tools and algorithms working with different feature model dialects in a plug-and-play manner. However, the scope of our algorithm is limited. Our evaluation on large feature models, including the Linux kernel, gives evidence that require and exclude constraints are not sufficient to express real-world feature models. Hence, we promote that research on feature models needs to consider arbitrary propositional formulas as cross-tree constraints prospectively. © 2017 Association for Computing Machinery.
"
10.1109/ICAC.2017.48,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85034424319&origin=inward,Conference Paper,SCOPUS_ID:85034424319,scopus,2017-08-08,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),language support for modular autonomic managers in reconfigurable software components,"
AbstractView references

Dynamic reconfiguration is a key capability of Component-based Software Systems to achieve self-adaptation as it provides means to cope with environment changes at runtime. The space of configurations is defined by the possible assemblies of components, and navigating this space while achieving goals and maintaining structural properties is managed in an auto- nomic loop. The natural architectural structure of component-based systems calls for hierarchy and modularity in the design and implementation of composites and their managers, and requires support for coordinated multiple autonomic loops.In this paper, we leverage the modularity capability to strengthen the Domain-Specific Language (DSL) Ctrl-F, targeted at the design of autonomic managers in component-based systems. Its original definition involved discrete control-theoretical management of reconfigurations, providing assur-ances on the automated behaviors. The objective of modularity is two-fold: from the design perspective, it allows designers to seamlessly decompose a complex system into smaller pieces of reusable architectural elements and adaptive behaviours. From the compilation point of view, we provide a systematical and generative approach to decompose control problems described in the architectural level while relying on mechanisms of modular Discrete Control Synthesis (DCS), which allows us to cope with the combinatorial complexity that is inherent to DCS problems. We show the applicability of our approach by applying it to the self-adaptive case study of the existing RUBiS/Brownout eBay-like web auction system. © 2017 IEEE.
"
10.1109/QRS-C.2017.67,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85034447611&origin=inward,Conference Paper,SCOPUS_ID:85034447611,scopus,2017-08-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),optimal test case generation for simulink models using slicing,"
AbstractView references

Simulink is widely used for avionics and automotive systems design within model driven approach. For system verification and validation effectively, it is essential to generate test cases for Simulink models which guarantee high coverage of requirements and completeness required by safety-critical systems certification. However, for large-scale Simulink models, there is limited ability of test generation tools because state-space explosions occur in calculations, and structure coverage of test cases cannot meet the standard because of complex dependency relations in the model. To overcome these drawbacks, we propose a model slicing technique for optimal test case generation, which includes a static slicing algorithm to decrease model scale and generate tests for requirements, and a dynamic slicing algorithm to improve the structure coverage of test cases. Furthermore, we evaluate the effectiveness of slicing approach with avionics Simulink models. The experimental shows that complexity of Simulink models can be reduced and high structure coverage can be reached. © 2017 IEEE.
"
10.1145/3077136.3080811,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85029379569&origin=inward,Conference Paper,SCOPUS_ID:85029379569,scopus,2017-08-07,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deep character-level click-through rate prediction for sponsored search,"
AbstractView references

Predicting the click-Through rate of an advertisement is a critical component of online advertising platforms. In sponsored search, the click-Through rate estimates the probability that a displayed advertisement is clicked by a user a.er she submits a query to the search engine. Commercial search engines typically rely on machine learning models trained with a large number of features to make such predictions. .is inevitably requires a lot of engineering efforts to define, compute, and select the appropriate features. In this paper, we propose two novel approaches (one working at character level and the other working at word level) that use deep convolutional neural networks to predict the click-Through rate of a queryadvertisement pair. Speci.cally, the proposed architectures only consider the textual content appearing in a query-Advertisement pair as input, and produce as output a click-Through rate prediction. By comparing the character-level model with the word-level model, we show that language representation can be learnt from scratch at character level when trained on enough data. .rough extensive experiments using billions of query-Advertisement pairs of a popular commercial search engine, we demonstrate that both approaches significantly outperform a baseline model built on well-selected text features and a state-of-The-Art word2vec-based approach. Finally, by combining the predictions of the deep models introduced in this study with the prediction of the model in production of the same commercial search engine, we significantly improve the accuracy and the calibration of the click-Through rate prediction of the production system. © 2017 Copyright held by the owner/author(s).
"
10.1109/TPDS.2017.2654246,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85028510650&origin=inward,Article,SCOPUS_ID:85028510650,scopus,2017-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),evaluating scalable distributed erlang for scalability and reliability,"
AbstractView references

Large scale servers with hundreds of hosts and tens of thousands of cores are becoming common. To exploit these platforms software must be both scalable and reliable, and distributed actor languages like Erlang are a proven technology in this area. While distributed Erlang conceptually supports the engineering of large scale reliable systems, in practice it has some scalability limits that force developers to depart from the standard language mechanisms at scale. In earlier work we have explored these scalability limitations, and addressed them by providing a Scalable Distributed (SD) Erlang library that partitions the network of Erlang Virtual Machines (VMs) into scalable groups (s-groups). This paper presents the first systematic evaluation of SD Erlang s-groups and associated tools, and how they can be used. We present a comprehensive evaluation of the scalability and reliability of SD Erlang using three typical benchmarks and a case study. We demonstrate that s-groups improve the scalability of reliable and unreliable Erlang applications on up to 256 hosts (6,144 cores). We show that SD Erlang preserves the class-leading distributed Erlang reliability model, but scales far better than the standard model. We present a novel, systematic, and tool-supported approach for refactoring distributed Erlang applications into SD Erlang. We outline the new and improved monitoring, debugging and deployment tools for large scale SD Erlang applications. We demonstrate the scaling characteristics of key tools on systems comprising up to 10 K Erlang VMs. © 1990-2012 IEEE.
"
10.1016/j.media.2017.06.002,S1361841517300890,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021136343&origin=inward,Article,SCOPUS_ID:85021136343,scopus,2017-08-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a multimodal spatiotemporal cardiac motion atlas from mr and ultrasound data,"Cardiac motion atlases provide a space of reference in which the motions of a cohort of subjects can be directly compared. Motion atlases can be used to learn descriptors that are linked to different pathologies and which can subsequently be used for diagnosis. To date, all such atlases have been formed and applied using data from the same modality. In this work we propose a framework to build a multimodal cardiac motion atlas from 3D magnetic resonance (MR) and 3D ultrasound (US) data. Such an atlas will benefit from the complementary motion features derived from the two modalities, and furthermore, it could be applied in clinics to detect cardiovascular disease using US data alone. The processing pipeline for the formation of the multimodal motion atlas initially involves spatial and temporal normalisation of subjects’ cardiac geometry and motion. This step was accomplished following a similar pipeline to that proposed for single modality atlas formation. The main novelty of this paper lies in the use of a multi-view algorithm to simultaneously reduce the dimensionality of both the MR and US derived motion data in order to find a common space between both modalities to model their variability. Three different dimensionality reduction algorithms were investigated: principal component analysis, canonical correlation analysis and partial least squares regression (PLS). A leave-one-out cross validation on a multimodal data set of 50 volunteers was employed to quantify the accuracy of the three algorithms. Results show that PLS resulted in the lowest errors, with a reconstruction error of less than 2.3 mm for MR-derived motion data, and less than 2.5 mm for US-derived motion data. In addition, 1000 subjects from the UK Biobank database were used to build a large scale monomodal data set for a systematic validation of the proposed algorithms. Our results demonstrate the feasibility of using US data alone to analyse cardiac function based on a multimodal motion atlas."
10.1109/ICSE.2017.73,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85027719228&origin=inward,Conference Paper,SCOPUS_ID:85027719228,scopus,2017-07-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),uml diagram refinement (focusing on class-and use case diagrams),"
AbstractView references

Large and complicated UML models are not useful, because they are difficult to understand. This problem can be solved by using several diagrams of the same system at different levels of abstraction. Unfortunately, UML does not define an explicit set of rules for ensuring that diagrams at different levels of abstraction are consistent. We define such a set of rules, that we call diagram refinement. Diagram refinement is intuitive, and applicable to several kinds of UML diagrams (mostly to structural diagrams but also to use case diagrams), yet it rests on a solid mathematical basis-the theory of graph homomorphisms. We illustrate its usefulness with a series of examples. © 2017 IEEE.
"
10.1109/ICSE.2017.10,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85027701767&origin=inward,Conference Paper,SCOPUS_ID:85027701767,scopus,2017-07-19,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),can latent topics in source code predict missing architectural tactics?,"
AbstractView references

Architectural tactics such as heartbeat, resource pooling, and scheduling provide solutions to satisfy reliability, security, performance, and other critical characteristics of a software system. Current design practices advocate rigorous up-front analysis of the system's quality concerns to identify tactics and where in the code they should be used. In this paper, we explore a bottom-up approach to recommend architectural tactics based on latent topics discovered in the source code of projects. We present a recommender system developed by building predictor models which capture relationships between topical concepts in source code and the use of specific architectural tactics in that code. Based on an extensive analysis of over 116,000 open source systems, we identify significant correlations between latent topics in source code and the usage of architectural tactics. We use this information to construct a predictor for generating tactic recommendations. Our approach is validated through a series of experiments which demonstrate the ability to generate package-level tactic recommendations. We provide further validation via two large-scale studies of Apache Hive and Hadoop to illustrate that our recommender system predicts tactics that are actually implemented by developers in later releases. © 2017 IEEE.
"
10.1145/3147704.3147725,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85038439895&origin=inward,Conference Paper,SCOPUS_ID:85038439895,scopus,2017-07-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a taxonomy and catalog of business process model patterns,"
AbstractView references

While a large number of business process model patterns have been suggested in the literature, it is currently difficult to find patterns that might be useful in a given context. The reason is that the relevant publications are spread in various journals and other types of publications, and there is no guidance for locating a pattern that can be useful for solving a given problem. In our article, we present the results of a literature survey that has been conducted with the aim to get an exhaustive overview on existing publications on business process modeling patterns. The results of the survey allowed us to propose a taxonomy of existing patterns as a first step towards a pattern language of business process model patterns. Furthermore, we created an online catalog that allows finding publications on business process model patterns based on various search criteria. It is intended to be useful both for business process modeling practitioners as for researchers in need of sound literature references. Currently, this catalog includes links to 89 publications (usually containing more than one pattern). It is our aim to populate the catalog with patterns published in the future. © 2017 Copyright is held by the owner/author(s).
"
10.4204/EPTCS.250.2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85030158000&origin=inward,Conference Paper,SCOPUS_ID:85030158000,scopus,2017-07-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),language-based abstractions for dynamical systems,"
AbstractView references

Ordinary differential equations (ODEs) are the primary means to modelling dynamical systems in many natural and engineering sciences. The number of equations required to describe a system with high heterogeneity limits our capability of effectively performing analyses. This has motivated a large body of research, across many disciplines, into abstraction techniques that provide smaller ODE systems while preserving the original dynamics in some appropriate sense. In this paper we give an overview of a recently proposed computer-science perspective to this problem, where ODE reduction is recast to finding an appropriate equivalence relation over ODE variables, akin to classical models of computation based on labelled transition systems. © Valentina Castiglioni & Simone Tini.
"
10.1186/s12911-017-0468-7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021702868&origin=inward,Article,SCOPUS_ID:85021702868,scopus,2017-07-05,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),entity recognition from clinical texts via recurrent neural network,"
AbstractView references

Background: Entity recognition is one of the most primary steps for text analysis and has long attracted considerable attention from researchers. In the clinical domain, various types of entities, such as clinical entities and protected health information (PHI), widely exist in clinical texts. Recognizing these entities has become a hot topic in clinical natural language processing (NLP), and a large number of traditional machine learning methods, such as support vector machine and conditional random field, have been deployed to recognize entities from clinical texts in the past few years. In recent years, recurrent neural network (RNN), one of deep learning methods that has shown great potential on many problems including named entity recognition, also has been gradually used for entity recognition from clinical texts. Methods: In this paper, we comprehensively investigate the performance of LSTM (long-short term memory), a representative variant of RNN, on clinical entity recognition and protected health information recognition. The LSTM model consists of three layers: input layer - generates representation of each word of a sentence; LSTM layer - outputs another word representation sequence that captures the context information of each word in this sentence; Inference layer - makes tagging decisions according to the output of LSTM layer, that is, outputting a label sequence. Results: Experiments conducted on corpora of the 2010, 2012 and 2014 i2b2 NLP challenges show that LSTM achieves highest micro-average F1-scores of 85.81% on the 2010 i2b2 medical concept extraction, 92.29% on the 2012 i2b2 clinical event detection, and 94.37% on the 2014 i2b2 de-identification, which is considerably competitive with other state-of-the-art systems. Conclusions: LSTM that requires no hand-crafted feature has great potential on entity recognition from clinical texts. It outperforms traditional machine learning methods that suffer from fussy feature engineering. A possible future direction is how to integrate knowledge bases widely existing in the clinical domain into LSTM, which is a case of our future work. Moreover, how to use LSTM to recognize entities in specific formats is also another possible future direction. © 2017 The Author(s).
"
10.1109/SEsCPS.2017.7,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85027437179&origin=inward,Conference Paper,SCOPUS_ID:85027437179,scopus,2017-07-03,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),mcfsm: globally taming complex systems,"
AbstractView references

Industrial computing devices, in particular cyber-physical, real-time andsafety-critical systems, focus on reacting to external events and the need tocooperate with other devices to create a functional system. They are oftenimplemented with languages that focus on a simple, local description of how acomponent reacts to external input data and stimuli. Despite the trend inmodern software architectures to structure systems into largely independentcomponents, the remaining interdependencies still create rich behaviouraldynamics even for small systems. Standard and industrial programming approachesdo usually not model or extensively describe the global properties of an entiresystem. Although a large number of approaches to solve this dilemma have beensuggested, it remains a hard and error-prone task to implement systems withcomplex interdependencies correctly. We introduce multiple coupled finite state machines (McFSMs), a novelmechanism that allows us to model and manage such interdependencies. It isbased on a consistent, well-structured and simple global description. A soundtheoretical foundation is provided, and associated tools allow us to generateefficient low-level code in various programming languages using model-driventechniques. We also present a domain specific language to express McFSMs andtheir connections to other systems, to model their dynamic behaviour, and toinvestigate their efficiency and correctness at compile-time. © 2017 IEEE.
"
10.1109/ICACSIS.2017.8355078,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051104611&origin=inward,Conference Paper,SCOPUS_ID:85051104611,scopus,2017-07-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),feature grouping using the abstract behavioral specification language,"
AbstractView references

Software Product Line Engineering (SPLE) aims to produce variant-rich software with features based on diverse user requirements. SPLE uses the term feature to express system commonalities and variabilities. The Abstract Behavioral Specification (ABS) is an executable modeling language that supports SPLE. It uses feature models to declare and organize software variability as a tree of nested features. Users select the specific features they need based on this feature model. Such a selection process can be a too complex task if the number of features is quite large. In this research we propose to apply a grouping mechanism to the features of a feature model in order to reduce the complexity of the feature selection performed by the user. Using this mechanism the user selects the features in a software product based on groups instead of the more complex original feature model structure. We implemented the grouping mechanism as part of the ABS tool suite. The resulting groups are visualized using a simple web application. Case studies were employed to evaluate the proposed grouping mechanism. © 2017 IEEE.
"
10.1109/ICSPIS.2017.8311598,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050810009&origin=inward,Conference Paper,SCOPUS_ID:85050810009,scopus,2017-07-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),text coherence new method using word2vec sentence vectors and most likely n-grams,"
AbstractView references

Discourse coherence modeling evaluation remains a challenge task in all Natural Language Processing subfields. Most proposed approaches focus on feature engineering, which accepts the sophisticated features to capture the logic, syntactic or semantic relationships between all sentences within a text. This paper investigates the automatic evaluation of text coherence. We introduce a fully-automatic rich statistical model of local and global coherence that uses word2vec approach to assess the coherence a document. Our modeling approach relies on numerical vectors derived from word2vec algorithm applied on a very large collection of texts. We successfully combined the word2vec vectors and most likely n-grams with cohesive LD-n-grams perplexity to assess the coherence and topic integrity of document. We present experimental results that assess the predictive power that it does not depend on the language and its semantic concepts. So it has the ability to apply on any language. Our model achieves state-of-the-art performance in coherence evaluation and order discrimination task on two datasets widely used in the previous methods. © 2017 IEEE.
"
10.1109/IKT.2017.8258625,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050661816&origin=inward,Conference Paper,SCOPUS_ID:85050661816,scopus,2017-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using models at run-time to measure quality of sas in the large-scale software systems,"
AbstractView references

[Context/ Motivation] The adaptation process in self-Adaptive software systems modifies the system based on new monitored conditions to make the system able to deserve SLA (QoS). Some Researchers assume the environment as a closed-world. So, adaptation actions are predicted at design-Time, to get applied at run-Time. But uncertainty causes prediction about the all different conditions be impossible. [Objective] In this study we aim to handle the uncertainty consequences in the open-world by the use of two quality models presented in the process level and the product level. [Method] To reduce the cost of adaptation at run-Time, an integrated approach for modeling and verifying the requirements of SAS is used as well as a measurement method to measure the level of satisficing quality factors using Model Driven Engineering (MDE). [Results] When the measurement (i.e.The deviation from desired behavior) is taken into account in early phases of development, not only the adaptation cost degrades, but also the undesired consequences do not propagate to the next phases. [Conclusion] To sum up, in this paper requirements are modeled based on GORE models. Then an SAS Quality model is augmented in the goal model. This embedded model is verified by OMEGA2/ IFX profile. The verified model is transformed to alternative architectural models by some defined transformation rules which is written in ATLAS language, implemented in Eclipse Modeling Framework (EMF). © 2017 IEEE.
"
10.1016/j.ifacol.2017.08.1376,S2405896317319183,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85031817205&origin=inward,Conference Paper,SCOPUS_ID:85031817205,scopus,2017-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an mbse approach to pass from requirements to functional architecture,"
                  With systems being more and more complex, especially in the case of Nuclear Power Plant design, Systems Engineering (SE) has seen an increasing interest in the industrial world. Although SE concepts and processes have become well-known, passing from Requirements to Functional Architectures has not been a fully addressed issue. This paper proposes a tooled method that offers three different design perspectives: a Requirement View, a Context View and a Behavioural View. Those views mainly help the designers to express requirements, structure their architecture design, work together, verify and partially validate their design by means of five interconnected Design Specific Modelling Languages (DSML).
               "
10.1016/j.ifacol.2017.08.504,S2405896317308728,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85031797035&origin=inward,Conference Paper,SCOPUS_ID:85031797035,scopus,2017-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a toolbox for analysis and design of model based diagnosis systems for large scale models,"
                  To facilitate the use of advanced fault diagnosis analysis and design techniques to industrial sized systems, there is a need for computer support. This paper describes a Matlab toolbox and evaluates the software on a challenging industrial problem, air-path diagnosis in an automotive engine. The toolbox includes tools for analysis and design of model based diagnosis systems for large-scale differential algebraic models. The software package supports a complete tool-chain from modeling a system to generating C-code for residual generators. Major design steps supported by the tool are modeling, fault diagnosability analysis, sensor selection, residual generator analysis, test selection, and code generation. Structural methods based on efficient graph theoretical algorithms are used in several steps. In the automotive diagnosis example, a diagnosis system is generated and evaluated using measurement data, both in fault-free operation and with faults injected in the control-loop. The results clearly show the benefit of the toolbox in a model-based design of a diagnosis system. Latest version of the toolbox can be downloaded at faultdiagnosistoolbox.github.io.
               "
10.1093/jamia/ocw180,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85026398638&origin=inward,Article,SCOPUS_ID:85026398638,scopus,2017-07-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deep learning for pharmacovigilance: recurrent neural network architectures for labeling adverse drug reactions in twitter posts,"
AbstractView references

Objective: Social media is an important pharmacovigilance data source for adverse drug reaction (ADR) identification. Human review of social media data is infeasible due to data quantity, thus natural language processing techniques are necessary. Social media includes informal vocabulary and irregular grammar, which challenge natural language processing methods. Our objective is to develop a scalable, deep-learning approach that exceeds state-of-the-art ADR detection performance in social media. Materials and Methods: We developed a recurrent neural network (RNN) model that labels words in an input sequence with ADR membership tags. The only input features are word-embedding vectors, which can be formed through task-independent pretraining or during ADR detection training. Results: Our best-performing RNN model used pretrained word embeddings created from a large, non- domain-specific Twitter dataset. It achieved an approximate match F-measure of 0.755 for ADR identification on the dataset, compared to 0.631 for a baseline lexicon system and 0.65 for the state-of-the-art conditional random field model. Feature analysis indicated that semantic information in pretrained word embeddings boosted sensitivity and, combined with contextual awareness captured in the RNN, precision. Discussion: Our model required no task-specific feature engineering, suggesting generalizability to additional sequence-labeling tasks. Learning curve analysis showed that our model reached optimal performance with fewer training examples than the other models. Conclusion: ADR detection performance in social media is significantly improved by using a contextually aware model and word embeddings formed from large, unlabeled datasets. The approach reduces manual datalabeling requirements and is scalable to large social media datasets. © The Author 2017. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.
"
10.1109/IPDPS.2017.42,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85027682973&origin=inward,Conference Paper,SCOPUS_ID:85027682973,scopus,2017-06-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),swhybrid: a hybrid-parallel framework for large-scale protein sequence database search,"
AbstractView references

Computer architectures continue to develop rapidly towards massively parallel and heterogeneous systems. Thus, easily extensible yet highly efficient parallelization approaches for a variety of platforms are urgently needed. In this paper, we present SWhybrid, a hybrid computing framework for large-scale biological sequence database search on heterogeneous computing environments with multi-core or many-core processing units (PUs) based on the Smith-Waterman (SW) algorithm. To incorporate a diverse set of PUs such as combinations of CPUs, GPUs and Xeon Phis, we abstract them as SIMD vector execution units with different number of lanes. We propose a machine model, associated with a unified programming interface implemented in C++, to abstract underlying architectural differences. Performance evaluation reveals that SWhybrid (i) outperforms all other tested state-of-the-art tools on both homogeneous and heterogeneous computing platforms, (ii) achieves an efficiency of over 80% on all tested CPUs and GPUs and over 70% on Xeon Phis, and (iii) achieves utlization rates of over 80% on all tested heterogeneous platforms. Our results demonstrate that there is enough commonality between vector-like instructions across CPUs and GPUs that one can develop higher-level abstractions and still specialize with close-to-peak performance. SWhybrid is open-source software and freely available at https://github.com/turbo0628/swhybrid. © 2017 IEEE.
"
10.1109/ICSE-SEIP.2017.1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85026838215&origin=inward,Conference Paper,SCOPUS_ID:85026838215,scopus,2017-06-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),context-based analytics - establishing explicit links between runtime traces and source code,"
AbstractView references

Diagnosing problems in large-scale, distributed applications runningin cloud environments requires investigating different sources ofinformation to reason about application state at any given time. Typical sources of information available to developers and operatorsinclude log statements and other runtime information collectedby monitors, such as application and system metrics. Just as importantly, developers rely on information related to changes to the source code andconfiguration files (program code) when troubleshooting. This information is generally scattered, and it is up to the troubleshooterto inspect multiple implicitly-connected fragments thereof. Currently, different tools need to be used in conjunction, e.g., logaggregation tools, source-code management tools, and runtime-metricdashboards, each requiring different data sources and workflows. Notsurprisingly, diagnosing problems is a difficult proposition. In this paper, we propose Context-Based Analytics, an approach that makes the links between runtime informationand program-code fragments explicit by constructing a graph based on anapplication-context model. Implicit connections between informationfragments are explicitly represented as edges in the graph. We designeda framework for expressing application-context models and implemented a prototype. Further, we instantiated our prototype framework with an application-contextmodel for two real cloud applications, one from IBM and another from a major telecommunications provider. We applied context-based analytics to diagnose twoissues taken from the issue tracker of the IBM application and foundthat our approach reduced the effort of diagnosing these issues. In particular, context-based analytics decreased the number of required analysis steps by 48% and the number ofneeded inspected traces by 40% on average as compared to a standard diagnosis approach. © 2017 IEEE.
"
10.1109/ICSE-C.2017.109,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85026776815&origin=inward,Conference Paper,SCOPUS_ID:85026776815,scopus,2017-06-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),understanding android app piggybacking,"
AbstractView references

The Android packaging model offers adequate opportunities for attackers to inject malicious code into popular benign apps, attempting to develop new malicious apps that can then be easily spread to a large user base. Despite the fact that the literature has already presented a number of tools to detect piggybacked apps, there is still lacking a comprehensive investigation on the piggybacking processes. To fill this gap, in this work, we collect a large set of benign/piggybacked app pairs that can be taken as benchmark apps for further investigation. We manually look into these benchmark pairs for understanding the characteristics of piggybacking apps and eventually we report 20 interesting findings. We expect these findings to initiate new research directions such as practical and scalable piggybacked app detection, explainable malware detection, and malicious code location. © 2017 IEEE.
"
10.1109/ICSE-NIER.2017.13,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85026730806&origin=inward,Conference Paper,SCOPUS_ID:85026730806,scopus,2017-06-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"darviz: deep abstract representation, visualization, and verification of deep learning models","
AbstractView references

Traditional software engineering programming paradigms are mostly object or procedure oriented, driven by deterministic algorithms. With the advent of deep learning and cognitive sciences there is an emerging trend for data-driven programming, creating a shift in the programming paradigm among the software engineering communities. Visualizing and interpreting the execution of a current large scale data-driven software development is challenging. Further, for deep learning development there are many libraries in multiple programming languages such as TensorFlow (Python), CAFFE (C++), Theano (Python), Torch (Lua), and Deeplearning4j (Java), driving a huge need for interoperability across libraries. We propose a model driven development based solution framework, that facilitates intuitive designing of deep learning models in a platform agnostic fashion. This framework could potentially generate library specific code, perform program translation across languages, and debug the training process of a deep learning model from a fault localization and repair perspective. Further we identify open research problems in this emerging domain, and discuss some new software tooling requirements to serve this new age data-driven programming paradigm. © 2017 IEEE.
"
10.1109/ICSE-C.2017.72,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85026725782&origin=inward,Conference Paper,SCOPUS_ID:85026725782,scopus,2017-06-30,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cross-project and within-project semi-supervised software defect prediction problems study using a unified solution,"
AbstractView references

When there exists not enough historical defect data for building accurate prediction model, semi-supervised defect prediction (SSDP) and cross-project defect prediction (CPDP) are two feasible solutions. Existing CPDP methods assume that the available source data is well labeled. However, due to expensive human efforts for labeling a large amount of defect data, usually, we can only make use of the suitable unlabeled source data to help build the prediction model. We call CPDP in this scenario as cross-project semi-supervised defect prediction (CSDP). As to within-project semi-supervised defect prediction (WSDP), although some WSDP methods have been developed in recent years, there still exists much room for improvement. In this paper, we aim to provide an effective solution for both CSDP and WSDP problems. We introduce the semi-supervised dictionary learning technique, an effective machine learning technique, into defect prediction and propose a semi-supervised structured dictionary learning (SSDL) approach for CSDP and WSDP. SSDL can make full use of the useful information in limited labeled defect data and a large amount of unlabeled data. Experiments on two public datasets indicate that SSDL can obtain better prediction performance than related SSDP methods in the CSDP scenario. © 2017 IEEE.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85030531483&origin=inward,Conference Paper,SCOPUS_ID:85030531483,scopus,2017-06-24,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a learning trajectory for developing computational thinking and programming,"
AbstractView references

This research study identifies the relationship between students' prior experiences with programming and their development of computational thinking and programming during their first year engineering experience. Many first year programs teach students basic programming concepts using languages like MATLAB or LABView. These languages are used because many of the disciplinary schools expect students to use computational models to analyze systems of interest. Some undergraduate engineering students are entering college with strong computational backgrounds, while others have no experience at all. This study is the first in a series to better identify students' transition into developing and reasoning with analytical tools. The learning progression across two programming languages is critical to developing a student's ability to generalize across various computational tools. The goal of this study is to identify how students progress in their ability to engage in computational thinking and programming relative to other learning outcomes for the course. This initial investigation uses students' prior background in programming and their exam scores to evaluate their interdependence of prior knowledge on learning programming across their first semester in university. As anticipated, learning a new language is difficult compared to the other course objectives. However, students with some prior knowledge of programming demonstrate a balanced performance between computational thinking and the other course objectives. Students who have limited programming experience do demonstrate a higher variance in their performance in problems related to computational thinking compared to their other course objectives. One of the leading factors is the time spent practicing programming. This paper will be of interest to instructors with the objective of developing computational thinking and programming in classrooms with a large variance in students' backgrounds with programming. © American Society for Engineering Education, 2017.
"
10.1109/ICSAW.2017.48,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85025689600&origin=inward,Conference Paper,SCOPUS_ID:85025689600,scopus,2017-06-23,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards recovering the software architecture of microservice-based systems,"
AbstractView references

Today the microservice architectural style is being adopted by many key technological players such as Netflix, Amazon, The Guardian. A microservice architecture is composed of a large set of small services, each running in its own process and communicating with lightweight mechanisms (often via REST APIs). If on one side having a large set of independently developed services helps in terms of developer productivity, scalability, maintainability, on the other side it is very difficult to have a clear understanding of the overall architecture of a microservice-based software system, specially when the deployment and operation of the involved microservices evolves at run-time. In this paper we present MicroART, an architecture recovery approach for microservice-based systems. By using Model-Driven Engineering techniques, we leverage a suitably defined domain-specific language for representing the key aspects of the architecture of a microservice-based system and provide a tool-chain for automatically extracting architecture models of the system. The only inputs of MicroART are: (i) a GitHub repository containing the source code of the system and (ii) a reference to the container engine managing it. We validated MicroART on a publicly available benchmark system, with promising results. © 2017 IEEE.
"
10.1109/ICSAW.2017.63,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85025637478&origin=inward,Conference Paper,SCOPUS_ID:85025637478,scopus,2017-06-23,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),thingml: a generative approach to engineer heterogeneous and distributed systems,"
AbstractView references

Cyber Physical Systems (CPS) typically rely on a highlyheterogeneous interconnection of platforms and devices offering a diversity of complementary capabilities: from cloudserver with their virtually unlimited resources to tiny microcontrollers supporting the connection to the physical world. This tutorial presents ThingML, a tool-supported Model-Driven Software Engineering (MDSE) approach targeting the heterogeneity and distribution challenges associated with the development of CPS. ThingML is based on a domain specific modelling languages integrating state-of-the-art concepts for modeling distributed systems, and comes with a set of compilers targeting a large set of platforms and communication protocols. ThingML has been iteratively elaborated over the past years based on a set of experiences and projects aiming at applying the state of the art in MDSE in practical contexts and with different industry partners. © 2017 IEEE.
"
10.1109/CCECE.2017.7946808,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021809688&origin=inward,Conference Paper,SCOPUS_ID:85021809688,scopus,2017-06-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),dcm: a python-based middleware for parallel processing applications on small scale devices,"
AbstractView references

Parallel programming has been an active area of research in computer science and software engineering for many years. Parallel programming should ideally provide a linear speedup to computational problems. In reality, this is rarely the case. While there are some algorithms that cannot be parallelized, many that can, still fail to provide the ideal linear speedup. For algorithms that can benefit from parallelization, it is often much more difficult to develop the parallel code than it is to write a sequential, single-threaded program. The existence of this gap between ideal parallel computing and parallel computing on real hardware and software has caused many developers to create new solutions in an attempt to move real parallel computing closer to its idealized model. While many of these solutions provide a great performance benefit on large-scale systems, they often lag behind when deployed on small-scale systems. In this paper, we introduce the design and implementation of DCM (Distributed Computing Middleware) - a Python-based middleware for writing parallel processing applications for execution on clusters of small-scale devices. Evaluation results show the feasibility of DCM. Our middleware and its test cases are publicly available on GitHub. © 2017 IEEE.
"
10.1142/S1793351X17500027,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85073237194&origin=inward,Article,SCOPUS_ID:85073237194,scopus,2017-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"aspect-oriented visual ontology editor (avoned): visual language, aspect-oriented editing concept and implementation","
AbstractView references

When it comes to design and editing, complex ontologies have much in common with large and complex software systems. The ontology editor presented in this article adapts two solution approaches from software engineering to the task of ontology editing following the rationale that similar problems can be tackled with similar solutions. The first of the approaches is aspect orientation which allows to look at a problem from different perspectives and editing each perspective separately even with different names for one and the same entity. The article describes this approach's theoretical foundations as well as the data model required for its implementation. The second approach is an auto-completion-like feature that checks whether editing steps on the ABox level are consistent with rules modeled on the TBox level and the ontology in general. The editor also features a visual language that is designed to facilitate editing OWL Lite based ontologies. © 2017 World Scientific Publishing Company.
"
10.1016/j.jbi.2017.05.002,S1532046417300977,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85019375942&origin=inward,Article,SCOPUS_ID:85019375942,scopus,2017-06-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),character-level neural network for biomedical named entity recognition,"Biomedical named entity recognition (BNER), which extracts important named entities such as genes and proteins, is a challenging task in automated systems that mine knowledge in biomedical texts. The previous state-of-the-art systems required large amounts of task-specific knowledge in the form of feature engineering, lexicons and data pre-processing to achieve high performance. In this paper, we introduce a novel neural network architecture that benefits from both word- and character-level representations automatically, by using a combination of bidirectional long short-term memory (LSTM) and conditional random field (CRF) eliminating the need for most feature engineering tasks. We evaluate our system on two datasets: JNLPBA corpus and the BioCreAtIvE II Gene Mention (GM) corpus. We obtained state-of-the-art performance by outperforming the previous systems. To the best of our knowledge, we are the first to investigate the combination of deep neural networks, CRF, word embeddings and character-level representation in recognizing biomedical named entities."
10.1007/978-981-10-4436-6_2,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85033336563&origin=inward,Book Chapter,SCOPUS_ID:85033336563,scopus,2017-05-10,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),model-based design and automated validation of arinc653 architectures using the aadl,"
AbstractView references

Safety-Critical Systems as used in avionics systems are now extremely software-reliant.As these systems are life- or mission-critical, softwaremust be carefully designed and certified according to stringent standards. One typical pitfall of corresponding development project is the late detection of safety issues or bugs at integration time that impose to redo development steps. Model-Based Engineering aims at capturing system concerns with specific notations and use models to drive the development process through all its phases-design, validation, implementation and ultimately, certification. Through a single consistent notation, such an approach would avoid undefined assumptions and traditional hurdles due to informal, textbased, specifications. In this chapter, we present recent contributions we pushed forward in the AADL architecture description language for the design and validation of Integrated Modular Avionics systems. First, we review modeling patterns to support abstractions for Integrated Modular Avionics systems.We then introduce capabilities to check all ARINC653 patterns are enforced at model-level. In addition, we review error modeling and safety analysis capabilities towards the production of safety reports conforming to ARP4761 recommendations, along with code generation strategies to map model elements to code. All these contributions are integrated in one uniform modeling process based on the AADL. © Springer Nature Singapore Pte Ltd. 2017. All rights reserved.
"
10.1007/978-3-319-56345-9_11,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054023050&origin=inward,Book Chapter,SCOPUS_ID:85054023050,scopus,2017-05-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),model-driven systems engineering: principles and application in the cpps domain,"
AbstractView references

To engineer large, complex, and interdisciplinary systems, modeling is considered as the universal technique to understand and simplify reality through abstraction, and thus, models are in the center as the most important artifacts throughout interdisciplinary activities within model-driven engineering processes. Model-Driven Systems Engineering (MDSE) is a systems engineering paradigm that promotes the systematic adoption of models throughout the engineering process by identifying and integrating appropriate concepts, languages, techniques, and tools. This chapter discusses current advances as well as challenges towards the adoption of model-driven approaches in cyber-physical production systems (CPPS) engineering. In particular, we discuss howmodeling standards, modeling languages, and model transformations are employed to support current systems engineering processes in the CPPS domain, and we show their integration and application based on a case study concerning a lab-sized production system. The major outcome of this case study is the realization of an automated engineering tool chain, including the languages SysML, AML, and PMIF, to perform early design and validation. © Springer International Publishing AG 2017.
"
10.1145/3027063.3053077,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85019596259&origin=inward,Conference Paper,SCOPUS_ID:85019596259,scopus,2017-05-06,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),talking about talk: coordination in large online communities,"
AbstractView references

Social computing systems and online communities develop varying strategies for managing collaborative processes such as consensus building, task delegation, and conflict management. Although these factors impact both the ways in which communities produce content and the content they produce, little prior work has undertaken a large comparative analysis of coordination dynamics across linguistically diverse communities engaged in the same activity. We describe and model the coordination processes of Wikipedia editors across the 24 largest language editions. Our results indicate that language edition is associated with a difference in quantity of coordination activity, as measured by talk page posts, with increases as high as 60% when compared against pages in English. Copyright © 2017 by the Association for Computing Machinery, Inc. (ACM).
"
10.1080/00223131.2017.1290559,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85013649825&origin=inward,Article,SCOPUS_ID:85013649825,scopus,2017-05-04,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),knowledge-based software design for defense-in-depth risk monitor system and application for ap1000,"
AbstractView references

As part of the new risk monitor system, the software for the plant Defense-in-Depth (DiD) risk monitor system was designed based on the state-transition and finite-state machine, and then the knowledge-based software was developed by object-oriented method utilizing the Unified Modeling Language (UML). Currently, there are mainly two functions in the developed plant DiD risk monitor software that are knowledge-base editor which is used to model the system in a hierarchical manner and the interaction simulator that simulates the interactions between the different actors in the model. In this paper, a model for playing its behavior is called an Actor which is modeled at the top level. The passive safety AP1000 power plant was studied and the small-break loss-of-coolant accident (SBLOCA) design basis accident transient is modeled using the plant DiD risk monitor software. Furthermore, the simulation result is shown for the interactions between the actors which are defined in the plant DiD risk monitor system as PLANT actor, OPERATOR actor, and SUPERVISOR actor. This paper shows that it is feasible to model the nuclear power plant knowledge base using the software modeling technique. The software can make the large knowledge base for the nuclear power plant with small effort. © 2017 Atomic Energy Society of Japan. All rights reserved.
"
10.1002/cite.201600114,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85017243820&origin=inward,Article,SCOPUS_ID:85017243820,scopus,2017-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),mosaic – enabling large-scale equation-based flow sheet optimization,"
AbstractView references

Dedicated software exists for both process simulation and optimization. Given the advantages of simultaneous optimization schemes, engineers are frequently tasked with reimplementing their simulation models in optimization languages or environments. In this contribution, a workflow is introduced to model chemical engineering models in MathML and to have an automatic code generation for both specialized simulation and optimization tools. Two examples are given to highlight the performance of this workflow implemented in the modeling, simulation, and optimization environment MOSAIC. © 2017 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim
"
10.1016/j.datak.2017.03.006,S0169023X1730109X,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85016398117&origin=inward,Article,SCOPUS_ID:85016398117,scopus,2017-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a conceptual framework for large-scale ecosystem interoperability and industrial product lifecycles,"
                  One of the most significant challenges in information system design is the constant and increasing need to establish interoperability between heterogeneous software systems at increasing scale. The automated translation of data between the data models and languages used by information ecosystems built around official or de facto standards is best addressed using model-driven engineering techniques, but requires handling both data and multiple levels of metadata within a single model. Standard modelling approaches are generally not built for this, compromising modelling outcomes. We establish the SLICER conceptual framework built on multilevel modelling principles and the differentiation of basic semantic relations (such as specialisation, instantiation, specification and categorisation) that dynamically structure the model. Moreover, it provides a natural propagation of constraints over multiple levels of instantiation. The presented framework is novel in its flexibility towards identifying the multilevel structure, the differentiation of relations often combined in other frameworks, and a natural propagation of constraints over multiple levels of instantiation.
               "
10.1016/j.datak.2017.03.001,S0169023X17301040,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85015291819&origin=inward,Conference Paper,SCOPUS_ID:85015291819,scopus,2017-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),special issue on conceptual modeling - 34th international conference on conceptual modeling (er 2015),
10.1016/j.compind.2017.02.003,S0166361517300374,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85014175388&origin=inward,Article,SCOPUS_ID:85014175388,scopus,2017-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),recalling the rationale of change from process model revision comparison – a change-pattern based approach,"
                  Industrial enterprises embody a rather large and heterogeneous business process landscape including hundreds to thousands of both manufacturing and supporting processes. To keep records of their business process architecture enterprises use informal and as well formal process descriptions. Formal process descriptions are often referred to as process models and are gaining increasing importance as a basis for process-aware enterprise information systems and automation purposes. Therefore process models are continuously adapted to changing business requirements. Keeping track of model changes is an important requirement to be able to understand past decisions and their impact on the process landscape. Hence, keeping track of changes is not easy if changes are not associated with the original rationale and the order of atomic changes is not preserved anymore. In this paper we present an approach that builds upon the concept of change patterns. For this purpose we systematically examined revision histories from a large process model collection and described them through a pattern language. In addition, we propose an algorithm to detect such change patterns. Our approach has been implemented in a modeling environment and has been evaluated with regard to effectiveness and performance.
               "
10.1016/j.cpc.2017.01.011,S0010465517300231,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85012248045&origin=inward,Article,SCOPUS_ID:85012248045,scopus,2017-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),fibrilj: imagej plugin for fibrils’ diameter and persistence length determination,"
                  Application of microscopy to evaluate the morphology and size of filamentous proteins and amyloids requires new and creative approaches to simplify and automate the image processing. The estimation of mean values of fibrils diameter, length and bending stiffness on micrographs is a major challenge. For this purpose we developed an open-source FibrilJ plugin for the ImageJ/FiJi program. It automatically recognizes the fibrils on the surface of a mica, silicon, gold or formvar film and further analyzes them to calculate the distribution of fibrils by diameters, lengths and persistence lengths. The plugin has been validated by the processing of TEM images of fibrils formed by Sup35NM yeast protein and artificially created images of rod-shape objects with predefined parameters. Novel data obtained by SEM for Sup35NM protein fibrils immobilized on silicon and gold substrates are also presented and analyzed.
                  
                     Program summary
                  
                  
                     Program title: FibrilJ
                  
                     Program Files doi: 
                     http://dx.doi.org/10.17632/ndxb93h4vc.1
                  
                  
                     Licensing provisions: Apache-2.0
                  
                     Programming language: ImageJ Macro Language
                  
                     Nature of problem: Amyloids are large protein aggregates that form unbranched fibrils. Formation of amyloids by different proteins leads to the emergence of a number of serious human diseases, including Alzheimer’s, Parkinson’s and Huntington’s diseases. Therefore, various amyloids, in particular their topology, are widely studied in an enormous number of papers. The structural organization of the amyloid aggregates at the molecular level is a key problem in this area. At the moment new methods of amyloids analysis based on electronic and probe microscopy are particularly popular. Usually it is required to process images containing hundreds of amyloids to obtain statistically reliable data on the distribution of the thickness, length and persistence length (a basic mechanical property quantifying the stiffness of a polymer). The lattermost parameter was introduced for the characteristic of bending stiffness of amyloids similarly to that in a model of wormlike chain for polymers. The manual measurements of these parameters take a lot of time and, due to the human factor, are insufficiently representative. ImageJ/FiJi is open-source software and one of the most commonly used program for image processing. The ImageJ/FiJi community provides BoneJ plugin for bones analysis and DiameterJ plugin for nanofibers analysis, which have the function of determining the diameter of the respective objects. Also, approaches that involve calculations of the end-to-end distances and contour lengths of biopolymers were used to determine their persistence lengths using ImageJ/FiJi. These above mentioned plugins are not designed for the study of amyloids and have never been used or validated for them. They do not provide the necessary functionality for the automatic recognition of amyloids on the substrate and filtering objects by a number of parameters. They have unspecified accuracy of these objects measurements and do not calculate the objects distribution on average diameters and persistence lengths.
                  
                     Solution method: For this purpose we developed a FibrilJ plugin for ImageJ/FiJi program with an open source code. It automatically recognizes the fibrils on the surface of a mica, silicon, gold or formvar film and further analyzes them to calculate the distribution of fibrils on average diameters, lengths and persistence lengths. FibrilJ usage may unify the process of diameter and persistence length calculation and it may also contribute to comparison of results obtained by different research groups. Plugin FibrilJ which is presentedin this paper is free of DiameterJ or BoneJ weaknesses. It has been validated by artificially created images of rod-shape objects with predefined parameters and micrographs of Sup35NM protein fibrils.
               "
10.1007/s10270-015-0481-1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84936803675&origin=inward,Article,SCOPUS_ID:84936803675,scopus,2017-05-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),contract-based modeling and verification of timed safety requirements within sysml,"
AbstractView references

In order to cope with the growing complexity of critical real-time embedded systems, systems engineering has adopted a component-based design technique driven by requirements. Yet, such an approach raises several issues since it does not explicitly prescribe how system requirements can be decomposed on components nor how components contribute to the satisfaction of requirements. The envisioned solution is to design, with respect to each requirement and for each involved component, an abstract specification, tractable at each design step, that models how the component is concerned by the satisfaction of the requirement and that can be further refined toward a correct implementation. In this paper, we consider such specifications in the form of contracts. A contract for a component consists in a pair (assumption, guarantee) where the assumption models an abstract behavior of the component’s environment and the guarantee models an abstract behavior of the component given that the environment behaves according to the assumption. Therefore, contracts are a valuable asset for the correct design of systems, but also for mapping and tracing requirements to components, for tracing the evolution of requirements during design and, most importantly, for compositional verification of requirements. The aim of this paper is to introduce contract-based reasoning for the design of critical real-time systems made of reactive components modeled with UML and/or SysML. We propose an extension of UML and SysML languages with a syntax and semantics for contracts and the refinement relations that they must satisfy. The semantics of components and contracts is formalized by a variant of timed input/output automata on top of which we build a formal contract-based theory. We prove that the contract-based theory is sound and can be applied for a relatively large class of SysML system models. Finally, we show on a case study extracted from the automated transfer vehicle (http://www.esa.int/ATV) that our contract-based theory allows to verify requirement satisfaction for previously intractable models. © 2015, Springer-Verlag Berlin Heidelberg.
"
10.1016/j.jbi.2017.02.008,S1532046417300333,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85015001388&origin=inward,Article,SCOPUS_ID:85015001388,scopus,2017-04-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cognitive systems at the point of care: the credo program,"CREDO is a framework for understanding human expertise and for designing and deploying systems that support cognitive tasks like situation and risk assessment, decision-making, therapy planning and workflow management. The framework has evolved through an extensive program of research on human decision-making and clinical practice. It draws on concepts from cognitive science, and has contributed new results to cognitive theory and understanding of human expertise and knowledge-based AI. These results are exploited in a suite of technologies for designing, implementing and deploying clinical services, early versions of which were reported by Das et al. (1997) [9] and Fox and Das (2000) [26]. A practical outcome of the CREDO program is a technology stack, a key element of which is an agent specification language (PROforma: Sutton and Fox (2003) [55]) which has proved to be a versatile tool for designing point of care applications in many clinical specialties and settings. Since software became available for implementing and deploying PROforma applications many kinds of services have been successfully built and trialed, some of which are in large-scale routine use. This retrospective describes the foundations of the CREDO model, summarizes the main theoretical, technical and clinical contributions, and discusses benefits of the cognitive approach."
10.1145/3027385.3029477,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85016460604&origin=inward,Conference Paper,SCOPUS_ID:85016460604,scopus,2017-03-13,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),what does student writing tell us about their thinking on social justice?,"
AbstractView references

In this work we investigate the use of deep learning for text analysis to measure elements of student thinking related to issues of privilege, oppression, diversity and social justice. We leverage historical expert annotations as well as a large lexical model to create a more generalizable vocabulary for identifying these characteristics in short student writing. We demonstrate the feasibility of this approach, and identify further areas for research. © 2017 ACM.
"
10.1080/03043797.2016.1249342,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84994613904&origin=inward,Article,SCOPUS_ID:84994613904,scopus,2017-03-04,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),current developments in the french engineering education system,"
AbstractView references

The French engineering education system has been established in quite a different way from others in Europe, such as the German and British systems, for instance. Due to both the whole state system and the private initiatives during the industrial revolution, the engineering education system today is composed of a large number (nearly 200) of rather small and specialised institutions, which have historically mostly developed outside universities. In the last decades, this system has had to face a powerful internationalisation movement. This has had major consequences on the curricula design, regarding foreign language teaching, international exchanges, and links with research. Currently, the French engineering education system is facing new challenges, regarding innovation and environmental and social issues, in a very competitive higher education context. © 2016 SEFI.
"
10.1016/j.cpc.2016.07.035,S001046551630251X,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85006988513&origin=inward,Article,SCOPUS_ID:85006988513,scopus,2017-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),raasaft: a framework enabling coarse-grained molecular dynamics simulations based on the saft-γ mie force field,"
                  We describe here raaSAFT, a Python code that enables the setup and running of coarse-grained molecular dynamics simulations in a systematic and efficient manner. The code is built on top of the popular HOOMD-blue code, and as such harnesses the computational power of GPUs. The methodology makes use of the SAFT-
                        γ
                      Mie force field, so the resulting coarse grained pair potentials are both closely linked to and consistent with the macroscopic thermodynamic properties of the simulated fluid. In raaSAFT both homonuclear and heteronuclear models are implemented for a wide range of compounds spanning from linear alkanes, to more complicated fluids such as water and alcohols, all the way up to nonionic surfactants and models of asphaltenes and resins. Adding new compounds as well as new features is made straightforward by the modularity of the code. To demonstrate the ease-of-use of raaSAFT, we give a detailed walkthrough of how to simulate liquid–liquid equilibrium of a hydrocarbon with water. We describe in detail how both homonuclear and heteronuclear compounds are implemented. To demonstrate the performance and versatility of raaSAFT, we simulate a large polymer-solvent mixture with 300 polystyrene molecules dissolved in 42 700 molecules of heptane, reproducing the experimentally observed temperature-dependent solubility of polystyrene. For this case we obtain a speedup of more than three orders of magnitude as compared to atomistically-detailed simulations.
               
                  Program summary
                  
                     Program title: raaSAFT
                  
                     Catalogue identifier: AFBE_v1_0
                  
                     Program summary URL:
                     http://cpc.cs.qub.ac.uk/summaries/AFBE_v1_0.html
                  
                  
                     Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland
                  
                     Licensing provisions: MIT Licence
                  
                     No. of lines in distributed program, including test data, etc.: 802350
                  
                     No. of bytes in distributed program, including test data, etc.: 30446478
                  
                     Distribution format: tar.gz
                  
                     Programming language: Python.
                  
                     Computer: Any computer, optionally with Nvidia GPU(s).
                  
                     Operating system: Linux, Mac OSX.
                  
                     RAM: Depends on number of atoms and cutoff size
                  
                     Classification: 7.7, 16.13.
                  
                     External routines: HOOMD-blue [1][2]
                  
                     Nature of problem:
                  
                  The behaviour and properties of simple and complex fluids, including mixtures
                  
                     Solution method:
                  
                  Coarse-grained molecular dynamics using the SAFT-
                        γ
                      Mie force field [3].
                  
                     Restrictions:
                  
                  Ions and ionic compounds are not supported yet. Jobscripts running with Python 2 require HOOMD-blue v1.3 or newer; for Python 3 there is no such restriction.
                  
                     Unusual features:
                  
                  Uses object-oriented programming to make reuse and sharing of models very simple. Allows the simulation to be set up and executed completely programmatically, i.e. without the use of a GUI or preprocessor. Force field parameters are available from an online database with more than 6000 molecules, http://www.bottledsaft.org  [4].
                  
                     Additional comments:
                  
                  The code is hosted on http://bitbucket.org/asmunder/raasaft
                  
                  
                     Running time:
                  
                  On a single high-end GPU in 2015 (Nvidia K40), around 2.5 nanoseconds per hour of walltime for a million atoms (not counting hydrogens).
                  
                     References:
                     
                        
                           [1]
                           J.A. Anderson, C.D. Lorenz, A. Travesset, General purpose molecular dynamics simulations fully implemented on graphics processing units, Journal of Computational Physics 227 (2008) 5342–5359.
                        
                        
                           [2]
                           J. Glaser, T.D. Nguyen, J.A. Anderson, P. Lui, F. Spiga, J.A. Millan, D.C. Morse, S.C. Glotzer, Strong scaling of general-purpose molecular dynamics simulations on GPUs, Computer Physics Communications 192 (2015) 97–107.
                        
                        
                           [3]
                           E.A. Müller, G. Jackson, Force-field parameters from the SAFT-
                                 γ
                               equation of state for use in coarse-grained molecular simulations, Annual review of chemical and biomolecular engineering 5 (2014) 405–427.
                        
                        
                           [4]
                           Å. Ervik, A. Mejía, E. A. Müller, Bottled SAFT: A web app providing SAFT-
                                 γ
                               Mie force field parameters for thousands of molecular fluids, In preparation. (2016).
                        
                     
                  
               "
10.1016/j.scico.2016.08.006,S016764231630106X,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84994520014&origin=inward,Article,SCOPUS_ID:84994520014,scopus,2017-03-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),reactive model transformation with atl,"Model-driven applications may maintain large networks of structured data models and transformations among them. The development of such applications is complicated by the need to reflect on the whole network any runtime update performed on models or transformation logic. If not carefully designed, the execution of such updates may be computationally expensive. In this paper we propose a reactive paradigm for programming model transformations, and we implement a reactive model-transformation engine. We argue that this paradigm facilitates the development of autonomous model-driven systems that react to update and request events from the host application by identifying and performing only the needed computation. We implement such approach by providing a reactive engine for the ATL transformation language. We evaluate the usage scenarios that this paradigm supports and we experimentally measure its ability to reduce computation time in transformation-based applications."
10.1109/CISP-BMEI.2016.7853031,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85016075281&origin=inward,Conference Paper,SCOPUS_ID:85016075281,scopus,2017-02-13,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an efficient transaction processing method on the distributed database,"
AbstractView references

Distributed system have shown its good robustness, extensibility and effectiveness in the processing, storage and transmission of large data. Now with the expansion of the amount of data and application, it has become a challenging task to ensure its transaction in a distributed and heterogeneous environment. The sub transactions of a distributed transaction not only need to be coordinated with the local other transactions, but also with other sub transactions that generated in the global manager. Based on the discussion of distributed transaction processing model and its transaction commit protocol, the failure reasons with the model analysis of general distributed transaction processing in the practical application in our information database system are given, based on the interface in a relational database management system and super text pre-treatment language of distributed transaction processing implementation method. The research results show that this distributed transaction processing method is reliable, and can simplify the implementation of global program. © 2016 IEEE.
"
10.1007/s00778-016-0434-5,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84978807326&origin=inward,Article,SCOPUS_ID:84978807326,scopus,2017-02-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),dissociation and propagation for approximate lifted inference with standard relational database management systems,"
AbstractView references

Probabilistic inference over large data sets is a challenging data management problem since exact inference is generally #P-hard and is most often solved approximately with sampling-based methods today. This paper proposes an alternative approach for approximate evaluation of conjunctive queries with standard relational databases: In our approach, every query is evaluated entirely in the database engine by evaluating a fixed number of query plans, each providing an upper bound on the true probability, then taking their minimum. We provide an algorithm that takes into account important schema information to enumerate only the minimal necessary plans among all possible plans. Importantly, this algorithm is a strict generalization of all known PTIME self-join-free conjunctive queries: A query is in PTIME if and only if our algorithm returns one single plan. Furthermore, our approach is a generalization of a family of efficient ranking methods from graphs to hypergraphs. We also adapt three relational query optimization techniques to evaluate all necessary plans very fast. We give a detailed experimental evaluation of our approach and, in the process, provide a new way of thinking about the value of probabilistic methods over non-probabilistic methods for ranking query answers. We also note that the techniques developed in this paper apply immediately to lifted inference from statistical relational models since lifted inference corresponds to PTIME plans in probabilistic databases. © 2016, Springer-Verlag Berlin Heidelberg.
"
10.1109/REW.2016.18,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85013059435&origin=inward,Conference Paper,SCOPUS_ID:85013059435,scopus,2017-01-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using workflow patterns to model and validate service requirements,"
AbstractView references

Service requirements documentation plays a crucial role on the quality of service-oriented systems to be developed. A large amount of service requirements are documented in the form of natural language, which are usually human-centric and therefore error-prone and inaccurate. In order to improve the quality of service requirements documents, we propose a service requirements modeling and validation method using workflow patterns. First, it extracts the process information using natural language processing tools. Then it formalizes the process information with a requirements modeling language - Workflow-Patterns-based Process Language (WPPL). Finally, the defects existed in service requirements are checked against a set of checking rules by matching with workflow patterns. A financial service example - Trade Order - was used to illustrate our approach. © 2016 IEEE.
"
10.1109/REW.2016.37,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85013036574&origin=inward,Conference Paper,SCOPUS_ID:85013036574,scopus,2017-01-12,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards security modeling of e-voting systems,"
AbstractView references

As voting systems evolve from paper ballots to electronic voting (E-voting) applications, we have noticed significant efforts to develop real-world securer solutions. E-voting systems are security-critical systems that require early identification of security requirements and controls based on the analyses of potential vulnerabilities, threats, attacks, and associated risks. General purpose modeling languages and current tool support to model security concerns exist. However, they lack a comprehensive solution that includes tool support for verification of security goal completeness and risk analysis in specific domains. Also, communication between stakeholders in large-scale systems is difficult, specially because security is not the core skill of many requirements engineers. To overcome these challenges in the electronic voting domain, we developed EVSec, a domain-specific visual modeling language. EVSec is process-centric language and allows modelers expressing activities and social interactions, while identifying security concerns with associated risks. Comprehensive tool support provides security goals completeness and assists users on the identification of critical parts of the model with higher security risks. We used EVSec to model the Brazilian national election, demonstrating its adequacy. © 2016 IEEE.
"
10.1080/09540091.2016.1271398,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85010699982&origin=inward,Article,SCOPUS_ID:85010699982,scopus,2017-01-02,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),reverse engineering the human: artificial intelligence and acting theory,"
AbstractView references

In two separate papers, Artificial Intelligence (AI)/Robotics researcher Guy Hoffman takes as a starting point that actors have been in the business of reverse engineering human behaviour for centuries. In this paper, I follow the similar trajectories of AI and acting theory (AT), looking at three primary questions, in the hope of framing a response to Hoffman's papers: (1) How are the problems of training a human to simulate a fictional human both similar to and different from training a machine to simulate a human? (2) How are the larger questions of AI design and architecture similar to the larger questions that still remain within the area of AT? (3) Is there anything in the work of AI design that might advance the work of acting theorists and practitioners? The paper explores the use of “swarm intelligence” in recent models of both AT and AI, and considers the issues of embodied cognition, and the kinds of intelligence that enhances or inhibits imaginative immersion for the actor, and concludes with a consideration of the ontological questions raised by the trend towards intersubjective, dynamic systems of generative thought in both AT and AI. © 2017 Informa UK Limited, trading as Taylor & Francis Group.
"
10.1108/JEIM-01-2015-0005,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136573090&origin=inward,Article,SCOPUS_ID:85136573090,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a novel method for providing relational databases with rich semantics and natural language processing,"
AbstractView references

Purpose – With the development of systems and applications, the number of users interacting with databases has increased considerably. The relational database model is still considered as the most used model for data storage and manipulation. However, it does not offer any semantic support for the stored data which can facilitate data access for the users. Indeed, a large number of users are intimidated when retrieving data because they are non-technical or have little technical knowledge. To overcome this problem, researchers are continuously developing new techniques for Natural Language Interfaces to Databases (NLIDB). Nowadays, the usage of existing NLIDBs is not widespread due to their deficiencies in understanding natural language (NL) queries. In this sense, the purpose of this paper is to propose a novel method for an intelligent understanding of NL queries using semantically enriched database sources. Design/methodology/approach – First a reverse engineering process is applied to extract relational database hidden semantics. In the second step, the extracted semantics are enriched further using a domain ontology. After this, all semantics are stored in the same relational database. The phase of processing NL queries uses the stored semantics to generate a semantic tree. Findings – The evaluation part of the work shows the advantages of using a semantically enriched database source to understand NL queries. Additionally, enriching a relational database has given more flexibility to understand contextual and synonymous words that may be used in a NL query. Originality/value – Existing NLIDBs are not yet a standard option for interfacing a relational database due to their lack for understanding NL queries. Indeed, the techniques used in the literature have their limits. This paper handles those limits by identifying the NL elements by their semantic nature in order to generate a semantic tree. This last is a key solution towards an intelligent understanding of NL queries to relational databases. © Emerald Publishing Limited.
"
10.1145/3106237.3106304,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107774979&origin=inward,Conference Paper,SCOPUS_ID:85107774979,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),generalized observational slicing for tree-represented modelling languages,"
AbstractView references

Model-driven software engineering raises the abstraction level making complex systems easier t+`o understand than if written in textual code. Nevertheless, large complicated software systems can have large models, motivating the need for slicing techniques that reduce the size of a model. We present a generalization of observation-based slicing that allows the criterion to be defined using a variety of kinds of observable behavior and does not require any complex dependence analysis. We apply our implementation of generalized observational slicing for tree-structured representations to Simulink models. The resulting slice might be the subset of the original model responsible for an observed failure or simply the sub-model semantically related to a classic slicing criterion. Unlike its predecessors, the algorithm is also capable of slicing embedded Stateflow state machines. A study of nine real-world models drawn from four different application domains demonstrates the effectiveness of our approach at dramatically reducing Simulink model sizes for realistic observation scenarios: for 9 out of 20 cases, the resulting model has fewer than 25% of the original model's elements. © 2017 ACM.
"
10.26868/25222708.2017.424,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107613619&origin=inward,Conference Paper,SCOPUS_ID:85107613619,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"thinking local, acting global: urban-scale energy modeling for global cities governance","
AbstractView references

Cities, undergoing rapid change in all parts of the globe, face a common set of ""metabolic"" challenges: Sustainably provisioning for energy, water, and food supplies under sanitary, healthy, economically productive living conditions. Diverse decision makers, such as government, utilities, project developers and bankers must be able to visualize multiple impacts of plans and proposals. Global urban governance initiatives are currently bringing major cities together to learn from one another in addressing common challenges. Energy-related emissions, produced primarily by cities, are a huge challenge to the global climate, which in impacts growing urban areas. Urban infrastructures will have to be planned to meet the needs of increasing populations under increasingly demanding circumstances. This paper discusses the role of urban energy and climate modeling to analyze and predict trends that are a consequence of today's fossil energy economy and to develop strategies for moving towards clean and carbon neutral urban energy systems. Efforts to mitigate climate change are still largely limited to political target setting at the local, national or transnational scale resulting in fragmented or very slow actual change in the levels of urban energy efficiency and renewable supply. There is a knowledge gap between global climate targets and how to translate these into concrete urban energy strategies that can be monitored, regularly assessed and reported back to decision makers. The translation of global climate targets to local and regional energy transformation strategies requires large amounts of data, many tools to model complex energy systems, and ways to inform how best to manage them. This paper suggests that the energy and infrastructure problems that cities face world-wide today are comparable and differ mainly by density, climatic boundary conditions and local resource availability. Thus, a global energy research agenda to address common urban problems seems possible. © 2017 Building Simulation Conference Proceedings. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072102680&origin=inward,Conference Paper,SCOPUS_ID:85072102680,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),predicting the resilience of obfuscated code against symbolic execution attacks via machine learning,"
AbstractView references

Software obfuscation transforms code such that it is more difficult to reverse engineer. However, it is known that given enough resources, an attacker will successfully reverse engineer an obfuscated program. Therefore, an open challenge for software obfuscation is estimating the time an obfuscated program is able to withstand a given reverse engineering attack. This paper proposes a general framework for choosing the most relevant software features to estimate the effort of automated attacks. Our framework uses these software features to build regression models that can predict the resilience of different software protection transformations against automated attacks. To evaluate the effectiveness of our approach, we instantiate it in a case-study about predicting the time needed to deobfuscate a set of C programs, using an attack based on symbolic execution. To train regression models our system requires a large set of programs as input. We have therefore implemented a code generator that can generate large numbers of arbitrarily complex random C functions. Our results show that features such as the number of community structures in the graph-representation of symbolic path-constraints, are far more relevant for predicting deobfuscation time than other features generally used to measure the potency of control-flow obfuscation (e.g. cyclomatic complexity). Our best model is able to predict the number of seconds of symbolic execution-based deobfuscation attacks with over 90% accuracy for 80% of the programs in our dataset, which also includes several realistic hash functions. © 2017 by The USENIX Association. All Rights Reserved.
"
10.18653/v1/d17-1088,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062060452&origin=inward,Conference Paper,SCOPUS_ID:85062060452,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),deep neural solver for math word problems,"
AbstractView references

This paper presents a deep neural solver to automatically solve math word problems. In contrast to previous statistical learning approaches, we directly translate math word problems to equation templates using a recurrent neural network (RNN) model, without sophisticated feature engineering. We further design a hybrid model that combines the RNN model and a similarity-based retrieval model to achieve additional performance improvement. Experiments conducted on a large dataset show that the RNN model and the hybrid model significantly outperform state-of-the-art statistical learning methods for math word problem solving. © 2017 Association for Computational Linguistics.
"
10.5220/0006589301280136,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055675176&origin=inward,Conference Paper,SCOPUS_ID:85055675176,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),hybqa: hybrid deep relation extraction for question answering on freebase,"
AbstractView references

Question Answering over knowledge-based data is one of the most important Natural Language Processing tasks. Despite numerous efforts that have been made in this field, it is not yet in the mainstream. Question Answering can be formulated as a Relation Extraction task between the question focus entity and the expected answer. Therefore, it requires high accuracy to solve a dual problem where the relation and answer are unknown. In this work, we propose a HybQA, a Hybrid Relation Extraction system to provide high accuracy for the Relation Extraction and the Question Answering tasks over Freebase. We propose a hybrid model that combines different types of state-of-the-art deep networks that capture the relation type between the question and the expected answer from different perspectives and combine their outputs to provide accurate relations. We then use a joint model to infer the possible relation and answer pairs simultaneously. However, since Relation Extraction might still be prone to errors due to the large size of the knowledge-base corpus (Freebase), we finally use evidence from Wikipedia as an unstructured knowledge base to select the best relation-answer pair. We evaluate the system on WebQuestions data and show that the system achieves a statistical significant improvement over the existing state-of-the-art models and provides the best accuracy which is 57%. © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.
"
10.1145/3106237.3106245,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054872125&origin=inward,Conference Paper,SCOPUS_ID:85054872125,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),symbolic execution of programmable logic controller code,"
AbstractView references

Programmable logic controllers (PLCs) are specialized computers for automating a wide range of cyber-physical systems. Since these systems are often safety-critical, software running on PLCs need to be free of programming errors. However, automated tools for testing PLC software are lacking despite the pervasive use of PLCs in industry. We propose a symbolic execution based method, named SymPLC, for automatically testing PLC software written in programming languages specified in the IEC 61131-3 standard. SymPLC takes the PLC source code as input and translates it into C before applying symbolic execution, to systematically generate test inputs that cover both paths in each periodic task and interleavings of these tasks. Toward this end, we propose a number of PLC-specific reduction techniques for identifying and eliminating redundant interleavings. We have evaluated SymPLC on a large set of benchmark programs with both single and multiple tasks. Our experiments show that SymPLC can handle these programs efficiently, and for multi-task PLC programs, our new reduction techniques outperform the state-of-the-art partial order reduction technique by more than two orders of magnitude. © 2017 ACM.
"
10.1201/b12667,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051778240&origin=inward,Book Chapter,SCOPUS_ID:85051778240,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),modeling and simulation of timing behavior with the timing definition language,"
AbstractView references

Traditional development of software for embedded systems is highly platform specific. Exploiting a specific platform enables reducing cost of hardware to a minimum, whereas high development costs of software are considered acceptable in the case of large quantities of devices being sold. Nowadays, with ever more powerful processors in the low-cost range, we observe even more of a shift of functionality from hardware to software and a general tendency toward more ambitious requirements. Modern cars or airplanes, for example, contain dozens of the so-called electronic control units interconnected by multiple buses and are driven by several million lines of code. To cope with the increased complexity of the embedded software, a platform-independent “high-level” programming style becomes mandatory, as testing alone can never identify all the errors. In particular, in the case of safety-critical real-time software, this applies not only to functional aspects but to the temporal behavior of the software as well. Dealing with time, however, is not covered at all by any of the existing high-level imperative languages. Simulation environments that offer delay blocks allow at best the approximation of the simulated behavior to the behavior on the execution platform. © 2013 by Taylor & Francis Group, LLC.
"
10.5220/0006573500860097,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048060469&origin=inward,Conference Paper,SCOPUS_ID:85048060469,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),representing ecological network specifications with semantic web techniques,"
AbstractView references

Ecological Networks (ENs) are a way to describe the structures of existing real ecosystems and to plan their expansion, conservation and improvement. In this work, we present a model to represent the specifications for the local planning of ENs in a way that can support reasoning, e.g., to detect violations within new proposals of expansion, or to reason about improvements of the networks. Moreover, we describe an OWL ontology for the representation of ENs themselves. In the context of knowledge engineering, ENs provide a complex, inherently geographic domain that demands for the expressive power of a language like OWL augmented with the GeoSPARQL ontology to be conveniently represented. More importantly, the set of specification rules that we consider (taken from the project for a local EN implementation) constitute a challenging problem for representing constraints over complex geographic domains, and evaluating whether a given large knowledge base satisfies or violates them. © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048008807&origin=inward,Book Chapter,SCOPUS_ID:85048008807,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),enhanced and improved symbolic circuit analysis using matlab,"
AbstractView references

The complexity of nowadays integrated circuits (ICs) is huge. A few examples of such intricacy is found in circuits such as fourth generation Core i7 processors, with nearly 2.6 billion transistors, or the Cyclone FPGA series solutions of ALTERA, with almost 3.9 billion transistors. The realization of this kind of electronics is possible thanks to the use of Electronics Design Automation (EDA) tools, which are software solutions for the design and verification of the functionality of the devices that constitute an electronic system. EDA tools come in many flavors, from high-level design tools such as VHDL, till low-level layout manager such as L-EDIT. Moreover, some alternatives include several solutions, including simulation, layout and/or PCB design, virtual instruments, symbolic analysis, filter synthesis, among others. Likewise the electronics industry, EDA tools are useful in education. This is specially true for today's multidisciplinary programs, where circuit analysis plays an important role. Such is the case of disciplines like mechatronics, automotive systems, telecommunications, energy, control, automation and electronics engineering, to name a few. Yet, some other software tools for systems analysis are preferred in education, as is the case of MATLAB, as a high-level programming language. An example of EDA tool is SCAM (acronym of Symbolic Circuit Analysis in MATLAB) that is available for the symbolic analysis of electric circuits. However, SCAM capabilities are limited to the use of the most basic circuit elements, i.e. resistors, capacitors, inductors, Op-amps and independent current and voltage sources. In its original version, SCAM is not able to solve circuits with a relatively high number of nodes. Henceforth, this chapter presents an augmented and improved SCAM version that includes some other elements available for circuit analysis: transformers, which are quite occupied in power and radio frequency electronics; gyrators, that are useful for active filter synthesis; and the four types of controlled sources, i.e. voltage-controlled voltage source (VCVS), voltage-controlled current source (VCCS), current-controlled voltage source (CCVS), and current-controlled current source (CCCS), which allow to model active circuits such as amplifiers. The introduced symbolic simulator allows to solve very large circuits in a short time, as highlighted by the diverse examples included to demonstrate its capabilities. © 2017 Nova Science Publishers, Inc.
"
10.5220/0006285405690576,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85046644736&origin=inward,Conference Paper,SCOPUS_ID:85046644736,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),towards user-centric dsls to manage iot systems,"
AbstractView references

Hidden behind the Internet of Things (IoT), many actors are activelly filling the market with devices and services. From this profusion of actors, a large amount of technologies and APIs, sometimes proprietary, are available, making difficult the interoperability and configuration of systems for IoT technicians. In order to define and manipulate devices deployed in domestic environments, we propose IoTDSL, a Domain-Specific Language meant to specify, assemble and describe the behaviour of interconnected devices. Relying on a high-level rule-based language, users in charge of the deployment of IoT infrastructures are able to describe and combine in a declarative manner structural configurations as well as event-based semantics for devices. This way, language users are freed from technical aspects, playing with high-level representations of devices, while the complexity of the concrete implementation is handled in a dedicated layer where high-level rules are mapped to vendor's API. Copyright © 2017 by SCITEPRESS-Science and Technology Publications, Lda. All rights reserved.
"
10.5220/0006194601120124,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85046630982&origin=inward,Conference Paper,SCOPUS_ID:85046630982,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),testing environment emulation - a model-based approach,"
AbstractView references

Modern enterprise software systems often need to interact with a large number of distributed and heterogeneous systems. As a result, integration testing has become a critical step in their software development lifecycle. Service virtualization is an emerging technique for creating testing environments with realistic executable models of server side production-like behaviours. However, building models in existing service virtualization approaches is very challenging, requiring either significant human effort or the availability of interactive tracing records. In this paper, we present a domain-specific modeling approach to generate complex, virtualized testing environments. Our approach allows domain experts to use a suite of domain-specific visual modeling languages to model key interface layers of applications at a high level of abstraction. These layered models are then transformed into a testing runtime environment for application integration testing. We have conducted a technical comparison with two other existing approaches and also carried out a user study. The user study demonstrated the acceptance of our new testing environment emulation approach from software testing experts and developers. © 2017 by SCITEPRESS - Science and Technology Publications, Lda.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041196997&origin=inward,Conference Paper,SCOPUS_ID:85041196997,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),using opensource software tools for data analysis in high intensity shock calibration of accelerometers,"
AbstractView references

This paper describes the concept and implementation of a model based data analysis of primary shock calibration. The concept follows to a large extent the scheme of ISO 16063-43. In addition, it uses classical statistics to combine a number of measurements. The implementation is programmed in Python, i.e. using open source software, in particular a new tool box for the analysis of dynamic measurements, PyDynamic, which features the integrated handling of uncertainties in terms of covariances.
"
10.1115/IMECE2017-70223,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85040929716&origin=inward,Conference Paper,SCOPUS_ID:85040929716,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),platform for mechatronics education using (1) mechatronics technology demonstrator and (2) web based virtual experimentation,"
AbstractView references

Increasing demands on the productivity of complex systems, such as machine tools and their steadily growing technological importance will require the application of new methods in the product development process. This paper shows that the analysis of the simulation results from the simulation based mechatronic model of a complex system followed by a procedure that allows a better understanding of the dynamic behavior and interactions of the components. This paper will highlight the results of interaction between National Institute of Technology, (NITK) Surathkal, India and University of District of Columbia (UDC) in the area of Mechatronics and virtual testing. Mechatronics is a design philosophy, which is an integrating approach to engineering design. Through a mechanism of simulating interdisciplinary ideas and techniques, mechatronics provides ideal conditions to raise the synergy, thereby providing a catalytic effect for the new solutions to technically complex situations. Many real-world systems can be modeled by the mass-spring-damper system and hence considering one such system, namely Mechatronics Technology Demonstrator (MTD) is taken as the first example. MTD is a portable low cost, technology demonstrator that can be used for teaching mechatronics system design. The paper highlights design optimization of several mechatronic products using the procedures derived by the use of mass spring damper based mechatronic system. The second example is on web based virtual experimentation, where the experiment is conducted by remote triggering of Torsion Testing Machine. Remote triggered (RT) experimentation is a method of remotely controlling the laboratory equipment by an internet based system from a webpage. RT lab is an excellent way for the students to get access to expensive state of the art labs and equipment. The present work deals with the systematic approach of realizing a remote triggered experimentation on a horizontal torsional testing machine which can be triggered from a tablet PC or a laptop through an internet connection directed to the server computer system. RT lab algorithms are built in the server computer and the information and controls will be displayed on an html webpage where the experiment can be conducted. In this experiment the machine is remotely started through a command in the webpage which will be directed to the main server computer system from a wireless handheld internet enabled device such as laptops or tablet PCs and render the suitable graph of the experiment in the device. The experiment is completely in the control of the user. The person can either on/off the main equipment with the help of the device within the given slot of time and the data from the graph can be retrieved for further analysis. The first example uses a software platform of VisSim and the second example uses a software platform LabView. Although located in two different locations and countries, this paper examines the common mechatronics philosophy and the design approach used in modeling, simulation, optimization and virtual experimentation in building robust mechatronics product and procedures. Copyright © 2017 ASME.
"
10.18653/v1/P17-1003,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85040923863&origin=inward,Conference Paper,SCOPUS_ID:85040923863,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),neural symbolic machines: learning semantic parsers on freebase with weak supervision,"
AbstractView references

Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine (NSM), which contains (a) a neural ""programmer"", i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic ""computer"", i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE we augment it with an iterative maximum-likelihood training process. NSM outperforms the state-of-the-art on the WEBQUESTIONSSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge. © 2017 Association for Computational Linguistics.
"
10.1007/978-3-319-72389-1_22,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85038076060&origin=inward,Conference Paper,SCOPUS_ID:85038076060,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),chinese named entity recognition based on b-lstm neural network with additional features,"
AbstractView references

Traditional methods for named entity recognition (NER) require heavy feature engineering to achieve high performance. We propose a novel neural network architecture for NER that detects word features automatically without feature engineering. Our approach uses word embedding as input, feeds them into a bidirectional long short-term memory (B-LSTM) for modeling the context within a sentence, and outputs the NER results. This study extends the neural network language model through B-LSTM, which outperforms other deep neural network models in NER tasks. Experimental results show that the B-LSTM with word embedding trained on a large corpus achieves the highest F-score of 0.9247, thus outperforming state-of-the-art methods that are based on feature engineering. © 2017, Springer International Publishing AG.
"
10.1007/978-3-319-70010-6_25,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85035138796&origin=inward,Conference Paper,SCOPUS_ID:85035138796,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),similarity assessment of uml sequence diagrams using dynamic programming,"
AbstractView references

Unified Modeling Language is a modelling language used to visualize software system during requirement engineering phase. It was accepted as a standard modeling language for visualizing, specifying and documenting software systems by International Organization for Standardization (ISO) as a standard specification. It contained different type of diagrams for specifying software system, among these diagrams is sequence diagrams which is used to specify the functional behavior of software system. The growing complexity of software systems is one of the motivation behind matching of UML diagrams in order to pave the way of reusing existing software to developed new software systems. Previous works on sequence diagrams matching are based on Graph representation in which there is node whenever there is message sending or received. However, the search space for these approach is very large due to the number of nodes in the graph which makes the matching computationally expensive. This paper employed the use of Dynamic Programming approach in order to improve the efficiency of matching between two or more sequence diagrams. © Springer International Publishing AG 2017.
"
10.1007/978-3-319-70241-4_20,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85035017075&origin=inward,Conference Paper,SCOPUS_ID:85035017075,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an integrated enterprise modeling framework using the rup/uml business use-case model and bpmn,"
AbstractView references

Various frameworks are available for modeling an organizational setting. Their constituting models nevertheless mostly choose a particular decision level to represent perceived reality meaning that some introduce coarse-grained (i.e. abstract) elements and some others fine-grained (i.e. detailed) ones. Sometimes, in a same model, elements of various levels of granularity can be mixed like for example in the i* strategic rationale model. The main drawback is that this leads to hard to read and complex models, not ideal for easy and quick understanding of the software problem. Also, within the industry, poor unification in the use of models does exist. The various Unified Modeling Language (UML) models and the Business Process Model and Notation (BPMN) are nevertheless rather popular. In this paper, we study the use of the Business Use Case Model – an extension of the classical UML use-case model defined in the Rational Unified Process (RUP) – and the BPMN Business Process Model (BPM) as a unified framework for knowledge representation at strategic, tactical and operational levels. By default, the RUP advises to use UML activity diagrams for operational-level knowledge representation. Their main drawback is that they have been engineered to model software behavior with respect to the user and not business process modeling at large. The BPMN BPM thus offers more perspectives for pure business process modeling; that is why it mostly used in the industry for this purpose. The use of these models in a unified way is ensured by traceability at the various levels of modeling. © IFIP International Federation for Information Processing 2017.
"
10.1115/POWER-ICOPE2017-3171,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85029891352&origin=inward,Conference Paper,SCOPUS_ID:85029891352,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),new correlations of weighted sum of grey gases model applicable to computational fluid dynamics for oxy-fuel combustion and implementation,"
AbstractView references

Radiation heat transfer is the dominant model of heat transfer in the large scale industry boiler, especially in oxy-fuel combustion condition. Radiative properties of combustion gases and char oxidation in the oxy-fuel condition are obviously different from the air-fuel combustion, due to the N2 replaced by CO2. Through researchers proposed many helpful correlations based on the air-fuel Weighted-Sum-of-Grey- Gases-Model (WSGGM), the absorption coefficients were commonly constant or correlations were discrete by the classical molar ratio of H2O to CO2 (MR), which were mismatching the continuous value of MR in the real furnace. Meanwhile, the discrete MR is also not applied to the computational fluid dynamics (CFD). In this paper, new correlations for the WSGGM are determined as polynomial function of MR and temperature, which can be conveniently employed in Fluent by the form of user-defined-functions in C language. Parameters of model are fitted by total emittances calculated based on the timely HITEMP 2010 database. New correlations are validated by comparing the emittances with line-by-line calculations and other classical models. New correlations are employed in the CFD for the real industrial oxy-fuel combustion with the temperature range of 400-2600K, pressure path-length between 0.01 and 60 bar m. Several assumed test cases have been investigated to evaluate the accuracy of the models. Modified correlations for WSGGM give a better accuracy of the total emittances for the mixed combustion gases in the real furnace. New models including radiative and chemical reaction mechanisms have been employed to CFD modeling of combustion process for a tangentially fired 300MWe utility boiler. The industrial boiler is modeled by a partition meshing method with the hexahedral structured mesh. Due to the atmosphere shift from N2 to CO2, three aspects are essential to be modified for oxy-fuel: radiation model, char oxidation model and homogeneous volatile oxidation model. To investigate the performance of the furnace, air-fuel combustion selected as the conference, three other cases employed are defined as Oxy21 (vol21%, O2), Oxy26 (vol26, O2) and Oxy29 (vol29%, O2), respectively. Temperature profile and heat transfer are investigated for the different test cases. Meanwhile, the simulation and calculation heat transfer in the furnace are also compared. The results show the new modified simulation has an approximate 4-11% lower than the thermodynamic calculation. To achieve an identical heat flux and temperature distributions with the air-fuel case, the molar fraction 29% of O2 is essential for the selected implementation. (CSPE) Copyright © 2017 ASME.
"
10.18293/SEKE2017-164,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85029511522&origin=inward,Conference Paper,SCOPUS_ID:85029511522,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),analyzing duplication on code generated by scaffolding frameworks for graphical user interfaces,"
AbstractView references

Scaffolding is an approach used by some modern web frameworks in order to generate an initial version of applications code based on domain model meta data. Since this temporary code should be customized by programmers to implement real systems, its quality metrics are important aspects. In this paper, a methodology is proposed and applied in order to relate domain model size and a quality metric -Amount of duplicated code - focusing on Graphical user interface implementation. Results show that code duplication grows at least linearly with the growth of the number of entities in domain model. There are also some scenarios where quadratic proportions were found. These observations suggest that, for large domain models, code quality and its evolution would be affected when scaffolding frameworks are used.
"
,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85029502053&origin=inward,Conference Paper,SCOPUS_ID:85029502053,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an automatic layout approach for istar models,"
AbstractView references

The comprehensive syntax of iStar modeling language allows requirements analysts to clearly capture stakeholder's needs, as well as dependencies among stakeholders. However, such iStar models cannot be automatically laid out using typical layout algorithms, such as hierarchical layout and circular layout. Thus, constructing and adjusting iStar models are laborious tasks, especially when dealing with large-scale models. In this paper, we propose a tentative approach to automatically lay out iStar models using a force-based layout algorithm. In particular, our approach has been designed by taking into account the syntax of iStar models in order to ensure both neatness and understandability of resulting models.
"
10.1117/12.2268377,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85029499523&origin=inward,Conference Paper,SCOPUS_ID:85029499523,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),optics simulations: a python workshop,"
AbstractView references

Numerical simulations allow teachers and students to indirectly perform sophisticated experiments that cannot be realizable otherwise due to cost and other constraints. During the past few decades there has been an explosion in the development of numerical tools concurrently with open source environments such as Python software. This availability of open source software offers an incredible opportunity for advancing teaching methodologies as well as in research. More specifically it is possible to correlate theoretical knowledge with experimental measurements using ""virtual"" experiments. We have been working on the development of numerical simulation tools using the Python program package and we have concentrated on geometric and physical optics simulations. The advantage of doing hands-on numerical experiments is that it allows the student learner to be an active participant in the pedagogical/learning process rather than playing a passive role as in the traditional lecture format. Even in laboratory classes because of constraints of space, lack of equipment and often-large numbers of students, many students play a passive role since they work in groups of 3 or more students. Furthermore these new tools help students get a handle on numerical methods as well simulations and impart a ""feel"" for the physics under investigation. © 2017 ICO, IEEE, OSA, SPIE.
"
10.1007/978-3-319-63121-9_14,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85028051271&origin=inward,Book Chapter,SCOPUS_ID:85028051271,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),firm deadline checking of safety-critical java applications with statistical model checking,"
AbstractView references

In cyber-physical applications many programs have hard real-time constraints that have to be stringently validated. In some applications, there are programs that have hard deadlines, which must not be violated. Other programs have soft deadlines where the value of the response decreases when the deadline is passed although it is still a valid response. In between, there are programs with firm deadlines. Here the response may be occasionally delayed; but this should not happen too often or with too large an overshoot. This paper presents an extension to an existing approach and tool for checking hard deadline constraints to the case of firm deadlines for application programs written in Safety-Critical Java (SCJ). The existing approach uses models and model checking with the Uppaal toolset; the extension uses the statistical model checking features of Uppaal-smc to provide a hold on firm deadlines and performance in the case of soft deadlines. The extended approach is illustrated with examples from applications. © 2017, Springer International Publishing AG.
"
10.1007/978-3-319-61482-3_9,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85025117215&origin=inward,Conference Paper,SCOPUS_ID:85025117215,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),iterative model-driven development of software extensions for web content management systems,"
AbstractView references

Dynamic web applications powered by Web Content Management Systems (WCMSs) such as Joomla,WordPress, or Drupal dominate today’s web. A main advantage of WCMSs is their functional extensibility by standardized WCMS extensions. However, the development and evolution of these extensions are challenging tasks. Due to dependencies to the core platform and otherWCMS extensions, the code structure of an extension includes a large defect potential. Mistakes usually lead to website crashes and are hard to find, especially for inexperienced developers. In this work, we define a model-driven development (MDD) process and apply it during the development of software extensions for the WCMS Joomla. To address two separate scenarios, involving the development of independent and dependent WCMS extensions, we use an MDD infrastructure, comprising a domain-specific language, a code editor, and reverse engineering facilities. In addition, we provide evidence indicating that our model-driven approach is useful to generate extensions with consistent interdependencies, demonstrating that the main issues of extension development in the WCMS domain can be addressed using a model-driven approach. By applying the MDD infrastructure on actual projects, we additionally present the lessons learned. © Springer International Publishing AG 2017.
"
10.1007/978-3-319-59294-7_4,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85022204086&origin=inward,Conference Paper,SCOPUS_ID:85022204086,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a modeling language for adaptive normative agents,"
AbstractView references

Agent-based software engineering has been proposed as a means of mastering the complexity associated with the development of large-scale distributed systems. However, agent-oriented software engineering has not been widely adopted, mainly due to lack of modeling languages that are expressive and comprehensive enough to represent relevant agent-related abstractions and support the refinement of design models into code. Most modeling languages do not define how these abstractions interact at run-time, but many software applications need to adapt their behavior, react to changes in their environments dynamically, and align with some form of individual or collective normative application behavior (e.g., obligations, prohibitions). In this paper, we propose a conceptual framework to developing adaptive normative agents. We believe the proposed approach will advance the state-of-the-art in agent systems so that software technologies for dynamic, adaptive, norm-based applications can be developed and implemented. © Springer International Publishing AG 2017.
"
10.5220/0006274502080219,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021431230&origin=inward,Conference Paper,SCOPUS_ID:85021431230,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),complete code generation from uml state machine,"
AbstractView references

An event-driven architecture is a useful way to design and implement complex systems. The UML State Machine and its visualizations are a powerful means to the modeling of the logical behavior of such an architecture. In Model Driven Engineering, executable code can be automatically generated from state machines. However, existing generation approaches and tools from UML State Machines are still limited to simple cases, especially when considering concurrency and pseudo states such as history, junction, and event types. This paper provides a pattern and tool for complete and efficient code generation approach from UML State Machine. It extends IF-ELSE-SWITCH constructions of programming languages with concurrency support. The code generated with our approach has been executed with a set of state-machine examples that are part of a test-suite described in the recent OMG standard Precise Semantics Of State Machine. The traced execution results comply with the standard and are a good hint that the execution is semantically correct. The generated code is also efficient: it supports multi-thread-based concurrency, and the (static and dynamic) efficiency of generated code is improved compared to considered approaches. © 2017 by SCITEPRESS - Science and Technology Publications, Lda.
"
10.1007/978-3-319-59466-8_10,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021210271&origin=inward,Conference Paper,SCOPUS_ID:85021210271,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),eye tracking experiments on process model comprehension: lessons learned,"
AbstractView references

For documenting business processes, there exists a plethora of process modeling languages. In this context, graphical process models are used to enhance the process comprehensibility of the stakeholders involved. The large number of available modeling languages, however, aggravates process model comprehension and increases the knowledge gap between domain and modeling experts. Upon this, one major challenge is to identify factors fostering the comprehension of process models. This paper discusses the experiences we gathered with the use of eye tracking in experiments on process model comprehension and the lessons learned in this context. The objective of the experiments was to study the comprehension of process models expressed in terms of four different modeling languages (i.e., BPMN, eGantt, EPC, and Petri Net). This paper further provides recommendations along nine identified categories that can foster related experiments on process model comprehension. © Springer International Publishing AG 2017.
"
10.1093/jamia/ocw156,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85019722825&origin=inward,Article,SCOPUS_ID:85019722825,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),de-identification of patient notes with recurrent neural networks,"
AbstractView references

Objective: Patient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identiﬁed notes, in order to protect the conﬁdentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) deﬁnes 18 types of protected health information that needs to be removed to de-identify patient notes. Manual de-identiﬁcation is impractical given the size of electronic health record databases, the limited number of researchers with access to non-de-identiﬁed notes, and the frequent mistakes of human annotators. A reliable automated de-identiﬁcation system would consequently be of high value. Materials and Methods: We introduce the ﬁrst de-identiﬁcation system based on artiﬁcial neural networks (ANNs), which requires no handcrafted features or rules, unlike existing systems. We compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identiﬁcation challenge dataset, which is the largest publicly available de-identiﬁcation dataset, and the MIMIC de-identiﬁcation dataset, which we assembled and is twice as large as the i2b2 2014 dataset. Results: Our ANN model outperforms the state-of-the-art systems. It yields an F1-score of 97.85 on the i2b2 2014 dataset, with a recall of 97.38 and a precision of 98.32, and an F1-score of 99.23 on the MIMIC deidentiﬁcation dataset, with a recall of 99.25 and a precision of 99.21. Conclusion: Our ﬁndings support the use of ANNs for de-identiﬁcation of patient notes, as they show better performance than previously published systems while requiring no manual feature engineering. © The Author 2016.
"
10.1007/978-3-319-58694-6_1,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85019719742&origin=inward,Conference Paper,SCOPUS_ID:85019719742,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),"modeling, generating, and publishing knowledge as linked data","
AbstractView references

The process of extracting, structuring, and organizing knowledge from one or multiple data sources and preparing it for the Semantic Web requires a dedicated class of systems. They enable processing large and originally heterogeneous data sources and capturing new knowledge. Offering existing data as Linked Data increases its shareability, extensibility, and reusability. However, using Linking Data as a means to represent knowledge can be easier said than done. In this tutorial, we elaborate on the importance of semantically annotating data and how existing technologies facilitate their mapping to Linked Data. We introduce [R2]RML languages to generate Linked Data derived from different heterogeneous data formats –e.g., DBs, XML, or JSON– and from different interfaces –e.g., files or Web apis. Those who are not Semantic Web experts can annotate their data with the RMLEditor, whose user interface hides all underlying Semantic Web technologies to data owners. Last, we show how to easily publish Linked Data on the Web as Triple Pattern Fragments. As a result, participants, independently of their knowledge background, can model, annotate and publish data on their own. © Springer International Publishing AG 2017.
"
10.15632/jtam-pl.55.2.447,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85019544760&origin=inward,Article,SCOPUS_ID:85019544760,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),an automated cad/cae integration system for the parametric design of aircraft wing structures,"
AbstractView references

In order to take advantage of the sophisticated features offered by CAD and CAE packages for modeling and analysis during the design process, it is essential to build a bridge assuring a coherent link between these tools. Furthermore, this integration procedure must be automated so as to get rid of the repetitive costing effort. In this paper, a new automated procedure for the CAD/CAE integration, implemented for the parametric design and structural analysis of aircraft wing structures is presented. This procedure is based on the automation capacity available in modern computer aided tools via build-in basic programming languages as well as the capacity of the model data exchange. The geometric and numerical models can be controlled to generate a large variety of possible design cases through parameters introduced beforehand.
"
10.3233/SW-160235,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85017384665&origin=inward,Article,SCOPUS_ID:85017384665,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),semfis: a flexible engineering platform for semantic annotations of conceptual models,"
AbstractView references

In this paper, we present SeMFIS-a flexible engineering platform for semantic annotations of conceptual models. Conceptual models have been used in the past for many purposes in the context of information systems' engineering. These purposes include for example the elicitation of requirements, the simulation of the behavior of future information systems, the generation of code or the interaction with information systems through models at runtime. Semantic annotations of conceptual models constitute a recently established approach for dynamically extending the semantic representation and semantic analysis scope of conceptual modeling languages. Thereby, elements in conceptual models are linked to concepts in ontologies via annotations. Thus, additional knowledge aspects can be represented without modifications of the modeling languages. These aspects can then be analyzed using queries, specifically designed algorithms or external tools and services. At its core, SeMFIS provides a set of meta models for visually representing ontologies and semantic annotations as models. In addition, the tool contains an analysis component, a web service interface, and an import/export component to query and exchange model information. SeMFIS has been implemented using the freely available ADOxx meta modeling platform. It can thus be directly added to the large variety of other modeling methods based on this platform or used as an additional service for other tools. We present the main features of SeMFIS and briefly discuss use cases where it has been applied. SeMFIS is freely available via OMiLAB at http://semfis-platform.org/. © 2017-IOS Press and the authors. All rights reserved.
"
10.1007/978-3-319-54042-9_12,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85014530330&origin=inward,Book Chapter,SCOPUS_ID:85014530330,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),a hybrid clp/mp approach to modeling and solving resource-constrained scheduling problems with logic constraints,"
AbstractView references

Constrained scheduling problems are common in everyday life and especially in: distribution, manufacturing, project management, logistics, supply chain management, software engineering, computer networks etc. A large number of integer and binary decision variables representing the allocation of different constrained resources to activities/jobs and constraints on these decision variables are typical elements of the resource-constrained scheduling problems (RCSPs) modeling. Therefore, the models of RCSPs are more demanding, particularly when methods of operations research (OR) are used. By contrast, most resource-constrained scheduling problems can be easily modeled as instances of the constraint satisfaction problems (CSPs) and solved using constraint logic programming (CLP) or others methods. Moreover, CLP-based environments enable easy modeling of various types of constraints including logic constraints. In the CLP-based environment the problem definition is separated from the algorithms and methods used to solve the problem. Therefore, a hybrid approach to resource-constrained scheduling problems that combines an OR-based approach for problem solving and a CLP-based approach for problem modeling is proposed. To evaluate the efficiency and flexibility of this approach, illustrative examples of resource-constrained scheduling problems with logic constraints are implemented using hybrid CLP/MP approach. © Springer International Publishing AG 2017.
"
10.1007/978-3-319-49622-1_11,,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85009827044&origin=inward,Conference Paper,SCOPUS_ID:85009827044,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),cml-wsn: a configurable multi-layer wireless sensor network simulator,"
AbstractView references

Wireless Sensor Networks (WSNs) have large applications in environments where access to human cannot be constant or where reliable and timely information is required to support decisions. WSNs must show high reliability, robustness, availability of information, monitoring capabilities, self-organization, among other aspects. Also, engineering requirements, such as low-cost implementation, operation, and maintenance are necessary. In this context, a simulator is a powerful tool for analyzing and improving network technologies used as a first step to investigate protocol design and performance test on large-scale systems without the need of real implementation. In this paper, we present a Configurable Multi-Layer WSN (CML-WSN) simulator. The CML-WSN simulator incorporates a configurable energy model to support any sensor specification as a one of its main features. The CML-WSN simulator is useful because it allows exploring prototypes with much less cost and time compared to the requirements needed in real networks implementations. © ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2017.
"
10.1016/j.tmaid.2016.09.002,S147789391630120X,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84992313631&origin=inward,Article,SCOPUS_ID:84992313631,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),the impact of crowd control measures on the occurrence of stampedes during mass gatherings: the hajj experience,"
                  Background
                  Mass gatherings present enormous challenges for emergency preparedness. Planners must anticipate and prepare for communicable and non-communicable disease outbreaks, illnesses, and injuries to participants, crowd control, and disaster responses to unforeseen natural or man-made threats. The Hajj, the largest annually recurring mass gathering event on earth. It attracts about 3 million pilgrims from over 180 countries who assemble in Mecca over a 1-week period.
               
                  Methods
                  A literature review was conducted using Medline and OVID, while searching for published data concerning human stampedes and crowd control measures implemented to prevent human stampedes. The review was further extended to include media reports and published numbers and reports about Hajj from the Saudi Arabian government, in both the English and Arabic languages.
               
                  Results
                  Because millions of pilgrims undertake their religious ritual within strict constraints in term of space and time; this rigour and strictness have led to a series of large crowd disasters over several years, thus putting pressure on the authorities. In the past few years, the government of Saudi Arabia have put an enormous effort to solve this difficulty using state of the art innovative scientific means. The use of crowd simulation models, assessment of the best ways of grouping and scheduling pilgrims, crowd management and control engineering technologies, luggage management, video monitoring, and changes in the construction of the transport system for the event.
               
                  Conclusions
                  A large gathering such as the Hajj still holds an increasing risk for future disasters. International collaboration and continued vigilance in planning efforts remains an integral part of these annual preparations. The development of educational campaigns for pilgrims regarding the possible dangers is also crucial. Lessons gleaned from experiences at the Hajj may influence planning for mass gatherings of any kind, worldwide.
               "
10.1016/j.infsof.2016.04.003,S095058491630057X,https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84964914212&origin=inward,Article,SCOPUS_ID:84964914212,scopus,2017-01-01,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),formal mutation testing for circus,"
                  
                     Context: The demand from industry for more dependable and scalable test-development mechanisms has fostered the use of formal models to guide the generation of tests. Despite many advancements having been obtained with state-based models, such as Finite State Machines (FSMs) and Input/Output Transition Systems (IOTSs), more advanced formalisms are required to specify large, state-rich, concurrent systems. 
                        Circus
                     , a state-rich process algebra combining Z, CSP and a refinement calculus, is suitable for this; however, deriving tests from such models is accordingly more challenging. Recently, a testing theory has been stated for 
                        Circus
                     , allowing the verification of process refinement based on exhaustive test sets.
                  
                     Objective: We investigate fault-based testing for refinement from 
                        Circus
                      specifications using mutation. We seek the benefits of such techniques in test-set quality assertion and fault-based test-case selection. We target results relevant not only for 
                        Circus
                     , but to any process algebra for refinement that combines CSP with a data language.
                  
                     Method: We present a formal definition for fault-based test sets, extending the 
                        Circus
                      testing theory, and an extensive study of mutation operators for 
                        Circus
                     . Using these results, we propose an approach to generate tests to kill mutants. Finally, we explain how prototype tool support can be obtained with the implementation of a mutant generator, a translator from 
                        Circus
                      to CSP, and a refinement checker for CSP, and with a more sophisticated chain of tools that support the use of symbolic tests.
                  
                     Results: We formally characterise mutation testing for 
                        Circus
                     , defining the exhaustive test sets that can kill a given mutant. We also provide a technique to select tests from these sets based on specification traces of the mutants. Finally, we present mutation operators that consider faults related to both reactive and data manipulation behaviour. Altogether, we define a new fault-based test-generation technique for 
                        Circus
                     .
                  
                     Conclusion: We conclude that mutation testing for 
                        Circus
                      can truly aid making test generation from state-rich model more tractable, by focussing on particular faults.
               "
