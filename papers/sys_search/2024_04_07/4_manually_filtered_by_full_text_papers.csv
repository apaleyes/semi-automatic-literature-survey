id,status,doi,publisher,database,query_name,query_value,url,publication_date,title,abstract,semantic_score
1,included,10.1007/s43681-023-00289-2,Springer,springer,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),http://dx.doi.org/10.1007/s43681-023-00289-2,2023-05-30 00:00:00,auditing large language models: a three-layered approach,"Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks posed by LLMs. However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLMs at all. Ultimately, this article seeks to expand the methodological toolkit available to technology providers and policymakers who wish to analyse and evaluate LLMs from technical, ethical, and legal perspectives.",0.8081768155097961
2,included,10.1109/raise.2019.00015,RAISE@ICSE,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/4167fbb4c9d3e4d0ab4982d4d102edc5c935ae1e,2019-01-01 00:00:00,towards concept based software engineering for intelligent agents,"The development of AI and machine learning applications at an industry mature level while maintaining quality and productivity goals is one of today's major challenges. Research in the field of intelligent agents has achieved many successes in recent years, especially due to various reinforcement learning techniques, and promises a high benefit in times of automation and autonomous systems. Bringing them into production, however, requires optimization against many other criteria than just accuracy. This leads to the emerging field of machine teaching. We already know many of the objectives used there from software engineering research, which has led to many well-established principles in recent decades. One of them is the component-based development whose idea finds an interesting counterpart in hierarchical reinforcement learning. We show that both areas can benefit from each other and introduce our approach of concept based software engineering, which is focused on supporting productivity and quality goals during the development of such systems.",0.8317070007324219
3,included,http://arxiv.org/abs/2302.07872v1,arxiv,arxiv,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),http://arxiv.org/abs/2302.07872v1,2023-02-14 00:00:00,data-centric governance,"Artificial intelligence (AI) governance is the body of standards and
practices used to ensure that AI systems are deployed responsibly. Current AI
governance approaches consist mainly of manual review and documentation
processes. While such reviews are necessary for many systems, they are not
sufficient to systematically address all potential harms, as they do not
operationalize governance requirements for system engineering, behavior, and
outcomes in a way that facilitates rigorous and reproducible evaluation. Modern
AI systems are data-centric: they act on data, produce data, and are built
through data engineering. The assurance of governance requirements must also be
carried out in terms of data. This work explores the systematization of
governance requirements via datasets and algorithmic evaluations. When applied
throughout the product lifecycle, data-centric governance decreases time to
deployment, increases solution quality, decreases deployment risks, and places
the system in a continuous state of assured compliance with governance
requirements.",0.8804633617401123
4,included,10.1007/978-3-030-60117-1_32,scopus,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094144617&origin=inward,2020-01-01 00:00:00,safety analytics for ai systems,"
AbstractView references

Growing AI technologies are a threat to safety and security in systems due to its obscurity and uncertainty. This study introduces a prevailing Deep Learning model, Convolutional Neural Network (CNN) and it’s deep weaknesses through a simple case study of the CNN model based on Keras for handwriting recognition. It reveals that CNN algorithms don’t adapt well to changes. Adding new cases to the training data may improve accuracy, but not to the same level as before. Synthetic training data may improve the accuracy superficially because of the similarity of data distributions between generated data and original data. Prevailing ML models such as Generative Adversarial Networks (GAN) have their limitations such as similarity-addiction and modality collapse. They could be toxic to safety engineering without domain expertise. The study proposed four test strategies: 1) AI systems should be tested by the third parties, not the developers; 2) test datasets should be categorically different from training datasets; the test data should not be a part of the training data; the test data should be collected from independent sources to increase the “diversity” of data modality; 3) avoid fake data, or simulated data; and 4) don’t collect the data that are conveniently available, but actively collect disastrous event data, unexpected, or the worst scenarios that may destroy the model. The study also introduces a multidimensional checklist for AI safety analysis, including sensors, data and environments, default and recovery mode, system architectures, and human-system interaction. © 2020, Springer Nature Switzerland AG.
",0.8118554949760437
5,included,10.1145/3375627.3375872,core,core,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),http://arxiv.org/abs/2002.05672,2020-11-09 00:00:00,steps towards value-aligned systems,"Algorithmic (including AI/ML) decision-making artifacts are an established
and growing part of our decision-making ecosystem. They are indispensable tools
for managing the flood of information needed to make effective decisions in a
complex world. The current literature is full of examples of how individual
artifacts violate societal norms and expectations (e.g. violations of fairness,
privacy, or safety norms). Against this backdrop, this discussion highlights an
under-emphasized perspective in the literature on assessing value misalignment
in AI-equipped sociotechnical systems. The research on value misalignment has a
strong focus on the behavior of individual tech artifacts. This discussion
argues for a more structured systems-level approach for assessing
value-alignment in sociotechnical systems. We rely primarily on the research on
fairness to make our arguments more concrete. And we use the opportunity to
highlight how adopting a system perspective improves our ability to explain and
address value misalignments better. Our discussion ends with an exploration of
priority questions that demand attention if we are to assure the value
alignment of whole systems, not just individual artifacts.Comment: Original version appeared in Proceedings of the 2020 AAAI ACM
  Conference on AI, Ethics, and Society (AIES '20), February 7-8, 2020, New
  York, NY, USA. 5 pages, 2 figures. Corrected some typos in this versio",0.8055118918418884
6,included,10.1109/ictc55196.2022.9952989,Information and Communication Technology Convergence,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/f4b42bdd510b993177c578830e89e58776060aab,2022-01-01 00:00:00,trustops: a risk-based ai engineering process,"We live in an era of artificial intelligence (AI) technology, which enables traditional HW/SW systems to help humans make wise decisions and even operate in intelligent ways. However, due to its inductive behaviors, this data-driven technology creates problems differ from those of conventional deductive systems. Therefore, the more AI is used in a wider range of industries and societies, the more problems and issues unseen before arise in human society. That's why most international organizations and nations released principles, and are struggling to find a technical methodology for assuring trustworthiness of AI. In many studies and papers, technical requirements are being presented individually and risk management is raised as the one of those. This paper intends to present an integrated and systematic engineering process that implements technical requirements in aspect of risk management.",0.8350556492805481
7,included,10.1515/auto-2022-0020,at - Automatisierungstechnik,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/20e4fac51b5a8ced6e66f9fe11cddad7d87c6d6e,2022-01-01 00:00:00,paise® – process model for ai systems engineering,"Abstract The application of artificial-intelligence-(AI)-based methods within the context of complex systems poses new challenges within the product life cycle. The process model for AI systems engineering, PAISE®, addresses these challenges by combining approaches from the disciplines of systems engineering, software development and data science. The general approach builds on a component-wise development of the overall system including an AI component. This allows domain specific development processes to be parallelized. At the same time, component dependencies are tested within interdisciplinary checkpoints, thus resulting in a refinement of component specifications.",0.9021071195602416
8,included,10.1109/isse54508.2022.10005383,IEEE,ieeexplore,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://ieeexplore.ieee.org/document/10005383/,2022-10-26 00:00:00,patients’ perceptions of integrating ai into healthcare: systems thinking approach,"Artificial intelligence (AI) has become more integrated into healthcare with a promising future. The expansion and application of Artificial Intelligence depend on technologies, policymakers, healthcare providers, as well as patients. Nevertheless, how to systematically understand AI interventions from the patients' perspectives should be further explored. In this paper, we first outline patients' perceptions of integrating AI into healthcare systems through a brief survey-based case study. Next, we emphasized the challenges and concerns of applying AI to complex healthcare systems while considering the components' interactions. Then a three-layer structure was proposed to highlight the complexity of Human-AI-Technology interactions linked to the governance system. Moreover, a causal loop diagram (CLD) is established to analyze the dynamic and causality of the adoption of AI in healthcare inspired by the systems thinking approach to help understand the patients' attitudes and perceptions of the whole picture.",0.811959445476532
9,included,10.1109/transai60598.2023.00015,IEEE,ieeexplore,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://ieeexplore.ieee.org/document/10387654/,2023-09-27 00:00:00,ai engineering to deploy reliable ai in industry,"To bring competitive advantage to industry through a sound AI deployment, we need an end-to-end “AI systems engineering” process covering the overall lifecycle of an AI system, both at component level and at system level, regardless of whether the specifications come from regulation and reliability concerns.",0.8573559522628784
10,included,10.1093/jamia/ocac006,J. Am. Medical Informatics Assoc.,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/a2736b7a0f7a0002e912c665b1dc99fd40b469c4,2021-01-01 00:00:00,defining amia's artificial intelligence principles,"Recent advances in the science and technology of artificial intelligence (AI) and growing numbers of deployed AI systems in healthcare and other services have called attention to the need for ethical principles and governance. We define and provide a rationale for principles that should guide the commission, creation, implementation, maintenance, and retirement of AI systems as a foundation for governance throughout the lifecycle. Some principles are derived from the familiar requirements of practice and research in medicine and healthcare: beneficence, nonmaleficence, autonomy, and justice come first. A set of principles follow from the creation and engineering of AI systems: explainability of the technology in plain terms; interpretability, that is, plausible reasoning for decisions; fairness and absence of bias; dependability, including ""safe failure""; provision of an audit trail for decisions; and active management of the knowledge base to remain up to date and sensitive to any changes in the environment. In organizational terms, the principles require benevolence-aiming to do good through the use of AI; transparency, ensuring that all assumptions and potential conflicts of interest are declared; and accountability, including active oversight of AI systems and management of any risks that may arise. Particular attention is drawn to the case of vulnerable populations, where extreme care must be exercised. Finally, the principles emphasize the need for user education at all levels of engagement with AI and for continuing research into AI and its biomedical and healthcare applications.",0.826555073261261
11,included,10.1109/wain52551.2021.00015,Workshop on AI Engineering - Software Engineering for AI,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/854aa07af911d493de2c465c5748b9453cee6a4d,2021-01-01 00:00:00,understanding and modeling ai-intensive system development,"Developers of AI-Intensive Systems—i.e., systems that involve both “traditional” software and Artificial Intelligence—are recognizing the need to organize development systematically and use engineered methods and tools. Since an AI-Intensive System (AIIS) relies heavily on software, it is expected that Software Engineering (SE) methods and tools can help. However, AIIS development differs from the development of “traditional” software systems in a few substantial aspects. Hence, traditional SE methods and tools are not suitable or sufficient by themselves and need to be adapted and extended.A quest for “SE for AI” methods and tools has started. We believe that, in this effort, we should learn from experience and avoid repeating some of the mistakes made in the quest for SE in past years. To this end, a fundamental instrument is a set of concepts and a notation to deal with AIIS and the problems that characterize their development processes.In this paper, we propose to describe AIIS via a notation that was proposed for SE and embeds a set of concepts that are suitable to represent AIIS as well. We demonstrate the usage of the notation by modeling some characteristics that are particularly relevant for AIIS.",0.8918479681015015
12,included,10.1145/3284869.3284875,International Conference on Smart Objects and Technologies for Social Good,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/0a33dfef5b2d1588b3a6796ad9b827054699c79d,2018-01-01 00:00:00,intelligent machines for good?: more focus on the context,"Machine learning and modern Artificial Intelligence (AI) systems are influencing several aspects of our human lives. Many of these algorithms, based on Artificial Neural Networks (ANNs), have been empowered to make decisions and take actions, based on the well-known notions of efficiency and speed. The aura of objectivity and infallibility of such algorithms, nonetheless, have been already put into question (e.g., refer to the debate about the recent tragic car crashes that have involved self-driving cars). In this setting, our intuition identifies a key issue around the problem of AI errors and bias into the insufficient or inaccurate (human) activity of comprehension and codification of the context where the ANNs will have to operate. We present here a simple cognification ANN-based case study, in an underwater scenario, where we recovered from a situation of partial failure, by including additional contextual factors that were initially disregarded. Our final reflection is that a nuanced consideration of a complex context, and subsequent technical actions, should be always kept in mind before an AI-based system takes its final shape. Because machines have still no context for what they are doing, it is a human duty and responsibility to codify it.",0.8012804985046387
13,included,10.1515/auto-2022-0076,at - Automatisierungstechnik,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/f8492b9265e21f7db13c71214b68d640c6bf3903,2022-01-01 00:00:00,ki-engineering – ai systems engineering,Abstract KI-Engineering – translated as AI Systems Engineering – aims at the development of a new engineering practice in the intersection of Systems Engineering and Artificial Intelligence. Its goal is to professionalize the use of AI methods in a systems engineering context. The article defines KI-Engineering and compares it with historical examples of research disciplines that founded engineering disciplines. It furthermore discusses the long-term challenges where further development is needed and which results were already achieved in the context of the Competence Center for KI-Engineering (CC-KING).,0.8826118111610413
14,included,10.48550/arxiv.2212.11854,Business &amp; Information Systems Engineering,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/70df7fec4224c0d98252a6c61ee5f835be6f9e0b,2022-01-01 00:00:00,data-centric artificial intelligence,"Data-centric artificial intelligence (data-centric AI) represents an emerging paradigm that emphasizes the importance of enhancing data systematically and at scale to  build effective and efficient AI-based systems. The novel paradigm complements recent model-centric AI, which focuses on improving the performance of AI-based systems based on changes in the model using a fixed set of data. The objective of this article is to introduce practitioners and researchers from the field of Business and Information Systems Engineering (BISE) to data-centric AI. The paper defines relevant terms, provides key characteristics to contrast the paradigm of data-centric AI with the model-centric one, and introduces a framework to illustrate the different dimensions of data-centric AI. In addition, an overview of available tools for data-centric AI is presented and this novel paradigm is differenciated from related concepts. Finally, the paper discusses the longer-term implications of data-centric AI for the BISE community.",0.8859785795211792
15,included,10.18293/seke2020-094,International Conference on Software Engineering and Knowledge Engineering,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/36e614624eca44d7854c982d089e033658b50431,2020-01-01 00:00:00,guidelines for quality assurance of machine learning-based artificial intelligence,"Significant effort is being put into developing industrial applications for artificial intelligence (AI), especially those using machine learning (ML) techniques. Despite the intensive support for building ML applications, there are still challenges when it comes to evaluating, assuring, and improving the quality or dependability. The difficulty stems from the unique nature of ML, namely, system behavior is derived from training data not from logical design by human engineers. This leads to black-box and intrinsically imperfect implementations that invalidate many principles and techniques in traditional software engineering. In light of this situation, the Japanese industry has jointly worked on a set of guidelines for the quality assurance of AI systems (in the Consortium of Quality Assurance for AI-based Products and Services) from the viewpoint of traditional quality-assurance engineers and test engineers. We report on the second version of these guidelines, which cover a list of quality evaluation aspects, catalogue of current state-of-the-art techniques, and domain-specific discussions in five representative domains. The guidelines provide significant insights for engineers in terms of methodologies and designs for tests driven by application-specific requirements.",0.8727734684944153
16,included,http://arxiv.org/abs/2101.03989v2,arxiv,arxiv,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),http://arxiv.org/abs/2101.03989v2,2021-01-11 00:00:00,technology readiness levels for machine learning systems,"The development and deployment of machine learning (ML) systems can be
executed easily with modern tools, but the process is typically rushed and
means-to-an-end. The lack of diligence can lead to technical debt, scope creep
and misaligned objectives, model misuse and failures, and expensive
consequences. Engineering systems, on the other hand, follow well-defined
processes and testing standards to streamline development for high-quality,
reliable results. The extreme is spacecraft systems, where mission critical
measures and robustness are ingrained in the development process. Drawing on
experience in both spacecraft engineering and ML (from research through product
across domain areas), we have developed a proven systems engineering approach
for machine learning development and deployment. Our ""Machine Learning
Technology Readiness Levels"" (MLTRL) framework defines a principled process to
ensure robust, reliable, and responsible systems while being streamlined for ML
workflows, including key distinctions from traditional software engineering.
Even more, MLTRL defines a lingua franca for people across teams and
organizations to work collaboratively on artificial intelligence and machine
learning technologies. Here we describe the framework and elucidate it with
several real world use-cases of developing ML methods from basic research
through productization and deployment, in areas such as medical diagnostics,
consumer computer vision, satellite imagery, and particle physics.",0.8114748001098633
17,included,10.18293/seke2019-094,International Conference on Software Engineering and Knowledge Engineering,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/876d346fb11be7636f30b11f597a525ce50dfd26,2019-01-01 00:00:00,safe-by-design development method for artificial intelligent based systems,"Albeit Artiﬁcial Intelligent (AI) based systems are nowa-days deployed in a variety of safety critical domains, current engineering methods and standards are barely applicable for their development and assurance. The lack of common criteria to assess safety levels as well as the dependency of certain development phases w.r.t. the chosen technology (e.g., machine learning modules) are among the identiﬁed drawbacks. In addition, the development of such engineering methods has been hampered by the emerging challenges in AI-based systems design mainly regarding autonomy, correctness and prevention of catastrophic risks. In this paper we propose an approach to conduct a safe-by-design development process for AI based systems. The approach relies upon a method which beneﬁts from a reference AI architecture and safety principles. This contribution helps to address safety concerns and to comprehend current AI architectures diversity and particularities.",0.8835116624832153
18,included,10.1109/syscon53536.2022.9773829,IEEE,ieeexplore,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://ieeexplore.ieee.org/document/9773829/,2022-04-28 00:00:00,closed systems paradigm for intelligent systems,"Intelligent systems ought to be distinguished as a special type of system. While some adopt this view informally, in practice, systems engineering methods for intelligent systems are still centered around traditional systems engineering notions of engineering by aggregation of components. We posit that this traditional approach follows from holding a notion of open systems as the fundamental precept, and that engineering intelligent systems, in contrast, requires an approach that holds notions of closed systems as fundamental precepts. We take a systems theoretic approach to defining closed system phenomena and their relation to engineering intelligence. We propose the concept of variety; particularly the law of requisite variety to enable closed view in engineering. We discuss how open and closed view approaches to engineering intelligent systems address variety differently, as well as the implications of this difference on engineering practice.",0.8415651321411133
19,included,http://arxiv.org/abs/2006.12497v3,arxiv,arxiv,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),http://arxiv.org/abs/2006.12497v3,2020-06-21 00:00:00,technology readiness levels for ai & ml,"The development and deployment of machine learning systems can be executed
easily with modern tools, but the process is typically rushed and
means-to-an-end. The lack of diligence can lead to technical debt, scope creep
and misaligned objectives, model misuse and failures, and expensive
consequences. Engineering systems, on the other hand, follow well-defined
processes and testing standards to streamline development for high-quality,
reliable results. The extreme is spacecraft systems, where mission critical
measures and robustness are ingrained in the development process. Drawing on
experience in both spacecraft engineering and AI/ML (from research through
product), we propose a proven systems engineering approach for machine learning
development and deployment. Our Technology Readiness Levels for ML (TRL4ML)
framework defines a principled process to ensure robust systems while being
streamlined for ML research and product, including key distinctions from
traditional software engineering. Even more, TRL4ML defines a common language
for people across the organization to work collaboratively on ML technologies.",0.804334282875061
20,included,10.1109/sose52739.2021.9497496,IEEE,ieeexplore,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://ieeexplore.ieee.org/document/9497496/,2021-06-18 00:00:00,system of systems engineering approach for complex deterministic and nondeterministic systems (acdans),"As new commercial and military systems evolve, engineers face significant challenges that require solutions beyond traditional systems engineering. For example, military commanders recognize that in order to confront threats from high-tech adversaries, an advanced system of systems (SoS) is required to coordinate combat across multiple battlefield domains: land, sea, air, space, and cyberspace. System of Systems Engineering (SoSE), along with associated Modeling and Simulation (M&S) tools, can fill some of this need, especially for operational decision-support for complex multi-domain environments. This paper presents an M&S-based SoSE approach for complex SoS composed of deterministic and non-deterministic subsystems supported with reinforcement learning. The paper presents this new methodology, use cases, and preliminary results that address specific SoS challenges for a set of complex decision-support challenges.",0.8016894459724426
21,included,10.1109/tem.2023.3268340,scopus,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159803879&origin=inward,2023-01-01 00:00:00,ai in the context of complex intelligent systems: engineering management consequences,"
AbstractView references

As artificial intelligence (AI) is increasingly integrated into the context of complex products and systems (CoPS), making complex systems more intelligent, this article explores the consequences and implications for engineering management in emerging complex intelligent systems (CoIS). Based on five engineering management aspects, including design objectives, system boundaries, architecting and modeling, predictability and emergence, and learning and adaptation, a case study representing future CoIS illustrates how these five aspects, as well as their relationship to criticality and generativity, emerge as AI becomes an integrated part of the system. The findings imply that a future combined perspective on allowing generativity and maintaining or enhancing criticality is necessary, and notably, the results suggest that the understanding of system integrators and CoPS management partly fundamentally alters and partly is complemented with the emergence of CoIS. CoIS puts learning and adaptation characteristics in the foreground, i.e., CoIS are associated with increasingly generative design objectives, fluid system boundaries, new architecting and modeling approaches, and challenges predictability. The notion of bounded generativity is suggested to emphasize the combination of generativity and criticality as a direction for transforming engineering management in CoPS contexts and demands new approaches for designing future CoIS and safeguard its important societal functions. Author
",0.8543676137924194
22,included,10.1109/smc.2019.8914324,IEEE,ieeexplore,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://ieeexplore.ieee.org/document/8914324/,2019-10-09 00:00:00,the digital (mission) twin: an integrating concept for future adaptive cyber-physical-human systems,"Top-down decomposition of a complex system of systems (SoS) requires representation of the behavior of multiple individuals and cyber-physical systems performing a wide variety of tasks, usually at different times and with different goals. In the future, as humans and machines learn and co-adapt in their mission context, systems engineering must develop approaches that support dynamic analysis and synthesis in both the design and operation of systems. The concept of a “digital (mission) twin” provides an integration framework for design and control of future complex cyber-physical-human SoS. The framework builds on mission function task (MFT) analysis, providing a basis for model-based systems engineering (MBSE) activities to identify and address needs for automation or other forms of advanced technology that improve overall system performance. However, human-driven adaptation and eventually machine adaptation of complex SoS poses organizational and methodological challenges. We discuss the need for and opportunities for convergence of three often segregated SE methodologies: product engineering, human systems integration, and mission/operations analysis by expanding the product or process engineering view of a digital twin to the mission level. In the present this gives us a framework to analyze and specify automation and adaptation opportunities at the mission level. In the future we envision this twin continuously operates with the real systems to manage both emergent mission conditions and lifecycle adaptation. We applied this framework to a complex military mission using a case where artificial intelligence can be incorporated widely across a set of mission tasks to significantly improve performance.",0.8081477880477905
23,included,10.1109/iv47402.2020.9304740,IEEE,ieeexplore,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://ieeexplore.ieee.org/document/9304740/,2020-11-13 00:00:00,can ai-based components be part of dependable systems?,"Artificial Intelligence and especially Machine Learning have become main topics in the scientific community as well as in industry. These techniques seem to be a solution for complex problems and are suggested even for critical applications such as the medical diagnosis, predictive analytic in finance or autonomous driving. But will it be feasible to employ such techniques in critical systems in the avionics, train, or medical domain taking into account the current regulations in domain-specific standard relating to dependability? Validation, verification and certification in these domains strongly rely on explicit traceability and provability of functional and dependability requirements down to the code level. This seems to be impossible for components derived using e.g. learning approaches. Nonetheless, scientific community and industry do not want to lose the advantages related to AI-based techniques - new ways to ensure the required level of confidence just need to be found. This is a process that requires people with different professional background to work together - and it has started. This paper presents selected aspects relating to the current state of the art with regard to AI and dependable systems and describes ongoing activities and ideas for obstacle detection and routing for autonomous driving at HAW Hamburg.",0.8174083828926086
24,included,10.48550/arxiv.2204.04211,arXiv.org,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/40ed2b01f62baeb649ec5d30363bf392c25c246e,2022-01-01 00:00:00,measuring ai systems beyond accuracy,"Current test and evaluation (T&E) methods for assessing ma- chine learning (ML) system performance often rely on incomplete metrics. Testing is additionally often siloed from the other phases of the ML system lifecycle. Research inves- tigating cross-domain approaches to ML T&E is needed to drive the state of the art forward and to build an Artificial Intelligence (AI) engineering discipline. This paper advo- cates for a robust, integrated approach to testing by outlining six key questions for guiding a holistic T&E strategy.",0.8078747987747192
25,included,10.1109/acsos55765.2022.00030,International Conference on Autonomic Computing and Self-Organizing Systems,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/d1e8e98848d6a4cad32c1fe9ad7eb051f37256bd,2022-01-01 00:00:00,a modular and composable approach to develop trusted artificial intelligence,"Trustworthy artificial intelligence (Trusted AI) is of utmost importance when learning-enabled components (LECs) are used in autonomous, safety-critical systems. When reliant on deep learning, these systems need to address the reliability, robustness, and interpretability of learning models. In addition to developing specific strategies to address each of these concerns, appropriate software architectures are needed to coordinate LECs and ensure they deliver acceptable behavior under uncertain conditions. This work proposes a model-driven framework of loosely-coupled modular services designed to monitor and control LECs with respect to Trusted AI assurance concerns. The proposed framework is composable, deploying independent services to improve the resilience and robustness of AI systems. The overarching objective of this framework is to support software engineering principles focusing on modularity, composability, and reusability in order to facilitate development and maintenance tasks, while also increasing stakeholder confidence in Trusted AI systems. To demonstrate this framework, it has been implemented to manage the operation of an autonomous rover’s vision-based LEC while exposed to uncertain environmental conditions.",0.8028196096420288
26,included,10.48550/arxiv.2208.02837,Artificial General Intelligence,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/21672a43ee8ea925173702fe034cbde18d9c773a,2022-01-01 00:00:00,core and periphery as closed-system precepts for engineering general intelligence,"Engineering methods are centered around traditional notions of decomposition and recomposition that rely on partitioning the inputs and outputs of components to allow for component-level properties to hold after their composition. In artificial intelligence (AI), however, systems are often expected to influence their environments, and, by way of their environments, to influence themselves. Thus, it is unclear if an AI system's inputs will be independent of its outputs, and, therefore, if AI systems can be treated as traditional components. This paper posits that engineering general intelligence requires new general systems precepts, termed the core and periphery, and explores their theoretical uses. The new precepts are elaborated using abstract systems theory and the Law of Requisite Variety. By using the presented material, engineers can better understand the general character of regulating the outcomes of AI to achieve stakeholder needs and how the general systems nature of embodiment challenges traditional engineering practice.",0.8783146739006042
27,included,10.1109/rew56159.2022.00037,2022 IEEE 30th International Requirements Engineering Conference Workshops (REW),semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/1cc70f06afd34865deede7001925f35d250cc456,2022-01-01 00:00:00,ai ethics impact assessment based on requirement engineering,"This paper proposes a methodology for evaluating the ethical impact of artificial intelligence (AI) systems on people and society based on AI ethics guidelines. The ethical impact of AI has been recognized as a social issue, and countries and organizations have formulated principles and guidelines on AI ethics, and laws and regulations will be enforced in Europe. Because these principles and guidelines are written in terms of philosophy and law, AI service providers, developers, and business users have the challenge of how they should practice the principles and guidelines to their AI systems. To address this challenge, we first analyzed cases of ethical problems caused by AI in the past and assumed that ethical problems could be linked to interactions between components of AI systems and stakeholders related to such systems. On the basis of this assumption, we then developed a methodology to comprehensively extract the ethical risks that an AI system poses. This methodology consists of two approaches. The first approach is to develop an AI ethics model that embodies ethics guidelines as necessary requirements for ethical AI systems and correlates these requirements with interactions. The second approach is an impact assessment process that uses the AI ethics models to extract ethical risks for individual AI systems. In this paper, we discuss the details of this methodology and show the results of an initial validation to verify the above assumption and the ease of the impact assessment process.",0.8575771450996399
28,included,10.1145/3489449.3490014,European Conference on Pattern Languages of Programs,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/98cbc71f45d131de722c04bffb18cb59f4f75b44,2021-01-01 00:00:00,architectural patterns for integrating ai technology into safety-critical systems,"Artificial Intelligence (AI) is widely acknowledged as one of the most disruptive technologies driving the digital transformation of industries, enterprises, and societies in the 21st century. Advances in computing speed, algorithmic improvements, and access to a vast amount of data contributed to the adaption of AI in many different domains. Due to the outstanding performance, AI technology is increasingly integrated into safety-critical applications. However, the established safety engineering processes and practices have been only successfully applied in conventional model-based system development and no commonly agreed approaches for integrating AI technology are available yet. This work presents two architectural patterns that can support designers and engineers in the conception of safety-critical AI-enhanced cyber-physical system (CPS) applications. The first pattern addresses the problem of integrating AI capabilities into safety-critical functions. The second pattern deals with architectural approaches to integrate AI technologies for monitoring and learning system-specific behavior at runtime.",0.8690606355667114
29,included,10.48550/arxiv.2203.00905,arXiv.org,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/da40fe10253e732460baff189e7c8cdac9e6033d,2022-01-01 00:00:00,responsible-ai-by-design: a pattern collection for designing responsible ai systems,"Although AI has significant potential to transform society, there are serious concerns about its ability to behave and make decisions responsibly. Many ethical regulations, principles, and guidelines for responsible AI have been issued recently. However, these principles are high-level and difficult to put into practice. In the meantime much effort has been put into responsible AI from the algorithm perspective, but they are limited to a small subset of ethical principles amenable to mathematical analysis. Responsible AI issues go beyond data and algorithms and are often at the system-level crosscutting many system components and the entire software engineering lifecycle. Based on the result of a systematic literature review, this paper identifies one missing element as the system-level guidance - how to design the architecture of responsible AI systems. We present a summary of design patterns that can be embedded into the AI systems as product features to contribute to responsible-AI-by-design.",0.8269172310829163
30,included,10.1177/15553434221097357,Journal of Cognitive Engineering and Decision Making,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/f47fe4e6a34083f5f9a48cf281acfdd1fe52c1ea,2022-01-01 00:00:00,a sociotechnical systems framework for the application of artificial intelligence in health care delivery,"In the coming years, artificial intelligence (AI) will pervade almost every aspect of the health care delivery system. AI has the potential to improve patient safety (e.g., diagnostic accuracy) as well as reduce the burden on clinicians (e.g., documentation-related workload); however, these benefits are yet to be realized. AI is only one element of a larger sociotechnical system that needs to be considered for effective AI application. In this paper, we describe the current challenges of integrating AI into clinical care and propose a sociotechnical systems (STS) approach for AI design and implementation. We demonstrate the importance of an STS approach through a case study on the design and implementation of a clinical decision support (CDS). In order for AI to reach its potential, the entire work system as well as clinical workflow must be systematically considered throughout the design of AI technology.",0.8117738962173462
31,included,10.1109/sose55472.2022.9812672,International Symposium on Service Oriented Software Engineering,semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/5a19c398c2b4738f5eb473a175807bb9b59e6d71,2022-01-01 00:00:00,engineering dependable ai systems,"If AI algorithms are now pervasive in our daily life, they essentially deliver non critical services, i.e., services which failures remain socially and economically acceptable. In order to introduce those algorithms in critical systems, new engineering practices must be defined to give a justified trust in the capability of the system to deliver the intended services. In this paper, we give an overview of the approach that we have put in place to reach this goal in the framework of the French Confiance.ai program. Based on the needs of the industrial partners of the program, we propose a model-based analysis framework capturing the two dimensions of the problem: the one related to the development and operation of the system and the one related to the trust in the system.",0.8794834017753601
32,included,10.1109/syscon48628.2021.9447069,IEEE,ieeexplore,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://ieeexplore.ieee.org/document/9447069/,2021-05-15 00:00:00,design strategies for integrating artificial intelligence into systems engineering environment,"The use of artificial intelligence capabilities for developing airborne safety-critical systems has been troublesome to the aerospace industry. This technology inserts new sources of non-determinism on process execution, increasing difficulty to ensure safety requirements. In this work, we evaluate the artificial intelligence capabilities for improving systems engineering methodology. From this analysis, we present design strategies to support the tool qualification process. The design strategies are a sound basis for applying artificial intelligence into the tools employed during the whole airborne systems life cycle.",0.8561747670173645
33,included,10.1115/1.4062597,scopus,scopus,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173951276&origin=inward,2023-12-01 00:00:00,zero-trust for the system design lifecycle,"
AbstractView references

In an age of worsening global threat landscape and accelerating uncertainty, the design and manufacture of systems must increase resilience and robustness across both the system itself and the entire systems design process. We generally trust our colleagues after initial clearance/background checks; and systems to function as intended and within operating parameters after safety engineering review, verification, validation, and/ or system qualification testing. This approach has led to increased insider threat impacts; thus, we suggest moving to the ""trust, but verify""approach embodied by the Zero-Trust paradigm. Zero-Trust is increasingly adopted for network security but has not seen wide adoption in systems design and operation. Achieving the goal of Zero-Trust throughout the systems lifecycle will help to ensure that no single bad actor-whether human or machine learning/artificial intelligence (ML/AI)-can induce failure anywhere in a system's lifecycle. Additionally, while ML/AI and their associated risks are already entrenched within the operations phase of many systems' lifecycles, ML/AI is gaining traction during the design phase. For example, generative design algorithms are increasingly popular, but there is less understanding of potential risks. Adopting the Zero-Trust philosophy helps ensure robust and resilient design, manufacture, operations, maintenance, upgrade, and disposal of systems. We outline the rewards and challenges of implementing Zero-Trust and propose the framework for Zero-Trust for the system design lifecycle. This article highlights several areas of ongoing research with focus on high priority areas where the community should focus efforts. © 2023 by ASME.
",0.8407002687454224
34,included,10.1145/3522664.3528598,2022 IEEE/ACM 1st International Conference on AI Engineering – Software Engineering for AI (CAIN),semantic_scholar,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),https://www.semanticscholar.org/paper/bfa3cb3fae2d4fcc66b18dba752467f4f5861073,2022-01-01 00:00:00,ai governance in the system development life cycle: insights on responsible machine learning engineering,"In this study we explore the incorporation of artificial intelligence (AI) governance to system development life cycle (SDLC) models. We conducted expert interviews among AI and SDLC professionals and analyzed the interview data using qualitative coding and clustering to extract AI governance concepts. Subsequently, we mapped these concepts onto three stages in the machine learning (ML) system development process: (1) design, (2) development, and (3) operation. We discovered 20 governance concepts, some of which are relevant to more than one of the three stages. Our analysis highlights AI governance as a complex process that involves multiple activities and stakeholders. As development projects are unique, the governance requirements and processes also vary. This study is a step towards understanding how AI governance is conceptually connected to ML systems’ management processes through the project life cycle. CCS CONCEPTS • Software and its engineering $^{\rightarrow}$ Software creation and management.",0.8494580388069153
