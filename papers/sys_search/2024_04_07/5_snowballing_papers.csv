doi,type,query_name,query_value,publication,publisher,publication_date,database,title,url,abstract,status,semantic_score,id
http://arxiv.org/abs/2311.12755v1,preprocessed,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),arxiv,arxiv,2023-11-21 00:00:00,arxiv,"digital twin framework for optimal and autonomous decision-making in
  cyber-physical systems: enhancing reliability and adaptability in the oil and
  gas industry",http://arxiv.org/abs/2311.12755v1,"The concept of creating a virtual copy of a complete Cyber-Physical System
opens up numerous possibilities, including real-time assessments of the
physical environment and continuous learning from the system to provide
reliable and precise information. This process, known as the twinning process
or the development of a digital twin (DT), has been widely adopted across
various industries. However, challenges arise when considering the
computational demands of implementing AI models, such as those employed in
digital twins, in real-time information exchange scenarios. This work proposes
a digital twin framework for optimal and autonomous decision-making applied to
a gas-lift process in the oil and gas industry, focusing on enhancing the
robustness and adaptability of the DT. The framework combines Bayesian
inference, Monte Carlo simulations, transfer learning, online learning, and
novel strategies to confer cognition to the DT, including model
hyperdimensional reduction and cognitive tack. Consequently, creating a
framework for efficient, reliable, and trustworthy DT identification was
possible. The proposed approach addresses the current gap in the literature
regarding integrating various learning techniques and uncertainty management in
digital twin strategies. This digital twin framework aims to provide a reliable
and efficient system capable of adapting to changing environments and
incorporating prediction uncertainty, thus enhancing the overall
decision-making process in complex, real-world scenarios. Additionally, this
work lays the foundation for further developments in digital twins for process
systems engineering, potentially fostering new advancements and applications
across various industrial sectors.",not included,0.8155604390537038,1
http://arxiv.org/abs/2311.00903v1,preprocessed,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),arxiv,arxiv,2023-11-02 00:00:00,arxiv,"artificial intelligence ethics education in cybersecurity: challenges
  and opportunities: a focus group report",http://arxiv.org/abs/2311.00903v1,"The emergence of AI tools in cybersecurity creates many opportunities and
uncertainties. A focus group with advanced graduate students in cybersecurity
revealed the potential depth and breadth of the challenges and opportunities.
The salient issues are access to open source or free tools, documentation,
curricular diversity, and clear articulation of ethical principles for AI
cybersecurity education. Confronting the ""black box"" mentality in AI
cybersecurity work is also of the greatest importance, doubled by deeper and
prior education in foundational AI work. Systems thinking and effective
communication were considered relevant areas of educational improvement. Future
AI educators and practitioners need to address these issues by implementing
rigorous technical training curricula, clear documentation, and frameworks for
ethically monitoring AI combined with critical and system's thinking and
communication skills.",not included,0.8036815471508924,2
http://arxiv.org/abs/2307.02867v1,preprocessed,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),arxiv,arxiv,2023-07-06 00:00:00,arxiv,"towards a safe mlops process for the continuous development and safety
  assurance of ml-based systems in the railway domain",http://arxiv.org/abs/2307.02867v1,"Traditional automation technologies alone are not sufficient to enable
driverless operation of trains (called Grade of Automation (GoA) 4) on
non-restricted infrastructure. The required perception tasks are nowadays
realized using Machine Learning (ML) and thus need to be developed and deployed
reliably and efficiently. One important aspect to achieve this is to use an
MLOps process for tackling improved reproducibility, traceability,
collaboration, and continuous adaptation of a driverless operation to changing
conditions. MLOps mixes ML application development and operation (Ops) and
enables high frequency software releases and continuous innovation based on the
feedback from operations. In this paper, we outline a safe MLOps process for
the continuous development and safety assurance of ML-based systems in the
railway domain. It integrates system engineering, safety assurance, and the ML
life-cycle in a comprehensive workflow. We present the individual stages of the
process and their interactions. Moreover, we describe relevant challenges to
automate the different stages of the safe MLOps process.",included,0.8090653699987075,3
http://arxiv.org/abs/2205.01070v1,preprocessed,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),arxiv,arxiv,2022-04-20 00:00:00,arxiv,five ps: leverage zones towards responsible ai,http://arxiv.org/abs/2205.01070v1,"There is a growing debate amongst academics and practitioners on whether
interventions made, thus far, towards Responsible AI would have been enough to
engage with root causes of AI problems. Failure to effect meaningful changes in
this system could see these initiatives to not reach their potential and lead
to the concept becoming another buzzword for companies to use in their
marketing campaigns. We propose that there is an opportunity to improve the
extent to which interventions are understood to be effective in their
contribution to the change required for Responsible AI. Using the notions of
leverage zones adapted from the 'Systems Thinking' literature, we suggest a
novel approach to evaluate the effectiveness of interventions, to focus on
those that may bring about the real change that is needed. In this paper we
argue that insights from using this perspective demonstrate that the majority
of current initiatives taken by various actors in the field, focus on low-order
interventions, such as short-term fixes, tweaking algorithms and updating
parameters, absent from higher-order interventions, such as redefining the
system's foundational structures that govern those parameters, or challenging
the underlying purpose upon which those structures are built and developed in
the first place(high-leverage). This paper presents a conceptual framework
called the Five Ps to identify interventions towards Responsible AI and
provides a scaffold for transdisciplinary question asking to improve outcomes
towards Responsible AI.",included,0.8183542332228493,4
http://arxiv.org/abs/2003.11816v1,preprocessed,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),arxiv,arxiv,2020-03-26 00:00:00,arxiv,"do deep minds think alike? selective adversarial attacks for
  fine-grained manipulation of multiple deep neural networks",http://arxiv.org/abs/2003.11816v1,"Recent works have demonstrated the existence of {\it adversarial examples}
targeting a single machine learning system. In this paper we ask a simple but
fundamental question of ""selective fooling"": given {\it multiple} machine
learning systems assigned to solve the same classification problem and taking
the same input signal, is it possible to construct a perturbation to the input
signal that manipulates the outputs of these {\it multiple} machine learning
systems {\it simultaneously} in arbitrary pre-defined ways? For example, is it
possible to selectively fool a set of ""enemy"" machine learning systems but does
not fool the other ""friend"" machine learning systems? The answer to this
question depends on the extent to which these different machine learning
systems ""think alike"". We formulate the problem of ""selective fooling"" as a
novel optimization problem, and report on a series of experiments on the MNIST
dataset. Our preliminary findings from these experiments show that it is in
fact very easy to selectively manipulate multiple MNIST classifiers
simultaneously, even when the classifiers are identical in their architectures,
training algorithms and training datasets except for random initialization
during training. This suggests that two nominally equivalent machine learning
systems do not in fact ""think alike"" at all, and opens the possibility for many
novel applications and deeper understandings of the working principles of deep
neural networks.",not included,0.8075019261416267,5
http://arxiv.org/abs/2001.09734v1,preprocessed,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),arxiv,arxiv,2020-01-27 00:00:00,arxiv,"one explanation does not fit all: the promise of interactive
  explanations for machine learning transparency",http://arxiv.org/abs/2001.09734v1,"The need for transparency of predictive systems based on Machine Learning
algorithms arises as a consequence of their ever-increasing proliferation in
the industry. Whenever black-box algorithmic predictions influence human
affairs, the inner workings of these algorithms should be scrutinised and their
decisions explained to the relevant stakeholders, including the system
engineers, the system's operators and the individuals whose case is being
decided. While a variety of interpretability and explainability methods is
available, none of them is a panacea that can satisfy all diverse expectations
and competing objectives that might be required by the parties involved. We
address this challenge in this paper by discussing the promises of Interactive
Machine Learning for improved transparency of black-box systems using the
example of contrastive explanations -- a state-of-the-art approach to
Interpretable Machine Learning.
  Specifically, we show how to personalise counterfactual explanations by
interactively adjusting their conditional statements and extract additional
explanations by asking follow-up ""What if?"" questions. Our experience in
building, deploying and presenting this type of system allowed us to list
desired properties as well as potential limitations, which can be used to guide
the development of interactive explainers. While customising the medium of
interaction, i.e., the user interface comprising of various communication
channels, may give an impression of personalisation, we argue that adjusting
the explanation itself and its content is more important. To this end,
properties such as breadth, scope, context, purpose and target of the
explanation have to be considered, in addition to explicitly informing the
explainee about its limitations and caveats...",included,0.8003887919818654,6
10.1109/iccw.2018.8403664,preprocessed,systems engineering,'systems engineering' AND ('generative ai' OR 'artificial intelligence'),semantic_scholar,2018 IEEE International Conference on Communications Workshops (ICC Workshops),2018-01-01 00:00:00,semantic_scholar,collaborative artificial intelligence (ai) for user-cell association in ultra-dense cellular systems,https://www.semanticscholar.org/paper/558d80a1fc4077090c3ff20d4fff582376fc477f,"In this paper, the problem of cell association between small base stations (SBSs) and users in dense wireless networks is studied using artificial intelligence (AI) techniques. The problem is formulated as a mean-field game in which the users' goal is to maximize their data rate by exploiting local data and the data available at neighboring users via an imitation process. Such a collaborative learning process prevents the users from exchanging their data directly via the cellular network's limited backhaul links and, thus, allows them to improve their cell association policy collaboratively with minimum computing. To solve this problem, a neural Q-learning learning algorithm is proposed that enables the users to predict their reward function using a neural network whose input is the SBSs selected by neighboring users and the local data of the considered user. Simulation results show that the proposed imitation-based mechanism for cell association converges faster to the optimal solution, compared with conventional cell association mechanisms without imitation.",not included,0.815056125907337,7
0fe809899a3c367f02afb271756962302c457718,preprocessed,citation,citation,semantic_scholar,arXiv.org,2024.0,semantic_scholar,envisioning the applications and implications of generative ai for news media,https://www.semanticscholar.org/paper/0fe809899a3c367f02afb271756962302c457718,"This article considers the increasing use of algorithmic decision-support systems and synthetic media in the newsroom, and explores how generative models can help reporters and editors across a range of tasks from the conception of a news story to its distribution. Specifically, we draw from a taxonomy of tasks associated with news production, and discuss where generative models could appropriately support reporters, the journalistic and ethical values that must be preserved within these interactions, and the resulting implications for design contributions in this area in the future. Our essay is relevant to practitioners and researchers as they consider using generative AI systems to support different tasks and workflows.",not included,0.8155604390537038,8
e53412b4f0f15330f8efa84e3c8f649bbe5e26f7,preprocessed,citation,citation,semantic_scholar,arXiv.org,2024.0,semantic_scholar,an empirical categorization of prompting techniques for large language models: a practitioner's guide,https://www.semanticscholar.org/paper/e53412b4f0f15330f8efa84e3c8f649bbe5e26f7,"Due to rapid advancements in the development of Large Language Models (LLMs), programming these models with prompts has recently gained significant attention. However, the sheer number of available prompt engineering techniques creates an overwhelming landscape for practitioners looking to utilize these tools. For the most efficient and effective use of LLMs, it is important to compile a comprehensive list of prompting techniques and establish a standardized, interdisciplinary categorization framework. In this survey, we examine some of the most well-known prompting techniques from both academic and practical viewpoints and classify them into seven distinct categories. We present an overview of each category, aiming to clarify their unique contributions and showcase their practical applications in real-world examples in order to equip fellow practitioners with a structured framework for understanding and categorizing prompting techniques tailored to their specific domains. We believe that this approach will help simplify the complex landscape of prompt engineering and enable more effective utilization of LLMs in various applications. By providing practitioners with a systematic approach to prompt categorization, we aim to assist in navigating the intricacies of effective prompt design for conversational pre-trained LLMs and inspire new possibilities in their respective fields.",not included,0.8036815471508924,9
0547d6069527ab26c9a587d4997b9760aa84811d,preprocessed,citation,citation,semantic_scholar,Journal of Global Information Management,2024.0,semantic_scholar,exploring the potential of large language models in supply chain management: a study using big data,https://www.semanticscholar.org/paper/0547d6069527ab26c9a587d4997b9760aa84811d,"This study aims to identify emerging topics, themes, and potential areas for applying large language models (LLMs) in supply chain management through data triangulation. This study involved the synthesis of 33 published articles and a total of 3421 social media documents, including tweets, posts, expert opinions, and industry reports on utilizing LLMs in supply chain management. By employing BERT models, four core themes were derived: Supply chain optimization, supply chain risk and security management, supply chain knowledge management, and automated contract intelligence, which provides the present status of LLM in the supply chain. The results of this study will empower managers to identify prospective applications and areas for improvement, affording them a comprehensive understanding of the antecedents, decisions, and outcomes detailed in the framework. The insights garnered from this study are highly valuable to both researchers and managers, equipping them to harness the latest advancements in LLM technology and its role within supply chain management.",not included,0.8090653699987075,10
fe06da28b6221bdb267f5a3045f6e8e54a790176,preprocessed,citation,citation,semantic_scholar,arXiv.org,2023.0,semantic_scholar,managing ai risks in an era of rapid progress,https://www.semanticscholar.org/paper/fe06da28b6221bdb267f5a3045f6e8e54a790176,"In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&D and governance.",not included,0.8183542332228493,11
c387a3999113f3f8bcf26681d95cf0f4313f64f4,preprocessed,citation,citation,semantic_scholar,arXiv.org,2023.0,semantic_scholar,towards best practices in agi safety and governance: a survey of expert opinion,https://www.semanticscholar.org/paper/c387a3999113f3f8bcf26681d95cf0f4313f64f4,"A number of leading AI companies, including OpenAI, Google DeepMind, and Anthropic, have the stated goal of building artificial general intelligence (AGI) - AI systems that achieve or exceed human performance across a wide range of cognitive tasks. In pursuing this goal, they may develop and deploy AI systems that pose particularly significant risks. While they have already taken some measures to mitigate these risks, best practices have not yet emerged. To support the identification of best practices, we sent a survey to 92 leading experts from AGI labs, academia, and civil society and received 51 responses. Participants were asked how much they agreed with 50 statements about what AGI labs should do. Our main finding is that participants, on average, agreed with all of them. Many statements received extremely high levels of agreement. For example, 98% of respondents somewhat or strongly agreed that AGI labs should conduct pre-deployment risk assessments, dangerous capabilities evaluations, third-party model audits, safety restrictions on model usage, and red teaming. Ultimately, our list of statements may serve as a helpful foundation for efforts to develop best practices, standards, and regulations for AGI labs.",not included,0.8075019261416267,12
6fd6d54bc19a7daf293f1b929ec3a749a86c6479,preprocessed,citation,citation,semantic_scholar,AI and Ethics,2023.0,semantic_scholar,how to design an ai ethics board,https://www.semanticscholar.org/paper/6fd6d54bc19a7daf293f1b929ec3a749a86c6479,"The development and deployment of artificial intelligence (AI) systems poses significant risks to society. To reduce these risks to an acceptable level, AI companies need an effective risk management process and sound risk governance. In this paper, we explore a particular way in which AI companies can improve their risk governance: by setting up an AI ethics board. We identify five key design choices: (1) What responsibilities should the board have? (2) What should its legal structure be? (3) Who should sit on the board? (4) How should it make decisions? (5) And what resources does it need? We break each of these questions down into more specific sub-questions, list options, and discuss how different design choices affect the board’s ability to reduce societal risks from AI. Several failures have shown that designing an AI ethics board can be challenging. This paper provides a toolbox that can help AI companies to overcome these challenges.",included,0.8003887919818654,13
a3715592161c25474e5394f023660c127da448ed,preprocessed,citation,citation,semantic_scholar,LegalAIIA@ICAIL,2023.0,semantic_scholar,strengthening the ai operating environment: distributed competence as a means to risk mitigation,https://www.semanticscholar.org/paper/a3715592161c25474e5394f023660c127da448ed,"In the rapidly evolving discourse on artificial intelligence (AI), the familiar refrain of “maximizing potential while mitigating risks” has become somewhat of a ubiquitous mantra, emphasizing the need for an effective risk mitigation framework. This paper briefly examines the current state of AI-enabled applications and discusses the various risk containment strategies being implemented. Initial efforts focused on establishing high-level principles for responsible AI use. More recent strategies have sought to operationalize these principles through normative instruments, such as industry best practices and legal statutes, that govern AI applications and their creators. While valuable, such a top-down approach is not sufficiently effective; a complementary, bottom-up approach focused on strengthening the environment in which AI is deployed is also necessary. The paperanalyzestwospecificinitiativesaimedatenhancingthehumancomponentofAIdeployment(creating abetter-informed public through AI benchmarks, creating a better-equipped public with resources for local validation) and offers insights on how this environment-focused track can contribute to risk containment. Furthermore, we suggest additional steps for leveraging this approach in tandem with top-down strategies to cultivate a more robust risk mitigation framework.",included,0.815056125907337,14
2805f2ae5c58a31517562360d17ffa235c8199a4,preprocessed,citation,citation,semantic_scholar,Journal of emerging investigators,2023.0,semantic_scholar,comparing model-centric and data-centric approaches to determine the efficiency of data-centric ai,https://www.semanticscholar.org/paper/2805f2ae5c58a31517562360d17ffa235c8199a4,"In current machine learning approaches, data is crucial, yet it is often overlooked and mishandled in artificial intelligence (AI). As a result, many hours are wasted fine-tuning a model based on faulty data. Hence, there exists a new trend in AI, which is data-centric AI. We hypothesized that data-centric AI would improve the performance of a machine learning model. To test this hypothesis, three models (two model-centric approaches and one data-centric approach) were used. The model-centric approaches included basic data cleaning techniques and focused on the model, while the data-centric approach featured advanced data preparation techniques and basic model-training. We found that the data-centric approach gave a higher accuracy than the model-centric approaches. The model-centric approaches achieved 91% and 90% accuracy, respectively, whereas the data-centric approach achieved 97% accuracy.",not included,0.8155604390537038,15
6bee1df962c23f50efbab5f25764222d00d87cf3,preprocessed,citation,citation,semantic_scholar,International Conference on Ubiquitous and Future Networks,2022.0,semantic_scholar,evaluating correctness of reinforcement learning based on actor-critic algorithm,https://www.semanticscholar.org/paper/6bee1df962c23f50efbab5f25764222d00d87cf3,"Deep learning is used for decision making and functional control in various fields, such as autonomous systems. However, rather than being developed by logical design, deep learning models are trained by itself through learning data. Moreover, only reward values are used to evaluate its performance, which does not provide enough information that the model learned properly. This paper proposes a new method to assess the correctness of reinforcement learning, considering other properties of the learning algorithm. The proposed method is applied for the evaluation of Actor-Critic Algorithms, and correctness-related insights of the algorithm are confirmed through experiments.",not included,0.8155604390537038,16
20a134a2220f742974889149f378010e3baf9d91,preprocessed,citation,citation,semantic_scholar,2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),2022.0,semantic_scholar,quality-driven machine learning-based data science pipeline realization: a software engineering approach,https://www.semanticscholar.org/paper/20a134a2220f742974889149f378010e3baf9d91,"The recently wide adoption of data science approaches to decision making in several application domains (such as health, business and even education) open new challenges in engineering and implementation of this systems. Considering the big picture of data science, Machine learning is the wider used technique and due to its characteristics, we believe that a better engineering methodology and tools are needed to realize innovative data-driven systems able to satisfy the emerging quality attributes (such as, debias and fariness, explainability, privacy and ethics, sustainability). This research project will explore the following three pillars: i) identify key quality attributes, formalize them in the context of data science pipelines and study their relationships; ii) define a new software engineering approach for data-science systems development that assures compliance with quality requirements; iii) implement tools that guide IT professionals and researchers in the realization of ML-based data science pipelines since the requirement engineering. Moreover, in this paper we also presents some details of the project showing how the feature models and model-driven engineering can be leveraged to realize our project.",included,0.8036815471508924,17
5329f48ce81e100081bfac562e603ddd09c3077d,preprocessed,citation,citation,semantic_scholar,IEEE Access,2022.0,semantic_scholar,systematic mapping: artificial intelligence techniques in software engineering,https://www.semanticscholar.org/paper/5329f48ce81e100081bfac562e603ddd09c3077d,"Artificial Intelligence (AI) has become a core feature of today’s real-world applications, making it a trending topic within the software engineering (SE) community. The rise in the availability of AI techniques encompasses the capability to make rapid, automated, impactful decisions and predictions, leading to the adoption of AI techniques in SE. With industry revolution 4.0, the role of software engineering has become critical for developing productive, efficient, and quality software. Thus, there is a major need for AI techniques to be applied to enhance and improve the critical activities within the software engineering phases. Software is developed through intelligent software engineering phases. This paper concerns a systematic mapping study that aimed to characterize the publication landscape of AI techniques in software engineering. Gaps are identified and discussed by mapping these AI techniques against the SE phases to which they contributed. Many systematic mapping review papers have been produced only for a specific AI technique or a specific SE phase or activity. Hence, to our best of knowledge within the last decade, there is no systematic mapping review that has fully explored the overall trends in AI techniques and their application to all SE phases.",not included,0.8090653699987075,18
d26f4154c20992939c5425963638d9fe4ce9f49f,preprocessed,citation,citation,semantic_scholar,arXiv.org,2023.0,semantic_scholar,learning to generate training datasets for robust semantic segmentation,https://www.semanticscholar.org/paper/d26f4154c20992939c5425963638d9fe4ce9f49f,"Semantic segmentation methods have advanced significantly. Still, their robustness to real-world perturbations and object types not seen during training remains a challenge, particularly in safety-critical applications. We propose a novel approach to improve the robustness of semantic segmentation techniques by leveraging the synergy between label-to-image generators and image-to-label segmentation models. Specifically, we design Robusta, a novel robust conditional generative adversarial network to generate realistic and plausible perturbed images that can be used to train reliable segmentation models. We conduct in-depth studies of the proposed generative model, assess the performance and robustness of the downstream segmentation network, and demonstrate that our approach can significantly enhance the robustness in the face of real-world perturbations, distribution shifts, and out-of-distribution samples. Our results suggest that this approach could be valuable in safety-critical applications, where the reliability of perception modules such as semantic segmentation is of utmost importance and comes with a limited computational budget in inference. We release our code at https://github.com/ENSTA-U2IS-AI/robusta.",not included,0.8036815471508924,19
70043a0b612b6253b37df7d363b3bf2ec3d581c7,preprocessed,citation,citation,semantic_scholar,ACM Computing Surveys,2021.0,semantic_scholar,trustworthy ai: from principles to practices,https://www.semanticscholar.org/paper/70043a0b612b6253b37df7d363b3bf2ec3d581c7,"The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people’s trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.",included,0.8183542332228493,20
a952ea94e1e837e065b2b528deacb43ddd55d2bf,preprocessed,citation,citation,semantic_scholar,arXiv.org,2023.0,semantic_scholar,a survey on ai risk assessment frameworks,https://www.semanticscholar.org/paper/a952ea94e1e837e065b2b528deacb43ddd55d2bf,"—The rapid development of artiﬁcial intelligence (AI) has led to increasing concerns about the capability of AI systems to make decisions and behave responsibly. Responsible AI (RAI) refers to the development and use of AI systems that beneﬁt humans, society, and the environment while minimising the risk of negative consequences. To ensure responsible AI, the risks associated with AI systems’ development and use must be identiﬁed, assessed and mitigated. Various AI risk assessment frameworks have been released recently by governments, organisations, and companies. However, it can be challenging for AI stakeholders to have a clear picture of the available frameworks and determine the most suitable ones for a speciﬁc context. Additionally, there is a need to identify areas that require further research or development of new frameworks. To ﬁll the gap, we present a survey of 16 existing RAI risk assessment frameworks from the industry, governments, and non-government organizations (NGOs). We identify key characteristics of each framework and analyse them in terms of RAI principles, stakeholders, system lifecycle stages, geographical locations, targeted domains, and assessment methods. Our study provides a comprehensive analysis of the current state of the frameworks and highlights areas of convergence and divergence among them. We also identify the deﬁciencies in existing frameworks and outlines the essential characteristics a concrete framework should possess. Our ﬁndings and insights can help relevant stakeholders choose suitable RAI risk assessment frameworks and guide the design of future frameworks towards concreteness.",not included,0.8155604390537038,21
88b0df12ca5075e0bb7b28ed0e9b7a55ad6a0463,preprocessed,citation,citation,semantic_scholar,,2023.0,semantic_scholar,a systematic mapping study on responsible ai risk assessment,https://www.semanticscholar.org/paper/88b0df12ca5075e0bb7b28ed0e9b7a55ad6a0463,"—The rapid development of artiﬁcial intelligence (AI) has led to increasing concerns about the capability of AI systems to make decisions and behave responsibly. Responsible AI (RAI) refers to the development and use of AI systems that beneﬁt humans, society, and the environment while minimising the risk of negative consequences. To ensure responsible AI, the risks associated with AI systems’ development and use must be identiﬁed, assessed and mitigated. Various AI risk assessment frameworks have been released recently by governments, organisations, and companies. However, it can be challenging for AI stakeholders to have a clear picture of the available frameworks and determine the most suitable ones for a speciﬁc context. Additionally, there is a need to identify areas that require further research or development of new frameworks. To ﬁll the gap, we present a mapping study of 16 existing RAI risk assessment frameworks from the industry, governments, and non-government organizations (NGOs). We identify key characteristics of each framework and analyse them in terms of RAI principles, stakeholders, system lifecycle stages, geographical locations, targeted domains, and assessment methods. Our study provides a comprehensive analysis of the current state of the frameworks and highlights areas of convergence and divergence among them. We also identify the deﬁciencies in existing frameworks and outlines the essential characteristics a concrete framework should possess. Our ﬁndings and insights can help relevant stakeholders choose suitable RAI risk assessment frameworks and guide the design of future frameworks towards concreteness.",not included,0.8036815471508924,22
