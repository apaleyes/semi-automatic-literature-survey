doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database
10.1109/MSM49833.2020.9201644,A Preliminary Investigation of an Autonomous Vehicle Validation Infrastructure for Smart Cities,IEEE,Conferences,"The research and development of autonomous vehicle has entered the era of commercialization. While the vehicle self-driving technology has been growing rapidly, the validation for autonomous vehicle in terms of driving model, human factor model and traffic model is still maturing. Most of previous infrastructures are mainly focused on validation of those three models separately resorting either on real driving test at physical infrastructure or software simulation in virtualized infrastructure. However, neither the real driving test can cover all possible scenarios of autonomous driving and human factors, nor the virtualized software simulation can generate a feasible model for practical on/off-road driving. Furthermore, future autonomous transport in smart cities requires comprehensive validation. In order for autonomous vehicles to meet the autonomous transport in such complex traffic environment, an integrated testing and simulation infrastructure has been built targeting the systematic validation for autonomous vehicles: the Multi-User Environment for Autonomous Vehicle Innovation (MUEAVI). A preliminary investigation of a new autonomous vehicle validation infrastructure that can serve a multitude of research projects for smart city is presented.",https://ieeexplore.ieee.org/document/9201644/,2020 International Conference Mechatronic Systems and Materials (MSM),1-3 July 2020,ieeexplore
10.1109/ROBOT.1986.1087458,An architecture for reflexive autonomous vehicle control,IEEE,Conferences,"We describe a software architecture to support the planning and control requirements of an autonomous land vehicle. This architecture is designed specifically to handle diverse terrain with maximal speed, efficacy and versatility through the use of a library of reflexive strategies specialized to particular needs. A hierarchy of control is built in which lower level modules perform tasks requiring greatest immediacy while higher level modules perform tasks involving greater assimilation of sensor data. With all levels of the hierarchy operating in parallel, higher level modules perform selective activation and deactivation of lower level responses with limited interference from demands for immediacy, allowing the system to exploit the advantages of both immediate and assimilated data. The reflexive component of this system has been demonstrated both in a detailed real-time simulation and on a small indoor robotic vehicle.",https://ieeexplore.ieee.org/document/1087458/,Proceedings. 1986 IEEE International Conference on Robotics and Automation,7-10 April 1986,ieeexplore
10.1109/IVS.2018.8500461,Autonomous Vehicle Testing and Validation Platform: Integrated Simulation System with Hardware in the Loop*,IEEE,Conferences,"With the development of autonomous driving, offline testing remains an important process allowing low-cost and efficient validation of vehicle performance and vehicle control algorithms in multiple virtual scenarios. This paper aims to propose a novel simulation platform with hardware in the loop (HIL). This platform comprises of four layers: the vehicle simulation layer, the virtual sensors layer, the virtual environment layer and the Electronic Control Unit (ECU) layer for hardware control. Our platform has attained multiple capabilities: (1) it enables the construction and simulation of kinematic car models, various sensors and virtual testing fields; (2) it performs a closed-loop evaluation of scene perception, path planning, decision-making and vehicle control algorithms, whilst also having multi-agent interaction system; (3) it further enables rapid migrations of control and decision-making algorithms from the virtual environment to real self-driving cars. In order to verify the effectiveness of our simulation platform, several experiments have been performed with self-defined car models in virtual scenarios of a public road and an open parking lot and the results are substantial.",https://ieeexplore.ieee.org/document/8500461/,2018 IEEE Intelligent Vehicles Symposium (IV),26-30 June 2018,ieeexplore
10.1109/CMPEUR.1992.218441,Developments in autonomous vehicle navigation,IEEE,Conferences,An approach to autonomous vehicle navigation using neural networks to imitate human driving behavior is presented. The results are based on the measured actions of a real human driver in a real car. The car environment and the information that is recorded during driving are described. The problem that should be solved by the neural network is discussed. The actual learning of the driving task is presented. The quality of the leaned driving behavior is reviewed. The experiments showed that medium-sized neural networks were able to approximate the driving behavior within a maximum error of 5%.&lt;<ETX>&gt;</ETX>,https://ieeexplore.ieee.org/document/218441/,CompEuro 1992 Proceedings Computer Systems and Software Engineering,4-8 May 1992,ieeexplore
10.1109/AICAS51828.2021.9458488,Evaluation of Machine Learning-based Detection against Side-Channel Attacks on Autonomous Vehicle,IEEE,Conferences,"Autonomous vehicles are becoming increasingly popular, but their reliance on computer systems to sense and operate in the physical world has introduced new security risks. Recent studies have shown that using Cache-based Side-Channel Attacks (SCAs) could infer sensitive users' information (e.g., which route the user is taking) highlighting significant vulnerability posed to today's computer systems. As a result, it is crucial to propose effective detection mechanisms against emerging microarchitectural SCAs on autonomous driving systems. In response, we first identify the threat model and victim applications of autonomous driving systems in this work. Next, we explore the suitability of various machine learning-based classifiers trained by information collected from built-in hardware performance counter registers available in modern autonomous vehicle systems. To this end, various supervised machine learning models are implemented for cache-based SCAs detection and precisely compared and characterized in terms of detection accuracy, robustness, and latency of the detection. Our experiments conducted on an Intel Xeon, which Waymo autonomous driving vendor uses, demonstrate that J48 achieves 99.5% accuracy with the highest efficiency compared with other investigated models.",https://ieeexplore.ieee.org/document/9458488/,2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),6-9 June 2021,ieeexplore
10.1109/VTCFall.2017.8288318,Impact to Longitude Velocity Control of Autonomous Vehicle from Human Driver's Distraction Behavior,IEEE,Conferences,"Driver distraction behaviors are usually blind to autonomous vehicles (AVs), leading to probable late preparation for AVs to take emergency measures. Hence, this paper aims to build a bridge between AV control and driver behavior detection, to assist AVs to predict the potential risk and avoid abnormal drivers carefully like experienced drivers. Our main contributions of this paper consist: i) put forward a practicable system framework integrating driver distraction monitoring, vehicle-to-vehicle communication and AV velocity control; ii) provide a real-time driver distraction monitoring implementation building on convolutional neural network trained offline; iii) propose a method of longitude velocity control of AV considering the risk of driver distraction behavior based on model predictive control strategy. Simulation results validate the effectiveness of our work.",https://ieeexplore.ieee.org/document/8288318/,2017 IEEE 86th Vehicular Technology Conference (VTC-Fall),24-27 Sept. 2017,ieeexplore
10.1109/ICMLC.2010.71,Microcontroller Based Neural Network Controlled Low Cost Autonomous Vehicle,IEEE,Conferences,"In this paper, design of a low cost autonomous vehicle based on neural network for navigation in unknown environments is presented. The vehicle is equipped with four ultrasonic sensors for hurdle distance measurement, a wheel encoder for measuring distance traveled, a compass for heading information, a GPS receiver for goal position information, a GSM modem for changing destination place on run time and a nonvolatile RAM for storing waypoint data; all interfaced to a low cost AT89C52 microcontroller. The microcontroller processes the information acquired from the sensors and generates robot motion commands accordingly through neural network. The neural network running inside the microcontroller is a multilayer feed-forward network with back-propagation training algorithm. The network is trained offline with tangent-sigmoid as activation function for neurons and is implemented in real time with piecewise linear approximation of tangent-sigmoid function. Results have shown that upto twenty neurons can be implemented in hidden layer with this technique. The vehicle is tested with varying destination places in outdoor environments containing stationary as well as moving obstacles and is found to reach the set targets successfully.",https://ieeexplore.ieee.org/document/5460762/,2010 Second International Conference on Machine Learning and Computing,9-11 Feb. 2010,ieeexplore
10.1109/URAI.2016.7734068,Monocular vision-based object recognition for autonomous vehicle driving in a real driving environment,IEEE,Conferences,"Nowadays, many attentions have been devoted to autonomous vehicles because the automation of driving technology has a large number of benefits, such as the minimization of risks, the improvement of mobility and ease of drivers. Among many technologies for autonomous driving, road environmental recognition is one of the key issues. In this paper, we present the test results of various object detection algorithms using single monocular camera for autonomous vehicle in real driving conditions. The vision recognition system tested in this paper has three main recognition parts: pedestrian detection, traffic sign and traffic light recognition. We use Histogram of Gradients (HOG) features and detect the pedestrians by Support Vector Machine (SVM). Also features of traffic signs are extracted by Principal Components Analysis (PCA) and canny edge detection is used for traffic lights. These two signals are classified by Neural Network (NN). Algorithms that we tested are implemented in General-Purpose computing on Graphics Processing Units (GPGPU). We show the effectiveness of these methods in real-time applications for autonomous driving.",https://ieeexplore.ieee.org/document/7734068/,2016 13th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI),19-22 Aug. 2016,ieeexplore
10.1109/ICICCS51141.2021.9432186,Pothole and Object Detection for an Autonomous Vehicle Using YOLO,IEEE,Conferences,"Object Detection is an key software and a fundamental task for an autonomous driving system that provides remarkable change in computer vision. İn recent years, company's are planning to launch autonomous vehicle in an full swing that's the most important ascpets for object detection and one of most challenging task for locating specific object from from multiple objects in a specific scenario. The computer vision and machine learning algorithm is the important tool for detecting objects in and around the environment. In this paper, which consists of two parts The first part is implemented on object detection in the surrounding with Yolo (You Only Look Once)Algorithm provides exact classification and position which is configured on newly created datasets for classes of object: a car, a person, a truck, a bus, traffic light, motorcycle, pothole, wetland uses the Convolutional Neural Network and max-polling layer for prediction that improves detecting of small target and these deep learning technique provides a high accuracy for detecting real world. Detecting potholes in Indian road help the autonomous vehicle to move smoothly without getting struck in the potholes. In part two of the proposed method is implemented on Raspberry pi4 a popular embedded computer board explores suitability for the running objects. That solves the real world problems and improves the impact on detecting objects. Knowing pothole and wetland detection for self-driving vehicle is needed badly to solve the road lay problems like: accident, slowing down the transport system these are solved by deep learning.",https://ieeexplore.ieee.org/document/9432186/,2021 5th International Conference on Intelligent Computing and Control Systems (ICICCS),6-8 May 2021,ieeexplore
10.1109/ICC42927.2021.9500318,Reinforcement Learning for Autonomous Vehicle Movements in Wireless Sensor Networks,IEEE,Conferences,"In this work we use autonomous vehicles to improve the performance of Wireless Sensor Networks (WSNs). In contrast to other autonomous vehicle applications, WSNs have two metrics for performance evaluation. First, quality of information (QoI) which is used to measure the quality of sensed data (e.g., measurement uncertainties or signal strength). Second, quality of service (QoS) which is used to measure the network’s performance for data forwarding (e.g., delay and packet losses). As a use case, we consider wireless acoustic sensor networks, where a group of speakers move inside a room and there are autonomous vehicles installed with microphones for streaming the audio data. We formulate the problem as a Markov decision problem (MDP) and solve it using Deep-Q-Networks (DQN). Additionally, we compare the performance of DQN solution to two different real-world implementations: speakers holding/passing microphones and microphones being preinstalled in fixed positions.We show using simulations that the performance of autonomous vehicles in terms of QoI and QoS is better than the real-world implementation in some scenarios. Moreover, we study the impact of the vehicles speed on the learning process of the DQN solution and show how low speeds degrade the performance. Finally, we compare the DQN solution to a heuristic one and provide theoretical analysis of the performance with respect to dynamic WSNs.",https://ieeexplore.ieee.org/document/9500318/,ICC 2021 - IEEE International Conference on Communications,14-23 June 2021,ieeexplore
10.1109/ISORC.2018.00025,Representative Safety Assessment of Autonomous Vehicle for Public Transportation,IEEE,Conferences,"The implementations and testing in real conditions of Autonomous Vehicles (AV) for private usage show important advances. However, a lack still exists in addressing the particularities of AVs for Public Transportation. Such particularities range from limited safety mechanisms aboard, risky situations associated to particular users and complex self-driving situations up to the limited passengers-vehicle interactions possible. Since, to our knowledge, no comprehensive safety assessment actually exists and the current automotive related standards do not address identified aspects, in this paper, we propose to conduct a minimal but representative safety assessment based upon a local but real autonomous vehicle implementation. To conduct our study, the Hazard Analysis and Risks Assessment introduced in the ISO 26262 standard is taken as a basis. Initial outcomes suggest that critical autonomy aspects, like machine learning of complex operational situations, the metrics for quantitative assessment of autonomy, and potential conflicts between autonomy principles and external safety fences can have critical safety impacts and demand further discussions.",https://ieeexplore.ieee.org/document/8421156/,2018 IEEE 21st International Symposium on Real-Time Distributed Computing (ISORC),29-31 May 2018,ieeexplore
,Road following for autonomous vehicle navigation using a concurrent neural classifier,IEEE,Conferences,"The paper presents an original approach for visual identification of road direction of an autonomous vehicle using a neural network classifier called Concurrent Self-Organizing Maps (CSOM), representing a winner-takes-all collection of neural modules. We present the experimental results obtained by computer simulation of our model. The path to be identified has been quantized in 5 output directions. For training and testing the neural model, we captured and labeled a road image data set which has been divided in two lots: 30 images for training and other 30 images for test. We have also performed, trained and tested a real time neural path follower based on CSOM model, implemented on a mobile robot (car toy).",https://ieeexplore.ieee.org/document/4699060/,2008 World Automation Congress,28 Sept.-2 Oct. 2008,ieeexplore
10.1109/ACMI53878.2021.9528185,Speed Bump &amp; Pothole Detection with Single Shot MultiBox Detector Algorithm &amp; Speed Control for Autonomous Vehicle,IEEE,Conferences,"The development of self-driving cars has always been an extensive research field for the automobile industry. To make a capable self-driving car, many challenges need to be resolved. Detection of the road condition is one of them. This paper focuses on a particular part-detection of speed bumps and potholes using a camera and analyzing the video feed with the help of artificial intelligence. To solve this problem a popular and lightweight algorithm, SSD (Single Shot Multibox Detector) is used. This is an optimal choice because of being lightweight and also accurate enough to run on mobile devices and to use in real-life situations. For detecting speed bumps and potholes, a dataset has been created based on the road structure of Bangladesh as the main priority of this system is to work on the local environment. Raspberry Pi has been used as the main processing unit because of being small but powerful. A warning system has been implemented so that it can warn the onboard driver about the upcoming pothole or speed bump. This system can also send a signal to the speed controller unit of the car to reduce the speed on detection to avoid accidents or damages to the car. The speed control unit is a microcontroller-based system that uses an ATmega328 microcontroller and L298 motor driver. This paper summarizes the combination of an artificial intelligence-based detection system injunction with a microcontroller-based speed control system in a cost-effective way that can be used in building self-driving cars.",https://ieeexplore.ieee.org/document/9528185/,"2021 International Conference on Automation, Control and Mechatronics for Industry 4.0 (ACMI)",8-9 July 2021,ieeexplore
10.1109/TCSII.2020.3011367,An Extensive Soft Error Reliability Analysis of a Real Autonomous Vehicle Software Stack,IEEE,Journals,"Automotive systems are integrating artificial intelligence and complex software stacks aiming to interpret the real world, make decisions, and perform actions without human input. The occurrence of soft errors in such systems can lead to wrong decisions, which might ultimately incur in life losses. This brief focuses on the soft error susceptibility assessment of a real automotive application running on top of unmodified Linux kernels, and considering two commercially available processors, and three cross-compilers. Results collected from more than 29 thousand simulation hours show that the occurrence of faults in critical functions may cause 2.16× more failures on the system.",https://ieeexplore.ieee.org/document/9146326/,IEEE Transactions on Circuits and Systems II: Express Briefs,Jan. 2021,ieeexplore
10.1109/ACCESS.2021.3125620,Autonomous Vehicle Evaluation: A Comprehensive Survey on Modeling and Simulation Approaches,IEEE,Journals,"In recent years, autonomous vehicles (AVs), which observe the driving environment and lead a few or all of the driving tasks, have garnered tremendous success. The field of AVs has been rapidly developing and has found many applications. As a safety requirement established by policymakers, these vehicles must be evaluated before their deployment. The evaluation process for AVs is challenging because crashes are rare events, and AVs can escape passing predefined test scenarios. Therefore, capturing crashes and creating real test scenarios should be considered in order to develop an evaluation approach that represents real-world scenarios. One evaluation approach is based on the naturalistic field operational test (N-FOT), in which prototype AVs are driven on roads by volunteers or test engineers. Unfortunately, this approach is time-consuming and costly because thousands of miles need to be driven to experience a police-reported collision and nearly millions of miles for a fatal crash. Another approach is the accelerated evaluation method. The core idea of the accelerated evaluation approach is to modify the statistics of naturalistic driving so that safety-critical events are emphasized. This paper presents a brief survey of the advances that have occurred in the area of the evaluation of partially or fully autonomous vehicles, starting with naturalistic field operational tests (N-FOTs). The review covers the test matrix evaluation, worst-case scenario evaluation (WCSE), Monte Carlo simulations, and accelerated evaluation (AE). We also present all the simulation-based and agent-based modeling approaches that do not follow any evaluation protocol listed above. This study provides a scientific analysis of each evaluation techniques, focusing on their advantages/disadvantages, inherent restrictions, practicability, and optimality. The results reveal that the accelerated evaluation approach outperforms naturalistic field operational tests (N-FOTs), test matrix evaluation, worst-case scenario evaluation (WCSE), and Monte Carlo simulation methods in some of the car-following and lane-change studies when using specific models. Moreover, the agent-based model and augmented and virtual reality approaches show promising results in AV evaluation. Furthermore, integrating machine and deep learning into the available AV evaluation methods can improve their performance and generate encouraging outcomes.",https://ieeexplore.ieee.org/document/9605690/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.2982963,Trajectory Tracking Control Algorithm for Autonomous Vehicle Considering Cornering Characteristics,IEEE,Journals,"Trajectory tracking control is a key technology in the research and development of autonomous vehicles. With the aim of addressing problems such as low control accuracy and poor real-time performance, which can occur easily when an autonomous vehicle avoids obstacles, this research focuses on the trajectory tracking control algorithm for autonomous vehicle considering cornering characteristics. First, the vehicle dynamics model and tire model are established through appropriate simplification. Then, based on the basic principle of model predictive control, a linear time-varying model predictive controller (LTV MPC) that considers the cornering characteristics is designed and optimized. Finally, using CarSim and MATLAB/Simulink software, a joint simulation model is established and the trajectory tracking performance of the controlled vehicle under different vehicle speeds and road adhesion conditions are tested through simulation experiments in combination with the double-shift line reference trajectory. The simulation results show the LTV MPC controller that considers cornering characteristics has good self-adaptability under complicated and severe working conditions, and no cases, such as car sideslip or track departure, were observed. Compared with other controllers and algorithms, the designed trajectory tracking controller has remarkable comprehensive performance, exhibits superior robustness and anti-interference ability, and significant improvements in the trajectory tracking control accuracy and real-time performance. The proposed control algorithm is of great importance in improving the tracking stability and driving safety of autonomous vehicles under complex extreme conditions and conducive to the further development and improvement of the technological level of intelligent vehicle driving assistance.",https://ieeexplore.ieee.org/document/9045954/,IEEE Access,2020,ieeexplore
10.1109/ISSCC.2009.4977352,A 201.4GOPS 496mW real-time multi-object recognition processor with bio-inspired neural perception engine,IEEE,Conferences,"The visual attention mechanism, which is the way humans perform object recognition, was applied to the implementation of a high performance object recognition chip. Even though the previous chip achieved 50% gain of computational cost, it could recognize only one object in a frame so that it is not suitable for advanced multi-object recognition applications such as video surveillance, intelligent robots, and autonomous vehicle",https://ieeexplore.ieee.org/document/4977352/,2009 IEEE International Solid-State Circuits Conference - Digest of Technical Papers,8-12 Feb. 2009,ieeexplore
10.1109/MSM49833.2020.9201644,A Preliminary Investigation of an Autonomous Vehicle Validation Infrastructure for Smart Cities,IEEE,Conferences,"The research and development of autonomous vehicle has entered the era of commercialization. While the vehicle self-driving technology has been growing rapidly, the validation for autonomous vehicle in terms of driving model, human factor model and traffic model is still maturing. Most of previous infrastructures are mainly focused on validation of those three models separately resorting either on real driving test at physical infrastructure or software simulation in virtualized infrastructure. However, neither the real driving test can cover all possible scenarios of autonomous driving and human factors, nor the virtualized software simulation can generate a feasible model for practical on/off-road driving. Furthermore, future autonomous transport in smart cities requires comprehensive validation. In order for autonomous vehicles to meet the autonomous transport in such complex traffic environment, an integrated testing and simulation infrastructure has been built targeting the systematic validation for autonomous vehicles: the Multi-User Environment for Autonomous Vehicle Innovation (MUEAVI). A preliminary investigation of a new autonomous vehicle validation infrastructure that can serve a multitude of research projects for smart city is presented.",https://ieeexplore.ieee.org/document/9201644/,2020 International Conference Mechatronic Systems and Materials (MSM),1-3 July 2020,ieeexplore
10.1109/UUST.1989.754727,A Testbed Processor for Embedded Multi-Computing,IEEE,Conferences,"A 16-node array of transputers is being installed in an undersea electronics bottle. A passive backplane IBM-AT compatible processor was previously configured and is the host for the array. The Experimental Autonomous Vehicle - West (EAVE-West) and Free Swimming Mine Neutralization Vehicle (FSMNV) will both use this new processor. The array will provide extended capability for future versions of these systems and their follow-on efforts. Also, the testbed processor is expected to provide valuable insights concerning undersea application of embeddable multi-computing.",https://ieeexplore.ieee.org/document/754727/,"Proceedings of the 6th International Symposium on Unmanned Untethered Submersible Technology,",12-14 June 1989,ieeexplore
10.1109/IROS.1997.649083,A mobile robot for service use: behaviour simulation system and intelligent control,IEEE,Conferences,"The structure of hardware and software of AI control system of a mobile robot for service use are described. Hardware of the mobile robot described include an autonomous wheel vehicle and a five degree of freedom manipulator. The software of the AI control system is based on soft computing including fuzzy control rules, fuzzy neural network and genetic algorithms. The intelligent control of cooperative motion between the autonomous vehicle and manipulator realises flexible operations such as navigation of a mobile robot in presence of static and dynamic obstacles, processes of opening door in rooms and pushing buttons of an elevator. New hierarchical structure of the AI control system includes direct human-robot communication line based on natural language and cognitive graphics, and a generator of virtual reality for simulation of artificial life conditions for the mobile service robot. Simulation and experimental results of navigation and technical operations with the manipulator mobile service robot used in office building are described.",https://ieeexplore.ieee.org/document/649083/,Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97,11-11 Sept. 1997,ieeexplore
10.1109/ICVES.2016.7548165,A new hopfield-type neural network approach to multi-goal vehicle navigation in unknown environments,IEEE,Conferences,"A Hopfield-type neural networks (HNN) algorithm associated with histogram navigation method is proposed in this paper for real-time map building and path planning for multiple goals applications. In real world applications such as rescue robots, service robots, mining mobile robots, and mine searching robots, etc., an autonomous vehicle needs to reach multiple goals with a shortest path that, in this paper, is capable of being implemented by a HNN method with minimized overall distance. Once a global trajectory is planned, a foraging-enabled trail is created to guide the vehicle to the multiple goals. A histogram-based local navigation algorithm is employed to plan a collision-free path along the trail planned by the global path planner. A re-planning-based algorithm aims to generate trajectory while an autonomous vehicle explores through a terrain with map building in unknown environments. In this paper, simulation and experimental results demonstrate that the real-time concurrent mapping and multi-goal navigation of an autonomous vehicle is successfully performed under unknown environments.",https://ieeexplore.ieee.org/document/7548165/,2016 IEEE International Conference on Vehicular Electronics and Safety (ICVES),10-12 July 2016,ieeexplore
,A robust control of mobile robot based on sonar sensors,IEEE,Conferences,"This paper describes the design and real implementation of wall following and fuzzy perception concept with a non-holonomic mobile robot named KHAN-Robo. The main focus of this paper is obtaining a fuzzy perception of the environment in the design of each reactive behavior and solving the problem of behavior combination to implement a fuzzy behavior based control architecture. It should be remarked that, the proposed technique of the nonholonomic constraints are considered in the design of each behavior. Furthermore, in order to improve the capabilities of the intelligent control system and its practical applicability, teleoperation and planned behaviors, together with their combination with reactive ones, have been considered. Experimental results, of an application to control the KHAN-Robo autonomous vehicle, demonstrate the robustness of the proposed method.",https://ieeexplore.ieee.org/document/6106329/,"2011 11th International Conference on Control, Automation and Systems",26-29 Oct. 2011,ieeexplore
10.1109/ISI49825.2020.9280513,A virtual simulation environment using deep learning for autonomous vehicles obstacle avoidance,IEEE,Conferences,"Autonomous vehicles which are capable of operating independently will be commercially available in the near future. Autonomous driving systems are becoming more complicated and must be successfully checked before implementation. Within this framework, falls our research work. The key purpose of this paper is to implement a simulation environment for autonomous vehicles. We first created this environment which is a novel high fidelity driving simulator that can connect arbitrary interfaces, build simulated worlds consisting of scenarios and incidents experienced by drivers in real-world driving, and incorporate fully autonomous driving. The simulator makes possible to clone the behavior of human driver face as well as some complex situations such as obstacle avoidance maneuvers. The work consists in creating a virtual simulation environment to collect training data used to train vehicles on how to steer themselves. The simulator is, thus, like a video game of car racing. Indeed we used the scenes to make some driving experiences. After collecting the training data, we chose to use deep learning explicitly Convolutional Neural Networks to create a model for autonomous vehicles that avoid obstacles. Clearly, the true challenge for an autonomous vehicle is to navigate without the possibility of collision. This simulator is invested to assess the performance of an autonomous vehicle and to analyze its self-driving activities. In this method, the suggested solution proves to be feasible, efficient and reliable for autonomous vehicle simulation research.",https://ieeexplore.ieee.org/document/9280513/,2020 IEEE International Conference on Intelligence and Security Informatics (ISI),9-10 Nov. 2020,ieeexplore
10.1109/EEEIC/ICPSEurope49358.2020.9160705,ASDVC - A Self-Driving Vehicle Controller using Unsupervised Machine Learning,IEEE,Conferences,"ASDVC is a self-driving vehicle controller that uses unsupervised machine learning methods, namely clustering based k-means, hierarchical, Gaussian Matrix Model and self-organizing mapping to optimize the path the vehicle follows from the source to destination. The real-time optimal selection of the unsupervised machine learning based motion control algorithm could provide fast response times of under one microsecond during the lateral, longitudinal and angular motion control of the autonomous vehicle. However, it is shown that a simple selection of one of the machine learning methods may not guarantee the optimality of the results. The successful implementation of ASDVC controller in self-driving vehicles could have a significant contribution towards making mobility more reliable and sustainable for the future vehicular transportation systems.",https://ieeexplore.ieee.org/document/9160705/,2020 IEEE International Conference on Environment and Electrical Engineering and 2020 IEEE Industrial and Commercial Power Systems Europe (EEEIC / I&CPS Europe),9-12 June 2020,ieeexplore
10.1109/DSN-W.2018.00027,AVFI: Fault Injection for Autonomous Vehicles,IEEE,Conferences,"Autonomous vehicle (AV) technology is rapidly becoming a reality on U.S. roads, offering the promise of improvements in traffic management, safety, and the comfort and efficiency of vehicular travel. With this increasing popularity and ubiquitous deployment, resilience has become a critical requirement for public acceptance and adoption. Recent studies into the resilience of AVs have shown that though the AV systems are improving over time, they have not reached human levels of automation. Prior work in this area has studied the safety and resilience of individual components of the AV system (e.g., testing of neural networks powering the perception function). However, methods for holistic end-to-end resilience assessment of AV systems are still non-existent.",https://ieeexplore.ieee.org/document/8416212/,2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W),25-28 June 2018,ieeexplore
10.1109/ICRA40945.2020.9197024,Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving,IEEE,Conferences,In the autonomous driving area synthetic data is crucial for cover specific traffic scenarios which autonomous vehicle must handle. This data commonly introduces domain gap between synthetic and real domains. In this paper we deploy data augmentation to generate custom traffic scenarios with VRUs in order to improve pedestrian recognition. We provide a pipeline for augmentation of the Cityscapes dataset with virtual pedestrians. In order to improve augmentation realism of the pipeline we reveal a novel generative network architecture for adversarial learning of the data-set lighting conditions. We also evaluate our approach on the tasks of semantic and instance segmentation.,https://ieeexplore.ieee.org/document/9197024/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore
10.1109/IROS40897.2019.8967743,Agent Prioritization for Autonomous Navigation,IEEE,Conferences,"In autonomous navigation, a planning system reasons about other agents to plan a safe and plausible trajectory. Before planning starts, agents are typically processed with computationally intensive models for recognition, tracking, motion estimation and prediction. With limited computational resources and a large number of agents to process in real time, it becomes important to efficiently rank agents according to their impact on the decision making process. This allows spending more time processing the most important agents. We propose a system to rank agents around an autonomous vehicle (AV) in real time. We automatically generate a ranking data set by running the planner in simulation on real-world logged data, where we can afford to run more accurate and expensive models on all the agents. The causes of various planner actions are logged and used for assigning ground truth importance scores. The generated data set can be used to learn ranking models. In particular, we show the utility of combining learned features, via a convolutional neural network, with engineered features designed to capture domain knowledge. We show the benefits of various design choices experimentally. When tested on real AVs, our system demonstrates the capability of understanding complex driving situations.",https://ieeexplore.ieee.org/document/8967743/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore
10.1109/ROBOT.1986.1087518,Architecture and early experience with planning for the ALV,IEEE,Conferences,"This paper describes the software architecture and the initial algorithms that have proved to be effective for a real time robot planning system. The architecture is designed to incorporate planning technology from research on artificial intelligence while at the same time supporting the high performance decision making needed to control a fast-moving autonomous vehicle. The symbolic representation of the vehicle's plan is a key element in this architecture. Our initial algorithms use an especially efficient version of dynamic programming to find the best routes. The route is then translated into a symbolic plan. Replanning happens at several levels with the cost of replanning proportionate to the scope of the changes. This software is currently running in an environment which simulates the vehicle and perception systems, but it will be transferred to the DARPA Autonomous Land Vehicle built by Martin Marietta Denver Aerospace [Lowrie 86].",https://ieeexplore.ieee.org/document/1087518/,Proceedings. 1986 IEEE International Conference on Robotics and Automation,7-10 April 1986,ieeexplore
10.1109/VTCSpring.2019.8746507,Autonomous Driving without a Burden: View from Outside with Elevated LiDAR,IEEE,Conferences,"The current autonomous driving architecture places a heavy burden in signal processing for the graphics processing units (GPUs) in the car. This directly translates into battery drain and lower energy efficiency, crucial factors in electric vehicles. This is due to the high bit rate of the captured video and other sensing inputs, mainly due to Light Detection and Ranging (LiDAR) sensor at the top of the car which is an essential feature in autonomous vehicles. LiDAR is needed to obtain a high precision map for the vehicle AI to make relevant decisions. However, this is still a quite restricted view from the car. This is the same even in the case of cars without a LiDAR such as Tesla. The existing LiDARs and the cameras have limited horizontal and vertical fields of visions. In all cases it can be argued that precision is lower, given the smaller map generated. This also results in the accumulation of a large amount of data in the order of several TBs in a day, the storage of which becomes challenging. If we are to reduce the effort for the processing units inside the car, we need to uplink the data to edge or an appropriately placed cloud. However, the required data rates in the order of several Gbps are difficult to be met even with the advent of 5G. Therefore, we propose to have a coordinated set of LiDAR's outside at an elevation which can provide an integrated view with a much larger field of vision (FoV) to a centralized decision making body which then sends the required control actions to the vehicles with a lower bit rate in the downlink and with the required latency. The calculations we have based on industry standard equipment from several manufacturers show that this is not just a concept but a feasible system which can be implemented.The proposed system can play a supportive role with existing autonomous vehicle architecture and it is easily applicable in an urban area.",https://ieeexplore.ieee.org/document/8746507/,2019 IEEE 89th Vehicular Technology Conference (VTC2019-Spring),28 April-1 May 2019,ieeexplore
10.1109/ZINC.2019.8769392,Classification of Objects Detected by the Camera based on Convolutional Neural Network,IEEE,Conferences,"Nowadays, we are trying to achieve as much vehicle autonomy as possible by developing Advanced Driver-Assistance Systems (ADAS). For such a system to make decisions, it should have insight into the environment of the vehicle, e.g. the objects surrounding the vehicle. During forward driving, the information about the objects in front of the vehicle is usually obtained by a front view in-vehicle camera. This paper describes the image classification method of the objects in the front of the vehicle based on deep convolutional neural networks (CNN). Such CNN is supposed to be implemented in embedded system of an autonomous vehicle and the inference should satisfy real-time constraints. This means that the CNN should be structured to have fast inference by reducing the number of operations as much as possible, but still having satisfying accuracy. This can be achieved by reducing the number of parameters which also means that the resulting network has lower memory requirements. This paper describes the process of realizing such a network, from image dataset development up to the CNN structuring and training. The proposed CNN is compared to the state-of-the-art deep neural network in terms of classification accuracy, inference speed and memory requirements.",https://ieeexplore.ieee.org/document/8769392/,2019 Zooming Innovation in Consumer Technologies Conference (ZINC),29-30 May 2019,ieeexplore
10.1109/ITSC48978.2021.9564566,Continual Unsupervised Domain Adaptation for Semantic Segmentation by Online Frequency Domain Style Transfer,IEEE,Conferences,"When deep neural networks are deployed in a highly automated vehicle for environment perception tasks in an unseen (target) domain that differs from the training (source) domain, the mismatch will result in decreased performance. Domain adaptation methods aim at overcoming this mismatch. Many recently investigated methods for unsupervised domain adaptation train a model using labeled source data and unlabeled target data at the same time. These methods assume that data from the target domain is available during the source domain training, which is not always the case in real applications. In this paper we present a way to perform an online style transfer for continual domain adaptation which improves performance on (multiple) unseen target domains using a given perception model. The approach is based on an image style transfer in the frequency domain and requires neither an adjustment of the given source-trained model parameters to the target domain, nor does it require any considerable amount of memory for storing its frequency domain representation of the source domain style, which is particularly important considering the hardware limitations in an autonomous vehicle.",https://ieeexplore.ieee.org/document/9564566/,2021 IEEE International Intelligent Transportation Systems Conference (ITSC),19-22 Sept. 2021,ieeexplore
10.1109/ASE.2019.00127,Coverage-Guided Fuzzing for Feedforward Neural Networks,IEEE,Conferences,"Deep neural network (DNN) has been widely applied to safety-critical scenarios such as autonomous vehicle, security surveillance, and cyber-physical control systems. Yet, the incorrect behaviors of DNNs can lead to severe accidents and tremendous losses due to hidden defects. In this paper, we present DeepHunter, a general-purpose fuzzing framework for detecting defects of DNNs. DeepHunter is inspired by traditional grey-box fuzzing and aims to increase the overall test coverage by applying adaptive heuristics according to runtime feedback. Specifically, DeepHunter provides a series of seed selection strategies, metamorphic mutation strategies, and testing criteria customized to DNN testing; all these components support multiple built-in configurations which are easy to extend. We evaluated DeepHunter on two popular datasets and the results demonstrate the effectiveness of DeepHunter in achieving coverage increase and detecting real defects. A video demonstration which showcases the main features of DeepHunter can be found at https://youtu.be/s5DfLErcgrc.",https://ieeexplore.ieee.org/document/8952279/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore
10.1109/ICOSEC51865.2021.9591747,Deep Learning based Object Detection Model for Autonomous Driving Research using CARLA Simulator,IEEE,Conferences,"Autonomous vehicle research has grown exponentially over the years with researchers working on different object detection algorithms to realize safe and competent self-driving systems while legal authorities are simultaneously looking into the ways of mitigating the risks posed by fully autonomous vehicles. These advancements can result in a much safer commuting environment, reduced accidents and also eliminate the necessity for human driving. The creation of data and access to data for autonomous driving research is difficult challenge that research communities are facing. Hence, open source simulators such as the CARLA simulator (CAR Learning to Act) help us train and test models and to gain insights into autonomous driving with ease. This paper proposes the application of object detection algorithm on CARLA simulator to derive useful results for autonomous driving research. Further, the comparison of CARLA simulator with other available simulators, key players in the field of autonomous vehicle technology, state-of-the-art algorithms being used for autonomous driving, real time implementation challenges and future technologies are also discussed.",https://ieeexplore.ieee.org/document/9591747/,2021 2nd International Conference on Smart Electronics and Communication (ICOSEC),7-9 Oct. 2021,ieeexplore
10.1109/ICCC51557.2021.9454613,Deep Learning-Based Automated Vehicle Steering,IEEE,Conferences,"Autonomous Vehicle applications are full of open challenges. Despite the advanced technologies, the lack of robust systems still exists due to the high complexity of the surrounded environments. The automated steering is one of the most complex autonomous driving system's application. Model predictive control is the most common control strategy used to implement the automated steering tasks due to its ability to solve an online quadratic optimization problem in the real-time, in addition to its efficiency in handling the constraints of the system's environments. MPC controller is used to drive the vehicle autonomously along the centerline of the road based on two main factors, the lateral deviation and relative yaw angle. Deep learning technology has been widely used in recent years because of the promising performance achieved in different applications and tasks. In this context, we suggested that the implementation of the Deep Neural Network (DNN) will provide a great improvement and it can be more computationally efficient than solving an online quadratic problem (QP), that will naturally lead to reduce the time, the complexity, and the computational loads of implementations. The main aims of this paper are to design a deep learning-based approach for automated vehicle steering based on the behaviour of the traditional MPC controller. In addition, to study the efficiency of the full replacement of the MPC controller by the suggested DNN model. The study is based on performing a comparison between the implementations of both controllers (MPC and DNN model) in terms of the performance and the execution time. The performance indicator is the ability of the controller to drive the decision variables (lateral deviation and yaw angle) to be close to zero in order to drive the vehicle autonomously along the desired path.",https://ieeexplore.ieee.org/document/9454613/,2021 22nd International Carpathian Control Conference (ICCC),31 May-1 June 2021,ieeexplore
10.1109/CMPEUR.1992.218441,Developments in autonomous vehicle navigation,IEEE,Conferences,An approach to autonomous vehicle navigation using neural networks to imitate human driving behavior is presented. The results are based on the measured actions of a real human driver in a real car. The car environment and the information that is recorded during driving are described. The problem that should be solved by the neural network is discussed. The actual learning of the driving task is presented. The quality of the leaned driving behavior is reviewed. The experiments showed that medium-sized neural networks were able to approximate the driving behavior within a maximum error of 5%.&lt;<ETX>&gt;</ETX>,https://ieeexplore.ieee.org/document/218441/,CompEuro 1992 Proceedings Computer Systems and Software Engineering,4-8 May 1992,ieeexplore
10.1109/UUST.1987.1158593,Eave-west: A testbed for plan execution,IEEE,Conferences,"The Experimental Autonomous Vehicle - West (EAVE-West) submersible testbed has been configured for demonstrating a distributable software architecture for Autonomous Undersea Vehicle (AUV) plan execution. Instead of using a machine planner aboard the AUV, plans are represented and then downloaded to the vehicle. This technique obviates the problems associated with planning and, as a result, the real-time response of the AUV can potentially be improved. A review of the architecture is given and the EAVE-West demonstration system is discussed.",https://ieeexplore.ieee.org/document/1158593/,Proceedings of the 1987 5th International Symposium on Unmanned Untethered Submersible Technology,June 1987,ieeexplore
10.1109/IV48863.2021.9575135,End-to-End Intersection Handling using Multi-Agent Deep Reinforcement Learning,IEEE,Conferences,"Navigating through intersections is one of the main challenging tasks for an autonomous vehicle. However, for the majority of intersections regulated by traffic lights, the problem could be solved by a simple rule-based method in which the autonomous vehicle behavior is closely related to the traffic light states. In this work, we focus on the implementation of a system able to navigate through intersections where only traffic signs are provided. We propose a multi-agent system using a continuous, model-free Deep Reinforcement Learning algorithm used to train a neural network for predicting both the acceleration and the steering angle at each time step. We demonstrate that agents learn both the basic rules needed to handle intersections by understanding the priorities of other learners inside the environment, and to drive safely along their paths. Moreover, a comparison between our system and a rule-based method proves that our model achieves better results especially with dense traffic conditions. Finally, we test our system on real world scenarios using real recorded traffic data, proving that our module is able to generalize both to unseen environments and to different traffic conditions.",https://ieeexplore.ieee.org/document/9575135/,2021 IEEE Intelligent Vehicles Symposium (IV),11-17 July 2021,ieeexplore
10.1109/CiSt49399.2021.9357196,End-to-End Neural Network for Vehicle Dynamics Modeling,IEEE,Conferences,"Autonomous vehicles have to meet high safety standards in order to be commercially viable. Before real-world testing of an autonomous vehicle, extensive simulation is required to verify software functionality and to detect unexpected behavior. This incites the need for accurate models to match real system behavior as closely as possible. During driving, planing and control algorithms also need an accurate estimation of the vehicle dynamics in order to handle the vehicle safely. Until now, vehicle dynamics estimation has mostly been performed with physics-based models. Whereas these models allow specific effects to be implemented, accurate models need a variety of parameters. Their identification requires costly resources, e.g., expensive test facilities. Machine learning models enable new approaches to perform these modeling tasks without the necessity of identifying parameters. Neural networks can be trained with recorded vehicle data to represent the vehicle's dynamic behavior. We present a neural network architecture that has advantages over a physics-based model in terms of accuracy. We compare both models to real-world test data from an autonomous racing vehicle, which was recorded on different race tracks with high- and low-grip conditions. The developed neural network architecture is able to replace a single-track model for vehicle dynamics modeling.",https://ieeexplore.ieee.org/document/9357196/,2020 6th IEEE Congress on Information Science and Technology (CiSt),5-12 June 2021,ieeexplore
10.1109/AICAS51828.2021.9458488,Evaluation of Machine Learning-based Detection against Side-Channel Attacks on Autonomous Vehicle,IEEE,Conferences,"Autonomous vehicles are becoming increasingly popular, but their reliance on computer systems to sense and operate in the physical world has introduced new security risks. Recent studies have shown that using Cache-based Side-Channel Attacks (SCAs) could infer sensitive users' information (e.g., which route the user is taking) highlighting significant vulnerability posed to today's computer systems. As a result, it is crucial to propose effective detection mechanisms against emerging microarchitectural SCAs on autonomous driving systems. In response, we first identify the threat model and victim applications of autonomous driving systems in this work. Next, we explore the suitability of various machine learning-based classifiers trained by information collected from built-in hardware performance counter registers available in modern autonomous vehicle systems. To this end, various supervised machine learning models are implemented for cache-based SCAs detection and precisely compared and characterized in terms of detection accuracy, robustness, and latency of the detection. Our experiments conducted on an Intel Xeon, which Waymo autonomous driving vendor uses, demonstrate that J48 achieves 99.5% accuracy with the highest efficiency compared with other investigated models.",https://ieeexplore.ieee.org/document/9458488/,2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),6-9 June 2021,ieeexplore
10.1109/IJCNN.2017.7966404,Following the leader using a tracking system based on pre-trained deep neural networks,IEEE,Conferences,"In this work, we present a software architecture to solve, at some level, the follow the leader problem. This problem consists of an autonomous vehicle trying to track and follow a leader vehicle. To track the leader position in consecutive camera images, we employed the Generic Object Tracking Using Regression Networks (GOTURN). GOTURN is a pre-trained Deep Neural Network capable of tracking generic objects, without application-specific training or fine-tuning. The proposed software architecture was evaluated using a real autonomous vehicle, in four stretches of a University ring road. In all experiments, the autonomous vehicle was able to follow the leader's path with maximum root mean square error of 0.28m.",https://ieeexplore.ieee.org/document/7966404/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore
10.1109/ICAMechS.2016.7813486,Intelligent adaptive precrash control for autonmous vehicle agents (CBR Engine &amp; hybrid A∗ path planner),IEEE,Conferences,"PreCrash problem of Intelligent Control of autonomous vehicles robot is a very complex problem, especially vehicle pre-crash scenariws and at points of intersections in real-time environmenta. This Paper presents a novel architecture of Intelligent adaptive control for autonomous vehicle agent that depends on Artificial Intelligence Techniques that applies case-based reasoning techniques, where Parallel CBR Engines are implemented for different scenarios' of PreCrash problem and sub-problems of intersection safety and collision avoidance, in the higher level of the controller and A* path planner for path planning and at lower-levels it also uses some features of autonomous vehicle dynamics. Moreover, the planner is enhanced by combination of Case-Based Planner. All modules are presented and discussed. Experimental results are conducted in the framework of Webots autonomous vehicle tool and overall results are good for the CBR Engine for Adaptive control and also for the hybrid Case-Based Planner, A* and D* motion planner along with conclusion and future work.",https://ieeexplore.ieee.org/document/7813486/,2016 International Conference on Advanced Mechatronic Systems (ICAMechS),30 Nov.-3 Dec. 2016,ieeexplore
10.1109/ITSC48978.2021.9564899,Learning a Model for Inferring a Spatial Road Lane Network Graph using Self-Supervision,IEEE,Conferences,"Interconnected road lanes are a central concept for navigating urban roads. Currently, most autonomous vehicles rely on preconstructed lane maps as designing an algorithmic model is difficult. However, the generation and maintenance of such maps is costly and hinders large-scale adoption of autonomous vehicle technology. This paper presents the first self-supervised learning method to train a model to infer a spatially grounded lane-level road network graph based on a dense segmented representation of the road scene generated from onboard sensors. A formal road lane network model is presented and proves that any structured road scene can be represented by a directed acyclic graph of at most depth three while retaining the notion of intersection regions, and that this is the most compressed representation. The formal model is implemented by a hybrid neural and search-based model, utilizing a novel barrier function loss formulation for robust learning from partial labels. Experiments are conducted for all common road intersection layouts. Results show that the model can generalize to new road layouts, unlike previous approaches, demonstrating its potential for real-world application as a practical learning-based lane-level map generator.",https://ieeexplore.ieee.org/document/9564899/,2021 IEEE International Intelligent Transportation Systems Conference (ITSC),19-22 Sept. 2021,ieeexplore
10.1109/ICTAI.2019.00220,Learning to Drive via Apprenticeship Learning and Deep Reinforcement Learning,IEEE,Conferences,"With the implementation of reinforcement learning (RL) algorithms, current state-of-art autonomous vehicle technology have the potential to get closer to full automation. However, most of the applications have been limited to game domains or discrete action space which are far from the real world driving. Moreover, it is very tough to tune the parameters of reward mechanism since the driving styles vary a lot among the different users. For instance, an aggressive driver may prefer driving with high acceleration whereas some conservative drivers prefer a safer driving style. Therefore, we propose an apprenticeship learning in combination with deep reinforcement learning approach that allows the agent to learn the driving and stopping behaviors with continuous actions. We use gradient inverse reinforcement learning (GIRL) algorithm to recover the unknown reward function and employ REINFORCE as well as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal policy. The performance of our method is evaluated in simulation-based scenario and the results demonstrate that the agent performs human like driving and even better in some aspects after training.",https://ieeexplore.ieee.org/document/8995417/,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),4-6 Nov. 2019,ieeexplore
10.23919/AEITAUTOMOTIVE50086.2020.9307387,LiDAR point-cloud processing based on projection methods: a comparison,IEEE,Conferences,"An accurate and rapid-response perception system is fundamental for autonomous vehicles to operate safely. 3D object detection methods handle point clouds given by LiDAR sensors to provide accurate depth and position information for each detection, together with its dimensions and classification. The information is then used to track vehicles and other obstacles in the surroundings of the autonomous vehicle, and also to feed control units that guarantee collision avoidance and motion planning. Nowadays, object detection systems can be divided into two main categories. The first ones are the geometric based, which retrieve the obstacles using geometric and morphological operations on the 3D points. The seconds are the deep learning-based, which process the 3D points, or an elaboration of the 3D point-cloud, with deep learning techniques to retrieve a set of obstacles. This paper presents a comparison between those two approaches, presenting one implementation of each class on a real autonomous vehicle. Accuracy of the estimates of the algorithms has been evaluated with experimental tests carried in the Monza ENI circuit. The positions of the ego vehicle and the obstacle are given by GPS sensors with real time kinematic (RTK) correction, which guarantees an accurate ground truth for the comparison. Both algorithms have been implemented on ROS and run on a consumer laptop.",https://ieeexplore.ieee.org/document/9307387/,2020 AEIT International Conference of Electrical and Electronic Technologies for Automotive (AEIT AUTOMOTIVE),18-20 Nov. 2020,ieeexplore
10.1109/IV48863.2021.9575925,LiDAR-based Object Detection Failure Tolerated Autonomous Driving Planning System,IEEE,Conferences,"A typical autonomous driving system usually relies on the detected objects from an environment perception module. Current research still cannot guarantee a perfect perception, and failure detections may cause collisions, leading to untrustworthy autonomous vehicles. This work proposes a trajectory planner to tolerate the detection failure of the LiDAR sensors. This method will plan the path relying on the detected objects as well as the raw sensor data. The overlapping and contradiction of both perception routes will be carefully addressed for safe and efficient driving. The object detector in this work uses a deep learning-based method, i.e., CNN-Segmentation neural network. The designed trajectory planner has multi-layers to handle the multi-resolution environment formed by different perception routes. The final system will dynamically adjust its attention to the detected objects or the point cloud to avoid collision due to detection failures. This method is implemented on a real autonomous vehicle to drive in an open urban area. The results show that when the autonomous vehicle fails to detect a surrounding object, e.g., vehicles or some undefined objects, the autonomous vehicles still can plan an efficient and safe trajectory. In the meantime, when the perception system works well, the A V will not be affected by the point clouds. This technology can make the autonomous vehicle trustworthy even with the black-box neural networks. The codes are open-source with our autonomous driving platform to help other researchers for A V development.",https://ieeexplore.ieee.org/document/9575925/,2021 IEEE Intelligent Vehicles Symposium (IV),11-17 July 2021,ieeexplore
10.1109/IntelliSys.2017.8324372,Machine learning and deep neural network — Artificial intelligence core for lab and real-world test and validation for ADAS and autonomous vehicles: AI for efficient and quality test and validation,IEEE,Conferences,"Autonomous vehicles are now the future of automobile industry. Human drivers can be completely taken out of the loop through the implementation of safe and intelligent autonomous vehicles. Although we can say that HW and SW development continues to play a large role in the automotive industry, test and validation of these systems is a must. The ability to test these vehicles thoroughly and efficiently will ensure their proper and flawless operation. When a large number of people with heterogeneous knowledge and skills try to develop an autonomous vehicle together, it is important to use a sensible engineering process. State of the art techniques for such development include Waterfall, Agile &amp; V-model, where test &amp; validation (T&amp;V) process is an integral part of such a development cycle. This paper will propose a new methodology using machine learning &amp; deep neural network (AI-core) for lab &amp; real-world T&amp;V for ADAS (Advanced driver assistance system) and autonomous vehicles. The methodology will initially connect T&amp;V of individual systems in each level of development and that of complete system efficiently, by using the proposed phase methodology, in which autonomous driving functions are grouped under categories, special T&amp;V processes are carried on simulation as well as in HIL systems. The complete transition towards AI in the field of T&amp;V will be a sequence of steps. Initially the AI-core is fed with available test scenarios, boundary conditions for the test cases and scenarios, and examples, the AI-core will conduct virtual tests on simulation environment using available test scenarios and further generates new test cases and scenarios for efficient and precise tests. These test cases and scenarios are meant to cover all available cases and concentrate on the area where bugs or failures occur. The complete surrounding environment in the simulation is also controlled by the AI-core which means that the system can attain endless/all-possible combinations of the surrounding environment which is necessary. Results of the tests are sorted and stored, critical and important tests are again repeated in the real-world environment using automated cars with other real subsystems to depict the surrounding environment, which are all controlled by the AI-core, and meanwhile the AI-core is always in the loop and learning from each and every executed test case and its results/outcomes. The main goal is to achieve efficient and high quality test and validation of systems for automated driving, which can save precious time in the development process. As a future scope of this methodology, we can step-up to make most parts of test and validation completely autonomous.",https://ieeexplore.ieee.org/document/8324372/,2017 Intelligent Systems Conference (IntelliSys),7-8 Sept. 2017,ieeexplore
10.1109/ITSC.2019.8917085,Machine learning method to ensure robust decision-making of AVs,IEEE,Conferences,"Replacing the human driver to perform the Dynamic Driving Task (DDT)[1] will require perception, complex analysis and assessment of traffic situation. The path leading to success the deployment of fully Autonomous Vehicle (AV) depends on the resolution of a lot of challenges. Both the safety and the security aspects of AV constitute the core of regulatory compliance and technical research. The Autonomous Driving System (ADS) should be designed to ensure a safe manoeuvre and a stable behaviour despite the technological limitations, the uncertainties and hazards which characterize the real traffic conditions. In fully Autonomous Driving situation, detecting all relevant objects and agents should be sufficient to generate a warning, however the ADS requires further complex data analysis steps to quantify and improve the safety of decision making. This paper aims to improve the robustness of decision-making in order to mimic human-like decision ability. The approach is based on machine learning to identify the criticality of the dynamic situation and enabling ADS to make appropriate decision and fulfil safe manoeuvre.",https://ieeexplore.ieee.org/document/8917085/,2019 IEEE Intelligent Transportation Systems Conference (ITSC),27-30 Oct. 2019,ieeexplore
10.1109/ICMLC.2010.71,Microcontroller Based Neural Network Controlled Low Cost Autonomous Vehicle,IEEE,Conferences,"In this paper, design of a low cost autonomous vehicle based on neural network for navigation in unknown environments is presented. The vehicle is equipped with four ultrasonic sensors for hurdle distance measurement, a wheel encoder for measuring distance traveled, a compass for heading information, a GPS receiver for goal position information, a GSM modem for changing destination place on run time and a nonvolatile RAM for storing waypoint data; all interfaced to a low cost AT89C52 microcontroller. The microcontroller processes the information acquired from the sensors and generates robot motion commands accordingly through neural network. The neural network running inside the microcontroller is a multilayer feed-forward network with back-propagation training algorithm. The network is trained offline with tangent-sigmoid as activation function for neurons and is implemented in real time with piecewise linear approximation of tangent-sigmoid function. Results have shown that upto twenty neurons can be implemented in hidden layer with this technique. The vehicle is tested with varying destination places in outdoor environments containing stationary as well as moving obstacles and is found to reach the set targets successfully.",https://ieeexplore.ieee.org/document/5460762/,2010 Second International Conference on Machine Learning and Computing,9-11 Feb. 2010,ieeexplore
10.1109/URAI.2016.7734068,Monocular vision-based object recognition for autonomous vehicle driving in a real driving environment,IEEE,Conferences,"Nowadays, many attentions have been devoted to autonomous vehicles because the automation of driving technology has a large number of benefits, such as the minimization of risks, the improvement of mobility and ease of drivers. Among many technologies for autonomous driving, road environmental recognition is one of the key issues. In this paper, we present the test results of various object detection algorithms using single monocular camera for autonomous vehicle in real driving conditions. The vision recognition system tested in this paper has three main recognition parts: pedestrian detection, traffic sign and traffic light recognition. We use Histogram of Gradients (HOG) features and detect the pedestrians by Support Vector Machine (SVM). Also features of traffic signs are extracted by Principal Components Analysis (PCA) and canny edge detection is used for traffic lights. These two signals are classified by Neural Network (NN). Algorithms that we tested are implemented in General-Purpose computing on Graphics Processing Units (GPGPU). We show the effectiveness of these methods in real-time applications for autonomous driving.",https://ieeexplore.ieee.org/document/7734068/,2016 13th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI),19-22 Aug. 2016,ieeexplore
10.1109/ITSC45102.2020.9294697,Monte Carlo Tree Search With Reinforcement Learning for Motion Planning,IEEE,Conferences,"Motion planning for an autonomous vehicle is most challenging for scenarios such as large, multi-lane, and unsignalized intersections in the presence of dense traffic. In such situations, the motion planner has to deal with multiple crossing-points to reach an objective in a safe, comfortable, and efficient way. In addition, motion planning challenges include real-time computation and scalability to complex scenes with many objects and different road geometries. In this work, we propose a motion planning system addressing these challenges. We enable real-time applicability of a Monte Carlo Tree Search algorithm with a deep-learning heuristic. We learn a fast evaluation function from accurate, but non real-time models. While using Deep Reinforcement Learning techniques we maintain a clear separation between making predictions and making decisions. We reduce the complexity of the search model and benchmark the proposed agent against multiple methods: rules-based, MCTS, A* search, deep learning, and Model Predictive Control. We show that our agent outperforms these other agents in a variety of challenging scenarios, where we benchmark safety, comfort and efficiency metrics.",https://ieeexplore.ieee.org/document/9294697/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore
10.1109/ICCAR.2017.7942721,Object detection on panoramic images based on deep learning,IEEE,Conferences,"Panoramic image can be widely used in many applications, such as virtual reality, visual surveillance and autonomous vehicle, because of its large field of view. However, the inherent distortion for panorama causes object detection to be a challenging task. This paper focuses on the multi-class objects detection in panoramic images using deep learning method. The proposed system uses three fisheye cameras to efficiently create panoramas and build a large dataset. A region based convolutional neutral network (R-CNN) is implemented to train and test on an indoor panoramic image dataset. Experiments show great improvement performance on ten categories of distorted indoor objects with a mean average precision of 68.7%.",https://ieeexplore.ieee.org/document/7942721/,"2017 3rd International Conference on Control, Automation and Robotics (ICCAR)",24-26 April 2017,ieeexplore
10.1109/IVS.2002.1187941,Pattern matching as the nucleus for either autonomous driving or driver assistance systems,IEEE,Conferences,"Concerns autonomous vehicle driving by pattern matching combined with reinforcement learning. In specific, this research focuses on the requirement to steer an autonomous car along a curvy and hilly road course with no intersections and no other vehicle or obstacle but with the strict requirement to self-improve driving behaviour. A camera is used to build quickly an abstract complete description (ACSD) of vehicle's current situation. This combines traditional edge finding operators with a new technique of Bayes prediction for each part of the video image. Those ACSD's are being stored together with the steering commands issued at that time and serve as the pattern database of possible driving behaviour which are being retrieved using an approximate nearest neighbour pattern matching algorithm with a O(n log m) characteristic compared to O(n/spl middot/m) for the conventional nearest neighbour calculation. In addition to this, any feedback on the quality or appropriateness of the driving behaviour has to be self-created (e.g. time measurement for a whole road section) and is therefore delayed and unspecific in relation to single issued steering commands. Consequently, a machine learning algorithm coping with those conditions is being implemented based on Reinforcement Learning.",https://ieeexplore.ieee.org/document/1187941/,"Intelligent Vehicle Symposium, 2002. IEEE",17-21 June 2002,ieeexplore
10.1109/ISCAS45731.2020.9180841,PointNet on FPGA for Real-Time LiDAR Point Cloud Processing,IEEE,Conferences,"LiDAR sensors have been widely used in many autonomous vehicle modalities, such as perception, mapping, and localization. This paper presents an FPGA-based deep learning platform for real-time point cloud processing targeted on autonomous vehicles. The software driver for the Velodyne LiDAR sensor is modified and moved into the on-chip processor system, while the programmable logic is designed as a customized hardware accelerator. As the state-of-art deep learning algorithm for point cloud processing, PointNet is successfully implemented on the proposed FPGA platform. Targeted on a Xilinx Zynq UltraScale+ MPSoC ZCU104 development board, the FPGA implementations of PointNet achieve the computing performance of 182.1 GOPS and 280.0 GOPS for classification and segmentation respectively. The proposed design can support an input up to 4096 points per frame. The processing time is 19.8 ms for classification and 34.6 ms for segmentation, which meets the real-time requirement for most of the existing LiDAR sensors.",https://ieeexplore.ieee.org/document/9180841/,2020 IEEE International Symposium on Circuits and Systems (ISCAS),12-14 Oct 2020,ieeexplore
10.1109/ICICCS51141.2021.9432186,Pothole and Object Detection for an Autonomous Vehicle Using YOLO,IEEE,Conferences,"Object Detection is an key software and a fundamental task for an autonomous driving system that provides remarkable change in computer vision. İn recent years, company's are planning to launch autonomous vehicle in an full swing that's the most important ascpets for object detection and one of most challenging task for locating specific object from from multiple objects in a specific scenario. The computer vision and machine learning algorithm is the important tool for detecting objects in and around the environment. In this paper, which consists of two parts The first part is implemented on object detection in the surrounding with Yolo (You Only Look Once)Algorithm provides exact classification and position which is configured on newly created datasets for classes of object: a car, a person, a truck, a bus, traffic light, motorcycle, pothole, wetland uses the Convolutional Neural Network and max-polling layer for prediction that improves detecting of small target and these deep learning technique provides a high accuracy for detecting real world. Detecting potholes in Indian road help the autonomous vehicle to move smoothly without getting struck in the potholes. In part two of the proposed method is implemented on Raspberry pi4 a popular embedded computer board explores suitability for the running objects. That solves the real world problems and improves the impact on detecting objects. Knowing pothole and wetland detection for self-driving vehicle is needed badly to solve the road lay problems like: accident, slowing down the transport system these are solved by deep learning.",https://ieeexplore.ieee.org/document/9432186/,2021 5th International Conference on Intelligent Computing and Control Systems (ICICCS),6-8 May 2021,ieeexplore
10.1109/ICSE43902.2021.00046,Prioritizing Test Inputs for Deep Neural Networks via Mutation Analysis,IEEE,Conferences,"Deep Neural Network (DNN) testing is one of the most widely-used ways to guarantee the quality of DNNs. However, labeling test inputs to check the correctness of DNN prediction is very costly, which could largely affect the efficiency of DNN testing, even the whole process of DNN development. To relieve the labeling-cost problem, we propose a novel test input prioritization approach (called PRIMA) for DNNs via intelligent mutation analysis in order to label more bug-revealing test inputs earlier for a limited time, which facilitates to improve the efficiency of DNN testing. PRIMA is based on the key insight: a test input that is able to kill many mutated models and produce different prediction results with many mutated inputs, is more likely to reveal DNN bugs, and thus it should be prioritized higher. After obtaining a number of mutation results from a series of our designed model and input mutation rules for each test input, PRIMA further incorporates learning-to-rank (a kind of supervised machine learning to solve ranking problems) to intelligently combine these mutation results for effective test input prioritization. We conducted an extensive study based on 36 popular subjects by carefully considering their diversity from five dimensions (i.e., different domains of test inputs, different DNN tasks, different network structures, different types of test inputs, and different training scenarios). Our experimental results demonstrate the effectiveness of PRIMA, significantly outperforming the state-of-the-art approaches (with the average improvement of 8.50%~131.01% in terms of prioritization effectiveness). In particular, we have applied PRIMA to the practical autonomous-vehicle testing in a large motor company, and the results on 4 real-world scene-recognition models in autonomous vehicles further confirm the practicability of PRIMA.",https://ieeexplore.ieee.org/document/9402064/,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),22-30 May 2021,ieeexplore
10.1109/ICRA48506.2021.9561747,Pylot: A Modular Platform for Exploring Latency-Accuracy Tradeoffs in Autonomous Vehicles,IEEE,Conferences,"We present Pylot, a platform for autonomous vehicle (AV) research and development, built with the goal to allow researchers to study the effects of the latency and accuracy of their models and algorithms on the end-to-end driving behavior of an AV. This is achieved through a modular structure enabled by our high-performance dataflow system that represents AV software pipeline components (object detectors, motion planners, etc.) as a dataflow graph of operators which communicate on data streams using timestamped messages. Pylot readily interfaces with popular AV simulators like CARLA, and is easily deployable to real-world vehicles with minimal code changes.To reduce the burden of developing an entire pipeline for evaluating a single component, Pylot provides several state-of-the-art reference implementations for the various components of an AV pipeline. Using these reference implementations, a Pylot-based AV pipeline is able to drive a real vehicle, and attains a high score on the CARLA Autonomous Driving Challenge. We also present several case studies enabled by Pylot, including evidence of a need for context-dependent components, and per-component time allocation. Pylot is open source, with the code available at https://github.com/erdos-project/pylot.",https://ieeexplore.ieee.org/document/9561747/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore
10.1109/NAECON46414.2019.9057988,Real-Time 3-D Segmentation on An Autonomous Embedded System: using Point Cloud and Camera,IEEE,Conferences,"Present day autonomous vehicle relies on several sensor technologies for it's autonomous functionality. The sensors based on their type and mounted-location on the vehicle, can be categorized as: line of sight and non-line of sight sensors and are responsible for the different level of autonomy. These line of sight sensors are used for the execution of actions related to localization, object detection and the complete environment understanding. The surrounding or environment understanding for an autonomous vehicle can be achieved by segmentation. Several traditional and deep learning related techniques providing semantic segmentation for an input from camera is already available, however with the advancement in the computing processor, the progression is on developing the deep learning application replacing traditional methods. This paper presents an approach to combine the input of camera and lidar for semantic segmentation purpose. The proposed model for outdoor scene segmentation is based on the frustum pointnet, and ResNet which utilizes the 3d point cloud and camera input for the 3d bounding box prediction across the moving and non-moving object and thus finally recognizing and understanding the scenario at the point-cloud or pixel level. For real time application the model is deployed on the RTMaps framework with Bluebox (an embedded platform for autonomous vehicle). The proposed architecture is trained with the CITYScpaes and the KITTI dataset.",https://ieeexplore.ieee.org/document/9057988/,2019 IEEE National Aerospace and Electronics Conference (NAECON),15-19 July 2019,ieeexplore
10.1109/ASYU50717.2020.9259830,Real-Time Implementation of Mini Autonomous Car Based on MobileNet - Single Shot Detector,IEEE,Conferences,"In this paper, in order to realize a prototype of an autonomous vehicle, we present a framework that consists of convolutional neural networks and image processing methods. The study is comprised of two main parts as software and hardware. In the hardware part, a small-sized smart video car kit is used as the prototype of the autonomous car. This programmable tool consists of Raspberry Pi, servo motors and a USB webcam whose angle of vision is equal to 120°. In the software part, we propose an algorithm in which we use Convolutional Neural Networks to detect the objects (vehicles, pedestrians, and traffic signs) and Hough transformation to detect the road lanes. Based on the outputs of the object and lane detections, the system decides the speed and the direction of the car in real-time. In our results, the vehicle performs autonomous driving in the scaled real-world application.",https://ieeexplore.ieee.org/document/9259830/,2020 Innovations in Intelligent Systems and Applications Conference (ASYU),15-17 Oct. 2020,ieeexplore
10.1109/EIT48999.2020.9208309,Real-Time Traffic Sign Detection and Classification Using Machine Learning and Optical Character Recognition,IEEE,Conferences,"Autonomous vehicle development is currently progressing at a very fast pace and traffic sign detection and classification has an important role in it. This paper analyzes a few possible approaches of doing this task in real-time using a portable system. The final solution uses a convolutional neural network for detection and classification combined with a custom optical character recognition algorithm for speed limit signs. The training and testing dataset is based on a combination of the Belgian Dataset, German Dataset, as well as images taken while driving in Illinois, United States.",https://ieeexplore.ieee.org/document/9208309/,2020 IEEE International Conference on Electro Information Technology (EIT),31 July-1 Aug. 2020,ieeexplore
10.1109/AIPR.2016.8010547,Real-time detection and classification of traffic light signals,IEEE,Conferences,"Traffic light detection is an important part of Advanced Driver Assist as well as autonomous vehicle systems which ensures timely and appropriate reaction to traffic lights (TLs) in cross sections. In this paper we introduce a robust and realtime approach to detect TLs and recognize its status in complex traffic scenes solely based on image processing techniques. The proposed system uses color properties of the scene to detect TLs in real-time. An innovative technique has been developed to significantly decrease compute requirement for detection of TL color by using one Lookup Table independent of lighting conditions. Each candidate region is further analyzed, using features analysis, to segregate actual TL signals among all candidate regions. As in similar machine learning techniques, an unsupervised classifier using a set of significant features has been developed to accurately segregate circular, semi-circular, and arrow shaped TL signals without using a training dataset. The final C++ code has been implemented and optimized on intelplatform using 1920×1080 frame resolution to recognize the status of TLs during day-time and night-time scenes, achieving 95% precision and 94.7% recall at 30FPS.",https://ieeexplore.ieee.org/document/8010547/,2016 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),18-20 Oct. 2016,ieeexplore
10.1109/ICC42927.2021.9500318,Reinforcement Learning for Autonomous Vehicle Movements in Wireless Sensor Networks,IEEE,Conferences,"In this work we use autonomous vehicles to improve the performance of Wireless Sensor Networks (WSNs). In contrast to other autonomous vehicle applications, WSNs have two metrics for performance evaluation. First, quality of information (QoI) which is used to measure the quality of sensed data (e.g., measurement uncertainties or signal strength). Second, quality of service (QoS) which is used to measure the network’s performance for data forwarding (e.g., delay and packet losses). As a use case, we consider wireless acoustic sensor networks, where a group of speakers move inside a room and there are autonomous vehicles installed with microphones for streaming the audio data. We formulate the problem as a Markov decision problem (MDP) and solve it using Deep-Q-Networks (DQN). Additionally, we compare the performance of DQN solution to two different real-world implementations: speakers holding/passing microphones and microphones being preinstalled in fixed positions.We show using simulations that the performance of autonomous vehicles in terms of QoI and QoS is better than the real-world implementation in some scenarios. Moreover, we study the impact of the vehicles speed on the learning process of the DQN solution and show how low speeds degrade the performance. Finally, we compare the DQN solution to a heuristic one and provide theoretical analysis of the performance with respect to dynamic WSNs.",https://ieeexplore.ieee.org/document/9500318/,ICC 2021 - IEEE International Conference on Communications,14-23 June 2021,ieeexplore
10.1109/ISORC.2018.00025,Representative Safety Assessment of Autonomous Vehicle for Public Transportation,IEEE,Conferences,"The implementations and testing in real conditions of Autonomous Vehicles (AV) for private usage show important advances. However, a lack still exists in addressing the particularities of AVs for Public Transportation. Such particularities range from limited safety mechanisms aboard, risky situations associated to particular users and complex self-driving situations up to the limited passengers-vehicle interactions possible. Since, to our knowledge, no comprehensive safety assessment actually exists and the current automotive related standards do not address identified aspects, in this paper, we propose to conduct a minimal but representative safety assessment based upon a local but real autonomous vehicle implementation. To conduct our study, the Hazard Analysis and Risks Assessment introduced in the ISO 26262 standard is taken as a basis. Initial outcomes suggest that critical autonomy aspects, like machine learning of complex operational situations, the metrics for quantitative assessment of autonomy, and potential conflicts between autonomy principles and external safety fences can have critical safety impacts and demand further discussions.",https://ieeexplore.ieee.org/document/8421156/,2018 IEEE 21st International Symposium on Real-Time Distributed Computing (ISORC),29-31 May 2018,ieeexplore
10.1109/ICAICTA49861.2020.9429073,Road Recognition System with Heuristic Method and Machine Learning,IEEE,Conferences,"Road recognition is one of essential information for determining an Autonomous Vehicle movement. Latest research has shown that machine learning could be used to obtain the information from images. Nevertheless, the system could be improved by effectivity and efficiency. This research proposed finding better feature combinations and using Artificial Neural Network algorithm to build higher accuracy road detection model for better effectivity. Region of Interest module using heuristic method also applied to reduce computation for better efficiency. These three new modules are implemented and combined with road recognition module to become road recognition system. The proposed method performance then tested and compared with the latest research. The experiment results shown that Artificial Neural Network cannot increase the system effectiveness. Nonetheless, with right feature and region of interest module, the proposed system successfully gives better performance. The prototype has accuracy increased from F1-score 0,94 to 0,95 and speed increased from 99 to 112 frames processed per second.",https://ieeexplore.ieee.org/document/9429073/,"2020 7th International Conference on Advance Informatics: Concepts, Theory and Applications (ICAICTA)",8-9 Sept. 2020,ieeexplore
10.1109/IROS.2014.6943162,Spatio-temporal motion features for laser-based moving objects detection and tracking,IEEE,Conferences,"This paper proposes a spatio-temporal motion feature detection and tracking method using range sensors working on a moving platform. The proposed spatio-temporal motion features are similar to optical flow but are extended on a moving platform with fusion of odometry and show much better classification accuracy with consideration of different uncertainties. In the proposal, the ego motion is compensated by odometry sensors and the laser scan points are accumulated and represented as space-time point clouds, from which the velocities and moving directions can be extracted. Based on these spatio-temporal features, a supervised learning technique is applied to classify the points as static or moving and Kalman filters are implemented to track the moving objects. A real experiment is performed during day and night on an autonomous vehicle platform and shows promising results in a crowded and dynamic environment.",https://ieeexplore.ieee.org/document/6943162/,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,14-18 Sept. 2014,ieeexplore
10.1109/IVS.1994.639471,The development of a fully autonomous ground vehicle (FAGV),IEEE,Conferences,"As a first step toward the creation of a fully autonomous vehicle that operates in a real world environment, we are currently developing a prototype autonomous ground vehicle (AGV) for use in factories and other industrial/business sites based on behavior-based artificial intelligence (AI) control. This flexible and fully autonomous AGV (FAGV) is expected to operate efficiently in a normal industrial environment without any external guidance. The crucial technique employed is a non-Cartesian way of organizing software agents for the creation of a highly responsive control program. The resulting software is considerably reduced in size. Through numerous experiments using mobile robots we confirmed that these new control programs excel in functionality, efficiency, flexibility and robustness. The second key technique in the planning stage is evolutionary computation, of which genetic algorithms are a principal technique. An online, real-time evolution of the control program will be incorporated in later phases of the project to make FAGVs adaptable to any given operational environment after deployment. The first prototype FAGV has an active vision and behaviour-based control system.",https://ieeexplore.ieee.org/document/639471/,Proceedings of the Intelligent Vehicles '94 Symposium,24-26 Oct. 1994,ieeexplore
10.1109/SIMPAR.2018.8376285,The sleepwalker framework: Verification and validation of autonomous vehicles by mixed reality LiDAR stimulation,IEEE,Conferences,"Verification and validation of autonomous mobile systems, such as autonomous vehicles, is indispensable, since conflicts and serious incidents are rarely acceptable when human beings are involved. Although integrative simulation frameworks are commonly applied to test these systems, such simulations are usually too idealistic, while real world tests are both, expensive and not reproducible. To overcome this problem, we present the framework Sleepwalker for verifying and validating autonomous vehicles: Similar to a human sleepwalker, our framework stimulates the automated driving function at a sensor close level with virtual laserscans mixed with sensor data from the real environment. Thus, the autonomous driving function explicitely builds up a mixed reality environment model as a basis for the subsequent components and therefore enables an overall performance assessment. The instantiation of the framework is adaptable so it to can be balanced between the required result's plausibility and scenario criticality. We demonstrate the distinguished benefits of our framework by different instantiations stimulating an autonomous vehicle and conclude with further research questions.",https://ieeexplore.ieee.org/document/8376285/,"2018 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)",16-19 May 2018,ieeexplore
10.1109/IVS.2014.6856427,Traversability analysis using terrain mapping and online-trained Terrain type classifier,IEEE,Conferences,"Path estimation is a big challenge for autonomous vehicle navigation, especially in unknown, dynamic environments, when road characteristics change often. 3D terrain information (e.g. stereo cameras) can provide useful hints about the traversability cost of certain regions. However, when the terrain tends to be flat and uniform, it is difficult to identify a better path using 3D map solely. In this scenario the use of a priori knowledge on the expected road's visual characteristics can support detection, but it has the drawback of being not robust to environmental changes. This paper presents a path detection method that mixes together 3D mapping and visual classification, trying to learn, in real time, the actual road characteristics. An on-line learning of visual characteristics is implemented to feedback a terrain classifier, so that the road characteristics are updated as the vehicle moves. The feedback data are taken from a 3D traversability cost map, which provides some hints on traversable and non-traversable regions. After several re-training cycles the algorithm converges on a better separation of the path and non-path regions. The fusion of both 3D traversability cost and visual characteristics of the terrain yields a better estimation when compared with either of these methods solely.",https://ieeexplore.ieee.org/document/6856427/,2014 IEEE Intelligent Vehicles Symposium Proceedings,8-11 June 2014,ieeexplore
10.1109/WACV45572.2020.9093332,Uncertainty-aware Short-term Motion Prediction of Traffic Actors for Autonomous Driving,IEEE,Conferences,"We address one of the crucial aspects necessary for safe and efficient operations of autonomous vehicles, namely predicting future state of traffic actors in the autonomous vehicle's surroundings. We introduce a deep learning-based approach that takes into account a current world state and produces raster images of each actor's vicinity. The rasters are then used as inputs to deep convolutional models to infer future movement of actors while also accounting for and capturing inherent uncertainty of the prediction task. Extensive experiments on real-world data strongly suggest benefits of the proposed approach. Moreover, following successful tests the system was deployed to a fleet of autonomous vehicles.",https://ieeexplore.ieee.org/document/9093332/,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),1-5 March 2020,ieeexplore
10.1109/MICAI.2011.25,Velocity Control of an Electric Vehicle over a CAN Network,IEEE,Conferences,"Distributed control applications require a reliable network for information exchange. The network discussed on this paper uses CAN bus as a means of communication to control the speed of an electric vehicle. National Instruments Programmable Automation Controller, Compact RIO, based on Lab VIEW programming environment is used to execute one of two different speed control algorithms (PID or fuzzy logic) to test the performance of the implemented vehicle network and the control algorithm itself. It also acts as a human-machine interface via a personal computer. The proposed network provides robustness in terms of communication and opens the possibility of expansion to develop complete control architecture in order to successfully build a fully autonomous vehicle.",https://ieeexplore.ieee.org/document/6119003/,2011 10th Mexican International Conference on Artificial Intelligence,26 Nov.-4 Dec. 2011,ieeexplore
10.1109/ITST.2017.7972192,Virtual assistants and self-driving cars,IEEE,Conferences,"Self-driving cars are technologically a reality and in the next decade they are expected to reach the highest level of automation. While there is general agreement that an advanced human-autonomous vehicle (HAV) interaction is key to achieve the benefits of self-driving cars, it is less clear what role artificial intelligence (AI) should play in this context. While the scientific community is debating on the role and intersections of AI, autonomous vehicles and related issues, above all ethics, the automotive industry is already presenting AI-based products and services that may influence, in a direction or in another, our technological and societal futures. This paper focuses on virtual assistants, the personification of the car intelligence incorporating, among others, an algorithmic “brain”, a synthetic human “voice” and powerful sensor-based “senses”. Should virtual assistants just assist humans or replace them whenever necessary? Should their scope of action be limited to safety-related driving tasks or to any activity performed in the car or controlled from the car? Although at a very early stage of commercial development, the paper will review the state-of-the-art of in-car virtual assistants underlining their role and functions in the connected and automated driving ecosystem. By drawing from earlier reflections on automation, robots and intelligent agents, it will then identify a series of issues to be addressed by the scientific community, policy-makers and the automotive industry stakeholders.",https://ieeexplore.ieee.org/document/7972192/,2017 15th International Conference on ITS Telecommunications (ITST),29-31 May 2017,ieeexplore
10.1109/TVT.2020.3027352,A Nash Q-Learning Based Motion Decision Algorithm With Considering Interaction to Traffic Participants,IEEE,Journals,"In order to improve the efficiency and comfort of autonomous vehicles while ensuring safety, the decision algorithm needs to interact with human drivers, infer the most probable behavior and then makes advantageous decision. This paper proposes a Nash-Q learning based motion decision algorithm to consider the interaction. First, the local trajectory of surrounding vehicle is predicted by kinematic constraints, which can reflect the short-term motion trend. Then, the future action space is built based the predicted local trajectory that consists of five basis actions. With that, the Nash-Q learning process can be implemented by the game between these basis actions. By elimination of strictly dominated actions and the Lemke-Howson method, the autonomous vehicle can decide the optimal action and infer the behavior of surrounding vehicle. Finally, the lane merging scenario is built to test the performance contrast to the existing methods. The driver in loop experiment is further designed to verify the interaction performance in multi-vehicle traffic. The results show that the Nash-Q learning based algorithm can improve the efficiency and comfort by 15.75% and 20.71% to the Stackelberg game and the no-interaction method respectively while the safety is ensured. It can also make real-time interaction with human drivers in multi-vehicle traffic.",https://ieeexplore.ieee.org/document/9207975/,IEEE Transactions on Vehicular Technology,Nov. 2020,ieeexplore
10.1109/ACCESS.2020.2964029,A New Vehicular Fog Computing Architecture for Cooperative Sensing of Autonomous Driving,IEEE,Journals,"The sensing coverage and accuracy of vehicles are vital for autonomous driving. However, the current sensing capability of a single autonomous vehicle is quite limited in the complicated road traffic environment, which leads to many sensing dead zones or frequent misdetection. In this paper, we propose to develop a Vehicular Fog Computing (VFC) architecture to implement cooperative sensing among multiple adjacent vehicles driving in the form of a platoon. Based on our VFC architecture greedy and Support Vector Machine (SVM) algorithms are adopted respectively to enhance the sensing coverage and accuracy in the platoon. Furthermore, the distributed deep learning is processed for trajectory prediction by applying the Light Gated Recurrent Unit (Li-GRU) neural network algorithm. Simulation results based on real-world traffic datasets indicate the sensing coverage and accuracy by the proposed algorithms can be significantly improved with low computational complexity.",https://ieeexplore.ieee.org/document/8950168/,IEEE Access,2020,ieeexplore
10.1109/TFUZZ.2004.832532,Automatic design of fuzzy controllers for car-like autonomous robots,IEEE,Journals,"This paper describes the design and implementation of a fuzzy control system for a car-like autonomous vehicle. The problem addressed is the diagonal parking in a constrained space, a typical problem in motion control of nonholonomic robots. The architecture proposed for the fuzzy controller is a hierarchical scheme which combines seven modules working in series and in parallel. The rules of each module employ the adequate fuzzy operators for its task (making a decision or generating a smoothly varying control output), and they have been obtained from heuristic knowledge and numerical data (with geometric information) depending on the module requirements (some of them are constrained to provide paths of near-minimal lengths). The computer-aided design tools of the environment Xfuzzy 3.0 (developed by some of the authors) have been employed to automate the different design stages: 1) translation of heuristic knowledge into fuzzy rules; 2) extraction of fuzzy rules from numerical data and their tuning to give paths of near-minimal lengths; 3) offline verification of the control system behavior; and 4) its synthesis to be implemented in a true robot and be verified on line. Real experiments with the autonomous vehicle ROMEO 4R (designed and built at the Escuela Superior de Ingenieros, University of Seville, Seville, Spain) demonstrate the efficiency of the described controller and of the methodology followed in its design.",https://ieeexplore.ieee.org/document/1321074/,IEEE Transactions on Fuzzy Systems,Aug. 2004,ieeexplore
10.1109/TIV.2020.3029853,Autonomous Wary Collision Avoidance,IEEE,Journals,"Handling of critical situations is an important part in the architecture of an autonomous vehicle. A controller for autonomous collision avoidance is developed based on a wary strategy that assumes the least tire-road friction for which the maneuver is still feasible. Should the friction be greater, the controller makes use of this, and performs better. The controller uses an acceleration-vector reference obtained from optimal control of a friction-limited particle, whose applicability is verified by using numerical optimization on a full vehicle model. By employing an analytical tire model of the tire-road friction limit, to determine slip references for steering, and body-slip control, the result is a controller where the computation of its output is explicit, and independent of the actual tire-road friction. When evaluated in real-time on a high-fidelity simulation model, the developed controller performs close to that achieved by offline numerical optimization.",https://ieeexplore.ieee.org/document/9219194/,IEEE Transactions on Intelligent Vehicles,June 2021,ieeexplore
10.1109/TMC.2019.2892451,Deep CNN-Based Real-Time Traffic Light Detector for Self-Driving Vehicles,IEEE,Journals,"Due to the unavailability of Vehicle-to-Infrastructure (V2I) communication in current transportation systems, Traffic Light Detection (TLD) is still considered an important module in autonomous vehicles and Driver Assistance Systems (DAS). To overcome low flexibility and accuracy of vision-based heuristic algorithms and high power consumption of deep learning-based methods, we propose a lightweight and real-time traffic light detector for the autonomous vehicle platform. Our model consists of a heuristic candidate region selection module to identify all possible traffic lights, and a lightweight Convolution Neural Network (CNN) classifier to classify the results obtained. Offline simulations on the GPU server with the collected dataset and several public datasets show that our model achieves higher average accuracy and less time consumption. By integrating our detector module on NVidia Jetson TX1/TX2, we conduct on-road tests on two full-scale self-driving vehicle platforms (a car and a bus) in normal traffic conditions. Our model can achieve an average detection accuracy of 99.3 percent (mRttld) and 99.7 percent (Rttld) at 10Hz on TX1 and TX2, respectively. The on-road tests also show that our traffic light detection module can achieve &lt;; + 1:5m errors at stop lines when working with other selfdriving modules.",https://ieeexplore.ieee.org/document/8611202/,IEEE Transactions on Mobile Computing,1 Feb. 2020,ieeexplore
10.1109/LES.2020.3009910,Designing Neural Networks for Real-Time Systems,IEEE,Journals,"Artificial neural networks (ANNs) are increasingly being used within safety-critical cyber-physical systems (CPSs). It is important to validate both the timing and functional correctness of these systems. However, most approaches in the literature consider guaranteeing only the functionality of the ANN-based controllers. This issue stems largely from the implementation strategies used within common neural network frameworks-their underlying source code is often simply unsuitable for formal techniques such as static timing analysis. As a result, developers of safety-critical CPS must rely on informal techniques, such as measurement-based approaches, to prove correctness, techniques that provide weak guarantees at best. In this letter, we address this challenge. We propose a design pipeline whereby neural networks trained using the popular deep learning framework Keras are compiled to functionally equivalent C code. This C code is restricted to simple constructs that may be analyzed by the existing static timing analysis tools. As a result, if compiled to a suitable time-predictable platform, all execution bounds may be statically derived. To demonstrate the benefits of our approach, we execute an ANN trained to drive an autonomous vehicle around a race track. We compile the ANN to the Patmos time-predictable controller and show that we can derive the worst-case execution timings.",https://ieeexplore.ieee.org/document/9143184/,IEEE Embedded Systems Letters,Sept. 2021,ieeexplore
10.1109/ACCESS.2020.2970728,LoPECS: A Low-Power Edge Computing System for Real-Time Autonomous Driving Services,IEEE,Journals,"To simultaneously enable multiple autonomous driving services on affordable embedded systems, we designed and implemented LoPECS, a Low-Power Edge Computing System for real-time autonomous robots and vehicles services. The contributions of this paper are three-fold: first, we developed a Heterogeneity-Aware Runtime Layer to fully utilize vehicle's heterogeneous computing resources to fulfill the real-time requirement of autonomous driving applications; second, we developed a vehicle-edge Coordinator to dynamically offload vehicle tasks to edge cloudlet to further optimize user experience in the way of prolonged battery life; third, we successfully integrated these components into LoPECS system and implemented it on Nvidia Jetson TX1. To the best of our knowledge, this is the first complete edge computing system in a production autonomous vehicle. Our implementation on Nvidia Jetson demonstrated that it could successfully support multiple autonomous driving services with only 11 W of power consumption, and hence proves the effectiveness of the proposed LoPECS system.",https://ieeexplore.ieee.org/document/8977507/,IEEE Access,2020,ieeexplore
10.1109/41.704895,Modeling of ultrasonic range sensors for localization of autonomous mobile robots,IEEE,Journals,"This paper presents a probabilistic model of ultrasonic range sensors using backpropagation neural networks trained on experimental data. The sensor model provides the probability of detecting mapped obstacles in the environment, given their position and orientation relative to the transducer. The detection probability can be used to compute the location of an autonomous vehicle from those obstacles that are more likely to be detected. The neural network model is more accurate than other existing approaches, since it captures the typical multilobal detection pattern of ultrasonic transducers. Since the network size is kept small, implementation of the model on a mobile robot can be efficient for real-time navigation. An example that demonstrates how the credence could be incorporated into the extended Kalman filter (EKF) and the numerical values of the final neural network weights are provided in the appendices.",https://ieeexplore.ieee.org/document/704895/,IEEE Transactions on Industrial Electronics,Aug. 1998,ieeexplore
10.1109/TITS.2020.3045096,<italic>N</italic><sup>2</sup><italic>C</italic>: Neural Network Controller Design Using Behavioral Cloning,IEEE,Journals,"Modern vehicles communicate data to and from sensors, actuators, and electronic control units (ECUs) using Controller Area Network (CAN) bus, which operates on differential signaling. An autonomous ECU responsible for the execution of decision commands to an autonomous vehicle is developed by assimilating the information from the CAN bus. The conventional way of parsing the decision commands is motion planning, which uses a path tracking algorithm to evaluate the decision commands. This study focuses on designing a robust controller using behavioral cloning and motion planning of autonomous vehicle using a deep learning framework. In the first part of this study, we explore the pipeline of parsing decision commands from the path tracking algorithm to the controller and proposed a neural network-based controller ( N<sup>2</sup>C) using behavioral cloning. The proposed network predicts throttle, brake, and torque when trained with the manual driving data acquired from the CAN bus. The efficacy of the proposed method is demonstrated by comparing the accuracy with the Proportional-Derivative-Integral (PID) controller in conjunction with the path tracking algorithm (pure pursuit and model predictive control based path follower). The second part of this study complements N<sup>2</sup>C, in which an end-to-end neural network for predicting the speed and steering angle is proposed with image data as an input. The performance of the proposed frameworks are evaluated in real-time and on the Udacity dataset, showing better metric scores in the former and reliable prediction in the later case when compared with the state-of-the-art methods.",https://ieeexplore.ieee.org/document/9312433/,IEEE Transactions on Intelligent Transportation Systems,July 2021,ieeexplore
10.1109/ACCESS.2020.2982963,Trajectory Tracking Control Algorithm for Autonomous Vehicle Considering Cornering Characteristics,IEEE,Journals,"Trajectory tracking control is a key technology in the research and development of autonomous vehicles. With the aim of addressing problems such as low control accuracy and poor real-time performance, which can occur easily when an autonomous vehicle avoids obstacles, this research focuses on the trajectory tracking control algorithm for autonomous vehicle considering cornering characteristics. First, the vehicle dynamics model and tire model are established through appropriate simplification. Then, based on the basic principle of model predictive control, a linear time-varying model predictive controller (LTV MPC) that considers the cornering characteristics is designed and optimized. Finally, using CarSim and MATLAB/Simulink software, a joint simulation model is established and the trajectory tracking performance of the controlled vehicle under different vehicle speeds and road adhesion conditions are tested through simulation experiments in combination with the double-shift line reference trajectory. The simulation results show the LTV MPC controller that considers cornering characteristics has good self-adaptability under complicated and severe working conditions, and no cases, such as car sideslip or track departure, were observed. Compared with other controllers and algorithms, the designed trajectory tracking controller has remarkable comprehensive performance, exhibits superior robustness and anti-interference ability, and significant improvements in the trajectory tracking control accuracy and real-time performance. The proposed control algorithm is of great importance in improving the tracking stability and driving safety of autonomous vehicles under complex extreme conditions and conducive to the further development and improvement of the technological level of intelligent vehicle driving assistance.",https://ieeexplore.ieee.org/document/9045954/,IEEE Access,2020,ieeexplore
10.1109/TVCG.2019.2899227,You or Me? Personality Traits Predict Sacrificial Decisions in an Accident Situation,IEEE,Journals,"Emergency situations during car driving sometimes force the driver to make a sudden decision. Predicting these decisions will have important applications in updating risk analyses in insurance applications, but also can give insights for drafting autonomous vehicle guidelines. Studying such behavior in experimental settings, however, is limited by ethical issues as it would endanger peoples' lives. Here, we employed the potential of virtual reality (VR) to investigate decision-making in an extreme situation in which participants would have to sacrifice others in order to save themselves. In a VR driving simulation, participants first trained to complete a difficult course with multiple crossroads in which the wrong turn would lead the car to fall down a cliff. In the testing phase, obstacles suddenly appeared on the “safe” turn of a crossroad: for the control group, obstacles consisted of trees, whereas for the experimental group, they were pedestrians. In both groups, drivers had to decide between falling down the cliff or colliding with the obstacles. Results showed that differences in personality traits were able to predict this decision: in the experimental group, drivers who collided with the pedestrians had significantly higher psychopathy and impulsivity traits, whereas impulsivity alone was to some degree predictive in the control group. Other factors like heart rate differences, gender, video game expertise, and driving experience were not predictive of the emergency decision in either group. Our results show that self-interest related personality traits affect decision-making when choosing between preservation of self or others in extreme situations and showcase the potential of virtual reality in studying and modeling human decision-making.",https://ieeexplore.ieee.org/document/8651471/,IEEE Transactions on Visualization and Computer Graphics,May 2019,ieeexplore
10.1109/EEEIC/ICPSEurope49358.2020.9160705,ASDVC - A Self-Driving Vehicle Controller using Unsupervised Machine Learning,IEEE,Conferences,"ASDVC is a self-driving vehicle controller that uses unsupervised machine learning methods, namely clustering based k-means, hierarchical, Gaussian Matrix Model and self-organizing mapping to optimize the path the vehicle follows from the source to destination. The real-time optimal selection of the unsupervised machine learning based motion control algorithm could provide fast response times of under one microsecond during the lateral, longitudinal and angular motion control of the autonomous vehicle. However, it is shown that a simple selection of one of the machine learning methods may not guarantee the optimality of the results. The successful implementation of ASDVC controller in self-driving vehicles could have a significant contribution towards making mobility more reliable and sustainable for the future vehicular transportation systems.",https://ieeexplore.ieee.org/document/9160705/,2020 IEEE International Conference on Environment and Electrical Engineering and 2020 IEEE Industrial and Commercial Power Systems Europe (EEEIC / I&CPS Europe),9-12 June 2020,ieeexplore
10.1109/EEEIC/ICPSEurope49358.2020.9160705,ASDVC - A Self-Driving Vehicle Controller using Unsupervised Machine Learning,IEEE,Conferences,"ASDVC is a self-driving vehicle controller that uses unsupervised machine learning methods, namely clustering based k-means, hierarchical, Gaussian Matrix Model and self-organizing mapping to optimize the path the vehicle follows from the source to destination. The real-time optimal selection of the unsupervised machine learning based motion control algorithm could provide fast response times of under one microsecond during the lateral, longitudinal and angular motion control of the autonomous vehicle. However, it is shown that a simple selection of one of the machine learning methods may not guarantee the optimality of the results. The successful implementation of ASDVC controller in self-driving vehicles could have a significant contribution towards making mobility more reliable and sustainable for the future vehicular transportation systems.",https://ieeexplore.ieee.org/document/9160705/,2020 IEEE International Conference on Environment and Electrical Engineering and 2020 IEEE Industrial and Commercial Power Systems Europe (EEEIC / I&CPS Europe),9-12 June 2020,ieeexplore
10.1109/ICICCS51141.2021.9432186,Pothole and Object Detection for an Autonomous Vehicle Using YOLO,IEEE,Conferences,"Object Detection is an key software and a fundamental task for an autonomous driving system that provides remarkable change in computer vision. İn recent years, company's are planning to launch autonomous vehicle in an full swing that's the most important ascpets for object detection and one of most challenging task for locating specific object from from multiple objects in a specific scenario. The computer vision and machine learning algorithm is the important tool for detecting objects in and around the environment. In this paper, which consists of two parts The first part is implemented on object detection in the surrounding with Yolo (You Only Look Once)Algorithm provides exact classification and position which is configured on newly created datasets for classes of object: a car, a person, a truck, a bus, traffic light, motorcycle, pothole, wetland uses the Convolutional Neural Network and max-polling layer for prediction that improves detecting of small target and these deep learning technique provides a high accuracy for detecting real world. Detecting potholes in Indian road help the autonomous vehicle to move smoothly without getting struck in the potholes. In part two of the proposed method is implemented on Raspberry pi4 a popular embedded computer board explores suitability for the running objects. That solves the real world problems and improves the impact on detecting objects. Knowing pothole and wetland detection for self-driving vehicle is needed badly to solve the road lay problems like: accident, slowing down the transport system these are solved by deep learning.",https://ieeexplore.ieee.org/document/9432186/,2021 5th International Conference on Intelligent Computing and Control Systems (ICICCS),6-8 May 2021,ieeexplore
10.1109/ICIP.2019.8803189,Recognizing Chinese Texts with 3D Convolutional Neural Network,IEEE,Conferences,"In this paper, we propose a deep learning system to localize and recognize Chinese texts in scenes with signage and road marks through 3D convolutional neural network. The proposed system adopts YOLO for detecting target location and exploits 3D convolutional neural network for recognizing the contents. The proposed design outperforms the existing designs based on LSTM and achieves real-time processing performance, which is feasible to be implemented on embedded platforms. The proposed system reaches over 90% accuracy in recognizing Chinese texts on bird's-eye viewing road marks in a self-driving vehicle equipped with a fisheye camera. In addition, this system can achieve 20 fps execution speed with NVIDIA DIGITS DevBox with 1080Ti GPU, which is fast enough for autonomous driving applications.",https://ieeexplore.ieee.org/document/8803189/,2019 IEEE International Conference on Image Processing (ICIP),22-25 Sept. 2019,ieeexplore
10.1109/ICIEM51511.2021.9445285,Simulation based vehicle movement tracking using Kalman Filter algorithm for Autonomous vehicles,IEEE,Conferences,"In the domain of Software automotive industry, one of the most widely used algorithms for performing analysis of driving operations is the Kalman filter algorithm. In today's world of advanced machine learning, the Kalman filter remains an important tool to fuse measurements from several sensors to estimate the real time state of robotics systems such as a self-driving vehicle. Kalman filter is able to update an estimate of evolving nature of continuously changing states of the common filters to take a probabilistic estimate. The driving scenario results are updated in real time using 2-steps update and correction method. In this paper, we have described the process of Kalman filter and its variant to estimate about the detection of moving object in a given traffic scenario using advance toolboxes of MATLAB. Results have been shown for multiple changing parameters.",https://ieeexplore.ieee.org/document/9445285/,2021 2nd International Conference on Intelligent Engineering and Management (ICIEM),28-30 April 2021,ieeexplore
,Vision Control Unit in Fully Self Driving Vehicles using Xilinx MPSoC and Opensource Stack,IEEE,Conferences,"Fully self-driving (FSD) vehicles are becoming increasing popular over the last few years and companies are investing significantly into its research and development. In the recent years, FSD technology innovators like Tesla, Google etc. have been working on proprietary autonomous driving stacks and have been able to successfully bring the vehicle to the roads. On the other end, organizations like Autoware Foundation and Baidu are fueling the growth of self-driving mobility using open source stacks. These organizations firmly believe in enabling autonomous driving technology for everyone and support developing software stacks through the open source community that is SoC vendor agnostic. In this proposed solution we describe a vision control unit for a fully self-driving vehicle developed on Xilinx MPSoC platform using open source software components.The vision control unit of an FSD vehicle is responsible for camera video capture, image processing and rendering, AI algorithm processing, data and meta-data transfer to next stage of the FSD pipeline. In this proposed solution we have used many open source stacks and frameworks for video and AI processing. The processing of the video pipeline and algorithms take full advantage of the pipelining and parallelism using all the heterogenous cores of the Xilinx MPSoC. In addition, we have developed an extensible, scalable, adaptable and configurable AI backend framework, XTA, for acceleration purposes that is derived from a popular, open source AI backend framework, TVM-VTA. XTA uses all the MPSoC cores for its computation in a parallel and pipelined fashion. XTA also adapts to the compute and memory parameters of the system and can scale to achieve optimal performance for any given AI problem. The FSD system design is based on a distributed system architecture and uses open source components like Autoware for autonomous driving algorithms, ROS and Distributed Data Services as a messaging middleware between the functional nodes and a real-time kernel to coordinate the actions. The details of image capture, rendering and AI processing of the vision perception pipeline will be presented along with the performance measurements of the vision pipeline.In this proposed solution we will demonstrate some of the key use cases of vision perception unit like surround vision and object detection. In addition, we will also show the capability of Xilinx MPSoC technology to handle multiple channels of real time camera and the integration with the Lidar/Radar point cloud data to feed into the decision-making unit of the overall system. The system is also designed with the capability to update the vision control unit through Over the Air Update (OTA). It is also envisioned that the core AI engine will require regular updates with the latest training values; hence a built-in platform level mechanism supporting such capability is essential for real world deployment.",https://ieeexplore.ieee.org/document/9371624/,2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC),18-21 Jan. 2021,ieeexplore
10.1109/TMC.2019.2892451,Deep CNN-Based Real-Time Traffic Light Detector for Self-Driving Vehicles,IEEE,Journals,"Due to the unavailability of Vehicle-to-Infrastructure (V2I) communication in current transportation systems, Traffic Light Detection (TLD) is still considered an important module in autonomous vehicles and Driver Assistance Systems (DAS). To overcome low flexibility and accuracy of vision-based heuristic algorithms and high power consumption of deep learning-based methods, we propose a lightweight and real-time traffic light detector for the autonomous vehicle platform. Our model consists of a heuristic candidate region selection module to identify all possible traffic lights, and a lightweight Convolution Neural Network (CNN) classifier to classify the results obtained. Offline simulations on the GPU server with the collected dataset and several public datasets show that our model achieves higher average accuracy and less time consumption. By integrating our detector module on NVidia Jetson TX1/TX2, we conduct on-road tests on two full-scale self-driving vehicle platforms (a car and a bus) in normal traffic conditions. Our model can achieve an average detection accuracy of 99.3 percent (mRttld) and 99.7 percent (Rttld) at 10Hz on TX1 and TX2, respectively. The on-road tests also show that our traffic light detection module can achieve &lt;; + 1:5m errors at stop lines when working with other selfdriving modules.",https://ieeexplore.ieee.org/document/8611202/,IEEE Transactions on Mobile Computing,1 Feb. 2020,ieeexplore
10.1109/ICCCSP49186.2020.9315223,Artificial Intelligence based Self-Driving Car,IEEE,Conferences,"The paper proposes a self-driving car model also called autonomous, robotic or driver-less car is one that operates and navigates using its intelligence. The basic idea behind the paper is to develop a 1/10 scale RC car to portray an automated car. The model consists of the following software and hardware components such as CNN (Convolutional neural network), Monocular vision algorithm, Haar cascade classifier, Raspberry Pi Board model B+, Pi camera, Arduino, and an Ultrasonic sensor. The Pi camera and ultrasonic sensor are attached to the raspberry pi board to collect input images along with sensor data to stream these data to the server which in our case is the laptop. The (CNN) convolutional neural network running on the server will be used to enable lane detection to provide steering predictions that are left, right, forward based on the input image. The haar cascade classifier will be used to detect signals and stop sign and monocular vision algorithms to calculate distance from them. The ultrasonic sensor will be used for front collision avoidance by stopping the car at a certain distance before the obstacle ahead. The navigation commands such as right, left, forward, stop will be sent to the car through Arduino which is connected to the RC car's remote, this will make the car drive autonomously based on neural network predictions and some hard coded rules. Thus this model will enable autonomous driving by self-navigation via lane detection, stopping at detection of stop signs, red lights and moving on green signal and front collision avoidance in a cost-effective manner. Hardcoded rules, obstacle avoidance mechanisms, machine learning models, and smart object discrimination will help the system follow traffic rules and navigate the car using artificial intelligence.",https://ieeexplore.ieee.org/document/9315223/,"2020 4th International Conference on Computer, Communication and Signal Processing (ICCCSP)",28-29 Sept. 2020,ieeexplore
10.1109/SMC.2014.6974391,Predicting dynamic computational workload of a self-driving car,IEEE,Conferences,"This study aims at developing a method that predicts the CPU usage patterns of software tasks running on a self-driving car. To ensure safety of such dynamic systems, the worst-case-based CPU utilization analysis has been used; however, the nature of dynamically changing driving contexts requires more flexible approach for an efficient computing resource management. To better understand the dynamic CPU usage patterns, this paper presents an effort of designing a feature vector to represent the information of driving environments and of predicting, using regression methods, the selected tasks' CPU usage patterns given specific driving contexts. Experiments with real-world vehicle data show a promising result and validate the usefulness of the proposed method.",https://ieeexplore.ieee.org/document/6974391/,"2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2014,ieeexplore
10.1109/GTSD.2018.8595590,Real-Time Self-Driving Car Navigation Using Deep Neural Network,IEEE,Conferences,"In this paper, a monocular vision-based self-driving car prototype using Deep Neural Network on Raspberry Pi is proposed. Self-driving cars are one of the most increasing interests in recent years as the definitely developing relevant hardware and software technologies toward fully autonomous driving capability with no human intervention. Level-3/4 autonomous vehicles are potentially turning into a reality in near future. Convolutional Neural Networks (CNNs) have been shown to achieve significant performance in various perception and control tasks in comparison to other techniques in the latest years. The key factors behind these impressive results are their ability to learn millions of parameters using a large amount of labeled data. In this work, we concentrate on finding a model that directly maps raw input images to a predicted steering angle as output using a deep neural network. The technical contributions of this work are two-fold. First, the CNN model parameters were trained by using data collected from vehicle platform built with a 1/10 scale RC car, Raspberry Pi 3 Model B computer and front-facing camera. The training data were road images paired with the time-synchronized steering angle generated by manually driving. Second, road tests the model on Raspberry to drive itself in the outdoor environment around oval-shaped and 8-shaped with traffic sign lined track. The experimental results demonstrate the effectiveness and robustness of autopilot model in lane keeping task. Vehicle's top speed is about 5-6km/h in a wide variety of driving conditions, regardless of whether lane markings are present or not.",https://ieeexplore.ieee.org/document/8595590/,2018 4th International Conference on Green Technology and Sustainable Development (GTSD),23-24 Nov. 2018,ieeexplore
10.1109/ICSSE52999.2021.9538460,Steering Angle Estimation for Self-driving Car Based on Enhanced Semantic Segmentation,IEEE,Conferences,"Common approaches for semantic segmentation using Convolutional Neural Networks (CNN) based around conventional U-shapes architectures were widely used. However, failures to retrieve global context information and memory issues made such models unable to compete against modern architectures considering accuracy and real-time capability. In this paper, an efficient method maintaining equivalent accuracy of a previous segmentation network and skillfully making a model more light-weighted for real-time inference was proposed. More concretely, we managed to alleviate five out of 17 million trainable parameters, which effectively reduce the amount of computation of the original PSPNet by 30% using the backbone of CSPNet. Our proposed network implementation achieved 73 mIoU scores on our custom dataset and reached 15 fps regarding real-time inference. We deployed the trained model on the multifunctional hardware and then connected it to a golf car to jointly navigate the natural environment and traffic sign detection task. Accordingly, the STM32 board and servo motor were used for controlling the steering wheel through a track-and-wheel drive system. As for the traffic sign detection task, we employed a small-size Yolov5 trained on the TT100K dataset running around 60fps and attained real-time performance with sufficient accuracy.",https://ieeexplore.ieee.org/document/9538460/,2021 International Conference on System Science and Engineering (ICSSE),26-28 Aug. 2021,ieeexplore
10.1109/AIVR.2018.00062,A Combination of Feedback Control and Vision-Based Deep Learning Mechanism for Guiding Self-Driving Cars,IEEE,Conferences,"The purpose of this paper is to develop an agent that can imitate the behavior of humans driving a car. When human beings driving a car, he/she majorly uses vision system to recognize the states of the car, including the position, velocity, and the surrounding environments. In this paper, we implemented a self-driving car which can drive itself on the track of a simulator. The self-driving car uses deep neural network as a computational framework to ""learn"" what is the position of the car related to the road. While the car understands the position of itself related to the track, it can use the information as a basis for feedback control.",https://ieeexplore.ieee.org/document/8613681/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore
10.1109/ISPA/IUCC.2017.00167,A cGANs-Based Scene Reconstruction Model Using Lidar Point Cloud,IEEE,Conferences,"Road scene reconstruction is a fundamental and crucial module at the perception phase for autonomous vehicles, and will influence the later phase, such as object detection, motion planing and path planing. Traditionally, self-driving car uses Lidar, camera or fusion of the two kinds of sensors for sensing the environment. However, single Lidar or camera-based approaches will miss crucial information, and the fusion-based approaches often consume huge computing resources. We firstly propose a conditional Generative Adversarial Networks (cGANs)-based deep learning model that can rebuild rich semantic scene images from upsampled Lidar point clouds only. This makes it possible to remove cameras to reduce resource consumption and improve the processing rate. Simulation on the KITTI dataset also demonstrates that our model can reestablish color imagery from a single Lidar point cloud, and is effective enough for real time sensing on autonomous driving vehicles.",https://ieeexplore.ieee.org/document/8367397/,2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC),12-15 Dec. 2017,ieeexplore
10.1109/ICCCSP49186.2020.9315223,Artificial Intelligence based Self-Driving Car,IEEE,Conferences,"The paper proposes a self-driving car model also called autonomous, robotic or driver-less car is one that operates and navigates using its intelligence. The basic idea behind the paper is to develop a 1/10 scale RC car to portray an automated car. The model consists of the following software and hardware components such as CNN (Convolutional neural network), Monocular vision algorithm, Haar cascade classifier, Raspberry Pi Board model B+, Pi camera, Arduino, and an Ultrasonic sensor. The Pi camera and ultrasonic sensor are attached to the raspberry pi board to collect input images along with sensor data to stream these data to the server which in our case is the laptop. The (CNN) convolutional neural network running on the server will be used to enable lane detection to provide steering predictions that are left, right, forward based on the input image. The haar cascade classifier will be used to detect signals and stop sign and monocular vision algorithms to calculate distance from them. The ultrasonic sensor will be used for front collision avoidance by stopping the car at a certain distance before the obstacle ahead. The navigation commands such as right, left, forward, stop will be sent to the car through Arduino which is connected to the RC car's remote, this will make the car drive autonomously based on neural network predictions and some hard coded rules. Thus this model will enable autonomous driving by self-navigation via lane detection, stopping at detection of stop signs, red lights and moving on green signal and front collision avoidance in a cost-effective manner. Hardcoded rules, obstacle avoidance mechanisms, machine learning models, and smart object discrimination will help the system follow traffic rules and navigate the car using artificial intelligence.",https://ieeexplore.ieee.org/document/9315223/,"2020 4th International Conference on Computer, Communication and Signal Processing (ICCCSP)",28-29 Sept. 2020,ieeexplore
10.1109/IJCNN.2019.8851947,Bio-Inspired Foveated Technique for Augmented-Range Vehicle Detection Using Deep Neural Networks,IEEE,Conferences,"We propose a bio-inspired foveated technique to detect cars in a long range camera view using a deep convolutional neural network (DCNN) for the IARA self-driving car. The DCNN receives as input (i) an image, which is captured by a camera installed on IARA's roof; and (ii) crops of the image, which are centered in the waypoints computed by IARA's path planner and whose sizes increase with the distance from IARA. We employ an overlap filter to discard detections of the same car in different crops of the same image based on the percentage of overlap of detections' bounding boxes. We evaluated the performance of the proposed augmented-range vehicle detection system (ARVDS) using the hardware and software infrastructure available in the IARA self-driving car. Using IARA, we captured thousands of images of real traffic situations containing cars in a long range. Experimental results show that ARVDS increases the Average Precision (AP) of long range car detection from 29.51% (using a single whole image) to 63.15%.",https://ieeexplore.ieee.org/document/8851947/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
10.1109/ICIIP47207.2019.8985977,Comparative Analysis and Implementation of Different Human Detection Techniques,IEEE,Conferences,"Object Detection is used in making several smart surveillance applications which are used in detecting and tracking suspicious activities. Object Detection also plays a very crucial role in several latest inventions like the Google self-driving car. Over the past decade several image processing algorithms have evolved which have been used for human detection after preprocessing of the images. But in several cases these algorithms were found to be less accurate and more time taking. With the advent of the era of Machine Learning, the focus has gradually shifted towards computer vision and deep learning methodologies for human detection. Deep learning algorithms are far more accurate than the traditional methodologies as they employ feature extraction from the images followed by classification according to the dataset provided to them, thus enabling more accurate detection than their ancestors. Nevertheless its also true that some deep learning algorithms like the different versions of R-CNN and YOLO take much more processing time than the traditional methodologies. In our paper we have compared the performance of some traditional and some deep learning algorithms in different scenarios.",https://ieeexplore.ieee.org/document/8985977/,2019 Fifth International Conference on Image Information Processing (ICIIP),15-17 Nov. 2019,ieeexplore
10.1145/3238147.3238187,DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving Systems,IEEE,Conferences,"While Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness. In this paper, we propose DeepRoad, an unsupervised DNN-based framework for automatically testing the consistency of DNN-based autonomous driving systems and online validation. First, DeepRoad automatically synthesizes large amounts of diverse driving scenes without using image transformation rules (e.g. scale, shear and rotation). In particular, DeepRoad is able to produce driving scenes with various weather conditions (including those with rather extreme conditions) by applying Generative Adversarial Networks (GANs) along with the corresponding real-world weather scenes. Second, DeepRoad utilizes metamorphic testing techniques to check the consistency of such systems using synthetic images. Third, DeepRoad validates input images for DNN-based systems by measuring the distance of the input and training images using their VGGNet features. We implement DeepRoad to test three well-recognized DNN-based autonomous driving systems in Udacity self-driving car challenge. The experimental results demonstrate that DeepRoad can detect thousands of inconsistent behaviors for these systems, and effectively validate input images to potentially enhance the system robustness as well.",https://ieeexplore.ieee.org/document/9000040/,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),3-7 Sept. 2018,ieeexplore
10.1145/3180155.3180220,DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars,IEEE,Conferences,"Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads. However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases. In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.",https://ieeexplore.ieee.org/document/8453089/,2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),27 May-3 June 2018,ieeexplore
10.1109/INFOCOM.2019.8737614,Dynamic Adaptive DNN Surgery for Inference Acceleration on the Edge,IEEE,Conferences,"Recent advances in deep neural networks (DNNs) have substantially improved the accuracy and speed of a variety of intelligent applications. Nevertheless, one obstacle is that DNN inference imposes heavy computation burden to end devices, but offloading inference tasks to the cloud causes transmission of a large volume of data. Motivated by the fact that the data size of some intermediate DNN layers is significantly smaller than that of raw input data, we design the DNN surgery, which allows partitioned DNN processed at both the edge and cloud while limiting the data transmission. The challenge is twofold: (1) Network dynamics substantially influence the performance of DNN partition, and (2) State-of-the-art DNNs are characterized by a directed acyclic graph (DAG) rather than a chain so that partition is greatly complicated. In order to solve the issues, we design a Dynamic Adaptive DNN Surgery (DADS) scheme, which optimally partitions the DNN under different network condition. Under the lightly loaded condition, DNN Surgery Light (DSL) is developed, which minimizes the overall delay to process one frame. The minimization problem is equivalent to a min-cut problem so that a globally optimal solution is derived. In the heavily loaded condition, DNN Surgery Heavy (DSH) is developed, with the objective to maximize throughput. However, the problem is NP-hard so that DSH resorts an approximation method to achieve an approximation ratio of 3. Real-world prototype based on self-driving car video dataset is implemented, showing that compared with executing entire the DNN on the edge and cloud, DADS can improve latency up to 6.45 and 8.08 times respectively, and improve throughput up to 8.31 and 14.01 times respectively.",https://ieeexplore.ieee.org/document/8737614/,IEEE INFOCOM 2019 - IEEE Conference on Computer Communications,29 April-2 May 2019,ieeexplore
10.1109/ICCE-Berlin.2018.8576190,End to End Learning based Self-Driving using JacintoNet,IEEE,Conferences,"Automated driving functions, like highway driving and parking assist, are getting increasing deployed in high-end cars with the trend moving towards the self-driving car. With the advent of deep learning, many traditional computer vision techniques have been replaced by deep convolutional neural networks (CNN). End to end learning is one of the paradigm for self-driving, in which user provides a input images from the front facing camera to the given neural network and the network outputs the car control signals such as throttle, steering and braking. The paper proposes an embedded friendly convolutional neural network, `Jacintonet', to demonstrate self-driving using end to end learning paradigm in a virtual simulation environment. Paper discusses key learning during the training methodology and presents the results on embedded platform. Texas Instruments (TI) TDA2x System on Chip (SoC) is used as embedded platform for running `Jacintonet', real-time to demonstrate self-driving car in the virtual simulator.",https://ieeexplore.ieee.org/document/8576190/,2018 IEEE 8th International Conference on Consumer Electronics - Berlin (ICCE-Berlin),2-5 Sept. 2018,ieeexplore
10.1109/SysCon48628.2021.9447097,Experimental Validation of a Steering Control System using an Adaptive Fuzzy Controller and Computer Vision,IEEE,Conferences,"This paper proposes an adaptive steering control strategy for self-driving cars based on a Fuzzy Expert System and Reinforcement Learning. Our objective consists in deriving an appropriate control law directly from a real vehicle that allows it to navigate on several types of lanes, by controlling the position in relation to the center of the tracks and also the translation speed of the vehicle. Using an on-line Reinforcement Learning approach, the Fuzzy expert controller is derived considering the coupling and non-linearity of the model on straight and winding tracks. To do this, an embedded camera captures the images and sends them to the computer vision algorithm responsible for performing tracks detection and recognition. From that, the control references which indicate the navigation path and direction on the lane are calculated. The main contribution of this work is to apply an online reinforcement learning approach to tune and optimize the fuzzy steering controller while the vehicle navigates through different routes. Using a real vehicle equipped with an embedded computer and also the implemented web user interface, the learning evolution of the adaptive fuzzy controller can be managed remotely during trial in actual environments. Experimental results showed that the learned fuzzy expert controller controls the self-driving car during the path tracking and precisely performs the execution of different maneuvers.",https://ieeexplore.ieee.org/document/9447097/,2021 IEEE International Systems Conference (SysCon),15 April-15 May 2021,ieeexplore
10.1109/ISQED.2018.8357325,Low cost and power CNN/deep learning solution for automated driving,IEEE,Conferences,"Automated driving functions, like highway driving and parking assist, are increasingly getting deployed in high-end cars with the ultimate goal of realizing self-driving car using Deep learning techniques like convolution neural network (CNN). For mass-market deployment, the embedded solution is required to address the right cost and performance envelope along with security and safety. In the case of automated driving, one of the key functionality is “finding drivable free space”, which is addressed using deep learning techniques like CNN. These CNN networks pose huge computing requirements in terms of hundreds of GOPS/TOPS (Giga or Tera operations per second), which seems beyond the capability of today's embedded SoC. This paper covers various techniques consisting of fixed-point conversion, sparse multiplication, fusing of layers and network pruning, for tailoring on the embedded solution. These techniques are implemented on the device by means of optimized Deep learning library for inference. The paper concludes by demonstrating the results of a CNN network running in real time on TI's TDA2X embedded platform producing a high-quality drivable space output for automated driving.",https://ieeexplore.ieee.org/document/8357325/,2018 19th International Symposium on Quality Electronic Design (ISQED),13-14 March 2018,ieeexplore
10.1109/ITSC.2018.8569665,Monocular Fisheye Camera Depth Estimation Using Sparse LiDAR Supervision,IEEE,Conferences,"Near-field depth estimation around a self-driving car is an important function that can be achieved by four wide-angle fisheye cameras having a field of view of over 180°. Depth estimation based on convolutional neural networks (CNNs) produce state of the art results, but progress is hindered because depth annotation cannot be obtained manually. Synthetic datasets are commonly used but they have limitations. For instance, they do not capture the extensive variability in the appearance of objects like vehicles present in real datasets. There is also a domain shift while performing inference on natural images illustrated by many attempts to handle the domain adaptation explicitly. In this work, we explore an alternate approach of training using sparse LiDAR data as ground truth for depth estimation for fisheye camera. We built our own dataset using our self-driving car setup which has a 64-beam Velodyne LiDAR and four wide angle fisheye cameras. To handle the difference in view-points of LiDAR and fisheye camera, an occlusion resolution mechanism was implemented. We started with Eigen's multiscale convolutional network architecture [1] and improved by modifying activation function and optimizer. We obtained promising results on our dataset with RMSE errors comparable to the state-of-the-art results obtained on KITTI.",https://ieeexplore.ieee.org/document/8569665/,2018 21st International Conference on Intelligent Transportation Systems (ITSC),4-7 Nov. 2018,ieeexplore
10.1109/SMC.2014.6974391,Predicting dynamic computational workload of a self-driving car,IEEE,Conferences,"This study aims at developing a method that predicts the CPU usage patterns of software tasks running on a self-driving car. To ensure safety of such dynamic systems, the worst-case-based CPU utilization analysis has been used; however, the nature of dynamically changing driving contexts requires more flexible approach for an efficient computing resource management. To better understand the dynamic CPU usage patterns, this paper presents an effort of designing a feature vector to represent the information of driving environments and of predicting, using regression methods, the selected tasks' CPU usage patterns given specific driving contexts. Experiments with real-world vehicle data show a promising result and validate the usefulness of the proposed method.",https://ieeexplore.ieee.org/document/6974391/,"2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2014,ieeexplore
10.1109/SAMI50585.2021.9378655,Psychophysiological modelling of trust in technology: Comparative analysis of algorithm ensemble methods,IEEE,Conferences,"Measuring user's trust in technology in real-time using psychophysiological signals depends on the availability of stable, accurate, variance sensitive, and non-bias trust classifier model which can be achieved through ensembling several algorithms. Prior efforts resulted to fairly accurate but unstable models. This article investigates what ensemble method is most suitable for developing an ensemble trust classifier model for assessing users trust in technology with psychophysiological signals. Using a self-driving car game, a within subject four condition experiment was implemented. During which 31 participant were involved, and multimodal psychophysiological data (EEG, ECG, EDA, and Facial-EMG) were recorded. An exhaustive 172 features from time and frequency domain were extracted. Six carefully selected algorithms were combined for developing ensemble trust classifier models using each of the four ensemble methods (voting, bagging, stacking, boosting). The result indicated that the Stack ensemble method was more superior, despite voting method dominating prior studies.",https://ieeexplore.ieee.org/document/9378655/,2021 IEEE 19th World Symposium on Applied Machine Intelligence and Informatics (SAMI),21-23 Jan. 2021,ieeexplore
10.1109/GTSD.2018.8595590,Real-Time Self-Driving Car Navigation Using Deep Neural Network,IEEE,Conferences,"In this paper, a monocular vision-based self-driving car prototype using Deep Neural Network on Raspberry Pi is proposed. Self-driving cars are one of the most increasing interests in recent years as the definitely developing relevant hardware and software technologies toward fully autonomous driving capability with no human intervention. Level-3/4 autonomous vehicles are potentially turning into a reality in near future. Convolutional Neural Networks (CNNs) have been shown to achieve significant performance in various perception and control tasks in comparison to other techniques in the latest years. The key factors behind these impressive results are their ability to learn millions of parameters using a large amount of labeled data. In this work, we concentrate on finding a model that directly maps raw input images to a predicted steering angle as output using a deep neural network. The technical contributions of this work are two-fold. First, the CNN model parameters were trained by using data collected from vehicle platform built with a 1/10 scale RC car, Raspberry Pi 3 Model B computer and front-facing camera. The training data were road images paired with the time-synchronized steering angle generated by manually driving. Second, road tests the model on Raspberry to drive itself in the outdoor environment around oval-shaped and 8-shaped with traffic sign lined track. The experimental results demonstrate the effectiveness and robustness of autopilot model in lane keeping task. Vehicle's top speed is about 5-6km/h in a wide variety of driving conditions, regardless of whether lane markings are present or not.",https://ieeexplore.ieee.org/document/8595590/,2018 4th International Conference on Green Technology and Sustainable Development (GTSD),23-24 Nov. 2018,ieeexplore
10.1109/ACMI53878.2021.9528185,Speed Bump &amp; Pothole Detection with Single Shot MultiBox Detector Algorithm &amp; Speed Control for Autonomous Vehicle,IEEE,Conferences,"The development of self-driving cars has always been an extensive research field for the automobile industry. To make a capable self-driving car, many challenges need to be resolved. Detection of the road condition is one of them. This paper focuses on a particular part-detection of speed bumps and potholes using a camera and analyzing the video feed with the help of artificial intelligence. To solve this problem a popular and lightweight algorithm, SSD (Single Shot Multibox Detector) is used. This is an optimal choice because of being lightweight and also accurate enough to run on mobile devices and to use in real-life situations. For detecting speed bumps and potholes, a dataset has been created based on the road structure of Bangladesh as the main priority of this system is to work on the local environment. Raspberry Pi has been used as the main processing unit because of being small but powerful. A warning system has been implemented so that it can warn the onboard driver about the upcoming pothole or speed bump. This system can also send a signal to the speed controller unit of the car to reduce the speed on detection to avoid accidents or damages to the car. The speed control unit is a microcontroller-based system that uses an ATmega328 microcontroller and L298 motor driver. This paper summarizes the combination of an artificial intelligence-based detection system injunction with a microcontroller-based speed control system in a cost-effective way that can be used in building self-driving cars.",https://ieeexplore.ieee.org/document/9528185/,"2021 International Conference on Automation, Control and Mechatronics for Industry 4.0 (ACMI)",8-9 July 2021,ieeexplore
10.1109/Ubi-Media.2019.00014,The Matter of Deep Reinforcement Learning Towards Practical AI Applications,IEEE,Conferences,"Reinforcement Learning (RL) is an extraordinarily paradigm that aims to solve a complex problem. This technique leverages the traditional feedforward networks with temporal-difference learning to overcome supervised and unsupervised real-world problems. However, RL is one of state-of-the-art topic due to the opaque aspects in design and implementation. Also, in which situation we will get performance gain from RL is still unclear. Therefore, This study firstly examines the impact of Experience Replay in Deep Q-Learning agent with Self-Driving Car application. Secondly, The impact of Eligibility Trace in RNN A3C agents with Breakout AI game application is studied. Our results indicated that these two techniques enhance RL performance by more than 20 percent as compared with traditional RL methods.",https://ieeexplore.ieee.org/document/9049567/,2019 Twelfth International Conference on Ubi-Media Computing (Ubi-Media),5-8 Aug. 2019,ieeexplore
10.1109/ICAMIMIA47173.2019.9223365,Autonomous Car Simulation Using Evolutionary Neural Network Algorithm,IEEE,Conferences,"Automation with artificial intelligence (AI) has widely implemented in robotics, transportation and manufacture. AI has become a powerful technology that change human life and help human more flexible doing something. In this paper, it will show a result of simulation from an autonomous car using the evolutionary neural network algorithm which combines genetic algorithm and neural network. The purpose of the simulation is to test the model that we develop to know the right direction based on the track, so the evolutionary neural network that implemented to the autonomous car be able to deliver the best solution before it implements in the real machine or car technology. Genetic algorithm combines with a neural network to reach an evolution condition. The evolution process is achieved through crossover, mutation and selection process, so the algorithm will give the best result from the iteration of the experiment. The result of our experiment shows that evolutionary neural network algorithm give the best result within 3 layer architecture, with iteration average is 14.5 reach finish point (check point) 3 in the track simulation. Based on the simulation, our car model can find out the right direction.",https://ieeexplore.ieee.org/document/9223365/,"2019 International Conference on Advanced Mechatronics, Intelligent Manufacture and Industrial Automation (ICAMIMIA)",9-10 Oct. 2019,ieeexplore
10.1109/ICST46399.2020.00019,Comparing Offline and Online Testing of Deep Neural Networks: An Autonomous Car Case Study,IEEE,Conferences,"There is a growing body of research on developing testing techniques for Deep Neural Networks (DNNs). We distinguish two general modes of testing for DNNs: Offline testing where DNNs are tested as individual units based on test datasets obtained independently from the DNNs under test, and online testing where DNNs are embedded into a specific application and tested in a close-loop mode in interaction with the application environment. In addition, we identify two sources for generating test datasets for DNNs: Datasets obtained from real-life and datasets generated by simulators. While offline testing can be used with datasets obtained from either sources, online testing is largely confined to using simulators since online testing within real-life applications can be time consuming, expensive and dangerous. In this paper, we study the following two important questions aiming to compare test datasets and testing modes for DNNs: First, can we use simulator-generated data as a reliable substitute to real-world data for the purpose of DNN testing? Second, how do online and offline testing results differ and complement each other? Though these questions are generally relevant to all autonomous systems, we study them in the context of automated driving systems where, as study subjects, we use DNNs automating end-to-end control of cars' steering actuators. Our results show that simulator-generated datasets are able to yield DNN prediction errors that are similar to those obtained by testing DNNs with real-life datasets. Further, offline testing is more optimistic than online testing as many safety violations identified by online testing could not be identified by offline testing, while large prediction errors generated by offline testing always led to severe safety violations detectable by online testing.",https://ieeexplore.ieee.org/document/9159088/,"2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)",24-28 Oct. 2020,ieeexplore
10.1109/ASYU50717.2020.9259830,Real-Time Implementation of Mini Autonomous Car Based on MobileNet - Single Shot Detector,IEEE,Conferences,"In this paper, in order to realize a prototype of an autonomous vehicle, we present a framework that consists of convolutional neural networks and image processing methods. The study is comprised of two main parts as software and hardware. In the hardware part, a small-sized smart video car kit is used as the prototype of the autonomous car. This programmable tool consists of Raspberry Pi, servo motors and a USB webcam whose angle of vision is equal to 120°. In the software part, we propose an algorithm in which we use Convolutional Neural Networks to detect the objects (vehicles, pedestrians, and traffic signs) and Hough transformation to detect the road lanes. Based on the outputs of the object and lane detections, the system decides the speed and the direction of the car in real-time. In our results, the vehicle performs autonomous driving in the scaled real-world application.",https://ieeexplore.ieee.org/document/9259830/,2020 Innovations in Intelligent Systems and Applications Conference (ASYU),15-17 Oct. 2020,ieeexplore
10.1109/TNNLS.2019.2892327,A Novel Dual Successive Projection-Based Model-Free Adaptive Control Method and Application to an Autonomous Car,IEEE,Journals,"In this paper, a novel model-free adaptive control (MFAC) algorithm based on a dual successive projection (DuSP)-MFAC method is proposed, and it is analyzed using the introduced DuSP method and the symmetrically similar structures of the controller and its parameter estimator of MFAC. Then, the proposed DuSP-MFAC scheme is successfully implemented in an autonomous car “Ruilong” for the lateral tracking control problem via converting the trajectory tracking problem into a stabilization problem by using the proposed preview-deviation-yaw angle. This MFAC-based lateral tracking control method was tested and demonstrated satisfactory performance on real roads in Fengtai, Beijing, China, and through successful participation in the Chinese Smart Car Future Challenge Competition held in 2015 and 2016.",https://ieeexplore.ieee.org/document/8641438/,IEEE Transactions on Neural Networks and Learning Systems,Nov. 2019,ieeexplore
10.1109/LRA.2021.3097345,Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement Learning,IEEE,Journals,"Autonomous car racing is a challenging task in the robotic control area. Traditional modular methods require accurate mapping, localization and planning, which makes them computationally inefficient and sensitive to environmental changes. Recently, deep-learning-based end-to-end systems have shown promising results for autonomous driving/racing. However, they are commonly implemented by supervised imitation learning (IL), which suffers from the distribution mismatch problem, or by reinforcement learning (RL), which requires a huge amount of risky interaction data. In this work, we present a general deep imitative reinforcement learning approach (DIRL), which successfully achieves agile autonomous racing using visual inputs. The driving knowledge is acquired from both IL and model-based RL, where the agent can learn from human teachers as well as perform self-improvement by safely interacting with an offline world model. We validate our algorithm both in a high-fidelity driving simulation and on a real-world 1/20-scale RC-car with limited onboard computation. The evaluation results demonstrate that our method outperforms previous IL and RL methods in terms of sample efficiency and task performance. Demonstration videos are available at https://caipeide.github.io/autorace-dirl/.",https://ieeexplore.ieee.org/document/9488179/,IEEE Robotics and Automation Letters,Oct. 2021,ieeexplore
10.1109/CRV.2018.00024,A Hierarchical Deep Architecture and Mini-batch Selection Method for Joint Traffic Sign and Light Detection,IEEE,Conferences,"Traffic light and sign detectors on autonomous cars are integral for road scene perception. The literature is abundant with deep learning networks that detect either lights or signs, not both, which makes them unsuitable for real-life deployment due to the limited graphics processing unit (GPU) memory and power available on embedded systems. The root cause of this issue is that no public dataset contains both traffic light and sign labels, which leads to difficulties in developing a joint detection framework. We present a deep hierarchical architecture in conjunction with a mini-batch proposal selection mechanism that allows a network to detect both traffic lights and signs from training on separate traffic light and sign datasets. Our method solves the overlapping issue where instances from one dataset are not labelled in the other dataset. We are the first to present a network that performs joint detection on traffic lights and signs. We measure our network on the Tsinghua-Tencent 100K benchmark for traffic sign detection and the Bosch Small Traffic Lights benchmark for traffic light detection and show it outperforms the existing Bosch Small Traffic light state-of-the-art method. We focus on autonomous car deployment and show our network is more suitable than others because of its low memory footprint and real-time image processing time. Qualitative results can be viewed at https://youtu.be/ YmogPzBXOw.",https://ieeexplore.ieee.org/document/8575742/,2018 15th Conference on Computer and Robot Vision (CRV),8-10 May 2018,ieeexplore
10.1109/I-SMAC.2018.8653736,A Novel Design of Autonomous Cars using IoT and Visual Features,IEEE,Conferences,"Autonomous car is a ground vehicle that is capable of driving without user interference. Traffic congestion and number of collisions are major issues in road traffic control due to rapid increase day-by-day. Autonomous cars provide a solution to this problem in an efficient and economical way. The proposed system utilizes mathematical models like neural networks and image processing techniques to sense the environment. This is implemented as three major components: curved road detection (steering), road sign and signal detection and obstacle detection (collision avoidance). Back Propagation is used for steering control with detection of curved roads; Haar features are used for road signal, sign detection and a distance sensor for collision avoidance. Data collected from the sensors is sent to a server for processing. Based on the result, a command is sent to the car. A GPS module attached to the car identifies the location of the car and with the help of a 3rd party location service, route to destination is identified and directions are sent to the car. Wireless networks are used to transmit data between sensors and the server. Python scripts are used to control and integrate all the units together. The designed system can attain high accuracy with real - time constraints.",https://ieeexplore.ieee.org/document/8653736/,"2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2018 2nd International Conference on",30-31 Aug. 2018,ieeexplore
10.1109/ICAMIMIA47173.2019.9223365,Autonomous Car Simulation Using Evolutionary Neural Network Algorithm,IEEE,Conferences,"Automation with artificial intelligence (AI) has widely implemented in robotics, transportation and manufacture. AI has become a powerful technology that change human life and help human more flexible doing something. In this paper, it will show a result of simulation from an autonomous car using the evolutionary neural network algorithm which combines genetic algorithm and neural network. The purpose of the simulation is to test the model that we develop to know the right direction based on the track, so the evolutionary neural network that implemented to the autonomous car be able to deliver the best solution before it implements in the real machine or car technology. Genetic algorithm combines with a neural network to reach an evolution condition. The evolution process is achieved through crossover, mutation and selection process, so the algorithm will give the best result from the iteration of the experiment. The result of our experiment shows that evolutionary neural network algorithm give the best result within 3 layer architecture, with iteration average is 14.5 reach finish point (check point) 3 in the track simulation. Based on the simulation, our car model can find out the right direction.",https://ieeexplore.ieee.org/document/9223365/,"2019 International Conference on Advanced Mechatronics, Intelligent Manufacture and Industrial Automation (ICAMIMIA)",9-10 Oct. 2019,ieeexplore
10.1109/IRC.2019.00073,Deep Grid Net (DGN): A Deep Learning System for Real-Time Driving Context Understanding,IEEE,Conferences,"Grid maps obtained from fused sensory information are nowadays among the most popular approaches for motion planning for autonomous driving cars. In this paper, we introduce Deep Grid Net (DGN), a deep learning (DL) system designed for understanding the context in which an autonomous car is driving. DGN incorporates a learned driving environment representation based on Occupancy Grids (OG) obtained from raw Lidar data and constructed on top of the Dempster-Shafer (DS) theory. The predicted driving context is further used for switching between different driving strategies implemented within EB robinos, Elektrobit's Autonomous Driving (AD) software platform. Based on genetic algorithms (GAs), we also propose a neuroevolutionary approach for learning the tuning hyperparameters of DGN. The performance of the proposed deep network has been evaluated against similar competing driving context estimation classifiers.",https://ieeexplore.ieee.org/document/8675636/,2019 Third IEEE International Conference on Robotic Computing (IRC),25-27 Feb. 2019,ieeexplore
10.1109/ICCE.2018.8326285,Deep learning in low-power stereo vision accelerator for automotive,IEEE,Conferences,"Various forms of Convolutional Neural Network (CNN) architectures are used as Deep Learning (DL) tools for learning the similarity measure on video patches in order to run the stereo matching algorithm - the most computationally intensive stage of the pipeline for the stereo vision function used in designing an autonomous car. We propose a hybrid system implementation of the algorithm for real-time, low-power and high-temperature environment. The accelerator part of our system is a programmable many-core system with a Map-Reduce Architecture. The paper describes and evaluates the proposed accelerator for different versions of the stereo matching algorithm.",https://ieeexplore.ieee.org/document/8326285/,2018 IEEE International Conference on Consumer Electronics (ICCE),12-14 Jan. 2018,ieeexplore
10.1109/ICUAS48674.2020.9213884,Image-Based Sense and Avoid of Small Scale UAV Using Deep Learning Approach,IEEE,Conferences,"Distance detection of target object is an important information for obstacle avoidance in many fields, such as autonomous car. When the distance of the obstacle is calculated, one can determine the potential risk of collision. In this paper, a monocular camera was utilized to get the distance from an incoming unmanned aerial vehicle (UAV) using deep learning approach. The distance detection of an UAV using You Only Look Once (YOLO) object detector was proposed in this study. The region which contain the detected UAV was processed into 100 by 100 pixel and was input into the proposed model to estimate the distance of the target object. For the proposed model, a Convolutional Neural Network (CNN) was adopted to solve the regression problem. First, the feature extraction based on VGG network was performed, and then its results was applied to the distance network to estimate distance. Finally, Kalman filter was used to improve the object tracking when YOLO detector is not able to detect UAV and to smooth the estimated distance. The proposed model was trained only by using synthetic images from animation software and was validated by using both synthetic and real flight videos.",https://ieeexplore.ieee.org/document/9213884/,2020 International Conference on Unmanned Aircraft Systems (ICUAS),1-4 Sept. 2020,ieeexplore
10.1109/IVS.2002.1187941,Pattern matching as the nucleus for either autonomous driving or driver assistance systems,IEEE,Conferences,"Concerns autonomous vehicle driving by pattern matching combined with reinforcement learning. In specific, this research focuses on the requirement to steer an autonomous car along a curvy and hilly road course with no intersections and no other vehicle or obstacle but with the strict requirement to self-improve driving behaviour. A camera is used to build quickly an abstract complete description (ACSD) of vehicle's current situation. This combines traditional edge finding operators with a new technique of Bayes prediction for each part of the video image. Those ACSD's are being stored together with the steering commands issued at that time and serve as the pattern database of possible driving behaviour which are being retrieved using an approximate nearest neighbour pattern matching algorithm with a O(n log m) characteristic compared to O(n/spl middot/m) for the conventional nearest neighbour calculation. In addition to this, any feedback on the quality or appropriateness of the driving behaviour has to be self-created (e.g. time measurement for a whole road section) and is therefore delayed and unspecific in relation to single issued steering commands. Consequently, a machine learning algorithm coping with those conditions is being implemented based on Reinforcement Learning.",https://ieeexplore.ieee.org/document/1187941/,"Intelligent Vehicle Symposium, 2002. IEEE",17-21 June 2002,ieeexplore
10.1109/ASYU50717.2020.9259830,Real-Time Implementation of Mini Autonomous Car Based on MobileNet - Single Shot Detector,IEEE,Conferences,"In this paper, in order to realize a prototype of an autonomous vehicle, we present a framework that consists of convolutional neural networks and image processing methods. The study is comprised of two main parts as software and hardware. In the hardware part, a small-sized smart video car kit is used as the prototype of the autonomous car. This programmable tool consists of Raspberry Pi, servo motors and a USB webcam whose angle of vision is equal to 120°. In the software part, we propose an algorithm in which we use Convolutional Neural Networks to detect the objects (vehicles, pedestrians, and traffic signs) and Hough transformation to detect the road lanes. Based on the outputs of the object and lane detections, the system decides the speed and the direction of the car in real-time. In our results, the vehicle performs autonomous driving in the scaled real-world application.",https://ieeexplore.ieee.org/document/9259830/,2020 Innovations in Intelligent Systems and Applications Conference (ASYU),15-17 Oct. 2020,ieeexplore
10.23919/ICCAS50221.2020.9268343,Robust Traffic Light Detection and Classification Under Day and Night Conditions,IEEE,Conferences,"Recently, traffic light detection and classification systems have been studied and developed to build an autonomous car by many research institutes, universities, and companies. However, the results of existing traffic light detection systems are still not stable under day and night conditions. It is difficult to detect the location of traffic light due to their small size. Moreover, traffic lights' shapes are also similar to advertisement lights in a city road. Therefore, this paper proposed a new approach to improve the performance of existing traffic light detection systems by using the benefits of hand-crafted features and deep learning techniques. Experimental results show that the proposed system obtained the detection rate of 80% under night conditions, while the color-based density method only got the detection rate of 50.43% under night conditions.",https://ieeexplore.ieee.org/document/9268343/,"2020 20th International Conference on Control, Automation and Systems (ICCAS)",13-16 Oct. 2020,ieeexplore
10.1109/ICAIS50930.2021.9396046,SSLA Based Traffic Sign and Lane Detection for Autonomous cars,IEEE,Conferences,"The Self-Driving Cars are also known as Autonomous Vehicles. This Car has the ability to sense around the environment. These sensed parameters are processed and according to it the different actuators in the car will work without any human involvement. An Autonomous car work like a normal car but without any human driver. Autonomous cars rely on sensors, actuators, machine learning algorithms and Software to perform all the Automated Functions. The Software part is very important for Autonomous vehicles. The Software architecture acts as a bridge between Hardware Components and Application. The Standardized Software for Automotive cars is AUTOSAR. The AUTOSAR is a Standardized Architecture between Application Software and Hardware. This Standardized Architecture provide all Communication Interfaces, Device Drivers, Basic Software and Run-Time Environment. There are two important modules in Self-Driving Cars. They are Lane Detection and Traffic Signal detection which works automatically without any Human Intervention. A Machine Learning Algorithm is proposed in this paper. This Algorithm is mainly used to train the shape models and helps to detect the shape for Traffic Sign detection and Lane Detection. These both tasks are programmed using python with Open cv2 library file, numpy library file and Hough Detection technique is used to detect the appropriate circles of the traffic signals.By using all these tools, all the shape models are trained using Supervised training Algorithm and the detection is performed in such a way to help Autonomous cars to detect the lane and traffic Sign.",https://ieeexplore.ieee.org/document/9396046/,2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS),25-27 March 2021,ieeexplore
10.1109/INMIC.2018.8595684,Self-Driving Cars Using CNN and Q-Learning,IEEE,Conferences,"DrivingMatter is an experiment carried out to understand the deeper side of an autonomous car. In 1900s, idea was to drive car on Moon from Earth. This was initial motivation which grew from there and now expanding to complex system of roads in the real world. A book-sized Raspberry Pi based autonomous car is built to carry out the experiment on hardware. Software side was accomplished by developing a Python based library for controlling and communicating with car over a network or locally within the car. For environment learning two methodologies are practiced; Supervised learning: Drove the car on an environment/road and collected 3, 000+ data-points. Based on this a CNN model was trained which achieved 73 % test 89 % train accuracy. Reinforcement learning: Car is trained for three different road signs; Stop, No left, and Traffic light using DQN with existing CNN model. These road signs are detected in the environment using OpenCV cascade classifiers.",https://ieeexplore.ieee.org/document/8595684/,2018 IEEE 21st International Multi-Topic Conference (INMIC),1-2 Nov. 2018,ieeexplore
10.1109/ITSC.2015.291,Sensor-Based Control with Digital Maps Association for Global Navigation: A Real Application for Autonomous Vehicles,IEEE,Conferences,"This paper presents a sensor-based control strategy applied in the global navigation of autonomous vehicles in urban environments. Typically, sensor-based control performs local navigation tasks regarding some features perceived from the environment. However, when there is more than one possibility to go, like in road intersection, the vehicle control fails to accomplish its global navigation. In order to solve this problem, we propose the vehicle global navigation based on a topological representation of the environment using only digital maps and low cost sensors. The approach was developed for two main tasks: road following and road intersection maneuvers. The final solution was completely implemented in a real autonomous car and tested in a challenge circuit, showing the viability of our solution.",https://ieeexplore.ieee.org/document/7313383/,2015 IEEE 18th International Conference on Intelligent Transportation Systems,15-18 Sept. 2015,ieeexplore
10.1109/UEMCON.2018.8796749,Smart City Software Revolution - Blackboard Systems for Smart City Solutions,IEEE,Conferences,"This paper examines the possibilities and requirements of using the blackboard software system in the context of a smart city. Using Unreal Engine 4, a virtual smart city was developed to test the capabilities and difficulties which would accompany the use of the blackboard. A number of simulations were run to generate insight regarding the viability of implementing the blackboard in a real-world setting. The tasks that the blackboard attempted addressed the following goals: a weather reaction system; a parking space availability system; and a daylight reaction system. An autonomous car running on preset routes was implemented in two separate ways to compare the performance between a blackboard system and blueprints.",https://ieeexplore.ieee.org/document/8796749/,"2018 9th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",8-10 Nov. 2018,ieeexplore
10.1109/InertialSensors.2018.8577143,Visual lnertial Hybridization Technique based on Beacons identified by Deep Learning,IEEE,Conferences,"Safran has been working for several years on autonomy of vehicles. Whether it is airborne with the UAV Patroller, or on the ground with the military vehicle eRider and with the civilian autonomous car in cooperation with Valeo. This paper focuses on the use of visual information to improve the localization of the car, more precisely, it presents, from a theoretical point of view, the hybridization of the inertial sensors measurement with the line of sight of known position visual beacons: road signs detected on the camera image thanks to deep learning. This fusion algorithm has been implemented in a prototype version of Safran's well know Epsilon 10 navigator and simulation results as well as first real-time results are presented.",https://ieeexplore.ieee.org/document/8577143/,2018 DGON Inertial Sensors and Systems (ISS),11-12 Sept. 2018,ieeexplore
10.1145/3125503.3125568,Work-in-progress: testing autonomous cyber-physical systems using fuzzing features from convolutional neural networks,IEEE,Conferences,Autonomous cyber-physical systems rely on modern machine learning methods such as deep neural networks to control their interactions with the physical world. Testing of such intelligent cyberphysical systems is a challenge due to the huge state space associated with high-resolution visual sensory inputs. We demonstrate how fuzzing the input using patterns obtained from the convolutional filters of an unrelated convolutional neural network can be used to test computer vision algorithms implemented in intelligent cyber-physical systems. Our method discovers interesting counterexamples to a pedestrian detection algorithm implemented in the popular OpenCV library. Our approach also unearths counterexamples to the correct behavior of an autonomous car similar to NVIDIA's end-to-end self-driving deep neural net running on the Udacity open-source simulator.,https://ieeexplore.ieee.org/document/8094374/,2017 International Conference on Embedded Software (EMSOFT),15-20 Oct. 2017,ieeexplore
10.1109/TNNLS.2019.2892327,A Novel Dual Successive Projection-Based Model-Free Adaptive Control Method and Application to an Autonomous Car,IEEE,Journals,"In this paper, a novel model-free adaptive control (MFAC) algorithm based on a dual successive projection (DuSP)-MFAC method is proposed, and it is analyzed using the introduced DuSP method and the symmetrically similar structures of the controller and its parameter estimator of MFAC. Then, the proposed DuSP-MFAC scheme is successfully implemented in an autonomous car “Ruilong” for the lateral tracking control problem via converting the trajectory tracking problem into a stabilization problem by using the proposed preview-deviation-yaw angle. This MFAC-based lateral tracking control method was tested and demonstrated satisfactory performance on real roads in Fengtai, Beijing, China, and through successful participation in the Chinese Smart Car Future Challenge Competition held in 2015 and 2016.",https://ieeexplore.ieee.org/document/8641438/,IEEE Transactions on Neural Networks and Learning Systems,Nov. 2019,ieeexplore
10.1109/LRA.2021.3063992,Real-Time Path Planning With Virtual Magnetic Fields,IEEE,Journals,"Humans and animals have learned or evolved to use magnetic fields for navigation. Knowing how to model and estimate these fields can be used for motion planning. However, computing the propagation of electromagnetic fields in a given environment requires solving complex differential equations with advanced numerical methods, and therefore it is not suitable for real-time decision making. In this latter, we present a real-time approximator for Maxwell's equations based on deep neural networks that predicts the distribution of a virtual magnetic field. We show how our approximator can be used to perform autonomous 2D navigation tasks, outperforming state-of-the-art navigation algorithms, ensuring completeness, and providing a near-optimal path up to 200 times per second without any post processing stage. We demonstrate the effectiveness of our method with physics-based simulations of an unmanned aerial vehicle, an autonomous car, as well as real-world experiments using a small off-road autonomous racing vehicle. Furthermore, we show how the approach can be applied to multi-robot systems, video game technology, and can be extended to 3D problems.",https://ieeexplore.ieee.org/document/9369851/,IEEE Robotics and Automation Letters,April 2021,ieeexplore
10.1109/LRA.2021.3097345,Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement Learning,IEEE,Journals,"Autonomous car racing is a challenging task in the robotic control area. Traditional modular methods require accurate mapping, localization and planning, which makes them computationally inefficient and sensitive to environmental changes. Recently, deep-learning-based end-to-end systems have shown promising results for autonomous driving/racing. However, they are commonly implemented by supervised imitation learning (IL), which suffers from the distribution mismatch problem, or by reinforcement learning (RL), which requires a huge amount of risky interaction data. In this work, we present a general deep imitative reinforcement learning approach (DIRL), which successfully achieves agile autonomous racing using visual inputs. The driving knowledge is acquired from both IL and model-based RL, where the agent can learn from human teachers as well as perform self-improvement by safely interacting with an offline world model. We validate our algorithm both in a high-fidelity driving simulation and on a real-world 1/20-scale RC-car with limited onboard computation. The evaluation results demonstrate that our method outperforms previous IL and RL methods in terms of sample efficiency and task performance. Demonstration videos are available at https://caipeide.github.io/autorace-dirl/.",https://ieeexplore.ieee.org/document/9488179/,IEEE Robotics and Automation Letters,Oct. 2021,ieeexplore
10.1109/SYSOSE.2017.7994953,Autonomous decision making for a driver-less car,IEEE,Conferences,"Autonomous driving has been a hot topic with companies like Google, Uber, and Tesla because of the complexity of the problem, seemingly endless applications, and capital gain. The technology's brain child is DARPA's autonomous urban challenge from over a decade ago. Few companies have had some success in applying algorithms to commercial cars. These algorithms range from classical control approaches to Deep Learning. In this paper, we will use Deep Learning techniques and the Tensor flow framework with the goal of navigating a driverless car through an urban environment. The novelty in this system is the use of Deep Learning vs. traditional methods of real-time autonomous operation as well as the application of the Tensorflow framework itself. This paper provides an implementation of AlexNet's Deep Learning model for identifying driving indicators, how to implement them in a real system, and any unforeseen drawbacks to these techniques and how these are minimized and overcome.",https://ieeexplore.ieee.org/document/7994953/,2017 12th System of Systems Engineering Conference (SoSE),18-21 June 2017,ieeexplore
10.23919/IConAC.2018.8748986,"Improvement of Driverless Cars' Passengers on Board Health and Safety, using Low-Cost Real-Time Heart Rate Monitoring System",IEEE,Conferences,"In this work, a real-time unobtrusive heart rate monitoring system is proposed and implemented. The proposed system aims to monitor the heart rate of the passengers by using a low-cost camera, which can be readily embedded in the car's rear-view mirror. Additionally, we integrate this system with the main system of our test driverless car, and we propose how driverless cars should act in response to serious medical emergency situations. Moreover, we investigate how this system can benefit from the promising features of Google I/O and Google AI. Our approach is based on Remote Photoplethysmography (rPPG), in which the heart rate is extracted from the subtle tiny changes occurring in the skin color of the face during every pulsation. The face is automatically detected and tracked, then the raw signal is calculated from each frame over a 10-seconds sliding window. After that, a series of signal processing techniques are implemented on the raw signals to recover the heart rate frequency. Finally, the resultant heart rate measurements are processed and stored, then we compare it with ground truth measurements values obtained using pulse oximeter.",https://ieeexplore.ieee.org/document/8748986/,2018 24th International Conference on Automation and Computing (ICAC),6-7 Sept. 2018,ieeexplore
10.1109/AFRICON46755.2019.9134033,A Software Agent for Vehicle Driver Modeling,IEEE,Conferences,"The world is experiencing a paradigm shift towards intelligent agents in form of machine learning for modeling any given task or process. Human vehicle drivers are agents that operate under stochastic environments, full of other agents. Such environments are complex to perceive and model. This study explores how a utility-based agent could be used to model human vehicle drivers. The motivation behind the study was established on the assumption that a driver agent founded on GPS data, Mixture Models and probabilistic reasoning methodologies could effectively model human vehicle drivers. The data collected by GPS receivers was appropriately analysed to establish a driver behaviour dataset. The dataset was then divided into three sets: training, test and validation sets that were used to formulate the driver agent. The agent's successive actions were evaluated against sets of performance metrics to determine accuracy, precision and recall levels. The evaluation yielded over 50% successful performance rates at all levels. The significance of the study is four-fold: First, the function of the system could be extended to providing advisory services to drivers in real-time. Second, data gathered from the system could be used by road safety stakeholders to vet drivers and to diagnose causes of road accidents. Thirdly, the resulting knowledge-base could establish standards of rationality in driving and/or formulate rules for use in driverless vehicle control systems. Finally, the model could be used to build a dataset on driver behaviour for any given vehicle driver and type and nature of operational environment.",https://ieeexplore.ieee.org/document/9134033/,2019 IEEE AFRICON,25-27 Sept. 2019,ieeexplore
10.1109/WCNC.2019.8886037,Pedestrian Detection for Autonomous Driving within Cooperative Communication System,IEEE,Conferences,"The ability to perceive and understand surrounding road-users behaviors is crucial for self-driving vehicles to correctly plan reliable reactions. Computer vision that relies mostly on machine learning techniques enables autonomous vehicles to perform several required tasks such as pedestrian detection. Furthermore, within a fully autonomous driving environment, driverless vehicle has to communicate and share perceived data with its neighboring vehicles for more safe navigation. In this context, our paper proposes a warning notification diffusion solution related to real-time pedestrian presence detection, through an inter-vehicle communication system. To achieve this purpose, pedestrian and vehicle recognition is required. Thus, we implemented intended detectors. We used Histogram of Oriented Gradients (HOG) descriptor with the linear Support Vector Machine (SVM) classifier for the pedestrian detector, and Haar feature-based cascade classifier to reach vehicle detection. The performance evaluation of our solution leads to fairly good detection accuracy around 90% for pedestrian and 88% for vehicle.",https://ieeexplore.ieee.org/document/8886037/,2019 IEEE Wireless Communications and Networking Conference (WCNC),15-18 April 2019,ieeexplore
10.23919/FRUCT52173.2021.9435505,Development of the Detecting System of the Landmark Tags to Increase the Navigation Accuracy of an Unmanned Vehicle in a Known Location,IEEE,Conferences,This paper proposes the landmark detection system for use in unmanned vehicle based on NVIDIA Jetson embedded device. The paper describes the synthetic dataset generation and training YOLOv4 neural network using data augmentation and transfer learning techniques to detect the landmark in incomplete data conditions. The real-time landmark detection module was implemented on the NVIDIA Jetson Xavier. The final mean average precision was resulted as 83.8%.,https://ieeexplore.ieee.org/document/9435505/,2021 29th Conference of Open Innovations Association (FRUCT),12-14 May 2021,ieeexplore
10.1109/ACC.2016.7525529,A Supervised Adaptive Learning-based Fuzzy Controller for a non-linear vehicle system using Neural Network Identification,IEEE,Conferences,"In this paper, a Supervised Adaptive Learning-based Fuzzy Controller (ALFC) with Neural Network Identification and Convex Parameterization is designed to identify and control the unmanned vehicle in an autonomous parking system. The objective is to achieve robust learning and control while maintaining a low implementation cost. The proposed algorithm design incorporates the following learning and control theorems - non-linear system identification using neural network, fuzzy logic, supervised adaptive learning as well as multiple model based convex parameterization. To demonstrate the algorithm in a more straight forward manner, we are using a real nonlinear unmanned autonomous driving system as an example to apply the algorithm and showing the superior performance of controller. In the autonomous driving system, the proposed method can be used for both estimating and further controlling a desired vehicle speed and steering wheel turning. With a supervised adaptive learning-based method, robustness can be also assured under various operating environments regardless of unpredictable disturbances. The convex parameterization further improves the speed of convergence of the adaptive learning process for the Fuzzy controller by using the multiple models concept. Last but not least, comparative experiments have also demonstrated that systems equipped with the new algorithm are able to achieve faster and smoother convergence.",https://ieeexplore.ieee.org/document/7525529/,2016 American Control Conference (ACC),6-8 July 2016,ieeexplore
10.23919/FRUCT52173.2021.9435505,Development of the Detecting System of the Landmark Tags to Increase the Navigation Accuracy of an Unmanned Vehicle in a Known Location,IEEE,Conferences,This paper proposes the landmark detection system for use in unmanned vehicle based on NVIDIA Jetson embedded device. The paper describes the synthetic dataset generation and training YOLOv4 neural network using data augmentation and transfer learning techniques to detect the landmark in incomplete data conditions. The real-time landmark detection module was implemented on the NVIDIA Jetson Xavier. The final mean average precision was resulted as 83.8%.,https://ieeexplore.ieee.org/document/9435505/,2021 29th Conference of Open Innovations Association (FRUCT),12-14 May 2021,ieeexplore
10.1109/AERO.2018.8396807,Learning safe recovery trajectories with deep neural networks for unmanned aerial vehicles,IEEE,Conferences,"Unmanned vehicles that use vision sensors for perception to aid autonomous flight are a highly popular area of research. However, these systems are often prone to failures that are often hard to model. Previous work has focused on using deep learning to detect these failures. In this work, we build on these failure detection systems and develop a pipeline that learns to identify the correct trajectory to execute that restores the vision system and the unmanned vehicle to a safe state. The key challenge with using a deep learning pipeline for this problem is the limited amount of training data available from a real world system. Ideally one requires millions of data points to sufficiently train a model from scratch. However, this is not feasible for an unmanned aerial vehicle. The dataset we operate with is limited to 400-500 points. To sufficiently learn from such a small dataset we leverage the idea of transfer learning and non linear dimensionality reduction. We deploy our pipeline on an unmanned aerial vehicle flying autonomously through outdoor clutter (in a GPS denied environment) and show that we are able to achieve long durations of safe autonomous flight.",https://ieeexplore.ieee.org/document/8396807/,2018 IEEE Aerospace Conference,3-10 March 2018,ieeexplore
10.1109/CICA.2009.4982774,Tutorial CICA-T Computing with intelligence for identification and control of nonlinear systems,IEEE,Conferences,"System characterization and identification are fundamental problems in systems theory and play a major role in the design of controllers. System identification and nonlinear control has been proposed and implemented using intelligent systems such as neural networks, fuzzy logic, reinforcement learning, artificial immune system and many others using inverse models, direct/indirect adaptive, or cloning a linear controller. Adaptive Critic Designs (ACDs) are neural networks capable of optimization over time under conditions of noise and uncertainty. The ACD technique develops optimal control laws using two networks - critic and action. There are merits for each approach adopted will be presented. The primary aim of this tutorial is to provide control and system engineers/researchers from industry/academia, new to the field of computational intelligence with the fundamentals required to benefit from and contribute to the rapidly growing field of computational intelligence and its real world applications, including identification and control of power and energy systems, unmanned vehicle navigation, signal and image processing, and evolvable and adaptive hardware systems.",https://ieeexplore.ieee.org/document/4982774/,2009 IEEE Symposium on Computational Intelligence in Control and Automation,30 March-2 April 2009,ieeexplore
10.1109/ACCESS.2021.3061634,RnR: Retrieval and Reprojection Learning Model for Camera Localization,IEEE,Journals,"Camera localization is an essential technique in many applications, such as robot navigation, mixed reality, and unmanned vehicle. We are committed to solving the problem of predicting the 6-DoF pose of cameras from a single color image in a given three-Dimensional (3D) environment. In this paper, we proposed a robust learning model for it. Basically, our proposed methodology consists of two steps: image retrieval and space reprojection. The former is in charge of simultaneous localization and mapping based on pre-captured reference images that rely on the correspondence between pixel points and scene coordinates; whereas the latter carries out camera calibration between the 2D image plane and the 3D scene. Given a two-Dimensional (2D) image, the initial localization is accomplished rapidly by matching a reference image using Siamese networks. More precise localization is achieved by camera calibration between the 2D image and the 3D scene using a fully convolutional network. The experimental results on the public dataset show that our model is more robust and expandable than the previous methods. At the end of this paper, we also apply the system to Unmanned Aerial Vehicle (UAV) localization and achieve good results.",https://ieeexplore.ieee.org/document/9361658/,IEEE Access,2021,ieeexplore
10.1109/ITAIC.2011.6030219,A redeployment strategy based on Unmanned Aerial Vehicle in wireless sensor network,IEEE,Conferences,"In reality, holes in large-scale sensor networks are exist due to various reasons, such as energy depletion and randomness deployment. These holes may degrade the detection performance of the entire sensor networks. Based on the probabilistic detection model with false alarm rate, this paper proposes a new redeployment method. A method of using an Unmanned Aerial Vehicle (UAV) was proposed for boundaries of holes detection. In particular, in this paper, the contour graph was used for redeploy the holes based on depth-first strategy. According to the simulation results, the proposed method can attain better coverage rate.",https://ieeexplore.ieee.org/document/6030219/,2011 6th IEEE Joint International Information Technology and Artificial Intelligence Conference,20-22 Aug. 2011,ieeexplore
10.1109/ICE2T.2017.8215979,Application framework for forest surveillance and data acquisition using unmanned aerial vehicle system,IEEE,Conferences,"An application framework is proposed in this paper that considers low cost surveillance mechanism and data acquisition in the forest. An application is developed as proof of concept with detailed design that can take advantage of unmanned urban vehicle to be directly configured and controlled in real-time. The advantages are numerous; it can be used for many purposes. For example, it can be used for observing critical and important area for intruder activities or to know the current state of any object of interest. We considered using machine learning and image processing and can be used for species of trees in the forest by color and size detection. A separate service running on separate remote server will be responsible for this. We have proposed a application framework particularly to be cheap and targeted to be easy to handle by non-technical persons and that it does not require large software system knowledge like Pix4D or DroneDeploy. This system will be useful for operations and research specially the forestry and palm oil plantation surveillance, and sustainable timber industry that specially needs carefully collected imageries and data from objects. Collection of raw data from sensor networks is also proposed in the system architecture.",https://ieeexplore.ieee.org/document/8215979/,2017 International Conference on Engineering Technology and Technopreneurship (ICE2T),18-20 Sept. 2017,ieeexplore
10.1109/UVS.2019.8658283,Automatic Fault Detection of Power Lines using Unmanned Aerial Vehicle (UAV),IEEE,Conferences,"Safety and automation are the two major challenges in the application of Unmanned Aerial Vehicle (UAV), commonly known as drone, to the power lines inspection and fault detection. While current state-of-the-art UAVs are equipped with collision avoidance features, there is less attention to the automatic and real-time fault detection of power lines using UAVs. This paper presents the architecture of three drone-oriented concept designs for automatic and real-time fault detection of power lines using UAVs. The proposed systems could be potential candidates for replacing traditional inspection methods of power lines, which are risky and costly. By incorporating a robust neural network, i.e., Artificial Intelligence (AI), and using appropriate and efficient sensors, the systems can automatically detect various faults and defects on power lines with high precision. We propose three concept design options comprised of different hardware/software components and their feasibility factors. For instance, FLIR Duo Pro R as a thermal sensor and Zenmuse XT for thermal vision have been proposed to be used in the concept designs. For data communication, the proposed designs use cloud-based virtual private network (VPN) for a secure connection between remote control (RC) of the UAV and the server. Based on the advantages and disadvantages of the three proposed design options, the most efficient design is also discussed. This design proposes a system with lightweight sensors, which could increase the flight time of the UAV. Further, the AI interface is coded on to the RC, making it economical, without any database for big data storage. The back-end of the neural network is stored in a cloud server. With the help of GSM antenna, the AI can run on the tablet if there is an available cellular network.",https://ieeexplore.ieee.org/document/8658283/,2019 1st International Conference on Unmanned Vehicle Systems-Oman (UVS),5-7 Feb. 2019,ieeexplore
10.1109/CIMCA.2005.1631523,Autonomous Landing of an Unmanned Aerial Vehicle,IEEE,Conferences,"In this paper, the design and implementation of a real-time landing algorithm for an autonomous helicopter is presented. The helicopter uses an onboard acquisition system to obtain the GPS and the sonar data to update its landing parameters. To control the path during the land step a fuzzy logic controller located in the fixed station is used. This controller is included in a low level module of a hierarchical-based control structure. Experimental results from flight tests in the field are presented, demonstrating that this control algorithm is accurate and robust",https://ieeexplore.ieee.org/document/1631523/,"International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)",28-30 Nov. 2005,ieeexplore
10.1109/SSCI.2018.8628656,Brain Emotional Learning-Based Path Planning and Intelligent Control Co-Design for Unmanned Aerial Vehicle in Presence of System Uncertainties and Dynamic Environment,IEEE,Conferences,"This paper proposes a novel intelligent path planning and control co-design for Unmanned Aerial Vehicles (UAVs) in the presence of system uncertainties and dynamic environments. In order to simultaneously handle the uncertainties from both the UAV platform itself and from the environment, a novel biologically-inspired approach based on a computational model of emotional learning in mammalian limbic system is adopted. The methodology, known as Brain Emotional Learning (BEL), is implemented in this application for the first time. Making use of the multi-objective properties and the real-time learning capabilities of BEL, the path planning and control co-design are applied in a synthetic UAV path planning scenario, successfully dealing with the challenges caused by system uncertainties and dynamic environments. A Lyapunov analysis demonstrates the convergence of the co-design, and a set of numerical results illustrate the effectiveness of the proposed approach. Furthermore, it is shown that the low computational complexity of the method guarantees its implementation in real-time applications.",https://ieeexplore.ieee.org/document/8628656/,2018 IEEE Symposium Series on Computational Intelligence (SSCI),18-21 Nov. 2018,ieeexplore
10.1109/SNPD.2007.475,Double Unmanned Aerial Vehicle's Path Planning for Scout via Cross-Entropy Method,IEEE,Conferences,"Cross-entropy has been recently applied to combinatorial optimization problems with satisfying results. This paper introduce the cross-entropy method theory, the way of model making, real time and robust, and the application, in military scout , Unmanned Aerial Vehicle(UAV) is a special tool, with so many advantage in real time and global space. A new method, based on cross-entropy method, was used for the optimal way of double UAV's path planning.",https://ieeexplore.ieee.org/document/4287760/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore
10.1109/ECAI.2018.8679050,Ground Control Station for an Unmanned Aerial Vehicle Integrated in IoT,IEEE,Conferences,The paper presents the Murros Unmanned Aerial System (UAS) and its integration in IoT. Murros UAS is a multisensorial robotic system used for aerial monitoring of wireless sensors network in precision agriculture. This document details the software and hardware architecture of Murros UAS Ground Control Station and common situations when we are operating in an over-the-internet configuration. Experimental results are obtained from real missions.,https://ieeexplore.ieee.org/document/8679050/,"2018 10th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",28-30 June 2018,ieeexplore
10.1109/ComPE49325.2020.9200107,Inspection of Concrete Structures by a Computer Vision Technique and an Unmanned Aerial Vehicle,IEEE,Conferences,"We have proposed a visual inspection technique for concrete structures using deep learning and a hardware ecosystem, an Unmanned Aerial Vehicle (UAV). The UAV is a quadcopter that can fly to unreachable sections of a site which consists of a camera that captures images of the concrete surfaces via a mobile device and feed the real time images in the CNN model. The images taken from such remote locations may contain different types of surfaces, shadowed regions and surfaces with holes. The cracks are properly detected by the CNN `AlexNet' algorithm and masking with sliding window technique in such conditions due to variation in the image data set. The experimental results were simulated on a standard online data set of 40,000 images of Mendeley Data which is freely available and 3000 images have been chosen from the entire data set for this method. The classes have been divided into 2 categories of `crack' and `no crack' for the proposed method's data set. There are 1050 training images and 450 testing images for each category. Experimental results were achieved on Google Colab cloud service using Python Tensorflow API (Application Programming Interface). The proposed `AlexNet' CNN algorithm achieves 98.4 % accuracy and the model is deployed to a masking technique with sliding window to detect cracks in a 3008×2000 pixel resolution image by breaking the image into 227×227 pixel resolution image patches. The experimental results have proved that the proposed method handles noisy background such as cracks with shadows and stains, cracks on rusty and rough surfaces and minor dimension cracks with good efficiency.",https://ieeexplore.ieee.org/document/9200107/,2020 International Conference on Computational Performance Evaluation (ComPE),2-4 July 2020,ieeexplore
10.1109/ICSTCC50638.2020.9259777,Integrated Fault Detection and Diagnosis of an Unmanned Aerial Vehicle using Time Difference of Arrival,IEEE,Conferences,"An integrated approach to the fault detection and diagnosis (FDD) of an unmanned aerial vehicle is presented. A novel approach using the Time Difference Of Arrival (TDOA) principle has been developed to detect, isolate and identify an incipient fault condition in the rotor dynamics. The requirements of a reconfigurable controller (RC) has been taken into account through the real-time implementation of a continuous forward algorithm (CFA) with a golden section search (GSS) combined with a meta-heuristic global optimization technique. The training and testing data for Radial Basis Function Neural Networks (RBF-NN) learning and prediction were supplied in discrete-time and its integration capacity validated through a Hardware-in-the-loop simulation (HILS) using a Teensy 3.6 microcontroller. The Pseudo real-time desktop simulation showed that the FDD algorithm was able to detect and isolate an incipient rotor fault and supply the RC a post-fault model and associated fault uncertainties. This method showed robustness towards prediction errors (bias and variance) and can be used in an integrated fault-tolerant control framework.",https://ieeexplore.ieee.org/document/9259777/,"2020 24th International Conference on System Theory, Control and Computing (ICSTCC)",8-10 Oct. 2020,ieeexplore
10.1109/MECO.2016.7525773,Neural network implementation of a principal component analysis tasks on board the unmanned aerial vehicle information processing in real time,IEEE,Conferences,"The application of principal component analysis to simplify the classification of pixels of images obtained with the help of unmanned aerial vehicle for the organization received information processing in real time, and simplify the process of orientation of unmanned aerial vehicle on the ground. We consider the neural network of the principal component analysis. The results of experiments showing the effectiveness of such use of principal component analysis.",https://ieeexplore.ieee.org/document/7525773/,2016 5th Mediterranean Conference on Embedded Computing (MECO),12-16 June 2016,ieeexplore
10.1109/ICRA40945.2020.9197071,On-board Deep-learning-based Unmanned Aerial Vehicle Fault Cause Detection and Identification,IEEE,Conferences,"With the increase in use of Unmanned Aerial Vehicles (UAVs)/drones, it is important to detect and identify causes of failure in real time for proper recovery from a potential crash-like scenario or post incident forensics analysis. The cause of crash could be either a fault in the sensor/actuator system, a physical damage/attack, or a cyber attack on the drone's software. In this paper, we propose novel architectures based on deep Convolutional and Long Short-Term Memory Neural Networks (CNNs and LSTMs) to detect (via Autoencoder) and classify drone mis-operations based on real-time sensor data. The proposed architectures are able to learn high-level features automatically from the raw sensor data and learn the spatial and temporal dynamics in the sensor data. We validate the proposed deep-learning architectures via simulations and realworld experiments on a drone. Empirical results show that our solution is able to detect (with over 90% accuracy) and classify various types of drone mis-operations (with about 99% accuracy (simulation data) and upto 85% accuracy (experimental data)).",https://ieeexplore.ieee.org/document/9197071/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore
10.1109/ICUAS.2016.7502588,Real-time unmanned aerial vehicle 3D environment exploration in a mixed reality environment,IEEE,Conferences,"This paper presents a novel human robot interaction system that can be used for real-time 3D environment exploration with an unmanned aerial vehicle (UAV). The method creates a mixed reality environment, in which a user can interactively control a UAV and visualize the exploration data in real-time. The method uses a combination of affordable sensors, and transforms the control and viewing space from the UAV to the controller's perspective. Different hardware and software configurations are studied so that the system can be adjusted to meet different needs and environments. A prototype system is presented and test results are discussed.",https://ieeexplore.ieee.org/document/7502588/,2016 International Conference on Unmanned Aircraft Systems (ICUAS),7-10 June 2016,ieeexplore
10.1049/cp.2012.1357,Small Unmanned Aerial Vehicle visual system for ground moving target positioning,IET,Conferences,"Recently, Small Unmanned Aerial Vehicles (SUAV) are used in a variety of reconnaissance, surveillance, combat applications and so on. Tracking and positioning ground moving targets using a camera mounted on SUAVs has important applications in military and civilian purposes. Although many approaches for a vision-based target positioning system have been developed, most researches are limited to target recognition algorithm or target state estimation. And hardware implementation schemes which can work well with the whole SUAV system are seldom discussed. In order to achieve an available systematic scheme, a complete visual system which incorporates a vision-based ground moving target positioning algorithm is presented in this paper. Experiment results are finally shown and commented. Experiments indicate that the system performs great real time response and high positioning precision.",https://ieeexplore.ieee.org/document/6492964/,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),3-5 March 2012,ieeexplore
10.23919/ACC45564.2020.9147911,Unmanned Aerial Vehicle Angular Velocity Control via Reinforcement Learning in Dimension Reduced Search Spaces,IEEE,Conferences,"Search space dimension reduction strategies are studied for reinforcement learning based angular velocity control of multirotor unmanned aerial vehicles. Reinforcement learning approximates the value function iteratively over the state-action space, which is 6-dimensional in the case of multirotor angular velocity control. An inverse-dynamics approach is applied to reduce the 6-dimensional state-action space to a 3-dimensional state-only search space while estimating the uncertain model parameters. The search space dimension is further reduced when the state variables are only allowed to vary following either a motion camouflage strategy or a hyperbolic tangent path. Simulation results show that the modified reinforcement learning algorithms can be implemented in real time for multirotor angular velocity control.",https://ieeexplore.ieee.org/document/9147911/,2020 American Control Conference (ACC),1-3 July 2020,ieeexplore
10.1109/ICTC.2017.8190968,Unmanned aerial vehicle surveillance system (UAVSS) for forest surveillance and data acquisition,IEEE,Conferences,"An application framework is proposed in this paper that considers low cost surveillance mechanism and data acquisition in the forest. An application is developed as proof of concept with detailed design that can take advantage of unmanned urban vehicle to be directly configured and controlled in real-time. The advantages are numerous; it can be used for many purposes. For example, it can be used for observing critical and important area for intruder activities or to know the current state of any object of interest. We considered using machine learning and image processing and can be used for species of trees in the forest by color and size detection. A separate service running on separate remote server will be responsible for this. We have proposed a application framework particularly to be cheap and easy to handle by non-technical persons and that it does not require large software system knowledge like Pix4D or DroneDeploy. This system will be useful for operations and research specially the forestry and palm oil plantation surveillance, and sustainable timber industry that specially needs carefully collected imageries and data from objects. Collection of raw data from sensor networks is also proposed in the system architecture.",https://ieeexplore.ieee.org/document/8190968/,2017 International Conference on Information and Communication Technology Convergence (ICTC),18-20 Oct. 2017,ieeexplore
10.1109/TIE.2017.2764849,A Vision-Aided Approach to Perching a Bioinspired Unmanned Aerial Vehicle,IEEE,Journals,"This paper presents the implementation of a machine learning approach for replicating highly adaptive avian perching behavior. With full consideration of both the configuration of flying vehicles and perching principles, a bioinspired aerial robot comprising one flight subsystem and one perching subsystem is designed. Based on the real-time landing speed and attitude, a novel type of soft grasping mechanism for dexterous perching is proposed to provide adhesive force and absorb impact force. During the critical perching phase, the dynamics of the perching actuator change with the touchdown conditions and the type of perching target. A hybrid automation of a time-to-contact theory-based attitude controller and a robust self-localization system are utilized to regulate the desired perching maneuvers. The experimental results are provided to attest to the effectiveness of the proposed perching method.",https://ieeexplore.ieee.org/document/8074761/,IEEE Transactions on Industrial Electronics,May 2018,ieeexplore
10.1109/ICRA.2019.8794446,A Reinforcement Learning Approach for Control of a Nature-Inspired Aerial Vehicle,IEEE,Conferences,"In this work, reinforcement learning is used to develop a position controller for an underactuated nature-inspired Unmanned Aerial Vehicle (UAV). This particular configuration of UAVs achieves lift by spinning its entire body contrary to standard multi-rotors or fixed-wing aircraft. Deep Deterministic Policy Gradients (DDPG) with Ape-X Distributed Prioritized Experience Replay was used to train neural network function approximators that were implemented as the final control policy. The reinforcement learning agent was trained in simulations and directly ported over to real-life hardware. Position control tests were performed on the learned control policy and compared to a baseline PID controller. The learned controller was found to exhibit better control over the inherent oscillations that arise from the non-linear dynamics of the platform.",https://ieeexplore.ieee.org/document/8794446/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore
10.1109/ICUAS.2019.8798329,A Vision-based Unmanned Aircraft System for Autonomous Grasp &amp; Transport,IEEE,Conferences,"The progress in sensor technologies, computer capabilities and artificial intelligence has endowed the unmanned aircraft system (UAS) with more autonomous abilities. Motivated by the 6th International Unmanned Aerial Vehicle Innovation Grand Prix (UAVGP), a UAS with high degree of autonomy was developed to perform the mission of building a simulated tower using prefabricated components. According to the requirement of the competition, the UAS was designed and implemented from the following four parts: 1) navigation and control, 2) recognition and location, 3) grasp and construction, and 4) task planning and scheduling. Different levels of autonomy have been given to the UAS based on these parts. The system hardware was developed on a quadrotor platform by integrating various components, including sensors, computers, power and grasp mechanism. Software which included precise navigation, mission planning, real-time perception and control was implemented and integrated with the developed UAS hardware. The performance in the test environment and actual competition showed that the UAS could perform the mission without human intervention with high autonomy and reliability. This paper addresses the major components and development process of the UAS and describes its application to the practical mission.",https://ieeexplore.ieee.org/document/8798329/,2019 International Conference on Unmanned Aircraft Systems (ICUAS),11-14 June 2019,ieeexplore
10.23919/URSI-AT-RASC.2018.8471616,A Wireless Method for Drone Identification and Monitoring Using AIS Technology,IEEE,Conferences,"The fast growth of UAV (Unmanned Aerial Vehicle) technology in the last years has allowed to extend the use of these devices in many applications. However, the massive use of drones has alerted many governments about an inadequate usage, mainly in terms of security and terrorism. In regards to this problem, some laws about drone usage have been approved by some countries. Among these laws, it is mandatory that all drones are identified and monitored all the time. In this paper, a wireless prototype to identify drones is proposed. To that end, the AIS (Automatic Identification System) is used to transmit parameters as name, ID, speed or the course of drones. This proposed solution can solve the drone identification problem and even, allows to monitor this device in real time. A prototype of this method has been implemented and tested in a real environment, around many locations in the island of Gran Canaria.",https://ieeexplore.ieee.org/document/8471616/,2018 2nd URSI Atlantic Radio Science Meeting (AT-RASC),28 May-1 June 2018,ieeexplore
10.1109/ITAIC.2011.6030219,A redeployment strategy based on Unmanned Aerial Vehicle in wireless sensor network,IEEE,Conferences,"In reality, holes in large-scale sensor networks are exist due to various reasons, such as energy depletion and randomness deployment. These holes may degrade the detection performance of the entire sensor networks. Based on the probabilistic detection model with false alarm rate, this paper proposes a new redeployment method. A method of using an Unmanned Aerial Vehicle (UAV) was proposed for boundaries of holes detection. In particular, in this paper, the contour graph was used for redeploy the holes based on depth-first strategy. According to the simulation results, the proposed method can attain better coverage rate.",https://ieeexplore.ieee.org/document/6030219/,2011 6th IEEE Joint International Information Technology and Artificial Intelligence Conference,20-22 Aug. 2011,ieeexplore
10.1109/AERO.2005.1559516,A scalable learning system for video recognition,IEEE,Conferences,"Learning has become an essential part of many image and video processing systems, but it is not often used as an end-to-end solution. Some of the most successful demonstrations of end-to-end learning have been with convolutional, or shared weight networks. We are interested in how this approach can scale and have developed a flexible framework for implementing and training large scale convolutional networks called Harpo. We present an overview of the Harpo framework and describe a multilevel learning strategy used to optimize convolutional networks for particular features of interest in video data streams. Harpo is designed to exploit reconfigurable hardware to accelerate massively parallel convolutional network components and achieve real-time processing speeds. In this paper, we present initial software experiments which use the system to segment exhaust plumes coming from military vehicles in unmanned aerial vehicle video data",https://ieeexplore.ieee.org/document/1559516/,2005 IEEE Aerospace Conference,5-12 March 2005,ieeexplore
10.1109/MED.2007.4433822,Attitude and heading refernce system I-AHRS for the EFIGENIA autonomous unmanned aerial vehicles UAV based on MEMS sensor and a neural network strategy for attitude estimation,IEEE,Conferences,"For the autonomous flight, navigation, guidance and control of the EFIGENIA unmanned aerial vehicle it is essential to have high performance 6-DOF attitude and heading reference system measurements. This paper presents the design and development of a real-time intelligent attitude and heading reference system I-AHRS, as in the hardware, as in the intelligent digital neural network software scheme, analysis, design and construction for the orientation calculation for the EFIGENIA EJ-1B MOZART and the EFIGENIA EJ-2B MARIA autonomous unmanned aerial vehicles UAVs. The EFIGENIA I-AHRS consists of three MEMS accelerometers, three MEMS rate-gyros and three magneto-resistive transducers that send its outputs to a digital neural network in which is possible to develop a strategy for attitude estimation. Additionally it is well known that the Kalman Filter is an option as multi-sensor data fusion and integration, however it has some adaptability limitations. In this paper, FPGA reconfigurable hardware digital neural network architecture is presented and utilized to replace the Kalman Filter in the integration of MEMS IMU inertial sensors signals and the Magneto resistive sensors.",https://ieeexplore.ieee.org/document/4433822/,2007 Mediterranean Conference on Control & Automation,27-29 June 2007,ieeexplore
10.1109/UVS.2019.8658283,Automatic Fault Detection of Power Lines using Unmanned Aerial Vehicle (UAV),IEEE,Conferences,"Safety and automation are the two major challenges in the application of Unmanned Aerial Vehicle (UAV), commonly known as drone, to the power lines inspection and fault detection. While current state-of-the-art UAVs are equipped with collision avoidance features, there is less attention to the automatic and real-time fault detection of power lines using UAVs. This paper presents the architecture of three drone-oriented concept designs for automatic and real-time fault detection of power lines using UAVs. The proposed systems could be potential candidates for replacing traditional inspection methods of power lines, which are risky and costly. By incorporating a robust neural network, i.e., Artificial Intelligence (AI), and using appropriate and efficient sensors, the systems can automatically detect various faults and defects on power lines with high precision. We propose three concept design options comprised of different hardware/software components and their feasibility factors. For instance, FLIR Duo Pro R as a thermal sensor and Zenmuse XT for thermal vision have been proposed to be used in the concept designs. For data communication, the proposed designs use cloud-based virtual private network (VPN) for a secure connection between remote control (RC) of the UAV and the server. Based on the advantages and disadvantages of the three proposed design options, the most efficient design is also discussed. This design proposes a system with lightweight sensors, which could increase the flight time of the UAV. Further, the AI interface is coded on to the RC, making it economical, without any database for big data storage. The back-end of the neural network is stored in a cloud server. With the help of GSM antenna, the AI can run on the tablet if there is an available cellular network.",https://ieeexplore.ieee.org/document/8658283/,2019 1st International Conference on Unmanned Vehicle Systems-Oman (UVS),5-7 Feb. 2019,ieeexplore
10.1109/WorldS4.2019.8904014,Autonomous Drone for Defence Machinery Maintenance and Surveillance,IEEE,Conferences,"This proposed research work focuses on the implementation of an autonomous unmanned aerial vehicle (UAV) which is controlled using a pix hawk flight controller. The Quad Copter is capable of navigating autonomously without any real-time input from the user and also programmed to follow a specified path autonomously. The algorithm enables a control technique by which quad copter is empowered to fly autonomously, trajectory tracking, graceful motion and accurate altitude hold performance. Surveillance and Machinery maintenance application are the primary applications designed for the Defense purposes in the Line Of Control and War-zones. This work is aimed to design a quad copter that will follow a command to fly through specified way points. The deep learning algorithm detects human motions and from data acquired camera and ultrasonic sensors to the cloud. On deviations from the standard protocol which is detected using a Sjcam 5000x elite Camera. Also, here a backup auxiliary mini drone is programmed to eject along with the data stored on the primary drone's memory on an unforeseen calamity or an attack that would damage the primary drone's ability to fly.",https://ieeexplore.ieee.org/document/8904014/,2019 Third World Conference on Smart Trends in Systems Security and Sustainablity (WorldS4),30-31 July 2019,ieeexplore
10.1109/UIC-ATC.2017.8397582,Autonomous UAV forced graffiti detection and removal system based on machine learning,IEEE,Conferences,"This paper proposes a smart graffiti clean-up system based on an autonomous Unmanned Aerial Vehicle (UAV) platform. This smart clean system is based on edge detection and machine learning algorithms to realize the detection and tracking of graffiti image in real time. In Graffiti detection, we aim to build a model to detect graffiti on walls which can help navigate the UAV to the correct coordinate and estimate the area of the graffiti. The data set which contain graffiti images are trained using machine learning techniques which will be used for the detections of the graffiti patterns. This will automate the process of detecting the location of the graffiti based on the edge detection technique and the model will be able to estimate the area of the graffiti. To achieve obstacle detection, and collision, a smart navigation approach is also proposed with the help of LiDAR and external camera. The overall graffiti cleanup system contains hardware and software that allow the user to use spray enamel with the reach and scale of an autonomous UAV.",https://ieeexplore.ieee.org/document/8397582/,"2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",4-8 Aug. 2017,ieeexplore
10.1109/GLOBECOM42002.2020.9322556,Caching Placement and Resource Allocation for AR Application in UAV NOMA Networks,IEEE,Conferences,"The cache-enabling unmanned aerial vehicle (UAV) cellular networks with massive access capability supported by non-orthogonal multiple access (NOMA) are investigated in this paper. The delivery of multi-media contents for the mixed augmented reality (AR) and normal multi-media application is assisted by multiple mobile UAV base stations, which cache popular contents for wireless backhaul link traffic offloading. To cope with the dynamic content requests and mobility of users in practical scenarios, the dynamic optimization problem for user association, caching placement of UAVs, real-time deployment of UAVs, and power allocation of NOMA is modeled as a stackelberg game to minimize the long-term content delivery delay. Specifically, the game is decomposed into a leader level problem and a number of follower level problems. A correction mechanism is added in deep reinforcement learning (DRL) to optimize the user association in leader level. A meta actor network is proposed in DRL to jointly optimize the UAVs caching placement, real-time UAVs deployment and power allocation of NOMA in follower level. Then, a dynamic caching placement and resource allocation algorithm based on multi-agent meta deep reinforcement learning is proposed to minimize the long-term content delivery delay. Finally, we demonstrate that the considerable gains are achieved by the proposed algorithm.",https://ieeexplore.ieee.org/document/9322556/,GLOBECOM 2020 - 2020 IEEE Global Communications Conference,7-11 Dec. 2020,ieeexplore
10.1109/ICAIIC51459.2021.9415209,Controller module implementation to reduce interrupt in CNPC uplink,IEEE,Conferences,"The unmanned aerial vehicle is used in diverse area. In order to make more use of the unmanned aerial vehicle, reliable communication system is required. CNPC has been developed to standardize the communication system for unmanned aerial vehicle over 150kg. CNPC uplink should support diverse UAV in TDD. To implement CNPC in real world, operating system and FPGA should be used with the interface between the two. To reduce the use of interrupts in uplink implementations on FPGA, simple controller is designed to generate signals which act as the interrupts whenever other user message is needed. To implement this controller in FPGA, this paper deals with timing diagram for this module.",https://ieeexplore.ieee.org/document/9415209/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/VTC2020-Fall49728.2020.9348616,Deep Q-Network Based Dynamic Movement Strategy in a UAV-Assisted Network,IEEE,Conferences,"Unmanned aerial vehicle (UAV)-assisted communications is a promising solution to improve the performance of future wireless networks, where UAVs are deployed as base stations for enhancing the quality of service (QoS) provided to ground users when traditional terrestrial base stations are unavailable or not sufficient. An effective framework is proposed in this paper to manage the dynamic movement of multiple unmanned aerial vehicles (UAVs) in response to ground user mobility, with the objective to maximize the sum data rate of the ground users. First, we discuss the relationship between the air-to-ground (A2G) path loss (PL) and the location of UAVs. Then a deep Q-network (DQN) based method is proposed to adjust the locations of UAVs to maximize the sum data rate of the user equipment (UE). Finally, simulation results show that the proposed method is capable of adjusting UAV locations in a real-time condition to improve the QoS of the entire network.",https://ieeexplore.ieee.org/document/9348616/,2020 IEEE 92nd Vehicular Technology Conference (VTC2020-Fall),18 Nov.-16 Dec. 2020,ieeexplore
10.23919/SoftCOM50211.2020.9238313,Deep Semantic Image Segmentation for UAV-UGV Cooperative Path Planning: A Car Park Use Case,IEEE,Conferences,"Navigation of Unmanned Ground Vehicles (UGV) in unknown environments is an active area of research for mobile robotics. A main hindering factor for UGV navigation is the limited range of the on-board sensors that process only restricted areas of the environment at a time. In addition, most existing approaches process sensor information under the assumption of a static environment. This restrains the exploration capability of the UGV especially in time-critical applications such as search and rescue. The cooperation with an Unmanned Aerial Vehicle (UAV) can provide the UGV with an extended perspective of the environment which enables a better-suited path planning solution that can be adjusted on demand. In this work, we propose a UAV-UGV cooperative path planning approach for dynamic environments by performing semantic segmentation on images acquired from the UAV's view via a deep neural network. The approach is evaluated in a car park scenario, with the goal of providing a path plan to an empty parking space for a ground-based vehicle. The experiments were performed on a created dataset of real-world car park images located in Croatia and Germany, in addition to images from a simulated environment. The segmentation results demonstrate the viability of the proposed approach in producing maps of the dynamic environment on demand and accordingly generating path plans for ground-based vehicles.",https://ieeexplore.ieee.org/document/9238313/,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",17-19 Sept. 2020,ieeexplore
10.1109/RAAI52226.2021.9508033,Development of Gasoline-Electric Hybrid Propulsion Surveillance and Reconnaissance VTOL UAV,IEEE,Conferences,"Vertical Take-Off and Landing (VTOL) Unmanned Aerial Vehicles (UAV) have been a high potential topic in the aerospace industry during the last decades due to its multirotor and fixed-wing nature of the aircraft. Besides, having the ability to rapidly deploy from a tight airstrip and gathering Intelligence, Surveillance, and Reconnaissance (ISR) information is the best way to be one step ahead of the enemy. In this paper, we present the implementation and development of gasoline-electric hybrid propulsion VTOL Unmanned Aerial vehicle respectively. The Hybrid propulsion VTOL UAV offers image and real-time video transmission to the ground station with fully autonomous control to get the best view of the enemy from the sky. The gasoline-electric hybrid propulsion system provides long flight endurance with efficient power consumption. The fundamentals of the multirotor and the conventional fixed-wing aircraft present the theoretical background of the aircraft. The accomplished design consists of high-performance multirotor motors with an efficient gasoline engine. Furthermore, the control system architecture, avionics, and power distribution system presented with addressing cost-effective trending design techniques. The performance of the system has been improved using commercially off-the-shelf (COTS) hardware.",https://ieeexplore.ieee.org/document/9508033/,"2021 IEEE International Conference on Robotics, Automation and Artificial Intelligence (RAAI)",21-23 April 2021,ieeexplore
10.1109/SNPD.2007.475,Double Unmanned Aerial Vehicle's Path Planning for Scout via Cross-Entropy Method,IEEE,Conferences,"Cross-entropy has been recently applied to combinatorial optimization problems with satisfying results. This paper introduce the cross-entropy method theory, the way of model making, real time and robust, and the application, in military scout , Unmanned Aerial Vehicle(UAV) is a special tool, with so many advantage in real time and global space. A new method, based on cross-entropy method, was used for the optimal way of double UAV's path planning.",https://ieeexplore.ieee.org/document/4287760/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore
,EXPRESS: An Energy-Efficient and Secure Framework for Mobile Edge Computing and Blockchain based Smart Systems,IEEE,Conferences,"As most smart systems such as smart logistic and smart manufacturing are delay sensitive, the current mainstream cloud computing based system architecture is facing the critical issue of high latency over the Internet. Meanwhile, as huge amount of data is generated by smart devices with limited battery and computing power, the increasing demand for energy-efficient machine learning and secure data communication at the network edge has become a hurdle to the success of smart systems. To address these challenges with using smart UAV (Unmanned Aerial Vehicle) delivery system as an example, we propose EXPRESS, a novel energy-efficient and secure framework based on mobile edge computing and blockchain technologies. We focus on computation and data (resource) management which are two of the most prominent components in this framework. The effectiveness of the EXPRESS framework is demonstrated through the implementation of a real-world UAV delivery system. As an open-source framework, EXPRESS can help researchers implement their own prototypes and test their computation and data management strategies in different smart systems. The demo video can be found at https://youtu.be/r3U1iU8tSmk.",https://ieeexplore.ieee.org/document/9286009/,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),21-25 Sept. 2020,ieeexplore
10.1109/ISAECT50560.2020.9523700,Edge-Cloud Architectures Using UAVs Dedicated To Industrial IoT Monitoring And Control Applications,IEEE,Conferences,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",https://ieeexplore.ieee.org/document/9523700/,2020 International Symposium on Advanced Electrical and Communication Technologies (ISAECT),25-27 Nov. 2020,ieeexplore
10.1109/AIPR50011.2020.9425341,Enhancing Network-edge Connectivity and Computation Security in Drone Video Analytics,IEEE,Conferences,"Unmanned Aerial Vehicle (UAV) systems with high-resolution video cameras are used for many operations such as aerial imaging, search and rescue, and precision agriculture. Multi-drone systems operating in Flying Ad Hoc Networks (FANETS) are inherently insecure and require efficient security schemes to defend against cyber-attacks such as e.g., Man-in-the-middle, Replay and Denial of Service attacks. In this paper, we propose a cloud-based, end-to-end security framework viz., ""DroneNet-Sec"" that provides secure network-edge connectivity, and computation security for drone video analytics to defend against common attack vectors in UAV systems. The DroneNet-Sec features a dynamic security scheme that uses machine learning to detect anomaly events and adopts countermeasures for computation security of containerized video analytics tasks. The security scheme comprises of a custom secure packet designed with MAVLink protocol for ensuring data privacy and integrity, without high degradation of the performance in a real-time FANET deployment. We evaluate DroneNet-Sec in a hybrid testbed that synergies simulation and emulation via an open-source network simulator (NS-3) and a research platform for mobile wireless networks (POWDER). Our performance evaluation experiments in our holistic hybrid-testbed show that DroneNet-Sec successfully detects learned anomaly events and effectively protects containerized tasks execution as well as communication in drones video analytics in a light-weight manner.",https://ieeexplore.ieee.org/document/9425341/,2020 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),13-15 Oct. 2020,ieeexplore
10.1109/ICCAS.2008.4694666,Enhancing situational awareness by means of hybrid adaptive neural control of vertical flight in unmanned helicopter,IEEE,Conferences,"This paper focuses on a critical component of the situational awareness, the neural control of autonomous vertical flight for an unmanned aerial vehicle. Autonomous vertical flight is a challenging but important task for tactical unmanned aerial vehicles to achieve high level of autonomy under adverse conditions. The fundamental requirement for vertical flight is the knowledge of the height above the ground, and a properly designed controller to govern the process. With the situational awareness strategy, we proposed a two stage flight control procedure using two adaptive neural networks to address the dynamics variation and performance requirement difference in initial and final stages of flight trajectory for a nontrivial small-scale helicopter model comprising five states, two inputs and two outputs. This control strategy for chosen helicopter model has been verified by simulation of descending and landing manoeuvres using software package Simulink and demonstrated good performance for fast situational awareness in real-time search-and-rescue operations.",https://ieeexplore.ieee.org/document/4694666/,"2008 International Conference on Control, Automation and Systems",14-17 Oct. 2008,ieeexplore
10.1109/ICUAS48674.2020.9214045,Extensions of the open-source framework Aerostack 3.0 for the development of more interactive flights between UAVs,IEEE,Conferences,"The basis for properly verified R&amp;D works is to provide reliable prototyping tools at three most important stages: computer simulation, laboratory tests and real-world experiments. In the laboratory-limited conditions, particular importance is attributed to the first two stages, especially in the context of the safety development of autonomous flights of unmanned aerial vehicle (UAV) groups in various missions. The open-source framework Aerostack support those needs and its effectiveness has been proven in the International Micro Air Vehicle Indoor Competitions (IMAV 2013, 2016, 2017) and Mohammed Bin Zayed International Robotics Challenge (MBZIRC 2020). In the paper, the exemplary functionalities for the new version of Aerostack Version 3.0 Distribution Sirocco (Aerial robotics framework for the industry), extended additionally with a library of new behaviors, are presented. The mission of UAVs can be developed fast and effectively in order to conduct test flights with real drones in lab, before one will decide to fly autonomously outdoor. The representative results obtained for low-cost AR.Drone 2.0 UAV models in two missions, are presented. The first mission is autonomous patrolling the area by a pair of UAVs, the second - intercepting the intruder in guarded area by the guard UAV.",https://ieeexplore.ieee.org/document/9214045/,2020 International Conference on Unmanned Aircraft Systems (ICUAS),1-4 Sept. 2020,ieeexplore
10.1109/TENCON.2019.8929613,Fish Detection and Tracking in Pisciculture Environment using Deep Instance Segmentation,IEEE,Conferences,"This study presents a novel approach in detecting and tracking of fish in pisciculture. Pisciculture in general involves challenging tasks of counting and monitoring fish in natural or nature like, man-made habitats such as inland fisheries for breeding, feeding and sorting purposes. These are presently achieved using conventional methods that are inefficient when implemented in large-scale commercial productions. To overcome such difficulties and improve the efficiency of the processes, images of fish and fish seeds are captured in natural murky water habitats through a vision sensor on board an unmanned aerial vehicle (UAV). In this research paper, a deep instance segmentation algorithm called Mask R-CNN along with GOTURN tracking algorithm is employed for real time fish detection and tracking. A comparison study is also carried out (i) fish detection on high resolution images (ii) fish detection on high resolution image multi-region parallel processing (iii) fish detection on high resolution image multi-region parallel processing with tracking. The results are found to be accurate with image multi-region parallel processing along with tracking, with an F1 score of 0.91 at 16 frames per seconds on in-land fishes environment.",https://ieeexplore.ieee.org/document/8929613/,TENCON 2019 - 2019 IEEE Region 10 Conference (TENCON),17-20 Oct. 2019,ieeexplore
10.1109/TELECOM50385.2020.9299566,Forest Monitoring System for Early Fire Detection Based on Convolutional Neural Network and UAV imagery,IEEE,Conferences,"Forest fires are one of the main reasons for environmental degradation. In their early stages, the fires are hard to discover, so a faster and more accurate detection method can help minimize the amount of damage they can inflict. In this paper, we present an approach for autonomous early fire detection, which is based on a system with high degree of reliability and with no need of service or human interaction. To provide the autonomous capabilities to the proposed system, we have developed an object detection method, based on a convolutional neural network, which is presented in the main part of the paper. In order to have a better field of view over the observed area, instead of traditional lookout towers and satellite based monitoring, we use live video feed from an unmanned aerial vehicle (UAV), which patrols over the risky area. To make better predictions on the fire probability, we use not only the optical camera of the UAV, but also an on-board thermal camera. With the help of the software platform Node-RED, we have developed a web-based platform, which can present the acquired data in real-time and can notify the interested parties. The workflow for the development of the web-platform is also described in this paper.",https://ieeexplore.ieee.org/document/9299566/,2020 28th National Conference with International Participation (TELECOM),29-30 Oct. 2020,ieeexplore
10.1109/ICC.2019.8762097,Green Mobility Management in UAV-Assisted IoT Based on Dueling DQN,IEEE,Conferences,"In most cases, the batteries of sensor nodes in the Internet of Things (IoT) are usually constrained by size and weight, and are difficult to recharge or replace. In traditional wireless sensor networks, data is transmitted in a multi-hop manner, which may cause the high data transmission delay and unbalanced traffic load. In this paper, an Unmanned Aerial Vehicle (UAV)-assisted IoT architecture is introduced, in which UAV is utilized to achieve low-latency and seamless-coverage acquisition of the sensing data. Furthermore, based on the recent advances on deep reinforcement learning algorithms, considering both data delay requirements and network energy consumption, a real-time flight path planning scheme of the UAV in the dynamic IoT sensor networks has been proposed based on dueling deep Q-network (DQN). Besides, the grid-based method is used to handle the network state modeling, which effectively reduces the complexity of the proposed scheme. Simulation results show that the proposed scheme significantly improves the network performance.",https://ieeexplore.ieee.org/document/8762097/,ICC 2019 - 2019 IEEE International Conference on Communications (ICC),20-24 May 2019,ieeexplore
10.1109/IAICT50021.2020.9172031,Human Target Search and Detection using Autonomous UAV and Deep learning,IEEE,Conferences,"An Unmanned Aerial Vehicle (UAV) is an airborne system or pilotless aircraft which is remotely controlled by a human operator on ground or by an onboard computer such that the vehicle moves autonomously. The range of applications in which UAVs are used is very large. This paper describes the application of developing an autonomous surveillance system using an UAV to identify a given target and/or objects of interest in the terrain over which it flies. Such a system can be used in rescue operations, especially in remote areas where physical access is difficult. It can also be used for military operations, farming or any field where surveillance of a given land area is required. The UAV developed in this work is capable of object detection. A mounted camera is used to give visual feedback, and an onboard processing unit runs image recognition software to identify the target in real time. Optimal algorithms are used to search and find the target from the given search area. After recognition of the target, the UAV can either be used to hold its position so as to have a video feed of the target, or return to its base station once the coordinates have been estimated using GPS modules or relay the GPS location to the base station. This paper describes the implementation of the hardware and software components that lead to the realization of the UAV and the application of object detection. The details of a new search algorithm and an example of object detection is presented . The work presented in this paper is the first part in the attempt to develop a cluster of UAVs meant to work in collaboration to be deployed for search and rescue operations.",https://ieeexplore.ieee.org/document/9172031/,"2020 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)",7-8 July 2020,ieeexplore
10.1109/ICUAS48674.2020.9213884,Image-Based Sense and Avoid of Small Scale UAV Using Deep Learning Approach,IEEE,Conferences,"Distance detection of target object is an important information for obstacle avoidance in many fields, such as autonomous car. When the distance of the obstacle is calculated, one can determine the potential risk of collision. In this paper, a monocular camera was utilized to get the distance from an incoming unmanned aerial vehicle (UAV) using deep learning approach. The distance detection of an UAV using You Only Look Once (YOLO) object detector was proposed in this study. The region which contain the detected UAV was processed into 100 by 100 pixel and was input into the proposed model to estimate the distance of the target object. For the proposed model, a Convolutional Neural Network (CNN) was adopted to solve the regression problem. First, the feature extraction based on VGG network was performed, and then its results was applied to the distance network to estimate distance. Finally, Kalman filter was used to improve the object tracking when YOLO detector is not able to detect UAV and to smooth the estimated distance. The proposed model was trained only by using synthetic images from animation software and was validated by using both synthetic and real flight videos.",https://ieeexplore.ieee.org/document/9213884/,2020 International Conference on Unmanned Aircraft Systems (ICUAS),1-4 Sept. 2020,ieeexplore
10.1109/ELECSYM.2018.8615503,Implementation of Victims Detection Framework on Post Disaster Scenario,IEEE,Conferences,"Disasters are prone to occur in Indonesia due to geographical factors, such as tectonic plate movements, which can cause an earthquake. Earthquakes are one of the most frequent disasters, they have broad impacts in a short time and are unpredictable. Thus, an extensive search process in a short time is highly critical to determine the victims location. In this paper, a victims detection framework is developed starting from acquiring images using an unmanned aerial vehicle and further processing using convolutional neural network (CNN) to locate victims robustly on post-disaster. Input images are then sent to victim detector dedicated ground station server for further high processing robustly locating the possibility of victims. A simulation system mimicking a real environment is developed to test our framework in real time. A transmission protocol is also developed for effectively transmitting data between the robot and the server. The treatment on the detection process of the victim is different from the normal human detection, some pre-processing stages are applied to increase the variation of the given dataset. An embedded system is used for taking images and additional sensors data, such as location and time using Global Navigation Satellite System.",https://ieeexplore.ieee.org/document/8615503/,2018 International Electronics Symposium on Engineering Technology and Applications (IES-ETA),29-30 Oct. 2018,ieeexplore
10.1109/ComPE49325.2020.9200107,Inspection of Concrete Structures by a Computer Vision Technique and an Unmanned Aerial Vehicle,IEEE,Conferences,"We have proposed a visual inspection technique for concrete structures using deep learning and a hardware ecosystem, an Unmanned Aerial Vehicle (UAV). The UAV is a quadcopter that can fly to unreachable sections of a site which consists of a camera that captures images of the concrete surfaces via a mobile device and feed the real time images in the CNN model. The images taken from such remote locations may contain different types of surfaces, shadowed regions and surfaces with holes. The cracks are properly detected by the CNN `AlexNet' algorithm and masking with sliding window technique in such conditions due to variation in the image data set. The experimental results were simulated on a standard online data set of 40,000 images of Mendeley Data which is freely available and 3000 images have been chosen from the entire data set for this method. The classes have been divided into 2 categories of `crack' and `no crack' for the proposed method's data set. There are 1050 training images and 450 testing images for each category. Experimental results were achieved on Google Colab cloud service using Python Tensorflow API (Application Programming Interface). The proposed `AlexNet' CNN algorithm achieves 98.4 % accuracy and the model is deployed to a masking technique with sliding window to detect cracks in a 3008×2000 pixel resolution image by breaking the image into 227×227 pixel resolution image patches. The experimental results have proved that the proposed method handles noisy background such as cracks with shadows and stains, cracks on rusty and rough surfaces and minor dimension cracks with good efficiency.",https://ieeexplore.ieee.org/document/9200107/,2020 International Conference on Computational Performance Evaluation (ComPE),2-4 July 2020,ieeexplore
10.1109/ICSTCC50638.2020.9259777,Integrated Fault Detection and Diagnosis of an Unmanned Aerial Vehicle using Time Difference of Arrival,IEEE,Conferences,"An integrated approach to the fault detection and diagnosis (FDD) of an unmanned aerial vehicle is presented. A novel approach using the Time Difference Of Arrival (TDOA) principle has been developed to detect, isolate and identify an incipient fault condition in the rotor dynamics. The requirements of a reconfigurable controller (RC) has been taken into account through the real-time implementation of a continuous forward algorithm (CFA) with a golden section search (GSS) combined with a meta-heuristic global optimization technique. The training and testing data for Radial Basis Function Neural Networks (RBF-NN) learning and prediction were supplied in discrete-time and its integration capacity validated through a Hardware-in-the-loop simulation (HILS) using a Teensy 3.6 microcontroller. The Pseudo real-time desktop simulation showed that the FDD algorithm was able to detect and isolate an incipient rotor fault and supply the RC a post-fault model and associated fault uncertainties. This method showed robustness towards prediction errors (bias and variance) and can be used in an integrated fault-tolerant control framework.",https://ieeexplore.ieee.org/document/9259777/,"2020 24th International Conference on System Theory, Control and Computing (ICSTCC)",8-10 Oct. 2020,ieeexplore
10.1109/ASCC.2015.7244614,Joint unscented Kalman filter for dual estimation in a bifilar pendulum for a small UAV,IEEE,Conferences,"It has always been difficult to accurately estimate the moment of inertia of an object, e.g. an unmanned aerial vehicle (UAV). Whilst various offline estimation methods exist to allow accurate parametric estimation by minimizing an error cost function, they require large memory consumption, high computational effort, and a long convergence time. The initial estimate's accuracy is also vital in attaining convergence. In this paper, a new real time solution to the model identification problem is provided with the use of a Joint Unscented Kalman Filter for dual estimation. The identification procedures can be easily implemented using a microcontroller, a gyroscope sensor, and a simple bifilar pendulum setup. Accuracy, robustness, and convergence speed are achieved.",https://ieeexplore.ieee.org/document/7244614/,2015 10th Asian Control Conference (ASCC),31 May-3 June 2015,ieeexplore
10.1109/AERO.2018.8396807,Learning safe recovery trajectories with deep neural networks for unmanned aerial vehicles,IEEE,Conferences,"Unmanned vehicles that use vision sensors for perception to aid autonomous flight are a highly popular area of research. However, these systems are often prone to failures that are often hard to model. Previous work has focused on using deep learning to detect these failures. In this work, we build on these failure detection systems and develop a pipeline that learns to identify the correct trajectory to execute that restores the vision system and the unmanned vehicle to a safe state. The key challenge with using a deep learning pipeline for this problem is the limited amount of training data available from a real world system. Ideally one requires millions of data points to sufficiently train a model from scratch. However, this is not feasible for an unmanned aerial vehicle. The dataset we operate with is limited to 400-500 points. To sufficiently learn from such a small dataset we leverage the idea of transfer learning and non linear dimensionality reduction. We deploy our pipeline on an unmanned aerial vehicle flying autonomously through outdoor clutter (in a GPS denied environment) and show that we are able to achieve long durations of safe autonomous flight.",https://ieeexplore.ieee.org/document/8396807/,2018 IEEE Aerospace Conference,3-10 March 2018,ieeexplore
10.1109/CCAI50917.2021.9447518,Lightweight Real-time Object Detection Model for UAV Platform,IEEE,Conferences,"Real-time detecting objects on captured images on UAV (Unmanned Aerial Vehicle) platforms, rather than barely transmitting images back to supporting equipment for post-processing, is a core requirement for advanced UAV applications. However, due to limited computing capacity and memory of UAV platforms, it is very challenging to deploy real-time detection models on them. In addition, there are more small objects in aerial images, which makes it more difficult to detect accurately. To solve these problems, this paper brings dense connection to Yolo(You Only Look Once)v3 network, and proposes Yolo-LiteDense model. The backbone of Yolo-LiteDense is densely connected, which improves the performance of feature extraction. Then, we enforce channel pruning to Yolo-LiteDense model by pruning less informative channels with less scaling factors. After pruning, parameters and weight size of the model are compressed significantly, and inference time is also shortened. Evaluation results on VisDrone2018-DET show that parameters and weight size of Yolo-LiteDense are 83% and inference time is 30% less than Yolov3-SPP with comparable average precision. In addition, this paper also proposes the lighter version of Yolo-LiteDense, Yolo-DenseNano. Parameters and weight size of Yolo-LiteDense are 70% less than Yolov3-tiny with 2.68 times greater average precision.",https://ieeexplore.ieee.org/document/9447518/,2021 International Conference on Computer Communication and Artificial Intelligence (CCAI),7-9 May 2021,ieeexplore
10.1109/AIKE.2018.00015,Machine Learning Models to Enhance the Science of Cognitive Autonomy,IEEE,Conferences,"Intelligent Autonomous Systems (IAS) are highly cognitive, reflective, multitask-able, and effective in knowledge discovery. Examples of IAS include software systems that are capable of automatic reconfiguration, autonomous vehicles, network of sensors with reconfigurable sensory platforms, and an unmanned aerial vehicle (UAV) respecting privacy by deciding to turn off its camera when pointing inside a private residence. Research is needed to build systems that can monitor their environment and interactions, learn their capabilities and limitations, and adapt to meet the mission objectives with limited or no human intervention. The systems should be fail-safe and should allow for graceful degradations while continuing to meet the mission objectives. In this paper, we provide an overview of our proposed new methodologies and workflows, and survey the existing approaches and new ones that can advance the science of autonomy in smart systems through enhancements in real-time control, auto-reconfigurability, monitoring, adaptability, and trust. This paper also provides the theoretical framework behind IAS.",https://ieeexplore.ieee.org/document/8527447/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore
10.1109/ICC.2019.8761117,Machine Learning for Position Prediction and Determination in Aerial Base Station System,IEEE,Conferences,"A novel framework for dynamic 3-D deployment of unmanned aerial vehicle (UAV) in the aerial base station system (ABSS) that based on the machine learning algorithms is proposed. In the framework, the UAV is deployed as an aerial base station to serve a group of ground users and is placed based on the prediction of the users' mobility. The joint problem of prediction of users' track and 3-D deployment of the UAV is formulated for maximizing the sum transmit rate. A two-step approach is proposed for predicting the movement of users and for determining the dynamic 3-D placement of the UAV. Firstly, an echo state network (ESN) based prediction algorithm is utilized for predicting the future positions of users based on the real-world datasets collected from Twitter. Secondly, an iterative K-Means based algorithm is proposed for obtaining the optimal placement of UAV at each time slot based on the output of ESN model. Numerical results are illustrated for showing the superiority of the proposed algorithm over the prevalent algorithm on prediction tasks. The accuracy and efficiency of the proposed framework are also investigated. Additionally, compared with static placement of the UAV, the advantage of dynamic 3-D deployment is demonstrated.",https://ieeexplore.ieee.org/document/8761117/,ICC 2019 - 2019 IEEE International Conference on Communications (ICC),20-24 May 2019,ieeexplore
10.1109/MECO.2016.7525773,Neural network implementation of a principal component analysis tasks on board the unmanned aerial vehicle information processing in real time,IEEE,Conferences,"The application of principal component analysis to simplify the classification of pixels of images obtained with the help of unmanned aerial vehicle for the organization received information processing in real time, and simplify the process of orientation of unmanned aerial vehicle on the ground. We consider the neural network of the principal component analysis. The results of experiments showing the effectiveness of such use of principal component analysis.",https://ieeexplore.ieee.org/document/7525773/,2016 5th Mediterranean Conference on Embedded Computing (MECO),12-16 June 2016,ieeexplore
10.1109/ICUAS.2017.7991518,Nonlinear controller for a UAV using Echo State Network,IEEE,Conferences,"A nonlinear adaptive controller for an unmanned aerial vehicle (UAV) has been developed using Echo State Network (ESN), which is a form of three-layered recurrent neural network (RNN). Online learning is used to train the ESN in real-time starting from randomized weights. The ESN is integrated into ArduPilot, an open source autopilot, for complex flight simulations. Software-in-the-loop and hardware-in-the-loop simulations are performed using the FlightGear Flight Simulator. The response of the UAV using the controller based on the ESN has surpassed the performance of the traditional controllers. Noise and external disturbances are added to show the effectiveness of the controllers. A UAV test platform is designed and built to gather aircraft flight data and test the ESN.",https://ieeexplore.ieee.org/document/7991518/,2017 International Conference on Unmanned Aircraft Systems (ICUAS),13-16 June 2017,ieeexplore
10.1109/SoSE50414.2020.9130475,Real time object detection for aerial search and rescue missions for missing persons,IEEE,Conferences,"This paper introduces a solution to stand-alone system based, real-time object-detection, can efficiently facilitate the search for missing persons with an unmanned aerial vehicle. The challenge is the real-time implementation of the systems and training the given deep neural network for the desired task. The paper describes the methods and procedures currently in use, as well as the possible tools. Subsequently, the autonomous aircraft system, which carries a real-time detection system, is introduced. In the section about real-time detection, we will introduce the TensorFlow lite-based application, building on SSD topology, in detail, which was implemented on mobile phones. We will also introduce the dataset used for training, testing and the results achieved. In summary, the recall achieved is 65.4% and precision is 96.4%, besides the fact that the android-based application, using the phone's camera, performs image analysis at a rate of 11 to 17 FPS in real-time, while continuously providing.",https://ieeexplore.ieee.org/document/9130475/,2020 IEEE 15th International Conference of System of Systems Engineering (SoSE),2-4 June 2020,ieeexplore
10.1109/ICIT.2009.4939663,Real-time Neural Network based Identification of a Rotary-Wing UAV dynamics for autonomous flight,IEEE,Conferences,"Real time flight implementation of a neural network based black-box identification (NNID) scheme to a rotary wing unmanned aerial vehicle (RUAV) is presented in this paper. The applicability of NNID scheme for real time identification of longitudinal and lateral dynamics of the RUAV is evaluated in flight. To show the efficacy of the method for real time applications, the identification results and error statistics are provided. The challenges involved in terms of hardware implementation, computational time requirements, and real time coding are investigated and reported. Results indicate that NNID is suitable for modeling the dynamics of the RUAV in real time.",https://ieeexplore.ieee.org/document/4939663/,2009 IEEE International Conference on Industrial Technology,10-13 Feb. 2009,ieeexplore
10.1109/ICTC46691.2019.8939564,Real-time UAV Detection based on Deep Learning Network,IEEE,Conferences,"This paper presents deep learning-based YOLO (You only look once), for the detection of an unmanned aerial vehicle (UAV). In common practice, the creation of own data set is an extensive and hectic task, that takes a long time because it requires proper resolution images from different angles. These issues make the data set creation an important task. Implementation of YOLOv2 and YOLOv3 is done on the own created data set for the real-time UAV's detection and to benchmark the performance of both models in terms of mean average precision (MAP) and accuracy. For the specifically created data set made, YOLOv3 is outperforming YOLOv2 both in MAP and accuracy.",https://ieeexplore.ieee.org/document/8939564/,2019 International Conference on Information and Communication Technology Convergence (ICTC),16-18 Oct. 2019,ieeexplore
10.1109/ICSMC.2007.4413945,Real-time multi-network based identification with dynamic selection implemented for a low cost UAV,IEEE,Conferences,This paper describes a system identification technique based on dynamic selection of multiple neural networks for the Unmanned Aerial Vehicle (UAV). The UAV is a multi- input multi-output (MIMO) nonlinear system. The neural network models are based on the autoregressive technique. The multi-network dynamic selection method allows a combination of online and offline neural network models to be used in the architecture where the most suitable output is selected based on the given criteria. The online network uses a novel training scheme with memory retention. Flight test validation results for online and offline models are presented. Real-time hardware in the loop (HIL) simulation results show that the multi-net dynamic selection technique performs better than the individual models.,https://ieeexplore.ieee.org/document/4413945/,"2007 IEEE International Conference on Systems, Man and Cybernetics",7-10 Oct. 2007,ieeexplore
10.1109/ICUAS.2016.7502588,Real-time unmanned aerial vehicle 3D environment exploration in a mixed reality environment,IEEE,Conferences,"This paper presents a novel human robot interaction system that can be used for real-time 3D environment exploration with an unmanned aerial vehicle (UAV). The method creates a mixed reality environment, in which a user can interactively control a UAV and visualize the exploration data in real-time. The method uses a combination of affordable sensors, and transforms the control and viewing space from the UAV to the controller's perspective. Different hardware and software configurations are studied so that the system can be adjusted to meet different needs and environments. A prototype system is presented and test results are discussed.",https://ieeexplore.ieee.org/document/7502588/,2016 International Conference on Unmanned Aircraft Systems (ICUAS),7-10 June 2016,ieeexplore
10.1109/ICMA.2019.8816557,Research on V-SLAM Methods,IEEE,Conferences,"With the development of intelligent mobile robots, SLAM, especially V-SLAM, as the basic technology of robot localization and navigation, has the advantages of strong adaptability, high precision and strong intelligence compared with the traditional localization technology. It is widely used in smart devices such as unmanned aerial vehicle, automatic driving and sweeping robots. According to different implementation methods, the visual SLAM is divided into: filter V-SLAM based on probability model, key frame BA-based V-SLAM using nonlinear optimization theory, direct tracking of V-SLAM under the assumption of luminosity invariance, space occupying V-SLAM that focuses on building three-dimensional dense maps. This paper focuses on representative systems of various V-SLAMs and gives their respective applicable scenarios and characteristics. Finally, this article forecasts the development of V-SLAM combining with multi-information fusion technology, semantic deep and learning technology.",https://ieeexplore.ieee.org/document/8816557/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore
10.1109/ICRA48506.2021.9560756,Siamese Anchor Proposal Network for High-Speed Aerial Tracking,IEEE,Conferences,"In the domain of visual tracking, most deep learning-based trackers highlight the accuracy but casting aside efficiency. Therefore, their real-world deployment on mobile platforms like the unmanned aerial vehicle (UAV) is impeded. In this work, a novel two-stage Siamese network-based method is proposed for aerial tracking, i.e., stage-1 for high-quality anchor proposal generation, stage-2 for refining the anchor proposal. Different from anchor-based methods with numerous pre-defined fixed-sized anchors, our no-prior method can 1) increase the robustness and generalization to different objects with various sizes, especially to small, occluded, and fast-moving objects, under complex scenarios in light of the adaptive anchor generation, 2) make calculation feasible due to the substantial decrease of anchor numbers. In addition, compared to anchor-free methods, our framework has better performance owing to refinement at stage-2. Comprehensive experiments on three benchmarks have proven the superior performance of our approach, with a speed of ∼200 frames/s.",https://ieeexplore.ieee.org/document/9560756/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore
10.1109/ICECE.2008.4769331,Situational awareness based on neural control of an autonomous helicopter during hovering manoeuvres,IEEE,Conferences,"This paper focuses on a critical component of the situational awareness, the neural network control of autonomous vertical flight for an unmanned aerial vehicle. Application of the proposed two stage flight strategy which uses two autonomous adaptive neural dynamical feedback controllers was carried out for a nontrivial small-scale helicopter model comprising five states, two inputs and two outputs. This control strategy for chosen helicopter model has been verified by simulation of hovering manoeuvres using software package Simulink and demonstrated good performance for fast situational awareness in real-time search-and-rescue operations.",https://ieeexplore.ieee.org/document/4769331/,2008 International Conference on Electrical and Computer Engineering,20-22 Dec. 2008,ieeexplore
10.1049/cp.2012.1387,Study on UAV flight based identification technology,IET,Conferences,"In order to verify the feasibility of applying identification technology to engineering control of such air vehicles as missiles, by taking into consideration the flight authenticity, repeatability, safety and reliability, the unmanned aerial vehicle (UAV) similar to missile flight control is selected as the controlled object. Based on identification theory and control guidance theory, this paper firstly makes theoretical analysis on UAV's identification technology, and then conducts flight practice to verify the theory. Firstly, The feasibility of UAV's closed-loop identification and the generation of excitation signals are studied. Secondly, identification experiments platform, namely UAV system platform, is constructed, including hardware platform and such software platforms as control guidance, signal transmission and data collection. Thirdly, multiple flight identification experiments are conducted on UAV, thus obtaining identification data. Finally, the mathematical model of UAV is obtained through analyzing and processing data, and this model is verified in time domain and frequency domain. Through a series of simplification method, a mathematical model, which is available in engineering, simplified and close to reality, is achieved, and its correctness is verified at last. The results show that identification's theory and ideology can be entirely applied to the engineering practice controlled by actual air vehicles.",https://ieeexplore.ieee.org/document/6492994/,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),3-5 March 2012,ieeexplore
10.1109/OCEANSKOBE.2018.8559324,Supervised vs Unsupervised Approaches for Real Time Hyperspectral Imaging Maritime Target Detection,IEEE,Conferences,This paper addresses the use of supervised and unsupervised methods for classification of hyperspectral imaging data in maritime border surveillance domain. In this work supervised (SVM) and unsupervised (HYDADE) approaches were implemented. An evaluation benchmark was performed in order to compare methods results using real hyperspectral imaging data taken from an Unmanned Aerial Vehicle in maritime border surveillance scenario.,https://ieeexplore.ieee.org/document/8559324/,2018 OCEANS - MTS/IEEE Kobe Techno-Oceans (OTO),28-31 May 2018,ieeexplore
10.1109/GHTC.2018.8601597,The EDNA Public Safety Drone: Bullet-Stopping Lifesaving,IEEE,Conferences,"Urban gun violence in cities across the world is a serious issue for public safety agencies and disaster management organizations. This led us to the development of the EDNA drone, an aerial robotics solution designed to equip first responders in high-risk settings with lifesaving-edge tools for situational awareness and non-lethal conflict resolution. The EDNA is an unmanned aerial vehicle (UAV) that delivers the patent-pending “Predictive Probable Cause” technology. The EDNA drone is designed to provide automated real-time analysis to assist teams entering high-risk situations where gun violence may occur. By leveraging machine learning, biometric sensors, and advanced materials in the field and routing feedback to an intuitive augmented-reality interface, the EDNA will provide autonomous threat detection and bullet-stopping capabilities wherever those features are needed--to groups such as Police and Sheriff's Departments, Fire Departments, and EMT and emergency rescue teams. Data from the EDNA drone's sensors is fed to machine learning algorithms running on the drone in real-time. Through a neural network trained on past data, the EDNA is able to detect the presence and location of firearms and explosives, even through walls or other obstacles. Through the use of advanced metal foams and composite materials, the armored drone can even stop bullets-functionality which has obvious benefits for humanitarian deployment.",https://ieeexplore.ieee.org/document/8601597/,2018 IEEE Global Humanitarian Technology Conference (GHTC),18-21 Oct. 2018,ieeexplore
10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00060,The Implementation of a Power Efficient BCNN-Based Object Detection Acceleration on a Xilinx FPGA-SoC,IEEE,Conferences,"This paper focuses on the power efficient design on the FPGA SoC for the object detection system based on Binary Convolutional Neural Network (BCNN). Especially, for the small IoT devices, such as an intelligent dash-cam, computer vision system installed on an unmanned aerial vehicle, the power consumption could be a significant factor of the performance and scalability. However, the optimized FPGA design has limitations to reduce the overall power consumption amount. We focus on the design of the FPGA Accelerator as well as the effective design of the peripherals including CPU. In our proposed FPGA-SoC design, it supports not only FPGA but also CPU and the peripheral component can be supported by additional virtual memory system for reducing the processing time. Overall customization including customized BCNN, virtual memories for CPU and FPGA part allows our testbed to achieve low power consumption without speed degradation. Our testbed is based on customized YOLOv2 which consists of applied binary and half precision convolution, and pipeline-based architecture with accelerated hardware design on the target device. The target device used in this paper is the Xilinx ZYNQ-SoC based PYNQ Z-1 board. Our proposed system achieves 15.15 frames per second (FPS) and 1.45 watts of power dissipation. Our result shows that our design technique is effective for real-time object detection and low power system.",https://ieeexplore.ieee.org/document/8875471/,"2019 International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)",14-17 July 2019,ieeexplore
10.1109/ICC.2019.8761110,Trajectory Prediction of UAV in Smart City using Recurrent Neural Networks,IEEE,Conferences,"The 5th generation (5G) wireless network with Unmanned aerial vehicle (UAV) is considered to be one of the most effective solutions for improving the communication coverage. However, UAV is easily affected by the wind, accompanied by a certain time delay during the air communication. Thus the inaccurate beamforming will be performed by the base station (BS), resulting in the unnecessary capacity loss. To address this issue, we propose a novel Recurrent Neural Networks (RNN)-based arrival angle predictor to predict the specific communication location of UAV under the 5G Internet of Things (IoT) networks in this paper. Specifically, a grid-based coordinate system is applied during the data preprocessing to make the training process easier and more effective. Moreover, the RNN model with the highest accuracy can be saved during the training process to ensure the real-time prediction. Simulation results reveal that the RNN-based predictor we proposed is of high prediction accuracy, which is 98% in average. Therefore, a more precise beamforming can be performed by BS to reduce the unnecessary capacity loss, resulting in a more effective and reliable communication system.",https://ieeexplore.ieee.org/document/8761110/,ICC 2019 - 2019 IEEE International Conference on Communications (ICC),20-24 May 2019,ieeexplore
10.1109/ECICE50847.2020.9301968,UAV Landmark Detection Based on Convolutional Neural Network,IEEE,Conferences,"The extensive use application of visual perception technology in Unmanned Aerial Vehicle (UAV) has brought great changes to the application of UAV in various fields. It is challenge to detect in landmark images for UAV. During UAV flight in different environments, the performance of landmark detection to deteriorate seriously have been caused by the uncertainty of landmark orientation, the diversity of landmark types and the similarities. This paper presents landmark detection of UAV based on Convolutional Neural Network (CNN). Theoretical analysis and experimental results demonstrate landmark recognition with an accuracy of at least 96% to match deployed in UAV, and the proposed CNN can make a correct classification.",https://ieeexplore.ieee.org/document/9301968/,"2020 IEEE Eurasia Conference on IOT, Communication and Engineering (ECICE)",23-25 Oct. 2020,ieeexplore
10.1109/RCAR47638.2019.9043987,UAV Path Planning Based on Biological Excitation Neural Network and Visual Odometer,IEEE,Conferences,"Unmanned aerial vehicle(UAV) have been widely used in military and civil fields due to their compact structure, flexible mobility, low cost and other advantages. With the development of artificial intelligence in recent years, more intelligent and advanced algorithms have appeared, in which machine vision, as an important branch in the field of artificial intelligence, has also been greatly developed. The limitation of space, load, endurance and computing capacity hinders the application of intelligent algorithms on UAV. In the paper a semi-autonomous control platform of the quadrotor UAV was developed and the upper and lower dual control core architecture is implemented. Based on the hardware platform, the improved visual inertia odometer (VIO) and the biological excitation neural network are used to improve the flight performance and the ability of autonomy. To solve the problem of the synchronization for VIO, a cubic spline interpolation function was employed. A biological excitation neural network was extended to solve UAV on-line path planning. It provides an on-board path planning approach for UAV in the 3D world considering the dynamic obstacles. Finally, the feasibility and stability of the designed system were verified by flight experiments.",https://ieeexplore.ieee.org/document/9043987/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore
10.23919/CISTI.2017.7975750,UAV simulator for grown-up people quality of life enhancement,IEEE,Conferences,This paper presents the development of a virtual reality simulator for the management of a UAV (Unmanned Aerial Vehicle) focused on improving the quality of life of grown-up people. The present research has collected characteristics of gestures and physical movements from users made by other related research in order to study the same interaction within a virtual world. Through this research a smaller number of gestures were created improving the user learning curve without affecting the usability. The following implementation uses a client-server architecture composed of 2 Raspberry Pi devices and a Smartphone acting as a server the communication between them was achieved by employing Bluetooth Low Energy technology. The immersive virtual experience is accomplished by using Unity 3D and Google VR tools that allowed the design and display of a playful virtual environment as an approach to promote physical and cognitive skills such as spatial thinking and hand-eye coordination. By performing maneuvers through an aerial circuit filled with obstacles the proposed UAV simulator encourages motor and mental activity while the user is being entertained. The result is the improvement of the user quality of life by avoiding cognitive and physical sedentarism.,https://ieeexplore.ieee.org/document/7975750/,2017 12th Iberian Conference on Information Systems and Technologies (CISTI),21-24 June 2017,ieeexplore
10.1109/ICAIIC48513.2020.9065203,UAV-assisted Real-time Data Processing using Deep Q-Network for Industrial Internet of Things,IEEE,Conferences,"Industrial internet of things (IIoT) enables edge computing technology to provide communication between the machines that produce a large amount of data and locate at the edge network. A task scheduling is implemented in the edge node. Furthermore, the real-time data can achieve with the lowest latency that allowed by the edge node near the edge network. However, a mobile machine such as an autonomous guided vehicle can interfere in this situation. Because the vehicle also needs service by the edge node. Over that, quality of service (QoS) performance can decrease. Therefore, this paper deploys an unmanned aerial vehicle (UAV) as an edge node to provide service to the edge network through optimizing the trajectory of UAV, where the edge network request task using a Deep Q-Network (DQN) Learning. The result shows that using machine learning, notably the DQN algorithm, can increase the number of the machine that can be provided service. Subsequently, the real-time data can achieve either the interrupt occurs at the edge node.",https://ieeexplore.ieee.org/document/9065203/,2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),19-21 Feb. 2020,ieeexplore
10.1109/ROBIO.2018.8665195,Unsupervised Feature Fusion Combined with Neural Network Applied to UAV Attitude Estimation,IEEE,Conferences,"In the field of an unmanned aerial vehicle (UAV), the navigation algorithm with high precision and easy implementation is a hot topic of research, and the key of UAV control is to obtain accurate and real-time attitude information. In this paper, a feature fusion algorithm based on unsupervised deep autoencoder (DAE) is proposed. It is used for data fusion of multiple sensors. The experimental results show that the unsupervised feature fusion algorithm can effectively improve the accuracy and has the potential to be applied to the data fusion of UAV sensors.",https://ieeexplore.ieee.org/document/8665195/,2018 IEEE International Conference on Robotics and Biomimetics (ROBIO),12-15 Dec. 2018,ieeexplore
10.1109/TIE.2019.2905808,"Design, Implementation, and Evaluation of a Neural-Network-Based Quadcopter UAV System",IEEE,Journals,"In this paper, a quadcopter unmanned aerial vehicle (UAV) system based on neural-network enhanced dynamic inversion control is proposed for multiple real-world application scenarios. A sigma-pi neural network (SPNN) is used as the compensator to reduce the model error and improve the system performance in the presence of the uncertainties of UAV dynamics, payload, and environment. Besides, we present a technical framework for fast and robust implementation of multipurpose UAV systems and develop a testbed for the evaluation of UAV control system by using a high-precision optical motion capture system. Both simulation results and experiment results demonstrate that the SPNN can reduce the inversion errors related to UAV parameter uncertainties as well as tracking errors related to unknown disturbances and unmodeled dynamics. With the help of an online neural network (NN) learning mechanism, the entire system can achieve much higher accuracy in attitude and trajectory control than that achieved by conventional proportional-integral derivative based control systems under varying flight conditions.",https://ieeexplore.ieee.org/document/8676108/,IEEE Transactions on Industrial Electronics,March 2020,ieeexplore
10.1109/TCAD.2019.2957724,Energy-Efficient Real-Time UAV Object Detection on Embedded Platforms,IEEE,Journals,"The recent technology advancement on unmanned aerial vehicle (UAV) has enabled diverse applications in vision-related outdoor tasks. Visual object detection is a crucial task among them. However, it is difficult to actually deploy detectors on embedded devices due to the challenges among energy consumption, accuracy, and speed. In this article, we address a few key challenges from the platform, application to the system, and propose an energy-efficient system for real-time UAV object detection on an embedded platform. The proposed system can achieve speed of 28.5 FPS and 2.7-FPS/W energy efficiency on the data set from 2018 low-power object detection challenges (LPODCs).",https://ieeexplore.ieee.org/document/8924612/,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,Oct. 2020,ieeexplore
10.1109/TMECH.2021.3079935,Geometrical-Based Displacement Measurement With Pseudostereo Monocular Camera on Bidirectional Cascaded Linear Actuator,IEEE,Journals,"This article details the development of a geometrical-based displacement extraction framework capable of automatically extracting critical infrastructure measurements in one sequence. The framework is a novel rail viaduct bearing inspection pipeline implemented on Bearing Inspector for Narrow-space Observation Version 2 (BINOv2). BINOv2 is a tethered custom unmanned aerial vehicle system utilized to supplant labor-intensive pipelines and enhance inspection accuracy of infrastructure conditions in confined remote locations. The algorithm accepts stereoscopic images taken from a single monocular camera on a bidirectional cascaded linear actuator system in a rack-and-pinion configuration. A point cloud model generated from the image sets then runs through a hierarchical neural network for 3-D segmentation to extract targeted regions of interest. Our training pipeline generates and forms the full model's training dataset using only a small sample of real point clouds. The point cloud generated is inadequate to form the full bearing geometry profile. Therefore, the proposed framework projects best-fit circles based on the point cloud curvature to form the full bearing geometry profile so that the required displacement measurement is available for extraction. Several experiments were conducted on a mock-up and actual operational site to validate the proposed framework's accuracy, its robustness and comparison with other state-of-the-art alternatives.",https://ieeexplore.ieee.org/document/9430727/,IEEE/ASME Transactions on Mechatronics,Aug. 2021,ieeexplore
10.1109/ACCESS.2020.3027825,Multi-Modal Data Fusion Using Deep Neural Network for Condition Monitoring of High Voltage Insulator,IEEE,Journals,"A novel Fusion Convolutional Network (FCN) is proposed in this research for potential real-time monitoring of insulators using unmanned aerial vehicle (UAV) edge devices. Precise airborne imaging of outdoor objects, such as high voltage insulators, suffers from varied object resolution, cluttered backgrounds, unclear or contaminated surfaces, and illumination conditions. Accurate information about the insulator surface condition is essential and is of a high priority since insulator breakdown is a leading cause of electrical failure. A multi-modal information fusion (MMIF) system is developed during this research to analyze and classify possible contaminations present on the electrical insulators. A novel system, referred to as FCN, consists of a Convolutional Neural Network (CNN) and a binary Multilayer Neural Network (MNN) sub-classifier. While constructing the MMIF dataset for training and testing the novel FCN, the image classification output of the CNN is combined with the leakage current values (LCV) obtained as the classification output of MNN. Each sample of the MMIF dataset is, therefore, represented as a series of fusions. Later, sub-classifiers, of the FCN, are trained to identify the contamination types in the fusion series by implementing a voting system of sub-classifiers which is trained to identify a given class. As a result of the implementation of the proposed FCN, the classification accuracy increased by 8.4%, i.e., from 92% to 99.76%. To compare and benchmark the performance of proposed FCN, conventional classification algorithms are also implemented on the fusion of features that are extracted employing the wavelet transform and PCA methods. State-of-the-art CNN architectures are also discussed on account of their time consumption and memory usage. The conceptualization of a potential hardware implementation of the proposed FCN, on emerging edge devices, is also provided for completeness of the discussion. Pertinent outcomes of this research can be further extended to other potential applications of airborne imaging.",https://ieeexplore.ieee.org/document/9210065/,IEEE Access,2020,ieeexplore
10.1109/LRA.2020.3010730,Nonlinear MPC for Collision Avoidance and Control of UAVs With Dynamic Obstacles,IEEE,Journals,"This letter proposes a Novel Nonlinear Model Predictive Control (NMPC) for navigation and obstacle avoidance of an Unmanned Aerial Vehicle (UAV). The proposed NMPC formulation allows for a fully parametric obstacle trajectory, while in this letter we apply a classification scheme to differentiate between different kinds of trajectories to predict future obstacle positions. The trajectory calculation is done from an initial condition, and fed to the NMPC as an additional input. The solver used is the nonlinear, non-convex solver Proximal Averaged Newton for Optimal Control (PANOC) and its associated software OpEn (Optimization Engine), in which we apply a penalty method to properly consider the obstacles and other constraints during navigation. The proposed NMPC scheme allows for real-time solutions using a sampling time of 50 ms and a two second prediction of both the obstacle trajectory and the NMPC problem, which implies that the scheme can be considered as a local path-planner. This letter will present the NMPC cost function and constraint formulation, as well as the methodology of dealing with the dynamic obstacles. We include multiple laboratory experiments to demonstrate the efficacy of the proposed control architecture, and to show that the proposed method delivers fast and computationally stable solutions to the dynamic obstacle avoidance scenarios.",https://ieeexplore.ieee.org/document/9145644/,IEEE Robotics and Automation Letters,Oct. 2020,ieeexplore
10.1109/ACCESS.2019.2900475,Pollution Source Localization Based on Multi-UAV Cooperative Communication,IEEE,Journals,"Harmful gas leakage accidents in chemical plants have occurred from time to time. The application of mobile robots to find odor source has become one of the hottest research topics. Compared to traditional robots, unmanned aerial vehicle (UAV) is more flexible and safer. Therefore, using multi-UAV to solve pollution source tracking is a meaningful study. In this paper, an air pollution source tracking algorithm based on artificial potential field and particle swarm optimization is proposed. The particle swarm optimization algorithm combined with artificial potential field method is used to guide the UAVs to track the plume and avoid the collisions among them. At the same time, adaptive inertia weights are used to help improve the convergence and the searchability of particles. We not only evaluated this algorithm in simulation experiments but also designed a multi-UAV pollution source tracking platform for real-world experiments. The experimental results show that the algorithm can accurately find the pollution source in a short time.",https://ieeexplore.ieee.org/document/8665856/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2020.3046499,Real-Time Energy Harvesting Aided Scheduling in UAV-Assisted D2D Networks Relying on Deep Reinforcement Learning,IEEE,Journals,"Unmanned aerial vehicle (UAV)-assisted device-to-device (D2D) communications can be deployed flexibly thanks to UAVs' agility. By exploiting the direct D2D interaction supported by UAVs, both the user experience and network performance can be substantially enhanced at public events. However, the continuous moving of D2D users, limited energy and flying time of UAVs are impediments to their applications in real-time. To tackle this issue, we propose a novel model based on deep reinforcement learning in order to find the optimal solution for the energy-harvesting time scheduling in UAV-assisted D2D communications. To make the system model more realistic, we assume that the UAV flies around a central point, the D2D users move continuously with random walk model and the channel state information encountered during each time slot is randomly time-variant. Our numerical results demonstrate that the proposed schemes outperform the existing solutions. The associated energy efficiency game can be solved in less than one millisecond by an off-the-shelf processor using trained neural networks. Hence our deep reinforcement learning techniques are capable of solving real-time resource allocation problems in UAV-assisted wireless networks.",https://ieeexplore.ieee.org/document/9303363/,IEEE Access,2021,ieeexplore
10.1109/TIM.2020.3001659,Real-Time Fault Detection for UAV Based on Model Acceleration Engine,IEEE,Journals,"With the wide applications of the unmanned aerial vehicle (UAV) in the civilian and military fields, its operational safety has drawn much attention. A series of fault detection methods are studied to avoid disasters. Due to the capabilities of strong feature extraction and massive flight data processing, the deep learning-based methods have received extensive attention. However, restricted by UAV airborne size, weight, and power consumption, a significant challenge is posed to deploy these complicated detection methods in the airborne application, which requires to run in real time. In this article, a fault detection model acceleration engine (FDMAE) for UAV real-time fault detection is realized under the airborne constraint. First, a high-performance detection model is designed based on stacked long short-term memory networks, and fault detection is achieved by a statistical threshold in this method. Second, a model pruning method based on principal component analysis is proposed to improve computing efficiency. Finally, the pruned fault detection method is optimized and integrated as a flexible acceleration engine through high-level synthesis and deployed on an airborne embedded computing platform based on a field-programmable gate array. Real UAV flight data are used to verify the proposed FDMAE. By comparing accuracy, the area under the receiver operating characteristic curve, speed, and power consumption, the effectiveness of the FDMAE is proven.",https://ieeexplore.ieee.org/document/9115090/,IEEE Transactions on Instrumentation and Measurement,Dec. 2020,ieeexplore
10.1109/LRA.2021.3063992,Real-Time Path Planning With Virtual Magnetic Fields,IEEE,Journals,"Humans and animals have learned or evolved to use magnetic fields for navigation. Knowing how to model and estimate these fields can be used for motion planning. However, computing the propagation of electromagnetic fields in a given environment requires solving complex differential equations with advanced numerical methods, and therefore it is not suitable for real-time decision making. In this latter, we present a real-time approximator for Maxwell's equations based on deep neural networks that predicts the distribution of a virtual magnetic field. We show how our approximator can be used to perform autonomous 2D navigation tasks, outperforming state-of-the-art navigation algorithms, ensuring completeness, and providing a near-optimal path up to 200 times per second without any post processing stage. We demonstrate the effectiveness of our method with physics-based simulations of an unmanned aerial vehicle, an autonomous car, as well as real-world experiments using a small off-road autonomous racing vehicle. Furthermore, we show how the approach can be applied to multi-robot systems, video game technology, and can be extended to 3D problems.",https://ieeexplore.ieee.org/document/9369851/,IEEE Robotics and Automation Letters,April 2021,ieeexplore
10.1109/ACCESS.2021.3061634,RnR: Retrieval and Reprojection Learning Model for Camera Localization,IEEE,Journals,"Camera localization is an essential technique in many applications, such as robot navigation, mixed reality, and unmanned vehicle. We are committed to solving the problem of predicting the 6-DoF pose of cameras from a single color image in a given three-Dimensional (3D) environment. In this paper, we proposed a robust learning model for it. Basically, our proposed methodology consists of two steps: image retrieval and space reprojection. The former is in charge of simultaneous localization and mapping based on pre-captured reference images that rely on the correspondence between pixel points and scene coordinates; whereas the latter carries out camera calibration between the 2D image plane and the 3D scene. Given a two-Dimensional (2D) image, the initial localization is accomplished rapidly by matching a reference image using Siamese networks. More precise localization is achieved by camera calibration between the 2D image and the 3D scene using a fully convolutional network. The experimental results on the public dataset show that our model is more robust and expandable than the previous methods. At the end of this paper, we also apply the system to Unmanned Aerial Vehicle (UAV) localization and achieve good results.",https://ieeexplore.ieee.org/document/9361658/,IEEE Access,2021,ieeexplore
10.1109/TMM.2019.2945167,Sensor-Augmented Neural Adaptive Bitrate Video Streaming on UAVs,IEEE,Journals,"Recent advances in unmanned aerial vehicle (UAV) technology have revolutionized a broad class of civil and military applications. However, the designs of wireless technologies that enable real-time streaming of high-definition video between UAVs and ground clients present a conundrum. Most existing adaptive bitrate (ABR) algorithms are not optimized for the air-to-ground links, which usually fluctuate dramatically due to the dynamic flight states of the UAV. In this paper, we present SA-ABR, a new sensor-augmented system that generates ABR video streaming algorithms with the assistance of various kinds of inherent sensor data that are used to pilot UAVs. By incorporating the inherent sensor data with network observations, SA-ABR trains a deep reinforcement learning (DRL) model to extract salient features from the flight state information and automatically learn an ABR algorithm to adapt to the varying UAV channel capacity through the training process. SA-ABR does not rely on any assumptions or models about UAV's flight states or the environment, but instead, it makes decisions by exploiting temporal properties of past throughput through the long short-term memory (LSTM) to adapt itself to a wide range of highly dynamic environments. We have implemented SA-ABR in a commercial UAV and evaluated it in the wild. We compare SA-ABR with a variety of existing state-of-the-art ABR algorithms, and the results show that our system outperforms the best known existing ABR algorithm by 21.4% in terms of the average quality of experience (QoE) reward.",https://ieeexplore.ieee.org/document/8855031/,IEEE Transactions on Multimedia,June 2020,ieeexplore
10.1109/ACCESS.2021.3050522,Spectrum-Sharing UAV-Assisted Mission-Critical Communication: Learning-Aided Real-Time Optimisation,IEEE,Journals,"We propose an unmanned aerial vehicle (UAV) communications scheme with spectrum-sharing mechanism to provide mission-critical services such as disaster recovery and public safety. Specifically, the UAVs can serve as flying base stations to provide extended network coverage for the affected area under spectrum-sharing cognitive radio networks (CRNs). To cope with the effects of network destruction in a disaster, we propose a real-time optimisation framework for resource allocation (e.g., power and number of UAVs) for CRNs assisted by UAV relays. The proposed optimisation scheme aims at optimising the network throughput of primary and secondary networks under the stringent constraint of maximum tolerable interference impinged on the primary users. We also propose a deep neural network (DNN) model to significantly reduce the execution time under real-time solution of mixed-integer UAV deployment problems. For both primary and secondary networks, our real-time optimisation algorithms impose low computational complexity, hence, have a low execution time in solving throughput optimisation problems, which demonstrates the benefit of our approached proposed for spectrum-sharing UAV-assisted mission-critical services.",https://ieeexplore.ieee.org/document/9319135/,IEEE Access,2021,ieeexplore
