id,type,publication,publisher,publication_date,database,title,url,abstract,domain
10.1016/j.eswa.2021.116203,Journal,Expert Systems with Applications,scopus,2022-03-15,sciencedirect,Semantic segmentation based stereo visual servoing of nonholonomic mobile robot in intelligent manufacturing environment,https://api.elsevier.com/content/abstract/scopus_id/85119331942,"In the interest of developing an intelligent manufacturing environment with an agile, efficient, and optimally utilized transportation system, mobile robots need to achieve a certain level of autonomy as they play an important role in carrying out transportation tasks. Bearing this in mind, in the paper we propose a novel stereo visual servoing method for nonholonomic mobile robot control based on semantic segmentation. Semantic segmentation provides a rich body of information required for an adequate decision-making process in a clustered, dynamic, and ever-changing manufacturing environment. The innovative idea behind the new visual servoing system is to utilize semantic information of the scene for visual servoing, as well as for other mobile robot tasks, such as obstacle avoidance, scene understanding, and simultaneous localization and mapping. Semantic segmentation is carried out by exploiting fully convolutional neural networks. The new visual servoing algorithm utilizes an intensity-based image registration procedure, which results in the image transformation matrix. The transformation matrix encompasses the relations of images taken at the current and desired pose, and that information is directly used for visual servoing. The developed algorithm is deployed on our own developed wheeled differential drive mobile robot RAICO (Robot with Artificial Intelligence based COgnition). The experimental evaluation is carried out in the 3D simulation environment and in the laboratory model of the real manufacturing environment. The experimental results show that the accuracy of the proposed approach is improved when compared to the state-of-the-art approaches while being robust to the partial occlusions of the scene and illumination changes.",industry
10.1016/j.ssci.2021.105529,Journal,Safety Science,scopus,2022-02-01,sciencedirect,A novel decision support system for managing predictive maintenance strategies based on machine learning approaches,https://api.elsevier.com/content/abstract/scopus_id/85118705579,"Nowadays, the industrial environment is characterised by growing competitiveness, short response times, cost reduction and reliability of production to meet customer needs. Thus, the new industrial paradigm of Industry 4.0 has gained interest worldwide, leading many manufacturers to a significant digital transformation. Digital technologies have enabled a novel approach to decision-making processes based on data-driven strategies, where knowledge extraction relies on the analysis of a large amount of data from sensor-equipped factories. In this context, Predictive Maintenance (PdM) based on Machine Learning (ML) is one of the most prominent data-driven analytical approaches for monitoring industrial systems aiming to maximise reliability and efficiency. In fact, PdM aims not only to reduce equipment failure rates but also to minimise operating costs by maximising equipment life. When considering industrial applications, industries deal with different issues and constraints relating to process digitalisation. The main purpose of this study is to develop a new decision support system based on decision trees (DTs) that guides the decision-making process of PdM implementation, considering context-aware information, quality and maturity of collected data, severity, occurrence and detectability of potential failures (identified through FMECA analysis) and direct and indirect maintenance costs. The decision trees allow the study of different scenarios to identify the conditions under which a PdM policy, based on the ML algorithm, is economically profitable compared to corrective maintenance, considered to be the current scenario. The results show that the proposed methodology is a simple and easy way to implement tool to support the decision process by assessing the different levels of occurrence and severity of failures. For each level, savings and the potential costs have been evaluated at leaf nodes of the trees aimed at defining the most suitable maintenance strategy implementation. Finally, the proposed DTs are applied to a real industrial case to illustrate their applicability and robustness.",industry
10.1016/j.eswa.2021.116045,Journal,Expert Systems with Applications,scopus,2022-02-01,sciencedirect,POSIMNET-R: An immunologic resilient approach to position routers in Industrial Wireless Sensor Networks,https://api.elsevier.com/content/abstract/scopus_id/85117584055,"Industry 4.0 has increased the interest in employing Industrial Wireless Sensor Network (IWSN) technologies in industrial automation. The advantages range from ease of installation and maintenance to reduced deployment time and infrastructure costs. However, industrial automation has critical requirements regarding network infrastructure, such as reliability and failure tolerance. Therefore, it is imperative to have an adequate placement of sensor and router nodes, to obtain a network with multiple paths, allowing the data to reach management systems within a reasonable time, even in the event of failures. The placement of router nodes has to consider latency, network lifespan, connectivity, and failure tolerance aspects in a possibly hostile environment, with classified areas and obstacles such as silos, tanks and buildings. We present a new approach, called POSIMNET-R, to place IWSN routing nodes in an industrial configuration, which circumvents forbidden areas and obstacles, based on Artificial Immunological Networks. The resulting network offers low failure rates and path redundancy criteria. The results have shown that POSIMNET-R was capable of providing a reliable network with multiple paths and resilience of the used routers equal to 81.50% in the basic case study and 73.66% in the real case scenario.",industry
10.1016/j.ress.2021.108119,Journal,Reliability Engineering and System Safety,scopus,2022-02-01,sciencedirect,Prognostics and Health Management (PHM): Where are we and where do we (need to) go in theory and practice,https://api.elsevier.com/content/abstract/scopus_id/85117331443,"We are performing the digital transition of industry, living the 4th industrial revolution, building a new World in which the digital, physical and human dimensions are interrelated in complex socio-cyber-physical systems. For the sustainability of these transformations, knowledge, information and data must be integrated within model-based and data-driven approaches of Prognostics and Health Management (PHM) for the assessment and prediction of structures, systems and components (SSCs) evolutions and process behaviors, so as to allow anticipating failures and avoiding accidents, thus, aiming at improved safe and reliable design, operation and maintenance.
                  There is already a plethora of methods available for many potential applications and more are being developed: yet, there are still a number of critical problems which impede full deployment of PHM and its benefits in practice. In this respect, this paper does not aim at providing a survey of existing works for an introduction to PHM nor at providing new tools or methods for its further development; rather, it aims at pointing out main challenges and directions of advancements, for full deployment of condition-based and predictive maintenance in practice.",industry
10.1016/j.future.2021.08.030,Journal,Future Generation Computer Systems,scopus,2022-02-01,sciencedirect,A wearable-based posture recognition system with AI-assisted approach for healthcare IoT,https://api.elsevier.com/content/abstract/scopus_id/85115908462,"Human posture recognition is a challenging task in the medical healthcare industry, when pursuing intelligence, accuracy, security, privacy, and efficiency, etc. Currently, the main posture recognition methods are captured-behaviors-based visual image analysis and wearable devices-based signal analysis. However, these methods suffer from issues such as high misjudgment rate, high-cost and low-efficiency. To address these issues, we propose a collaborative AI-IoT-based solution (namely, WMHPR) that embeds with advanced AI-assisted approach. In WMHPR, we propose the multi-posture recognition (MPR), an offline algorithm is implemented on wearable hardware, to identify posture based on multi-dimensions data. Meanwhile, an AI-based algorithm running on the cloud server (online), named Cascade-AdaBoosting-CART (CACT), is proposed to further enhance the reliability and accuracy of MPR. We recruit 20 volunteers for real-life experiments to evaluate the effectiveness, and the results show our solution is significantly outstanding in terms of accuracy and reliability while comparing with other typical algorithms.",industry
10.1016/j.comcom.2021.10.036,Journal,Computer Communications,scopus,2022-01-15,sciencedirect,LSTM-MFCN: A time series classifier based on multi-scale spatial–temporal features,https://api.elsevier.com/content/abstract/scopus_id/85119299619,"Time series classification (TSC) task attracts huge interests, since they correspond to the real-world problems in a wide variety of fields, such as industry monitoring. Deep learning methods, especially CNN and FCN, shows competitive performance in TSC task by their virtue of good adaption for raw time series and self-adapting extraction of features. Then various variants of CNN are proposed so as to make further breakthrough by the better perception to characteristics of data. Among them, LSTM-FCN and GRU-FCN who learn spatial and temporal features simultaneously are the most remarkable ones, achieving state of the art results. Therefore, inspired by their success and in consideration of the discriminative features implied in time series are diverse in size, a multimodal network LSTM-MFCN composed of multi-scale FCN (MFCN) and LSTM are proposed in this work. The gate-based network LSTM naturally fits to various terms time dependencies, and FCN with multi-scale sets of filters are capable to perceive spatial features of different range from time series curves. Besides, dilation convolution is deployed to build multi-scale receptive fields in larger level without increasing the parameters to be trained. The full perception of large multi-scale spatial–temporal features lead LSTM-MFCN to possess comprehensive and thorough grasp to time series, thus achieve even better accuracies. Finally, two representative architectures are presented specifically and their experiments on UCR datasets reveals the effectiveness and superiority of proposed LSTM-MFCN.",industry
10.1016/j.energy.2021.122359,Journal,Energy,scopus,2022-01-15,sciencedirect,Fuzzy inference system application for oil-water flow patterns identification,https://api.elsevier.com/content/abstract/scopus_id/85117714992,"Prediction of oil-water two-phase flow pattern provides an effective solution for reducing oil production costs. In this research, the fuzzy inference system (FIS) is utilized to predict fluid flow patterns and establish a new adaptable prediction model. This paper takes No. 10 industrial white oil and tap water as the research objects to simulate fluids, and analyzes the changes of the pipeline angle, the total flow of oil-water two-phase flow and the convective pattern of water cut. A data set containing 60 samples was used to create the model, and the Mamdani fuzzy model was established using MATLAB software. The results show that compared with the BP neural network algorithm, the model set forth in the present paper has higher accuracy and reliability, and can achieve real-time monitoring and effectively reduce errors, especially in the case of decision-making. In addition, the fuzzy model is demonstrated that in the entire production logging process of non-vertical wells, the use of a fuzzy inference system to predict fluid flow patterns can greatly save production costs while ensuring the safe operation of production equipment.",industry
10.1016/j.cose.2021.102500,Journal,Computers and Security,scopus,2022-01-01,sciencedirect,AntiViruses under the microscope: A hands-on perspective,https://api.elsevier.com/content/abstract/scopus_id/85118529412,"AntiViruses (AVs) are the main defense line against attacks for most users and much research has been done about them, especially proposing new detection procedures that work in academic prototypes. However, as most current and commercial AVs are closed-source solutions, in practice, little is known about their real internals: information such as what is a typical AV database size, the detection methods effectively used in each operation mode, and how often on average the AVs are updated are still unknown. This prevents research work from meeting the industrial practices more thoroughly. To fill this gap, in this work, we systematize the knowledge about AVs. To do so, we first surveyed the literature and identified existing knowledge gaps in AV internals’ working. Further, we bridged these gaps by analyzing popular (Windows, Linux, and Android) AV solutions to check their operations in practice. Our methodology encompassed multiple techniques, from tracing to fuzzing. We detail current AV’s architecture, including their multiple components, such as browser extensions and injected libraries, regarding their implementation, monitoring features, and self-protection capabilities. We discovered, for instance, a great disparity in the set of API functions hooked by the distinct AV’s libraries, which might have a significant impact in the viability of academically-proposed detection models (e.g., machine learning-based ones).",industry
10.1016/j.compind.2021.103556,Journal,Computers in Industry,scopus,2022-01-01,sciencedirect,C-Ports: A proposal for a comprehensive standardization and implementation plan of digital services offered by the “Port of the Future”,https://api.elsevier.com/content/abstract/scopus_id/85118477493,"In this paper we address the topic of a possible path to standardize the ICT services expected to be delivered by the so-called “Port of the Future”. How the most relevant technologies and Information Systems are used by the Port Communities for their businesses is discussed together with a detailed analysis of the on-going actions carried on by Standard Setting Organizations. Considering the examples given by the C-ITS Platform and the C-Roads programme at EU level, a proposal of contents to be considered in a comprehensive standardization action is given. The innovation services are therefore grouped into four bundles: (i) Vessel & Marine Navigation, (ii) e-Freight & (Intermodal) Logistics, (iii) Passenger Transport, (iv) Environmental sustainability. The standardized version of these applications will be finally labeled as C-Port services. Alongside the standardization plan, a proposal for ranking the ports on the basis of a specially-defined C-Port vector is discussed with the purpose of addressing the well-known lack of consensus around the mathematical definition of the Smart Port Index. Considering the good practice and the background offered by the Port of Livorno in terms of innovation actions, the prospected final user applications are then labeled as Day 1, Day 1.5, and Day 2 services in consideration of the technical and commercial gaps to be filled. As a case study about the evolution in the C-Port vector experienced by the Port of Livorno in the last years will also be discussed.",industry
10.1016/j.knosys.2021.107607,Journal,Knowledge-Based Systems,scopus,2021-12-25,sciencedirect,Adaptive multi-objective service composition reconfiguration approach considering dynamic practical constraints in cloud manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85117828056,"Dynamic uncertainty factors such as equipment faults are common in practically implemented cloud manufacturing (CMfg) environments, often causing the manufacturing service to be invalidated. In that case, efficient reconfiguration of the original service composition under practical constraints is critical; however, existing research scarcely focuses on it. This paper proposes a dynamic service composition reconfiguration model to bridge the gap by considering practical constraints (DSCRPC) in a real-life cloud manufacturing environment. Based on the constraints considered in this study, the DSCRPC model redefines three objectives: time (T*), cost (C*), and product service quality (Q*S*). To optimize the DSCRPC model, this study developed an adaptive multi-population multi-objective whale optimization algorithm (AMPOWOA) based on the Pareto strategy. The algorithm adopts four balancing strategies and adaptively optimizes and adjusts the key parameters under various balancing strategies through well-designed reinforcement learning models. Finally, we conduct numerical experiments and actual application case tests to compare the performances of AMPOWOA and other algorithms (MOWOA, MOHHO, NSGA-II). The results show that DSCRPC can continuously tackle the cloud manufacturing service composition (CMSC) reconfiguration issue with constraints until an order is completed. Moreover, AMPOWOA is superior to the other algorithms optimizing the DSCRPC model. This significantly enhances the robustness of service composition reconfiguration in real-life CMfg.",industry
10.1016/j.apenergy.2021.117857,Journal,Applied Energy,scopus,2021-12-15,sciencedirect,A hybrid deep learning-based online energy management scheme for industrial microgrid,https://api.elsevier.com/content/abstract/scopus_id/85115173233,"The fluctuations in electricity prices and intermittency of renewable energy systems necessitate the adoption of online energy management schemes in industrial microgrids. However, it is challenging to design effective and optimal online rolling horizon energy management strategies that can deliver assured optimality, subject to the uncertainties of volatile electricity prices and stochastic renewable resources. This paper presents an adaptable online energy management scheme for industrial microgrids that minimizes electricity costs while meeting production requirements by repeatedly solving an optimization problem over a moving control window, taking advantage of forecasted future prices and renewable energy profiles implemented by a hybrid deep learning model. The predicted values over the control horizon are assumed to be uncertain, and a multivariate Gaussian distribution is used to handle the variations in electricity prices and renewable resources around their predicted nominal values. Simulation results under different scenarios using real-world data verify the effectiveness of the proposed online energy management scheme, assessed by the corresponding gaps with respect to several selected benchmark strategies and the ideal boundaries of the best and worst known solutions. Furthermore, the robustness of the scheme is verified by considering severe errors in forecasted electricity prices and renewable profiles.",industry
10.1016/j.apenergy.2021.117733,Journal,Applied Energy,scopus,2021-12-15,sciencedirect,Controlling distributed energy resources via deep reinforcement learning for load flexibility and energy efficiency,https://api.elsevier.com/content/abstract/scopus_id/85114713033,"Behind-the-meter distributed energy resources (DERs), including building solar photovoltaic (PV) technology and electric battery storage, are increasingly being considered as solutions to support carbon reduction goals and increase grid reliability and resiliency. However, dynamic control of these resources in concert with traditional building loads, to effect efficiency and demand flexibility, is not yet commonplace in commercial control products. Traditional rule-based control algorithms do not offer integrated closed-loop control to optimize across systems, and most often, PV and battery systems are operated for energy arbitrage and demand charge management, and not for the provision of grid services. More advanced control approaches, such as MPC control have not been widely adopted in industry because they require significant expertise to develop and deploy. Recent advances in deep reinforcement learning (DRL) offer a promising option to optimize the operation of DER systems and building loads with reduced setup effort. However, there are limited studies that evaluate the efficacy of these methods to control multiple building subsystems simultaneously. Additionally, most of the research has been conducted in simulated environments as opposed to real buildings. This paper proposes a DRL approach that uses a deep deterministic policy gradient algorithm for integrated control of HVAC and electric battery storage systems in the presence of on-site PV generation. The DRL algorithm, trained on synthetic data, was deployed in a physical test building and evaluated against a baseline that uses the current best-in-class rule-based control strategies. Performance in delivering energy efficiency, load shift, and load shed was tested using price-based signals. The results showed that the DRL-based controller can produce cost savings of up to 39.6% as compared to the baseline controller, while maintaining similar thermal comfort in the building. The project team has also integrated the simulation components developed during this work as an OpenAIGym environment and made it publicly available so that prospective DRL researchers can leverage this environment to evaluate alternate DRL algorithms.",industry
10.1016/j.ijpe.2021.108296,Journal,International Journal of Production Economics,scopus,2021-12-01,sciencedirect,An integrated Delphi-MCDM-Bayesian Network framework for production system selection,https://api.elsevier.com/content/abstract/scopus_id/85114948077,"Several attempts are needed to choose the most compatible production system for achieving the desired manufacturing outputs. The significant role of manufacturing strategy deployment is selecting the production system best suited for a manufacturing firm. The appropriately chosen production system (strategic process choice) facilitates a firm to produce “order winning” outputs and provides a production competence to achieve business success. This research presents a novel framework to determine the compatible production system for a manufacturing firm. An integrated three-stage Delphi-MCDM-Bayesian Network (BN) framework has been proposed. The process choice criteria (PCC) considered for deciding production systems are identified through an in-depth literature review and then validated by experts through a Delphi method in the first stage. It resulted in the determination of twenty-six PCC. In the second stage, the multi-criteria decision-making (MCDM) based voting analytical hierarchy process (VAHP) method is adopted to determine each criterion's relative importance for a firm. The relative weights obtained are then used as input for the machine learning (ML) technique- Bayesian network (BN) in the third stage. The BN model quantifies the selection probability of production systems. The proposed Delphi-MCDM-BN framework is demonstrated using a real-life case of a “hydraulic and pneumatic valve” manufacturing firm to select a suitable production system. The three-stage framework is a novel contribution to the literature, which can be used by researchers, practitioners, and manufacturing strategists to choose an appropriate production system for any manufacturing firm.",industry
10.1016/j.asoc.2021.107859,Journal,Applied Soft Computing,scopus,2021-12-01,sciencedirect,Securing Smart Cities using LSTM algorithm and lightweight containers against botnet attacks,https://api.elsevier.com/content/abstract/scopus_id/85114806873,"Smart Cities contains millions of IoT sensors supporting critical applications such as Smart Transport, Buildings, Intelligent Vehicles, and Logistics. A central administrator appointed by the government manages and maintains the security of each node. Smart City relies upon millions of sensors that are heterogeneous and do not support standard security architecture. Different manufacturers have weak protection protocols for their products and do not update their firmware upon newly identified operating systems’ vulnerabilities. Adversaries using brute force methods exploit the lack of inbuilt security systems on IoT devices to grow their bot network. Smart cities require a standard framework combining soft computing and Deep Learning (DL) for device fleet management and complete control of sensor operating systems for absolute security. This paper presents a real-world application for IoT fleet management security using a lightweight container-based botnet detection (C-BotDet) framework. Using a three-phase approach, the framework using Artificial Intelligence detects compromised IoT devices sending malicious traffic on the network. Balena Cloud revokes API keys and prevents a compromised device from infecting other devices to form a more giant botnet. VPN (Virtual Private Network) prevents inter-device communication and routes all malicious traffic through an external server. The framework quickly updates the standard Linux-based operating system IoT device fleet without relying on different manufacturers to update their system security individually. The simulation and analysis of the C-BotDet framework are presented in a practical working environment to demonstrate its implementation feasibility.",industry
10.1016/j.livsci.2021.104700,Journal,Livestock Science,scopus,2021-11-01,sciencedirect,A review of deep learning algorithms for computer vision systems in livestock,https://api.elsevier.com/content/abstract/scopus_id/85118744270,"In livestock operations, systematically monitoring animal body weight, biometric body measurements, animal behavior, feed bunk, and other difficult-to-measure phenotypes is manually unfeasible due to labor, costs, and animal stress. Applications of computer vision are growing in importance in livestock systems due to their ability to generate real-time, non-invasive, and accurate animal-level information. However, the development of a computer vision system requires sophisticated statistical and computational approaches for efficient data management and appropriate data mining, as it involves massive datasets. This article aims to provide an overview of how deep learning has been implemented in computer vision systems used in livestock, and how such implementation can be an effective tool to predict animal phenotypes and to accelerate the development of predictive modeling for precise management decisions. First, we reviewed the most recent milestones achieved with computer vision systems and the respective deep learning algorithms implemented in Animal Science studies. Then, we reviewed the published research studies in Animal Science which used deep learning algorithms as the primary analytical strategy for image classification, object detection, object segmentation, and feature extraction. The great number of reviewed articles published in the last few years demonstrates the high interest and rapid development of deep learning algorithms in computer vision systems across livestock species. Deep learning algorithms for computer vision systems, such as Mask R-CNN, Faster R-CNN, YOLO (v3 and v4), DeepLab v3, U-Net and others have been used in Animal Science research studies. Additionally, network architectures such as ResNet, Inception, Xception, and VGG16 have been implemented in several studies across livestock species. The great performance of these deep learning algorithms suggests an improved predictive ability in livestock applications and a faster inference. However, only a few articles fully described the deep learning algorithms and their implementation. Thus, information regarding hyperparameter tuning, pre-trained weights, deep learning backbone, and hierarchical data structure were missing. We summarized peer-reviewed articles by computer vision tasks (image classification, object detection, and object segmentation), deep learning algorithms, animal species, and phenotypes including animal identification and behavior, feed intake, animal body weight, and many others. Understanding the principles of computer vision and the algorithms used for each application is crucial to develop efficient systems in livestock operations. Such development will potentially have a major impact on the livestock industry by predicting real-time and accurate phenotypes, which could be used in the future to improve farm management decisions, breeding programs through high-throughput phenotyping, and optimized data-driven interventions.",industry
10.1016/j.jmapro.2021.09.048,Journal,Journal of Manufacturing Processes,scopus,2021-11-01,sciencedirect,Joint active search and neuromorphic computing for efficient data exploitation and monitoring in additive manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85117322411,"The recent integration of imaging technology with additive manufacturing (AM) leads to the plethora of in-process and high-dimensional data. Machine learning (ML) methods have been implemented to improve understanding of defect formation in AM-built parts and controlling process variability in real-time. However, modern ML methods, in particular deep neural networks, are empowered by massive high-quality labeled data, which are limited in AM due to the following reasons: First, large data labeling is often tedious, costly, and requires substantial human efforts with considerable expertise. Second, the performance of the learning methods depends to a great extent on the presence of positive data instances (i.e., defective) as they are more informative for monitoring. Third, the rare positives result in a severe imbalanced dataset poses critical challenges in training ML methods designed with the assumption that the input contains an equal number of instances from each class. In this research, we propose novel annotation and learning with limited number of data through the integration of active search and hyperdimensional computing (HDC). The active search is developed to benefit from a single bandit model to learn about the data distribution (exploration) while sampling from the regions potentially containing more positives (exploitation). HDC is introduced as an alternative computing method that mimics important brain functionalities and encodes data with high-dimensional vectors, thereby enabling single-pass learning with just a few samples. Experimental results on a real-world case study of drag link joint build show the proposed model locates the rare positives thoroughly and detects lack of fusion defects with the accuracy of 89.58%, in 3.221 ± 0.029 second training time and with only 66 sample data. The joint active search and neuromorphic computing framework is shown to have strong potentials for general applications in a diverse set of domains with in-situ imaging data.",industry
10.1016/j.addma.2021.102328,Journal,Additive Manufacturing,scopus,2021-11-01,sciencedirect,In situ infrared temperature sensing for real-time defect detection in additive manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85115355988,"Melt pool temperature is a critical parameter for the majority of additive manufacturing processes. Monitoring of the melt pool temperature can facilitate the real-time detection of various printing defects such as voids, over-extrusion, filament breakage, clogged nozzle, etc. that occur either naturally or as the result of malicious hacking activity. This study uses an in situ, multi-sensor approach for monitoring melt pool temperature in which non-contact infrared temperature sensors with customized field of view move along with the extruder of a fused deposition modeling-based printer and sense melt pool temperature from a very short working distance regardless of its X-Y translational movements. A statistical method for defect detection is developed and utilized to identify temperature deviations caused by intentionally implemented defects. Effective detection for multiple defect types and sizes is demonstrated using both a simple L-shaped test geometry and a more complex industry standard test article. Strengths and limitations of this approach are presented, and the potential for expansion via more advanced data analysis techniques such as machine learning are discussed.",industry
10.1016/j.psj.2021.101437,Journal,Poultry Science,scopus,2021-11-01,sciencedirect,Pharmacokinetic/pharmacodynamic profiles of baicalin against Mycoplasma gallisepticum in an in vivo infection model,https://api.elsevier.com/content/abstract/scopus_id/85115144118,"Mycoplasma gallisepticum (
                        M. gallisepticum
                     ), a devastating avian pathogen that commonly causes chronic respiratory disease in chicken, is responsible for tremendous economic losses to the poultry industry. Baicalin is the main constituent of Scutellaria baicalensis that shows potential therapeutic effects against M. gallisepticum. However, the pharmacokinetic/pharmacodynamics (PK/PD) profiles of baicalin against M. gallisepticum are not well understood. The main objective of the present study was to determine the relationship between the PK/PD index and efficacy of baicalin in the M. gallisepticum infection model in chickens. The experiments were carried out on 10-day-old chickens that were challenged with M. gallisepticum in the bilateral air sacs. While, baicalin was orally administrated once in a day for 3 consecutive days, started from d 3 postinfection. Ultra-performance liquid chromatography (UPLC) was used to evaluate the PK parameters of baicalin at doses of 200, 400, and 600 mg/kg in M. gallisepticum-infected chickens. Real-time PCR (RT-PCR) was used for the quantitative detection of M. gallisepticum in lungs. The PK and PD data were fitted to WinNonlin software to evaluate the PK/PD profiles of baicalin against M. gallisepticum. The minimum inhibitory concentration (MIC) of baicalin against M. gallisepticum strain Rlow was 31.25 µg/mL. The in vivo data suggested that baicalin concentration in the lung tissues was higher than plasma (1.21–1.73 times higher). The ratios of AUC24h/MIC of baicalin against bacteriostatic, bactericidal, and eradication were 0.62, 1.33, and 1.49 h, respectively. In conclusion, these results provided potential reference for future clinical dose selection of baicalin and evaluation of susceptibility breakpoints.",industry
10.1016/j.jmbbm.2021.104728,Journal,Journal of the Mechanical Behavior of Biomedical Materials,scopus,2021-11-01,sciencedirect,What can artificial intelligence and machine learning tell us? A review of applications to equine biomechanical research,https://api.elsevier.com/content/abstract/scopus_id/85112485329,"Artificial intelligence (AI) and machine learning (ML) are fascinating interdisciplinary scientific domains where machines are provided with an approximation of human intelligence. The conjecture is that machines are able to learn from existing examples, and employ this accumulated knowledge to fulfil challenging tasks such as regression analysis, pattern classification, and prediction. The horse biomechanical models have been identified as an alternative tool to investigate the effects of mechanical loading and induced deformations on the tissues and structures in humans. Many reported investigations into bone fatigue, subchondral bone damage in the joints of both humans and animals, and identification of vital parameters responsible for retaining integrity of anatomical regions during normal activities in all species are heavily reliant on equine biomechanical research. Horse racing is a lucrative industry and injury prevention in expensive thoroughbreds has encouraged the implementation of various measurement techniques, which results in massive data generation. ML substantially accelerates analysis and interpretation of data and provides considerable advantages over traditional statistical tools historically adopted in biomechanical research. This paper provides the reader with: a brief introduction to AI, taxonomy and several types of ML algorithms, working principle of a feedforward artificial neural network (ANN), and, a detailed review of the applications of AI, ML, and ANN in equine biomechanical research (i.e. locomotory system function, gait analysis, joint and bone mechanics, and hoof function). Reviewing literature on the use of these data-driven tools is essential since their wider application has the potential to: improve clinical assessments enabling real-time simulations, avoid and/or minimize injuries, and encourage early detection of such injuries in the first place.",industry
10.1016/j.asoc.2021.107784,Journal,Applied Soft Computing,scopus,2021-11-01,sciencedirect,Towards learning behavior modeling of military logistics agent utilizing profit sharing reinforcement learning algorithm,https://api.elsevier.com/content/abstract/scopus_id/85112396580,"Agent-based modeling has become a beneficial tool in describing the complex and intelligent decision-making behaviors of military logistics entities, which is essential in exploring military logistics system. A challenging task in this field is the learning behavior modeling of military logistics agents. Profit sharing (PS) reinforcement learning algorithm is a representative exploitation-oriented method describing empirical reinforcement learning mechanism, and has been successfully applied to a variety of real-world problems. However, constructing the learning behavior model of military logistics agents is difficult by merely using the original PS algorithm. This difficulty is due to the actual characteristics of equipment support operations and military requirements, such as experience sharing, cooperative action, and hierarchical control. To address this issue, we propose an improved PS algorithm by introducing cooperative task reward correction parameters, experience sharing learning function, and superior command controlled function. We use the research methodology centering on the basic process of the improved PS algorithm as basis to construct the architecture of the learning behavior model of military logistics agents and its corresponding model of elements. Furthermore, we design the implementation algorithm of the learning behavior model. Lastly, we conduct a case study of a tactical military industrial logistics simulation system, thereby verifying the feasibility and effectiveness of the learning behavior model. We find that the improved PS algorithm and corresponding learning behavior model have more advantages than the original PS algorithm.",industry
10.1016/j.aquaeng.2021.102192,Journal,Aquacultural Engineering,scopus,2021-11-01,sciencedirect,"An integrated framework of sensing, machine learning, and augmented reality for aquaculture prawn farm management",https://api.elsevier.com/content/abstract/scopus_id/85112001426,"The rapid growth of prawn farming on an international scale will play an important role in meeting the protein requirements of an expanding global population. Efficient management of the commercial ponds for healthy production of prawns is the key mantra of success in this industry. It is a necessity to maintain the water quality parameters in these ponds within specific ranges to create an ideal environment of optimal growth of healthy prawns. The current practice of water quality data collection and their usage for decision making on most farms is not efficient and does not take full advantage of the latest technologies. The research presented in this paper aimed at addressing this problem by systematic investigation and development of an integrated framework where (i) modern sensors were investigated for their suitability and deployed for continuous monitoring of the water quality variables in prawn ponds; (ii) novel machine learning models were investigated based on collected data and deployed to accurately forecast pond status over next 24 h. This provides farmers insight into upcoming situations and take necessary measures to avoid catastrophic situations; and (iii) augmented reality-based visualisation methods were investigated for improved data capture process and efficient decision making through real-time interactive interfaces. The paper presents the integrated framework as well as the details of sensing, machine learning, and augmented reality components. We found that (i) YSI EXO2 Multi-Sonde is the best sensor for continuous monitoring of prawn ponds; (ii) ForecastNet (our developed machine learning model) provides best forecasting results with symmetric mean absolute percentage error of 6.1 %, 9.6 %, and 8.5 % for dissolved oxygen, pH, and temperature; and (iii) augmented reality-based interactive interface achieves accuracy as high as 89.2 % for management decisions with at least 41 % less time. The experience of the project as presented in this paper can act as a guide for researchers as well as prawn farmers to take advantage of latest sensors, machine learning algorithms and augmented reality tools.",industry
10.1016/j.talanta.2021.122608,Journal,Talanta,scopus,2021-11-01,sciencedirect,"PIXE based, Machine-Learning (PIXEL) supported workflow for glass fragments classification",https://api.elsevier.com/content/abstract/scopus_id/85109431731,"This paper presents a structured workflow for glass fragment analysis based on a combination of Elemental Analysis using PIXE and Machine Learning tools, with the ultimate goal of standardizing and helping forensic efforts. The proposed workflow was implemented on glass fragments received from the Israeli DIFS (Israeli Police Force's Division of Identification and Forensic Sciences) that were collected from various vehicles, including glass fragments from different manufacturers and years of production. We demonstrate that this workflow can produce models with high (>80%) accuracy in identifying glass fragment's origins and provide a test-case demonstrating how the model can be applied in real-life forensic events. We provide a standard, reproducible methodology that can be used in many forensic domains beyond glass fragments, for example, Gun Shot Residue, flammable liquids, illegal substances, and more.",industry
10.1016/j.renene.2021.05.155,Journal,Renewable Energy,scopus,2021-11-01,sciencedirect,A deep learning approach towards the detection and recognition of opening of windows for effective management of building ventilation heat losses and reducing space heating demand,https://api.elsevier.com/content/abstract/scopus_id/85107941088,"Building ventilation accounts for up to 30% of the heat loss in commercial buildings and 25% in industrial buildings. To effectively aid the reduction of energy consumption in the building sector, the development of demand-driven control systems for heating ventilation and air-conditioning (HVAC) is necessary. In countries with temperate climates such as the UK, many buildings depend on natural ventilation strategies such as openable windows, which are useful for reducing overheating prevalence during the summer. The manual opening and adjustment of windows by occupants, particularly during the heating season, can lead to substantial heat loss and consequent energy consumption. This could also result in the unnecessary or over ventilation of the space, or the fresh air is more than what is required to ensure adequate air quality. Furthermore, energy losses build up when windows are left open for extended periods. Hence, it is important to develop control strategies that can detect and recognise the period and amount of window opening in real-time and at the same time adjust the HVAC systems to minimise energy wastage and maintain indoor environment quality and thermal comfort. This paper presents a vision-based deep learning framework for the detection and recognition of manual window operation in buildings. A trained deep learning model is deployed into an artificial intelligence-powered camera. To assess the proposed strategy's capabilities, building energy simulation was used with various operation profiles of the opening of the windows based on various scenarios. Initial experimental tests were conducted within a university lecture room with a south-facing window. Deep learning influenced profile (DLIP) was generated via the framework, which uses real-time window detection and recognition data. The generated DLIP were compared with the actual observations, and the initial detection results showed that the method was capable of identifying windows that were opened and had an average accuracy of 97.29%. The results for the three scenarios showed that the proposed strategy could potentially be used to help adjust the HVAC setpoint or alert the occupants or building managers to prevent unnecessary heating demand. Further developments include enhancing the framework ability to detect multiple window opening types and sizes and the detection accuracy by optimising the model.",industry
10.1016/j.ymssp.2021.107915,Journal,Mechanical Systems and Signal Processing,scopus,2021-11-01,sciencedirect,Machine learning based frequency modelling,https://api.elsevier.com/content/abstract/scopus_id/85103975336,"Detection of cracks in structures has always been an important research topic in the industrial domain closely associated with aerospace, mechanical, marine and civil engineering. The presence of the cracks alters the dynamic response properties. Hence, it becomes crucial to locate these cracks in the structures to avoid any catastrophic failures and maintain structural integrity and performance. The study's objective is to propose two distinct statistical procedures for conducting the machine learning experiment for modelling the frequency and show the effect of experiment design on the results. In the study, the predictive performance of machine learning models and their ensembles is compared within each experiment design and between two experimental designs for the task of prediction of first six natural frequencies of a fixed ended cracked beam. The study highlights the significance of more than one experimental design to reduce the confirmation bias in the research and discusses the proposed methods' generalizability over the different modelling constraints and modelling parameters. The study also discusses a real-world implementation of the learned machine learning models from the perspective of Bayesian optimization.",industry
10.1016/j.jhlste.2020.100275,Journal,"Journal of Hospitality, Leisure, Sport and Tourism Education",scopus,2021-11-01,sciencedirect,Industry 4.0 technologies in tourism education: Nurturing students to think with technology,https://api.elsevier.com/content/abstract/scopus_id/85092173436,"The Industry 4.0 revolution is bringing major transformations in the tourism systems design suitable for technologically oriented consumers. Indeed, methods and technologies introduced by Big Data, Automation, Virtual and augmented reality, Robotics and ICT well fit with the Tourism 4.0 paradigm. However, tourism students are not yet trained on techniques, issues and methods related to the Industry 4.0 framework.
                  Hence, relying on a careful examination of the literature on tourism market trends linked to the offer of innovative technological services, we identified conceptual, methodological, technological and practical skills to be developed in an academic curriculum for Tourism Science students. Learning path were focused on: i) processes of data acquisition from social media, ii) data analysis using Machine Learning techniques and iii) data design into significant elements useful to implement communication systems in the tourism field.
               
                  Results
                  showed that the most of participants achieved a medium-high evaluation for the implementation of the communication systems, applying appropriately techniques and tools learned along the course. Furthermore, the high percentage of students satisfaction registered in relation to the course, revealed that students enjoyed this experience. Outcomes reflects the acquisition and the awareness of those skills that will enable students to be conscious protagonists of their role in tourism 4.0.",industry
10.1016/j.probengmech.2021.103173,Journal,Probabilistic Engineering Mechanics,scopus,2021-10-01,sciencedirect,Machine learning based digital twin for stochastic nonlinear multi-degree of freedom dynamical system,https://api.elsevier.com/content/abstract/scopus_id/85117922944,"The potential of digital twin technology is immense, specifically in the infrastructure, aerospace, and automotive sector. However, practical implementation of this technology is not at an expected speed, specifically because of lack of application-specific details. In this paper, we propose a novel digital twin framework for stochastic nonlinear multi-degree of freedom (MDOF) dynamical systems. The proposed digital twin has four modules — (a) a physics-based nominal model, (b) a data collection module, (c) algorithm for real-time update of the digital twin and (d) module for predicting future state. The modules for real-time update and prediction are based on the so-called gray-box modeling approach, and utilizes both physics based and data driven frameworks; this enables the proposed digital twin to generalize and predict future responses. The gray box modeling framework used within the digital twin is developed by coupling Bayesian filtering and machine learning algorithm. Although, the proposed digital twin can be used with any machine learning regression algorithm, we have used Gaussian process in this study. Performance of the proposed approach is illustrated using two examples. Results obtained indicate the applicability and excellent performance of the proposed digital twin framework.",industry
10.1016/j.jocs.2021.101443,Journal,Journal of Computational Science,scopus,2021-10-01,sciencedirect,Towards versatile conversations with data-driven dialog management and its integration in commercial platforms,https://api.elsevier.com/content/abstract/scopus_id/85115363526,"Conversational interfaces have recently become a ubiquitous element in both the personal sphere by easing access to services, and industrial environments by the automation of services, improved customer support and its corresponding cost savings. However, designing the dialog model used by these interfaces to decide system responses is still a hard-to-accomplish task for complex conversational interactions. This paper describes a data-driven dialog management technique, which provides flexibility to develop, deploy and maintain this module. Various configurations for classification algorithms are assessed with two dialog corpora of different application domains, size, dimensionalities and set of possible system responses. The results of the evaluation show satisfactory accuracy and coherence rates in both tasks. As a proof of concept, our proposal has also been integrated with DialogFlow, a platform provided by Google to design conversational user interfaces. Our proposal has been assessed with a real use case, proving that it can be deployed in conjunction with commercial platforms, obtaining satisfactory results for the objective and subjective assessments completed.",industry
10.1016/j.techfore.2021.120986,Journal,Technological Forecasting and Social Change,scopus,2021-10-01,sciencedirect,Big data and firm marketing performance: Findings from knowledge-based view,https://api.elsevier.com/content/abstract/scopus_id/85113910474,"A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications. This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms’ strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms’ activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.",industry
10.1016/j.asoc.2021.107702,Journal,Applied Soft Computing,scopus,2021-10-01,sciencedirect,OrbitNet: A new CNN model for automatic fault diagnostics of turbomachines,https://api.elsevier.com/content/abstract/scopus_id/85111487515,"Unplanned outage due to faults in a high-fidelity turbomachine such as steam turbine and centrifugal compressor often results in the reduced reliability and productivity of a factory while increasing its maintenance costs. Shaft orbit images generated from turbomachine vibration signals have been used to diagnose component faults. However, the existing methods were developed mostly by either using features extracted from orbits or utilizing simulation data which may produce inaccurate results in practical applications due to system complexity and data uncertainties. This paper presents a novel deep learning convolution neural network methodology for accurately automatic diagnostics of multiple faults in general rotating machines by adeptly integrating advanced signal processing with orbit images augmentation, considering the high non-linearity and uncertainty of sensed vibration signals. Environmental noise in vibration signals are filtered through the integration of multiresolution discrete wavelet packet transform and Bayesian hypothesis testing-based automatic thresholding. Shaft orbit images generated from the cleansed vibration data are augmented to increase their representativity and generalization. A novel multi-layer convolutional neural network model, OrbitNet, is specially designed to improve its generality and robustness while avoid possible overfitting in fault identification of various turbomachines. The proposed model retains the pattern information in the axis trajectory to the greatest extent, with the ability of accurately capturing features of various faults in different turbomachines. A generic implementation procedure is proposed for automatic fault diagnosis of rotating machinery based on the presented methodology. A comparison study is conducted to demonstrate the effectiveness and feasibility of the proposed methodology by using the sensed vibration signals collected from three real-world centrifugal compressors, two steam turbines and one generator with four different fault modes including imbalance, friction, misalignment and oil whirl.",industry
10.1016/j.conengprac.2021.104903,Journal,Control Engineering Practice,scopus,2021-10-01,sciencedirect,Synthesizing labeled data to enhance soft sensor performance in data-scarce regions,https://api.elsevier.com/content/abstract/scopus_id/85111246946,"Quality variables are key indicators of the operating performance in industrial processes. Because they are difficult to measure, soft sensor models can be adopted to predict them timely. For accurate prediction, sufficient training data are necessary to construct a good soft sensor model. In practical industrial processes, however, data labeled with quality variables are usually deficient in the desired region. Particularly, when the process is just switched to a new mode, available data in this new mode are initially quite a few. In this paper, a novel data synthesis method based on the regressor-embedded semi-supervised variational autoencoder (RSSVAE) model is proposed to generate synthetic labeled data when the original labeled data are inadequate. The proposed model utilizes not only the original data in the data-scarce region but also the data in other regions, which share some common information with the scarce data. Meanwhile, data synthesis and model correction mechanism are implemented iteratively to avoid model biases. Once the synthetic labeled data of the data-scarce region are acquired, they are combined with the original labeled data to establish a local soft sensor and predict the quality variables of the unlabeled data. Finally, a real ammonia synthesis process is introduced to demonstrate the effectiveness of the proposed method.",industry
10.1016/j.asoc.2021.107644,Journal,Applied Soft Computing,scopus,2021-10-01,sciencedirect,Production scheduling in industrial mining complexes with incoming new information using tree search and deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85109174667,"Industrial mining complexes have implemented digital technologies and advanced sensors to monitor and gather real-time data about their different operational aspects, starting from the supply of materials from the mineral deposits involved to the products provided to customers. However, technologies are not available to respond in real-time to the incoming new information to adapt the short-term production schedule of a mining complex. A short-term production schedule determines the daily/weekly/monthly sequence of extraction, the destination of materials and utilization of processing streams. This paper presents a novel self-learning artificial intelligence algorithm for mining complexes that learns, from its own experience, to adapt the short-term production scheduling decisions by responding to incoming new information. The algorithm plays the game of short-term production scheduling on its own using a Monte Carlo tree search to train a deep neural network agent that adapts the short-term production schedule with incoming new information. The deep neural network agent evaluates the short-term production scheduling decisions and, in parallel, performs searches using the Monte Carlo tree search to generate experiences. The experiences are then used to train the agent. The agent improves the strength of the tree search, which results in an even stronger self-play to generate better experiences. An application of the proposed algorithm at a real-world copper mining complex shows its exceptional performance to adapt the 13-week short-term production schedule almost in real-time. The adapted production schedule successfully meets the different production requirements and makes better use of the processing capabilities, while also increasing copper concentrate production by 7% and cash flows by 12% compared to the initial production schedule. A video of the proposed algorithm can be found at https://youtu.be/_gSbzxMc_W8.",industry
10.1016/j.cofs.2021.03.014,Journal,Current Opinion in Food Science,scopus,2021-10-01,sciencedirect,Novel digital technologies implemented in sensory science and consumer perception,https://api.elsevier.com/content/abstract/scopus_id/85104656313,"New and emerging digital technologies have been implemented in sensory science, which minimize subjectivity and biases in data acquisition and interpretation compared to traditional methods. These technologies have enabled the incorporation of physiological and emotional responses of panelists elicited by food, beverage, and packaging stimuli through accurate and unbiased information from different sensor technologies. This review focused on recent advances of digital technologies used for sensory science, such as (i) software for sensory science, (ii) integration of biometrics to assess physiological and emotional responses of panelists, (iii) incorporation of virtual, augmented, and mixed reality, and (iv) sensor technology (electronic noses and tongues) for sensory analysis. Rapid data acquisition and results’ interpretation could open the way to automation and implementation of Artificial Intelligence that could revolutionize the food and beverage industries. It also presents a proposed framework for integrating and implementing digital technologies through the food chain from farm/manufacturing facilities to the palate.",industry
10.1016/j.rcim.2021.102176,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2021-10-01,sciencedirect,Robotic grasping: from wrench space heuristics to deep learning policies,https://api.elsevier.com/content/abstract/scopus_id/85104603575,"The robotic grasping task persists as a modern industry problem that seeks autonomous, fast implementation, and efficient techniques. Domestic robots are also a reality demanding a delicate and accurate human–machine interaction, with precise robotic grasping and handling. From decades ago, with analytical heuristics, to recent days, with the new deep learning policies, grasping in complex scenarios is still the aim of several works’ that propose distinctive approaches. In this context, this paper aims to cover recent methodologies’ development and discuss them, showing state-of-the-art challenges and the gap to industrial applications deployment. Given the complexity of the related issue associated with the elaborated proposed methods, this paper formulates some fair and transparent definitions for results’ assessment to provide researchers with a clear and standardised idea of the comparison between the new proposals.",industry
10.1016/j.knosys.2021.107261,Journal,Knowledge-Based Systems,scopus,2021-09-27,sciencedirect,Federated conditional generative adversarial nets imputation method for air quality missing data,https://api.elsevier.com/content/abstract/scopus_id/85111226892,"The air quality is a topic of extreme concern that attracts a lot of attention in the world. Many intelligent air quality monitoring networks have been deployed in various places, especially in big cities. These monitoring networks collect air quality data with some missing data for some reasons which pose an obstacle for air quality publishing and studies. Generative adversarial nets (GAN) methods have achieved state-of-the-art performance in missing data imputation. GAN-based imputation method needs enough training data while one monitoring network has just a few and poor quality monitoring data and these data sets do not meet the independent identical distribution (IID) condition. Therefore, one monitoring network side needs to utilize more monitoring data from other sides as far as possible. However, in the real world, these air quality monitoring networks are owned by different organizations — companies, the government even some secret units. Many of them cannot share detailed monitoring data due to security, privacy, and industrial competition. In this paper, it is the first time to propose a conditional GAN imputation method under a federated learning framework to solve the data sets that come from diverse data-owners without sharing. Furthermore, we improve the vanilla conditional GAN performance with Wasserstein distance and “Hint mask” trick. The experimental results show that our GAN-based imputation methods can achieve the best performance. And our federated GAN imputation method outperforms the GAN imputation method trained locally for each participant which means our imputation model can work. Our proposed federated GAN method can benefit model quality by increasing access to air quality data through private multi-institutional collaborations. We further investigate the effects of data geographical distribution across collaborating participants on model quality and, interestingly, we find that the GAN training process with a federated learning framework performs more stable.",industry
10.1016/j.pmcj.2021.101445,Journal,Pervasive and Mobile Computing,scopus,2021-09-01,sciencedirect,Towards generating a reliable device-specific identifier for IoT devices,https://api.elsevier.com/content/abstract/scopus_id/85111971982,"A significant number of IoT devices are being deployed in the wild, mostly in remote locations and in untrusted conditions. This could include monitoring an electronic perimeter fence or a critical infrastructure such as telecom and power grids. Such applications rely on the fidelity of data reported from the IoT devices, and hence it is imperative to identify the trustworthiness of the remote device before taking decisions. Existing approaches use a secret key usually stored in volatile or non-volatile memory for creating an encrypted digital signature. However, these techniques are vulnerable to malicious attacks and have significant computation and energy overhead. This paper presents a novel device-specific identifier, IoT-ID that captures the device characteristics and can be used towards device identification. IoT-ID is based on physically unclonable functions (PUFs), that exploit variations in the manufacturing process to derive a unique fingerprint for integrated circuits. In this work, we design novel PUFs for Commercially Off the Shelf (COTS) components such as clock oscillators and ADC, to derive IoT-ID for a device. Hitherto, system component PUFs are invasive and rely on additional dedicated hardware circuitry to create a unique fingerprint. A highlight of our PUFs is doing away with special hardware. IoT-ID is non-invasive and can be invoked using simple software APIs running on COTS components. IoT-ID has the following key properties viz., constructability, real-time, uniqueness, and reproducibility, making them robust device-specific identifiers.
                  We present detailed experimental results from our live deployment of 50 IoT devices running over a month. Our edge machine learning algorithm has 100% accuracy in uniquely identifying the 50 devices in our deployment and can run locally on the resource-constrained IoT device. We show the scalability of IoT-ID with the help of numerical analysis on 1000s of IoT devices. Further, we discuss approaches to evaluate and improve the reliability of the IoT-ID.
                        1
                     
                     
                        1
                        This manuscript is an extension of the paper ‘IoT-ID: A Novel Device-Specific Identifier Based on Unique Hardware Fingerprints’ Vaidya et al. (2020) published in 2020 IEEE/ACM Fifth International Conference on Internet-of-Things Design and Implementation (IoTDI).",industry
10.1016/j.ijcip.2021.100424,Journal,International Journal of Critical Infrastructure Protection,scopus,2021-09-01,sciencedirect,Industrial intrusion detection based on the behavior of rotating machine,https://api.elsevier.com/content/abstract/scopus_id/85111013464,"In this study, a new industrial intrusion detection method is introduced for the control system of rotating machines as critical assets in many industries. Data tampering is a major attack on the control systems which disrupts the functionality of the asset. Hence, our objective is to detect data manipulations in the system. We use the behavior of the rotating machine to propose new industrial intrusion detection for the control system of the rotating machine by machine learning techniques. The behavior is elicited by the data of sensors under all the conditions of the rotating machine operation. In this work, the nonlinear regression, novelty detection, outlier detection, and classification approaches are implemented to create behavioral model. On each implementation, online data are compared with the real data of behavior prediction model during the operation of the rotating machine to detect any abnormality. According to our experimental results, the accuracy of the behavioral models created by the One-classSVM novelty detection, k- Nearest Neighbor (kNN) outlier detection, decision tree classifier, k-Neighbors classifier, random forest classifier, and AdaBoost classifier is obtained as 0.98, 0.994, 0.999, 0.999, 0.999, and 0.999, respectively. The results indicate that the proposed industrial intrusion detection method is able to detect the data tampering attacks on the control system of the rotating machines very accurately.",industry
10.1016/j.segan.2021.100511,Journal,"Sustainable Energy, Grids and Networks",scopus,2021-09-01,sciencedirect,Multi-interval programming based scheduling of appliances with user preferences and dynamic pricing in residential area,https://api.elsevier.com/content/abstract/scopus_id/85110609599,"In industrial and commercial sectors, numerous countries had successfully implemented the dynamic pricing as a solution to the problem of high power demand in peak hours. But, an extensive use of real-time pricing in the residential electricity sector is hugely missing. In order to boost the efficiency of electricity market by demand response, real-time pricing needs to be implemented into residential sector also. In this paper the proposed algorithm is implemented for residential consumers of different categories with real time pricing data of ComEd, Northern Illinois Power Company, and Alactra Utilities Corporation. The proposed algorithm incorporates single interval and multi interval programming for different power pricing schemes. The proposed algorithm is suggested using metaheuristic optimization techniques viz. cuckoo search (CS), adaptive cuckoo search (ACS) and Hybrid GA–PSO for the optimum scheduling of residential appliances. The objective of this paper is to minimize the monthly electricity bill cost as well as peak demand under uncertain electricity prices. The comparative analysis of optimal solutions obtained by various artificial intelligence techniques validates the high performance of proposed algorithm. It facilitates both the residential consumer and utilities with benefits.",industry
10.1016/j.ijcip.2021.100452,Journal,International Journal of Critical Infrastructure Protection,scopus,2021-09-01,sciencedirect,Adversarial attacks and mitigation for anomaly detectors of cyber-physical systems,https://api.elsevier.com/content/abstract/scopus_id/85107972529,"The threats faced by cyber-physical systems (CPSs) in critical infrastructure have motivated research into a multitude of attack detection mechanisms, including anomaly detectors based on neural network models. The effectiveness of anomaly detectors can be assessed by subjecting them to test suites of attacks, but less consideration has been given to adversarial attackers that craft noise specifically designed to deceive them. While successfully applied in domains such as images and audio, adversarial attacks are much harder to implement in CPSs due to the presence of other built-in defence mechanisms such as rule checkers (or invariant checkers). In this work, we present an adversarial attack that simultaneously evades the anomaly detectors and rule checkers of a CPS. Inspired by existing gradient-based approaches, our adversarial attack crafts noise over the sensor and actuator values, then uses a genetic algorithm to optimise the latter, ensuring that the neural network and the rule checking system are both deceived. We implemented our approach for two real-world critical infrastructure testbeds, successfully reducing the classification accuracy of their detectors by over 50% on average, while simultaneously avoiding detection by rule checkers. Finally, we explore whether these attacks can be mitigated by training the detectors on adversarial samples.",industry
10.1016/j.asoc.2021.107574,Journal,Applied Soft Computing,scopus,2021-09-01,sciencedirect,Nonlinear-based Chaotic Harris Hawks Optimizer: Algorithm and Internet of Vehicles application,https://api.elsevier.com/content/abstract/scopus_id/85107718159,"Harris Hawks Optimizer (HHO) is one of the many recent algorithms in the field of metaheuristics. The HHO algorithm mimics the cooperative behavior of Harris Hawks and their foraging behavior in nature called surprise pounce. HHO benefits from a small number of controlling parameters setting, simplicity of implementation, and a high level of exploration and exploitation. To alleviate the drawbacks of this algorithm, a modified version called Nonlinear based Chaotic Harris Hawks Optimization (NCHHO) is proposed in this paper. NCHHO uses chaotic and nonlinear control parameters to improve HHO’s optimization performance. The main goal of using the chaotic maps in the proposed method is to improve the exploratory behavior of HHO. In addition, this paper introduces a nonlinear control parameter to adjust HHO’s exploratory and exploitative behaviors. The proposed NCHHO algorithm shows an improved performance using a variety of chaotic maps that were implemented to identify the most effective one, and tested on several well-known benchmark functions. The paper also considers solving an Internet of Vehicles (IoV) optimization problem that showcases the applicability of NCHHO in solving large-scale, real-world problems. The results demonstrate that the NCHHO algorithm is very competitive, and often superior, compared to the other algorithms. In particular, NCHHO provides 92% better results in average to solve the uni-modal and multi-modal functions with problem dimension sizes of D = 30 and 50, whereas, with respect to the higher dimension problem, our proposed algorithm shows 100% consistent improvement with D = 100 and 1000 compared to other algorithms. In solving the IoV problem, the success rate was 62.5%, which is substantially better in comparison with the state-of-the-art algorithms. To this end, the proposed NCHHO algorithm in this paper demonstrates a promising method to be widely used by different applications, which brings benefits to industries and businesses in solving their optimization problems experienced daily , such as resource allocation, information retrieval, finding the optimal path for sending data over networks, path planning, and so many other applications.",industry
10.1016/j.jss.2021.110993,Journal,Journal of Systems and Software,scopus,2021-09-01,sciencedirect,Automated defect prioritization based on defects resolved at various project periods,https://api.elsevier.com/content/abstract/scopus_id/85107687766,"Defect prioritization is mainly a manual and error-prone task in the current state-of-the-practice. We evaluated the effectiveness of an automated approach that employs supervised machine learning. We used two alternative techniques, namely a Naive Bayes classifier and a Long Short-Term Memory model. We performed an industrial case study with a real project from the consumer electronics domain. We compiled more than 15,000 issues collected over 3 years. We could reach an accuracy level up to 79.36% and we had 3 observations. First, Long Short-Term Memory model has a better accuracy when compared with a Naive Bayes classifier. Second, structured features lead to better accuracy compared to textual descriptions. Third, accuracy is not improved by considering increasingly earlier defects as part of the training data. Increasing the size of the training data even decreases the accuracy compared to the results, when we use data only regarding the recently resolved defects.",industry
10.1016/j.jnca.2021.103116,Journal,Journal of Network and Computer Applications,scopus,2021-09-01,sciencedirect,Traffic Engineering in Hybrid Software Defined Network via Reinforcement Learning,https://api.elsevier.com/content/abstract/scopus_id/85107660469,"The emergence of Software Defined Network (SDN) provides a centralized and flexible approach to route network flows. Due to the technical and economic challenges in upgrading to a fully SDN-enabled network, hybrid SDN, with a partial deployment of SDN switches in a traditional network, has been a prevailing network architecture. Meanwhile, Traffic Engineering (TE) in the hydbrid SDN has attracted wide attentions from academia and industry. Previous studies on TE in the hybrid SDN are either traffic-oblivious or time-consuming, which causes routing schemes failed in responding to the dynamically-changing traffic rapidly and intelligently. Therefore, in this paper, we propose a Reinforcement Learning (RL) based method, which learns a traffic-splitting agent to address the dynamically-changing traffic and achieve the link load balancing in the hybrid SDN. Specifically, to rapidly and intelligently determine a routing scheme to the new traffic demands, a traffic-splitting agent is designed and learnt offline by exploiting the RL algorithm to establish the direct relationship between traffic demands and traffic-splitting policies. Once the traffic-splitting agent is learnt, the effective traffic-splitting policies, which are used to determine the traffic-splitting ratios on SDN switches, can be generated rapidly. Additionally, to meet the interactive requirements for learning a traffic-splitting agent, a reasonable simulation environment is proposed to be constructed to avoid routing loops when traffic-splitting policies are taken. Extensive evaluations on different topologies and real traffic demands demonstrate that the proposed method achieves the comparable network performance and performs superiorities in rapidly generating the satisfying routing schemes.",industry
10.1016/j.scs.2021.103009,Journal,Sustainable Cities and Society,scopus,2021-09-01,sciencedirect,Applying machine learning in intelligent sewage treatment: A case study of chemical plant in sustainable cities,https://api.elsevier.com/content/abstract/scopus_id/85106305327,"Nowadays, sewage treatment in sustainable cities attracts more researchers both from academic and industrial communities. Especially, since industrial sewage is normally highly toxic, which could cause serious pollution in a city and lead to health problems of residents, it is critical to monitor and predictably maintain sewage treatment facilities in cities. This paper presents an intelligent sewage treatment system based on machine learning and Internet of Things sensors to assist to manage the sewage treatment in a fine chemical plant. The implemented system has operated for twenty months, acquired multi-dimension data such as temperatures in different treatment processes, operation parameters of devices, and real-time Chemical Oxygen Demand (COD). Since the change trend of outflow COD is highly related to operation status, this paper innovatively uses different types of temperature and water inflow data as model inputs and applies three algorithms to make prediction, which are Support Vector Regression (SVR), Long Short-Term Memory (LSTM) neural network, and Gated Recurrent Unit (GRU) neural network. The experimental results show that GRU model performs better (MAPE = 10.18%, RMSE = 35.67, MAE = 31.16) than LSTM and SVR. This study can be extended to various sewage treatment scenarios in sustainable cities.",industry
10.1016/j.energy.2021.120700,Journal,Energy,scopus,2021-09-01,sciencedirect,Nonlinear generalized predictive controller based on ensemble of NARX models for industrial gas turbine engine,https://api.elsevier.com/content/abstract/scopus_id/85105736036,"New design and operation of modern gas turbine engines (GTEs) are becoming more and more complex where several limitations and control modes should be fulfilled at the same time to accomplish a safe and ideal performance for the engine. For this purpose, a constrained multi-input multi-output (MIMO) non-linear model predictive controller (NMPC) based on neural network model is designed to fulfill the control requirements of a Siemens SGT-A65 three-spool aero-derivative gas turbine engine (ADGTE) used for power generation. However, the implementation of NMPC in real time has two challenges: Firstly, the design of an accurate non-linear model, which can run many times faster than real time. Secondly, the usage of a rapid and reliable optimization algorithm to solve the optimization problem in real time. To solve these issues, the constrained MIMO NMPC is created based on the generalized predictive control (GPC) algorithm as a result of its clarity, ease of use, and capacity to deal with problems in one algorithm. In addition, seven ensembles of eight multi-input single-output (MISO) non-linear autoregressive network with exogenous inputs (NARX) models are used as a base model for the GPC controller to predict the future process outputs. Estimation of free and forced responses of the GPC based on the neural network (NN) model of the plant each sampling time without performing instantaneous linearization is proposed in this study, which reduces the NMPC optimization problem to a linear optimization problem at each sampling step. In addition, the Hildreth's quadratic programming algorithm is used to solve the quadratic optimization problem within the NMPC controller, which offers ease of use and reliability in real time applications. To demonstrate the performance of the NNGPC controller developed in this study, we have compared the performance of the neural network generalized predictive control (NNGPC) controller to the existing controller of the SGT-A65 engine. The simulation results show that the NNGPC has demonstrated output responses with less oscillatory behavior and smoother control actions to the sudden variation in the electric load disturbance than those observed in the existing min-max controller. However, the min-max controller has faster response than that of the NNGPC controller.",industry
10.1016/j.asoc.2021.107465,Journal,Applied Soft Computing,scopus,2021-09-01,sciencedirect,Click-event sound detection in automotive industry using machine/deep learning,https://api.elsevier.com/content/abstract/scopus_id/85105315919,"In the automotive industry, despite the robotic systems on the production lines, factories continue employing workers in several custom tasks getting for semi-automatic assembly operations. Specifically, the assembly of electrical harnesses of engines comprises a set of connections between electrical components. Despite the task is easy to perform, employees tend not to notice that a few components are not being connected properly due to physical fatigue provoked by repetitive tasks. This yields a low quality of the assembly production line and possible hazards. In this work, we propose a sound detection system based on machine/deep learning (ML/DL) approaches to identify click sounds produced when electrical harnesses are connected. The purpose of this system is to count the number of connections properly made and to feedback to the employees. We collect and release a public dataset of 25,000 click sounds of 25 ms length at 22 kHz during three months of assembly operations in an automotive production line located in Mexico. Then, we design an ML/DL-based methodology for click sound detection of assembled harnesses under real conditions of a noisy environment (noise level ranging from 
                        
                           −
                           16
                           .
                           67
                        
                      dB to 
                        
                           −
                           12
                           .
                           87
                        
                      dB) including other machinery sounds. Our best ML/DL model (i.e., a combination between five acoustic features and an optimized convolutional neural network) is able to detect click sounds in a real assembly production line with an accuracy of 
                        
                           94
                           .
                           55
                           ±
                           0
                           .
                           83
                        
                      %. To the best of our knowledge, this is the first time a click sounds detection system in assembling electrical harnesses of engines for giving feedback to the workers is proposed and implemented in a real-world automotive production line. We consider this work valuable for the automotive industry on how to apply ML/DL approaches for improving the quality of semi-automatic assembly operations.",industry
10.1016/j.simpa.2021.100081,Journal,Software Impacts,scopus,2021-08-01,sciencedirect,OpenICS: Open image compressive sensing toolbox and benchmark[Formula presented],https://api.elsevier.com/content/abstract/scopus_id/85115856142,"The real-world application of image compressive sensing is largely limited by the lack of standardization in implementation and evaluation. To address this limitation, we present OpenICS, an image compressive sensing toolbox that implements multiple popular image compressive sensing algorithms into a unified framework with a standardized user interface. Furthermore, a corresponding benchmark is also proposed to provide a fair and complete evaluation of the implemented algorithms. We hope this work can serve the growing research community of compressive sensing and the industry to facilitate the development and application of image compressive sensing.",industry
10.1016/j.cag.2021.04.035,Journal,Computers and Graphics (Pergamon),scopus,2021-08-01,sciencedirect,DIMNet: Dense implicit function network for 3D human body reconstruction,https://api.elsevier.com/content/abstract/scopus_id/85105858338,"In recent years, with the improvement of artificial intelligence technology, it has become possible to reconstruct high-precision 3D human body models based on ordinary RGB images. The current 3D human body reconstruction technology requires complex external equipment to scan all angles of the human body, which is complicated to be implemented and cannot be popularized. In order to solve this problem, this paper applies deep learning models on reconstructing 3D human body based on monocular images. First of all, this paper uses Stacked Hourglass network to perform convolution operations on monocular images collected from different views. Then Multi-Layer Perceptrons (MLPs) are used to decode the encoded high-level images. The feature codes in the two views(main and side) are fused, and the interior and exterior points are classified by the fusion features, so as to obtain the corresponding 3D occupancy field. At last, the Marching Cube algorithm is used for 3D reconstruction with a specific threshold and then we use Laplace smoothing algorithm to remove artifacts. This paper proposes a dense sampling strategy based on the important joint points of the human body, which has a certain optimization effect on the realization of high-precision 3D reconstruction. The performance of the proposed scheme has been validated on the open source datasets, MGN dataset and the THuman dataset, provided by Tsinghua University. The proposed scheme can reconstruct features such as clothing folds, color textures, and facial details,and has great potential to be applied in different applications.",industry
10.1016/j.eswa.2021.114820,Journal,Expert Systems with Applications,scopus,2021-08-01,sciencedirect,Machine Learning for industrial applications: A comprehensive literature review,https://api.elsevier.com/content/abstract/scopus_id/85102967505,"Machine Learning (ML) is a branch of artificial intelligence that studies algorithms able to learn autonomously, directly from the input data. Over the last decade, ML techniques have made a huge leap forward, as demonstrated by Deep Learning (DL) algorithms implemented by autonomous driving cars, or by electronic strategy games. Hence, researchers have started to consider ML also for applications within the industrial field, and many works indicate ML as one the main enablers to evolve a traditional manufacturing system up to the Industry 4.0 level. Nonetheless, industrial applications are still few and limited to a small cluster of international companies. This paper deals with these topics, intending to clarify the real potentialities, as well as potential flaws, of ML algorithms applied to operation management. A comprehensive review is presented and organized in a way that should facilitate the orientation of practitioners in this field. To this aim, papers from 2000 to date are categorized in terms of the applied algorithm and application domain, and a keyword analysis is also performed, to details the most promising topics in the field. What emerges is a consistent upward trend in the number of publications, with a spike of interest for unsupervised and especially deep learning techniques, which recorded a very high number of publications in the last five years. Concerning trends, along with consolidated research areas, recent topics that are growing in popularity were also discovered. Among these, the main ones are production planning and control and defect analysis, thus suggesting that in the years to come ML will become pervasive in many fields of operation management.",industry
10.1016/j.eswa.2021.114753,Journal,Expert Systems with Applications,scopus,2021-08-01,sciencedirect,A league-winner algorithm for defect classification in an industrial web inspection system,https://api.elsevier.com/content/abstract/scopus_id/85102641846,"This paper presents a modification to be added to multiclass classifiers, that improves their performance when classifying, in this case, defects appearing in polyethylene films. It aims to classify a new defect by confronting every defect type against each of the other types. In a simplified way, the type that results winner in more matches is the type that the defect belongs to. Different ways of implementing neural networks have been tested, using Gradient Descent and techniques for backpropagation. These techniques have been formally and understandably explained. In addition, a method based on decision trees has been included for comparison. Different issues related to the practical implementation of the detection and identification system within an installed production chain are addressed. The resulting system has been incorporated as a real inspection automatism in a polyethylene manufacturing line, and trained with defects previously obtained from the same line.",industry
10.1016/j.jclepro.2021.127385,Journal,Journal of Cleaner Production,scopus,2021-07-25,sciencedirect,Improving degradation of real wastewaters with self-heating magnetic nanocatalysts,https://api.elsevier.com/content/abstract/scopus_id/85105709482,"Industrial effluents contain a wide range of organic pollutants that present harmful effects on the environment and deprived communities with no access to clean water. As this organic matter is resistant to conventional treatments, Advanced Oxidation Processes (AOPs) have emerged as a suitable option to counteract these environmental challenges. Engineered iron oxide nanoparticles have been widely tested in AOPs catalysis, but their full potential as magnetic induction self-heating catalysts has not been studied yet on real and highly contaminated industrial wastewaters. In this study we have designed a self-heating catalyst with a finely tuned structure of small cores (10 nm) aggregates to develop multicore particles (40 nm) with high magnetic moment and high colloidal stability. This nanocatalyst, that can be separated by magnetic harvesting, is able to increase reaction temperatures (up to 90 °C at 1 mg/mL suspension in 5 min) under the action of alternating magnetic fields. This efficient heating was tested in the degradation of a model compound (methyl orange) and real wastewaters, such as leachate from a solid landfill (LIX) and colored wastewater from a textile industry (TIW). It was possible to increase reaction rates leading to a reduction of the chemical oxygen demand of 50 and 90%, for TIW and LIX. These high removal and degradation ability of the magnetic nanocatalyst was sustained with the formation of strong reactive oxygen species by a Fenton-like mechanism as proved by electron paramagnetic resonance. These findings represent an important advance for the industrial implementation of a scalable, non-toxic, self-heating catalysts that can certainly enhance AOP for wastewater treatment in a more sustainable and efficient way.",industry
10.1016/j.procs.2021.06.013,Conference Proceeding,Procedia Computer Science,scopus,2021-07-01,sciencedirect,Mathematical model of chemical process prediction for industrial safety risk assessment,https://api.elsevier.com/content/abstract/scopus_id/85112600838,The article presents a mathematical model of the functioning of the technological process of styrene production using neural network technologies. The use of a direct propagation neural network with a single hidden layer trained on an experimental sample is considered. An algorithm for forming a neural network is proposed. The model is implemented as a software module. The results of predicting the process of chemical production of styrene based on real data and recommendations for using the developed model in the process of assessing the industrial safety of particularly dangerous production processes are presented.,industry
10.1016/j.jmsy.2021.04.005,Journal,Journal of Manufacturing Systems,scopus,2021-07-01,sciencedirect,LearningADD: Machine learning based acoustic defect detection in factory automation,https://api.elsevier.com/content/abstract/scopus_id/85106283308,"Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061 s in 99 % probability.",industry
10.1016/j.tifs.2021.04.042,Journal,Trends in Food Science and Technology,scopus,2021-07-01,sciencedirect,Efficient extraction of deep image features using convolutional neural network (CNN) for applications in detecting and analysing complex food matrices,https://api.elsevier.com/content/abstract/scopus_id/85105814254,"Background
                  The development of techniques and methods for rapidly and reliably detecting and analysing food quality and safety products is of significance for the food industry. Traditional machine learning algorithms based on handcrafted features normally have poor performance due to their limited representation capacity for complex food characteristics. Recently, the convolutional neural network (CNN) emerges as an effective and potential tool for feature extraction, which is considered the most popular architecture of deep learning and has been increasingly applied for the detection and analysis of complex food matrices.
               
                  Scope and approach
                  In the current review, the structure of CNN, the method of feature extraction based on 1-D, 2-D and 3-D CNN models, and multi-feature aggregation methods are introduced. Applications of CNN as a depth feature extractor for detecting and analyzing complex food matrices are discussed, including meat and aquatic products, cereals and cereal products, fruits and vegetables, and others. In addition, data sources, model architecture and overall performance of CNN with other existing methods are compared, and trends of future studies on applying CNN for food detection and analysis are also highlighted.
               
                  Key findings and conclusions
                  CNN combined with nondestructive detection techniques and computer vision system show great potential for effectively and efficiently detecting and analysing complex food matrices, and the features based on CNN show better performance and outperform the features handcrafted or those extracted by machine learning algorithms. Although there still remains some challenges in using CNN, it is expected that CNN models will be deployed on mobile devices for real-time detection and analysis of food matrices in future.",industry
10.1016/j.ins.2021.01.013,Journal,Information Sciences,scopus,2021-07-01,sciencedirect,Attributed community search based on effective scoring function and elastic greedy method,https://api.elsevier.com/content/abstract/scopus_id/85101624086,"In recent years, with the proliferation of rich attribute information available for entities in real-world networks and the increasing demand for more personalized community searches, attributed community search (ACS), an upgraded version of the community search problem, has attracted great attention from the both academic and industry areas. Some algorithms have been proposed to solve this novel research problem. However, they have a deficiency in evaluating the quality of the attributed community structure, which may mislead them and discover less valuable structures. In this paper, we make up for this defect, and propose the SFEG algorithm to better solve the ACS problem. SFEG designs a more effective scoring function to measure the quality of the discovered attributed community structure, and presents an elastic greedy optimization method to quickly maximize the function value to determine the target community with a specific meaning. The extensive experiments conducted on the attributed graph datasets with ground-truth communities show that our algorithm significantly outperforms the state-of-the-art.",industry
10.1016/j.ymssp.2020.107510,Journal,Mechanical Systems and Signal Processing,scopus,2021-06-16,sciencedirect,Metric-based meta-learning model for few-shot fault diagnosis under multiple limited data conditions,https://api.elsevier.com/content/abstract/scopus_id/85100211264,"The real-world large industry has gradually become a data-rich environment with the development of information and sensor technology, making the technology of data-driven fault diagnosis acquire a thriving development and application. The success of these advanced methods depends on the assumption that enough labeled samples for each fault type are available. However, in some practical situations, it is extremely difficult to collect enough data, e.g., when the sudden catastrophic failure happens, only a few samples can be acquired before the system shuts down. This phenomenon leads to the few-shot fault diagnosis aiming at distinguishing the failure attribution accurately under very limited data conditions. In this paper, we propose a new approach, called Feature Space Metric-based Meta-learning Model (FSM3), to overcome the challenge of the few-shot fault diagnosis under multiple limited data conditions. Our method is a mixture of general supervised learning and episodic metric meta-learning, which will exploit both the attribute information from individual samples and the similarity information from sample groups. The experiment results demonstrate that our method outperforms a series of baseline methods on the 1-shot and 5-shot learning tasks of bearing and gearbox fault diagnosis across various limited data conditions. The time complexity and implementation difficulty have been analyzed to show that our method has relatively high feasibility. The feature embedding is visualized by t-SNE to investigate the effectiveness of our proposed model.",industry
10.1016/j.chemolab.2021.104314,Journal,Chemometrics and Intelligent Laboratory Systems,scopus,2021-06-15,sciencedirect,A scalable approach for the efficient segmentation of hyperspectral images,https://api.elsevier.com/content/abstract/scopus_id/85105360467,"The number of applications of hyperspectral imaging (HSI) is steadily increasing, as technology evolves and cameras become more affordable. However, the volume of data in a hyperspectral image is large (order of Gigabytes) and standard off-the-shelf algorithms for multi-channel image analysis cannot be readily applied, due to the prohibitive computational time and large memory requirements. Therefore, new scalable approaches are required to perform hyperspectral image analysis. In this article we address an efficient methodology for conducting Unsupervised Image Segmentation – one of the basic and most fundamental image analysis operations. In the methodology proposed, unsupervised segmentation is conducted after transforming the spectral and spatial dimensions of the raw hyperspectral image into a more compact representation using multivariate and multiresolution techniques. The clusters identified in the compact image representation are then used to train a discriminative classifier. The classifier is then adapted and transferred for application to the raw image, where it will efficiently label all the original pixels. With the proposed methodology, the computational expensive operations (unsupervised clustering and classifier learning) are minimized, whereas the efficient implementation of the classifier guarantees the analysis at the native resolution. The effectiveness of the proposed methodology was tested on a real case study considering an industrial hyperspectral image capturing the reflectance spectrum for several objects made of different unknown materials. A significant reduction in the computational cost was achieved without compromising the quality of the unsupervised segmentation, demonstrating the potential of the proposed approach.",industry
10.1016/j.addma.2021.101961,Journal,Additive Manufacturing,scopus,2021-06-01,sciencedirect,Deep representation learning for process variation management in laser powder bed fusion,https://api.elsevier.com/content/abstract/scopus_id/85105695571,"Laser Powder Bed Fusion (LPBF) is an additive manufacturing process where laser power is applied to fuse the spread powder and fabricate industrial parts in a layer by layer fashion. Despite its great promise in fabrication flexibility, print quality has long been a major barrier for its widespread implementation. Traditional offline post-manufacturing inspections to detect the defects in finished products are expensive and time-consuming and thus cannot be applied in real-time monitoring and control. In-situ monitoring methods by relying on the in-process sensor data, on the other hand, can provide viable alternatives to aid with the online detection of anomalies during the process. Given the crucial importance of melt pool characteristics to the quality of final products, this paper provides a framework to process the melt pool images by a configuration of Convolutional Auto-Encoder (CAE) neural networks. The network’s corresponding bottleneck layer learns a deep yet low-dimensional representation from melt pools while preserving the spatial correlation and complex features intrinsic in the images. As opposed to the manual annotation of data by X-ray imaging or destructive tests, an agglomerative clustering algorithm is applied to these representations to automatically extract the anomalies and annotate the data accordingly. A control charting scheme based on Hotelling’s T
                     2 and S
                     2 statistics is then developed to monitor the process’s stability by keeping track of the learned representations and residuals obtained from the reconstruction of original images. Testing the proposed methodology on the collected data from an experimental build demonstrates that the method can extract a set of complex features that are inextricable otherwise by using hand-crafted feature engineering methods. Moreover, through extensive numerical studies, it is shown that the proposed feature extraction and statistical process monitoring scheme is capable of detecting the anomalies in real-time with accuracy and F
                     1 score of about 95% and 82%, respectively.",industry
10.1016/j.compeleceng.2021.107121,Journal,Computers and Electrical Engineering,scopus,2021-06-01,sciencedirect,Efficient neural networks for edge devices,https://api.elsevier.com/content/abstract/scopus_id/85103242184,"Due to limited computation and storage resources of industrial internet of things (IoT) edge devices, many emerging intelligent industrial IoT applications based on deep neural networks (DNNs) heavily depend on cloud computing for computation and storage. However, cloud computing faces technical issues in long latency, poor reliability, and weak privacy, resulting in the need for on-device computation and storage. On-device computation is essential for many time-critical industrial IoT applications, which require real-time data processing. In this paper, we review three major research areas for on-device computation, specifically quantization, pruning, and network architecture design. The three techniques could enable a DNN model to be deployed on edge devices for real-time computation and storage, mainly due to the reduction of computation and space complexity. More importantly, these techniques could make DNNs applicable to industrial IoT devices.",industry
10.1016/j.renene.2021.03.008,Journal,Renewable Energy,scopus,2021-06-01,sciencedirect,Intelligent energy management based on SCADA system in a real Microgrid for smart building applications,https://api.elsevier.com/content/abstract/scopus_id/85102248554,"Energy management is one of the main challenges in Microgrids (MGs) applied to Smart Buildings (SBs). Hence, more studies are indispensable to consider both modeling and operating aspects to utilize the upcoming results of the system for the different applications. This paper presents a novel energy management architecture model based on complete Supervisory Control and Data Acquisition (SCADA) system duties in an educational building with an MG Laboratory (Lab) testbed, which is named LAMBDA at the Electrical and Energy Engineering Department of the Sapienza University of Rome. The LAMBDA MG Lab simulates in a small scale a SB and is connected with the DIAEE electrical network. LAMBDA MG is composed of a Photovoltaic generator (PV), a Battery Energy Storage System (BESS), a smart switchboard (SW), and different classified loads (critical, essential, and normal) some of which are manageable and controllable (lighting, air conditioning, smart plugs operating into the LAB). The aim of the LAMBDA implementation is making the DIAEE smart for energy saving purposes. In the LAMBDA Lab, the communication architecture consists in a complex of master/slave units and actuators carried out by two main international standards, Modbus (industrial serial standard for electrical and technical monitoring systems) and Konnex (an open standard for commercial and domestic building automation). Making the electrical department smart causes to reduce the required power from the main grid. Hence, to achieve the aims, results have been investigated in two modes. Initially, the real-time mode based on the SCADA system, which reveals real daily power consumption and production of different sources and loads. Next, the simulation part is assigned to shows the behavior of the main grid, loads and BESS charging and discharging based on energy management system. Finally, the proposed model has been examined in different scenarios and evaluated from the economic aspect.",industry
10.1016/j.cja.2020.09.011,Journal,Chinese Journal of Aeronautics,scopus,2021-06-01,sciencedirect,Framework and development of data-driven physics based model with application in dimensional accuracy prediction in pocket milling,https://api.elsevier.com/content/abstract/scopus_id/85097765922,"In the manufacturing of thin wall components for aerospace industry, apart from the side wall contour error, the Remaining Bottom Thickness Error (RBTE) for the thin-wall pocket component (e.g. rocket shell) is of the same importance but overlooked in current research. If the RBTE reduces by 30%, the weight reduction of the entire component will reach up to tens of kilograms while improving the dynamic balance performance of the large component. Current RBTE control requires the off-process measurement of limited discrete points on the component bottom to provide the reference value for compensation. This leads to incompleteness in the remaining bottom thickness control and redundant measurement in manufacturing. In this paper, the framework of data-driven physics based model is proposed and developed for the real-time prediction of critical quality for large components, which enables accurate prediction and compensation of RBTE value for the thin wall components. The physics based model considers the primary root cause, in terms of tool deflection and clamping stiffness induced Axial Material Removal Thickness (AMRT) variation, for the RBTE formation. And to incorporate the dynamic and inherent coupling of the complicated manufacturing system, the multi-feature fusion and machine learning algorithm, i.e. kernel Principal Component Analysis (kPCA) and kernel Support Vector Regression (kSVR), are incorporated with the physics based model. Therefore, the proposed data-driven physics based model combines both process mechanism and the system disturbance to achieve better prediction accuracy. The final verification experiment is implemented to validate the effectiveness of the proposed method for dimensional accuracy prediction in pocket milling, and the prediction accuracy of AMRT achieves 0.014 mm and 0.019 mm for straight and corner milling, respectively.",industry
10.1016/j.apenergy.2021.116688,Journal,Applied Energy,scopus,2021-05-15,sciencedirect,Advanced price forecasting in agent-based electricity market simulation,https://api.elsevier.com/content/abstract/scopus_id/85102042154,"Machine learning and agent-based modeling are two popular tools in energy research. In this article, we propose an innovative methodology that combines these methods. For this purpose, we develop an electricity price forecasting technique using artificial neural networks and integrate the novel approach into the established agent-based electricity market simulation model PowerACE. In a case study covering ten interconnected European countries and a time horizon from 2020 until 2050 at hourly resolution, we benchmark the new forecasting approach against a simpler linear regression model as well as a naive forecast. Contrary to most of the related literature, we also evaluate the statistical significance of the superiority of one approach over another by conducting Diebold–Mariano hypothesis tests. Our major results can be summarized as follows. Firstly, in contrast to real-world electricity price forecasts, we find the naive approach to perform very poorly when deployed model-endogenously (mean absolute percentage error 0.40–0.53). Secondly, although the linear regression performs reasonably well (mean absolute percentage error 0.17–0.32), it is outperformed by the neural network approach (mean absolute percentage error 0.17–0.21). Thirdly, the use of an additional classifier for outlier handling substantially improves the forecasting accuracy, particularly for the linear regression approach. Finally, the choice of the model-endogenous forecasting method has a clear impact on simulated electricity prices. This latter finding is particularly crucial since these prices are a major results of electricity market models.",industry
10.1016/j.comnet.2021.107955,Journal,Computer Networks,scopus,2021-05-08,sciencedirect,Elastic Computing Resource Virtualization Method for a Service-centric Industrial Internet of Things,https://api.elsevier.com/content/abstract/scopus_id/85102477910,"The industrial Internet of Things (IIoT) enables the interconnection of machines, devices, resources, and computing technologies to improve the reliability of manufacturing services. The role of Software-Defined Networks (SDNs) and Network Function Virtualization (NFV) are exploited in the IIoT environment to ensure effective management and computing resource utilization. Based on the SDN and NFV paradigms, this article introduces a novel elastic computing resource virtualization (ECRV) method to improve the flexibility of resource management in the IIoT. The need for virtualization is obtained by identifying the control and process platforms used in industrial task management. Support vector machine-based classification learning is used to achieve balanced identification, and prevents unnecessary distribution of limited resources, Support vector machine helps to retain flexibility in task control processes that use available industrial resources. By separating the process and control platforms, service dissemination is improved and backlogs in task processing are decreased. The proposed method could provide flexible virtualization and reduces the service response time and task failure.",industry
10.1016/j.eti.2021.101527,Journal,Environmental Technology and Innovation,scopus,2021-05-01,sciencedirect,Barriers to the digitalisation and innovation of Australian Smart Real Estate: A managerial perspective on the technology non-adoption,https://api.elsevier.com/content/abstract/scopus_id/85104052467,"The real estate sector brings a fortune to the global economy. But, presently, this sector is regressive and uses traditional methods and approaches. Therefore, it needs a technological transformation and innovation in line with the Industry 4.0 requirements to transform into smart real estate. However, it faces the barriers of disruptive digital technology (DDT) adoption and innovation that need effective management to enable such transformation. These barriers present managerial challenges that affect DDT adoption and innovation in smart real estate. The current study assesses these DDTs adoption and innovation barriers facing the Australian real estate sector from a managerial perspective. Based on a comprehensive review of 72 systematically retrieved and shortlisted articles, we identify 21 key barriers to digitalisation and innovation. The barriers are grouped into the technology-organisation-external environment (TOE) categories using a Fault tree. Data is collected from 102 real estate and property managers to rate and rank the identified barriers. The results show that most of the respondents are aware of the DDTs and reported AI (22.5% of respondents), big data (12.75%) and VR (12.75%) as the most critical technologies not adopted so far due to costs, organisation policies, awareness, reluctance, user demand, tech integration, government support and funding. Overall, the highest barrier (risk) scores are observed for high costs of software and hardware (T1), high complexity of the selected technology dissemination system (T2) and lack of government incentives, R&D support, policies, regulations and standards (E1). Among the TOE categories, as evident from the fault tree analysis, the highest percentage of failure to adopt the DDT is attributed to E1 in the environmental group. For the technological group, the highest failure reason is attributed to T2. And for the organisational group, the barrier with the highest failure chances for DDT adoption is the lack of organisational willingness to invest in digital marketing (O4). These barriers must be addressed to pave the way for DDT adoption and innovation in the Australian real estate sector and move towards smart real estate.",industry
10.1016/j.eswa.2020.114399,Journal,Expert Systems with Applications,scopus,2021-05-01,sciencedirect,A reinforcement learning-based algorithm for the aircraft maintenance routing problem,https://api.elsevier.com/content/abstract/scopus_id/85098682553,"With recent developments in the airline industry worldwide, the competition among the industry has increased largely with many key players in the market. In order to generate profits, the industry has paid much attention to generate optimal routes that are maintenance feasible. The main aim of operational aircraft maintenance routing problem (OAMRP) is to generate these optimal routes for each aircraft that are maintenance feasible and follow the constraints defined by the Federal Aviation Administration (FAA). In this paper, the OAMRP is studied with two main objectives. First, to propose a formulation of a network flow-based Integer Linear Programming (ILP) framework for the OAMRP that considers three main maintenance constraints simultaneously: maximum flying-hour, limit on the number of take-offs between two consecutive maintenance checks and the work-force capacity. Second, to develop a new reinforcement learning-based algorithm which can be used to solve the problem, quickly and efficiently, as compared to commonly available optimization software. Finally, the evaluation of the proposed algorithm on real case datasets obtained from a major airline located in the Middle East verifies that the algorithm generates high-quality solutions quickly for both medium and large-scale flight schedule dataset.",industry
10.1016/j.jmsy.2021.02.012,Journal,Journal of Manufacturing Systems,scopus,2021-04-01,sciencedirect,Robust diagnosis with high protection to gas turbine failures identification based on a fuzzy neuro inference monitoring approach,https://api.elsevier.com/content/abstract/scopus_id/85101807612,"Modern industry requires the development of new monitoring and diagnostic procedures, which enable the detection, localization, and isolation of faults. For sustainable solutions in terms of operational safety and availability, while bringing out zero accidents, zero downtime, and zero faults, for a trend acting on environmental issues. Towards this development, this work proposes solutions for the monitoring of gas turbines and their real-time implementation, in order to approximate and predict the degradation of the components of this system, by an approach of faults detection and isolation, based on an adaptive neural-fuzzy inference system. This will develop a reliable approach to maintain and monitor gas turbines, in case of failure or accident to prevent in real-time and makes it possible to achieve high power with efficiency and small footprint with High performance by operating this rotating machine. However, the application of the Adaptive Neuro-Fuzzy Inference System Observer-Based Approach, makes it possible to increase the life of the examined turbine and keep better reliability for their monitoring system and satisfy the techno-economic and environmental performance impacts. For the purpose of controlling failures and the occurrence of turbine system malfunctions, and avoiding their consequences on the safety and productivity of the installation.",industry
10.1016/j.enconman.2021.113856,Journal,Energy Conversion and Management,scopus,2021-04-01,sciencedirect,The mutual benefits of renewables and carbon capture: Achieved by an artificial intelligent scheduling strategy,https://api.elsevier.com/content/abstract/scopus_id/85101129959,"Renewable power and carbon capture are key technologies to transfer the power industry into low carbon generation. Renewables have been developed fast, however, the intermittent nature has imposed higher requirement for the flexibility of the power grid. Retrofitting carbon capture technologies to existing fossil-fuel fired power plants is an important solution to avoid the “lock-in” of emissions, but the high operating costs hinders their large scale application. The coexistence of renewable power and carbon capture opens up a new avenue that the deployment of carbon capture can provide additional flexibility for better accommodation of renewable power while excess renewables can be used to reduce the operating costs of carbon capture. To this end, this paper proposes an artificial intelligence based optimal scheduling strategy for the power plant-carbon capture system in the context of renewable power penetration to show that the mutual benefits between carbon capture and renewable power can be achieved when the carbon capture process is made fully adjustable. An artificial intelligent deep belief neural network is used to reflect the complex interactions between carbon, heat and electricity within the power plant carbon capture system. Multiple operating goals are considered in the scheduling such as minimizing the operating costs, renewable power curtailment and carbon emission, and the particle swarm heuristic optimization is employed to find the optimal solution. The impacts of carbon capture constraint mode, carbon emission penalty coefficient, carbon dioxide production constraints and renewable power installed capacity are investigated to provide broader insight on the potential benefit of carbon capture in future low-carbon energy system. A case study using real world data of weather condition and load demand shows that renewable power curtailment can be reduced by 51% with the integration of post-combustion capture systems and 35% of total carbon emission are captured by the use of excess renewable power through optimal scheduling. This paper points out a new way of using artificial intelligent technologies to coordinate the couplings between carbon and electricity for efficient and environmentally friendly operation of future low-carbon energy system.",industry
10.1016/j.asoc.2020.107069,Journal,Applied Soft Computing,scopus,2021-04-01,sciencedirect,Deep learning feature exploration for Android malware detection,https://api.elsevier.com/content/abstract/scopus_id/85098947132,"Android mobile devices and applications are widely deployed and used in industry and smart city. Malware detection is one of the most powerful and effective approaches to guarantee security of Android systems, especially for industrial platform and smart city. Recently, researches using machine learning-based techniques for Android malware detection increased rapidly. Nevertheless, most of the appeared approaches have to perform feature analysis and selection, so-called feature engineering, which is time-consuming and relies on artificial experience. To solve the inefficiency problem of feature engineering, we propose TC-Droid, an automatic framework for Android malware detection based on text classification method. The core idea of TC-Droid is derived from the field of text classification. TC-Droid feeds on the text sequence of APPs analysis reports generated by AndroPyTool, applies a convolutional neural network (CNN) to explore significant information (or knowledge) under original report text, instead of manual feature engineering. In an evaluation with different number of real-world samples, TC-Droid outperforms state-of-the-art model (Drebin) and several classic models (NB, LR, KNN, RF) as well. With multiple experimental settings and corresponding comparisons, TC-Droid achieves effective and flexible performance in Android malware detection task.",industry
10.1016/j.neucom.2020.10.097,Journal,Neurocomputing,scopus,2021-03-21,sciencedirect,3D-RVP: A method for 3D object reconstruction from a single depth view using voxel and point,https://api.elsevier.com/content/abstract/scopus_id/85097471582,"Three-dimensional object reconstruction technology has a wide range of applications such as augment reality, virtual reality, industrial manufacturing and intelligent robotics. Although deep learning-based 3D object reconstruction technology has developed rapidly in recent years, there remain important problems to be solved. One of them is that the resolution of reconstructed 3D models is hard to improve because of the limitation of memory and computational efficiency when deployed on resource-limited devices. In this paper, we propose 3D-RVP to reconstruct a complete and accurate 3D geometry from a single depth view, where R, V and P represent Reconstruction, Voxel and Point, respectively. It is a novel two-stage method that combines a 3D encoder-decoder network with a point prediction network. In the first stage, we propose a 3D encoder-decoder network with residual learning to output coarse prediction results. In the second stage, we propose an iterative subdivision algorithm to predict the labels of adaptively selected points. The proposed method can output high-resolution 3D models by increasing a small number of parameters. Experiments are conducted on widely used benchmarks of a ShapeNet dataset in which four categories of models are selected to test the performance of neural networks. Experimental results show that our proposed method outperforms the state-of-the-arts, and achieves about 
                        
                           2.7
                           %
                        
                      improvement in terms of the intersection-over-union metric.",industry
10.1016/j.fbp.2020.12.009,Journal,Food and Bioproducts Processing,scopus,2021-03-01,sciencedirect,Study of Galactooligosaccharides production from dairy waste by FTIR and chemometrics as Process Analytical Technology,https://api.elsevier.com/content/abstract/scopus_id/85099356128,"Galactooligosaccharides (GOS) production from whey, a relevant by-product of dairy industry, answers to the Circular Economy principle of extending the life cycle of products. Indeed, it allows the reuse of dairy waste to produce prebiotics to be used in functional food preparations. For this purpose, the effective monitoring of GOS production should be performed in real time and by environmentally friendly techniques. Thus, FTIR spectroscopy, combined with different chemometric approaches, has been tested to assess a Process Analytical Technology to follow GOS production from cheese whey. Partial Least Square regression models were reliable for lactose, glucose and galactose determination (Root Mean Square Error of Prediction of 21.9, 11.1 and 12.4 mg mL−1, respectively). Furthermore, Multivariate Curve Resolution – Alternating Least Square models were proposed to describe trends of the reaction components along the process being an interesting alternative to chromatographic determinations. The real time implementation of the proposed approach will provide the dairy industry with a reliable and green Process Analytical Technology for dairy waste reallocation, avoiding sample pre-processing, large use of organic solvents and long times of analysis.",industry
10.1016/j.future.2020.10.031,Journal,Future Generation Computer Systems,scopus,2021-03-01,sciencedirect,DISCERNER: Dynamic selection of resource manager in hyper-scale cloud-computing data centres,https://api.elsevier.com/content/abstract/scopus_id/85096157784,"Data centres constitute the engine of the Internet, and run a major portion of large web and mobile applications, content delivery and sharing platforms, and Cloud-computing business models. The high performance of such infrastructures is therefore critical for their correct functioning. This work focuses on the improvement of data-centre performance by dynamically switching the main data-centre governance software system: the resource manager. Instead of focusing on the development of new resource-managing models as soon as new workloads and patterns appear, we propose DISCERNER, a decision-theory model that can learn from numerous data-centre execution logs to determine which existing resource-managing model may optimise the overall performance for a given time period. Such a decision-theory system employs a classic machine-learning classifier to make real-time decisions based on past execution logs and on the current data-centre operational situation. A set of extensive and industry-guided experiments has been simulated by a validated data-centre simulation tool. The results obtained show that the values of key performance indicators may be improved by at least 20% in realistic scenarios.",industry
10.1016/j.future.2020.10.018,Journal,Future Generation Computer Systems,scopus,2021-03-01,sciencedirect,Large-scale online multi-view graph neural network and applications,https://api.elsevier.com/content/abstract/scopus_id/85095762066,"Recently popularized Graph Neural Network (GNN) has been attaching great attention along with its successful industry applications. This paper focuses on two challenges traditional GNN frameworks face: (i) most of them are transductive and mainly concentrate on homogeneous networks considering single typed nodes and edges; (ii) they are difficult to handle the real-time changing network structures as well as scale to big graph data. To address these issues, a novel attention-based Heterogeneous Multi-view Graph Neural Network (aHMGNN) solution is introduced. aHMGNN models a more intricate heterogeneous multi-view network, where various node and edge types co-exist and each of these objects also contain specific attributes. It is end-to-end, and two stages are designed for node embeddings learning and multi-typed node and edge representations fusion, respectively. Experimental studies on large-scale spam detection and link prediction tasks clearly verify the efficiency and effectiveness of our proposed aHMGNN. Furthermore, we have implemented our approach in one of the largest e-commerce platforms which further verifies that aHMGNN is arguably promising and scalable in real-world applications.",industry
10.1016/j.knosys.2020.106679,Journal,Knowledge-Based Systems,scopus,2021-02-15,sciencedirect,Federated learning for machinery fault diagnosis with dynamic validation and self-supervision,https://api.elsevier.com/content/abstract/scopus_id/85098734354,"Intelligent data-driven machinery fault diagnosis methods have been successfully and popularly developed in the past years. While promising diagnostic performance has been achieved, the existing methods generally require large amounts of high-quality supervised data for training, which are mostly difficult and expensive to collect in real industries. Therefore, it is motivated that the distributed data of multiple clients can be integrated and exploited to build a powerful data-driven model. However, that basically requires data sharing among different users, and is not preferred in most industrial cases due to potential conflict of interests. In order to address the data island problem, a federated learning method for machinery fault diagnosis is proposed in this paper. Model training is locally implemented within each participated client, and a self-supervised learning scheme is proposed to enhance the learning performance. The server aggregates the locally updated models in each training round under the dynamic validation scheme, and a global fault diagnosis model can be established. Only the models are mutually communicated rather than the data, which ensures data privacy among different clients. The experiments on two datasets suggest the proposed method offers a promising approach on confidential decentralized learning.",industry
10.1016/j.abb.2020.108730,Journal,Archives of Biochemistry and Biophysics,scopus,2021-02-15,sciencedirect,Artificial intelligence in the early stages of drug discovery,https://api.elsevier.com/content/abstract/scopus_id/85098095696,"Although the use of computational methods within the pharmaceutical industry is well established, there is an urgent need for new approaches that can improve and optimize the pipeline of drug discovery and development. In spite of the fact that there is no unique solution for this need for innovation, there has recently been a strong interest in the use of Artificial Intelligence for this purpose. As a matter of fact, not only there have been major contributions from the scientific community in this respect, but there has also been a growing partnership between the pharmaceutical industry and Artificial Intelligence companies. Beyond these contributions and efforts there is an underlying question, which we intend to discuss in this review: can the intrinsic difficulties within the drug discovery process be overcome with the implementation of Artificial Intelligence? While this is an open question, in this work we will focus on the advantages that these algorithms provide over the traditional methods in the context of early drug discovery.",industry
10.1016/j.patter.2020.100195,Journal,Patterns,scopus,2021-02-12,sciencedirect,Topic classification of electric vehicle consumer experiences with transformer-based deep learning,https://api.elsevier.com/content/abstract/scopus_id/85100638713,"The transportation sector is a major contributor to greenhouse gas (GHG) emissions and is a driver of adverse health effects globally. Increasingly, government policies have promoted the adoption of electric vehicles (EVs) as a solution to mitigate GHG emissions. However, government analysts have failed to fully utilize consumer data in decisions related to charging infrastructure. This is because a large share of EV data is unstructured text, which presents challenges for data discovery. In this article, we deploy advances in transformer-based deep learning to discover topics of attention in a nationally representative sample of user reviews. We report classification accuracies greater than 91% (F1 scores of 0.83), outperforming previously leading algorithms in this domain. We describe applications of these deep learning models for public policy analysis and large-scale implementation. This capability can boost intelligence for the EV charging market, which is expected to grow to US$27.6 billion by 2027.",industry
10.1016/j.ijepes.2021.107505,Journal,International Journal of Electrical Power and Energy Systems,scopus,2021-02-01,sciencedirect,Global sensitivity analysis for a real-time electricity market forecast by a machine learning approach: A case study of Mexico,https://api.elsevier.com/content/abstract/scopus_id/85113278481,"The study presents the hybridization of global sensitivity analysis with data-driven techniques to evaluate the Mexican electricity market interaction and assess the impact of individual parameters concerning locational marginal prices. The study case pertains to Yucatan, Mexico's electricity grid and market characteristics. A comparison of three artificial intelligence techniques in the electricity market is presented to forecast electricity prices in real-time market conditions. The study contemplates exogenous input parameters classified as regional, operational, meteorological, and economic indicators. A sensitivity analysis was carried out to the model with the best performance of the Artificial Intelligence techniques. The results showed that the impact of the variables fluctuates according to market and consumption conditions. In this study, the most relevant variables were electricity generation (17.06%), fossil fuel costs (natural gas 12.54% and diesel 8.63%), load zone (11.17%), and the day of the year (8.51%). From the qualitative point of view, the complex behavior of the parameters was analyzed; moreover, the quantitative results weighted the relevance of the variables in the Locational Marginal Prices. The meteorological and economic parameters allow assessing the environment where it interacts and serves as an instrument for decision-making in the planning of the energy sector. The presented methodology can be implemented as an alternative tool for market participants to analyze electricity prices.",industry
10.1016/j.jmapro.2020.12.050,Journal,Journal of Manufacturing Processes,scopus,2021-02-01,sciencedirect,Online tool condition monitoring for ultrasonic metal welding via sensor fusion and machine learning,https://api.elsevier.com/content/abstract/scopus_id/85099501543,"In ultrasonic metal welding (UMW), tool wear significantly affects the weld quality and tool maintenance constitutes a substantial part of production cost. Thus, tool condition monitoring (TCM) is crucial for UMW. Despite extensive literature focusing on TCM for other manufacturing processes, limited studies are available on TCM for UMW. Existing TCM methods for UMW require offline high-resolution measurement of tool surface profiles, which leads to undesirable production downtime and delayed decision-making. This paper proposes a completely online TCM system for UMW using sensor fusion and machine learning (ML) techniques. A data acquisition (DAQ) system is designed and implemented to obtain in-situ sensing signals during welding processes. A large feature pool is then extracted from the sensing signals. A subset of features are selected and subsequently used by ML-based classification models. A variety of classification models are trained, validated, and tested using experimental data. The best-performing classification models can achieve close to 100% classification accuracy for both training and test datasets. The proposed TCM system not only provides real-time TCM for UMW but also can support optimal decision-making in tool maintenance. The TCM system can be extended to predict remaining useful life (RUL) of tools and integrated with a controller to adjust welding parameters accordingly.",industry
10.1016/j.micpro.2020.103628,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Consumer decision-making and smart logistics planning based on FPGA and convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85097718891,"In the fourth Industrial Revolution, cost-effective planning and rational management were the key to the success of the revolution. This paper mainly studies the development and application of models in machine learning technology. The abnormal activities monitored in real time are rectified so that the customer's electronic orders can be displayed through the support of big data, thus laying the foundation for the development of intelligent logistics. Under the data system, an exception model is created and classified and regressed. In this model, the security and stability of customer orders in the network can be automatically detected, and the abnormal data can be analyzed and evaluated. Unusual circumstances of this kind need to be in an intelligent logistics environment, and delivery tasks must be called intuitive for special care. Early detection of abnormal order events is expected to improve the accuracy of delivery planning. To enable new technical solutions, the logistics industry and economic decision-makers often lack the IT background and expertise needed to start developing new systems and technical solutions. Evaluate the benefits of using. Implementation and integration complexity is seen as one of the three major obstacles to the success of the IoT above. This is by hindering long-term investment in new technologies from slowing down digitization.",industry
10.1016/j.micpro.2020.103594,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Enterprise financial risk management platform based on 5 G mobile communication and embedded system,https://api.elsevier.com/content/abstract/scopus_id/85097578772,"5 G technology has been applied to the financial sector. As a mortgage and supervision of sensors, network cameras, mobile device to improve the financial performance of real-time data generated by the data, bank loan credit risk management has been used. There are many risks of financial credit in modern society, the most important of which is the financial danger on mobile Internet. In the case of mobile phone payment popularity, financial risk has also greatly improved. This makes the traditional statistics and models can not fully meet the needs of the development of modern society. Bank credit risk has also improved to some extent. Therefore, there is a practical need for a more robust risk prediction model of artificial intelligence to predict the default behavior with good accuracy and competency-based big data analytics. This paper presents data mining method optimization and 5 G mobile communications and embedded systems commercial banks, based on financial risk management. It is safe to protect personal privacy, consider these requirements 5 G system design. To successfully connect to the ability to make money, telecom service providers need to ask their players to match their products and the industry. In addition to simple connections, they need a unified high-level function, such as coordination of network resources, analytical capabilities, and automated business and operations. Mob ileum provides Business Assurance Analytics to improve and develop a strong customer value proposition during 5 G technologies deployment. Experimental results show that the risk management models have fast convergence, powerful forecasting capabilities, and effectively perform screening default behavior. Simultaneously, distributed significant data clusters to achieve significantly reduce the processing time model training and testing.",industry
10.1016/j.apenergy.2020.116049,Journal,Applied Energy,scopus,2021-02-01,sciencedirect,Adaptive prognostics in a controlled energy conversion process based on long- and short-term predictors,https://api.elsevier.com/content/abstract/scopus_id/85097470918,"The pulp and paper industry is a fundamental sector of the economy of many countries. However, this sector requires real collaboration and initiatives from stakeholders to reduce its significant consumption of energy and emission of greenhouse gases. Heat exchangers are examples of equipment in pulp mills that are subjected to undesirable and complex phenomena such as evolution of fouling over time, which leads to inefficiency in terms of energy consumption and unplanned shutdowns, resulting in ineffective maintenance strategies and production costs. Therefore, there is a clear need to develop an accurate predictive maintenance tool that helps mill operators avoid such situations. It is necessary for that tool to effectively track the fouling evolution level and, based on it, deploy a reliable prognostics approach to estimate more accurately the time-to-clean of this equipment. This study presents a new hybrid prognostics approach for fouling prediction in heat exchangers. The proposed approach relies on the fusion of information of different prediction horizons to estimate the time-to-clean. Employing long short-term memory, it allows adaptation of long-term predictions by accurate short-term predictions using multiple non-linear auto-regressive exogenous models. This fusion not only captures the changes in degradation trend over time, but also ensures a good accuracy of prognostics results in both the short- and long-term horizons for planning maintenance actions. The effectiveness of the proposed approach was successfully proven on real industrial data collected from a pulp mill heat exchanger located in Canada.",industry
10.1016/j.apenergy.2020.116297,Journal,Applied Energy,scopus,2021-02-01,sciencedirect,An Echo State Network for fuel cell lifetime prediction under a dynamic micro-cogeneration load profile,https://api.elsevier.com/content/abstract/scopus_id/85097452614,"Improving Proton Exchange Membrane Fuel Cell durability is a key that paves the way to its large scale industrial deployment. During the last five years, the prognostics discipline emerged as an interesting field for Proton Exchange Membrane Fuel Cell state of health prediction and lifetime estimation. The information provided by the prognostic module is crucial for optimizing the control strategy to extend the fuel cell lifetime. In this paper, an approach based on Echo State Network for fuel cell prognostics under a variable load is developed. The novelty of this paper is to perform prognostics under a variable load profile without prior knowledge of this latter. Two solutions are developed in this work. The first one consists of evaluating the remaining useful lifetime under a repeated load cycle. The second one is based on using Markov chains to generate estimations of the future load profile, allowing thus to overcome the need of real future load profile prior knowledge. Both proposed solutions give accurate prediction results of proton exchange membrane fuel cell remaining useful lifetime, with low uncertainties.",industry
10.1016/j.micpro.2020.103579,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Development of cultural tourism platform based on FPGA and convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85097346419,"Data mining can be described as a typical analysis of large datasets to investigate early unknown types, styles, and interpersonal relationships to generate the right decision information. It improves their markets and today to maintain control over whether these companies are forced into the data mining tools and technologies they use to develop and manage tourism products and services in the market. It is falling out of the favorable situation of the travel and tourism industry. Objective work is to provide and display its application in data mining and tourism. Advances in mobile technology provide an opportunity to obtain real-time information of travelers, such as time and space behavior, at the destination they visit. This study analyzed a large-scale mobile phone data set to capture the mobile phone traces of international tourists who visited South Korea. We adopt the trajectory data mining method to understand tourism activities’ spatial structure in three different destinations. The research reveals tourist destinations and multiple “hot spots” (or popular areas) that interact spatially in these places through spatial cluster analysis and sequential pattern mining. Therefore, this article provides the planning of spatial model destinations to integrate important tourism influences, which is based on tourism design. The proposed system is modelled in Field Programmable Gate Array (FPGA) using Xilinx software.",industry
10.1016/j.micpro.2020.103318,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Enterprise financial cost management platform based on FPGA and neural network,https://api.elsevier.com/content/abstract/scopus_id/85094858518,"At present, the domestic costs of most construction companies are relatively scattered with the cost data of various business agents. Unless it is controlled by an experienced manager, decision-makers cannot have the real-time dynamic cost of a project. In the information age, it is of vital importance to use the information to control the cost of construction projects dynamically. Cost management and the establishment of an information platform are ways to control the platform integrated cost data, operators, computer software and hardware, and corresponding method information, and its core is cost data information. The place for financial cost analysis and decision making is a conceptually rich field where information is a commercial product which is complicated, extensive, and invaluable. In this model, first,a set of extracts from the macro-credit feature space is designed and then, FPGA and neural network (FPGA, NN) models for credit evaluation is built based on these indicators, eventually it is applied scientifically, and reasonably, practically. Several state credit metrics are randomly selected. Our model shows applications that are both practical and competent. Using this model, authorities can analyze local credit conditions, allowing investors to make wise decisions to invest while saving on operating and credit costs. Most importantly, this model can help impulsive local government leaders, businesses, and even everyone to enhance competitiveness and capacities of attractive regions, thereby foster a good atmosphere for a credit culture.",industry
10.1016/j.micpro.2020.103301,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,IoT enabled cancer prediction system to enhance the authentication and security using cloud computing,https://api.elsevier.com/content/abstract/scopus_id/85094168107,"In recent days, Internet of Things, Cloud Computing, Deep learning, Machine learning and Artificial Intelligence are considered to be an emerging technologies to solve variety of real world problems. These techniques are importantly applied in various fields such as healthcare systems, transportation systems, agriculture and smart cities to produce fruitful results for number of issues in today's environment. This research work focuses on one such application in the field of IoT together with cloud computing. More number of sensors that are deployed in human body is used to collect patient related data such as deviation in body temperature and others which leads to variation in blood cells that turned to be cancerous cells. Main intention of this work is design a cancer prediction system using Internet of Things upon extracting the details of blood results to test whether it is normal or abnormal. In addition to this, encryption is done on the blood results of cancer affected patient and store it in cloud for quick reference through Internet for the doctor or healthcare nurse to handle the patient data secretly. This research work concentrates on enhancing the health care computations and processing. It provides a framework to enhance the performance of the existing health care industry across the globe. As the entire medical data has to be saved in cloud, the traditional medical treatment limitations can be overcome. Encryption and decryption is done using AES algorithm in order to provide authentication and security in handling cancer patients. The main focus is to handle healthcare data effectively for the patient when they are away from the home town since the needed cancer treatment details are stored in cloud. The task completion time is greatly reduce from 400 to 160  by using VMs. CloudSim gives an adaptable simulation structure that empowers displaying and reproduced results.",industry
10.1016/j.isatra.2020.08.024,Journal,ISA Transactions,scopus,2021-02-01,sciencedirect,Data-driven adaptive modeling method for industrial processes and its application in flotation reagent control,https://api.elsevier.com/content/abstract/scopus_id/85089898141,"In real industrial processes, new process “excitation” patterns that largely deviate from previously collected training data will appear due to disturbances caused by process inputs. To reduce model mismatch, it is important for a data-driven process model to adapt to new process “excitation” patterns. Although efforts have been devoted to developing adaptive process models to deal with this problem, few studies have attempted to develop an adaptive process model that can incrementally learn new process “excitation” patterns without performance degradation on old patterns. In this study, efforts are devoted to enabling data-driven process models with incremental learning ability. First, a novel incremental learning method is proposed for process model updating. Second, an adaptive neural network process model is developed based on the novel incremental learning method. Third, a nonlinear model predictive control based on the adaptive process model is implemented and applied for flotation reagent control. Experiments based on historical data provide evidence that the newly developed adaptive process model can accommodate new process “excitation” patterns and preserve its performance on old patterns. Furthermore, industry experiments carried out in a real-world lead–zinc froth flotation plant provide industrial evidence and show that the newly designed controller is promising for practical flotation reagent control.",industry
10.1016/j.rcim.2020.102029,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2021-02-01,sciencedirect,Towards manufacturing robotics accuracy degradation assessment: A vision-based data-driven implementation,https://api.elsevier.com/content/abstract/scopus_id/85088120602,"In this manuscript we report on a vision-based data-driven methodology for industrial robot health assessment. We provide an experimental evidence of the usefulness of our methodology on a system comprised of a 6-axis industrial robot, two monocular cameras and five binary squared fiducial markers. The fiducial marker system permits to accurately track the deviation of the end-effector along a fixed non-trivial trajectory. Moreover, we monitor the trajectory deflection using three gradually increasing weights attached to the end-effector. When the robot is loaded with the maximum allowed payload, a deviation of 0.77mm is identified in the Z-coordinate of the end-effector. Tracing trajectory information, we train five supervised learning regression models. Such models are afterwards used to predict the deviation of the end-effector, using the pose estimation provided by the visual tracking system. As a result of this study, we show that this procedure is a stable, robust, rigorous and reliable tool for robot trajectory deviation estimation and it even allows to identify the mechanical element producing non-kinematic errors.",industry
10.1016/j.apenergy.2021.118127,Journal,Applied Energy,scopus,2021-01-01,sciencedirect,Data-driven control of room temperature and bidirectional EV charging using deep reinforcement learning: Simulations and experiments,https://api.elsevier.com/content/abstract/scopus_id/85118721393,"The control of modern buildings is a complex multi-loop problem due to the integration of renewable energy generation, storage devices, and electric vehicles (EVs). Additionally, it is a complex multi-criteria problem due to the need to optimize overall energy use while satisfying users’ comfort. Both conventional rule-based (RB) controllers, which are difficult to apply in multi-loop settings, and advanced model-based controllers, which require an accurate building model, cannot fulfil the requirements of the building automation industry to solve this problem optimally at low development and commissioning costs. This work presents a fully data-driven pipeline to obtain an optimal control policy from historical building and weather data, thus avoiding the need for complex physics-based modelling. We demonstrate the potential of this method by jointly controlling a room temperature and an EV to minimize the cost of electricity while retaining the comfort of the occupants. We model the room temperature with a recurrent neural network and use it as a simulation environment to learn a deep reinforcement learning (DRL) control policy. It achieves on average 17% energy savings and 19% better comfort satisfaction than a standard RB room temperature controller. When a bidirectional EV is connected additionally and a two-tariff electricity pricing is applied, it successfully leverages the battery and decreases the overall cost of electricity. Finally, we deployed it on a real building, where it achieved up to 30% energy savings while maintaining similar comfort levels compared to a conventional RB room temperature controller.",industry
10.1016/j.promfg.2021.06.086,Conference Proceeding,Procedia Manufacturing,scopus,2021-01-01,sciencedirect,Pervasive environmental sensing for Industry 4.0 as an educational tool,https://api.elsevier.com/content/abstract/scopus_id/85117930435,"The reduced cost of implementing pervasive industrial sensing networks enables universities to incorporate these tools in engineering curricula. They provide engineering students from increasingly computerized backgrounds, such as mechanical and automotive engineering, the opportunity to work alongside students from technical schools who bring different skill sets than what students may be used to, synthesize historical data, and drive the sensing system’s physical system design and implementation. This paper outlines this convergent curriculum’s initial implementation stage, including the wireless environmental sensing Internet of Things (IoT) network, focusing on laboratory environmental sensing. Students placing many sensors around the lab and on equipment generates a wealth of real-time and historical data for use in the classroom and provides them a tangible example of learning to measure the world around them. This setup parallels the current varied Industry 4.0 state of the manufacturing industry, where Big Data exists but is underutilized, and where additional sensors and intelligent machine data streams are added each year. Students in each class are given a defined portion of a broader roadmap to a fully instrumented and intelligent laboratory environment. In the first step, student-programmed environmental sensors were placed around the lab and provide temperature, humidity, pressure, and gas mixture measures every five minutes. Classroom use of the aggregated data includes visualizing the laboratory and essential equipment’s current status using a Microsoft PowerBI dashboard and historical data visualization and analysis through trend forecasting and outlier detection in Python JupyterLab notebooks. The IoT system’s installation also provided an infrastructure for further study of future student-designed IoT projects.",industry
10.1016/j.orp.2021.100204,Journal,Operations Research Perspectives,scopus,2021-01-01,sciencedirect,A review of approximate dynamic programming applications within military operations research,https://api.elsevier.com/content/abstract/scopus_id/85117385700,"Sequences of decisions that occur under uncertainty arise in a variety of settings, including transportation, communication networks, finance, defence, etc. The classic approach to find an optimal decision policy for a sequential decision problem is dynamic programming; however its usefulness is limited due to the curse of dimensionality and the curse of modelling, and thus many real-world applications require an alternative approach. Within operations research, over the last 25 years the use of Approximate Dynamic Programming (ADP), known as reinforcement learning in many disciplines, to solve these types of problems has increased in popularity. These efforts have resulted in the successful deployment of ADP-generated decision policies for driver scheduling in the trucking industry, locomotive planning and management, and managing high-value spare parts in manufacturing. In this article we present the first review of applications of ADP within a defence context, specifically focusing on those which provide decision support to military or civilian leadership. This article’s main contributions are twofold. First, we review 18 decision support applications, spanning the spectrum of force development, generation, and employment, that use an ADP-based strategy and for each highlight how its ADP algorithm was designed, evaluated, and the results achieved. Second, based on the trends and gaps identified we discuss five topics relevant to applying ADP to decision support problems within defence: the classes of problems studied; best practices to evaluate ADP-generated policies; advantages of designing policies that are incremental versus complete overhauls when compared to currently practiced policies; the robustness of policies as scenarios change, such as a shift from high to low intensity conflict; and sequential decision problems not yet studied within defence that may benefit from ADP.",industry
10.1016/j.procs.2021.08.095,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,SODA: A real-time simulation framework for object detection and analysis in smart manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85116946450,"For modern manufacturing firms, automation has already become a norm but constantly needs to be improved as firms still face strong demand to increase their productivity. This can be achieved by reducing dependability on manpower, reaching lean and even unmanned production and this is where some of the standards of Industry 4.0 come in useful, not to mention: Machine Vision, Image Recognition or Machine Learning. In our paper, we present SODA – our approach to build a flexible ML and AI enabled framework for object detection, analysis, and simulation. The framework is designed to support a development process of solutions requiring real-time analysis of images of different types of moving objects on a conveyor belt. In our work we discuss architectural challenges of the developed framework as well as the basic components of the system. We do also provide information on how to use the framework and present a sample implementation of an actual system employing some of the machine learning methods.",industry
10.1016/j.procs.2021.09.013,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Exploiting supervised machine learning for driver detection in a real-world environment,https://api.elsevier.com/content/abstract/scopus_id/85116888149,"The proliferation of info-entertainment systems in today’s vehicles has provided a really cheap and easy-to-deploy platform with the ability to gather information about the vehicle under analysis. Ultra-response connectivity networks with a latency below 10 milliseconds are providing the perfect infrastructure in which this information can be sent to improve safety and security. With the purpose of providing an architecture to increase safety and security in an automotive context, we in this paper propose a method for detecting the driver in real-time exploiting supervised machine learning techniques. The experimental analysis performed on real-world data shows that the proposed method obtains encouraging results.",industry
10.1016/j.procs.2021.09.233,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,A note on the applications of artificial intelligence in the hospitality industry: Preliminary results of a survey,https://api.elsevier.com/content/abstract/scopus_id/85116885410,"Intelligent technologies are widely implemented in different areas of modern society but specific approaches should be applied in services. Basic relationships refer to supporting customers and people responsible for services offering for these customers. The aim of the paper is to analyze and evaluate the state-of-the art of artificial intelligence (AI) applications in the hospitality industry. Our findings show that the major deployments concern in-person customer services, chatbots and messaging tools, business intelligence tools powered by machine learning, and virtual reality & augmented reality. Moreover, we performed a survey (n = 178), asking respondents about their perceptions and attitudes toward AI, including its implementation within a hotel space. The paper attempts to discuss how the hotel industry can be motivated by potential customers to apply selected AI solutions. In our opinion, these results provide useful insights for understanding the phenomenon under investigation. Nevertheless, since the results are not conclusive, more research is still needed on this topic. Future studies may concern both qualitative and quantitative methods, devoted to developing models that: a) quantify the potential benefits and risks of AI implementations, b) determine and evaluate the factors affecting the AI adoption by the customers, and c) measure the user (guest) experience of the hotel services, fueled by AI-based technologies.",industry
10.1016/j.matpr.2021.03.109,Conference Proceeding,Materials Today: Proceedings,scopus,2021-01-01,sciencedirect,Real-time applications and novel manufacturing strategies of incremental forming: An industrial perspective,https://api.elsevier.com/content/abstract/scopus_id/85114180763,"Incremental Sheet-Metal Forming (ISMF) is a flexible and evolving metal forming technology for rapid free-form prototyping and small-batch metal components manufacturing. The end product has evolved by means of localized deformation in addition bi-axial stretching during that deforming tool squeezed on blank with predefined process variables. Owing to a unique process advantages and low manufacturing cost, its market requirement continuous enlargement and the process gradually transforms from prototyping to real-time manufacturing perspective. Over the preceding decades, ISMF technology has been adequately established in research and development, although it is less explored in the real-time industrial environment. The main intention of this exploration is to bring-forth insight into potential applications such as aviation, automotive, bio-medical, research and concept development through implementation of ISMF. Further, component evaluation performed to establish a convenient and feasible solution from deep-drawing and hydro-forming. For customized part forming, conventional forming process seems to be insufficient. Due to industrial transformation, dependent on cost-effectiveness, even prototyping and low-volume manufactured components relying on superior quality. Although, understanding the effect and influence of process variable, which needs the data analysis with implementing the optimization models and Artificial neural-network (ANN) model. These types of analysis majorly focus on monitoring and predict target values at each cycle and also reconfigure to optimistic or organize the iterative method for describing the appropriate process guidelines. Further, recent advances in ISMF process variants are explored, while looking at the benefits of ISMF for real-time part production. ISMF continues to mature into technology for production applications, while exploring the potential field to transform the way sheet components are fabricated in the new-era of digital manufacturing. This study will, in turn, enhance the capabilities of ISMF technology, which has grown significantly over the preceding decades, allowing technology adopters to innovate new design principle and achieve greater production flexibility.",industry
10.1016/j.dss.2021.113653,Journal,Decision Support Systems,scopus,2021-01-01,sciencedirect,AI-based industrial full-service offerings: A model for payment structure selection considering predictive power,https://api.elsevier.com/content/abstract/scopus_id/85114151068,"Artificial Intelligence and servitization reshape the way that manufacturing companies derive value. Aiming to sustain competitive advantage and intensify customer loyalty, full-service providers offer the use of their products as a service to achieve continuous revenues. For this purpose, companies implement AI classification algorithms to enable high levels of service at controllable costs. However, traditional asset sellers who become service providers require previously atypical payment structures, as classic payment methods involving a one-time fee for production costs and profit margins are unsuitable. In addition, a low predictive power of the implemented classification algorithm can lead to misclassifications, which diminish the achievable level of service and the intended net present value of the resultant service. While previous works focus solely on the costs of such misclassifications, our decision model highlights implications for payment structures, service levels, and – ultimately – the net present value of such data-driven service offerings. Our research suggests that predictive power can be a major factor in selecting a suitable payment structure and the overall design of service level agreements. Therefore, we compare common payment structures for data-driven services and investigate their relationship to predictive power. We develop our model using a design science methodology and iteratively evaluate our results using a four-step approach that includes interviews with industry experts and the application of our model to a real-world use case. In summary, our research extends the existing knowledge of servitization and data-driven services in the manufacturing industry through a quantitative decision model.",industry
10.1016/B978-0-323-88506-5.50132-7,Book Series,Computer Aided Chemical Engineering,scopus,2021-01-01,sciencedirect,Implementation of first-principles surface interactions in a hybrid machine learning assisted modelling of flocculation,https://api.elsevier.com/content/abstract/scopus_id/85110537894,"Machine learning algorithms are drawing attention for modelling processes in the chemical and biochemical industries. Due to a lack of fundamental understanding of complex processes and a lack of reliable real-time measurement methods in bio-based manufacturing, machine learning approaches have become more important. Hybrid modelling approaches that combine detailed process understanding with machine learning can provide an opportunity to integrate prior process knowledge with various measurement data for efficient modelling of the (bio) chemical processes. In this study, the application of a hybrid modelling framework that combines various first-principles models with machine learning algorithms is demonstrated through a laboratory-scale case of flocculation of silica particles in water. Since flocculation is a process that occurs across length- and time scales, an integrated hybrid multi-scale modelling framework can improve the phenomenological understanding of the process. The first-principles models utilized in this study are molecular scale particle surface interaction models such as combined with a larger-scale population balance model.",industry
10.1016/B978-0-323-88506-5.50161-3,Book Series,Computer Aided Chemical Engineering,scopus,2021-01-01,sciencedirect,Artificial Intelligence Based Prediction of Exergetic Efficiency of a Blast Furnace,https://api.elsevier.com/content/abstract/scopus_id/85110444162,"The iron melting furnaces are the most energy-consuming equipment of the iron and steel industry. The energy efficiency of the furnace is affected by process conditions such as the inlet temperature, velocity of the charge, and its composition. Hence, optimum values of these process conditions are vital in the efficient operation of the furnace. Computational methods have been very helpful in the optimum design and operation of process equipment. In this study, a first principle (FP) model was developed for an iron-making furnace to visualize its internal dynamics. To minimize the large computational time required for the FP-based analysis, a data-based model, i.e., Artificial Neural Networks (ANN), is developed using data extracted from the FP model. The ANN model was developed using data sets comprised of the values of temperature of the charge and gasses, velocity, concentration of the oxygen, pressure, airflow directions, energy and exergy profiles, and overall exergy efficiency of the furnace along with its height. The ANN model was highly accurate in prediction and is suitable for real-time implementation in a steel manufacturing plant.",industry
10.1016/B978-0-323-88506-5.50144-3,Book Series,Computer Aided Chemical Engineering,scopus,2021-01-01,sciencedirect,Machine learning-based approach to identify the optimal design and operation condition of organic solvent nanofiltration (OSN),https://api.elsevier.com/content/abstract/scopus_id/85110354404,"Organic solvent nanofiltration (OSN) is one of the most anticipated separation technologies that provides wide-ranged industrial applications such as solvent recovery, solute concentration, and diluent separation. Despite of technical merits of the OSN technology, the numerous characteristics and perplexing nonlinearity on the OSN system have been a critical obstacle for understanding the governing principles, thereby prohibiting practical deployments. Recently, machine learning (ML) based approaches have been widely used for the modelling, discovery and optimization of complex design problems in chemical engineering area such as catalysis, electrochemistry and physicochemical systems. Therefore, this study aims to develop a new ML-based approach for modelling and optimizing the design scheme and operating condition of the OSN system. By collecting commercial OSN data through literatures reviews, the major descriptors for the prediction of the OSN membrane, such as MWCO, solute mole weight, solute concentration, solvent parameter, temperature, pressure, flux, were defined. We then screened noises and outliers of the collected data to ensure a high and consistent density and uniqueness. Support vector machine (SVM) was implemented as a prediction models to simulate the OSN performance and identify the optimal conditions as well as the process scheme. As a result, the optimal operation strategies (i.e., pressure, temperature and solvent and solvent types) were analyzed to meet the targeted specification of the OSN system (mass flux and rejection rate). The proposed ML-based approach can promote a real-world OSN application by reducing a number of time-consuming and expensive experiments for establishing OSN design and operation strategy.",industry
10.1016/B978-0-323-88506-5.50194-7,Book Series,Computer Aided Chemical Engineering,scopus,2021-01-01,sciencedirect,Attack Detection Using Unsupervised Learning Algorithms in Cyber-Physical Systems,https://api.elsevier.com/content/abstract/scopus_id/85110277992,"Cyber-Physical Systems (CPS) are collections of physical and computer components that are integrated with each other to operate a process safely and efficiently. Examples of CPS include industrial control systems, water systems, robotics systems, smart grid, etc. However, the security aspect of CPS is still a concern that makes them vulnerable to cyber attacks on the control elements, network or physical systems. The work reported here is an attempt towards detecting cyber attacks and improving process monitoring in CPS; using unsupervised machine learning anomaly detection algorithms such as one-class SVM, isolation forest, elliptic envelope. These algorithms are evaluated using the dataset of a real Water Distribution Plant (WADI) built at the iTrust centre at Singapore University of Technology and Design for cyber security research. For modelling purposes, process 1 and 2 of the aforementioned plant were taken into consideration because the implemented attacks were closely related to only these sub-processes. The result of the experiment shows that one-class SVM is found to be the most effective algorithm in determining anomalies for this particular dataset.",industry
10.1016/j.isatra.2021.06.010,Journal,ISA Transactions,scopus,2021-01-01,sciencedirect,A real-world application of Markov chain Monte Carlo method for Bayesian trajectory control of a robotic manipulator,https://api.elsevier.com/content/abstract/scopus_id/85108508566,"Reinforcement learning methods are being applied to control problems in robotics domain. These algorithms are well suited for dealing with the continuous large scale state spaces in robotics field. Even though policy search methods related to stochastic gradient optimization algorithms have become a successful candidate for coping with challenging robotics and control problems in recent years, they may become unstable when abrupt variations occur in gradient computations. Moreover, they may end up with a locally optimal solution. To avoid these disadvantages, a Markov chain Monte Carlo (MCMC) algorithm for policy learning under the RL configuration is proposed. The policy space is explored in a non-contiguous manner such that higher reward regions have a higher probability of being visited. The proposed algorithm is applied in a risk-sensitive setting where the reward structure is multiplicative. Our method has the advantages of being model-free and gradient-free, as well as being suitable for real-world implementation. The merits of the proposed algorithm are shown with experimental evaluations on a 2-Degree of Freedom robot arm. The experiments demonstrate that it can perform a thorough policy space search while maintaining adequate control performance and can learn a complex trajectory control task within a small finite number of iteration steps.",industry
10.1016/j.cirp.2021.04.046,Journal,CIRP Annals,scopus,2021-01-01,sciencedirect,Semi-Double-loop machine learning based CPS approach for predictive maintenance in manufacturing system based on machine status indications,https://api.elsevier.com/content/abstract/scopus_id/85108064671,"The paper presents two original and innovative contributions: 1) the model of machine learning (ML) based approach for predictive maintenance in manufacturing system based on machine status indications only, and 2) semi-Double-loop machine learning based intelligent Cyber-Physical System (I-CPS) architecture as a higher-level environment for ML based predictive maintenance execution. Considering only the machine status information provides rapid and very low investment-based implementation of an advanced predictive maintenance paradigm, especially important for SMEs. The model is validated in real-life situations, exploring different learning algorithms and strategies for learning maintenance predictive models. The findings show very high level of prediction accuracy.",industry
10.1016/j.procir.2021.05.031,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,Artificial intelligence enhanced interaction in digital twin shop-floor,https://api.elsevier.com/content/abstract/scopus_id/85107885361,"As an enabling technology for smart manufacturing, digital twin has been widely applied in manufacturing shop-floor. A great deal of research focuses on the key issues in implementing digital twin shop-floor (DTS), including scheduling, production planning, fault diagnosis and prognostics. However, DTS puts forward higher requirements in terms of real-time interaction. Artificial intelligence (AI), as an effective approach to improve the intelligence of the physical shop-floor, provides a new method to meet the above requirements. In this paper, a framework of AI-enhanced DTS in interaction is proposed. AI-enhanced DTS improves the real-time interaction through predictive control. The implementation mechanism of AI-enhanced interaction in DTS is also presented in detail. Enabling technologies for interaction in DTS are introduced at last.",industry
10.1016/j.procs.2021.03.074,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Requirements towards optimizing analytics in industrial processes,https://api.elsevier.com/content/abstract/scopus_id/85106735396,"Modern production systems are composed of complex manufacturing processes with highly technology specific cause-effect relationships. Developments in sensor technology and computational science allow for data-driven decision making that facilitate effcient and objective production management. However, process data may only be beneficial if it is enriched with meta information and process expertise, reduced to relevant information and modelling results interpreted correctly. The importance of data integration in the heterogeneous industrial environment rises at the same momentum as new metrology techniques are deployed. In this paper, we focus on optimizing analytics, containing data-driven decision making for predictive quality and maintenance. We summarize key requirements for data analytics and machine learning application in industrial processes. With a use case from automotive component manufacturing we characterize industrial production, categorize process data and put requirements in context to a real-world example.",industry
10.1016/j.procs.2021.02.026,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,DDNN based data allocation method for IIoT,https://api.elsevier.com/content/abstract/scopus_id/85104871428,"With the complete application of artificial intelligence in the field of industrial production and manufacturing and the rapid development of edge computing, industrial processing sites often need to deploy machine learning tasks at edges and terminals. We propose a data allocation method based on Distributed Deep Neural Networks (DDNN) framework, which allocates data to edge servers or stays locally for processing. DDNN divides deep learning tasks and deploys pre-trained shallow neural networks and deep neural networks at local or edges, respectively. However, all data is processed locally, and the failure is sent to the edge server or the cloud. It will lead to excessive pressure on local terminal equipment and long-term idle edge servers, which cannot meet industrial production’s real-time requirements on user privacy and time-sensitive tasks. In this paper, the complexity and inference error rate of machine learning model, the data processing speed of local equipment and edge server, and the transmission time are comprehensively considered to establish the system model. A joint optimization problem is proposed to minimize the total data processing delay. The optimal solution is derived analytically, and the optimal data allocation methhod is given. Simulation experiments are designed to verify the method’s effectiveness and study the influence of key parameters on the allocation method.",industry
10.1016/j.procs.2021.03.075,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Input doubling method based on SVR with RBF kernel in clinical practice: Focus on small data,https://api.elsevier.com/content/abstract/scopus_id/85104314419,"In recent years, machine-learning-based approaches have become of considerable interest to the efficient processing of short or limited data samples. Its so-called small data approach. This is due to the significant growth of new intellectual analysis tasks in various industries, which are characterized by limited historical data. These include Materials Science, Economics, Medicine, and so on. An effective processing of short datasets is especially acute in medicine. Insufficient number of vectors, significant gaps in the data collected during the supervision of patient’s treatment or rehabilitation, reduces the effectiveness or prevents effective intellectual analysis based on them. This paper presents a new approach to processing short medical data samples. The basis of the developed method is SVR with RBF kernel. The algorithmic implementation of the method in both operation modes is described. Experimental modeling on a real short data set (Trabecular bone data) is conducted. It contained only 35 observations. A comparison of the method with a number of existing machine learning methods is conducted. It is experimental established the highest accuracy of the method among those considered. The developed method has potential opportunities for wide application in various fields of medicine.",industry
10.1016/j.procir.2021.01.128,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,A Machine Vision-based Cyber-Physical Production System for Energy Efficiency and Enhanced Teaching-Learning Using a Learning Factory,https://api.elsevier.com/content/abstract/scopus_id/85102656008,"Machine vision (MV) can help in achieving real-time data analysis in a manufacturing environment. This can be implemented in any industry to achieve real-time monitoring of workpieces for geometric defects and material irregularities. Identification of defects, sorting of workpieces based on their physical parameters, and analysis of process abnormalities can be achieved by using the real-time data from simple and cost-effective raspberry pi with camera and open source machine learning platform TensorFlow to run convolutional neural network (CNN) model. The proposed cyber-physical production system enables to develop a MV based system for data acquisition integrating physical entities of learning factory (LF) with the cyber world. Nowadays, LFs are widely used to train the workforce for developing competencies for emerging technologies and challenges faced due to technological advancements in Industry 4.0. This paper demonstrates the application of a cost-effective MV system in a learning factory environment to achieve real-time data acquisition and energy efficiency. The proposed low-cost machine vision is found to detect geometric irregularities, colours and surface defects. The simple cost effective MV system has enhanced the energy efficiency and reduced the total carbon footprint by 18.37 % and 78.83 % depending upon the location of MV system along the flow. The teaching-learning experience is also enhanced through action-based learning strategies. This not only ensures less rework, better control, unbiased decisions, 100% quality assurance but also the need of workers/operators can be reduced.",industry
10.1016/j.procir.2021.01.115,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,Development of a Decision Support System for 3D Printing Processes based on Cyber Physical Production Systems,https://api.elsevier.com/content/abstract/scopus_id/85102637852,"3D printing, an additive manufacturing (AM) technology, potentially provides sustainability advantages such as less waste generation, lightweight geometries, reduced material and energy consumption, lower inventory waste, etc. This paper proposes a decision support system for the 3D printing process based on Cyber Physical Production System (CPPS). The user is enabled to dynamically assess the carbon footprint based on the energy and material usage for their 3D printed object. A CPPS framework for the environmental sustainability of the 3D printing process is presented, which supports the derivation of improved strategies for product design and production. A physical world for 3D printing is used with the internet of things (IoT) devices like sensor node, webcam, smart plugs, and raspberry pi to host printer Management Software (PMS) for real-time monitoring and control of material and energy consumption during the printing process. Experiments have been conducted based on Taguchi L9 orthogonal array with polylactic Acid (PLA) as a filament material to estimate the product-related manufacturing energy consumption with the carbon footprint. The proposed framework can be effectively used by the users to supports the decision-making process for saving resources and energy; and minimizing the effect on the environment.",industry
10.1016/j.procir.2021.01.010,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,Analysis of Barriers to Industry 4.0 adoption in Manufacturing Organizations: An ISM Approach,https://api.elsevier.com/content/abstract/scopus_id/85102622489,"Industry 4.0 has enabled technological integration of cyber physical systems and internet based communication in manufacturing value creation processes. As of now, many people use it as a collective term for advanced technologies, i.e. advanced robotics, artificial intelligence, machine learning, big data analytics, cloud computing, smart sensors, internet of things, augmented reality, etc. This substantially improves flexibility, quality, productivity, cost, and customer satisfaction by transforming existing centralized manufacturing systems towards digital and decentralized one. Despite having potential benefits of industry 4.0, the organizations are facing typical obstacles and challenges in adopting new technologies and successful implementation in their business models. This paper aims to identify potential barriers which may hinder the implementation of industry 4.0 in manufacturing organizations. The identified barriers, through comprehensive literature review and on the basis of opinions collected from industry experts, are: poor value-chain integration, cyber-security challenges, uncertainty about economic benefits, lack of adequate skills in workforce, high investment requirements, lack of infrastructure, jobs disruptions, challenges in data management and data quality, lack of secure standards and norms, and resistance to change. Interpretive Structural Modeling (ISM) is used to establish relationships among these barriers to develop a hierarchical model and MICMAC analysis for further classification of identified barriers for better understanding. An analysis of driving and dependence of the barriers may help in clear understanding of these for successful implementation of Industry 4.0 practices in the organizations.",industry
10.1016/j.procs.2021.01.348,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Procedure model for the development and launch of intelligent assistance systems,https://api.elsevier.com/content/abstract/scopus_id/85101779152,"The paper analyses the current state of knowledge on approaches for the practical implementation of machine learning based assistance systems for production planning and control.
                  A concept of a procedure model for application-oriented projects in the field of industrial series production is proposed. It focusses on order sequencing and machine allocation in a real time production environment. As part of an application-oriented research project, a use case is referenced. In this paper, a first conceptual approach is presented, using the example of an industrial production of printed circuit boards.
                  In the following steps, practical suitability is checked on the basis of the practical reference, conclusions are drawn and the methodology will be developed further. The aim is a generally valid procedure model for industrial series production.",industry
10.1016/j.jtice.2021.01.007,Journal,Journal of the Taiwan Institute of Chemical Engineers,scopus,2021-01-01,sciencedirect,On the evaluation of solubility of hydrogen sulfide in ionic liquids using advanced committee machine intelligent systems,https://api.elsevier.com/content/abstract/scopus_id/85099564514,"Ionic Liquids (ILs) are increasingly emerging as new innovating green solvents with great importance from academic, industrial, and environmental perspectives. This surge of interest in considering ILs in various applications is owed to their attractive properties. Involvements in the gas sweetening and the reduction of the amounts of sour and acid gasses are among the most promising applications of ILs. In this study, new advanced committee machine intelligent systems (CMIS) were introduced for predicting the solubility of hydrogen sulfide (H2S) in various ILs. The implemented CMIS models were gained by linking robust data-driven techniques, namely multilayer perceptron (MLP) and cascaded forward neural network (CFNN) beneath rigorous schemes using group method of data handling (GMDH) and genetic programming (GP). The proposed paradigms were developed using an extensive database encompassing 1243 measurements of H2S solubility in 33 ILs. The performed comprehensive error investigation revealed that the newly implemented paradigms yielded very satisfactory prediction performance. Besides, it was found that CMIS-GP provided more accurate estimations of H2S solubility in ILs compared with both the other intelligent models and the best-prior paradigms. In this regard, the developed CMIS-GP exhibited overall average absolute relative deviation (AARD) and coefficient of determination (R2) values of 2.3767% and 0.9990, respectively. Lastly, the trend analyses demonstrated that the tendencies of CMIS-GP predictions were in excellent accordance with the real variations of H2S solubility in ILs with respect to pressure and temperature.",industry
10.1016/j.aei.2021.101246,Journal,Advanced Engineering Informatics,scopus,2021-01-01,sciencedirect,"A systematic literature review on intelligent automation: Aligning concepts from theory, practice, and future perspectives",https://api.elsevier.com/content/abstract/scopus_id/85099458674,"With the recent developments in robotic process automation (RPA) and artificial intelligence (AI), academics and industrial practitioners are now pursuing robust and adaptive decision making (DM) in real-life engineering applications and automated business workflows and processes to accommodate context awareness, adaptation to environment and customisation. The emerging research via RPA, AI and soft computing offers sophisticated decision analysis methods, data-driven DM and scenario analysis with regard to the consideration of decision choices and provides benefits in numerous engineering applications. The emerging intelligent automation (IA) – the combination of RPA, AI and soft computing – can further transcend traditional DM to achieve unprecedented levels of operational efficiency, decision quality and system reliability. RPA allows an intelligent agent to eliminate operational errors and mimic manual routine decisions, including rule-based, well-structured and repetitive decisions involving enormous data, in a digital system, while AI has the cognitive capabilities to emulate the actions of human behaviour and process unstructured data via machine learning, natural language processing and image processing. Insights from IA drive new opportunities in providing automated DM processes, fault diagnosis, knowledge elicitation and solutions under complex decision environments with the presence of context-aware data, uncertainty and customer preferences. This sophisticated review attempts to deliver the relevant research directions and applications from the selected literature to the readers and address the key contributions of the selected literature, IA’s benefits, implementation considerations, challenges and potential IA applications to foster the relevant research development in the domain.",industry
10.1016/j.matdes.2020.109201,Journal,Materials and Design,scopus,2021-01-01,sciencedirect,Online prediction of mechanical properties of hot rolled steel plate using machine learning,https://api.elsevier.com/content/abstract/scopus_id/85092064894,"In industrial steel plate production, process parameters and steel grade composition significantly influence the microstructure and mechanical properties of the steel produced. But determining the exact relationship between process parameters and mechanical properties is a challenging process. This work aimed to devise a deep learning model, to predict mechanical properties of industrial steel plate including yield strength (YS), ultimate tensile strength (UTS), elongation (EL), and impact energy (Akv); based on the process parameters as well as composition of raw steel, and apply it online to a real steel manufacturing plant. An optimal deep neural network (DNN) model was formulated with 27 inputs parameters, 2 hidden layers each having 200 nodes and 4 output parameters (27 × 200 × 200 × 4) with an initial learning rate 0.0001, using Adam optimizer and subjected to Z pre-processing method, to yield an accurate model with R2 = 0.907. The tuned DNN model, had a root mean square error of 21.06 MPa, 16.67 MPa, 2.36%, and 39.33 J, and root mean square percentage error of 4.7%, 2.9%, 7.7%, and 16.2%, for YS, UTS, EL and Akv respectively. Through comparative analysis, it was found that the accuracy of DNN model was higher than other classic machine learning algorithms. To interpret the model assumptions and findings, several local linear models were devised and analyzed to establish the link between process parameters and mechanical properties. Finally the tuned DNN model was deployed in the real-steel plant for online monitoring and control of steel mechanical properties, and to guide the production of targeted steel plates with tailored mechanical properties.",industry
10.1016/bs.adcom.2020.08.013,Book Series,Advances in Computers,scopus,2021-01-01,sciencedirect,Empowering digital twins with blockchain,https://api.elsevier.com/content/abstract/scopus_id/85090745248,"A digital twin is an exact digital/logical/cyber/virtual representation/replica of any tangible physical system or process. And the digital twin runs on a competent IT infrastructure (say, cloud centers). In essence, a digital twin is typically a software program that takes various real-world data about a ground-level physical system as prospective inputs and produces useful outputs in the form of insights. The outputs generally are the value-adding and decision-enabling predictions or simulations of how that physical system will act on those inputs. These help in quickly and easily realizing highly optimized and organized products with less cost and risk.
                  The manufacturing industry had embraced the digital twin technology long time back to be modern in their operations, outputs, and offerings. The distinct contributions of the digital twin paradigm, since then, have gone up significantly with the seamless synchronization with a number of pioneering technologies such as the Internet of Things (IoT), artificial intelligence (AI), big and streaming data analytics, data lakes, software-defined cloud environments, blockchain, etc. With the concept of cyber physical systems (CPS) is being adopted and adapted widely and wisely, complicated yet sophisticated electronics devices at the ground level are being blessed with their corresponding digital twins. The digital twins enable data scientists and system designers to optimize a number of things including process excellence, knowledge discovery and dissemination in time, better system design, robust verification and validation, etc. In the recent past, with the flourishing of the blockchain technology, the scope for digital twins has gone up remarkably. This unique combination is bound to produce additional competencies and fresh use cases for enterprises. This chapter is to explain how they integrate and initiate newer opportunities to be grabbed and gained for a better tomorrow.",industry
10.1016/j.jclepro.2020.124022,Journal,Journal of Cleaner Production,scopus,2021-01-01,sciencedirect,Artificial intelligence in nuclear industry: Chimera or solution?,https://api.elsevier.com/content/abstract/scopus_id/85090601822,"Nuclear industry is in crisis and innovation is the central theme of its survival in future. Artificial intelligence has made a quantum leap in last few years. This paper comprehensively analyses recent advancement in artificial intelligence for its applications in nuclear power industry. A brief background of machine learning techniques researched and proposed in this domain is outlined. A critical assessment of various nuances of artificial intelligence for nuclear industry is provided. Lack of operational data from real power plant especially for transients and accident scenario is a major concern regarding the accuracy of intelligent systems. There is no universally agreed opinion among researchers for selecting the best artificial intelligence techniques for a specific purpose as intelligent systems developed by various researchers are based on different data set. Interlaboratory work frame or round-robin programme to develop the artificial intelligent tool for any specific purpose, based on the same data base, can be crucial in claiming the accuracy and thus the best technique. The black box nature of artificial techniques also poses a serious challenge for its implementation in nuclear industry, as it makes them prone to fooling.",industry
10.1016/j.jobe.2020.101601,Journal,Journal of Building Engineering,scopus,2021-01-01,sciencedirect,Trainingless multi-objective evolutionary computing-based nonintrusive load monitoring: Part of smart-home energy management for demand-side management,https://api.elsevier.com/content/abstract/scopus_id/85087827958,"Electricity is the most widely used form of energy in modern society. One method of satisfying the continuously increasing industrial, commercial, and residential electrical-energy demands of consumers in smart grids is to use an Internet-of-things (IoT) service-oriented electrical-energy management system (EMS) to intrusively monitor and manage electrical loads, which can effectively react to demand-response schemes for demand-side management (DSM). Nonintrusive load monitoring (NILM), a viable cost-effective load disaggregation technique, has recently gained considerable attention as a nonintrusive alternative to EMS in the research field of smart grids. This paper presents a smart IoT-oriented home EMS founded on trainingless multi-objective evolutionary computing-based NILM for DSM in a smart grid. Evolutionary computing-based NILM is considered and addressed as a multi-objective combinatorial optimization problem. The proposed NILM technique can determine the electrical appliances based on their individual electrical characteristics extracted from composite electrical-load consumption with no intrusive deployment of smart plugs or power meters. A fully nonintrusive NILM alternative is considered and proposed. In addition, this alternative is different from conventional NILM because conventional NILM considers artificial intelligence including artificial neural networks (NNs) and deep NN as load classifiers of NILM where training and retraining stages and a hyperparameter tuning procedure are required. The proposed smart IoT-oriented home EMS was experimentally investigated with the trainingless multi-objective evolutionary computing-based NILM in a real house environment. The experimental results confirm that the proposed methodology is feasible.",industry
10.1016/j.jmsy.2020.06.012,Journal,Journal of Manufacturing Systems,scopus,2021-01-01,sciencedirect,"A digital twin to train deep reinforcement learning agent for smart manufacturing plants: Environment, interfaces and intelligence",https://api.elsevier.com/content/abstract/scopus_id/85087690907,"Filling the gaps between virtual and physical systems will open new doors in Smart Manufacturing. This work proposes a data-driven approach to utilize digital transformation methods to automate smart manufacturing systems. This is fundamentally enabled by using a digital twin to represent manufacturing cells, simulate system behaviors, predict process faults, and adaptively control manipulated variables. First, the manufacturing cell is accommodated to environments such as computer-aided applications, industrial Product Lifecycle Management solutions, and control platforms for automation systems. Second, a network of interfaces between the environments is designed and implemented to enable communication between the digital world and physical manufacturing plant, so that near-synchronous controls can be achieved. Third, capabilities of some members in the family of Deep Reinforcement Learning (DRL) are discussed with manufacturing features within the context of Smart Manufacturing. Trained results for Deep Q Learning algorithms are finally presented in this work as a case study to incorporate DRL-based artificial intelligence to the industrial control process. As a result, developed control methodology, named Digital Engine, is expected to acquire process knowledges, schedule manufacturing tasks, identify optimal actions, and demonstrate control robustness. The authors show that integrating a smart agent into the industrial platforms further expands the usage of the system-level digital twin, where intelligent control algorithms are trained and verified upfront before deployed to the physical world for implementation. Moreover, DRL approach to automated manufacturing control problems under facile optimization environments will be a novel combination between data science and manufacturing industries.",industry
10.1016/j.jclepro.2020.123365,Journal,Journal of Cleaner Production,scopus,2020-12-20,sciencedirect,An active preventive maintenance approach of complex equipment based on a novel product-service system operation mode,https://api.elsevier.com/content/abstract/scopus_id/85089891280,"The product-service system (PSS) business model has received increasing attention in equipment maintenance studies, as it has the potential to provide high value-added services for equipment users and construct ethical principles for equipment providers to support the implementation of circular economy. However, the PSS providers in equipment industry are facing many challenges when implementing Industry 4.0 technologies. One important challenge is how to fully collect and analyse the operational data of different equipment and diverse users in widely varied conditions to make the PSS providers create innovative equipment management services for their customers. To address this challenge, an active preventive maintenance approach for complex equipment is proposed. Firstly, a novel PSS operation mode was developed, where complex equipment is offered as a part of PSS and under exclusive control by the providers. Then, a solution of equipment preventive maintenance based on the operation mode was designed. A deep neural network was trained to predict the remaining effective life of the key components and thereby, it can pre-emptively assess the health status of equipment. Finally, a real-world industrial case of a leading CNC machine provider was developed to illustrate the feasibility and effectiveness of the proposed approach. Higher accuracy for predicting the remaining effective life was achieved, which resulted in predictive identification of the fault features, proactive implementation of the preventive maintenance, and reduction of the PSS providers’ maintenance costs and resource consumption. Consequently, the result shows that it can help PSS providers move towards more ethical and sustainable directions.",industry
10.1016/j.oceaneng.2020.108261,Journal,Ocean Engineering,scopus,2020-12-15,sciencedirect,Real-time data-driven missing data imputation for short-term sensor data of marine systems. A comparative study,https://api.elsevier.com/content/abstract/scopus_id/85093700362,"In the maritime industry, sensors are utilised to implement condition-based maintenance (CBM) to assist decision-making processes for energy efficient operations of marine machinery. However, the employment of sensors presents several challenges including the imputation of missing values. Data imputation is a crucial pre-processing step, the aim of which is the estimation of identified missing values to avoid under-utilisation of data that can lead to biased results. Although various studies have been developed on this topic, none of the studies so far have considered the option of imputing incomplete values in real-time to assist instant data-driven decision-making strategies. Hence, a methodological comparative study has been developed that examines a total of 20 widely implemented machine learning and time series forecasting algorithms. Moreover, a case study on a total of 7 machinery system parameters obtained from sensors installed on a cargo vessel is utilised to highlight the implementation of the proposed methodology. To assess the models’ performance seven metrics are estimated (Execution time, MSE, MSLE, RMSE, MAPE, MedAE, Max Error). In all cases, ARIMA outperforms the remaining models, yielding a MedAE of 0.08 r/min and a Max Error of 2.4 r/min regarding the main engine rotational speed parameter.",industry
10.1016/j.eswa.2020.113653,Journal,Expert Systems with Applications,scopus,2020-12-15,sciencedirect,Cost-sensitive learning classification strategy for predicting product failures,https://api.elsevier.com/content/abstract/scopus_id/85088008188,"In the current era of Industry 4.0, sensor data used in connection with machine learning algorithms can help manufacturing industries to reduce costs and to predict failures in advance. This paper addresses a binary classification problem found in manufacturing engineering, which focuses on how to ensure product quality delivery and at the same time to reduce production costs. The aim behind this problem is to predict the number of faulty products, which in this case is extremely low. As a result of this characteristic, the problem is reduced to an imbalanced binary classification problem. The authors contribute to imbalanced classification research in three important ways. First, the industrial application coming from the electronic manufacturing industry is presented in detail, along with its data and modelling challenges. Second, a modified cost-sensitive classification strategy based on a combination of Voronoi diagrams and genetic algorithm is applied to tackle this problem and is compared to several base classifiers. The results obtained are promising for this specific application. Third, in order to evaluate the flexibility of the strategy, and to demonstrate its wide range of applicability, 25 real-world data sets are selected from the KEEL repository with different imbalance ratios and number of features. The strategy, in this case implemented without a predefined cost, is compared with the same base classifiers as those used for the industrial problem.",industry
10.1016/j.ijpx.2020.100058,Journal,International Journal of Pharmaceutics: X,scopus,2020-12-01,sciencedirect,Deep convolutional neural networks: Outperforming established algorithms in the evaluation of industrial optical coherence tomography (OCT) images of pharmaceutical coatings,https://api.elsevier.com/content/abstract/scopus_id/85096171040,"This paper presents a novel evaluation approach for optical coherence tomography (OCT) image analysis of pharmaceutical solid dosage forms based on deep convolutional neural networks (CNNs). As a proof of concept, CNNs were applied to image data from both, in- and at-line OCT implementations, monitoring film-coated tablets as well as single- and multi-layered pellets. CNN results were compared against results from established algorithms based on ellipse-fitting, as well as to human-annotated ground truth data. Performance benchmarks used include, efficiency (computation speed), sensitivity (number of detections from a defined test set) and accuracy (deviation from the reference method). The results were validated by comparing the output of several algorithms to data manually annotated by human experts and microscopy images of cross-sectional cuts of the same dosage forms as a reference method. In order to guarantee comparability for all results, the algorithms were executed on the same hardware. Since modern OCT systems must operate under real-time conditions in order to be implemented in-line into manufacturing lines, the necessary steps are discussed on how to achieve this goal without sacrificing the algorithmic performance and how to tailor a deep CNN to cope with the high amount of image noise and alterations in object appearance. The developed deep learning approach outperforms static algorithms currently available in pharma applications with respect to performance benchmarks, and represents the next level in real time evaluation of challenging industrial OCT image data.",industry
10.1016/j.compind.2020.103329,Journal,Computers in Industry,scopus,2020-12-01,sciencedirect,A Middleware Platform for Intelligent Automation: An Industrial Prototype Implementation,https://api.elsevier.com/content/abstract/scopus_id/85092922057,"The development of dynamic data-based Decision Support Systems (DSSs) along with the increasing availability of data in the industry, makes real-time data acquisition and management a challenge. Intelligent automation appears as a holistic combination of automation with analytics and decisions made by artificial intelligence, delivering smart manufacturing and mass customization while improving resource efficiency. However, challenges towards the development of intelligent automation architectures include the lack of interoperability between systems, complex data preparation steps, and the inability to deal with both high-frequency and high-volume data in a timely fashion. This paper contributes to industrial frameworks focused on the development of standardized system architectures for Industry 4.0, closing the gap between generic architectures and physical realizations. It proposes a platform for intelligent automation relying on a gateway or middleware between field devices, enterprise databases, and DSSs in real-time scenarios. This is achieved by providing the middleware interoperability, determinism, and automatic data structuring over an industrial communication infrastructure such as the OPC UA Standard over Time Sensitive Networks (TSN). Cloud services and database warehousing used to address some of the challenges are handled using fog computing and a multi-workload database. This paper presents an implementation of the platform in the pharmaceutical industry, providing interoperability and real-time reaction capability to changes to an industrial prototype using dynamic scheduling algorithms.",industry
10.1016/j.autcon.2020.103354,Journal,Automation in Construction,scopus,2020-12-01,sciencedirect,Real-time online detection of trucks loading via genetic neural network,https://api.elsevier.com/content/abstract/scopus_id/85091689126,"This article focuses on real-time online detection of trucks loading via genetic neural network. Firstly, according to the state structure of the truck and the deployment of the sensor in the monitoring system, a mathematical model that magnetic sensors detecting the weight of the truck is established, it provides a theoretical basis for the calculation of the compensator deviation. Secondly, a feedback compensator for disturbance signals is designed by genetic neural network in the load monitoring system. Thirdly, the stability of the control system is analyzed by the Lyapunov stability theory. Fourthly, a real-time monitoring system is proposed for the loading of trucks. Finally, a complete experiment is processed to in-depth discussion and analysis. Field experiments showed that this scheme solves the problem of real-time load detection of trucks, it proposes a monitoring system for transportation in the construction industry.",industry
10.1016/j.autcon.2020.103387,Journal,Automation in Construction,scopus,2020-12-01,sciencedirect,Virtual prototyping- and transfer learning-enabled module detection for modular integrated construction,https://api.elsevier.com/content/abstract/scopus_id/85090569290,"Modular integrated construction is one of the most advanced off-site construction technologies and involves the repetitive process of installing prefabricated prefinished volumetric modules. Automatic detection of location and movement of modules should facilitate progress monitoring and safety management. However, automatic module detection has not been implemented previously. Hence, virtual prototyping and transfer-learning techniques were combined in this study to develop a module-detection model based on mask regions with convolutional neural network (Mask R-CNN). The developed model was trained with datasets comprising both virtual and real images, and it was applied to two modular construction projects for automatic progress monitoring. The results indicate the effectiveness of the developed model in module detection. The proposed method using virtual prototyping and transfer learning not only facilitates the development of automation in modular construction, but also provides a new approach for deep learning in the construction industry.",industry
10.1016/j.ssci.2020.104967,Journal,Safety Science,scopus,2020-12-01,sciencedirect,Risk assessment by failure mode and effects analysis (FMEA) using an interval number based logistic regression model,https://api.elsevier.com/content/abstract/scopus_id/85090003051,"In order to reduce risks of failure, industries use a methodology called Failure Mode and Effects Analysis (FMEA) in terms of the Risk Priority Number (RPN). The RPN number is a product of ordinal scale variables, severity (S), occurrence (O) and detection (D) and product of such ordinal variables is debatable. The three risk attributes (S, O, and D) are generally given equal weightage, but this assumption may not be suitable for real-world applications. Apart from severity, occurrence, and detection, the presence of other risk attributes may also influence the risk of failure and hence should be considered for achieving a holistic approach towards mitigating failure modes. This paper proposes a systematic approach for developing a standard equation for RPN measure, using the methodology of interval number based logistic regression. Instead of utilizing RPN in product form for each failure, this method is benefited from decisions based on probability of risk of failure, 
                        
                           '
                           P
                           '
                        
                      which is more realistic in practical applications. A case study is presented to illustrate the application of the proposed methodology in finding the risk of failure of high capacity submersible pumps in the power plant. The developed logistic regression model (logit model) using R software helped in generating the probability of risk of failure equation for predicting the failures. The model showed the correct classification rate to be 77.47%. The Receiver Operating Characteristic (ROC) curve showed the logit-model to be 81.98% accurate with an optimal cut-off value of 0.56.",industry
10.1016/j.epsr.2020.106742,Journal,Electric Power Systems Research,scopus,2020-12-01,sciencedirect,Learning model of generator from terminal data,https://api.elsevier.com/content/abstract/scopus_id/85089545570,"Assuming that a generator is monitored by the system operator via a PMU device positioned at the generator’s terminal bus, we pose and resolve the question of the real-time, data-driven and automatic monitoring of the generator’s performance. We establish regimes of optimal performance for four complementary techniques ranging from the computationally light (a) Vector Auto-Regressive Model, suitable for normal, linear or almost linear regime, via (b) Long-Short-Term-Memory and (c) Neural ODE Deep Learning models, appropriate to monitor mildly nonlinear regimes, and finally to the (d) physics-informed model. For example, the physics-informed model is capable of fast identification of nonlinear transients and providing interpretable results, suitable, in particular, for corrective actions. The conclusions are reached in the result of validating the models on synthetic data generated in a realistic setting from an open-source, state-of-the-art modeling software. Advanced analysis is followed by a summary and conclusion suitable for the next step - validation of the hierarchy of the suggested data-driven schemes in the industry setting.",industry
10.1016/j.jclepro.2020.123125,Journal,Journal of Cleaner Production,scopus,2020-12-01,sciencedirect,A systematic literature review on machine tool energy consumption,https://api.elsevier.com/content/abstract/scopus_id/85088635681,"Energy efficiency has become an integral part of the metal manufacturing industries as a means to improve economic and environmental performance, and increase competitiveness. Machine tools are not only the major energy consumer in the manufacturing industry but also have very low efficiency. Therefore, the analysis of energy consumption by the machine tools is primarily important to understand their complex and dynamic energy consumption behavior. This will lead to the development of better corrective measures. Literature review helps in identifying and assessing the existing knowledge to recognize the future research areas for fostering the research interest on the specific topic. In this review article, the reference literature is identified using a systematic methodology followed by descriptive and content analysis to understand the evolution of research in machining energy. The review focuses on four machining energy aspects – classification, modelling, improvement strategies, and efficiency evaluation. A six level hierarchical model is proposed for better understanding of machining energy classification. The literature review shows that the research in this field intensified after 2009. It is observed that the research focus has shifted towards micro level classification of machining energy including transient states. More detailed and accurate energy consumption models are developed in recent years with increased use of soft computational methods. Real time energy data monitoring and its use for online optimization of machining processes is witnessed. The use of micro analysis, energy benchmarking and standardization of energy assessment indices require more research. Deployment of machining energy models for improving the sustainability of machine tools; data analytics and AI applications; and integration with industry 4.0 are new research opportunities in the field.",industry
10.1016/j.joes.2020.03.003,Journal,Journal of Ocean Engineering and Science,scopus,2020-12-01,sciencedirect,Developing a predictive maintenance model for vessel machinery,https://api.elsevier.com/content/abstract/scopus_id/85086839612,"The aim of maintenance is to reduce the number of failures in equipment and to avoid breakdowns that may lead to disruptions during operations. The objective of this study is to initiate the development of a predictive maintenance solution in the shipping industry based on a computational artificial intelligence model using real-time monitoring data. The data analysed originates from the historical values from sensors measuring the vessel´s engines and compressors health and the software used to analyse these data was R. The results demonstrated key parameters held a stronger influence in the overall state of the components and proved in most cases strong correlations amongst sensor data from the same equipment. The results also showed a great potential to serve as inputs for developing a predictive model, yet further elements including failure modes identification, detection of potential failures and asset criticality are some of the issues required to define prior designing the algorithms and a solution based on artificial intelligence. A systematic approach using big data and machine learning as techniques to create predictive maintenance strategies is already creating disruption within the shipping industry, and maritime organizations need to consider how to implement these new technologies into their business operations and to improve the speed and accuracy in their maintenance decision making.",industry
10.1016/j.ejor.2020.05.010,Journal,European Journal of Operational Research,scopus,2020-12-01,sciencedirect,Data-driven optimization model customization,https://api.elsevier.com/content/abstract/scopus_id/85086372620,"When embedded in software-based decision support systems, optimization models can greatly improve organizational planning. In many industries, there are classical models that capture the fundamentals of general planning decisions (e.g., designing a delivery route). However, these models are generic and often require customization to truly reflect the realities of specific operational settings. Yet, such customization can be an expensive and time-consuming process. At the same time, popular cloud computing software platforms such as Software as a Service (SaaS) are not amenable to customized software applications. We present a framework that has the potential to autonomously customize optimization models by learning mathematical representations of customer-specific business rules from historical data derived from model solutions and implemented plans. Because of the wide-spread use in practice of mixed integer linear programs (MILP) and the power of MILP solvers, the framework is designed for MILP models. It uses a common mathematical representation for different optimization models and business rules, which it encodes in a standard data structure. As a result, a software provider employing this framework can develop and maintain a single code-base while meeting the needs of different customers. We assess the effectiveness of this framework on multiple classical MILPs used in the planning of logistics and supply chain operations and with different business rules that must be observed by implementable plans. Computational experiments based on synthetic data indicate that solutions to the customized optimization models produced by the framework are regularly of high-quality.",industry
10.1016/j.patter.2020.100107,Journal,Patterns,scopus,2020-11-13,sciencedirect,Wiz: A Web-Based Tool for Interactive Visualization of Big Data,https://api.elsevier.com/content/abstract/scopus_id/85097417500,"In an age of information, visualizing and discerning meaning from data is as important as its collection. Interactive data visualization addresses both fronts by allowing researchers to explore data beyond what static images can offer. Here, we present Wiz, a web-based application for handling and visualizing large amounts of data. Wiz does not require programming or downloadable software for its use and allows scientists and non-scientists to unravel the complexity of data by splitting their relationships through 5D visual analytics, performing multivariate data analysis, such as principal component and linear discriminant analyses, all in vivid, publication-ready figures. With the explosion of high-throughput practices for materials discovery, information streaming capabilities, and the emphasis on industrial digitalization and artificial intelligence, we expect Wiz to serve as an invaluable tool to have a broad impact in our world of big data.",industry
10.1016/j.jclepro.2020.122870,Journal,Journal of Cleaner Production,scopus,2020-11-10,sciencedirect,Enhancing the adaptability: Lean and green strategy towards the Industry Revolution 4.0,https://api.elsevier.com/content/abstract/scopus_id/85088397153,"Industry 4.0 has brought forth many advantages and challenges for the industry players. Many organizations are strategizing to take advantage of this industrial paradigm shift, thus improving the sustainability of the enterprise. However, there are many factors such as talent development, machinery advancement and infrastructure development which involve huge investment that need to be considered. This paper presents an enhanced adaptive model for the implementation of the lean and green (L&G) strategy in processing sectors to solve dynamic industry problems associated with Industry 4.0. A feature of this enhanced adaptive model is that it combines experts’ experience and operational data as input in dealing with real industry application. A lean and green index is coupled in the model to serve as a benchmark and process improvement tracking indicator. This allows the industrialists to set a lean and green index (LGI) target for effective process improvement. From this integrated model, an ensemble of backpropagation optimizers is then used to identify the best-optimized strategy. This ensemble optimizer is formulated to perform operation improvement and update the targeted LGI automatically when a higher index is achieved for continuous improvement. A case study on a combined heat and power plant is performed and reflects an improvement of 18.25% on the LGI. This work serves as a practical transition strategy for the industrialist desiring to improve the sustainability of the facility with Industry 4.0 elements at minimum investment cost.",industry
10.1016/j.cie.2020.106868,Journal,Computers and Industrial Engineering,scopus,2020-11-01,sciencedirect,Simulation in industry 4.0: A state-of-the-art review,https://api.elsevier.com/content/abstract/scopus_id/85091194972,"Simulation is a key technology for developing planning and exploratory models to optimize decision making as well as the design and operations of complex and smart production systems. It could also aid companies to evaluate the risks, costs, implementation barriers, impact on operational performance, and roadmap toward Industry 4.0. Although several advances have been made in this domain, studies that systematically characterize and analyze the development of simulation-based research in Industry 4.0 are scarce. Therefore, this study aims to investigate the state-of-the-art research performed on the intersecting area of simulation and the field of Industry 4.0. Initially, a conceptual framework describing Industry 4.0 in terms of enabling technologies and design principles for modeling and simulation of Industry 4.0 scenarios is proposed. Thereafter, literature on simulation technologies and Industry 4.0 design principles is systematically reviewed using the preferred reporting items for systematic reviews and meta-analyses (PRISMA) methodology. This study reveals an increasing trend in the number of publications on simulation in Industry 4.0 within the last four years. In total, 10 simulation-based approaches and 17 Industry 4.0 design principles were identified. A cross-analysis of concepts and evaluation of models’ development suggest that simulation can capture the design principles of Industry 4.0 and support the investigation of the Industry 4.0 phenomenon from different perspectives. Finally, the results of this study indicate hybrid simulation and digital twin as the primary simulation-based approaches in the context of Industry 4.0.",industry
10.1016/j.infsof.2020.106368,Journal,Information and Software Technology,scopus,2020-11-01,sciencedirect,Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions,https://api.elsevier.com/content/abstract/scopus_id/85087690796,"Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems.
                  
                     Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges.
                  
                     Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment.
                  
                     Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions.
                  
                     Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research.",industry
10.1016/j.petrol.2020.107509,Journal,Journal of Petroleum Science and Engineering,scopus,2020-11-01,sciencedirect,"Design and construction of the knowledge base system for geological outfield cavities classifications: An example of the fracture-cavity reservoir outfield in Tarim basin, NW China",https://api.elsevier.com/content/abstract/scopus_id/85087076723,"Tahe oilfield, located in NW Tarim Basin, is one of the largest and most difficult fracture cavity reservoirs in the world. Different fracture cavities, different generated mechanisms, and different oil production capacities. In order to study the significant parameters that can characterize the categories of facture-cavity. This research adopted outfield manual measurement, 3D digital modeling technique to obtain characterization parameters. According to experienced geological survey, typical outcrops were selected, then scanned by UAV (Unmanned aerial vehicle). Consequently, 3D digital models, including real coordinates and parameter information, were established by Agisoft Photoscan. Through geological testing results, various combination characteristic patterns of relative categories were analyzed. By using digital measure tool, combined with manually measured data, the parameters were extracted from the 3D digital model (DM). Then an initial geological database was established. For furtherly analyzing the database, the mathematic statistics methods of multiple linear regression (MLR), neural network technique (NNT) and discriminative classification technique (DCT) were applied. Using software of SPSS statistics 17.0, more than 200 groups of geological data (various categories of fracture-cavity) were optimally processed. Consequently, the significant characteristic parameters were interpreted to determine diverse categories. The results showed that: (1) cavity width, height, fracture length and cavity aspect ratio were significant parameters to classify runoff cavity categories. (2) Fault-controlled cavities could be accurately classified by fracture length and fracture density. (3) The main cavity categories could be distinguished by cavity width, cavity height and fracture density. Performances of the approach have been examined with 10 percentages of the samples, and a good agreement performed in the simulated results, and anastomosis rate was more than 80%. The researched results have critical guiding significance to evaluate types of fracture-cavity, develop and explore of fracture-cavity reservoirs. The construction technique of knowledge base can be applied for diverse fracture-cavity reservoirs in the various formations in different areas in the world.",industry
10.1016/j.measurement.2020.108043,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2020-11-01,sciencedirect,Smart frost measurement for anti-disaster intelligent control in greenhouses via embedding IoT and hybrid AI methods,https://api.elsevier.com/content/abstract/scopus_id/85086577761,"A novel Agro-industrial IoT (AIIoT) technology and architecture for intelligent frost forecasting in greenhouses via hybrid Artificial Intelligence (AI), is reported. The Internet of Things (IoT) allows the objects interconnection on the physical world using sensors and actuators via the Internet. The smart system was designed and implemented through a climatological station equipped with Artificial Neural Networks (ANN) and a fuzzy associative memory (FAM) for ecological control of the anti-frost disaster irrigation. The ANN forecasts the inside temperature of the greenhouses and the fuzzy control predicts the cropland temperatures for the activation of five output levels of the water pump. The results were compared to a Fourier-statistical analysis of hourly data, showing that the ANN models provide a temperature prediction with effectiveness higher than 90%, as compared to monthly data model. Moreover, results of this process were validated through the determination of the coefficient of variance analysis method (
                        
                           
                              
                                 R
                              
                              2
                           
                        
                     ).",industry
10.1016/j.measurement.2020.108052,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2020-11-01,sciencedirect,Deep learning-based prognostic approach for lithium-ion batteries with adaptive time-series prediction and on-line validation,https://api.elsevier.com/content/abstract/scopus_id/85086367941,"Prognostics for lithium-ion batteries is very critical in many industrial applications, and accurate prediction of battery state of health (SOH) is of great importance for health management. This paper proposes a novel deep learning-based prognostic method for lithium-ion batteries with on-line validation. An effective variant of recurrent neural network, i.e. long short-term memory structure, is used with variable input dimension, that facilitates network training with additional labeled samples. Adaptive time-series predictions are carried out for prognostics. An on-line validation method is further proposed for parameter optimization in real time based on the available system information, which allows for continuous model improvement. Experiments on a popular lithium-ion battery dataset are implemented to validate the effectiveness and superiority of the proposed method. The experimental results show the prognostic performances are promising both for the multi-steps-ahead predictions and long-horizon SOH estimations.",industry
10.1016/j.micpro.2020.103227,Journal,Microprocessors and Microsystems,scopus,2020-10-01,sciencedirect,Fog Computing-inspired Smart Home Framework for Predictive Veterinary Healthcare,https://api.elsevier.com/content/abstract/scopus_id/85089574391,"Domestic Pet Care has been an important domain in the healthcare industry. In the presented study, a comprehensive framework of the Smart VetCare system for the health monitoring of domestic pets has been presented. The work is focused on the remote surveillance of domestic animals’ health conditions inside the home environment using IoMT Technology. Specifically, pet health is analyzed for vulnerability in the ambient home environment and ubiquitous activities over a fog computing platform of FogBus. Furthermore, a temporal data granule is formulated and the Probability of Health Vulnerability (PoHV) is defined for determining the health severity of the animal. Additionally, the Temporal Sensitivity Measure (TSM) is defined for real-time pet healthcare analysis, which is visualized using the Self Organized Mapping (SOM) Technique. For validation purposes, the framework is deployed in the smart home environment using 12 IoMT WiSense Nodes and Health Sensor belt for monitoring a domestic dog of American Bully breed over the dynamic resource management platform of FogBus and iFogSim simulator. Based on the comparison with numerous state-of-the-art techniques, the proposed framework can register a better precision value (94.78%), accuracy value (95.38%), sensitivity value (93.71%), and f-measure value (94.41%).",industry
10.1016/j.nucengdes.2020.110817,Journal,Nuclear Engineering and Design,scopus,2020-10-01,sciencedirect,Machine learning enabled advanced manufacturing in nuclear engineering applications,https://api.elsevier.com/content/abstract/scopus_id/85089553568,"Advanced manufacturing has gained tremendous interest in both research and industry in the past few years. Over nearly the same period of time, machine learning (ML) has made phenomenal advancements, finding its way into many aspects of manufacturing. For the nuclear engineering field, the adoption of advanced manufacturing is a compelling argument due to the ambitious challenges the field faces. The combination of advanced manufacturing with ML holds great potential in the nuclear engineering field, and even further development is needed to accelerate their deployment towards real-world applications. This review paper seeks to detail several key aspects of ML enabled advanced manufacturing that are used or could prove useful to nuclear applications ranging from radiation detector materials to reactor parts fabrication. The applications covered here include new material extrapolation, manufacturing defect detection, and additive manufacturing parameters’ optimization.",industry
10.1016/j.aei.2020.101136,Journal,Advanced Engineering Informatics,scopus,2020-10-01,sciencedirect,Ensemble deep learning based semi-supervised soft sensor modeling method and its application on quality prediction for coal preparation process,https://api.elsevier.com/content/abstract/scopus_id/85087393963,"Coal preparation is the most effective and economical technique to reduce impurities and improve the product quality for run-of-mine coal. The timely and accurate prediction for key quality characteristics of separated coal plays a significant role in condition monitoring and production control. However, these quality characteristics are usually difficult to directly measure online in industrial practices Although some computation intelligence based soft sensor modeling methods have been developed and reported in existing research for these quality variables estimation, some problems still exist, i.e., manual feature extraction, considerable unlabeled data, temporal dynamic behavior in data, which will influence the accuracy and efficiency for established soft sensor model. To address above-mentioned problem and develop an more excellent quality prediction model for coal preparation process, a novel deep learning based semi-supervised soft sensor modeling approach is proposed which combining the advantage of unsupervised deep learning technique (i.e., Stacked Auto-Encoder (SAE)) with the advantage of supervised deep bidirectional recurrent learner (i.e., Bidirectional Long Short-Term Memory (BLSTM)). More specifically, the unsupervised SAE networks are implemented to learn the representative features hidden in all available input data (labeled and unlabeled samples) and store them as context vector. Then, partial context vector with corresponding labels and the quality variable measure value at previous time are concatenated to form a new merged input feature vector. After that, the temporal and dynamic features are further extracted from the new merged input feature vector via BLSTM networks. Subsequently, the fully connected layers (FCs) are exploited to learn the higher-level features from the last hidden layer of the BLSTM. Lastly, the learned output features by FCs are fed into a supervised liner regression layer to predict the coal quality metrics. Meanwhile, to avoid over-fitting, some regularization techniques are utilized and discussed in proposed network. The application in ash content estimation for a real dense medium coal preparation process and some comparison experiment result demonstrate that the effectiveness and priority of proposed soft sensor modeling approach.",industry
10.1016/j.patrec.2020.06.028,Journal,Pattern Recognition Letters,scopus,2020-10-01,sciencedirect,On the use of a full stack hardware/software infrastructure for sensor data fusion and fault prediction in industry 4.0,https://api.elsevier.com/content/abstract/scopus_id/85087339064,"Aspects related to prognostics are becoming a crucial part in the industrial sector. In this sense, Industry 4.0 is considered as a new paradigm that leverages on the IoT to propose increasingly more solutions to provide an estimate on the working conditions of an industrial plant. However, in context like the industrial sector where the number and heterogeneity of sensors can be very large, and the time requirements are very stringent, emerges the challenge to design effective infrastructures to interact with these complex systems. In this paper, we propose a full stack hardware/software infrastructure to collect, manage, and analyze the data gathered from a set of heterogeneous sensors attached to a real scale replica industrial plant available in our laboratory. On top of the proposed infrastructure we designed and implemented a fault prediction algorithm which exploits sensors data fusion with the aim to assess the working conditions of the industrial plant. The result section shows the obtained results in terms of accuracy from testing our proposed model and provides a comparison with a traditional Deep Neural Network (DNN) topology.",industry
10.1016/j.ins.2020.05.028,Journal,Information Sciences,scopus,2020-10-01,sciencedirect,Input selection methods for data-driven Soft sensors design: Application to an industrial process,https://api.elsevier.com/content/abstract/scopus_id/85086080455,"Soft Sensors (SSs) are inferential models which are widely used in industry. They are generally built through data-driven approaches that exploit industry historical databases. Selection of input variables is one of the most critical issues in SSs design. This paper aims at highlighting difficulties arising from the implementation of data-driven input selection methods when solving real-world case studies. A procedure is, therefore, proposed for input selection, based on both data-driven and expert-driven input selection methods. The procedure allows designing SSs with good prediction accuracy and a low number of inputs.
                  The design of an SS for a real-world industrial process is used. The results reported show that the selection methods proposed in literature do not give consistent results when applied to the considered case study. The key role for plant expert knowledge emerges, outlining the opportunity of judicious use of automatic data-driven procedures.",industry
10.1016/j.knosys.2020.106178,Journal,Knowledge-Based Systems,scopus,2020-09-27,sciencedirect,Deep learning-based unsupervised representation clustering methodology for automatic nuclear reactor operating transient identification,https://api.elsevier.com/content/abstract/scopus_id/85087409980,"Transient identification of condition monitoring data in nuclear reactor is important for system health assessment. Conventionally, the operating transients are correlated with the pre-designed ones by human operators during operations. However, due to necessary conservatism and significant differences between the operating and pre-designed transients, it has been less effective to manually identify transients, that usually contribute to different system degradation modes. This paper proposes a deep learning-based unsupervised representation clustering method for automatic transient pattern recognition based on the on-site condition monitoring data. Sample entropy is used as indicator for transient extraction, and a pre-training stage is implemented using an auto-encoder architecture for learning high-level features. An iterative representation clustering algorithm is further proposed to enhance the clustering effects, where a novel distance metric learning strategy is integrated. Experiments on a real-world nuclear reactor condition monitoring dataset validate the effectiveness and superiority of the proposed method, which provides a promising tool for transient identification in the real industrial scenarios. This study offers a new perspective in exploring unlabeled data with deep learning, and the end-to-end implementation scheme facilitates applications in the real nuclear industry.",industry
10.1016/j.cjche.2020.06.015,Journal,Chinese Journal of Chemical Engineering,scopus,2020-09-01,sciencedirect,Deep learning technique for process fault detection and diagnosis in the presence of incomplete data,https://api.elsevier.com/content/abstract/scopus_id/85089986909,"In modern industrial processes, timely detection and diagnosis of process abnormalities are critical for monitoring process operations. Various fault detection and diagnosis (FDD) methods have been proposed and implemented, the performance of which, however, could be drastically influenced by the common presence of incomplete or missing data in real industrial scenarios. This paper presents a new FDD approach based on an incomplete data imputation technique for process fault recognition. It employs the modified stacked autoencoder, a deep learning structure, in the phase of incomplete data treatment, and classifies data representations rather than the imputed complete data in the phase of fault identification. A benchmark process, the Tennessee Eastman process, is employed to illustrate the effectiveness and applicability of the proposed method.",industry
10.1016/j.robot.2020.103578,Journal,Robotics and Autonomous Systems,scopus,2020-09-01,sciencedirect,Real-time topological localization using structured-view ConvNet with expectation rules and training renewal,https://api.elsevier.com/content/abstract/scopus_id/85086575996,"Mobile service robots possess high potential of providing numerous assistances in the working areas. In an attempt to develop a mobile service robot which is dynamically balanced for faster movement and taller manipulation capability, we designed and prototyped J4.alpha, which is intended for swift navigation and nimble manipulation. Previously, we devised a pure visual method based on a supervised deep learning model for real-time recognition of nodal locations. Four low-resolution RGB cameras are installed around J4.alpha to capture the surrounding visual features for training and detection. As the method is developed for ease of implementation, fast real-time application, accurate detection, and low cost, we further improve the accuracy and the practicality of the method in this study. Specifically, a set of expectation rules are introduced to reject outlier detections, and a scheme of training renewal is devised to effectively react to environmental modifications. In our previous tests, precision and recall rates of the location coordinate detection by the ConvNet models were generally between 0.78 and 0.91; by introducing the expectation rules, precision and recall are improved by approximately 10%. A large scale field test is also carried out here for both corridor and factory scenarios; the performance of the proposed method was tested for detection accuracy and verified for 2 m and 0.5 m nodal intervals. The scheme of training renewal designed for capturing and reflecting environmental modifications was also proved to be effective.",industry
10.1016/j.scs.2020.102252,Journal,Sustainable Cities and Society,scopus,2020-09-01,sciencedirect,A deep learning-based IoT-oriented infrastructure for secure smart City,https://api.elsevier.com/content/abstract/scopus_id/85085594643,"In recent years, the Internet of Things (IoT) infrastructures are developing in various industrial applications in sustainable smart cities and societies such as smart manufacturing, smart industries. The Cyber-Physical System (CPS) is also part of IoT-oriented infrastructure. CPS has gained considerable success in industrial applications and critical infrastructure with a distributed environment. This system aims to integrate the physical world to computational facilities as cyberspace. However, there are many challenges, such as security and privacy, centralization, communication latency, scalability in such an environment. To mitigate these challenges, we propose a Deep Learning-based IoT-oriented infrastructure for a secure smart city where Blockchain provides a distributed environment at the communication phase of CPS, and Software-Defined Networking (SDN) establishes the protocols for data forwarding in the network. A deep learning-based cloud is utilized at the application layer of the proposed infrastructure to resolve communication latency and centralization, scalability. It enables cost-effective, high-performance computing resources for smart city applications such as the smart industry, smart transportation. Finally, we evaluated the performance of our proposed infrastructure. We compared it with existing methods using quantitative analysis and security and privacy analysis with different measures such as scalability and latency. The evaluation of our implementation results shows that performance is improved.",industry
10.1016/j.compind.2020.103226,Journal,Computers in Industry,scopus,2020-09-01,sciencedirect,Perspective on holonic manufacturing systems: PROSA becomes ARTI,https://api.elsevier.com/content/abstract/scopus_id/85085261123,"Looking back at 30 years of research into holonic manufacturing systems, these explorations made a lasting scientific contribution to the overall architecture of intelligent manufacturing systems. Most notably, holonic architectures are defined in terms of their world-of-interest (Van Brussel et al., 1998). They do not have an information layer, a communication layer, etc. Instead, they have components that relate to real-world assets (e.g. machine tools) and activities (e.g. assembly). And, they mirror and track the structure of their world-of-interest, which allows them to scale and adapt accordingly.
                  This research has wandered around, at times learning from its mistakes, and progressively carved out an invariant structure while it translated and applied scientific insights from complex-adaptive systems theory (e.g. autocatalytic sets) and from bounded rationality (e.g. holons). This paper presents and discusses the outcome of these research efforts.
                  At the top level, the holonic structure distinguishes intelligent beings (or digital twins) from intelligent agents. These digital twins inherit the consistency from reality, which they mirror. They are intelligent beings when they reflect what exists in the world without imposing artificial limitations in this reality. Consequently, a conflict with a digital twin is a conflict with reality.
                  In contrast, intelligent agents typically transform NP-hard challenges into computations with low-polynomial complexity. Unavoidably, this involves arbitrariness (e.g. don’t care choices). Likewise, relying on case-specific properties, to ensure an outcome in polynomial time, usually renders the validity of an agent’s choices both short-lived and situation-dependent. Here, intelligent agents create conflicts by imposing limitations of their own making in their world-of-interest.
                  Real-world smart systems are aggregates comprising both intelligent beings and intelligent agents. They are performers. Inside these performers, digital twins may constitute the foundations, supporting walls, support beams and pillars because these intelligent beings are protected by their real-world counterpart. Further refining the top-level of this architecture, a holonic structure enables these digital twins to shadow their real-world counterpart whenever it changes, adapts and evolves.
                  In contrast, the artificial limitations, imposed by the intelligent agents, cannot be allowed to build up inertia, which would hamper the undoing of arbitrary or case-specific limitations. To this end, performers explicitly manage the rights over their assets. Revoking such rights from a limitation-imposing agent will free the assets. This will be at the cost of reduced services from the agent. When other service providers rely on this agent, their services may be affected as well; that’s how the inertia builds up and how harmful legacy is created. Thus, the services of digital twins are to be preferred over the services of an intelligent agent by developers of holonic manufacturing systems.
                  Finally, digital twins corresponding to the decision making in the world-of-interest (a non-physical asset) allow to mirror the world-of-interest in a predictive mode (in addition to track and trace). It allows to generate short-term forecasts while preserving the benefits of intelligent beings. These twins are the intentions of the decision-making intelligent agents. Evidently, when intentions change, the forecasts needs to be regenerated (i.e. tracking the corresponding reality by the twin). This advanced feature can be deployed in a number of configurations (cf. annex).",industry
10.1016/j.compind.2020.103244,Journal,Computers in Industry,scopus,2020-09-01,sciencedirect,Machine learning for predictive scheduling and resource allocation in large scale manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85084401966,"The digitalization processes in manufacturing enterprises and the integration of increasingly smart shop floor devices and software control systems caused an explosion in the data points available in Manufacturing Execution Systems. The degree in which enterprises can capture value from big data processing and extract useful insights represents a differentiating factor in developing controls that optimize production and protect resources. Machine learning and Big Data technologies have gained increased traction being adopted in some critical areas of planning and control. Cloud manufacturing allows using these technologies in real time, lowering the cost of implementing and deployment. In this context, the paper offers a machine learning approach for reality awareness and optimization in cloud.
                  Specifically, the paper focuses on predictive production planning (operation scheduling, resource allocation) and predictive maintenance. The main contribution of this research consists in developing a hybrid control solution that uses Big Data techniques and machine learning algorithms to process in real time information streams in large scale manufacturing systems, focusing on energy consumptions that are aggregated at various layers. The control architecture is distributed at the edge of the shop floor for data collecting and format transformation, and then centralized at the cloud computing platform for data aggregation, machine learning and intelligent decisions. The information is aggregated in logical streams and consolidated based on relevant metadata; a neural network is trained and used to determine possible anomalies or variations relative to the normal patterns of energy consumption at each layer. This novel approach allows for accurate forecasting of energy consumption patterns during production by using Long Short-term Memory neural networks and deep learning in real time to re-assign resources (for batch cost optimization) and detect anomalies (for robustness) based on predicted energy data.",industry
10.1016/j.neucom.2020.02.109,Journal,Neurocomputing,scopus,2020-08-04,sciencedirect,Tracking control of redundant mobile manipulator: An RNN based metaheuristic approach,https://api.elsevier.com/content/abstract/scopus_id/85082490397,"In this paper, we propose a topology of Recurrent Neural Network (RNN) based on a metaheuristic optimization algorithm for the tracking control of mobile-manipulator while enforcing nonholonomic constraints. Traditional approaches for tracking control of mobile robots usually require the computation of Jacobian-inverse or linearization of its mathematical model. The proposed algorithm uses a nature-inspired optimization approach to directly solve the nonlinear optimization problem without any further transformation. First, we formulate the tracking control as a constrained optimization problem. The optimization problem is formulated on position-level to avoid the computationally expensive Jacobian-inversion. The nonholonomic limitation is ensured by adding equality constraints to the formulated optimization problem. We then present the Beetle Antennae Olfactory Recurrent Neural Network (BAORNN) algorithm to solve the optimization problem efficiently using very few mathematical operations. We present a theoretical analysis of the proposed algorithm and show that its computational cost is linear with respect to the degree of freedoms (DOFs), i.e., O(m). Additionally, we also prove its stability and convergence. Extensive simulation results are prepared using a simulated model of IIWA14, a 7-DOF industrial-manipulator, mounted on a differentially driven cart. Comparison results with particle swarm optimization (PSO) algorithm are also presented to prove the accuracy and numerical efficiency of the proposed controller. The results demonstrate that the proposed algorithm is several times (around 75 in the worst case) faster in execution as compared to PSO, and suitable for real-time implementation. The tracking results for three different trajectories; circular, rectangular, and rhodonea paths are presented.",industry
10.1016/j.heliyon.2020.e04667,Journal,Heliyon,scopus,2020-08-01,sciencedirect,Effects of mobile augmented reality apps on impulse buying behavior: An investigation in the tourism field,https://api.elsevier.com/content/abstract/scopus_id/85089806662,"Many of today's online services are designed specifically to encourage impulse buying. Moreover, many studies have shown that with the assistance of Mobile Augmented Reality, retailers have the potential to significantly improve their sales. However, the effects of Mobile AR on consumer impulse buying behavior have yet to be examined, particularly in the tourism field. Consequently, the present study integrates the Technology Acceptance Model (TAM), Stimulus-Organism-Response (SOR) framework, and flow theory to examine the effects of Mobile AR apps on tourist impulse buyingbehavior. The research model is implemented using an online questionnaire, with the results analyzed by Partial-Least-Squares Structural Equation Modeling (PLS-SEM) approach. The results obtained from 479 valid samples show that the characteristics of Mobile AR apps play an important role in governing tourist behavior in making unplanned purchases. In particular, as the utility, ease-of-use, and interactivity of the apps increase, the perceived enjoyment and satisfaction of the user also increase and give rise to a stronger impulse buying behavior. The results also reveal a mediating effect of the flow experience on the relationship between the perceived ease of use of the Mobile AR app and the user satisfaction in using the app. Overall, the findings presented in this study provide a useful source of reference for Mobile AR app developers, retailers, and tourism marketers in better understanding users' preferences for Mobile AR apps and strengthening their impulse buying behavior in the tourism context as a result.",industry
10.1016/j.aei.2020.101101,Journal,Advanced Engineering Informatics,scopus,2020-08-01,sciencedirect,Predictive model-based quality inspection using Machine Learning and Edge Cloud Computing,https://api.elsevier.com/content/abstract/scopus_id/85084733420,"The supply of defect-free, high-quality products is an important success factor for the long-term competitiveness of manufacturing companies. Despite the increasing challenges of rising product variety and complexity and the necessity of economic manufacturing, a comprehensive and reliable quality inspection is often indispensable. In consequence, high inspection volumes turn inspection processes into manufacturing bottlenecks.
                  In this contribution, we investigate a new integrated solution of predictive model-based quality inspection in industrial manufacturing by utilizing Machine Learning techniques and Edge Cloud Computing technology. In contrast to state-of-the-art contributions, we propose a holistic approach comprising the target-oriented data acquisition and processing, modelling and model deployment as well as the technological implementation in the existing IT plant infrastructure. A real industrial use case in SMT manufacturing is presented to underline the procedure and benefits of the proposed method. The results show that by employing the proposed method, inspection volumes can be reduced significantly and thus economic advantages can be generated.",industry
10.1016/j.physa.2019.124049,Journal,Physica A: Statistical Mechanics and its Applications,scopus,2020-08-01,sciencedirect,Fast Super-Paramagnetic Clustering,https://api.elsevier.com/content/abstract/scopus_id/85078038012,"We map stock market interactions to spin models to recover their hierarchical structure using a simulated annealing based Super-Paramagnetic Clustering (SPC) algorithm. This is directly compared to a modified implementation of a maximum likelihood approach we call fast Super-Paramagnetic Clustering (f-SPC). The methods are first applied to standard toy test-case problems, and then to a data-set of 447 stocks traded on the New York Stock Exchange (NYSE) over 1249 days. The signal to noise ratio of stock market correlation matrices is briefly considered. Our result recover approximately clusters representative of standard economic sectors and mixed ones whose dynamics shine light on the adaptive nature of financial markets and raise concerns relating to the effectiveness of industry based static financial market classification in the world of real-time data analytics. A key result is that we show that f-SPC maximum likelihood solutions converge to ones found within the Super-Paramagnetic Phase where the entropy is maximum, and those solutions are qualitatively better for high dimensionality data-sets.",industry
10.1016/j.measurement.2020.107768,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2020-07-15,sciencedirect,"Intelligent fault diagnosis of rotating machinery via wavelet transform, generative adversarial nets and convolutional neural network",https://api.elsevier.com/content/abstract/scopus_id/85082880587,"The fault detection of rotating machinery systems especially its typical components such as bearings and gears is of special importance for maintaining machine systems working normally and safely. However, due to the change of working conditions, the disturbance of environment noise, the weakness of early features and various unseen compound failure modes, it is quite hard to achieve high-accuracy intelligent failure monitoring task of rotating machinery using existing intelligent fault diagnosis approaches in real industrial applications. In the paper, a novel and high-accuracy fault detection approach named WT-GAN-CNN for rotating machinery is presented based on Wavelet Transform (WT), Generative Adversarial Nets (GANs) and convolutional neural network (CNN). The proposed WT-GAN-CNN approach includes three parts. To begin with, WT is employed for extracting time-frequency image features from one-dimension raw time domain signals. Secondly, GANs are used to generate more training image samples. Finally, the built CNN model is used to accomplish the fault detection of rotating machinery by the original training time-frequency images and the generated fake training time-frequency images. Two experiment studies are implemented to assess the effectiveness of our proposed approach and the results demonstrate it is higher in testing accuracy than other intelligent failure detection approaches in the literatures even in the interference of strong environment noise or when working conditions are changed. Furthermore, its result in the stability of testing accuracy is also quite excellent.",industry
10.1016/j.telpol.2020.101960,Journal,Telecommunications Policy,scopus,2020-07-01,sciencedirect,Innovation ecosystems theory revisited: The case of artificial intelligence in China,https://api.elsevier.com/content/abstract/scopus_id/85083340447,"Beyond the mainstream discussion on the key role of China in the global AI landscape, the knowledge about the real performance and future perspectives of the AI ecosystem in China is still limited. This paper evaluates the status and prospects of China's AI innovation ecosystem by developing a Triple Helix framework particularized for this case. Based on an in-depth qualitative study and on interviews with experts, the analysis section summarizes the way in which the AI innovation ecosystem in China is being built, which are the key features of the three spheres of the Triple Helix -governments, industry and academic/research institutions-as well as the dynamic context of the ecosystem through the identification of main aspects related to the flows of skills, knowledge and funding and the interactions among them. Using this approach, the discussion section illustrates the specificities of the AI innovation ecosystem in China, its strengths and its gaps, and which are its prospects. Overall, this revisited ecosystem approach permits the authors to address the complexity of emerging environments of innovation to draw meaningful conclusions which are not possible with mere observation. The results show how a favourable context, the broad adoption rate and the competition for talent and capital among regional-specialized clusters are boosting the advance of AI in China, mainly in the business to customer arena. Finally, the paper highlights the challenges ahead in the current implementation of the ecosystem that will largely determine the potential global leadership of China in this domain.",industry
10.1016/j.ins.2020.03.063,Journal,Information Sciences,scopus,2020-07-01,sciencedirect,Generating behavior features for cold-start spam review detection with adversarial learning,https://api.elsevier.com/content/abstract/scopus_id/85083304717,"Due to the wide applications, spam detection has long been a hot research topic in both academia and industry. Existing studies show that behavior features are effective in distinguishing the spam and legitimate reviews. However, it usually takes a long time to collect such features and thus is hard to apply them to cold-start spam review detection tasks. Recent advances leveraged the neural network to encode the various types of textual, behavior, and attribute information for this task. However, the inherent problem, i.e., lack of effective behavior features for new users who post just one review, is still unsolved.
                  In this paper, we exploit the generative adversarial network (GAN) for addressing this problem. The key idea is to generate synthetic behavior features (SBFs) for new users from their easily accessible features (EAFs). Specifically, we first select six well recognized real behavior features (RBFs) existing for regular users. We then train a GAN framework including a generator to generate SBFs from their EAFs including text, rating, and attribute features, and a discriminator to discriminate RBFs and SBFs. We design a new implementation of generator and discriminator for effective training. The trained GAN is finally applied to new users for generating synthetic behavior features. We conduct extensive experiments on two Yelp datasets. Experimental results demonstrate that our proposed framework significantly outperforms the state-of-the-art methods.",industry
10.1016/j.petrol.2020.107087,Journal,Journal of Petroleum Science and Engineering,scopus,2020-07-01,sciencedirect,Transformation of academic teaching and research: Development of a highly automated experimental sucker rod pumping unit,https://api.elsevier.com/content/abstract/scopus_id/85079611752,"Sucker rod pumps are one of the most popular solutions for artificial lift since their inception in the 19th century with minimum changes in design. Presently, companies are deploying digital technology in the field and, there has been a big push for a networked oilfield in recent years. This means technology is now able to control machines in remote places, evaluate their performances and control safety operating parameters. But these digital solutions are still not available in universities, causing a technological and technical gap for students and researchers.
                  This study presents a prototype of a new dedicated Interactive Digital Sucker Rod Pumping Unit (ID-SRP) system at the University of Oklahoma with representative operating conditions. The prototype mimics sucker rod pump working principles and also imitates different realistic rod string motions. The application and solutions are focused on providing authentic learning experiences for petroleum engineers. The system is also designed to address and optimize SRP well performance and safety through Model Predictive Controller (MPC) implementation and meeting industrial requirements. It connects the physical and virtual interaction with learning technologies. The objective is to bridge the tangible and the abstract for a better understanding of sucker rod concept and implement existing theories into the digital system. Additionally, it aids our future petroleum engineers on how to apply basic industry principles and upsurge their problem-solving skills.
                  The developed unit is capable of simulating any situations in real time and using Internet of Things (IoT) for data acquisition to create tailored diagnostic tools that students and laboratory staff can utilize. The software selected for the system is LabVIEW, which controls all the necessary equipment. This system can build personalized dynocard graphs, intake live data and export them to other programs live Excel, MATLAB, Python or any other programming languages.",industry
10.1016/j.eswa.2020.113251,Journal,Expert Systems with Applications,scopus,2020-07-01,sciencedirect,Integrating complex event processing and machine learning: An intelligent architecture for detecting IoT security attacks,https://api.elsevier.com/content/abstract/scopus_id/85079340111,"The Internet of Things (IoT) is growing globally at a fast pace: people now find themselves surrounded by a variety of IoT devices such as smartphones and wearables in their everyday lives. Additionally, smart environments, such as smart healthcare systems, smart industries and smart cities, benefit from sensors and actuators interconnected through the IoT. However, the increase in IoT devices has brought with it the challenge of promptly detecting and combating the cybersecurity attacks and threats that target them, including malware, privacy breaches and denial of service attacks, among others. To tackle this challenge, this paper proposes an intelligent architecture that integrates Complex Event Processing (CEP) technology and the Machine Learning (ML) paradigm in order to detect different types of IoT security attacks in real time. In particular, such an architecture is capable of easily managing event patterns whose conditions depend on values obtained by ML algorithms. Additionally, a model-driven graphical tool for security attack pattern definition and automatic code generation is provided, hiding all the complexity derived from implementation details from domain experts. The proposed architecture has been applied in the case of a healthcare IoT network to validate its ability to detect attacks made by malicious devices. The results obtained demonstrate that this architecture satisfactorily fulfils its objectives.",industry
10.1016/j.neucom.2020.01.083,Journal,Neurocomputing,scopus,2020-06-07,sciencedirect,Integrating adaptive moving window and just-in-time learning paradigms for soft-sensor design,https://api.elsevier.com/content/abstract/scopus_id/85079267624,"Most applications of soft sensors in process industries require learning from a stream of data, which may exhibit nonstationary dynamics, or concept drift. In this study, we develop a relevance vector machine (RVM) based novel adaptive learning algorithm called MWAdp-JITL, to meet the demands of continuous processes. The resulting algorithm combines active and passive learning: A moving window (MW) algorithm, which adapts the window size against virtual/real concept drifts, is coupled with a just-in-time learning (JITL) model, constructed using an appropriate region of historical data, and the ensemble weights of the MW and JITL models are adjusted for each query point. Tests on four real industrial datasets and a synthetic data, comprising various concept drift scenarios, show that MWAdp-JITL yields superior prediction accuracy and is generally more robust to changes in algorithm parameters compared to conventional adaptive learning methods and state-of-the-art algorithms from the literature. MWAdp-JITL complies with time limits of online prediction, and is applicable for high dimensional processes under various types of concept drifts. It is seen that MWAdp-JITL can successfully achieve a good balance in bias-variance tradeoff, justifying the use of only two exquisitely selected learners in ensemble learning.",industry
10.1016/j.cola.2020.100970,Journal,Journal of Computer Languages,scopus,2020-06-01,sciencedirect,"Visual Programming Environments for End-User Development of intelligent and social robots, a systematic review",https://api.elsevier.com/content/abstract/scopus_id/85085272330,"Robots are becoming interactive and robust enough to be adopted outside laboratories and in industrial scenarios as well as interacting with humans in social activities. However, the design of engaging robot-based applications requires the availability of usable, flexible and accessible development frameworks, which can be adopted and mastered by researchers and practitioners in social sciences and adult end users as a whole. This paper surveys Visual Programming Environments aimed at enabling a paradigm fostering the so-called End-User Development of applications involving robots with social capabilities. The focus of this article is on those Visual Programming Environments that are designed to support social research goals as well as to cater for professional needs of people not trained in more traditional text-based computer programming languages. This survey excludes interfaces aimed at supporting expert programmers, at allowing industrial robots to perform typical industrial tasks (such as pick and place operations), and at teaching children how to code. After having performed a systematic search, sixteen programming environments have been included in this survey. Our goal is two-fold: first, to present these software tools with their technical features and Authoring Artificial Intelligence modeling approaches, and second, to present open challenges in the development of Visual Programming Environments for end users and social researchers, which can be informative and valuable to the community. The results show that the most recent such tools are adopting distributed and Component-Based Software Engineering approaches and web technologies. However, few of them have been designed to enable the independence of end users from high-tech scribes. Moreover, findings indicate the need for (i) more objective and comparative evaluations, as well as usability and user experience studies with real end users; and (ii) validations of these tools for designing applications aimed at working “in-the-wild” rather than only in laboratories and structured settings.",industry
10.1016/j.asoc.2020.106208,Journal,Applied Soft Computing Journal,scopus,2020-06-01,sciencedirect,Dynamic scheduling for flexible job shop with new job insertions by deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85081140568,"In modern manufacturing industry, dynamic scheduling methods are urgently needed with the sharp increase of uncertainty and complexity in production process. To this end, this paper addresses the dynamic flexible job shop scheduling problem (DFJSP) under new job insertions aiming at minimizing the total tardiness. Without lose of generality, the DFJSP can be modeled as a Markov decision process (MDP) where an intelligent agent should successively determine which operation to process next and which machine to assign it on according to the production status of current decision point, making it particularly feasible to be solved by reinforcement learning (RL) methods. In order to cope with continuous production states and learn the most suitable action (i.e. dispatching rule) at each rescheduling point, a deep Q-network (DQN) is developed to address this problem. Six composite dispatching rules are proposed to simultaneously select an operation and assign it on a feasible machine every time an operation is completed or a new job arrives. Seven generic state features are extracted to represent the production status at a rescheduling point. By taking the continuous state features as input to the DQN, the state–action value (Q-value) of each dispatching rule can be obtained. The proposed DQN is trained using deep Q-learning (DQL) enhanced by two improvements namely double DQN and soft target weight update. Moreover, a “softmax” action selection policy is utilized in real implementation of the trained DQN so as to promote the rules with higher Q-values while maintaining the policy entropy. Numerical experiments are conducted on a large number of instances with different production configurations. The results have confirmed both the superiority and generality of DQN compared to each composite rule, other well-known dispatching rules as well as the stand Q-learning-based agent.",industry
10.1016/j.rcim.2019.101887,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2020-06-01,sciencedirect,Deep learning-based smart task assistance in wearable augmented reality,https://api.elsevier.com/content/abstract/scopus_id/85074770255,"Wearable augmented reality (AR) smart glasses have been utilized in various applications such as training, maintenance, and collaboration. However, most previous research on wearable AR technology did not effectively supported situation-aware task assistance because of AR marker-based static visualization and registration. In this study, a smart and user-centric task assistance method is proposed, which combines deep learning-based object detection and instance segmentation with wearable AR technology to provide more effective visual guidance with less cognitive load. In particular, instance segmentation using the Mask R-CNN and markerless AR are combined to overlay the 3D spatial mapping of an actual object onto its surrounding real environment. In addition, 3D spatial information with instance segmentation is used to provide 3D task guidance and navigation, which helps the user to more easily identify and understand physical objects while moving around in the physical environment. Furthermore, 2.5D or 3D replicas support the 3D annotation and collaboration between different workers without predefined 3D models. Therefore, the user can perform more realistic manufacturing tasks in dynamic environments. To verify the usability and usefulness of the proposed method, we performed quantitative and qualitative analyses by conducting two user studies: 1) matching a virtual object to a real object in a real environment, and 2) performing a realistic task, that is, the maintenance and inspection of a 3D printer. We also implemented several viable applications supporting task assistance using the proposed deep learning-based task assistance in wearable AR.",industry
10.1016/j.comcom.2020.04.053,Journal,Computer Communications,scopus,2020-05-15,sciencedirect,IOT and cloud computing based parallel implementation of optimized RBF neural network for loader automatic shift control,https://api.elsevier.com/content/abstract/scopus_id/85089243295,"One of the key issues in automatic shift control of V-type cyclical loaders is determining how to find the best gear for the current conditions according to a certain mapping relation, but this complex and nonlinear mapping is difficult to express by a mathematical relation. However, to solve such nonlinear problems, a radial basis function (RBF) neural network is the best choice. In this paper, a certain type of wheel loader is taken as the research object, and an RBF neural network algorithm based on an improved genetic algorithm (GA) optimization is proposed. The global search ability of the GA is improved by adaptively adjusting the crossover probability and mutation probability. The RBF neural network expansion coefficient is optimized by an improved GA. Using industrial IOT technology, an optimized RBF neural network based on Map-Reduce on a cloud computing cluster is designed. The diesel engine computer and transmission computer on the loader are integrated to achieve dual-processor distributed parallel data processing and calculation. Then the loader automatic variable speed control algorithm model of improved GA optimized RBF neural network based on IOT cloud computing is established. The network model is trained and simulated using real vehicle automatic shift test data. The simulation results show that the improved GA-RBF neural network algorithm can achieve a correct recognition rate of 97.92%. The error matrix norm reaches the minimum value when the algorithm is iterated to the 17th generation. The improved algorithm has the advantages of a high gear recognition rate, fast convergence speed and strong real-time shift performance and is an effective new shift control method. The test results show that the shift boost time is less than 0.15 s and has a certain gradient. Compared with the manual shift process performed in the past, some improvements are achieved in the optimal shift time, shift response speed and shift quality. Compared with the traditional single computer based on serial training RBF neural network learning algorithm, whether it is Great progress has been made in convergence speed, training time, recognition rate, and data processing capabilities. Through the simulation and test, the validity of the intelligent shift control method of the improved GA optimized RBF neural network based on IOT cloud computing is verified. It has better engineering application value.",industry
10.1016/j.microrel.2020.113640,Journal,Microelectronics Reliability,scopus,2020-05-01,sciencedirect,Two phase cooling with nano-fluid for highly dense electronic systems-on-chip – A pilot study,https://api.elsevier.com/content/abstract/scopus_id/85083093178,"In recent days, electronics gadgets need to design for higher functionalities with dense populated systems in order to meet the demands like lower in size, weight and power consumption. Even industrial electronic component and system design also prefer same slim fashion. On other hand the overheating of electronic components reduces its performance, life and by the way the reliability of such electronic product/system is greatly affected due to overheating. The conventional cooling methods failed to offer best performances. Hence this part of research proposed a effective two phase cooling technique with nano-fluid. The objective of this research is to maintain the maximum temperature at the junction and hot spots in order to break a new ground in the cooling of electronic systems. The Maximum permissible operating temperature for any commercial electronic applications is only upto 70 °C (equal to 343.15 K) and above which most of the inherent electronic circuits may malfunction and destroy the entire application.The HotSpot Simulator-6.0 software employed for establish, verify the simulated model and trial runs to answer many ‘what if’ questions. In the simulation, hottest spot has been found in Int_Reg region, where the steady temperature grows beyond the threshold temperature level. The temperature has to be decreased in order to provide reliable working environment. Hence, HFO 1234ze nano-fluid employed with flow rate of 1100 ml per minute. The nanofliuid minimizes the temperature of the simulated electronic circuit from 351.80 K to 326.86 K in UUT. The proposed two phase nano-fluid cooling system for 3-D Unit-Under Test (UUT) was verified and validated with real system and simulated for experiments. Thus, a high range of temperature difference from the initial and final steady state temperature has been evidently shown in the proposed two phase nano fluid cooling method. The system found outperforms as best of both the worlds. The nanofluid cooling system can be used in thermal-aware systems and highly dense systems to maintain the temperature not much than 343.15 K, even at full load conditions.",industry
10.1016/j.engappai.2020.103589,Journal,Engineering Applications of Artificial Intelligence,scopus,2020-05-01,sciencedirect,Deep reinforcement one-shot learning for artificially intelligent classification in expert aided systems,https://api.elsevier.com/content/abstract/scopus_id/85081990367,"In recent years there has been a sharp rise in applications, in which significant events need to be classified but only a few training instances are available. These are known as cases of one-shot learning. To handle this challenging task, organizations often use human analysts to classify events under high uncertainty. Existing algorithms use a threshold-based mechanism to decide whether to classify an object automatically or send it to an analyst for deeper inspection. However, this approach leads to a significant waste of resources since it does not take the practical temporal constraints of system resources into account. By contrast, the focus in this paper is on rigorously optimizing the resource consumption in the system which applies to broad application domains, and is of a significant interest for academic research, industrial developments, as well as society and citizens benefit. The contribution of this paper is threefold. First, a novel Deep Reinforcement One-shot Learning (DeROL) framework is developed to address this challenge. The basic idea of the DeROL algorithm is to train a deep-Q network to obtain a policy which is oblivious to the unseen classes in the testing data. Then, in real-time, DeROL maps the current state of the one-shot learning process to operational actions based on the trained deep-Q network, to maximize the objective function. Second, the first open-source software for practical artificially intelligent one-shot classification systems with limited resources is developed for the benefit of researchers and developers in related fields. Third, an extensive experimental study is presented using the OMNIGLOT dataset for computer vision tasks, the UNSW-NB15 dataset for intrusion detection tasks, and the Cleveland Heart Disease Dataset for medical monitoring tasks that demonstrates the versatility and efficiency of the DeROL framework.",industry
10.1016/j.ijmultiphaseflow.2019.103194,Journal,International Journal of Multiphase Flow,scopus,2020-05-01,sciencedirect,Bubble patterns recognition using neural networks: Application to the analysis of a two-phase bubbly jet,https://api.elsevier.com/content/abstract/scopus_id/85079560188,"Gas-liquid two-phase bubbly flows are found in different areas of science and technology such as nuclear energy, chemical industry, or piping systems. Optical diagnostics of two-phase bubbly flows with modern panoramic techniques makes it possible to capture simultaneously instantaneous characteristics of both continuous and dispersed phases with a high spatial resolution. In this paper, we introduce a novel approach based on neural networks to recognize bubble patterns in images and identify their geometric parameters. The originality of the proposed method consists in training of a neural network ensemble using synthetic images that resemble real photographs gathered in experiment. The use of neural networks in combination with automatically generated data allowed us to detect overlapping, blurred, and non-spherical bubbles in a broad range of volume gas fractions. Experiments on a turbulent bubbly jet proved that the implemented method increases the identification accuracy, reducing errors of various kinds, and lowers the processing time compared to conventional recognition methods. Furthermore, utilizing the new method of bubbles recognition, the primary physical parameters of a dispersed phase, such as bubble size distribution and local gas content, were calculated in a near-to-nozzle region of the bubbly jet. The obtained results and integral experimental parameters, especially volume gas fraction, are in good agreement with each other.",industry
10.1016/j.marpol.2020.103829,Journal,Marine Policy,scopus,2020-05-01,sciencedirect,Analyzing gaps in policy: Evaluation of the effectiveness of minimum landing size (MLS) regulations in Turkey,https://api.elsevier.com/content/abstract/scopus_id/85079518573,"The Mediterranean and Black Sea host the most intense overfishing and Turkey has the largest commercial fisheries in them (when both seas considered). However, the state of the Turkish fisheries is in critical condition as both the quality (i.e, in number of caught species, value and sizes of fish) and quantity of fisheries catches have been rapidly declining in recent decades. One pioneer fisheries management initiative thoroughly evaluated here pertains to minimum landing size (MLS) regulations for commercial taxa, with the aim of promoting stock sustainability by ensuring fish reproduce before they are caught. This study examines 29 taxa in relation to MLS by analyzing changes in catch per unit effort trends pre-and post MLS to gauge regulation effectiveness, changes to MLS regulations since implementation, and finally evaluates the Turkish MLS sizes in relation to Turkish maturity sizes, to provide advice for taxa requiring changes. It seems intensive fishing may have reduced the size at maturity for many species in Turkey, as they mature smaller here than the Mediterranean and global averages. Eleven taxa listed in MLS regulations are under the lengths of first maturity (Lmat) sizes in Turkish waters and need to be increased, especially that of bonito, hake, swordfish and bluefish (by 18 cm, 10 cm, 10 cm and 8 cm, respectively), while 16 taxa still require national studies to determine their Lmat sizes in Turkish waters. In conclusion, in Turkey, MLS regulations are completely ineffective due to a lack of monitoring and control for juvenile fish at landing sites, markets and processing plants, along with insufficient penalties for such infractions, yet, there remains plenty of room for improvement. To improve the state of the fisheries, MLS measures could be improved by increasing fines, monitoring and control, making some gear types more selective and use of real-time closures and no fishing zones to protect spawning and nursery habitats.",industry
10.1016/j.ress.2020.106821,Journal,Reliability Engineering and System Safety,scopus,2020-05-01,sciencedirect,Towards Efficient Robust Optimization using Data based Optimal Segmentation of Uncertain Space,https://api.elsevier.com/content/abstract/scopus_id/85078707908,"Performing multi-objective optimization under uncertainty is a common requirement in industries and academia. Robust optimization (RO) is considered as an efficient and tractable approach provided one has access to behavioral data for the uncertain parameters. However, solutions of RO may be far from the real solution and less reliable due to inability to map the uncertain space accurately, especially when the data appears discontinuous and scattered in the uncertain domain. Amalgamating machine learning algorithms with RO, this paper proposes a data-driven methodology, where a novel fuzzy clustering mechanism is implemented along-with boundary construction, to transcript the uncertain space such that the specific regions of uncertainty are identified. Subsequently, using intelligent Sobol sampling, samples are generated in the mapped uncertain regions. Results of two test cases are presented along with a comprehensive comparison study. Considered case-studies include highly nonlinear model for continuous casting process from steelmaking industries, where a multi-objective optimization problem under uncertainty is solved to balance the conflict between productivity and energy consumption. The Pareto-optimal solutions of the resulting RO problem are obtained through Non-Dominated Sorting Genetic Algorithm – II, and ~23–29% improvement is observed in the uncertain objective function. Further, the spread and diversity metrics are enhanced by ~10–95% as compared to those obtained using other standard uncertainty sets.",industry
10.1016/j.jss.2020.110519,Journal,Journal of Systems and Software,scopus,2020-05-01,sciencedirect,Traceability Link Recovery between Requirements and Models using an Evolutionary Algorithm Guided by a Learning to Rank Algorithm: Train control and management case,https://api.elsevier.com/content/abstract/scopus_id/85078162306,"Traceability Link Recovery (TLR) has been a topic of interest for many years within the software engineering community. In recent years, TLR has been attracting more attention, becoming the subject of both fundamental and applied research. However, there still exists a large gap between the actual needs of industry on one hand and the solutions published through academic research on the other.
                  In this work, we propose a novel approach, named Evolutionary Learning to Rank for Traceability Link Recovery (TLR-ELtoR). TLR-ELtoR recovers traceability links between a requirement and a model through the combination of evolutionary computation and machine learning techniques, generating as a result a ranking of model fragments that can realize the requirement.
                  TLR-ELtoR was evaluated in a real-world case study in the railway domain, comparing its outcomes with five TLR approaches (Information Retrieval, Linguistic Rule-based, Feedforward Neural Network, Recurrent Neural Network, and Learning to Rank). The results show that TLR-ELtoR achieved the best results for most performance indicators, providing a mean precision value of 59.91%, a recall value of 78.95%, a combined F-measure of 62.50%, and a MCC value of 0.64. The statistical analysis of the results assesses the magnitude of the improvement, and the discussion presents why TLR-ELtoR achieves better results than the baselines.",industry
10.1016/j.ejor.2019.10.015,Journal,European Journal of Operational Research,scopus,2020-05-01,sciencedirect,From one-class to two-class classification by incorporating expert knowledge: Novelty detection in human behaviour,https://api.elsevier.com/content/abstract/scopus_id/85074814083,"One-class classification is the standard procedure for novelty detection. Novelty detection aims to identify observations that deviate from a determined normal behaviour. Only instances of one class are known, whereas so called novelties are unlabelled. Traditional novelty detection applies methods from the field of outlier detection. These standard one-class classification approaches have limited performance in many real business cases. The traditional techniques are mainly developed for industrial problems such as machine condition monitoring. When applying these to human behaviour, the performance drops significantly. This paper proposes a method that improves existing approaches by creating semi-synthetic novelties in order to have labelled data for the two classes. Expert knowledge is incorporated in the initial phase of this data generation process. The method was deployed on a real-life test case where the goal was to detect fraudulent subscriptions to a telecom family plan. This research demonstrates that the two-class expert model outperforms a one-class model on the semi-synthetic dataset. In a next step the model was validated on a real dataset. A fraud detection team of the company manually checked the top predicted novelties. The results show that incorporating expert knowledge to transform a one-class problem into a two-class problem is a valuable method.",industry
10.1016/j.dss.2020.113266,Journal,Decision Support Systems,scopus,2020-04-01,sciencedirect,ForeSim-BI: A predictive analytics decision support tool for capacity planning,https://api.elsevier.com/content/abstract/scopus_id/85079423691,"This paper proposes a decision support tool for maintenance capacity planning of complex product systems. The tool – ForeSim-BI – addresses the problem faced by maintenance organizations in forecasting the workload of future maintenance interventions and in planning an adequate capacity to face that expected workload. Developed and implemented from a predictive analytics perspective in the particular context of a Portuguese aircraft maintenance organization, the tool integrates four main modules: (1) a forecasting module used to predict future and unprecedented maintenance workloads from historical data; (2) a Bayesian inference module used to transform prior workload forecasts, resulting from the forecasting module, into predictive forecasts after observations on the maintenance interventions being predicted become available; (3) a simulation module used to characterize the forecasted total workloads through sets of random variables, including maintenance work types, maintenance work phases, and maintenance work skills; and (4) a Bayesian network module used to combine the simulated workloads with historical data through probabilistic inference. A linear programming model is also developed to improve the efficiency of the decision-making process supported by Bayesian networks. The tool uses real industrial data, comprising 171 aircraft maintenance projects collected at the host organization, and is validated by comparing its results with real observations of a given maintenance intervention to which predictions were made and with a model simulating current forecasting practices employed in industry. Significantly more accurate forecasts have been obtained with the proposed tool, resulting in an important cost saving potential for maintenance organizations.",industry
10.1016/j.aei.2020.101052,Journal,Advanced Engineering Informatics,scopus,2020-04-01,sciencedirect,Deep learning-based method for vision-guided robotic grasping of unknown objects,https://api.elsevier.com/content/abstract/scopus_id/85079340469,"Nowadays, robots are heavily used in factories for different tasks, most of them including grasping and manipulation of generic objects in unstructured scenarios. In order to better mimic a human operator involved in a grasping action, where he/she needs to identify the object and detect an optimal grasp by means of visual information, a widely adopted sensing solution is Artificial Vision. Nonetheless, state-of-art applications need long training and fine-tuning for manually build the object’s model that is used at run-time during the normal operations, which reduce the overall operational throughput of the robotic system. To overcome such limits, the paper presents a framework based on Deep Convolutional Neural Networks (DCNN) to predict both single and multiple grasp poses for multiple objects all at once, using a single RGB image as input. Thanks to a novel loss function, our framework is trained in an end-to-end fashion and matches state-of-art accuracy with a substantially smaller architecture, which gives unprecedented real-time performances during experimental tests, and makes the application reliable for working on real robots. The system has been implemented using the ROS framework and tested on a Baxter collaborative robot.",industry
10.1016/j.autcon.2019.103062,Journal,Automation in Construction,scopus,2020-04-01,sciencedirect,Towards automated clash resolution of reinforcing steel design in reinforced concrete frames via Q-learning and building information modeling,https://api.elsevier.com/content/abstract/scopus_id/85078657649,"The design of reinforcing steel bars (rebars) is critical to reinforced concrete (RC) structures. Generally, a good number of rebars are required by a design code, particularly at member connections. As such, rebar clashes (i.e., collisions and congestions) would be inevitable. It would be impractical, labor-intensive, and error-prone to avoid all possible clashes manually or even using standard design software. The building information modeling (BIM) technology has been utilized by the present architecture, engineering, and construction (ACE) industry for clash-free rebar designs. However, most existing BIM-based approaches offer the clash resolution strategy for moving components with an optimization algorithm, and are only applicable to the RC structures with regular shapes. In particular, the optimized path of rebars cannot be adjusted to avoid the obstacles, thus limiting the practical applications. Furthermore, most existing studies lack the learning from design code and constructibility constraints to realize automatic and intelligent arrangement and adjustment of rebars for avoiding the obstacles encountered in complex RC joints and frame structures. Considering these shortcomings, the authors have recently proposed an immediate reward-based multi-agent reinforcement learning (MARL) system with BIM, towards automatic clash-free rebar designs of RC joints without clashes. However, as the immediate reward is required in the MARL system for guiding the learning of a rebar design, it will not succeed in clash-free rebar designs of complex RC structures where immediate reward is often unavailable. In this study, this study further extends the previous work with Q-learning (a model-free reinforcement learning algorithm) for more realistic path planning considering both immediate and delayed rewards in clash-free rebar designs for real-world RC structures. In particular, the rebar design problem is treated as a path-planning problem of multi-agent system, where each rebar is deemed as an intelligence reinforcement learning agent. Next, by employing the Q-learning as the reinforcement learning engine, the particular form of state, action, and immediate and delayed rewards for the reinforcement MARL for automatic rebar designs considering more actual constructible constraints and design codes can be developed. Comprehensive experiments on three typical beam-column joints and a two-story RC building frame were conducted to evaluate the efficiency of the proposed method. The study results of paths of rebar designs, success rates, and average time confirm that the proposed framework with MARL and BIM is effective and efficient.",industry
10.1016/j.talanta.2019.120664,Journal,Talanta,scopus,2020-04-01,sciencedirect,Modelling of bioprocess non-linear fluorescence data for at-line prediction of etanercept based on artificial neural networks optimized by response surface methodology,https://api.elsevier.com/content/abstract/scopus_id/85076829838,"In the last years, regulatory agencies in biopharmaceutical industry have promoted the design and implementation of Process Analytical Technology (PAT), which aims to develop rapid and high-throughput strategies for real-time monitoring of bioprocesses key variables, in order to improve their quality control lines. In this context, spectroscopic techniques for data generation in combination with chemometrics represent alternative analytical methods for on-line critical process variables prediction. In this work, a novel multivariate calibration strategy for the at-line prediction of etanercept, a recombinant protein produced in a mammalian cells-based perfusion process, is presented. For data generation, samples from etanercept processes were daily obtained, from which fluorescence excitation-emission matrices were generated in the spectral ranges of 225.0 and 495.0 nm and 250.0 and 599.5 nm for excitation and emission modes, respectively. These data were correlated with etanercept concentration in supernatant (measured by an off-line HPLC-based reference univariate technique) by implementing different chemometric strategies, in order to build predictive models. Partial least squares (PLS) regression evidenced a non-linear relation between signal and concentration when observing actual vs. predicted concentrations. Hence, a non-parametric approach was implemented, based on a multilayer perceptron artificial neural network (MLP). The MLP topology was optimized by means of the response surface methodology. The prediction performance of MLP model was superior to PLS, since the first is able to cope with non-linearity in calibration models, reaching percentage mean relative error in predictions of about 7.0% (against 12.6% for PLS). This strategy represents a fast and inexpensive approach for etanercept monitoring, which conforms the principles of PAT.",industry
10.1016/j.compag.2020.105284,Journal,Computers and Electronics in Agriculture,scopus,2020-03-01,sciencedirect,An experimental study of stunned state detection for broiler chickens using an improved convolution neural network algorithm,https://api.elsevier.com/content/abstract/scopus_id/85079902403,"Effective recognition method of broiler stunned state has always been an important issue in real industries. In recent years, recognition methods such as neural networks have been receiving increasing attention due to their great merits of high diagnostic accuracy and easy implementation. To improve the accuracy and efficiency of broiler stunned state recognition, an improved fast region-based convolutional neural network (You Only Look Once + Multilayer Residual Module (YOLO + MRM)) algorithm was proposed and applied to the recognition of three broiler stunned states: insufficient, appropriate and excessive stuns. The images were collected from a broiler-slaughtering line using a complementary metal-oxide semiconductor (CMOS) camera. The area of the head and wings of a broiler in the original image was marked according to the PASCAL VOC data format and the dataset of each broiler stunned state was obtained. The results showed that the YOLO + MRM algorithm achieved good performance with an accuracy of 96.77%. To compare YOLO + MRM with other models, similar experiments were conducted using a conventional back propagation neural network (BP-NN) classifier, as well as YOLO, and the recognition accuracies were 90.11% and 94.74%, respectively. YOLO + MRM can complete the detection task of more than 180,000 broilers per hour. Compared with the traditional method, little prior expertise on image recognition is required, the recognition accuracy and speed are improved obviously. This study has provided a foundation and highlighted the potential for automatically detecting the stunned state of broiler chickens, which is crucial for the success of an automatic electric stunning process in the poultry industry.",industry
10.1016/j.adhoc.2019.102047,Journal,Ad Hoc Networks,scopus,2020-03-01,sciencedirect,An intelligent Edge-IoT platform for monitoring livestock and crops in a dairy farming scenario,https://api.elsevier.com/content/abstract/scopus_id/85076174369,"Today’s globalized and highly competitive world market has broadened the spectrum of requirements in all the sectors of the agri-food industry. This paper focuses on the dairy industry, on its need to adapt to the current market by becoming more resource efficient, environment-friendly, transparent and secure. The Internet of Things (IoT), Edge Computing (EC) and Distributed Ledger Technologies (DLT) are all crucial to the achievement of those improvements because they allow to digitize all parts of the value chain, providing detailed information to the consumer on the final product and ensuring its safety and quality. In Smart Farming environments, IoT and DLT enable resource monitoring and traceability in the value chain, allowing producers to optimize processes, provide the origin of the produce and guarantee its quality to consumers. In comparison to a centralized cloud, EC manages the Big Data generated by IoT devices by processing them at the network edge, allowing for the implementation of services with shorter response times, and a higher Quality of Service (QoS) and security. This work presents a platform oriented to the application of IoT, Edge Computing, Artificial Intelligence and Blockchain techniques in Smart Farming environments, by means of the novel Global Edge Computing Architecture, and designed to monitor the state of dairy cattle and feed grain in real time, as well as ensure the traceability and sustainability of the different processes involved in production. The platform is deployed and tested in a real scenario on a dairy farm, demonstrating that the implementation of EC contributes to a reduction in data traffic and an improvement in the reliability in communications between the IoT-Edge layers and the Cloud.",industry
10.1016/j.future.2019.10.043,Journal,Future Generation Computer Systems,scopus,2020-03-01,sciencedirect,HealthFog: An ensemble deep learning based Smart Healthcare System for Automatic Diagnosis of Heart Diseases in integrated IoT and fog computing environments,https://api.elsevier.com/content/abstract/scopus_id/85074613864,"Cloud computing provides resources over the Internet and allows a plethora of applications to be deployed to provide services for different industries. The major bottleneck being faced currently in these cloud frameworks is their limited scalability and hence inability to cater to the requirements of centralized Internet of Things (IoT) based compute environments. The main reason for this is that latency-sensitive applications like health monitoring and surveillance systems now require computation over large amounts of data (Big Data) transferred to centralized database and from database to cloud data centers which leads to drop in performance of such systems. The new paradigms of fog and edge computing provide innovative solutions by bringing resources closer to the user and provide low latency and energy efficient solutions for data processing compared to cloud domains. Still, the current fog models have many limitations and focus from a limited perspective on either accuracy of results or reduced response time but not both. We proposed a novel framework called HealthFog for integrating ensemble deep learning in Edge computing devices and deployed it for a real-life application of automatic Heart Disease analysis. HealthFog delivers healthcare as a fog service using IoT devices and efficiently manages the data of heart patients, which comes as user requests. Fog-enabled cloud framework, FogBus is used to deploy and test the performance of the proposed model in terms of power consumption, network bandwidth, latency, jitter, accuracy and execution time. HealthFog is configurable to various operation modes which provide the best Quality of Service or prediction accuracy, as required, in diverse fog computation scenarios and for different user requirements.",industry
10.1016/j.compind.2019.103164,Journal,Computers in Industry,scopus,2020-02-01,sciencedirect,Integrating artificial intelligent techniques and continuous time simulation modelling. Practical predictive analytics for energy efficiency and failure detection,https://api.elsevier.com/content/abstract/scopus_id/85099790267,"Energy efficiency and reliability needs are growing in many economic sectors, where predictive analytics are becoming essential tools for these key variables forecasting.
                  When predicting these variables, in many occasions, the problem to simplify the prediction model format when dealing with similar systems, which are placed in different functional locations, is a very complex problem due to model unavoidable dependency on changing operating conditions (per time and location). So effort is placed in this paper to develop tools that can easily adapt prediction models’ structure to existing operating conditions, for a given time period and place where the asset is located. Furthermore, these tools may allow the model to be easily trained and tested for automated implementation within the plant’s remote surveillance system.
                  To this end, Artificial Intelligence (AI) techniques, and in particular artificial neural network (ANN) models, have been selected in this paper as prediction models, since their structure can be adapted to improve predictions accuracy and they can also learn from dynamic changes in environmental conditions.
                  To demonstrate the adaptability for prediction accuracy and self-learning capabilities of the model, we have implemented an ANN with a backpropagation algorithm as a continuous time simulation model, which is then implemented using Vensim simulation environment, to benefit of the outstanding software optimization features for fast training.
                  Using this model we provide predictions of asset degradation and operational risk under existing real time internal and locational variables. We can also dynamically release preventive maintenance activities. This prediction model is exemplified in an industrial case for failures in cryogenic pumps of LNG tanks.",industry
10.1016/j.comcom.2020.01.018,Journal,Computer Communications,scopus,2020-02-01,sciencedirect,Enhanced resource allocation in mobile edge computing using reinforcement learning based MOACO algorithm for IIOT,https://api.elsevier.com/content/abstract/scopus_id/85077781443,"The Mobile networks deploy and offers a multiaspective approach for various resource allocation paradigms and the service based options in the computing segments with its implication in the Industrial Internet of Things (IIOT) and the virtual reality. The Mobile edge computing (MEC) paradigm runs the virtual source with the edge communication between data terminals and the execution in the core network with a high pressure load. The demand to meet all the customer requirements is a better way for planning the execution with the support of cognitive agent. The user data with its behavioral approach is clubbed together to fulfill the service type for IIOT. The swarm intelligence based and reinforcement learning techniques provide a neural caching for the memory within the task execution, the prediction provides the caching strategy and cache business that delay the execution. The factors affecting this delay are predicted with mobile edge computing resources and to assess the performance in the neighboring user equipment. The effectiveness builds a cognitive agent model to assess the resource allocation and the communication network is established to enhance the quality of service. The Reinforcement Learning techniques Multi Objective Ant Colony Optimization (MOACO) algorithms has been applied to deal with the accurate resource allocation between the end users in the way of creating the cost mapping tables creations and optimal allocation in MEC.",industry
10.1016/j.cie.2019.106225,Journal,Computers and Industrial Engineering,scopus,2020-02-01,sciencedirect,Fuzzy possibility regression integrated with fuzzy adaptive neural network for predicting and optimizing electrical discharge machining parameters,https://api.elsevier.com/content/abstract/scopus_id/85076689961,"An electrical discharge machining (EDM) is one of the special production methods that are widely used in moldings, repairs and production of specific industrial components. Due to extensive production costs, optimal machining specifications are significant. Machining specifications are effective on output quality and thus attract more customers leading to higher profits. In this study, the impact of EDM parameters on surface roughness, material removal rate and electrode corrosion percentage have been investigated. In order to consider uncertainty of real production environments, the fuzzy theory is employed. Also, using the design of experiment (DOE) parameters calibration is performed and mathematical programming approach is applied for optimization purpose. The relationship between the machining parameters and the output process specification is examined by a fuzzy possibility regression model. Then, the mathematical relation of exact inputs and fuzzy outputs of the EDM process are extracted. The effectiveness of the three outputs is evaluated by interfacing models and fuzzy hypothesis testing. To determine the optimal levels of each output, a fuzzy adaptive neural network is used and appropriate models are prepared to be adapted with a fitted model of fuzzy possibility regression for comparison purposes. Validation tests imply the effectiveness of the proposed method. The integrated model is implemented in real case study. The results show that, fitted models can predict the material removal rate, surface fineness, and corrosion percentage of the electrode. The prediction accuracy of the proposed method is shown in comparison with the optimal fuzzy adaptive neural network outputs considering error value. Also, the proposed method is successful in identifying the optimal process parameters for EDM with reliable accuracy. The proposed integrated prediction and optimization model can be used as a calibration decision support in production systems to handle dynamic data structures and provide real time machining specifications to increase the output quality.",industry
10.1016/j.micpro.2019.102906,Journal,Microprocessors and Microsystems,scopus,2020-02-01,sciencedirect,Area and power efficient pipelined hybrid merged adders for customized deep learning framework for FPGA implementation,https://api.elsevier.com/content/abstract/scopus_id/85073599282,"With the rapid growth of deep learning and neural network algorithms, various fields such as communication, Industrial automation, computer vision system and medical applications have seen the drastic improvements in recent years. However, deep learning and neural network models are increasing day by day, while model parameters are used for representing the models. Although the existing models use efficient GPU for accommodating these models, their implementation in the dedicated embedded devices needs more optimization which remains a real challenge for researchers. Thus paper, carries an investigation of deep learning frameworks, more particularly as review of adders implemented in the deep learning framework. A new pipelined hybrid merged adders (PHMAC) optimized for FPGA architecture which has more efficient in terms of area and power is presented. The proposed adders represent the integration of the principle of carry select and carry look ahead principle of adders in which LUT is re-used for the different inputs which consume less power and provide effective area utilization. The proposed adders were investigated on different FPGA architectures in which the power and area were analyzed. Comparison of the proposed adders with the other adders such as carry select adders (CSA), carry look ahead adder (CLA), Carry skip adders and Koggle Stone adders has been made and results have proved to be highly vital into a 50% reduction in the area, power and 45% when compared with above mentioned traditional adders.",industry
10.1016/j.rcim.2019.101847,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2020-02-01,sciencedirect,Trajectory smoothing method using reinforcement learning for computer numerical control machine tools,https://api.elsevier.com/content/abstract/scopus_id/85070511935,"Tool-path codes output by computer-aided manufacturing software for high-speed machining are composed of discontinuous G01 line segments. The discontinuity of these tool movements causes computer numerical control (CNC) inefficiency. To achieve high-speed continuous motion, corner smoothing algorithms based on pre-planning methods are widely used. However, it is difficult to optimize smoothing trajectories in real-time systems. To obtain smooth trajectories efficiently, this paper proposes a neural network-based direct trajectory smoothing method. An intelligent neural network agent outputs servo commands directly based on the current tool path and running state in every cycle. To achieve direct control, motion feature and reward models were built, and reinforcement learning was used to train the neural network parameters without additional experimental data. The proposed method provides higher cutting efficiency than the local and global smoothing algorithms. Given its simple structure and low computational demands, it can easily be applied to real-time CNC systems.",industry
10.1016/j.ejor.2019.07.057,Journal,European Journal of Operational Research,scopus,2020-02-01,sciencedirect,Automating the planning of container loading for Atlas Copco: Coping with real-life stacking and stability constraints,https://api.elsevier.com/content/abstract/scopus_id/85070381903,"The Atlas Copco☆ distribution center in Allen, TX, supplies spare parts and consumables to mining and construction companies across the world. For some customers, packages are shipped in sea containers. Planning how to load the containers is difficult due to several factors: heterogeneity of the packages with respect to size, weight, stackability, positioning and orientation; the set of packages differs vastly between shipments; it is crucial to avoid cargo damage. Load plan quality is ultimately judged by shipping operators.
                  This container loading problem is thus rich with respect to practical considerations. These are posed by the operators and include cargo and container stability as well as stacking and positioning constraints. To avoid cargo damage, the stacking restrictions are modeled in detail. For solving the problem, we developed a two-level metaheuristic approach and implemented it in a decision support system. The upper level is a genetic algorithm which tunes the objective function for a lower level greedy-type constructive placement heuristic, to optimize the quality of the load plan obtained.
                  The decision support system shows load plans on the forklift laptops and has been used for over two years. Management has recognized benefits including reduction of labour usage, lead time, and cargo damage risk.",industry
10.1016/j.neucom.2019.09.082,Journal,Neurocomputing,scopus,2020-01-29,sciencedirect,A scalable and reconfigurable in-memory architecture for ternary deep spiking neural network with ReRAM based neurons,https://api.elsevier.com/content/abstract/scopus_id/85073152550,"Neuromorphic computing using post-CMOS technologies is gaining increasing popularity due to its promising potential to resolve the power constraints in Von-Neumann machine and its similarity to the operation of the real human brain. In this work, we propose a scalable and reconfigurable architecture that exploits the ReRAM-based neurons for deep Spiking Neural Networks (SNNs). In prior publications, neurons were implemented using dedicated analog or digital circuits that are not area and energy efficient. In our work, for the first time, we address the scaling and power bottlenecks of neuromorphic architecture by utilizing a single one-transistor-one-ReRAM (1T1R) cell to emulate the neuron. We show that the ReRAM-based neurons can be integrated within the synaptic crossbar to build extremely dense Process Element (PE)–spiking neural network in memory array–with high throughput. We provide microarchitecture and circuit designs to enable the deep spiking neural network computing in memory with an insignificant area overhead. Simulation results on MNIST and CIFAR-10 datasets with spiking Resnet (SResnet) and spiking Squeezenet (SSqueez) show that compared to the baseline CPU only solution, our proposed architecture achieves energy saving between 1222 ×  and 1853 ×  and speed improvement between 791 ×  to 1120 ×.",industry
10.1016/j.jclepro.2019.118788,Journal,Journal of Cleaner Production,scopus,2020-01-20,sciencedirect,Rapid evaluation of micro-scale photovoltaic solar energy systems using empirical methods combined with deep learning neural networks to support systems’ manufacturers,https://api.elsevier.com/content/abstract/scopus_id/85073926377,"Solar energy is becoming one of the most attractive renewable sources. In many cases, due to a wide range of financial or installation limitations, off-grid small scale micro power panels are favoured as modular systems to power lighting in gardens or to be integrated together to power small devices such as mobile phone chargers and distributed smart city facilities and services. Manufacturers and systems’ integrators have a wide range of options of micro-scale photo voltaic panels to choose from. This makes the selection of the right panel a challenging task and risky investment. To address this and to help manufacturers, this paper suggests and evaluates a novel approach based on integrating empirical lab-testing with short-term real data and neural networks to assess the performance of micro-scale photovoltaic panels and their suitability for a specific application in specific environment. The paper outlines the combination of lab testing power output under seasonal and hourly conditions during the year combined with environmental and operating conditions such as temperature, dust accumulation and tilt angle performance. Based on the lab results, a short in-situ experimental work is implemented and the performance over the year in the selected location in Kuwait is evaluated using deep learning neural networks. The findings of this approach are compared with simulation and long-term real data. The results show a maximum error of 23% of the neural network output when compared with the actual data, and a correlation values with previous work within 87.3% and 91.9% which indicate that the proposed approach could provide an experimental rapid and accurate assessment of the expected power output. Hence, supporting the rapid decision-making process for manufacturers and reducing investment risks.",industry
10.1016/j.fuel.2019.116250,Journal,Fuel,scopus,2020-01-15,sciencedirect,"On the development of experimental methods to determine the rates of asphaltene precipitation, aggregation, and deposition",https://api.elsevier.com/content/abstract/scopus_id/85072862208,"Despite the efforts throughout the last few decades, asphaltene deposition remains as one of the greatest challenges in the petroleum industry. In this work, we present a comprehensive series of experimental studies to better understand the asphaltene precipitation, aggregation, and deposition mechanisms. Here, we introduce a simple method to determine the amount of precipitated asphaltene using NIR spectroscopy measurements without the implementation of calibration curves. Moreover, the kinetics of asphaltene precipitation and aggregation is simultaneously investigated by a newly developed, fast, and reliable NIR spectroscopy technique. In the new method, only less than 2 ml of sample is required for each experiment. In addition, unlike gravimetric techniques, less time consuming and labor-intensive measurements can be performed. In addition, the temperature can be controlled; hence, experiments can be conducted to evaluate the effects of temperature and the driving force on the kinetics of asphaltene precipitation and aggregation. Subsequently, the quantified precipitated asphaltene amount can be used to calibrate the precipitation and aggregation kinetic parameters of the asphaltene deposition model. The results obtained from the kinetics experiments facilitate in establishing a function to scale the precipitation kinetic parameter from laboratory-scale experiments to real field high-pressure high-temperature conditions. Additionally, a multi-section stainless steel packed bed column is proposed to study asphaltene deposition at high temperature and under dynamic conditions. In these experiments, the amount of deposited asphaltene is directly quantified. The results from the packed bed column deposition tests can be used to calibrate the deposition kinetic parameter of the asphaltene deposition model.",industry
10.1016/j.neucom.2019.09.004,Journal,Neurocomputing,scopus,2020-01-02,sciencedirect,Digital neuromorphic real-time platform,https://api.elsevier.com/content/abstract/scopus_id/85072526243,"Hardware implementations of spiking neural networks in portable devices can improve many applications of robotics, neurorobotics or prosthetic fields in terms of power consumption, high-speed processing and learning mechanisms. Analog and digital platforms have been previously proposed to run these networks. Analog designs are closer to biology since they implement the original mathematical model. However, digital platforms are, to some extent, abstractions of this model so far. In this paper, a full digital platform to design, implement and run real-time analog-like spiking neural networks is presented. Specifically, we present the design and implementation of digital circuits to run real-time biologically plausible spiking neural networks on a Field Programmable Gate Array (FPGA). The circuit designed for the neuron implements the Leaky Integrate and Fire (LIF) model. The synapsis implemented is a bi-exponential current-based one. The synaptic circuit design consists of one static memory with the baseline current and a dynamic memory which stores the updated contribution over time of each pre-synaptic connection. All the parameters of both the neuron and the synapse are configurable. The results of the circuits are validated by running the same experiments on the Brian simulator. The circuits, which are totally original and independent of the technology, use only 136 slice registers of hardware resources. Thus, these designs allow the scale of the network. These circuits aim to be the basis of the spiking neural networks on digital devices. This platform allows the user to first simulate their network within the Brian simulator and then, confidently, move to the hardware platform replicating the same performance or even replace their analog platform with the digital one.",industry
10.1016/j.ifacol.2020.12.2866,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Reinforcement learning for dual-resource constrained scheduling,https://api.elsevier.com/content/abstract/scopus_id/85107805245,"This paper proposes using reinforcement learning to solve scheduling problems where two types of resources of limited availability must be allocated. The goal is to minimize the makespan of a dual-resource constrained flexible job shop scheduling problem. Efficient practical implementation is very valuable to industry, yet it is often only solved combining heuristics and expert knowledge. A framework for training a reinforcement learning agent to schedule diverse dual-resource constrained job shops is presented. Comparison with other state-of-the-art approaches is done on both simpler and more complex instances that the ones used for training. Results show the agent produces competitive solutions for small instances that can outperform the implemented heuristic if given enough time. Other extensions are needed before real-world deployment, such as deadlines and constraining resources to work shifts.",industry
10.1016/j.ifacol.2020.12.2856,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,A deep learning unsupervised approach for fault diagnosis of household appliances,https://api.elsevier.com/content/abstract/scopus_id/85107800132,Fault detection and fault diagnosis are crucial subsystems to be integrated within the control architecture of modern industrial processes to ensure high quality standards. In this paper we present a two-stage unsupervised approach for fault detection and diagnosis in household appliances. In particular a suitable testing procedure has been implemented on a real industrial production line in order to extract the most meaningful features that allow to efficiently classify different types of fault by consecutively exploiting deep autoencoder neural network and k-means or hierarchical clustering techniques.,industry
10.1016/j.ifacol.2020.12.2855,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Fault prediction as a service in the smart factory: Addressing common challenges for an effective implementation,https://api.elsevier.com/content/abstract/scopus_id/85107753365,"Fault prediction in manufacturing systems has consistently been an important theme in engineering research. Data-driven methods to deliver this service are gaining momentum due to developments regarding information and communication technologies. Particularly, fault prediction may be interpreted as a supervised learning classification problem, in which algorithms trained by operational data gathered from the shop-floor are capable of informing managers whether a machine might enter in a failure state or not. Despite the relevance of this approach, implementations are hindered by several challenges. In this work, we review approaches aimed to deal with four of these challenges, namely: limited amount of training data, unbalanced training data sets, uncertainty regarding which variables should be monitored, and uncertainty regarding how exactly historical data should be employed in the algorithm’s training. To deal with training sets with limited size, learning procedures observed to perform well with a lower volume of training data can be used, such as the Random Forests technique. Alternatively, transfer learning techniques can be utilized to adapt models trained in a virtual domain with abundant synthetic data to the real manufacturing system domain. To deal with unbalance among classification classes, cost-sensitive learning methods can be employed to alter the penalties incurred when misclassifications occurs in the minority class. Alternatively, resampling methods can be applied before learning occurs. Lastly, both the decisions regarding which variables to track, and to what extent historical data should be included in the training process, can be addressed through the use of specific feature selection methods.",industry
10.1016/j.ifacol.2020.12.299,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Artificial intelligence platform proposal for paint structure quality prediction within the industry 4.0 concept,https://api.elsevier.com/content/abstract/scopus_id/85103128034,"This article provides an artificial intelligence platform proposal for paint structure quality prediction using Big Data analytics methodologies. The whole proposal fits into the current trends that are outlined in the Industry 4.0 concept. The painting process is very complex, producing huge volumes of data, but the main problem is that the data comes from different data sources, often heterogeneous, and it is necessary to propose a way to collect and integrate them into a common repository. The motivation for this work were the industry requirements to solve specific problems that cannot be solved by standard methods but require a sophisticated and holistic approach. It is the application of artificial intelligence that suggests a solution that is not otherwise visible, and the use of standard methods would not give any satisfactory results. The result is the design of an artificial intelligence platform that has been deployed in a real manufacturing process, and the initial results confirm the correctness and validity of this step. We also present a data collection and integration architecture, which is an integral part of every big data analytics solution, and a principal component analysis that was used to reduce the dimensionality of the large number of production process data.",industry
10.1016/j.matpr.2020.08.718,Conference Proceeding,Materials Today: Proceedings,scopus,2020-01-01,sciencedirect,Hybrid clustering algorithm for an efficient brain tumor segmentation,https://api.elsevier.com/content/abstract/scopus_id/85102451494,"This work describes the data mining methods, techniques and algorithms used for implementation. It is an emerging field of IT industry and research. There are many other fields such as Artificial Intelligence, Machine Learning, Deep Learning, Virtualization, Visualization, Parallel Computing and Image Processing. The human internal Brain can be seen or visualized by the Magnetic Resonance Imaging scan or Computerized Tomography scan. The MRI image is scanned and will be taken as input for processing. The MRI scan is more advantageous and more comfortable than CT scan for diagnosis. MRI scan provides detailed picture of organs. It does not affect the human health and body condition. It doesn't use any radiation. It is purely based on the magnetic field and radio waves. LIPC technique makes the training samples from the patients and arranges them into different group of classes used to construct different dictionaries. Image segmentation is a technique of dividing an image into different multiple portions, which is used to spot out objects and boundaries in images. There are many image segmentation techniques applicable for image processing. No acceptable method is available for solving all kinds of segmentation problem. Every method has merits and demerits. So, choosing good method is the challenging task. The hybrid clustering method is proposed in this work. The k-means algorithm and fuzzy c-means algorithm is proposed for brain tumor segmentation. The algorithm is implemented in synthetic and real time dataset. From the experimental results, this method provides better results in the form of accuracy.",industry
10.1016/j.procir.2020.03.134,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Reconstructing CNC platform for EDM machines towards smart manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85102047186,"CNC (computer numerical control) systems play an ultimately important role for controlling EDM (electrical discharge machining) machine tools and their machining processes. Till now, existing CNC systems do not offer sufficient openness that supports researchers and engineers to expend its capabilities and functionalities in response to the increasing demands of smart manufacturing; on the other hand, transforming an EDM machine made by small and medium-sized machine tool builders, into a smart manufacturing system has never been an easy job. To address the issues and overcome the difficulties which block the way towards smart manufacturing, this paper proposes an open architecture CNC platform for EDM machine tools. This platform utilizes the state-of-the-art technologies in implementation of the hardware and software without compromising with the constraints of obsolete techniques. For demonstrating the unique capabilities, the generalized unit arc length increment (GUALI) interpolation method and the Digitizer/Player system architecture are adopted. To exhibit the feasibilities of the newly developed platform, three kinds of EDM machine tools are applied associated with advanced functionalities such as machining process adaptive control, applications of machine learning, 6-axis EDM of shrouded turbine blisks etc. In addition, a small-scale smart manufacturing unit for drilling film cooling holes of turbine blades is built up into real production by applying the new CNC system and related software applications. From the practitioner’s viewpoint, openness and standardization are the keys that enable the people from academia and industry bringing in their domain knowledge to enrich the smart manufacturing ecosystem.",industry
10.1016/j.procir.2020.07.006,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Operator support in human-robot collaborative environments using AI enhanced wearable devices,https://api.elsevier.com/content/abstract/scopus_id/85100836551,"Nowadays, in order to cover the needs of market for product mass customization, industries have started to move to hybrid production cells, involving both robots and human operators. Research has been done during previous years to promote and improve the collaboration between humans and robots, trying to address topics such as safety, awareness and cognitive support in form of Augmented Reality based instructions. Results of previous research show bottlenecks related to the way of interaction of the operators with such supportive systems though. Direct interaction approach with the use of push buttons or indirect-gesture based interaction, which are most often adopted by the researchers, require operators to constantly occupy their hands performing the relevant button presses or gestures. Moreover, previous approaches are hardware dependent and need a lot of customization to work with different hardware. This work tries to address these bottlenecks proposing the usage of wearable devices enhanced with AI in order to support the interaction of human operators with robots in human-robot collaborative environments in a seamless and non-intrusive way, wrapped around a framework called “Operator Support Module” (OSM). Among others, OSM supports a variety of hardware to easily fit in various industrial scenarios. Two case studies will be presented to demonstrate the approach.",industry
10.1016/j.promfg.2020.11.012,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Application of machine learning and vision for real-time condition monitoring and acceleration of product development cycles,https://api.elsevier.com/content/abstract/scopus_id/85100766330,"Development work within an experimental environment, in which certain properties are investigated and optimized, requires many test runs and is therefore often associated with long execution times, costs and risks. This can affect product, material and technology development in industry and research. New digital driver technologies offer the possibility to automate complex manual work steps in a cost-effective way, to increase the relevance of the results and to accelerate the processes many times over. In this context, this article presents a low-cost, modular and open-source machine vision system for test execution and evaluates it on the basis of a real industrial application. For this purpose a methodology for the automated execution of the load intervals, the process documentation and for the evaluation of the generated data by means of machine learning to classify wear levels. The software and the mechanical structure are designed to be adaptable to different conditions, components and for a variety of tasks in industry and research. The mechanical structure is required for tracking the test object and represents a motion platform with independent positioning by machine vision operators or machine learning. An evaluation of the state of the test object is performed by the transfer learning after the initial documentation run. The manual procedure for classifying the visually recorded data on the state of the test object is described for the training material. This leads to an increased resource efficiency on the material as well as on the personnel side since on the one hand the significance of the tests performed is increased by the continuous documentation and on the other hand the responsible experts can be assigned time efficiently. The presence and know-how of the experts are therefore only required for defined and decisive events during the execution of the experiments. Furthermore, the generated data are suitable for later use as an additional source of data for predictive maintenance of the developed object.",industry
10.1016/j.procs.2020.10.091,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Towards smart manufucturing: Implementation and benefits,https://api.elsevier.com/content/abstract/scopus_id/85099879698,"Production activities are generating a large amount of data in different types (i.e., text, images), that is not well exploited. This data can be translated easily to knowledge that can help to predict all the risks that can impact the business, solve problems, promote efficiency of the manufacturing to the maximum, make the production more flexible and improving the quality of making smart decisions, however, implementing the Smart Manufacturing(SM) concept provides this opportunity supported by the new generation of the technologies. Internet Of Things (IoT) for more connectivity and getting data in real time, Big Data to store the huge volume of data and Deep Learning algorithms(DL) to learn from the historical and real time data to generate knowledge, that can be used, predict all the risks, problem solving, and better decision-making.
                  In this paper, we will introduce SM and the main technologies to success the implementation, the benefits, and the challenges.",industry
10.1016/j.promfg.2020.10.053,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Enabling real-time quality inspection in smart manufacturing through wearable smart devices and deep learning,https://api.elsevier.com/content/abstract/scopus_id/85099870958,"In this paper, we present a novel method for utilising wearable devices with Convolutional Neural Networks (CNN) trained on acoustic and accelerometer signals in smart manufacturing environments in order to provide real-time quality inspection during manual operations. We show through our framework how recorded or streamed sound and accelerometer data gathered from a wrist-attached device can classify certain user actions as successful or unsuccessful. The classification is designed with a Deep CNN model trained on Mel-frequency Cepstral Coefficients (MFCC) from the acoustic input signals. The wearable device provides feedback on three different modalities: audio, visual and haptic; thus ensuring the worker’s awareness at all time. We validate our findings through deployments of the complete AI-enabled device in production facilities of Mercedes-Benz AG. From the conducted experiments it is concluded that the use of acoustic and accelerometer data is valuable to train a classifier with the purpose of action examination during industrial assembly operations, and provides an intuitive interface for ensuring continued and improved quality inspection.",industry
10.1016/j.promfg.2020.10.126,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Simulation-as-a-service for reinforcement learning applications by example of heavy plate rolling processes,https://api.elsevier.com/content/abstract/scopus_id/85099821374,"In the production industry, the digital transformation enables a significant optimization potential. The concept of reinforcement learning offers a suitable approach to train agents on learning control strategies, further advancing automation. While applications training directly on real-world processes are rare due to economical and safety constraints, simulations offer a way to develop and evaluate agents prior to deployment. With the rise of service-based business models, the simulation owner and the machine learning expert are likely to be different stakeholders in a joint project. Due to different requirements for both simulations and reinforcement-learning agents, the stakeholders may be reluctant or unable to grant full access to the respective software. This poses a serious impediment to the potential of the digital transformation. In this paper, a distributed architecture is proposed, which allows the remote training of reinforcement learning agents on a simulation. It is shown that this architecture allows the cooperation between two stakeholders by exposing a suitable technical interface to the simulation. The proposed architecture is implemented for a simulation of the multi-step metal forming process of heavy plate rolling. Furthermore, the implemented architecture is used to successfully train a reinforcement-learning agent on the task of designing optimal parameter schedules.",industry
10.1016/j.promfg.2020.05.123,Conference Proceeding,,scopus,2020-01-01,sciencedirect,Integrated tool condition monitoring systems and their applications: A comprehensive review,https://api.elsevier.com/content/abstract/scopus_id/85095576577,"In conventional metal cutting, different tool wear modes, and their individual deterioration rates play vital roles in overall production performance. For a given tool (i.e., geometry or materials), many shop floors still follow a standard rule by pre-setting a tool life, which is conservative but not realistic. Premature failure of a tool can cause unexpected machine downtime and material losses, while another tool could serve beyond that pre-set life. As a result, optimized tool life and productivity cannot be achieved. Moreover, nowadays, there is an increased demand of process monitoring and optimization on the unmanned and the semi-automated shop floors.
                  Tool condition monitoring (TCM) systems for process improvement and optimization have been in research for several decades. Both offline and online TCM systems are invented and discussed. A wide range of original publications are reported focusing on different sub-topics, e.g., specific machining process-based TCM methods, measurement or signal acquisition methods, processing methods, and classifiers. With the recent evolution of smart sensors in the era of Industry 4.0, development of online TCM systems received much attention to the researchers. Accordingly, research on some sub-topics also gets motivated into different directions, such as, feasibility of power or current sensors, machine vision technique, and combination of multi-sensors. Thus, from the industrial viewpoint, the current state of implementation of the proposed TCM systems for (near) real-time process monitoring and control needs to be clear. This paper presents the state-of-the-art of the TCM systems covering three major machining operations, discusses their application feasibility in industry environments, and states some current TCMS implementations. Challenges being faced by the industry are concluded, along with direction and suggestions for future researches.",industry
10.1016/j.promfg.2020.05.140,Conference Proceeding,,scopus,2020-01-01,sciencedirect,Development of real-time diagnosis framework for angular misalignment of robot spot-welding system based on machine learning,https://api.elsevier.com/content/abstract/scopus_id/85095131276,"This paper focuses on the real-time online monitoring and diagnosis framework for the angular misalignment of the robot spot-welding system, which can result in significant quality degradation of a weld nugget such as porosity. The data-driven approach is applied by installing the voltage and current sensors, collecting the associated mass data and processing them under normal and abnormal (angular misalignment) conditions. Two categories of features are extracted from the dynamic resistance (DR) and the voltage and current ones that are decomposed by wavelet transform (WT). The DR features are extracted from the DR profile and some critical features are selected by a t-test methodology. In the case of the WT-based features, the critical ones are selected by a max-relevance and min-redundancy (mRMR) and a sequential backward selection (SBS) wrapper. Consequently, three types of critical feature sets, such as DR features, WT features, and hybrid features combining those, are prepared to train machine learning-based models. Support vector machine (SVM) and probabilistic neural network (PNN) are applied to establish the diagnosis models, and the diagnostic accuracy and robustness are evaluated. Finally, the software for the on-line monitoring and diagnosis for angular misalignment of robot spot-welding system is developed and demonstrates its real-time applicability in an industrial site.",industry
10.1016/j.promfg.2020.05.146,Conference Proceeding,,scopus,2020-01-01,sciencedirect,One-shot recognition of manufacturing defects in steel surfaces,https://api.elsevier.com/content/abstract/scopus_id/85095111982,"Quality control is an essential process in manufacturing to make the product defect-free as well as to meet customer needs. The automation of this process is important to maintain high quality along with the high manufacturing throughput. With recent developments in deep learning and computer vision technologies, it has become possible to detect various features from the images with near-human accuracy. However, many of these approaches are data intensive. Training and deployment of such a system on manufacturing floors may become expensive and time-consuming. The need for large amounts of training data is one of the limitations of the applicability of these approaches in real-world manufacturing systems. In this work, we propose the application of a Siamese convolutional neural network to do one-shot recognition for such a task. Our results demonstrate how one-shot learning can be used in quality control of steel by identification of defects on the steel surface. This method can significantly reduce the requirements of training data and can also be run in real-time.",industry
10.1016/j.procir.2020.04.158,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Image processing based on deep neural networks for detecting quality problems in paper bag production,https://api.elsevier.com/content/abstract/scopus_id/85092428222,"It is critical for manufacturers to identify quality issues in production and prevent defective products being delivered to customers. We investigate the use of deep neural networks to perform automatic quality inspections based on image processing to eliminate the current manual inspection. A deep neural network was implemented in a real-world industrial case study, and its ability to detect quality problems was evaluated and analyzed. The results show that the network has an accuracy of 94.5%, which is considered good in comparison to the 70–80% accuracy of a trained human inspector.",industry
10.1016/j.procir.2020.04.135,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Application of Artificial Intelligence to an Electrical Rewinding Factory Shop,https://api.elsevier.com/content/abstract/scopus_id/85091693237,"The evolution of artificial intelligence (AI) and big data resulted in the full potential realization of technologies through convergence. Tremendous acceptance, adoption and implementation of the United Nations Sustainable Development Goals (SDG) Agenda 2030, has resulted in original equipment manufacturers (OEM) developing various designs of rotary machines in a bid to improve energy efficiency, with more improvements expected in the coming decade. An effective technique to manage energy efficiency in the smart grid is through integration of demand side management, inclusive of optimization of rewinding of an electric motor in a machine shop. This paper aims to conceptualize application of AI and augmented reality (AR) towards process visibility of remanufacturing rotary machine stators by robotic vision. SLT is the triangulation methodology used in laser scanning for 3D modelling, and instantaneous condition assessment of the core. A pre-defined robotic path is used towards identification of features for range image acquisition. Therefore, the potential of industry 4.0 in resuscitation of end-of-life products through service remanufacturers by RE in a rewinding shop are presented.",industry
10.1016/j.procs.2020.05.068,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Adoption of the conceive-design-implement-operate approach to the third year project in a team-based design-build environment,https://api.elsevier.com/content/abstract/scopus_id/85089026312,"The high-quality engineering education is one of the challenges in the 21stcentury, where the teaching-learning process will be enhanced by integrating and involving the students in the teaching process and the course will be delivered in an interesting and engaging way. So in the process of reforming the engineering education, the Department of Mechanical Engineering of RIT, Rajaramnagar has initiated Conceive, Design, Implement and Operate (CDIO) approach to produce the 21st-century engineers. CDIO approach caters an engineering education that stresses fundamentals set in the context of real-world systems and products. It provides a universal structure for a strong engineering education integrating an entire set of graduate attributes. Developing undergraduate students into successful engineers requires integration of technical knowledge and soft skills. As a part of CDIO implementation, the mechanical program has been modified to provide integrated team-based project learning. The paper presents the implementation methodology of the concept with an emphasis on the Third year design projects. As per the CDIO standards and syllabus, the program has been modified and the required facilities were made available in the college campus such as Workspaces and laboratories. The structured research-driven approach is provided to monitor and review the implementation of the CDIO principles and standards. It is observed that CDIO provides integrated learning to develop deep learning of technical knowledge whilst simultaneously develops personal, interpersonal, process, product, and system developing skills. The primary outcome of the CDIO is the students are exposed to the work environment used in the industry for the product and process development and the secondary outcome is that it provides a useful tool for devolvement and assessment of the skill set of the students. Finally, the effect of CDIO initiative for project enrichment is discussed and future plan is presented.",industry
10.1016/j.procs.2020.04.199,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Perspective Vehicle License Plate Transformation using Deep Neural Network on Genesis of CPNet,https://api.elsevier.com/content/abstract/scopus_id/85086630682,"Recent development in vehicular industries and increased number of cars in modern society leads the people to pay more attention on Vehicle License Plate Recognition (V-LPR). V-LPR plays a major role in traffic related application such as road traffic monitoring, vehicle parking lots access control etc. Existing state of the art V-LPR systems in real world deployment works under restricted conditions, such as static illumination, fixed background etc. Most of them fails to work when any of the above given conditions are violated. Hence to address this issue, a novel V-LPR system is designed using modern deep learning framework called ""Capsule Network"". The proposed system is robust and works fine in any condition. Further, the proposed method aims to improve the processing time by integrating the segmentation process within the CN framework which involves the training and recognizing of entire license plate cropped region. Moreover, the feature extraction is performed by CN framework over a segmented alphanumeric character. Finally, Data augmentation technique is also used as a supplement to the CN framework to strengthen the process of training with various orientations like rotation, shift and flip for improving the recognition task.",industry
10.1016/j.procs.2020.03.027,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Strategic zoning approach for urban areas: Towards a shared transportation system,https://api.elsevier.com/content/abstract/scopus_id/85085571988,"Investigating downstream freight demand is a prerequisite to accomplishing the overall strategic implementation of transportation systems. Machine learning has recently become widely applied in order to support decision-making in several logistic operational levels: travel/arrival time prediction, occupancy forecasting of logistic spaces, route optimization and so on. Nevertheless, strategic decision-making often overlooks flow tendencies forecasting. Targeting this perspective, the present paper aims at proposing an urban zoning approach based on time series forecasting of supply chain demand through clustering customers. To conduct our approach, we have selected a set of machine learning algorithms that are believed to be robust according to the literature and the achieved accuracy benchmarks. Considering real-life data-based computational results, a number of analytical insights are illustrated.",industry
10.1016/j.procs.2020.03.044,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,An artificial intelligence based crowdsensing solution for on-demand accident scene monitoring,https://api.elsevier.com/content/abstract/scopus_id/85085566175,"Road traffic crashes have a devastating impact on societies by claiming more than 1.35 million lives each year and causing up to 50 million injuries. Improving the efficiency of emergency management systems constitutes a key measure to reduce road traffic deaths and injuries. In this work, we propose a comprehensive crowdsensing-based solution for the real-time collection and the analysis of accident scene intelligence as a means to improve the efficiency of the emergency response process and help reduce road fatalities. The solution leverages sensory, mobile, and web technologies for the real-time monitoring of accident scenes, and employs Artificial Intelligence for the automatic analysis of the accident scene data, to allow the automatic generation of accident intelligence reports. Police officers and rescue teams can use those reports for fast and accurate situational assessment and effective response to emergencies. The proposed system was fully implemented and its operation was successfully tested using a variety of scenarios. This work gives interesting insights into the possibility of leveraging crowdsensing and artificial intelligence for offering emergency situational awareness and improving the efficiency of emergency response operations.",industry
10.1016/j.procs.2020.03.004,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Activity Recognition in Smart Homes using UWB Radars,https://api.elsevier.com/content/abstract/scopus_id/85085563629,"In the last decade, smart homes have transitioned from a potential solution for aging-in-place to a real set of technologies being deployed in the real-world. This technological transfer has been mostly supported by simple, commercially available sensors such as passive infrared and electromagnetic contacts. On the other hand, many teams of research claim that the sensing capabilities are still too low to offer accurate, robust health-related monitoring and services. In this paper, we investigate the possibility of using Ultra-wideband (UWB) Doppler radars for the purpose of recognizing the ongoing ADLs in smart homes. Our team found out that with simple configuration and classical features engineering, a small set of UWB radars could reasonably be used to recognize ADLs in a realistic home environment. A dataset was built from 10 persons performing 15 different ADLs in a 40 square meters apartment with movement on the other side of the wall. Random Forest was able to attain 80% accuracy with an F1-Score of 79%, and a Kappa of 77%. Those results indicate the use of Doppler radars can be a good research avenue for smart homes.",industry
10.1016/j.procs.2020.03.036,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Air Quality Forecasting using LSTM RNN and Wireless Sensor Networks,https://api.elsevier.com/content/abstract/scopus_id/85085553433,"In the past few decades, many urban areas around the world have suffered from severe air pollution and the health hazards that come with it, making gathering real-time air quality and air quality forecasting very important to take preventive and corrective measures. This paper proposes a scalable architecture to monitor and gather real-time air pollutant concentration data from various places and to use this data to forecast future air pollutant concentrations. Two sources are used to collect air quality data. The first being a wireless sensor network that gathers and sends pollutant concentrations to a server, with its sensor nodes deployed in various locations in Bengaluru city in South India. The second source is the real-time air quality data gathered and made available by the Government of India as a part of its Open Data initiative. Both sources provide average concentrations of various air pollutants on an hourly basis. Due to its proven track record of success with time-series data, a Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) model was chosen to perform the task of air quality forecasting. This paper critically analyses the performance of the model in two regions that exhibit a significant difference in temporal variations in air quality. As these variations increase, the model suffers performance degradation necessitating adaptive modelling.",industry
10.1016/j.promfg.2020.04.017,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,PPE compliance detection using artificial intelligence in learning factories,https://api.elsevier.com/content/abstract/scopus_id/85085527469,"This project demonstrates the application of Artificial Intelligence (AI) and machine vision for the identification of Personal Protective Equipment (PPE), particularly safety glasses in zones of the Learning Factory, where safety risks exist. The objective is to design and implement an automated system for ensuring the safety of personnel when they are in the vicinity of machinery that presents potential risks to the eyes. Microsoft Azure Custom Vision AI and Intelligent AI Services, in conjunction with low-cost vision devices with lightweight onboard AI capability, provide a platform for a deep learning neural network model using publicly available images under the Creative Commons License. A combination of cloud-based and on-premises AI is used in this proof of concept system to provide a real-time vision-based safety system capable of detecting and recording potential safety breaches, promoting compliance, and ultimately preventing accidents before they happen. This system can be used to initiate different control actions in the event of safety violations and can detect multiple forms of protective wear. The flexibility of the system offers multiple benefits to learning factories and manufacturing organizations such as improved user safety, reduced insurance costs, and better detection and recording of safety violations. The hybrid AI architecture approach allows for flexibility in training and deployment based on the capability of local computing resources.",industry
10.1016/j.promfg.2020.04.082,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Integrating virtual and physical production processes in learning factories,https://api.elsevier.com/content/abstract/scopus_id/85085519100,"Scaled learning factories are industrial learning environments that provide production systems and processes for learners on a model scale rather than using actual productive machines. This approach has benefits as for instance lower invest, increased approachability and higher safety levels. At the same time, constraints for implementation of actual production processes and required abstraction levels from industrial processes are limitations. To bridge the gap between benefits and limitation we propose the integration of virtual production processes in a prevalent physical learning factories. Resulting mixed reality solutions bear the potential to combine real and virtual objects at the same time and thus extend the physical model environment with virtually represented processes. Based on an initial analysis we develop a concept using spatial augmented reality and a game engine based simulation to realize a virtual integrated production process. The theoretical concept as well as the technical implementation is described. A first evaluation indicates a high rate of acceptance by trainees and illustrates the benefits for learning performance.",industry
10.1016/j.promfg.2020.04.055,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,From digital shop floor to real-time reporting: An IIoT based educational use case,https://api.elsevier.com/content/abstract/scopus_id/85085498833,"The Smart Production Lab (Lab)at the FH JOANNEUM in Kapfenberg, Austria, is a digital learning and research factory with an interdisciplinary focus on vertical and horizontal IT-integration. It is aiming at a higher transparency and productivity by applying latest digital technologies. The key technology is the Industrial Internet of Things (IIoT). Therefore, research driven IoT use cases are further developed such as hybrid IoT-concepts and architectures involving edge and cloud computing. State-of-the-Art use cases apply of-the-shelf technologies for ready-to-use implementations and teaching purposes. This paper introduces a case-based teaching concept in the area of IIoT. It provides students with a hands-on experience as well as deep insights in what is meant by modelling and implementing an IoT data flow from the shop floor to real-time reporting. For this purpose, on the operational technology (OT) layer IoT nodes were attached to the machinery in the Lab gathering and providing data for the IoT middleware layer, based on Open Platform Communication Unified Architecture (OPC UA). This central middleware-layer is represented by the open source platform Node-RED. In the respective use case the data is transformed in order to be stored in a NoSQL database, from where it can be accessed for real-time reporting either by cloud or on premise applications. The interdisciplinary nature of these use case consists of integrating the different aspects of a digital production, involving disciplines such as automation, digital retrofitting, operational technology, and informational technology. Thus, it provides students with a comprehensive understanding of the benefits and limitations of IIoT.",industry
10.1016/j.promfg.2020.04.037,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Implementing AR/MR - Learning factories as protected learning space to rise the acceptance for mixed and augmented reality devices in production,https://api.elsevier.com/content/abstract/scopus_id/85085498037,"When talking about digitization, changes in the way of working are inevitable: The implementation of intelligent machines or dealing with real-time data lead to new tasks supported by new technology. Also digital technologies such as Augmented and Mixed Reality (AR/MR) are pushing the market and setting new standards in collaboration, prototyping or maintenance. The correct handling of AR/MR devices requires a change in the employees’ behavior; changing working routines are followed by a new skill set and a change in the culture. The acceptance of employees can therefore be regarded as a critical success factor for the implementation of such technologies. Thus, the present paper answers the research question ‘what factors influence the employee’s acceptance of AR and MR data glasses in industry’. On the basis of a comprehensive literature analysis, an implementation workshop was developed and validated in cooperation with an industrial partner. The results were transformed into a workshop within the learning and research factory ‘Smart Production Lab’ to give employees and students the opportunity to train the handling of data glasses in a protected learning space in order to increase the acceptance for the technology.",industry
10.1016/j.promfg.2020.04.066,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,5G and AI technology application in the AMTC learning factory,https://api.elsevier.com/content/abstract/scopus_id/85085489730,"5G and AI (Artificial Intelligence) are changing industrial production and offer great potential for manufacturing enterprises. One of the effects resulting from the increasing quantity of production data is the increasing demands of transmission of large amounts of data, fast transmission speed, and rapid data analysis. However, merely relying on traditional communication technology and manual data processing does not lead to high transmission performance and low analysis time. It is essential to integrate 5G and AI technology to flexibly transmit large amounts of data and real-time data. To demonstrate the feasibility and potential of these two technologies, a concept was developed at the Advanced Manufacturing Technology Center (AMTC) at the Tongji University (Shanghai, China) and further implemented in the AMTC learning factory in cooperation with wbk of Karlsruhe Institute of Technology (Karlsruhe, Germany) and Ruhr-University Bochum (Bochum, Germany). This paper presents the learning factory design in detail, describing the concept design, training environment and training phases in the AMTC learning factory. It is followed by a case study consisting of specific examples of 5G and AI, implemented in the AMTC learning factory. The importance of integrated 5G and AI applications is pointed out and discussed.",industry
10.1016/j.promfg.2020.04.038,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Seamless data integration in the CAM-NC process chain in a learning factory,https://api.elsevier.com/content/abstract/scopus_id/85085473050,"The seamless data integration of different components in the CAM-NC process chain (tool management software, tool dispensing system, presetting machine and machine tool) is essential for maximizing the efficiency and minimizing the total error rate. This is done by entering the data into the system of the network only once. Then this data is available for all participants in this network at any time. This paper describes the aforementioned integration by using the example of creating a digital tool, which is used in a CAM simulation afterwards. Then the real set-up and machining process is discussed. The process chain explained in this paper was implemented at the smartfactory@tugraz - the Learning Factory at Graz University of Technology.",industry
10.1016/j.softx.2020.100419,Journal,SoftwareX,scopus,2020-01-01,sciencedirect,TWINKLE: A digital-twin-building kernel for real-time computer-aided engineering,https://api.elsevier.com/content/abstract/scopus_id/85079158568,"TWINKLE is a library for building families of solvers to perform Canonical Polyadic Decomposition (CPD) of tensors. The common characteristic of these solvers is that the data structure supporting the tuneable solution strategy is based on a Galerkin projection of the phase space. This allows processing and recovering tensors described by highly sparse and unstructured data. For achieving high performance, TWINKLE is written in C++ and uses the Armadillo open source library for linear algebra and scientific computing, based on LAPACK (Linear Algebra PACKage) and BLAS (Basic Linear Algebra Subprograms) routines. The library has been implemented keeping in mind its future extensibility and adaptability to fulfil the different users’ needs in academia and industry regarding Reduced Order Modelling (ROM) and data analysis by means of tensor decomposition. It is especially focused on post-processing data from Computer-Aided-Engineering (CAE) simulation tools.",industry
10.1016/j.aei.2020.101044,Journal,Advanced Engineering Informatics,scopus,2020-01-01,sciencedirect,IoT edge computing-enabled collaborative tracking system for manufacturing resources in industrial park,https://api.elsevier.com/content/abstract/scopus_id/85078852726,"In manufacturing industry, the movement of manufacturing resources in production logistics often affects the overall efficiency. This research is motivated by a world-leading air-conditioner manufacturer. In order to provide the right manufacturing resources for subsequent production steps, excessive time and human effort has been consumed in locating the manufacturing resources in a huge industrial park. The development of Internet of Things (IoT) has made a profound impact on establish smart manufacturing workshop and tracking applications, however a growing trend of data quantity that generated from massive, heterogeneous and bottomed manufacturing resources objects pose challenge to centralized decision. In this study, the concept of edge-computing deeply integrated in collaborative tracking purpose in virtue of IoT technology. An IoT edge computing enabled collaborative tracking architecture is developed to offload the computation pressure and realize distributed decision making. A supervised learning of genetic tracking method is innovatively presented to ensure tracking accuracy and effectiveness. Finally, the research output is developed and implemented in a real-life industrial park for verification. The results show that the proposed tracking method not only performs constant improving accuracy up to 96.14% after learning compared to other tracking method, but also ensure quick responsiveness and scalability.",industry
10.1016/j.aei.2020.101037,Journal,Advanced Engineering Informatics,scopus,2020-01-01,sciencedirect,A smart surface inspection system using faster R-CNN in cloud-edge computing environment,https://api.elsevier.com/content/abstract/scopus_id/85078666892,"Automated surface inspection has become a hot topic with the rapid development of machine vision technologies. Traditional machine vision methods need experts to carefully craft image features for defect detection. This limits their applications to wider areas. The emerging convolutional neural networks (CNN) can automatically extract features and yield good results in many cases. However, the CNN-based image classification methods are more suitable for flat surface texture inspection. It is difficult to accurately locate small defects in geometrically complex products. Furthermore, the computational power required in CNN algorithms is usually high and it is not efficient to be implemented on embedded hardware. To solve these problems, a smart surface inspection system is proposed using faster R-CNN algorithm in the cloud-edge computing environment. The faster R-CNN as a CNN-based object detection method can efficiently identify defects in complex product images and the cloud-edge computing framework can provide fast computation speed and evolving algorithm models. A real industrial case study is presented to illustrate the effectiveness of the proposed method. The results show that the proposed method can provide high detection accuracy within a short time.",industry
10.1016/j.aei.2019.101013,Journal,Advanced Engineering Informatics,scopus,2020-01-01,sciencedirect,Guidelines for applied machine learning in construction industry—A case of profit margins estimation,https://api.elsevier.com/content/abstract/scopus_id/85075778987,"The progress in the field of Machine Learning (ML) has enabled the automation of tasks that were considered impossible to program until recently. These advancements today have incited firms to seek intelligent solutions as part of their enterprise software stack. Even governments across the globe are motivating firms through policies to tape into ML arena as it promises opportunities for growth, productivity and efficiency. In reflex, many firms embark on ML without knowing what it entails. The outcomes so far are not as expected because the ML, as hyped by tech firms, is not the silver bullet. However, whatever ML offers, firms urge to capitalise it for their competitive advantage. Applying ML to real-life construction industry problems goes beyond just prototyping predictive models. It entails intensive activities which, in addition to training robust ML models, provides a comprehensive framework for answering questions asked by construction folks when intelligent solutions are getting deployed at their premises to substitute or facilitate their decision-making tasks. Existing ML guidelines used in the IT industry are vastly restricted to training ML models. This paper presents guidelines for Applied Machine Learning (AML) in the construction industry from training to operationalising models, which are drawn from our experience of working with construction folks to deliver Construction Simulation Tool (CST). The unique aspect of these guidelines lies not only in providing a novel framework for training models but also answering critical questions related to model confidence, trust, interpretability, bias, feature importance and model extrapolation capabilities. Generally, ML models are presumed black boxes; hence argued that nobody knows what a model learns and how it generates predictions. Even very few ML folks barely know approaches to answer questions asked by the end users. Without explaining the competence of ML, the broader adoption of intelligent solutions in the construction industry cannot be attained. This paper proposed a detailed process for AML to develop intelligent solutions in the construction industry. Most discussions in the study are elaborated in the context of profit margin estimation for new projects.",industry
10.1016/bs.adcom.2019.09.005,Book Series,Advances in Computers,scopus,2020-01-01,sciencedirect,Impact of cloud security in digital twin,https://api.elsevier.com/content/abstract/scopus_id/85073737509,"Digital Twin is a way to virtually represent or model a physical object using the real time data. This innovation sets up a way to deal with industries and organizations to supervise their products, consequently bridging the gap between design and implementations. As the name suggests, “Digital Twin” infers that a reproduction of the product is made in order to have a nearby relationship with the live item. The procedure of computerized twin begins by gathering real time data, processed data, and operational data and performs distinctive investigation which helps in anticipating the future. This additionally enhances the customer experiences by giving a digital feel of their product. The objective behind all these is the job of gathering information and putting them in a place, i.e., the cloud which could store exorbitant data. The user experience gets enhanced by the intervention of digital twin technology which could help in the successful working of the products geographically distributed. The impact of Internet of Things and Cloud Computing lifts up the digital twin.
                  The information gathered from the sources can be arranged in terms of utilization and prospect to change on a timely basis. These data, as they are stored require proper coordination and a legitimate use.
                  Digital Twin innovation assumes incredible opportunities in the field of manufacturing, healthcare, smart cities, automobile and so on. The effect of having a digital twin for the product makes it simple for activities and recognize the blemishes, if any happened. This approach can help reduce the workload and furthermore can get trained on the virtual machine without the need of a specific training.
                  With the most prevailing technologies of today, like Artificial Intelligence, Machine Learning and Internet of Things more prominent approach to train and monitor products, taking care of its own execution, collaborating to different frameworks, performing self-repairs are made possible. Hence the future is getting unfolded with the emerging DIGITAL TWIN era. The massive data utilized in the field of digital twin is prone to severe security breaches. Thus digital twin technology should be handled with extreme care so as to protect the data. Hence, this chapter identifies the ways and means of collecting, organizing and storing the data in a secured cloud environment. The data is filtered according to the use and priority and pushed into the cloud. It is determined to implement an exclusive algorithm for a secured cloud which would greatly benefit the users and the providers to handle and process it effectively.",industry
10.1016/j.vehcom.2019.100198,Journal,Vehicular Communications,scopus,2020-01-01,sciencedirect,In-vehicle network intrusion detection using deep convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85073150001,"The implementation of electronics in modern vehicles has resulted in an increase in attacks targeting in-vehicle networks; thus, attack detection models have caught the attention of the automotive industry and its researchers. Vehicle network security is an urgent and significant problem because the malfunctioning of vehicles can directly affect human and road safety. The controller area network (CAN), which is used as a de facto standard for in-vehicle networks, does not have sufficient security features, such as message encryption and sender authentication, to protect the network from cyber-attacks. In this paper, we propose an intrusion detection system (IDS) based on a deep convolutional neural network (DCNN) to protect the CAN bus of the vehicle. The DCNN learns the network traffic patterns and detects malicious traffic without hand-designed features. We designed the DCNN model, which was optimized for the data traffic of the CAN bus, to achieve high detection performance while reducing the unnecessary complexity in the architecture of the Inception-ResNet model. We performed an experimental study using the datasets we built with a real vehicle to evaluate our detection system. The experimental results demonstrate that the proposed IDS has significantly low false negative rates and error rates when compared to the conventional machine-learning algorithms.",industry
10.1016/j.eng.2019.02.013,Journal,Engineering,scopus,2019-12-01,sciencedirect,Artificial Intelligence in Steam Cracking Modeling: A Deep Learning Algorithm for Detailed Effluent Prediction,https://api.elsevier.com/content/abstract/scopus_id/85074530083,"Chemical processes can benefit tremendously from fast and accurate effluent composition prediction for plant design, control, and optimization. The Industry 4.0 revolution claims that by introducing machine learning into these fields, substantial economic and environmental gains can be achieved. The bottleneck for high-frequency optimization and process control is often the time necessary to perform the required detailed analyses of, for example, feed and product. To resolve these issues, a framework of four deep learning artificial neural networks (DL ANNs) has been developed for the largest chemicals production process—steam cracking. The proposed methodology allows both a detailed characterization of a naphtha feedstock and a detailed composition of the steam cracker effluent to be determined, based on a limited number of commercial naphtha indices and rapidly accessible process characteristics. The detailed characterization of a naphtha is predicted from three points on the boiling curve and paraffins, iso-paraffins, olefins, naphthenes, and aronatics (PIONA) characterization. If unavailable, the boiling points are also estimated. Even with estimated boiling points, the developed DL ANN outperforms several established methods such as maximization of Shannon entropy and traditional ANNs. For feedstock reconstruction, a mean absolute error (MAE) of 0.3 wt% is achieved on the test set, while the MAE of the effluent prediction is 0.1 wt%. When combining all networks—using the output of the previous as input to the next—the effluent MAE increases to 0.19 wt%. In addition to the high accuracy of the networks, a major benefit is the negligible computational cost required to obtain the predictions. On a standard Intel i7 processor, predictions are made in the order of milliseconds. Commercial software such as COILSIM1D performs slightly better in terms of accuracy, but the required central processing unit time per reaction is in the order of seconds. This tremendous speed-up and minimal accuracy loss make the presented framework highly suitable for the continuous monitoring of difficult-to-access process parameters and for the envisioned, high-frequency real-time optimization (RTO) strategy or process control. Nevertheless, the lack of a fundamental basis implies that fundamental understanding is almost completely lost, which is not always well-accepted by the engineering community. In addition, the performance of the developed networks drops significantly for naphthas that are highly dissimilar to those in the training set.",industry
10.1016/j.jmapro.2019.10.020,Journal,Journal of Manufacturing Processes,scopus,2019-12-01,sciencedirect,Data-driven smart manufacturing: Tool wear monitoring with audio signals and machine learning,https://api.elsevier.com/content/abstract/scopus_id/85074281429,"Tool wear in machining could result in poor surface finish, excessive vibration and energy consumption. Monitoring tool wear in real-time is crucial to improve manufacturing productivity and quality. While numerous sensor-based tool wear monitoring techniques have been demonstrated in laboratory environments, few tool wear monitoring systems have been deployed in factories because it is not realistic to install some of the important sensors such as dynamometers on manufacturing machines. To address this issue, a novel audio signal processing approach is introduced. This technique does not require expensive sensors but audio sensors only. A blind source separation method is used to separate source signals from noise. An extended principal component analysis is used for dimensionality reduction. Real-time multi-channel audio signals are collected during a set of milling tests under varying cutting conditions. The experimental data are used to develop and validate a predictive model. Experimental results have shown that the predictive model is capable of classifying tool wear conditions with high accuracy.",industry
10.1016/j.petrol.2019.106332,Journal,Journal of Petroleum Science and Engineering,scopus,2019-12-01,sciencedirect,Machine learning methods applied to drilling rate of penetration prediction and optimization - A review,https://api.elsevier.com/content/abstract/scopus_id/85070879413,"Drilling wells in challenging oil/gas environments implies in large capital expenditure on wellbore's construction. In order to optimize the drilling related operation, real-time decisions making have been put in place, so that prediction of rate of penetration (ROP) with accuracy is essential. Despite many efforts (theoretical and experimental) throughout the years, modeling the ROP as a mathematical function of some key variables is not so trivial, due to the highly non-linearity behavior experienced. Therefore, several researches in the recent years have been proposing to use data-driven models from artificial intelligence field for ROP prediction and optimization.
                  This paper presents an extensive review of the literature on ROP prediction, especially, with machine learning techniques, as well as how these models can be used to optimize the drilling activities. The ROP models are classified as traditional models (based on physics-models), statistical models (e.g. multiple regression), or machine learning methods. This review enables to see that machine learning techniques can potentially outperform in terms of ROP-prediction accuracy on top of traditional or statistical models. Throughout this work, an extensive analysis of different ways of obtaining ROP models is carried out, concluding with different strategies adopted in literature to perform data-driven model optimization.
                  Despite the saving potential which can be achieved with real-time optimization based on data-driven ROP models, it is noticeable that there is a lack of implementation of those techniques in the industry, as per literature review. To take a step forward in real implementations, the petroleum industry must be aware that yet no rule of thumb already exists on this specific area, but still, good and very reasonable results can be achieved by following the best practices identified in this review. In addition, the modern practices of machine learning provide promising guidelines for implementing projects in oil and gas industry.",industry
10.1016/j.sigpro.2019.06.019,Journal,Signal Processing,scopus,2019-12-01,sciencedirect,A generic parallel computational framework of lifting wavelet transform for online engineering surface filtration,https://api.elsevier.com/content/abstract/scopus_id/85068175044,"Nowadays, complex geometrical surface texture measurement and evaluation require advanced filtration techniques. Discrete wavelet transform (DWT), especially the second-generation wavelet (Lifting Wavelet Transform – LWT), is the most adopted one due to its unified and abundant characteristics in measured data processing, geometrical feature extraction, manufacturing process planning, and production monitoring. However, when dealing with varied complex functional surfaces, the computational payload for performing DWT in real-time often becomes a core bottleneck in the context of massive measured data and limited computational capacities. It is a more prominent problem for the areal surface texture filtration by using 2D DWT. To address the issue, this paper presents a generic parallel computational framework for lifting wavelet transform (GPCF-LWT) based on Graphics Process Unit (GPU) clusters and the Compute Unified Device Architecture (CUDA). Due to its cost-effective hardware design and the powerful parallel computing capacity, the proposed framework can support online (or near real-time) engineering surface filtration for micro- and nano-scale surface metrology through exploring a novel parallel method named LBB model, the improved algorithms of lifting scheme and three implementation optimizations on the heterogeneous multi-GPU systems. The innovative approach enables optimizations on individual GPU node through an overarching framework that is capable of data-oriented dynamic load balancing (DLB) driven by a fuzzy neural network (FNN). The paper concludes with a case study on filtering and extracting manufactured surface topographical characteristics from real surfaces. The experimental results have demonstrated substantial improvements on the GPCF-LWT implementation in terms of computational efficiency, operational robustness, and task generalization.",industry
10.1016/j.rcim.2019.05.008,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2019-12-01,sciencedirect,A real-time human-robot interaction framework with robust background invariant hand gesture detection,https://api.elsevier.com/content/abstract/scopus_id/85066259834,"In the light of factories of the future, to ensure productive and safe interaction between robot and human coworkers, it is imperative that the robot extracts the essential information of the coworker. We address this by designing a reliable framework for real-time safe human-robot collaboration, using static hand gestures and 3D skeleton extraction. OpenPose library is integrated with Microsoft Kinect V2, to obtain a 3D estimation of the human skeleton. With the help of 10 volunteers, we recorded an image dataset of alpha-numeric static hand gestures, taken from the American Sign Language. We named our dataset OpenSign and released it to the community for benchmarking. Inception V3 convolutional neural network is adapted and trained to detect the hand gestures. To augment the data for training the hand gesture detector, we use OpenPose to localize the hands in the dataset images and segment the backgrounds of hand images, by exploiting the Kinect V2 depth map. Then, the backgrounds are substituted with random patterns and indoor architecture templates. Fine-tuning of Inception V3 is performed in three phases, to achieve validation accuracy of 99.1% and test accuracy of 98.9%. An asynchronous integration of image acquisition and hand gesture detection is performed to ensure real-time detection of hand gestures. Finally, the proposed framework is integrated in our physical human-robot interaction library OpenPHRI. This integration complements OpenPHRI by providing successful implementation of the ISO/TS 15066 safety standards for “safety rated monitored stop” and “speed and separation monitoring” collaborative modes. We validate the performance of the proposed framework through a complete teaching by demonstration experiment with a robotic manipulator.",industry
10.1016/j.isatra.2018.12.025,Journal,ISA Transactions,scopus,2019-12-01,sciencedirect,Deep residual learning-based fault diagnosis method for rotating machinery,https://api.elsevier.com/content/abstract/scopus_id/85059116434,"Effective fault diagnosis of rotating machinery has always been an important issue in real industries. In the recent years, data-driven fault diagnosis methods such as neural networks have been receiving increasing attention due to their great merits of high diagnosis accuracy and easy implementation. However, it is mostly difficult to fully train a deep neural network since gradients in optimization may vanish or explode during back-propagation, which results in deterioration and noticeable variance in model performance. In fault diagnosis researches, larger data sequence of machinery vibration signal containing sufficient information is usually preferred and consequently, deep models with large capacity are generally adopted. In order to improve network training, a residual learning algorithm is proposed in this paper. The proposed architecture significantly improves the information flow throughout the network, which is well suited for processing machinery vibration signal with variable sequential length. Little prior expertise on fault diagnosis and signal processing is required, that facilitates industrial applications of the proposed method. Experiments on a popular rolling bearing dataset are implemented to validate the proposed method. The results of this study suggest that the proposed intelligent fault diagnosis method for rotating machinery offers a new and promising approach.",industry
10.1016/j.inffus.2018.11.020,Journal,Information Fusion,scopus,2019-12-01,sciencedirect,Data fusion based coverage optimization in heterogeneous sensor networks: A survey,https://api.elsevier.com/content/abstract/scopus_id/85059069923,"Sensor networks, as a promising network paradigm, have been widely applied in a great deal of critical real-world applications. A key challenge in sensor networks is how to improve and optimize coverage quality which is a fundamental metric to characterize how well a point or a region or a barrier can be sensed by the geographically deployed heterogeneous sensors. Because of the resource-limited, battery-powered and type-diverse features of the sensors, maintaining and optimizing coverage quality includes a significant amount of challenges in heterogeneous sensor networks. Many researchers from both academic and industrial communities have performed numerous significant works on coverage optimization problem in the past decades. Some of them also have surveyed the current models, theories and solutions on the problem of coverage optimization. However, most of the existing surveys and analytical studies ignore how to exploit data fusion and cooperation of the deployed sensors to enhance coverage performance. In this paper, we provide an insightful and comprehensive summarization and classification on the data fusion based coverage optimization problem and techniques. Aiming at overcoming the shortcomings existed in current solutions, we also discuss the future issues and challenges in this area and sketch a general research framework in the context of reinforcement learning.",industry
10.1016/j.eswa.2019.05.052,Journal,Expert Systems with Applications,scopus,2019-11-30,sciencedirect,Unsupervised collective-based framework for dynamic retraining of supervised real-time spam tweets detection model,https://api.elsevier.com/content/abstract/scopus_id/85067174995,"Twitter is one of the most popular social platforms. It has changed the way of communication and information dissemination through its real-time messaging mechanism. Recently, it has been used by researchers and industries as a new source of data for various intelligent systems, such as tweet sentiment analysis and recommendation systems, which require high data quality. However, due to its flexibility and popularity, Twitter has become the main target for spamming activities such as phishing legitimate users or spreading malicious software, which introduces new security issues and waste resources. Therefore, researchers have developed various machine-learning algorithms to reveal Twitter spam. However, as spammers have become smarter and more crafty, the characteristics of the spam tweets are varying over time making these methods inefficient to detect new spammers tricks and strategies. In addition, some of the employed methods (e.g. blacklisting) or spammer features (e.g. graph-based features) are extremely time-consuming, which hinders the ability to detect spammer activities in real-time. In this paper, we introduce a framework to deal with the volatility of the spam contents and new spamming patterns, called the spam drift. The framework combines the strength of unsupervised machine learning approach, which learns from unlabeled tweets, to retrain a real-time supervised tweet-level spam detection model in a batch mode. A set of experiments on a large-scale data set show the effectiveness of the proposed online unsupervised method in adaptively discovers and learns the patterns of new spam activities and achieve stable recall values reaching more than 95%. Although the average spam precision of our method is around 60%, the high spam recall values show the ability of our proposed method in reducing spam drift problems compared to traditional machine learning algorithms.",industry
10.1016/j.jclepro.2019.117870,Journal,Journal of Cleaner Production,scopus,2019-11-20,sciencedirect,"Digestate evaporation treatment in biogas plants: A techno-economic assessment by Monte Carlo, neural networks and decision trees",https://api.elsevier.com/content/abstract/scopus_id/85070258305,"Biogas production is one of the most promising pathways toward fully utilizing green energy within a circular economy. The anaerobic digestion process is the industry standard technology for biogas production due to its lowered energy consumption and its reliance on microbiology. Even in such an environmental-friendly process, liquid digestate is still produced from the remains of digested bio-feedstock and will require treatment. With unsuitable treatment procedure for liquid digestate, the mass of bio-feedstock can potentially escape the circular supply chain within the economy. This paper recommends the implementation of evaporator systems to provide a sustainable liquid digestate treating mechanism within the economy. Studied evaporator systems are represented by vacuum evaporation in combination with ammonia scrubber, stripping and reverse osmosis. Nevertheless, complex multi-dimensional decisions should be made by stakeholders before implementing such systems. Our work utilizes a novel techno-economics model to study the techno-economics robustness in implementing recent state-of-art vacuum evaporation systems with exploitation of waste heat from combined heat and power (CHP) units in biogas plants (BGP). To take into the account the stochasticity of the real world and robustness of the analysis, we used the Monte-Carlo simulation technique to generate more than 20,000 of different possibilities for the implementation of the evaporation system. Favourable decision pathways are then selected using a novel methodology which utilizes the artificial neural network and a hyper-optimized decision tree classifier. Two pathways that give the highest probability of providing a fast payback period are identified. Descriptive statistics are also used to analyse the distributions of decision parameters that lead to success in implementing the evaporator system. The results highlighted that integration of evaporation system are favourable when transport costs and incentives for CHP units are large and while feed-in tariffs for electricity production and specific investment costs are low. The result of this work is expected to pave the way for BGP stakeholders and decision makers in implementing liquid digestate treating technologies within the currently existing infrastructure.",industry
10.1016/j.compchemeng.2019.05.037,Journal,Computers and Chemical Engineering,scopus,2019-11-02,sciencedirect,Modern day monitoring and control challenges outlined on an industrial-scale benchmark fermentation process,https://api.elsevier.com/content/abstract/scopus_id/85071606321,"This paper outlines real-world control challenges faced by modern-day biopharmaceutical facilities through the extension of a previously developed industrial-scale penicillin fermentation simulation (IndPenSim). The extensions include the addition of a simulated Raman spectroscopy device for the purpose of developing, evaluating and implementation of advanced and innovative control solutions applicable to biotechnology facilities. IndPenSim can be operated in fixed or operator controlled mode and generates all the available on-line, off-line and Raman spectra for each batch. The capabilities of IndPenSim were initially demonstrated through the implementation of a QbD methodology utilising the three stages of the PAT framework. Furthermore, IndPenSim evaluated a fault detection algorithm to detect process faults occurring on different batches recorded throughout a yearly campaign. The simulator and all data presented here are available to download at www.industrialpenicillinsimulation.com and acts as a benchmark for researchers to analyse, improve and optimise the current control strategy implemented on this facility. Additionally, a highly valuable data resource containing 100 batches with all available process and Raman spectroscopy measurements is freely available to download. This data is highly suitable for the development of big data analytics, machine learning (ML) or artificial intelligence (AI) algorithms applicable to the biopharmaceutical industry.",industry
10.1016/j.cie.2019.106031,Journal,Computers and Industrial Engineering,scopus,2019-11-01,sciencedirect,Machine learning based concept drift detection for predictive maintenance,https://api.elsevier.com/content/abstract/scopus_id/85071975175,"In this work we present a machine learning based approach for detecting drifting behavior – so-called concept drifts – in continuous data streams. The motivation for this contribution originates from the currently intensively investigated topic Predictive Maintenance (PdM), which refers to a proactive way of triggering servicing actions for industrial machinery. The aim of this maintenance strategy is to identify wear and tear, and consequent malfunctioning by analyzing condition monitoring data, recorded by sensor equipped machinery, in real-time. Recent developments in this area have shown potential to save time and material by preventing breakdowns and improving the overall predictability of industrial processes. However, due to the lack of high quality monitoring data and only little experience concerning the applicability of analysis methods, real-world implementations of Predictive Maintenance are still rare. Within this contribution, we present a method, to detect concept drift in data streams as potential indication for defective system behavior and depict initial tests on synthetic data sets. Further on, we present a real-world case study with industrial radial fans and discuss promising results gained from applying the detailed approach in this scope.",industry
10.1016/j.enconman.2019.111932,Journal,Energy Conversion and Management,scopus,2019-11-01,sciencedirect,Cultural coyote optimization algorithm applied to a heavy duty gas turbine operation,https://api.elsevier.com/content/abstract/scopus_id/85070893013,"In the past decades, the quantity of researches regarding industrial gas turbines (GT) has increased exponentially in terms of number of publications and diversity of applications. The GTs offer high power output along with a high combined cycle efficiency and high fuel flexibility. As consequence, the energy efficiency, the pressure oscillations, the pollutant emissions and the fault diagnosis have become some of the recent concerns related to this type of equipment. In order to solve these GTs related problems and many other real-world engineering and industry 4.0 issues, a set of new technological approaches have been tested, such as the combination of Artificial Neural Networks (ANN) and metaheuristics for global optimization. In this paper, the recently proposed metaheuristic denoted Coyote Optimization Algorithm (COA) is applied to the operation optimization of a heavy duty gas turbine placed in Brazil and used in power generation. The global goal is to find the best valves setup to reduce the fuel consumption while coping with environmental and physical constraints from its operation. In order to treat it as an optimization problem, an integrated simulation model is implemented from original data-driven models and others previously proposed in literature. Moreover, a new version of the COA that links some concepts from Cultural Algorithms (CA) is proposed, which is validated under a set of benchmarks functions from the Institute of Electrical and Electronics Engineers (IEEE) Congress on Evolutionary Computation (CEC) 2017 and tested to the GT problem. The results show that the proposed Cultural Coyote Optimization Algorithm (CCOA) outperforms its counterpart for benchmark functions. Further, non-parametric statistical significance tests prove that the CCOA’s performance is competitive when compared to other state-of-the-art metaheuristics after a set of experiments for five case studies. In addition, the convergence analysis shows that the cultural mechanism employed in the CCOA has improved the COA balance between exploration and exploitation. As a result, the CCOA can improve the current GT operation significantly, reducing the fuel consumption up to 
                        
                           3.6
                           %
                        
                      meanwhile all constraints are accomplished.",industry
10.1016/j.engfracmech.2019.106642,Journal,Engineering Fracture Mechanics,scopus,2019-10-01,sciencedirect,Necking-induced fracture prediction using an artificial neural network trained on virtual test data,https://api.elsevier.com/content/abstract/scopus_id/85071523401,"The imperfection-based necking model by Marciniak and Kuczyński (MK) is frequently used for predicting the onset of localized necking under proportional and non-proportional loading, which can be considered a lower limit for the occurrence of fracture in a vehicle body structure subjected to crash loading. A large number of virtual imperfection lines at different orientation angles have to be analysed simultaneously in order to find the critical imperfection causing necking under arbitrary loading. This, and the continuous computation of a “distance to necking” quantity, representing a crucial output quantity for the simulation engineer, makes the model computationally expensive and limits industrial use in full-scale vehicle crash simulations.
                  In this work, an extended MK model is used for creating a virtual test data base under proportional and non-proportional loading for training of a computationally more efficient simple feed-forward neural network (NN). Both models are implemented in a User Material routine of an explicit crash code, where the predictions of the NN are in good agreement with the predictions of the MK reference model, however at a significantly reduced computational cost. Besides a pure numerical validation study, an experimental validation study has been performed, imposing biaxial tension loading followed by plane strain tension loading until necking using a special punch test apparatus. Whereas MK and NN are in good agreement with the experimental observations, the agreement of classical necking models, applied in conjunction with a linear damage accumulation (forming severity) concept was less accurate.",industry
10.1016/j.ibiod.2019.104744,Journal,International Biodeterioration and Biodegradation,scopus,2019-10-01,sciencedirect,Comparative evaluation of Pseudomonas species in single chamber microbial fuel cell with manganese coated cathode for reactive azo dye removal,https://api.elsevier.com/content/abstract/scopus_id/85069842075,"Microbial fuel cell (MFCs), distinguished by different strains of Pseudomonas species; Pseudomonas aeruginosa (MPEM-MFC I) and Pseudomonas fluorescens (MPEM-MFC II), was analyzed. Results have shown that, over a period of 360 h in the presence of 0.5 mM of model dye, MPEM MFC I produced the maximum power density of 2887 ± 13 μW m−2 (RO-16) and 1906 ± 7 μW m−2 (RB-5) compared with MPEM-MFC II with 1896 ± 15 μW m−2 (RO-16) and 1028 ± 9 μW m−2 (RB-5). Decolorization efficiency of MPEM-MFC I was 98 ± 1.2% (RO-16) and 95 ± 2% (RB-5). Total phenazine production in MPEM-MFC I was 12.3 ± 0.5 μg mL−1 higher than that of 8.9 ± 0.05 μg mL−1 (MPEM-MFC II) and its production have positive influence of electron shuttling that brought out high power output. Addition of phenazine externally reduced the dye degradation. Bioadhesion capability of P. aeruginosa on the anode reduced the internal resistance in MFCs. Thus the implementation of MFC is a most promising technology for the complete decolorization of reactive azo dyes and it has potential economic benefits in real-life industrial application.",industry
10.1016/j.future.2019.04.014,Journal,Future Generation Computer Systems,scopus,2019-10-01,sciencedirect,TIDE: Time-relevant deep reinforcement learning for routing optimization,https://api.elsevier.com/content/abstract/scopus_id/85065443852,"Routing optimization has been researched in network design for a long time, and plenty of optimization schemes have been proposed from both academia and industry. However, such schemes are either too complicated in applications or far from the optimal performance. In recent years, with the development of Software-defined Networking (SDN) and Artificial Intelligence (AI), AI-based methods of routing strategy are being considered. In this paper, we propose TIDE, an intelligent network control architecture based on deep reinforcement learning that can dynamically optimize routing strategies in an SDN network without human experience. TIDE is implemented and validated on a real network environment. Experiment result shows that TIDE can adjust the routing strategy dynamically according to the network condition and can improve the overall network transmitting delay by about 9% compared with traditional algorithms.",industry
10.1016/j.enbuild.2019.07.029,Journal,Energy and Buildings,scopus,2019-09-15,sciencedirect,Whole building energy model for HVAC optimal control: A practical framework based on deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85069552761,"Whole building energy model (BEM) is a physics-based modeling method for building energy simulation. It has been widely used in the building industry for code compliance, building design optimization, retrofit analysis, and other uses. Recent research also indicates its strong potential for the control of heating, ventilation and air-conditioning (HVAC) systems. However, its high-order nature and slow computational speed limit its practical application in real-time HVAC optimal control. Therefore, this study proposes a practical control framework (named BEM-DRL) that is based on deep reinforcement learning. The framework is implemented and assessed in a novel radiant heating system in an existing office building as a case study. The complete implementation process is presented in this study, including: building energy modeling for the novel heating system, multi-objective BEM calibration using the Bayesian method and the Genetic Algorithm, deep reinforcement learning training and simulation results evaluation, and control deployment. By analyzing the real-life control deployment data, it is found that BEM-DRL achieves 16.7% heating demand reduction with more than 95% probability compared to the old rule-based control. However, the framework still faces the practical challenges including building energy modeling of novel HVAC systems and multi-objective model calibration. Systematic study is also needed for the design of deep reinforcement learning training to provide a guideline for practitioners.",industry
10.1016/j.jnca.2019.06.003,Journal,Journal of Network and Computer Applications,scopus,2019-09-15,sciencedirect,MAPLE: A Machine Learning Approach for Efficient Placement and Adjustment of Virtual Network Functions,https://api.elsevier.com/content/abstract/scopus_id/85067443855,"As one of the many advantages of cloud computing, Network Function Virtualization (NFV) has revolutionized the network and telecommunication industry through enabling the migration of network functions from expensive dedicated hardware to software-defined components that run in the form of Virtual Network Functions (VNFs). However, with NFV comes numerous challenges related mainly to the complexity of deploying and adjusting VNFs in the physical networks, owing to the huge number of nodes and links in today's datacenters, and the inter-dependency among VNFs forming a certain network service. Several contributions have been made in an attempt to answer these challenges, where most of the existing solutions focus on the static placement of VNFs and overlook the dynamic aspect of the problem, which arises mainly due to the ever-changing resource availability in the cloud datacenters and the continuous mobility of the users. Few attempts have been lately made to incorporate the dynamic aspect to the VNF deployment solutions. The main problem of these approaches lies in their reactive readjustment scheme which determines the placement/migration strategy upon the receipt of a new request or the happening of a certain event, thus resulting in high setup latencies. In this paper, we take advantage of machine learning to reduce the complexity of the placement and readjustment processes through designing a cluster-based proactive solution. The solution consists of (1) an Integer Linear Programming (ILP) model that considers a tradeoff between the minimization of the latency, Service-Level Objective (SLO) violation cost, hardware utilization, and VNF readjustment cost, (2) an optimized k-medoids clustering approach which proactively partitions the substrate network into a set of disjoint on-demand clusters and (3) data-driven cluster-based placement and readjustment algorithms that capitalize on machine learning to intelligently eliminate some cost functions from the optimization problem to boost its feasibility in large-scale networks. Simulation results show that the proposed solution considerably reduces the readjustment time and decrease the hardware utilization compared to the K-means, original k-medoids and migration without clustering approaches.",industry
10.1016/j.ifacol.2019.11.102,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,Sustainable operations management for industry 4.0 and its social return,https://api.elsevier.com/content/abstract/scopus_id/85078948022,"In today’s industrial environment, where concepts of smart factories are consolidating their application in companies, it is still necessary to approach management decision making from a perspective that encompasses all aspects of sustainability without losing sight of the social return to which they must contribute. In order to obtain a reliable prediction, of the operation of a Sustainable Manufacturing System (SMS) and its Social Return (SR), this paper develops a methodology and procedures that allow predicting the system performance as a whole. This will allow us to assist management decision making in industries 4.0, supported by multi-criteria methods in knowledge management, simulation, value analysis and operational research by means of:
                  a) Study the economic, social and environmental impacts in the organization and management of the efficient operation of an SMS with the selection of strategies and alternatives in production chains to minimize and / or mitigate environmental and labor risks.
                  b) Encourage of industrial symbiosis or eco-industries networks that create opportunities increasing eco-efficiency and the positive social return of production systems.
                  This proposed methodology will facilitate changes in the structure of production systems in order to implement industry 4.0 paradigms through facilitator technologies such as simulation and virtual reality. This framework will allow Small and Medium Enterprises (SMEs) and other companies to address the decision-making activities that improve the economic-functional efficiency, which will lead to reduce the environmental impact and increase the positive social return of certain production strategies, considering working conditions.
                  The proposed approach went validated, in the area of the Euroregion Galicia North of Portugal, to favour the implementation of the decision-making through the Industry 4.0 Technologies.",industry
10.1016/j.ifacol.2019.11.172,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,Machine learning framework for predictive maintenance in milling,https://api.elsevier.com/content/abstract/scopus_id/85078904429,"In the Industry 4.0 era, artificial intelligence is transforming the manufacturing industry. With the advent of Internet of Things (IoT) and machine learning methods, manufacturing systems are able to monitor physical processes and make smart decisions through realtime communication and cooperation with humans, machines, sensors, and so forth. Artificial intelligence enables manufacturers to reduce equipment downtime, spot production defects, improve the supply chain, and shorten design times by using machine learning technologies which learn from experiences. One of the last application of these technologies is the development of Predictive Maintenance systems. Predictive maintenance combines Industrial IoT technologies with machine learning to forecast the exact time in which manufacturing equipment will need maintenance, allowing problems to be solved and adaptive decisions to be made in a timely fashion. This study will discuss the implementation of a milling Cutting-tool Predictive Maintenance solution (including Wear Monitoring), applied to a real milling data set as validation of the framework. More generally, this work provides a basic framework for creating a tool to monitor the wear level, preventing the breakdown, of a generic manufacturing tool, in order to improve human-machine interaction and optimize the production process.",industry
10.1016/j.ifacol.2019.11.385,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,Towards a data-driven predictive-reactive production scheduling approach based on inventory availability,https://api.elsevier.com/content/abstract/scopus_id/85078884096,"To survive in a competitive business environment, manufacturing systems require the proper deployment of advanced technologies coming from Industry 4.0. These technologies allow access to quasi-real-time data that provide a continuously updated picture of the production system, including the state of available inventory. Data-driven predictive-reactive production scheduling has the potential to support the anticipation and prompt reaction to overcome different kinds of disruptions that occur in production execution nowadays. This research paper aims to propose a conceptual model for a data-driven predictive-reactive production scheduling approach combining machine learning and simulation-based optimization, considering current inventory of raw material, work in process and final products inventory to characterize a job-shop production execution state. The approach supports decision-making in dynamic situations related to inventory availability that can affect production schedules.",industry
10.1016/j.ifacol.2019.11.465,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,"Integration of automatic generated simulation models, machine control projects and management tools to support whole life cycle of industrial digital twins",https://api.elsevier.com/content/abstract/scopus_id/85078871061,"The paper presents a framework of automatic generation of industrial digital twins. These digital twins will be suitable to support preliminary design phases of systems development, but also to support next phases of detailed designs implementation and systems running phases. These digital twin allow, from the preliminary designing phase, to generate a complete simulation of the target industrial system. But, at the same time, and without the need to develop and add any subsequent code, they should be a valuable support for the phases and tasks of exploitation: maintenance, machine or system learning, etc. The problem is that the requirements for first development phases are much more generic than those for later phases. For this reason, instead of incorporating specificities in the simulation system, the framework takes advantage of the applications which are being developed for the implementation of the real system. In these applications (the control program and the decisions and the high level management system), the specificities have had to be taken into account. The system has been specialized in industrial transportation and warehouse systems which, although have a finite number or building objects, they have an infinite set of final configurations, very different one from each other. The paper presents an evaluation of current simulation platforms suitable to be used as part of the framework, and the digital twin industrial system generation framework itself. An example of application is as well presented.",industry
10.1016/j.trip.2019.100028,Journal,Transportation Research Interdisciplinary Perspectives,scopus,2019-09-01,sciencedirect,Translation software: An alternative to transit data standards,https://api.elsevier.com/content/abstract/scopus_id/85075931935,"Data standardization is recognized in many disciplines as a critical aspect of data stewardship. Establishing and implementing data specifications increases the usefulness of data collection efforts and facilitates analysis techniques. With the advent of large quantities of machine-generated data, the use of standardized data formats feeds opportunities for visualization and advanced applications with machine-learning and Artificial Intelligence (AI). The transportation industry made substantial progress with data format specifications in the late 1990s, primarily for highway traffic. Unfortunately, establishing data standards has been an on-going challenge for the transit community. Archived Intelligent Transportation Systems (ITS) transit data (e.g., Automatic Vehicle Location (AVL), Automatic Passenger Counters (APCs), Automatic Fare Card (AFC)) still lack industry standards for data formats. Recent advancements in electronic transit scheduling (e.g., General Transit Feed Specifications (GTFS)) met a portion of this challenge with Open Data specifications. Now GTFS provides transit riders with agile information on services available at any location where the data is provided to developers of mobile device application (apps). Due to system and vendor limitations, the Metropolitan Transportation Authority (MTA), serving the New York City region, publishes its real-time subway system data in GTFS-R and its bus data in SIRI. This research develops an Application Programming Interface (API) to translate GTFS-R into SIRI to overcome the lack of standards making it possible to harmonize the subway and bus systems for the New York region. This solution offers the opportunity to develop a novel set of analytical tools, including pseudo-surveillance data for performance metrics.",industry
10.1016/j.jngse.2019.102933,Journal,Journal of Natural Gas Science and Engineering,scopus,2019-09-01,sciencedirect,Machine learning for surveillance of fluid leakage from reservoir using only injection rates and bottomhole pressures,https://api.elsevier.com/content/abstract/scopus_id/85068973220,"Carbon-neutral economies would require preventing the release of industrial-scale CO2 into the atmosphere by injecting into geologic formations. Large-scale injection of CO2 into deep reservoirs carries a potential for its undesired leakage into above zones, which can act as an obstacle to its large-scale implementation. Current methods for surveillance of CO2 leaks are costly and not very robust, especially the methods that simulate expected pressure behavior based on an assumed reservoir model.
                  This study proposes a machine learning method for surveillance of fluid leakage using deconvolution response function (a non-linear function of time varying bottomhole pressure and injection rates) from injection and monitoring wells as a measure of leakage that is simulated via multivariate linear regression of all the wells present in the reservoir. Leakage is detected by comparing “expected” (baseline without leaks) deconvolution response of all monitoring wells with their “observed” deconvolution response. Three key advantages of the proposed method are that it i) uses only injection rates and bottomhole pressure data (with no reservoir or geological model), ii) is independent of physical process parameterization uncertainties, and iii) applicable to both conventional and unconventional (e.g. fractured tight formations) reservoirs with any fluid (e.g. compressible, incompressible). The proposed method is first trained to learn well history with no leakage, followed by its validation after which it can be used to detect leakage by tracking a meaningful deviation error (at least twenty times the error of no leakage base scenario over same time period) between expected well response and observed well response at all monitoring wells. The well history required for the proposed method comes directly from measurements made at wells in a real field, but in absence of field data the proposed method is illustrated through well history simulated by reservoir simulations; no such numerical simulations are required for application of this method in a real world scenario with well measurements.",industry
10.1016/j.jss.2019.05.026,Journal,Journal of Systems and Software,scopus,2019-09-01,sciencedirect,Sentiment based approval prediction for enhancement reports,https://api.elsevier.com/content/abstract/scopus_id/85065795226,"The maintenance and evolution of the software application is a continuous phase in the industry. Users are frequently proposing enhancement requests for further functionalities. However, although only a small part of these requests are finally adopted, developers have to go through all of such requests manually, which is tedious and time consuming. To this end, in this paper we propose a sentiment based approach to predict how likely enhancement reports would be approved or rejected so that developers can first handle likely-to-be-approved requests. This could help the software applications to compete in the industry by upgrading their features in time as per user’s requirements. First, we preprocess enhancement reports using natural language preprocessing techniques. Second, we identify the words having positive and negative sentiments in the summary attribute of the enhancements reports and calculate the sentiment of each enhancement report. Finally, with the history data of real software application, we train a machine learning based classifier to predict whether a given enhancement report would be approved. The proposed approach has been evaluated with the history data from real software applications. The cross-application validation suggests that the proposed approach outperforms the state-of-the-art. The evaluation results suggest that the proposed approach increases the accuracy from 70.94% to 77.90% and improves the F-measure significantly from 48.50% to 74.53%.",industry
10.1016/j.compind.2019.04.016,Journal,Computers in Industry,scopus,2019-09-01,sciencedirect,A comparison of fog and cloud computing cyber-physical interfaces for Industry 4.0 real-time embedded machine learning engineering applications,https://api.elsevier.com/content/abstract/scopus_id/85065718296,"Industrial cyber-physical systems are the primary enabling technology for Industry 4.0, which combine legacy industrial and control engineering, with emerging technology paradigms (e.g. big data, internet-of-things, artificial intelligence, and machine learning), to derive self-aware and self-configuring factories capable of delivering major production innovations. However, the technologies and architectures needed to connect and extend physical factory operations to the cyber world have not been fully resolved. Although cloud computing and service-oriented architectures demonstrate strong adoption, such implementations are commonly produced using information technology perspectives, which can overlook engineering, control and Industry 4.0 design concerns relating to real-time performance, reliability or resilience. Hence, this research compares the latency and reliability performance of cyber-physical interfaces implemented using traditional cloud computing (i.e. centralised), and emerging fog computing (i.e. decentralised) paradigms, to deliver real-time embedded machine learning engineering applications for Industry 4.0. The findings highlight that despite the cloud’s highly scalable processing capacity, the fog’s decentralised, localised and autonomous topology may provide greater consistency, reliability, privacy and security for Industry 4.0 engineering applications, with the difference in observed maximum latency ranging from 67.7%–99.4%. In addition, communication failures rates highlighted differences in both consistency and reliability, with the fog interface successfully responding to 900,000 communication requests (i.e. 0% failure rate), and the cloud interface recording failure rates of 0.11%, 1.42%, and 6.6% under varying levels of stress.",industry
10.1016/j.eswa.2019.03.011,Journal,Expert Systems with Applications,scopus,2019-08-15,sciencedirect,Constraint learning based gradient boosting trees,https://api.elsevier.com/content/abstract/scopus_id/85063576343,"Predictive regression models aim to find the most accurate solution to a given problem, often without any constraints related to the model’s predicted values. Such constraints have been used in prior research where they have been applied to a subpopulation within the training dataset which is of greater interest and importance. In this research we introduce a new setting of regression problems, in which each instance can be assigned a different constraint, defined based on the value of the target (predicted) attribute. The new use of constraints is taken into account and incorporated into the learning process, and is also considered when evaluating the induced model. We propose two algorithms which are modifications to the regression boosting method. There are two advantages of the proposed algorithms: they are not dependent on the base learner used during the learning process, and they can be adopted by any boosting technique. We implemented the algorithms by modifying the gradient boosting trees (GBT) model, and we also introduced two measures for evaluating the models that were trained to solve the constraint problems. We compared the proposed algorithms to three baseline algorithms using four real-life datasets. Due to the algorithms’ focus on satisfying the constraints, in most cases the results showed significant improvement in the constraint-related measures, with just a minimal effect on the general prediction error. The main impact of the proposed approach is in its ability to derive a model with a higher level of assurance for specific cases of interest (i.e., the constrained cases). This is extremely important and has great significance in various use cases and expert and intelligent systems, particularly critical systems, such as critical healthcare systems (e.g., when predicting blood pressure or blood sugar level), safety systems (e.g., when aiming to estimate the distance of cars or airplanes from other objects), or critical industrial systems (e.g., require to estimate their usability along time). In each of these cases, there is a subpopulation of all instances that is of greater interest to the expert or system, and the sensitivity of the model’s error changes according to the real value of the predicted feature. For example, for a subpopulation of patients (e.g., patients under the age of eight, or patients known to be at risk), physicians often require a sensitive model that accurately predicts blood pressure values.",industry
10.1016/j.mfglet.2019.08.003,Journal,Manufacturing Letters,scopus,2019-08-01,sciencedirect,A self-aware and active-guiding training &amp; assistant system for worker-centered intelligent manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85070738030,"Training and on-site assistance is critical to help workers master required skills, improve worker productivity, and guarantee the product quality. Traditional training methods lack worker-centered considerations that are particularly in need when workers are facing ever-changing demands. In this study, we propose a worker-centered training & assistant system for intelligent manufacturing, which is featured with self-awareness and active-guidance. Multi-modal sensing techniques are applied to perceive each individual worker and a deep learning approach is developed to understand the worker’s behavior and intention. Moreover, an object detection algorithm is implemented to identify the parts/tools the worker is interacting with. Then the worker’s current state is inferred and used for quantifying and assessing the worker performance, from which the worker’s potential guidance demands are analyzed. Furthermore, onsite guidance with multi-modal augmented reality is provided actively and continuously during the operational process. Two case studies are used to demonstrate the feasibility and great potential of our proposed approach and system for applying to the manufacturing industry for frontline workers.",industry
10.1016/j.compind.2019.04.010,Journal,Computers in Industry,scopus,2019-08-01,sciencedirect,Managing workflow of customer requirements using machine learning,https://api.elsevier.com/content/abstract/scopus_id/85065732680,"Customer requirements – product specifications issued by the customer – organize the dialog between suppliers and customers and, hence, affect the dynamics of supply networks. These large and complex documents are frequently updated over time, while changes are seldom marked by the customers who issue the requirements. The lack of structure and defined responsibilities, thus, demands an expert to manually process the requirements. Here, the possibility to improve the usual workflow with machine learning algorithms is explored.
                  The whole requirements management process has two major bottlenecks, which can be automatized. The first one, detecting changes, can be accomplished via a document comparison tool. The second one, recognizing the responsibilities and assigning them to the right department, can be solved with standard machine learning algorithms. Here, such algorithms are applied to a dataset obtained from a global automotive industry supplier.
                  The proposed method improves the requirements management process by reducing an expert’s workload and thus decreasing the time for processing one document was reduced from 2 weeks to 1 h. Moreover, the method gives a high accuracy of department assignment and can self-improve once implemented into a requirements management system.
                  Although the machine learning methods are very popular nowadays, they are seldom used to improve business processes in real companies, especially in the case of processes that did not require digitalization in the past. Here we show, how such methods can solve some of the management problems and improve their workflow.",industry
10.1016/j.compind.2019.05.001,Journal,Computers in Industry,scopus,2019-08-01,sciencedirect,Industrial robot control and operator training using virtual reality interfaces,https://api.elsevier.com/content/abstract/scopus_id/85065132267,"Nowadays, we are involved in the fourth industrial revolution, commonly referred to as “Industry 4.0,” where cyber-physical systems and intelligent automation, including robotics, are the keys. Traditionally, the use of robots has been limited by safety and, in addition, some manufacturing tasks are too complex to be fully automated. Thus, human-robot collaborative applications, where robots are not isolated, are necessary in order to increase the productivity ensuring the safety of the operators with new perception systems for the robot and new interaction interfaces for the human. Moreover, virtual reality has been extended to the industry in the last years, but most of its applications are not related to robots. In this context, this paper works on the synergies between virtual reality and robotics, presenting the use of commercial gaming technologies to create a totally immersive environment based on virtual reality. This environment includes an interface connected to the robot controller, where the necessary mathematical models have been implemented for the control of the virtual robot. The proposed system can be used for training, simulation, and what is more innovative, for robot controlling in an integrated, non-expensive and unique application. Results show that the immersive experience increments the efficiency of the training and simulation processes, offering a cost-effective solution.",industry
10.1016/j.ece.2019.05.003,Journal,Education for Chemical Engineers,scopus,2019-07-01,sciencedirect,"Learning distillation by a combined experimental and simulation approach in a three steps laboratory: Vapor pressure, vapor-liquid equilibria and distillation column",https://api.elsevier.com/content/abstract/scopus_id/85066038830,"Distillation is one of the most important separation process in industrial chemistry. This operation is based on a deep knowledge of the fluid phase equilibria involved in the mixture to be separated. In particular, the most important aspects are the determination of the vapor pressures of the single compounds and the correct representation of the eventual not ideality of the mixture. Simulation science is a fundamental tool for managing these complex topics and chemical engineers students have to learn and to use it on real case-studies. To give to the students a complete overview of these complex aspects, a laboratory experience is proposed. Three different work stations were set up: i) determination of vapor pressure of two pure compounds; ii) the study of vapor-liquid equilibria of a binary mixture; iii) the use of a continuous multistage distillation column in dynamic and steady-state conditions. The simulation of all these activities by a commercial software, PRO II by AVEVA, allows to propose and verify the thermodynamic characteristics of the mixture and to correctly interpret the distillation column data. Moreover, the experimental plants and the data elaboration by classical equations are presented. The students are request to prepare a final report in which the description of the experimental plants and experimental procedure, the interpretation of the results and the simulation study are critically discussed in order to encourage them to reason and to acquire the concepts of the course.
                  Two different questionnaires each with 7 questions, for the course and for the laboratory, are proposed and analyzed. The final evaluation of the students was strongly positive both for the course as a whole and for the proposed laboratory activities.",industry
10.1016/j.psep.2019.05.016,Journal,Process Safety and Environmental Protection,scopus,2019-07-01,sciencedirect,An intelligent fire detection approach through cameras based on computer vision methods,https://api.elsevier.com/content/abstract/scopus_id/85065893982,"Fire that is one of the most serious accidents in petroleum and chemical factories, may lead to considerable production losses, equipment damages and casualties. Traditional fire detection was done by operators through video cameras in petroleum and chemical facilities. However, it is an unrealistic job for the operator in a large chemical facility to find out the fire in time because there may be hundreds of video cameras installed and the operator may have multiple tasks during his/her shift. With the rapid development of computer vision, intelligent fire detection has received extensive attention from academia and industry. In this paper, we present a novel intelligent fire detection approach through video cameras for preventing fire hazards from going out of control in chemical factories and other high-fire-risk industries. The approach includes three steps: motion detection, fire detection and region classification. At first, moving objects are detected through cameras by a background subtraction method. Then the frame with moving objects is determined by a fire detection model which can output fire regions and their locations. Since false fire regions (some objects similar with fire) may be generated, a region classification model is used to identify whether it is a fire region or not. Once fire appears in any camera, the approach can detect it and output the coordinates of the fire region. Simultaneously, instant messages will be immediately sent to safety supervisors as a fire alarm. The approach can meet the needs of real-time fire detection on the precision and the speed. Its industrial deployment will help detect fire at the very early stage, facilitate the emergency management and therefore significantly contribute to loss prevention.",industry
10.1016/j.cie.2019.04.054,Journal,Computers and Industrial Engineering,scopus,2019-07-01,sciencedirect,Agent-based modelling and heuristic approach for solving complex OEM flow-shop productions under customer disruptions,https://api.elsevier.com/content/abstract/scopus_id/85065083945,"The application of the agent-based simulation approach in the flow-shop production environment has recently gained popularity among researchers. The concept of agent and agent functions can help to automate a variety of difficult tasks and assist decision-making in flow-shop production. This is especially so in the large-scale Original Equipment Manufacturing (OEM) industry, which is associated with many uncertainties. Among these are uncertainties in customer demand requirements that create disruptions that impact production planning and scheduling, hence, making it difficult to satisfy demand in due time, in the right order delivery sequence, and in the right item quantities. It is however important to devise means of adapting to these inevitable disruptive problems by accommodating them while minimising the impact on production performance and customer satisfaction.
                  In this paper, an innovative embedded agent-based Production Disruption Inventory-Replenishment (PDIR) framework, which includes a novel adaptive heuristic algorithm and inventory replenishment strategy which is proposed to tackle the disruption problems. The capabilities and functionalities of agents are utilised to simulate the flow-shop production environment and aid learning and decision making. In practice, the proposed approach is implemented through a set of experiments conducted as a case study of an automobile parts facility for a real-life large-scale OEM. The results are presented in term of Key Performance Indicators (KPIs), such as the number of late/unsatisfied orders, to determine the effectiveness of the proposed approach. The results reveal a minimum number of late/unsatisfied orders, when compared with other approaches.",industry
10.1016/j.est.2019.04.015,Journal,Journal of Energy Storage,scopus,2019-06-01,sciencedirect,Comparison of a physical and a data-driven model of a Packed Bed Regenerator for industrial applications,https://api.elsevier.com/content/abstract/scopus_id/85064907454,"Thermal Energy Storage systems are promising technologies to match intermittent heat supply with demand and improve the energy efficiency of industrial processes. To optimally integrate these energy storage systems in industry, reliable and industrially applicable models are required. This work examines two different modeling approaches for a Sensible Thermal Energy Storage device, namely a Packed Bed Regenerator. A physical 1D-model using finite difference methods and a data-driven grey box model using Recurrent Neural Networks are described. Experimental data from a Packed Bed Regenerator test rig is used to create the data-driven model and to compare the results of both models with real measurements. A quantitative and qualitative comparison of the data-driven and the physical model is conducted. The results of the quantitative investigation show, that both models are able to capture the complex behavior of the Packed Bed Regenerator. With the qualitative analysis, the features of the different models are highlighted and advantages and limitations are discussed. Thus, it provides an orientation in the decision-making process for the choice of an appropriate modeling approach. The findings of this work can support the creation of physical, as well as data-driven models of sensible energy storage systems and strengthen their implementation to industrial processes. The generic grey box modeling approach and the findings of the qualitative comparison of the models can be also applied to other modeling tasks.",industry
10.1016/j.scitotenv.2019.02.213,Journal,Science of the Total Environment,scopus,2019-05-20,sciencedirect,Passive sampling of volatile organic compounds in industrial atmospheres: Uptake rate determinations and application,https://api.elsevier.com/content/abstract/scopus_id/85061829807,"This study describes the implementation of a passive sampling-based method followed by thermal desorption gas-chromatography-mass spectrometry (TD-GC–MS) for the monitoring of volatile organic compounds (VOCs) in industrial atmospheres. However, in order to employ passive sampling as a reliable sampling technique, a specific diffusive uptake rate is required for each compound. Accordingly, the aim of the present study was twofold. First, the experimental diffusive uptake rates of the target VOCs were determined under real industrial air conditions using Carbopack X thermal desorption tubes, and active sampling as reference method. The sampling campaigns carried out between October 2017 and May 2018 provided us of experimental diffusive uptake rates between 0.40 mL min−1 and 0.70 mL min−1 and stable over time (RSD % < 8%) for up to 41 VOCs. Secondly, the uptake rates obtained experimentally were applied for the determination of VOCs concentrations at 16 sampling sites in the North Industrial Complex of Tarragona. The results showed i-pentane, n-pentane and the compounds known as BTEX as the most representative ones. Moreover, some sporadic peaks of 1,3-butadiene, acrylonitrile, ethylbenzene and styrene resulting from certain industrial activities were detected.",industry
10.1016/j.engappai.2019.03.011,Journal,Engineering Applications of Artificial Intelligence,scopus,2019-05-01,sciencedirect,Distributed parallel deep learning of Hierarchical Extreme Learning Machine for multimode quality prediction with big process data,https://api.elsevier.com/content/abstract/scopus_id/85063385858,"In this work, the distributed and parallel Extreme Learning Machine (dp-ELM) and Hierarchical Extreme Learning Machine (dp-HELM) are proposed for multimode process quality prediction with big data. The efficient ELM algorithm is transformed into the distributed and parallel modeling form according to the MapReduce framework. Since the deep learning network structure of HELM is more accurate than the single layer of ELM in feature representation, the dp-HELM is further developed through decomposing the ELM-based Auto-encoders (ELM-AE) of deep hidden layers into a loop of MapReduce jobs. Additionally, the multimode issue is solved through the “divide and rule” strategy. The distributed and parallel K-means (dp-K-means) is utilized to divide the process modes, which are further trained in a synchronous parallel way by dp-ELM and dp-HELM. Finally, the Bayesian model fusion technique is utilized to integrate the local models for online prediction. The proposed algorithms are deployed on a Hadoop MapReduce computing cluster and the feasibility and efficiency are illustrated through building a real industrial quality prediction model with big process data.",industry
10.1016/j.jlp.2019.03.003,Journal,Journal of Loss Prevention in the Process Industries,scopus,2019-05-01,sciencedirect,A fuzzy expert system for mitigation of risks and effective control of gas pressure reduction stations with a real application,https://api.elsevier.com/content/abstract/scopus_id/85063113351,"Environmental changes and increased uncertainty due to technical damage, explosions and large fires have caused the risk of an inevitable element in the gas industry. This study purposes developing a new hybrid fuzzy expert system as a decision support system to mitigate the risk associated with gas transmission stations. The designed knowledge-based system combines the procedural and descriptive rules based on experts’ judgments to analyze the complex relationships between the different components of a gas pressure reduction station. The developed fuzzy expert system is coded in C language integrated production system (CLIPS) and is linked with MATLAB software for calling fuzzy functions. A real case study of gas pressure reduction stations in Iranian gas industry is conducted to validate the proposed expert system model. The expert system provides more than one thousand rules based on expert knowledge to prevent the pressure drop and the quality loss of gas or shutting off gas flow which accordingly increases gas flow stability. The proposed expert system could minimize the risk of hazardous scenarios, such as leakage and corrosion, in the gas industry and provide an acceptable precision in the provision of periodic control strategies and appropriate response under an emergency condition.",industry
10.1016/j.engappai.2019.02.019,Journal,Engineering Applications of Artificial Intelligence,scopus,2019-05-01,sciencedirect,A new hierarchical approach to requirement analysis of problems in automated planning,https://api.elsevier.com/content/abstract/scopus_id/85062901276,"The use of Knowledge Engineering (KE) processes to analyze and configure domains in automated planning is becoming more appealing since it was noticed that this issue could make a difference to solve real problems. The contrast between a generic domain independent approach, taken as canonical in AI, and alternative processes that include knowledge engineering – eventually adding specific knowledge – has been discussed by Computer and Engineering communities. A big impact has been noticed mainly in the early phase of requirement analysis when KE approach is normally introduced. Requirement analysis is responsible for carrying out the Knowledge modeling of both problem and work domains, which is a key issue to guide different planner algorithms to come out with efficient solutions. Also, there is the scalability issue that appear in most real problems. To face that, hierarchical methods played an important hole in the history of planning and inspired several solutions since the proposal of NONLIN in the 70’s. Since then, the idea of associating hierarchical relational nets with partial ordered actions has prevailed when large systems were considered. However, there is still a gap between the hierarchical approach and the state of art of requirements analysis to allow features anticipated by KE approach to really appear in the requirements of a planning process. This paper proposes a pathway to solve this gap starting with requirements elicitation represented first in the conventional semi-formal (diagrammatic) language – UML – that is translated to Hierarchical Petri Nets (HPNs) by a new enhanced algorithm. The proposed process was installed in a software tool – developed by one of the authors – that analyzes the performance of the KE planning model: itSIMPLE (Integrated Tools Software Interface for Modeling Planning Environment). This tool was initially designed to use classic Place/Transition nets and an old version of UML (2.1). It is now enhanced to use UML 2.4 and a hierarchical Petri Net extension, also developed by the authors. Realistic examples illustrate the process which is now being applied to larger problems related to the manufacturing of car sequencing domain, one of challenge of ROADEF 2005 (French Operations Research & Decision Support Society). Finally, we consider the possibility to introduce another approach to the KE process by using KAOS (Keep All Object Satisfied) to make the planning design more accurate.",industry
10.1016/j.ijheatmasstransfer.2018.12.170,Journal,International Journal of Heat and Mass Transfer,scopus,2019-05-01,sciencedirect,Visualization-based nucleate boiling heat flux quantification using machine learning,https://api.elsevier.com/content/abstract/scopus_id/85059864859,"Processes involving complex phenomena are ubiquitous in nature and industry, many of which are difficult to simulate computationally. Nucleate boiling heat transfer, for instance, has numerous practical applications, while the film boiling is an undesirable operation regime. So far, most correlations and computer simulations to quantify boiling heat transfer rely on direct measurement of thermohydraulic data, such as heater temperature, which is often invasive. Here it is demonstrated that neural network-based models can quantify heat transfer using only direct and indirect visual information of the boiling phenomenon, without any prior knowledge of the governing equations, which enables the non-intrusive measurement of heat flux based on boiling process imaging. It is shown that neural networks can encode bubble morphology and its correlation with heat flux returning errors as low as 7% when compared with precise experimental measurements, a significant improvement over current prediction methods of boiling heat transfer. Furthermore, it is shown that these systems may be implemented in inexpensive, compact computers, such as the Raspberry Pi, to infer heat flux in real time from visualization.",industry
10.1016/j.mfglet.2019.05.003,Journal,Manufacturing Letters,scopus,2019-04-01,sciencedirect,A blockchain enabled Cyber-Physical System architecture for Industry 4.0 manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85066168835,"Cyber-Physical Production Systems (CPPSs) are complex manufacturing systems which aim to integrate and synchronize machine world and manufacturing facility to the cyber computational space. However, having intensive interconnectivity and a computational platform is crucial for real-world implementation of CPPSs. In this paper, the potential impacts of blockchain technology in development and realization of real-world CPPSs are discussed. A unified three-level blockchain architecture is proposed as a guideline for researchers and industries to clearly identify the potentials of blockchain and adapt, develop, and incorporate this technology with their manufacturing developments towards Industry 4.0.",industry
10.1016/j.compag.2019.02.023,Journal,Computers and Electronics in Agriculture,scopus,2019-04-01,sciencedirect,Real-time nondestructive monitoring of Common Carp Fish freshness using robust vision-based intelligent modeling approaches,https://api.elsevier.com/content/abstract/scopus_id/85062029959,"In the current research, the potential of a novel method based on the artificial neural network was investigated to diagnose the freshness of common carp (Cyprinus carpio) during ice storage. Fish as an aquaculture product has high nutrients and low-fat content. So, people have consumed it as a safe and high-value foodstuff in their daily diet. Investigation of fish freshness is proposed as a significant issue in the aquaculture industry since fish spoils rapidly. The applied system of this study is comprised of the following steps: First, images of samples were captured and the pre-processing operation was done on the images. Then, particular channels including R, G, B, H, S, I, L*, a*, and b* were computed. Next, feature extraction was performed to obtain 6 types of texture features from each channel. Afterward, the hybrid Artificial Bee Colony-Artificial Neural Network (ABC-ANN) algorithm was applied to select the best features. Finally, the Support Vector Machine (SVM), K-Nearest Neighbor (K-NN) and Artificial Neural Network (ANN) algorithms as the most common methods were used to classify fish images. The best performance of the K-NN classifier was calculated in the k = 8 neighborhood size with the accuracy of 90.48. The best kernel function for the SVM algorithm was polynomial with C, sigma, and accuracy of 1, 2 and 91.52 percent, respectively. In this system, the input layer has consisted of 22 neurons based on the feature selection operation and 4 classes including most fresh, fresh, fairly fresh and spoiled have been used as the number of output layer. At the end, the best results of the MLP networks were achieved by LM learning algorithm and 6 neurons in the hidden layer with the 22–10–4 topology and accuracy of 93.01 percent. The achieved results demonstrate the high performance of the ANN classifier for evaluation of common carp freshness during ice storage as a rapid, accurate, non-destructive, real-time and automated method. It shows the potential of computer vision method in combination with artificial neural networks as an intelligent technique for evaluation of fish freshness.",industry
10.1016/j.compeleceng.2018.03.015,Journal,Computers and Electrical Engineering,scopus,2019-03-01,sciencedirect,BCI cinematics – A pre-release analyser for movies using H <inf>2</inf> O deep learning platform,https://api.elsevier.com/content/abstract/scopus_id/85046107707,"Entertainment industry has seen a phenomenal growth throughout the globe in recent times and movie industry enjoys a crucial role in the above emergence. A movie can capture the attention of a viewer and can trigger cognitive and emotional processes in the brain. In this article we assess the emotional outcome of the viewer while they watch the movie before its actual release that is, during its preview. Traditionally FMRI was used to assess the activity of brain but proved to be non-feasible and costly so we used EEG Sensors to monitor and record the functioning of the brain of movie viewer for further analysis. The collected data through EEG sensor were analysed using deep learning framework. H2O package of deep learning was employed to find high and low of different brain waves mapping to the emotions depicted in the every scene of the movie. Our proposed system named BCI cinematics obtained 85% accuracy and results were validated by obtaining the feedback from the stake holders. The outcome of this work will assist the creators to understand the emotional impact of movie over a normal viewer impartially thus enable them to modify certain scenes or change sequence of scenes and so on. When deployed in real time our system prove to be a cost saver for movie makers.",industry
10.1016/j.future.2018.02.011,Journal,Future Generation Computer Systems,scopus,2019-03-01,sciencedirect,Collaborative prognostics in Social Asset Networks,https://api.elsevier.com/content/abstract/scopus_id/85042391186,"With the spread of Internet of Things (IoT) technologies, assets have acquired communication, processing and sensing capabilities. In response, the field of Asset Management has moved from fleet-wide failure models to individualised asset prognostics. Individualised models are seldom truly distributed, and often fail to capitalise the processing power of the asset fleet. This leads to hardly scalable machine learning centralised models that often must find a compromise between accuracy and computational power. In order to overcome this, we present a novel theoretical approach to collaborative prognostics within the Social Internet of Things. We introduce the concept of Social Asset Networks, defined as networks of cooperating assets with sensing, communicating and computing capabilities. In the proposed approach, the information obtained from the medium by means of sensors is synthesised into a Health Indicator, which determines the state of the asset. The Health Indicator of each asset evolves according to an equation determined by a triplet of parameters. Assets are given the form of the equation but they ignore their parametric values. To obtain these values, assets use the equation in order to perform a non-linear least squares fit of their Health Indicator data. Using these estimated parameters, they are interconnected to a subset of collaborating assets by means of a similarity metric. We show how by simply interchanging their estimates, networked assets are able to precisely determine their Health Indicator dynamics and reduce maintenance costs. This is done in real time, with no centralised library, and without the need for extensive historical data. We compare Social Asset Networks with the typical self-learning and fleet-wide approaches, and show that Social Asset Networks have a faster convergence and lower cost. This study serves as a conceptual proof for the potential of collaborative prognostics for solving maintenance problems, and can be used to justify the implementation of such a system in a real industrial fleet.",industry
10.1016/j.enbuild.2018.12.034,Journal,Energy and Buildings,scopus,2019-02-15,sciencedirect,"IntelliMaV: A cloud computing measurement and verification 2.0 application for automated, near real-time energy savings quantification and performance deviation detection",https://api.elsevier.com/content/abstract/scopus_id/85059816255,"Energy conservation measures (ECMs) are implemented in all sectors with the objective of improving the efficiency with which energy is consumed. Measurement and verification (M&V) is required to verify the performance of every ECM to ensure its successful implementation and operation. The methodologies implemented to achieve this are currently evolving to a more dynamic state, known as measurement and verification 2.0, through the use of automated and advanced analytics. The primary barrier to the adoption of M&V 2.0 practices are the tools available to practitioners. This paper aims to populate the knowledge gap in the industrial buildings sector by presenting a novel cloud computing-based application, IntelliMaV, that applies advanced machine learning techniques on large datasets to automatically verify the performance of ECMs in near real-time. Additionally, a performance deviation detection system is incorporated, ensuring persistence of savings beyond the typical period of analysis in M&V.
                  IntelliMaV allows M&V practitioners to quantify energy savings with minimum levels of uncertainty by applying powerful analytics to data readily available in industrial facilities. The use of a cloud computing-based architecture reduces the resources required on-site and decreases the time required to train the baseline energy model through the use of parallel processing. The robust nature of the application ensures it is applicable across the broad spectrum of ECMs in the industrial buildings sector. A case study carried out in a large biomedical manufacturing facility demonstrates the ease of use of the application and the benefits realised through its adoption. The energy savings from an ECM were calculated to be 2,353,225 kWh/yr with 25.5% uncertainty at a 90% confidence interval.",industry
10.1016/j.oceaneng.2019.01.003,Journal,Ocean Engineering,scopus,2019-02-01,sciencedirect,Data management for structural integrity assessment of offshore wind turbine support structures: data cleansing and missing data imputation,https://api.elsevier.com/content/abstract/scopus_id/85061324147,"Structural Health Monitoring (SHM) and Condition Monitoring (CM) Systems are currently utilised to collect data from offshore wind turbines (OWTs), to enhance the accurate estimation of their operational performance. However, industry accepted practices for effectively managing the information that these systems provide have not been widely established yet. This paper presents a four-step methodological framework for the effective data management of SHM systems of OWTs and illustrates its applicability in real-time continuous data collected from three operational units, with the aim of utilising more complete and accurate datasets for fatigue life assessment of support structures. Firstly, a time-efficient synchronisation method that enables the continuous monitoring of these systems is presented, followed by a novel approach to noise cleansing and the posterior missing data imputation (MDI). By the implementation of these techniques those data-points containing excessive noise are removed from the dataset (Step 2), advanced numerical tools are employed to regenerate missing data (Step 3) and fatigue is estimated for the results of these two methodologies (Step 4). Results show that after cleansing, missing data can be imputed with an average absolute error of 2.1%, while this error is kept within the [+ 15.2%−11.0%] range in 95% of cases. Furthermore, only 0.15% of the imputed data fell outside the noise thresholds. Fatigue is found to be underestimated both, when data cleansing does not take place and when it takes place but MDI does not. This makes this novel methodology an enhancement to conventional structural integrity assessment techniques that do not employ continuous datasets in their analyses.",industry
10.1016/j.therap.2018.12.002,Journal,Therapie,scopus,2019-02-01,sciencedirect,"Early access to health products in France: Major advances of the French “Conseil stratégique des industries de santé” (CSIS) to be implemented (modalities, regulations, funding)",https://api.elsevier.com/content/abstract/scopus_id/85061149651,"In a context of perpetual evolution of treatments, access to therapeutic innovation is a major challenge for patients and the various players involved in the procedures of access to medicines. The revolutions in genomic and personalized medicine, artificial intelligence and biotechnology will transform the medicine of tomorrow and the organization of our health system. It is therefore fundamental that France prepares for these changes and supports the development of its companies in these new areas. The recent “Conseil stratégique des industries de santé” launched by Matignon makes it possible to propose a regulatory arsenal conducive to the implementation and diffusion of therapeutic innovations. In this workshop, we present a number of proposals, our approach having remained pragmatic with a permanent concern to be effective in the short term for the patients and to simplify the procedures as much as possible. This was achieved thanks to the participation in this workshop of most of the players involved (industrial companies, “Agence nationale de sécurité du médicament et des produits de santé”, “Haute Autorité de santé”, “Institut national du cancer”, “Les entreprises du médicament”, hospitals, “Observatoire du médicament, des dispositifs médicaux et de l’innovation thérapeutique”…). The main proposals tend to favor the implementation of clinical trials on our territory, especially the early phases, a wider access to innovations by favoring early access programs and setting up a process called “autorisation temporaire d’utilisation d’extension” (ATUext) that make it possible to prescribe a medicinal product even if the latter has a marketing authorisation in another indication. In addition, we propose a conditional reimbursement that will be available based on preliminary data but will require re-evaluation based on consolidated data from clinical trials and/or real-life data. Finally, in order to better carry out these assessments, with a view to access or care, we propose the establishment of partnership agreements with health agencies/hospitals in order to encourage the emergence of field experts, in order to prioritize an ascending expertise closer to patients’ needs and to real life.",industry
10.1016/j.cie.2018.08.018,Journal,Computers and Industrial Engineering,scopus,2019-02-01,sciencedirect,Ensemble-based big data analytics of lithofacies for automatic development of petroleum reservoirs,https://api.elsevier.com/content/abstract/scopus_id/85052098750,"Big data-driven ensemble learning is explored in this paper for quantitative geological lithofacies modeling, which is an integral and challenging part of petroleum reservoir development and characterization. Quantitative lithofacies modeling involves detection and recognition of underlying subsurface rock’s lithofacies. It requires real-time data acquisition, handling, storage, conditioning, analysis, and interpretation of raw sensory petroleum logging data. The real-time well-logs data collected from the sensor-based tools suffer from complications such as noise, nonlinearity, imbalance, and high-dimensionality which makes the prediction task more challenging. The existing literature on quantitative lithofacies modeling includes several data-driven techniques ranging from conventional well-logs to artificial intelligence (AI). Recently, multiple classifiers based Ensemble learners have been found to be more robust and reliable paradigms for detection and identification tasks in various machine learning applications, however, these are not well embraced in the petroleum industry. Ensemble methodology combines diverse expert’s opinions to obtain overall ensemble decision which in turn reduces the risk of a wrong decision. Thus, the uncertainties associated with complex reservoir data can be better handled by the use of Ensemble learners than the existing single learner based conventional models. Ensemble-based big data analytics, proposed in the paper, includes development and comparative performance testing of five popular ensemble methods (viz. Bagging, AdaBoost, Rotation forest, Random subspace, and DECORATE) for quantitative lithofacies modeling. Seven state-of-the-art base classifiers were used as members of different Ensemble learners for the analysis of Kansas (U.S.A.) oil-field data. The proposed techniques have been implemented on the widely used WEKA platform. The comparative performance analysis of the proposed techniques, presented in the paper, confirms its supremacy over the existing techniques used for quantitative lithofacies modeling.",industry
10.1016/j.matpr.2020.03.363,Conference Proceeding,Materials Today: Proceedings,scopus,2019-01-01,sciencedirect,Real-time Thermal Error Compensation Strategy for Precision Machine tools,https://api.elsevier.com/content/abstract/scopus_id/85085555603,"Present manufacturing trend is towards producing precision components with better accuracy. Machine errors like geometrical, thermal and process errors affect the component accuracy. Among these errors, thermal error contributes more than 50-60% of the total machining error. This paper mainly focuses on the development of a real-time thermal error compensation module for precision machine tools and talks about effective modeling of thermal errors, development of thermal error compensation model using feed-forward backpropagation neural network and also simplified model using regression analysis technique, algorithm development for real-time compensation and implementation of module onto the open architecture CNC controller. The developed module has been successfully tested on a Diamond Turning Machine (DTM) by machining the precision component and also verified the effectiveness of the module",industry
10.1016/j.promfg.2020.01.033,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Deep learning-based production forecasting in manufacturing: A packaging equipment case study,https://api.elsevier.com/content/abstract/scopus_id/85083533827,"We propose a Deep Learning (DL)-based approach for production performance forecasting in fresh products packaging. On the one hand, this is a very demanding scenario where high throughput is mandatory; on the other, due to strict hygiene requirements, unexpected downtime caused by packaging machines can lead to huge product waste. Thus, our aim is predicting future values of key performance indexes such as Machine Mechanical Efficiency (MME) and Overall Equipment Effectiveness (OEE). We address this problem by leveraging DL-based approaches and historical production performance data related to measurements, warnings and alarms. Different architectures and prediction horizons are analyzed and compared to identify the most robust and effective solutions. We provide experimental results on a real industrial case, showing advantages with respect to current policies implemented by the industrial partner both in terms of forecasting accuracy and maintenance costs. The proposed architecture is shown to be effective on a real case study and it enables the development of predictive services in the area of Predictive Maintenance and Quality Monitoring for packaging equipment providers.",industry
10.1016/j.promfg.2020.01.031,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,A deep learning approach for anomaly detection with industrial time series data: A refrigerators manufacturing case study,https://api.elsevier.com/content/abstract/scopus_id/85083532061,"In refrigerators production, vacuum creation is fundamental to guarantee the correct manufacturing of the product. Before inserting the refrigerant in the refrigerator cabinet, the vacuum is tested through a Pirani gauge that assesses the pressure within the cabinet. Such readings are used to evaluate the vacuum creation process and to verify if leakings are present. In this work, we employ a Deep Learning-based Anomaly Detection approach to associate an Anomaly Score to each pressure profile; this score can be exploited to optimize actions performed by human operators like more detailed inspections or unit exclusion from the downstream production stages. We propose a native time series-based approach based on Deep Learning and compare it with classic ones based on hand-craft features. The proposed approach is designed to be deployed in a Decision Support System for assisting human operators in the following testing operations, helping them in reducing evaluation bias and attention losses that are inevitable in production line environment. Moreover, costs associated with false positives (normally operating units detected as anomalous) and false negatives (undetected anomalies) are considered here to optimize decision making in a cost-reduction perspective. We also describe promising results obtained on real industrial data spanning on a 5-month period and consisting of thousands of tested household units.",industry
10.1016/j.promfg.2020.01.333,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Prognostic health management of production systems. New proposed approach and experimental evidences,https://api.elsevier.com/content/abstract/scopus_id/85082764769,"Prognostic Health Management (PHM) is a maintenance policy aimed at predicting the occurrence of a failure in components and consequently minimizing unexpected downtimes of complex systems. Recent developments in condition monitoring (CM) techniques and Artificial Intelligence (AI) tools enabled the collection of a huge amount of data in real-time and its transformation into meaningful information that will support the maintenance decision-making process. The emerging Cyber-Physical Systems (CPS) technologies connect distributed physical systems with their virtual representations in the cyber computational world. The PHM assumes a key role in the implementation of CPS in manufacturing contexts, since it allows to keep CPS and its machines in proper conditions. On the other hand, CPS-based PHM provide an efficient solution to maximize availability of machines and production systems. In this paper, evolving and unsupervised approaches for the implementation of PHM at a component level are described, which are able to process streaming data in real-time and with almost-zero prior knowledge about the monitored component. A case study from a real industrial context is presented. Different unsupervised and online anomaly detection methods are combined with evolving clustering models in order to detect anomalous behaviours in streaming vibration data and integrate the so-generated knowledge into supervised and adaptive models; then, the degradation model for each identified fault is built and the resulting RUL prediction model integrated into the online analysis. Supervised methods are applied to the same dataset, in batch mode, to validate the proposed procedure.",industry
10.1016/B978-0-12-409547-2.00449-2,Book,Encyclopedia of Analytical Science,scopus,2019-01-01,sciencedirect,Quality assurance | laboratory information management systems,https://api.elsevier.com/content/abstract/scopus_id/85079080236,"In today’s competitive laboratory environment, managers are under increased pressure to deliver high quality data, quickly, and cost effectively, with limited resources. This can only be achieved via laboratory automation. It is more critical than ever to ensure that the modern laboratory is equipped with the latest software Laboratory Information Management Software (LIMS) and tools together with automation technology to ensure that they remain competitive. LIMS together with laboratory automation (positive ID, Robotics, AI, etc.) imparts many benefits that include time savings, resource maximization, efficiency, quality improvements, along with cost reductions. For all businesses that rely on delivering high quality, reliable products, regardless of industry, defects can be responsible for huge losses, from laboratory/company reputation to associated costs of recalls and possibly lawsuits. Compared to manual procedures, automated tasks offer significant benefits which include, reproducibility, increased accuracy, speed (high throughput), enhanced communication, increased responsiveness, automated and effective reporting that results in higher customer satisfaction. Today, LIMS can be delivered on-demand via the Software as a Service (SaaS) model for organizations that either do not have the infrastructure to host the software or who find it more cost effective to utilize the tools hosted in the cloud, eliminating the need for an IT resource. Organizations that employ best practices and implement LIMS have all of their laboratory operational data in a centralized, secure database greatly facilitating access to real-time data, KPIs, document control, quality management as well as regulatory compliance.",industry
10.1016/j.procir.2019.05.017,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,The growing path in search of an industrial design identity,https://api.elsevier.com/content/abstract/scopus_id/85076752868,"Knowing that the education system must be reinvented periodically to face the changes of social and cultural paradigm, was reviewed the pedagogical organization of a set of disciplines of an industrial design course that were in operation for a decade. Thus, in view of the objective of restructuring the disciplinary group of industrial design, a new structure has been developed and implemented that could offer students the opportunity to explore problems and challenges that have real applications, increasing the possibility of acquiring competences effectively needed to practice the profession of designer.
                  This restructuring had as its starting point the concept of Project-based learning, which is designated as student-centered pedagogy that involves a dynamic classroom approach in which it is believed that students acquire a deeper knowledge through active exploration of real-world challenges and problems. Consequently, resulting in a learning process organized into levels with increasing degree of complexity. As well, different assimilations of markets and design scenarios.
                  Starting from the first year of the course, where students are still understanding the context of industrial design and its potentialities. At a time when their techniques, principles and methods are still very raw and basic. They are initiated in a LOW-ID and local industry context, to acquire basic skills. The second year allows embark on an intermediate level called MID-ID, with new skills in international brands approach. In the last year of the course the 3rd level is reached, HIGH-ID, with projects with the national industry.
                  The first year of implementation of this curriculum structure showed good results. Thus, favoring a solid interdisciplinary formation with, skills and competences that allow future designers to intervene creatively and competently in a variety of fields. This process allows to progress to the next academic degree to complete and validate the entire formation of the student.",industry
10.1016/j.procir.2019.03.212,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,Contribution to the development of a Digital Twin based on product lifecycle to support the manufacturing process,https://api.elsevier.com/content/abstract/scopus_id/85076726437,"The current manufacture challenges are closely linked to the aim of digitalizing the product, the process and the means of production. In such aspects, information about the production processes is available in real-time, allowing managers to act on digital models and, through them, apply decisions in real systems. Thus, having a mirror model or a Digital Twin enables real-time absorption, simulation and implementation of manufacturing variations from the real environment, allowing faster detection of physical problems, and faster production response. The Digital Twin is a virtual representation of the physical system, which is equipped with sensors and actuators and feed the digital system, where the monitoring of data and simulation of variations, for instance, take place. From the synchronized interactions of both components, it is possible to deliver the mentioned faster production responses. Brazilian and German universities joined efforts to develop a Digital Twin based on product lifecycle to support the Manufacturing Process to address these challenges. The proposed Digital Twin seeks to integrate the product twin and the twin of its development process. It shall represent the manufacturing process, enabling the monitoring and optimization of the real production process. The Digital Twin itself is addressed as a product inside the production system and, therefore, its development process will follow the product lifecycle perspective, from the conception and planning to its implementation and usage. The Digital Twin will be further improved with the introduction of Artificial Intelligence tools, characterizing a Smart Digital Twin of the Manufacturing Process. Thus, this paper aims to present the concepts of a research project that is being developed in a joint Brazilian-German Cooperative Research.",industry
10.1016/j.ifacol.2019.09.143,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,Machine Learning approaches for Anomaly Detection in Multiphase Flow Meters,https://api.elsevier.com/content/abstract/scopus_id/85076262725,"Multiphase Flow Meters (MPFM) are important metering tools in the oil and gas industry. A MPFM provides real-time measurements of gas, oil and water flows of a well without the need to separate the phases, a time-consuming procedure that has been classically adopted in the industry. Evaluating the composition of the flow is fundamental for the well management and productivity prediction; therefore, procedures for measuring quality assessment are of crucial importance. In this work we propose an Anomaly Detection approach to MPFM that is effectively able to hand the complexity and variability associated with MPFM data. The proposed approach is designed for embedded implementation and it exploits unsupervised Anomaly Detection approaches like Cluster Based Local Outlier Factor and Isolation Forest.",industry
10.1016/j.ifacol.2019.08.225,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,Curriculum change for graduate-level control engineering education at the Universidad Pontificia Bolivariana,https://api.elsevier.com/content/abstract/scopus_id/85076258553,"This paper addresses the graduate-level control engineering curriculum change performed at the Universidad Pontificia Bolivariana (UPB), Medellin, Colombia. New proposed methodologies include active learning activities using a new multipurpose experimental test bed that was developed with industrial components. The renovated graduate-level control engineering related courses include: Continuous Processes, Discrete Processes, Fuzzy Logic, Neural Networks and Genetic Algorithms, Linear Control, Nonlinear Control, and Optimal Estimation. The new experimental station was developed for teaching, research, and industrial training activities for the School of Engineering at the UPB. In this work, we report the use of the station in an Optimal Estimation course to replace a traditional homework/exams evaluation approach with an applied work that required independent study, the implementation of different observers in a real lab-scale industrial plant, and a paper-style written report. Increasing independent study activities resulted in academic discussions that are valuable for the learning process of the student. The use of the experimental station and the real comparison of estimation algorithms, implemented by using industrial controllers and high-level programming environments, provided the student skills that cannot be acquired by using only simulations in which real implementation restrictions/challenges do not appear. This work represents one of the first approaches for the implementation of the new curriculum model at the UPB for graduate education. The methodology used in the Optimal Estimation class promoted independent learning, critical thinking and writing skills through significant learning activities.",industry
10.1016/j.procs.2019.09.169,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,IAssistMe - Adaptable assistant for persons with eye disabilities,https://api.elsevier.com/content/abstract/scopus_id/85076257910,"Visually challenged people may experience certain difficulties in their daily interaction with technology. That is essentially because the main way to exchange and process information is by written text, images or videos. Since the basic purpose of innovation is to improve people’s lifestyle, in this paper we propose a system that can make technology accessible to a broader group. Our prototype is presented as a mobile application based on vocal interaction, which can help people facing visual disorders consult their personal agenda, create an event, invite other friends to attend it, check the weather in certain areas and many other day-to-day tasks. Regarding the implementation, the project consists of a mobile application that interacts with a cloud based system, which makes it reliable and low in latency due to the resource availability in multiple global regions, provided by the newly emerging platform used in building the infrastructure. The novelty of the system lays in the highly flexible serverless architecture [1] that is open to extension and closed to modification through the set of autonomous cloud processing methods that sustain the base of the functionality. This distributed processing approach guarantees that the user always receives a response from his personal assistant, either by using artificial intelligence context generated phrases, by real-time cloud function processing or by fallback to the training answers.",industry
10.1016/j.procs.2019.09.069,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,An Innovative Technology: Augmented Reality Based Information Systems,https://api.elsevier.com/content/abstract/scopus_id/85076255225,"In our generation the information systems evolve with new technologies: augmented reality (AR), IoT, artificial intelligence, blockchain etc. Anymore they perform information exchange by sensors. It is estimated that the systems will be in a state of extreme interaction and reach 50 billion devices connected in Internet in 2020. We know that everything around us will be in interaction and they will do everything without any need of human interference. For example, when our dishwasher is full, it will start to wash automatically, or when the run out of the gasoline, our car will drive to the nearest station, or even when a burglar is entered to our house, it will automatically be detected and be announced to the police office. In business life, the processes will be automatical in maximum level and this technology will increase productivity and efficiency. Next to mobile technology, it is thought that these new generation information systems (IS) will take the biggest place in our lives. AR also will be integrated to these systems to augment the information in real world. Humanity will augment its habitat in an innovative way thanks to these AR based IS. This paper surveys the current state-of-the-art AR systems related with aerospace & defense, industry, education, medical and gaming sectors. The connection of AR based IS and innovation is explained with a technological insight. In addition to international use cases HAVELSAN’s use cases are also given that are performed from the aspect of applied open innovation strategy. This strategy is addressed specific to the implemented activities of AR based IS.",industry
10.1016/j.promfg.2018.12.017,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,AI based injection molding process for consistent product quality,https://api.elsevier.com/content/abstract/scopus_id/85072584818,"In manufacturing processes, Injection Molding is widely used for producing plastic components with large lot size. So, continuous improvements in product quality consistency is crucial to maintaining a competitive edge in the injection molding industry. Various optimization techniques like ANN, GA, Iterative method, and simulation based are being used for optimization of Injection Molding process and obtaining optimal processing conditions. But still due to variation during molding cycles, quality failure occurs. As many constituents like process, Material, machine together yields product quality. This paper is focused on Real time AI based control of process parameters in injection molding cycle. Process parameters and their interrelationship with quality failure has been studied and later supposed to be used to generate algorithm for compensating the deviation of process parameters. Pressure and temperature sensor assisted monitoring system is used to collect data in real time and based on its comparison with the standard values an interrelationship is formed between parameters and plastic material properties. Algorithm generates new process parameter values to compensate the deviation and machine control follows the same. The entire process is supposed to be smart and automatic after being trained with AI and machine learning techniques. Simulation using Moldflow software and real industry collected data has been used for understanding whole molding process establishing relationship between failure and parameters. An automotive product in real industry is chosen for data acquisition, implementation and validation of entire AI based system.",industry
10.1016/j.promfg.2018.12.026,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Hybrid artificial intelligence system for the design of highly-automated production systems,https://api.elsevier.com/content/abstract/scopus_id/85072561400,"The automated design of production systems is a young field of research which has not been widely explored by industry nor research in recent decades. Currently, the effort spent in production system design is increasing significantly in automotive industry due to the number of product variants and product complexity. Intelligent methods can support engineers in repetitive tasks and give them more opportunity to focus on work which requires their core competencies. This paper presents a novel artificial intelligence methodology that automatically generates initial production system configurations based on real industrial scenarios in the automotive field of body-in-white production. The hybrid methodology reacts flexibly against data sets of different content and has been implemented in a software prototype.",industry
10.1016/j.procs.2019.04.090,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,Deep neural network method of recognizing the critical situations for transport systems by video images,https://api.elsevier.com/content/abstract/scopus_id/85071926362,"The deep neural network method of recognizing critical situations for transport systems according to video frames from the intelligent vehicles cameras is offered, that is effective in terms of accuracy and high-speed performance. Unlike the known solutions for the objects and normal or critical situations detection and recognition, it uses the classification with the subsequent reinforcement on the basis of several video stream frames and with the automatic annotation algorithm. The adapted architectures of neural networks are offered: the dual network to identify drivers and passengers according to the face image, the network with independent recurrent layers to classify situations according to the video fragment. The scheme of the intellectual distributed city system of transport safety using the cameras and on-board computers united in a single network is offered. Software modules in Python are developed and natural experiments are made. The possibility of the offered algorithms and programs in UGV or in the driver assistant systems implementation is shown with the illustrating examples in real-time.",industry
10.1016/j.promfg.2020.01.288,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Action recognition in manufacturing assembly using multimodal sensor fusion,https://api.elsevier.com/content/abstract/scopus_id/85070765380,"Production innovations are occurring faster than ever. Manufacturing workers thus need to frequently learn new methods and skills. In fast changing, largely uncertain production systems, manufacturers with the ability to comprehend workers’ behavior and assess their operation performance in near real-time will achieve better performance than peers. Action recognition can serve this purpose. Despite that human action recognition has been an active field of study in machine learning, limited work has been done for recognizing worker actions in performing manufacturing tasks that involve complex, intricate operations. Using data captured by one sensor or a single type of sensor to recognize those actions lacks reliability. The limitation can be surpassed by sensor fusion at data, feature, and decision levels. This paper presents a study that developed a multimodal sensor system and used sensor fusion methods to enhance the reliability of action recognition. One step in assembling a Bukito 3D printer, which composed of a sequence of 7 actions, was used to illustrate and assess the proposed method. Two wearable sensors namely Myo-armband captured both Inertial Measurement Unit (IMU) and electromyography (EMG) signals of assembly workers. Microsoft Kinect, a vision based sensor, simultaneously tracked predefined skeleton joints of them. The collected IMU, EMG, and skeleton data were respectively used to train five individual Convolutional Neural Network (CNN) models. Then, various fusion methods were implemented to integrate the prediction results of independent models to yield the final prediction. Reasons for achieving better performance using sensor fusion were identified from this study.",industry
10.1016/j.procir.2019.03.041,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,"Design, implementation and evaluation of reinforcement learning for an adaptive order dispatching in job shop manufacturing systems",https://api.elsevier.com/content/abstract/scopus_id/85068485505,"Modern production systems tend to have smaller batch sizes, a larger product variety and more complex material flow systems. Since a human oftentimes can no longer act in a sufficient manner as a decision maker under these circumstances, the demand for efficient and adaptive control systems is rising. This paper introduces a methodical approach as well as guideline for the design, implementation and evaluation of Reinforcement Learning (RL) algorithms for an adaptive order dispatching. Thereby, it addresses production engineers willing to apply RL. Moreover, a real-world use case shows the successful application of the method and remarkable results supporting real-time decision-making. These findings comprehensively illustrate and extend the knowledge on RL.",industry
10.1016/j.promfg.2019.03.047,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,A Practical Approach of Teaching Digitalization and Safety Strategies in Cyber-Physical Production Systems,https://api.elsevier.com/content/abstract/scopus_id/85065658005,"Digitalization strategies in cyber-physical production systems (CPPS) are one of the key factors of Industry 4.0. The topic not only addresses data preparation, real-time data processing, big data analytics, visualization and machine interface design but also cyber security and safety. Especially, unauthorized access to protected (personal or enterprise) data or unauthorized control of production facilities imply risks when it comes to digitalization. Because of the increased complexity of state-of-the-art technologies, educational institutions need to provide practice-oriented teaching methods in learning factories to help engineers of today understand the impact of those developments.
                  In the light of this fact, this paper presents a practical approach of teaching digitalization strategies in CPPS. Planning, implementing and impacts of digitalization strategies are taught on a use-case with human-robot-collaboration. The objective of the use-case is to realize a real-time obstacle avoidance approach for a collaborative application based on a local positioning system. Here, students not only learn how to model the kinematics of a robot and program a robot but also how to design machine interfaces for real-time data transfer and processing as well as impacts of digitalization on safety and security.
                  The implementation of the use-case is part of the TU Wien teaching portfolio and thus part of its learning factory, where students and apprentices have the possibility to experiment and gain experiences by deliberate error simulations.",industry
10.1016/j.procir.2019.02.101,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,Autonomous order dispatching in the semiconductor industry using reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85065424368,"Cyber Physical Production Systems (CPPS) provide a huge amount of data. Simultaneously, operational decisions are getting ever more complex due to smaller batch sizes, a larger product variety and complex processes in production systems. Production engineers struggle to utilize the recorded data to optimize production processes effectively because of a rising level of complexity. This paper shows the successful implementation of an autonomous order dispatching system that is based on a Reinforcement Learning (RL) algorithm. The real-world use case in the semiconductor industry is a highly suitable example of a cyber physical and digitized production system.",industry
10.1016/j.cirpj.2018.12.002,Journal,CIRP Journal of Manufacturing Science and Technology,scopus,2019-01-01,sciencedirect,"From factory floor to process models: A data gathering approach to generate, transform, and visualize manufacturing processes",https://api.elsevier.com/content/abstract/scopus_id/85058703955,"The need for tools to help guide decision making is growing within the manufacturing industry. The analysis performed by these tools will help operators and engineers to understand the behaviour of the manufacturing stations better and thereby take data-driven decisions to improve them. The tools use techniques borrowed from fields such as Data Analytics, BigData, Predictive Modelling, and Machine Learning. However, to be able to use these tools efficiently, data from the factory floor is required as input. This data needs to be extracted from two sources, the PLCs, and the robots. In practice, methods to extract usable data from robots are rather scarce. The present work describes an approach to capture data from robots, which can be applied to both legacy and current state-of-the-art manufacturing systems. The described approach is developed using Sequence Planner – a tool for modelling and analyzing production systems – and is currently implemented at an automotive company as a pilot project to visualize and examine the ongoing process. By exploiting the robot code structure, robot actions are converted to event streams that are abstracted into operations. We then demonstrate the applicability of the resulting operations, by visualizing the ongoing process in real-time as Gantt charts, that support the operators performing maintenance. And, the data is also analyzed off-line using process mining techniques to create a general model that describes the underlying behaviour existing in the manufacturing station. Such models are used to derive insights about relationships between different operations, and also between resources.",industry
10.1016/j.impact.2018.12.001,Journal,NanoImpact,scopus,2019-01-01,sciencedirect,SUNDS probabilistic human health risk assessment methodology and its application to organic pigment used in the automotive industry,https://api.elsevier.com/content/abstract/scopus_id/85058641247,"The increasing use of engineered nanomaterials (ENMs) in nano-enabled products (NEPs) has raised societal concerns about their possible health and ecological implications. To ensure a high level of human and environmental protection it is essential to properly estimate the risks of these new materials and to develop adequate risk management strategies. To this end, we propose a quantitative Human Health Risk Assessment (HHRA) methodology, which was developed in the European Seventh Framework research project SUN (Sustainable Nanotechnologies) and implemented in the web-based SUN Decision Support System (SUNDS). One of the major strengths of this probabilistic approach as compared to its deterministic alternatives is its ability to clearly communicate the uncertainties in the estimated risks in order to support better risk communication for more objective decision making by industries and regulators.
                  To demonstrate this methodology, we applied it in a real case study involving a nanoscale organic red pigment used in the automotive industry. Our analysis clearly showed that the main source of uncertainty was the extrapolation from (sub)acute in vivo toxicity data to long-term risk. This extrapolation was necessary due to a lack of (sub)chronic in vivo studies for the investigated nanomaterial. Despite the high uncertainty in the final results due to the conservative assumptions made in the risks assessment, the estimated risks are acceptable for all investigated exposure scenarios along the product lifecycle.",industry
10.1016/j.ijepes.2018.07.022,Journal,International Journal of Electrical Power and Energy Systems,scopus,2019-01-01,sciencedirect,Design and implementation of flexible Numerical Overcurrent Relay on FPGA,https://api.elsevier.com/content/abstract/scopus_id/85050986246,"This paper presents the contemporary design and implementation of an intelligent revelation in the field of the over-current relay to meet the challenges of the modern grid. The unique three-neuron single layered architecture of Artificial Neural Network (ANN) provides flexibility by exploiting its universal function approximation capabilities. The Unique Selling Proposition (USP) of the present development is the simple design of ANN, suitable for low-end, low-cost Field Programmable Gate Array (FPGA) implementation. The nano-scaled internal processing time for three-phase design, with the provision of remotely controlled adaptive relay settings, would definitely an innovative solution for grid connection of renewable energy sources. The proposed design of the universal over-current relay, confirmed by the real-time testing, is a true fusion of electrical power, communication and information technology to meet the global trend of the electrical power industries.",industry
10.1016/j.powtec.2018.08.064,Journal,Powder Technology,scopus,2018-11-01,sciencedirect,"Settling velocity of drill cuttings in drilling fluids: A review of experimental, numerical simulations and artificial intelligence studies",https://api.elsevier.com/content/abstract/scopus_id/85052516468,"In this paper, a comprehensive review of experimental, numerical and artificial intelligence studies on the subject of cuttings settling velocity in drilling muds made by researchers over the last seven decades is brought to the fore. In this respect, 91 experimental, 13 numerical simulations and 7 artificial intelligence researches were isolated, reviewed, tabulated and discussed. A comparison of the three methods and the challenges facing each of these methods were also reviewed. The major outcomes of this review include: (1) the unanimity among experimental researchers that mud rheology, particle size and shape and wall effect are major parameters affecting the settling velocity of cuttings in wellbores; (2) the prevalence of cuttings settling velocity experiments done with the mud in static conditions and the wellbore in the vertical configuration; (3) the extensive use of rigid particles of spherical shape to represent drill cuttings due to their usefulness in experimental visualization, particle tracking, and numerical implementation; (4) the existence of an artificial intelligence technique - multi-gene genetic programming (MGGP) which can provide an explicit equation that can help in predicting settling velocity; (5) the limited number of experimental studies factoring in the effect of pipe rotation and well inclination effects on the settling velocity of cuttings and (6) the most applied numerical method for determining settling velocity is the finite element method. Despite these facts, there is need to perform more experiments with real drill cuttings and factor in the effects of conditions such as drillstring rotation and well inclination and use data emanating therefrom to develop explicit models that would include the effects of these. It should be noted however, that the aim of this paper is not to create an encyclopaedia of particle settling velocity research, but to provide to the researcher with a basic, theoretical, experimental and numerical overview of what has so far been achieved in the area of cuttings settling velocity in drilling muds.",industry
10.1016/j.petrol.2018.06.072,Journal,Journal of Petroleum Science and Engineering,scopus,2018-11-01,sciencedirect,Data driven model for sonic well log prediction,https://api.elsevier.com/content/abstract/scopus_id/85050476760,"Near wellbore failure during the exploration of hydrocarbon reservoirs presents a serious concern to the oil and gas industry. To predict the probability of these undesirable phenomena, engineers study the mechanical rock properties of the formation such as Young's modulus, Bulk modulus, shear modulus and Poisson's ratio. Conventionally, these are measured indirectly using the established petro physical relationships from sonic wave velocities which can be obtained from sonic well logs. Unfortunately, reliable sonic well logs are not always available, due to poor borehole conditions (wash out), damaged tools and offset well data. Most offset well log data are not acquired with dipole sonic tools; they are acquired with a borehole compensated logging tool. This limits the application of acoustic measurements to estimate the mechanical rock properties.
                  In this study, a three-layer feedforward multilayered perceptron artificial neural network model is presented. This model aims to estimate compressional wave transit time and shear wave transit time using real gamma ray and formation density logs. The validation of the model is confirmed by using an oil and gas offshore shaley sandstone reservoir located in West Africa. The results of the validation show that the model presented in this study can be used to determine the sanding potential of the formation without performing a compressive geoscientific analysis in the absence of sonic well logs. The developed model's effectiveness is tested by comparing the predicted results with results obtained from the measured well log. The paper provides a tool to give preliminary recommendations of the likelihood of the formation to produce sand. Implementation of the proposed model can serve as a cost-effective and reliable alternative for the oil and gas industry.",industry
10.1016/j.measurement.2018.05.099,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2018-11-01,sciencedirect,Parallel three-dimensional electrical capacitance data imaging using a nonlinear inversion algorithm and L<sup>p</sup> norm-based model regularization,https://api.elsevier.com/content/abstract/scopus_id/85049483981,"In order to improve image reconstructions, different classes of nonlinear inversion algorithms are developed and used in different research topics like imaging processes in oil industry or the characterization of complex porous media or multiphase flows. These algorithms are able to avoid local minima and to reach more adapted minima of a given misfit function between observed/measured and computed data. Techniques as different as electrical, ultrasound or potential methods, are used. We present here a nonlinear algorithm that allows us to produce permittivity images by using electrical capacitance tomography (ECT). ECT is a non-invasive technique to image non-conductive permittivity distributions and is used in many oil industry imaging applications such as multiphase flows in pipelines, fluidized bed reactors, mixing vessels, and tanks of phase separation. Even if the ECT technique provides low resolution reconstructions, it is cheap, robust and very fast when compared to other imaging tools. In this method one or more rings of electrodes excite a medium to be imaged at high frequencies, and more particularly at frequencies for which a static electrical potential field has fully developed. In many studies of other research groups only one ring of sources is introduced but the reconstruction accuracy was not totally satisfactory due to the 3D nature of the problem to be solved. Instead of using nonlinear stochastic algorithms like the simulated annealing (SA) technique that we optimized in previous studies to image permittivity distributions of granular or solid materials as well as real oil–gas or two-phase flows in 2D cylindrical vessel configurations, we propose here a new ECT inversion tool to image permittivities in a 3D cylindrical configuration. 3D stochastic optimization methods such as SA, neural networks, genetic algorithms can become computationally too prohibitive, and classical local or linear inversion methods excessively smooth images in many cases. Therefore, we propose here a 3D parallel inversion procedure with different numbers of rings and different 
                        
                           
                              
                                 L
                              
                              
                                 p
                              
                           
                        
                      norms, with
                        
                           1
                           <
                           p
                           ⩽
                           2
                        
                     , applied to the model regularization of the misfit function to increase the resolution of the models after inversion. We are able to better reconstruct two-phase and three-phase (oil, gas and solids) mixtures by combining 
                        
                           
                              
                                 L
                              
                              
                                 p
                              
                           
                        
                     -norm regularizations of the misfit function to minimize and several rings of electrodes. All these algorithms have been implemented in a more general parallel framework TOMOFAST-X designed for multi-physics joint inversion purposes, and could also be used in other fields of research such as larger-scale geophysical exploration for instance.",industry
10.1016/j.ssci.2018.06.012,Journal,Safety Science,scopus,2018-11-01,sciencedirect,Occupational health and safety in the industry 4.0 era: A cause for major concern?,https://api.elsevier.com/content/abstract/scopus_id/85049323662,"Real-time communication, Big Data, human–machine cooperation, remote sensing, monitoring and process control, autonomous equipment and interconnectivity are becoming major assets in modern industry. As the fourth industrial revolution or Industry 4.0 becomes the predominant reality, it will bring new paradigm shifts, which will have an impact on the management of occupational health and safety (OHS).
                  In the midst of this new and accelerating industrial trend, are we giving due consideration to changes in OHS imperatives? Are the OHS consequences of Industry 4.0 being evaluated properly? Do we stand to lose any of the gains made through proactive approaches? Are there rational grounds for major concerns? In this article, we examine these questions in order to raise consciousness with regard to the integration of OHS into Industry4.0.
                  It is clear that if the technologies driving Industry 4.0 develop in silos and manufacturers’ initiatives are isolated and fragmented, the dangers will multiply and the net impact on OHS will be negative. As major changes are implemented, previous gains in preventive management of workplace health and safety will be at risk. If we are to avoid putting technological progress and OHS on a collision course, researchers, field experts and industrialists will have to collaborate on a smooth transition towards Industry 4.0.",industry
10.1016/j.vetmic.2018.08.026,Journal,Veterinary Microbiology,scopus,2018-10-01,sciencedirect,Detection of non-notifiable H4N6 avian influenza virus in poultry in Great Britain,https://api.elsevier.com/content/abstract/scopus_id/85053845094,"A 12-month pilot project for notifiable avian disease (NAD) exclusion testing in chicken and turkey flocks in Great Britain (GB) offered, in partnership with industry, opportunities to carry out differential diagnosis in flocks where NAD was not suspected, and to identify undetected or undiagnosed infections. In May 2014, clinical samples received from a broiler breeder chicken premises that had been experiencing health and production problems for approximately one week tested positive by avian influenza (AI) real-time reverse transcription polymerase chain reaction (RRT-PCR). Following immediate escalation to an official, statutory investigation to rule out the presence of notifiable AI virus (AIV; H5 or H7 subtypes), a non-notifiable H4N6 low pathogenicity (LP) AIV was detected through virus isolation in embryonated specific pathogen free (SPF) fowls’ eggs, neuraminidase inhibition test, cleavage site sequencing and AIV subtype H4-specific serology. Premises movement restrictions were lifted, and no further disease control measures were implemented as per the United Kingdom (UK) legislation. Phylogenetic analysis of the haemagglutinin and neuraminidase genes of the virus revealed closest relationships to viruses from Mallard ducks in Sweden during 2007 and 2009. In June 2014, clinical suspicion of NAD was reported in a flock of free-range laying chickens elsewhere in GB, due to increasing daily mortality and reduced egg production over a five-day period. An H4N6 LPAIV with an intravenous pathogenicity index of 0.50 was isolated. This virus was genetically highly similar, but not identical, to the virus detected during May 2014. Full viral genome analyses showed characteristics of a strain that had not recently transferred from wild birds, implying spread within the poultry sector had occurred. A stalk deletion in the neuraminidase gene sequence indicated an adaptation of the virus to poultry. Furthermore, there was unexpected evidence of systemic spread of the virus on post-mortem. No other cases were reported. Infection with LPAIVs often result in variable clinical presentation in poultry, making detection of disease more difficult.",industry
10.1016/j.mfglet.2018.09.002,Journal,Manufacturing Letters,scopus,2018-10-01,sciencedirect,Industrial Artificial Intelligence for industry 4.0-based manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85053749537,"The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.",industry
10.1016/j.compind.2018.07.004,Journal,Computers in Industry,scopus,2018-10-01,sciencedirect,IDARTS – Towards intelligent data analysis and real-time supervision for industry 4.0,https://api.elsevier.com/content/abstract/scopus_id/85050319341,"The manufacturing industry represents a data rich environment, in which larger and larger volumes of data are constantly being generated by its processes. However, only a relatively small portion of it is actually taken advantage of by manufacturers. As such, the proposed Intelligent Data Analysis and Real-Time Supervision (IDARTS) framework presents the guidelines for the implementation of scalable, flexible and pluggable data analysis and real-time supervision systems for manufacturing environments. IDARTS is aligned with the current Industry 4.0 trend, being aimed at allowing manufacturers to translate their data into a business advantage through the integration of a Cyber-Physical System at the edge with cloud computing. It combines distributed data acquisition, machine learning and run-time reasoning to assist in fields such as predictive maintenance and quality control, reducing the impact of disruptive events in production.",industry
10.1016/j.cie.2018.07.016,Journal,Computers and Industrial Engineering,scopus,2018-10-01,sciencedirect,New decision support system for strategic planning in process industries: Computational results,https://api.elsevier.com/content/abstract/scopus_id/85049776857,"The impact of a Stochastic Linear Programming (SLP) based Decision Support System in a manufacturing company, such as an integrated aluminum plant, is measured by two important parameters, the VSS and EVPI. With the real data of an integrated steel plant in India, we demonstrate that SLP based DSS can be very effective in managing demand uncertainty and performing futuristic integrated planning, and their financial impact can be in millions of dollars. A two stage stochastic programming model with recourse is implementedin the DSS here. A set of experiments is conducted. Real data from an aluminum company is used to validate the system. The importance of SLP based DSS can be realized from the fact that the value of the stochastic solution (VSS) is USD 3.58 million with 30% demand variability and equally likely demand distribution. The VSS as a percentage of Expectation of Expected Value (EEV) ranges from 0.90% to 18.93% across experiments.",industry
10.1016/j.jpdc.2018.04.005,Journal,Journal of Parallel and Distributed Computing,scopus,2018-10-01,sciencedirect,A malicious threat detection model for cloud assisted internet of things (CoT) based industrial control system (ICS) networks using deep belief network,https://api.elsevier.com/content/abstract/scopus_id/85047404138,"Internet of Things (IoT) devices are extensively used in modern industries combined with the conventional industrial control system (ICS) network through the industrial cloud to make the production data easily available to the corporate business management and easier control for highly profitable production systems. The different devices within the conventional ICS network originally manufactured to run on an isolated network and was not considered for the privacy and security of the control and production/architecture data being trafficked over the manufacturing plant to the corporate. Due to their extensive integration with the industrial cloud network over the internet, these ICS networks are exposed to a significant threat of malicious activities created by malicious software. Protecting ICS from such attacks requires continuous update of their database of anti-malware tools which requires efforts from manual experts on a regular basis. This limits real time protection of ICS.
                  Earlier work by Huda et al. (2017) based on a semi-supervised approach performed well. However training process of the semi-supervised-approach (Huda et al., 2017) is complex procedure which requires a hybridization of feature selection, unsupervised clustering and supervised training techniques. Therefore, it could be time consuming for ICS network for real time protection. In this paper, we propose an adaptive threat detection model for industrial cloud of things (CoT) based on deep learning. Deep learning has been used in many domain of pattern recognition and a popular approach for its simple training procedure. Most importantly, deep learning can learn the hidden patterns of the domain in an unsupervised manner which can avoid the requirements of huge expensive labeled data. We used this particular characteristic of deep learning to design our detection model.
                  Two different types of deep learning based detection models are proposed in this work. The first model uses a disjoint training and testing data for a deep belief network (DBN) and corresponding artificial neural network (ANN). In the second proposed detection model, DBN is trained using new unlabeled data to provide DBN with additional knowledge about the changes in the malicious attack patterns. Novelty of the proposed detection models is that the models are adaptive where training procedures is simpler than earlier work (Huda et al, 2017) and can adapt new malware behaviors from already available and cheap unlabeled data at the same time. This will avoid expensive manual labeling of new attacks and corresponding time complexity making it feasible for ICS networks. Performances of standard DBNs are sensitive to its configurations and values for the hyper-parameters including number of hidden nodes, learning rate and number epochs. Therefore proposed detection models find an optimal configuration by varying the structure of DBNs and other parameters. The proposed detection models are extensively tested on a real malware test bed. Experimental results show that the proposed approaches achieve higher accuracies than standard detection algorithms and obtain similar performances with earlier semi-supervised work (Huda et al., 2017) but provide a comparatively simplified training model.",industry
10.1016/j.apenergy.2018.06.040,Journal,Applied Energy,scopus,2018-09-15,sciencedirect,Optimal scheduling of a microgrid in a volatile electricity market environment: Portfolio optimization approach,https://api.elsevier.com/content/abstract/scopus_id/85048767400,"This paper proposes an optimal scheduling strategy for a microgrid participating in a volatile electricity market. The microgrid system includes photovoltaic generators, a wind turbine, a load, grid connection, and a battery storage system. An optimal microgrid operation is achieved by maximizing the utility function represented by the exponential rate of growth of the electricity market value through electricity transactions between the microgrid and main grid, on the premise of satisfying the power balance and generation limit of system components. The uncertainties occurring during the microgrid operation are represented by generator output, load demand, and electricity price fluctuation. The proposed strategy utilizes the Kelly Criterion, an optimal strategy that maximizes the growth rate of an asset’s net worth over repeated investments, coupled with an artificial neural network forecast of electricity price to deal with the volatile energy market. The proposed algorithm provides significant improvements in microgrid scheduling by eliminating the reliance on renewable generation and load forecasts, which makes it computationally inexpensive and thus feasible for real-time implementation. In representative case scenarios, using real-world tracers, we show that the algorithm has no dependency on meteorological forecasts and performs optimally in a volatile electricity market.",industry
10.1016/j.jisa.2018.05.002,Journal,Journal of Information Security and Applications,scopus,2018-08-01,sciencedirect,Identification of malicious activities in industrial internet of things based on deep learning models,https://api.elsevier.com/content/abstract/scopus_id/85047072990,"Internet Industrial Control Systems (IICSs) that connect technological appliances and services with physical systems have become a new direction of research as they face different types of cyber-attacks that threaten their success in providing continuous services to organizations. Such threats cause firms to suffer financial and reputational losses and the stealing of important information. Although Network Intrusion Detection Systems (NIDSs) have been proposed to protect against them, they have the difficult task of collecting information for use in developing an intelligent NIDS which can proficiently detect existing and new attacks. In order to address this challenge, this paper proposes an anomaly detection technique for IICSs based on deep learning models that can learn and validate using information collected from TCP/IP packets. It includes a consecutive training process executed using a deep auto-encoder and deep feedforward neural network architecture which is evaluated using two well-known network datasets, namely, the NSL-KDD and UNSW-NB15. As the experimental results demonstrate that this technique can achieve a higher detection rate and lower false positive rate than eight recently developed techniques, it could be implemented in real IICS environments.",industry
10.1016/j.neucom.2018.03.014,Journal,Neurocomputing,scopus,2018-06-14,sciencedirect,ACDIN: Bridging the gap between artificial and real bearing damages for bearing fault diagnosis,https://api.elsevier.com/content/abstract/scopus_id/85044327325,"Data-driven algorithms for bearing fault diagnosis have achieved much success. However, it is difficult and even impossible to collect enough data containing real bearing damages to train the classifiers, which hinders the application of these methods in industrial environments. One feasible way to address the problem is training the classifiers with data generated from artificial bearing damages instead of real ones. In this way, the problem changes to how to extract common features shared by both kinds of data because the differences between the artificial one and the natural one always baffle the learning machine. In this paper, a novel model, deep inception net with atrous convolution (ACDIN), is proposed to cope with the problem. The contribution of this paper is threefold. First and foremost, ACDIN improves the accuracy from 75% (best results of conventional data-driven methods) to 95% on diagnosing the real bearing faults when trained with only the data generated from artificial bearing damages. Second, ACDIN takes raw temporal signals as inputs, which means that it is pre-processing free. Last, feature visualization is used to analyze the mechanism behind the high performance of the proposed model.",industry
10.1016/j.enconman.2018.03.044,Journal,Energy Conversion and Management,scopus,2018-06-01,sciencedirect,Adaptive air-fuel ratio control of dual-injection engines under biofuel blends using extreme learning machine,https://api.elsevier.com/content/abstract/scopus_id/85044138298,"Dual-injection engines, which allow real-time control and injection of two different fuels, are capable of varying the ratio of biofuel blends at different engine operating conditions for optimal engine performance. However, while many experiments have been carried out on these engines to demonstrate their advantages, very few studies have focused on the corresponding air–fuel ratio (AFR) control strategy. In order to achieve stable engine operation, it is essential to maintain transient AFR during the change of fuel blend ratio. Therefore, this study proposes an adaptive controller for AFR control of dual-injection engines. The proposed controller is designed based on a recently developed machine learning method called extreme learning machine, and its stability is verified with Lyapunov analysis. Simulations have been performed on an industry-level engine simulation software to verify the controller. Since dual-injection engines are not available in the market, a spark-ignition engine has been retrofitted for dual-injection operation so that the proposed controller can be implemented and evaluated experimentally. Both simulation and experiment results show that the proposed controller can effectively regulate the AFR to desired level. The results also show that the proposed controller outperforms the engine built-in AFR controller, indicating its significance for dual-injection engines.",industry
10.1016/j.autcon.2018.01.003,Journal,Automation in Construction,scopus,2018-05-01,sciencedirect,Transfer learning and deep convolutional neural networks for safety guardrail detection in 2D images,https://api.elsevier.com/content/abstract/scopus_id/85041454603,"Safety has been a concern for the construction industry for decades. Unsafe conditions and behaviors are considered as the major causes of construction accidents. The current safety inspection of conditions and behaviors heavily rely on human efforts which are limited onsite. To improve the safety performance of the industry, a more efficient approach to identify the unsafe conditions on site is required to supplement the current manual inspection practice. A promising way to supplement the current manual safety inspection is automated and intelligent monitoring/inspection through information and sensing technologies, including localization techniques, environment monitoring, image processing and etc. To assess the potential benefits of contemporary technologies for onsite safety inspection, the authors focused on real-time guardrail detection, as unprotected edges are the ones cause for workers falling from heights.
                  In this paper, the authors developed a safety guardrail detection model based on convolutional neural network (CNN). An augmented data set is generated with the addition of background image to guardrail 3D models and used as training set. Transfer learning is utilized and the Visual Geometry Group architecture with 16 layers (VGG-16) model is adopted to construct the basic features extraction for the neural network. In the CNN implementation, 4000 augmented images were used to train the proposed model, while another 2000 images collected from real construction jobsites and 2000 images from Google were used to validate the proposed model. The proposed CNN-based guardrail detection model obtained a high accuracy of 96.5%. In addition, this study indicates that the synthetic images generated by augment technology can be used to create a large training dataset, and CNN-based image detection algorithm is a promising approach in construction jobsite safety monitoring.",industry
10.1016/j.neucom.2018.01.002,Journal,Neurocomputing,scopus,2018-04-12,sciencedirect,Robot manipulator control using neural networks: A survey,https://api.elsevier.com/content/abstract/scopus_id/85041636063,"Robot manipulators are playing increasingly significant roles in scientific researches and engineering applications in recent years. Using manipulators to save labors and increase accuracies are becoming common practices in industry. Neural networks, which feature high-speed parallel distributed processing, and can be readily implemented by hardware, have been recognized as a powerful tool for real-time processing and successfully applied widely in various control systems. Particularly, using neural networks for the control of robot manipulators have attracted much attention and various related schemes and methods have been proposed and investigated. In this paper, we make a review of research progress about controlling manipulators by means of neural networks. The problem foundation of manipulator control and the theoretical ideas on using neural network to solve this problem are first analyzed and then the latest progresses on this topic in recent years are described and reviewed in detail. Finally, toward practical applications, some potential directions possibly deserving investigation in controlling manipulators by neural networks are pointed out and discussed.",industry
10.1016/j.energy.2018.01.159,Journal,Energy,scopus,2018-04-01,sciencedirect,Multiobjective optimization of ethylene cracking furnace system using self-adaptive multiobjective teaching-learning-based optimization,https://api.elsevier.com/content/abstract/scopus_id/85041748366,"The ethylene cracking furnace system is crucial for an olefin plant. Multiple cracking furnaces are used to convert various hydrocarbon feedstocks to smaller hydrocarbon molecules, and the operational conditions of these furnaces significantly influence product yields and fuel consumption. This paper develops a multiobjective operational model for an industrial cracking furnace system that describes the operation of each furnace based on current feedstock allocations, and uses this model to optimize two important and conflicting objectives: maximization of key products yield, and minimization of the fuel consumed per unit ethylene. The model incorporates constraints related to material balance and the outlet temperature of transfer line exchanger. The self-adaptive multiobjective teaching-learning-based optimization algorithm is improved and used to solve the designed multiobjective optimization problem, obtaining a Pareto front with a diverse range of solutions. A real industrial case is investigated to illustrate the performance of the proposed model: the set of solutions returned offers a diverse range of options for possible implementation, including several solutions with both significant improvement in product yields and lower fuel consumption, compared with typical operational conditions.",industry
10.1016/j.measurement.2017.12.026,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2018-03-01,sciencedirect,NARX ANN-based instrument fault detection in motorcycle,https://api.elsevier.com/content/abstract/scopus_id/85039147782,"In the context of motorcycle, we can assist to an increasing interest toward semi-active suspension control systems able to improve both the comfort and the passenger’s safety in both racing and original equipment manufacturer applications. Such systems implement suitable strategies based on the measure of several quantities, among which the relative velocity of the wheels respect to the vehicle body with the aim of regulating in real-time the damping forces. The actual effectiveness of such strategy strongly depends on the reliability and accuracy of the data measured by the sensors involved in the control loop. Due to their simplicity and good performance in terms of linearity, the most used sensors for suspension displacement measurements are based on linear potentiometers but such kind of sensors suffer of wear and tear and aging higher than the other sensors involved in the control loop strategy. As a consequence, the fault detection of such sensor is strongly recommended to avoid wrong and in some cases dangerous suspension behaviors.
                  To this aim, in this paper a Fault Detection scheme for the rear suspension stroke sensor is designed and verified. The residual generation is based on the use of a Nonlinear Auto-Regressive with eXogenous inputs (NARX) network which is able to effectively take into account for the system nonlinearity. Experimental results have proven the good promptness and reliability of the scheme in detecting different kind of faults as “un-calibration faults” (e.g. due to slight variations of the input/output sensor curve), “hold-faults” (e.g. due to the breaking of the potentiometer cursor), “open circuit” and “short circuit” (e.g. due to electrical interruptions and short circuits, respectively).
                  In addition, to verify the feasibility of a real-time implementation on actual processing units employed in such context, the scheme has been successfully implemented on a microcontroller STM32 based on the general-purpose ARM-M4 architecture. The validation tests and analysis have shown that the proposed Instrument Fault Detection scheme could be successfully developed on these kind of architectures by assuring a real-time operating.",industry
10.1016/j.cose.2017.11.014,Journal,Computers and Security,scopus,2018-03-01,sciencedirect,Intelligent agents defending for an IoT world: A review,https://api.elsevier.com/content/abstract/scopus_id/85038807783,"Transition to the Internet of Things (IoT) is progressing without realization. In light of this securing traditional systems is still a challenging role requiring a mixture of solutions which may negatively impact, or simply, not scale to a desired operational level. Rule and signature based intruder detection remains prominent in commercial deployments, while the use of machine learning for anomaly detection has been an active research area. Behavior detection means have also benefited from the widespread use of mobile and wireless applications. For the use of smart defense systems we propose that we must widen our perspective to not only security, but also to the domains of artificial intelligence and the IoT in better understanding the challenges that lie ahead in hope of achieving autonomous defense. We investigate how intruder detection fits within these domains, particularly as intelligent agents. How current approaches of intruder detection fulfill their role as intelligent agents, the needs of autonomous action regarding compromised nodes that are intelligent, distributed and data driven. The requirements of detection agents among IoT security are vulnerabilities, challenges and their applicable methodologies. In answering aforementioned questions, a survey of recent research work is presented in avoiding refitting old solutions into new roles. This survey is aimed toward security researchers or academics, IoT developers and information officers concerned with the covered areas. Contributions made within this review are the review of literature of traditional and distributed approaches to intruder detection, modeled as intelligent agents for an IoT perspective; defining a common reference of key terms between fields of intruder detection, artificial intelligence and the IoT, identification of key defense cycle requirements for defensive agents, relevant manufacturing and security challenges; and considerations to future development. As the turn of the decade draws nearer we anticipate 2020 as the turning point where deployments become common, not merely just a topic of conversation but where the need for collective, intelligent detection agents work across all layers of the IoT becomes a reality.",industry
10.1016/j.microrel.2017.11.002,Journal,Microelectronics Reliability,scopus,2018-02-01,sciencedirect,Prognostics of aluminum electrolytic capacitors using artificial neural network approach,https://api.elsevier.com/content/abstract/scopus_id/85033701102,"In this work, an effort is being made to monitor the condition of in-circuit aluminum electrolytic capacitor using artificial neural network (ANN). Recent industrial surveys on the reliability of power electronic systems shows that most of faults occur due to the wear out of aluminum electrolytic capacitors and thermal stress is the major cause for its parametric degradation. The condition of target capacitors can be estimated by monitoring variation in equivalent series resistance (ESR) from the initial pristine state value. ANN is used to estimate ESR of pristine and weak target capacitors at the test conditions. The data set for training and testing of proposed back-propagation trained artificial neural network are experimentally obtained from the developed test bed. Using the test bed, target capacitors are subjected to different operating frequency and temperature in the output section of DC/DC buck converter circuit to determine the effect of variation in electrical and thermal stress on ESR value. After off-line training, the proposed ANN is implemented using National Instruments LabVIEW software. A low cost microcontroller is programmed for real time data acquisition of target capacitors and the serial transmission of acquired dataset to the LabVIEW software installed at host computer. The performance of the proposed method is evaluated in real time by comparing the resulting ESR with the experimental values of in-circuit target capacitors. The proposed ANN, once trained properly, can be used for different circuits and in different operating conditions because of its generalization capability.",industry
10.1016/j.neucom.2017.08.036,Journal,Neurocomputing,scopus,2018-01-31,sciencedirect,Data-driven model-free slip control of anti-lock braking systems using reinforcement Q-learning,https://api.elsevier.com/content/abstract/scopus_id/85029168035,"This paper proposes the design and implementation of a model-free tire slip control for a fast and highly nonlinear Anti-lock Braking System (ABS). A reinforcement Q-learning optimal control approach is inserted in a batch neural fitted scheme using two neural networks to approximate the value function and the controller, respectively. The transition samples required for learning high performance control can be collected by interacting with the process either by online exploiting the current iteration controller (or policy) under an ε-greedy exploration strategy, or by using data collected under any other controller that is capable of ensuring efficient exploration of the action-state space. Both approaches are highlighted in the paper. Fortunately, the ABS process fits this type of learning-by-interaction because it does not need an initial stabilizing controller. The validation case studies conducted on a real laboratory setup reveal that high control system performance can be achieved using the proposed approaches. Insightful comments on the observed control behavior are offered along with performance comparisons with several types of model-based and model-free controllers including relay, model-based optimal PI, an original model-free neural network state-feedback VRFT controller and a model-free neural network adaptive actor-critic one. With the ability to improve control performance starting from different supervisory controllers or to learn high performance controllers from scratch, the proposed Q-learning optimal control approach proves its performance in a wide operating range and is therefore recommended to its industrial application on ABS.",industry
10.1016/j.chemolab.2017.12.005,Journal,Chemometrics and Intelligent Laboratory Systems,scopus,2018-01-15,sciencedirect,A new reconstruction-based auto-associative neural network for fault diagnosis in nonlinear systems,https://api.elsevier.com/content/abstract/scopus_id/85037701407,"Auto-associative neural network (AANN) is a typical nonlinear principal component analysis method, which is widely used in industry for fault diagnosis purposes, especially in nonlinear systems. However, the basic AANN often suffers from “smearing effects” problems that may lead to misdiagnosis, particularly with regards to the complex faults involving multiple variables. In this work, a new reconstruction-based AANN (RBAANN) method is proposed to enhance the capacity of fault diagnosis. In RBAANN, a generic derivative equation is developed to investigate the effects of AANN model inputs on the prediction error between model inputs and outputs. Based on the derivative equation, the reconstruction-based index for single or multiple variables, which is defined as the minimum prediction error, is obtained by tuning the corresponding model inputs iteratively. However, without the prior knowledge of the real faulty variables, all the possible variable sets need to be evaluated by the reconstruction-based index, and this may result in an exhaustive search and cause a huge computational burden. Thus, a branch and bound algorithm is introduced into RBAANN to solve the variable selection problem. Finally, an efficient fault diagnosis strategy by integrating RBAANN and branch and bound algorithm (BAB-RBAANN) is implemented to further pinpoint the source of the detected faults. This BAB-RBAANN method can handle both single and multiple variable(s) faults for nonlinear systems without prior knowledge efficiently. The effectiveness of the proposed methods is evaluated on a validation example and an industrial example. Comparisons with other methods, including principal component analysis techniques, are also presented.",industry
10.1016/B978-0-12-813314-9.00011-6,Book,Computational Intelligence for Multimedia Big Data on the Cloud with Engineering Applications,scopus,2018-01-01,sciencedirect,Unsupervised anomaly detection for high dimensional data-An exploratory analysis,https://api.elsevier.com/content/abstract/scopus_id/85081928867,"Context: Anomaly detection is a crucial area engaging the attention of many researchers. It is a process of finding an unusual point or pattern in a given dataset. It is useful in many real time applications such as industry damage detection, detection of fraudulent usage of credit card, detection of failures in sensor nodes, detection of abnormal health and network intrusion detection. Algorithms proposed for anomaly detection in low dimensional data are not suitable for high dimensional data due to the well-known “dimensionality curses”.
               
                  Motivation: To tackle this issue, a plethora of algorithms dedicated to high dimensional data has been proposed. However, unsupervised algorithms have many problems and challenges, as there is no predefined data label to predict anomaly.
               
                  Objective: We aim at providing a complete view of unsupervised anomaly detection for high dimensional data which gives a clear perception of the concept.
               
                  Contribution: In this paper, existing algorithms and real time applications of unsupervised anomaly detection for high dimensional data have been studied. Evaluation measures, datasets and tools used by different authors have been discussed in detail. In addition, a hybrid framework of unsupervised anomaly detection algorithm called DBN–K means applied two different disease dataset is also proposed.
               
                  Future work: As future work, the proposed framework could be implemented and analyzed in other applications. High dimensional streaming data is another interesting area for further investigation, following this research work.",industry
10.1016/j.procir.2018.01.036,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,"Intuitive robot programming through environment perception, augmented reality simulation and automated program verification",https://api.elsevier.com/content/abstract/scopus_id/85061975291,"The increasing complexity of products and machines as well as short production cycles with small lot sizes present great challenges to production industry. Both, the programming of industrial robots in online mode using hand-held control devices or in offline mode using text-based programming requires specific knowledge of robotics and manufacturer-dependent robot control systems. In particular for small and medium-sized enterprises the machine control software needs to be easy, intuitive and usable without time-consuming learning steps, even for employees with no in-depth knowledge of information technology. To simplify the programming of application programs for industrial robots, we extended a cloud-based, task-oriented robot control system with environment perception and plausibility check functions. For the environment perception a depth camera and pointcloud processing hardware were installed. We detect objects located in the robot’s workspace by pointcloud processing with ROS and the PCL and add them to the augmented reality user interface of the robot control. The combination of process knowledge from task-oriented application programming and information about available workpieces from automated image processing enables a plausibility check and verification of the robot program before execution. After a robot program has been approved by the plausibility check, it is tested in an augmented reality simulation for collisions with the detected objects before deployment to the physical robot hardware. Experiments were carried out to evaluate the effectiveness of the developed extensions and confirmed their functionality.",industry
10.1016/j.procir.2018.09.067,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,A Conceptual Design for Smell Based Augmented Reality: Case Study in Maintenance Diagnosis,https://api.elsevier.com/content/abstract/scopus_id/85059916374,"The trend of Industry 4.0 encourages the next generation of manufacturing to be flexible, intelligent, and interoperable. The implementations of the Artificial Intelligence (AI) technology could potentially enhance maintenance in efficiency, and accuracy. However, it will not be a substitution to the human operator’s flexibility, decision-making and information received by the natural five senses. Augmented reality (AR) is commonly understood as a technology that overlays virtual information onto the existing environment to provide users a new and improved experience to assist their daily activities. However, AR can be used to enhance all human five senses rather than just overlay virtual imagery. In this paper, a design and a practical plan of smell augmentation for diagnosis is initialised, via a case study in maintenance. The aim of this paper is to evaluate the feasibilities, identify challenges, and summarise initial results of overlaying information through smell augmentations.",industry
10.1016/j.promfg.2018.04.009,Conference Proceeding,Procedia Manufacturing,scopus,2018-01-01,sciencedirect,Mixed Reality in Learning Factories,https://api.elsevier.com/content/abstract/scopus_id/85052906978,"Supported by rapid technological development, mixed reality (MR) applications are increasingly deployed in industrial practice. In manufacturing, MR can be utilized for information visualization, remote collaboration, human-machine-interfaces, design tools and education and training. This development makes new demands on learning factories in two major fields: One is the empowerment of users to work with MR in industrial applications. The second field is the utilization of the potential of MR for teaching and learning in learning factories. A great potential lies in the new possibilities of connecting digital content with the physical world. To analyze the potential applications of MR in learning factories in a structured way, an overview of potential MR applications based on the reality-virtuality continuum is presented with an analysis of case studies of applications in a learning factory including a mixed-reality-hackathon.",industry
10.1016/j.promfg.2018.04.026,Conference Proceeding,Procedia Manufacturing,scopus,2018-01-01,sciencedirect,Design and implementation of a low cost RFID track and trace system in a learning factory,https://api.elsevier.com/content/abstract/scopus_id/85052890798,"The factories of the future will make use of actuators, sensors and cyber-physical systems (CPS) to provide an environment in which human beings, machines, and resources will communicate as in a social network. In such a network, communication between various “objects” relay the current state of the physical world. Business decisions are made using the information and it is therefore critical that this information is accurate and in real-time. Information flow is a key enabler of such future factories. Industrial engineers, as designers and improvement agents of such factories of the future, will need to develop better skills in various aspects of data analytics and information communication technologies. This paper describes the development and implementation of a low cost RFID track and trace system (by students) for application in a Learning Factory for teaching undergraduate industrial engineering students key concepts related to Industry 4.0 and “smart factories”. The benefit of this system is not only a demonstrator to be used in the Learning Factory, but also can be used to teach students in a “learning by doing” fashion critical skills related to real time tracking in a manufacturing environment. The system also demonstrates potential low cost implementation of such technologies in SME’s.",industry
10.1016/j.ifacol.2018.08.421,Conference Proceeding,,scopus,2018-01-01,sciencedirect,A Multi Agent System architecture to implement Collaborative Learning for social industrial assets,https://api.elsevier.com/content/abstract/scopus_id/85052888258,"The ‘Industrial Internet of Things’ aims to connect industrial assets with one another and benefit from the data that is generated, and shared, among these assets. In recent years, the extensive instrumentation of machines and the advancements in Information Communication Technologies are re-shaping the role of assets in our industrial systems. An emerging concept here is that of ‘social assets’: assets that collaborate with each other in order to improve system optimisation. Cyber-Physical Systems (CPSs) are formed by embedding the assets with computers, or microcontrollers, which run real-time decision-making algorithms over the data originating from the asset. These are known as the ‘Digital Twins’ of the assets, and form the backbone of social assets. It is essential to have an architecture which enables a seamless integration of these technological advances for an industry. This paper proposes a Multi Agent System (MAS) architecture for collaborative learning, and presents the findings of an implementation of this architecture for a prognostics problem. Collaboration among assets is performed by calculating inter-asset similarity during operating condition to identify ‘friends’ and sharing operational data within these clusters of friends. The architecture described in this paper also presents a generic model for the Digital Twins of assets. Prognostics is demonstrated for the C-MAPSS turbofan engine degradation simulated data-set (Saxena and Goebel (2008)).",industry
10.1016/j.procs.2018.07.108,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Ambience Inhaling: Speech Noise Inhaler in Mobile Robots using Deep Learning,https://api.elsevier.com/content/abstract/scopus_id/85051344062,"Audio based, machine learning human-computer interface with speech recognition systems performs sensibly well with the human voice under clean ambience, but become frail in applied technological implementation involving real-life interface. In mobile robotic systems, the speech machines are normally retrained with new changing acoustic ambience conditions are to be met. To inhale, classify, and track the real-world ambience noise with the new changing acoustic condition, we introduce an Ambience Inhaling (AI) framework in this article. This framework of an AI is to seek out complete noise information from speech data, in contrast with noise-nature discovery. Our proposed framework uses a deep convolutional neural network (CNN) based learning for classification with speech spectrogram patch segments, including a hybrid Harold Hotelling's T-square algorithm with Bayesian statistics for segmentation analysis. We use a symposium presentation-ambience as a test platform. In the symposium presentation-ambience, noise modeling is done with n-gram language having the parameter of n = 2. The impulsive or short-term noise which is superimposed with long-term noise caused degradation in classification. This degradation caused the classification errors. The provision of decision was made. The Gaussian mixture model and hidden Markova model are used with noise-only and noisy speech respectively. Time and frequency pooling are used with spectrogram also. The classification scores of 62.26%, 65.89%, and 69.12% are achieved with 5, 10 and 15 CNN filters respectively. As a significance, an AI is efficient and innovative.",industry
10.1016/B978-0-444-64241-7.50087-2,Book Series,Computer Aided Chemical Engineering,scopus,2018-01-01,sciencedirect,Reinforcement Learning Applied to Process Control: A Van der Vusse Reactor Case Study,https://api.elsevier.com/content/abstract/scopus_id/85050599810,"With recent advances in industrial automation, data acquisition, and successful applications of Machine Learning methods to real-life problems, data-based methods can be expected to grow in use within the process control community in the near future. Model-based control methods rely on accurate models of the process to be effective. However, such models may be laborious to obtain and, even when available, the optimization problem underlying the online control problem may be too computationally demanding. Furthermore, the process degradation with time imposes that the model should be periodically updated to stay reliable. One way to address these drawbacks is through the merging of Reinforcement Learning (RL) techniques into the classical process control framework. In this work, a methodology to tackle the control of nonlinear chemical processes with RL techniques is proposed and tested on the wellknown benchmark problem of the non-isothermal CSTR with the Van de Vusse reaction. The controller proposed herein is based on the implementation of a policy that associates each state of the process to a certain control action. This policy is directly deduced from a measure of the expected performance gain, given by a value function dependent on the states and actions. In other words, in a given state, the action that provides the highest expected performance gain is chosen and implemented. The value function is approximated by a neural network that can be trained with pre-simulated data and adapted online with the continuous inclusion of new process data through the implementation of an RL algorithm. The results show that the proposed adaptive RLbased controller successfully manages to control and optimize the Van de Vusse reactor against unmeasured disturbances.",industry
10.1016/j.ifacol.2018.06.356,Conference Proceeding,,scopus,2018-01-01,sciencedirect,Design Principles Behind the Construction of an Autonomous Laboratory-Scale Drilling Rig,https://api.elsevier.com/content/abstract/scopus_id/85050080748,"In recent years, hot topics such as digitalization, machine learning, digital twin and big data have evolved from being envisions on the paper to state of art solutions, expected to revolutionize drilling efficiency in the industry. Drilling automation tomorrow is all about exploiting the current state of technologies available to the entire operation of drilling a well. Not only can drilling automation limit costs and reduce the risk to rig personnel and the environment, but they also give access to locations of considerable potential that previously have been regarded unsafe or uneconomical to operate in. There are however some challenges in keeping up with the ever-increasing pace of the development. For one, testing of novel and innovative solutions is often very expensive because of non-productive rig time during implementation, trial runs and data evaluation. Also, the modern technologies require extensive R&D before on-site testing can even commence. While on land-rigs, some of these costs and risks can be greatly minimized, many offshore solutions lack that luxury. This paper presents an overview of the design principles that go into the construction of a fully autonomous laboratory-scale drilling rig at the University of Stavanger. It aims at describing 1) the engineering principles involved to resemble full-scale drilling operations on the laboratory scale, 2) design considerations and components, 3) component requirements for the rig, 4) control system algorithms for real-time optimization of drilling parameters and detection and handling of drilling anomalies, 5) development of drilling models (drill string dynamics, bit-vibration, etc.) and 6) benefits and future work with the laboratory-scale system. Some of the concepts that are presented in this paper have yet to be implemented during 2018.",industry
10.1016/j.procir.2018.03.168,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,Reinforcement learning in real-time geometry assurance,https://api.elsevier.com/content/abstract/scopus_id/85049605436,"To improve the assembly quality during production, expert systems are often used. These experts typically use a system model as a basis for identifying improvements. However, since a model uses approximate dynamics or imperfect parameters, the expert advice is bound to be biased. This paper presents a reinforcement learning agent that can identify and limit systematic errors of an expert systems used for geometry assurance. By observing the resulting assembly quality over time, and understanding how different decisions affect the quality, the agent learns when and how to override the biased advice from the expert software.",industry
10.1016/j.procir.2018.03.022,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,Fostering Robust Human-Robot Collaboration through AI Task Planning,https://api.elsevier.com/content/abstract/scopus_id/85049587790,"Recent advances in Artificial Intelligence (AI) are facilitating the deployment of intelligent systems in manufacturing. In Human-Robot Collaboration (HRC), industrial robots offer accuracy and efficiency while humans guarantee both experience and specialized and not replaceable skills. The seamless coordination of such different abilities constitutes one of the current challenges. This paper presents a dynamic task sequencing system for robust HRC developed within a EU-funded project. The proposed solution uses AI techniques to deal with the temporal variance entailed by the active presence of humans as well as to dynamically adapt task plans according to actual behavior of the pair human-worker/robot. The tool has been deployed in a real pilot plant.",industry
10.1016/j.procs.2018.05.142,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Optimization of Software Testing,https://api.elsevier.com/content/abstract/scopus_id/85049103381,"The goal of any business is to satisfy the needs of its target customers, and IT industry is not an exception from that rule. Thus, the upgraded version of the V-model testing is supposed to deal with the weaknesses of the original version in question by combining it with the method known as agile testing. At the beginning of the report, hypothesis such as the strengths and weaknesses of the existing V-model testing via literature review and interviews with respective specialists in the sphere were analysed. Successively, the possible advantages of agile method of testing were then considered. Moreover, the report comes up with the ways in which the two models could be naturally combined to produce a much more effective one. Once the new model was presented, its strengths and weaknesses were assessed by the means of a case study analysis using metric and a data analysis through a survey were conducted to evaluate the credibility of the futurist model. Promptly, the research found that the suggested testing model provides better results than the common version of V-model testing. Firstly, a real case scenarios under metric evaluation of the models have indicated that the proposed model is better than the V-model, since it can handle the following aspects; reduced testing time, debugging, prioritization of requirements, easy mapping of roles and improved visibility of project resources. Secondly, a survey data analysis highlighted various advantages of the future model. The top priorities of the new model from the respondent’s perception were; the new model manages rapidly changing priorities, it accelerates time to market, it increases productivity and it improves quality.",industry
10.1016/j.procs.2018.05.113,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Real Time High Performance of Sliding Mode Controlled Induction Motor Drives,https://api.elsevier.com/content/abstract/scopus_id/85049099142,"Several industrial applications demand high performance speed functioning and require new control techniques so as to ensure a fast dynamic response. The present work investigates real time implementation and experimental sliding mode controlled (SMC) induction motor drives (IM). The strategy of sliding mode control is a powerful tool to ensure robustness. Nevertheless, the chattering phenomenon is a major disadvantage for non linear systems. For this purpose, two different types of analysis such as layer boundary methods are implemented in dSPACE 1104 controller board and compared between them in order to obtain the best method to reduce or eliminate chattering phenomenon. An experimental results using dSPACE 1104 based on TMS320F240 DSP are described in this work.",industry
10.1016/j.cirp.2018.04.041,Journal,CIRP Annals,scopus,2018-01-01,sciencedirect,Reinforcement learning for adaptive order dispatching in the semiconductor industry,https://api.elsevier.com/content/abstract/scopus_id/85045954603,"The digitalization of production systems tends to provide a huge amount of data from heterogeneous sources. This is particularly true for the semiconductor industry wherein real time process monitoring is inherently required to achieve a high yield of good parts. An application of data-driven algorithms in production planning to enhance operational excellence for complex semiconductor production systems is currently missing. This paper shows the successful implementation of a reinforcement learning-based adaptive control system for order dispatching in the semiconductor industry. Furthermore, a performance comparison of the learning-based control system with the traditionally used rule-based system shows remarkable results. Since a strict rulebook does not bind the learning-based control system, a flexible adaption to changes in the environment can be achieved through a combination of online and offline learning.",industry
10.1016/j.procir.2017.12.230,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,A Conceptual Model for Developing a Smart Process Control System,https://api.elsevier.com/content/abstract/scopus_id/85044679399,"Current Manufacturing Execution Systems (MES) are not supporting a full integration into overall processes across the supply chain. Thus, optimization is limited to single areas. The SemI40 project is aimed at developing an integrated concept of Smart Process Control System (SPCS), which enhances the overall agility and productivity. The system, therefore, autonomously acquires and interprets process data to allow product individual optimization and enhancing logistics management. It also provides full traceability across the supply chain in real-time and allows model based process simulation and decision making support. The concept is developed based on requirements elicitation in cooperation with industry partners and combines state-of-the-art technologies with current trends, like vertical integration, big data and machine learning.",industry
10.1016/j.mfglet.2017.12.013,Journal,Manufacturing Letters,scopus,2018-01-01,sciencedirect,Artificial neural network based framework for cyber nano manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85042371124,"Nanomanufacturing plays an important role for high performance products in several applications. The challenge for fabricating products with nanomaterials is the inability to interconnect and interface with nano/micro manufacturing equipment. This paper presents a framework for cyber nanomanufacturing. Input part designs of nano/micro scale components are evaluated with an artificial neural network (ANN) based smart agent to predict optimal nanomanufacturing processes. An internet-of-things (IoT) based cyber-interface simulator is implemented to simulate real-time machine availability. Further, an application program interface (API) is developed to integrate the ANN smart agent and IoT simulator outcomes to predict dynamic machine allocations in real-time.",industry
10.1016/j.addma.2017.11.009,Journal,Additive Manufacturing,scopus,2018-01-01,sciencedirect,Anomaly detection and classification in a laser powder bed additive manufacturing process using a trained computer vision algorithm,https://api.elsevier.com/content/abstract/scopus_id/85035797198,"Despite the rapid adoption of laser powder bed fusion (LPBF) Additive Manufacturing by industry, current processes remain largely open-loop, with limited real-time monitoring capabilities. While some machines offer powder bed visualization during builds, they lack automated analysis capability. This work presents an approach for in-situ monitoring and analysis of powder bed images with the potential to become a component of a real-time control system in an LPBF machine. Specifically, a computer vision algorithm is used to automatically detect and classify anomalies that occur during the powder spreading stage of the process. Anomaly detection and classification are implemented using an unsupervised machine learning algorithm, operating on a moderately-sized training database of image patches. The performance of the final algorithm is evaluated, and its usefulness as a standalone software package is demonstrated with several case studies.",industry
10.1016/j.ins.2017.09.027,Journal,Information Sciences,scopus,2018-01-01,sciencedirect,Incorporating negative information to process discovery of complex systems,https://api.elsevier.com/content/abstract/scopus_id/85029528097,"The discovery of a formal process model from event logs describing real process executions is a challenging problem that has been studied from several angles. Most of the contributions consider the extraction of a model as a one-class supervised learning problem where only a set of process instances is available. Moreover, the majority of techniques cannot generate complex models, a crucial feature in some areas like manufacturing. In this paper we present a fresh look at process discovery where undesired process behaviors can also be taken into account. This feature may be crucial for deriving process models which are less complex, fitting and precise, but also good on generalizing the right behavior underlying an event log. The technique is based on the theory of convex polyhedra and satisfiability modulo theory (SMT) and can be combined with other process discovery approach as a post processing step to further simplify complex models. We show in detail how to apply the proposed technique in combination with a recent method that uses numerical abstract domains. Experiments performed in a new prototype implementation show the effectiveness of the technique and the ability to be combined with other discovery techniques.",industry
10.1016/j.biosystems.2017.10.001,Journal,BioSystems,scopus,2017-12-01,sciencedirect,Towards a first implementation of the WLIMES approach in living system studies advancing the diagnostics and therapy in personalized medicine,https://api.elsevier.com/content/abstract/scopus_id/85033459793,"The goal of this paper is to advance an extensible theory of living systems using an approach to biomathematics and biocomputation that suitably addresses self-organized, self-referential and anticipatory systems with multi-temporal multi-agents. Our first step is to provide foundations for modelling of emergent and evolving dynamic multi-level organic complexes and their sustentative processes in artificial and natural life systems. Main applications are in life sciences, medicine, ecology and astrobiology, as well as robotics, industrial automation, man-machine interface and creative design. Since 2011 over 100 scientists from a number of disciplines have been exploring a substantial set of theoretical frameworks for a comprehensive theory of life known as Integral Biomathics. That effort identified the need for a robust core model of organisms as dynamic wholes, using advanced and adequately computable mathematics. The work described here for that core combines the advantages of a situation and context aware multivalent computational logic for active self-organizing networks, Wandering Logic Intelligence (WLI), and a multi-scale dynamic category theory, Memory Evolutive Systems (MES), hence WLIMES. This is presented to the modeller via a formal augmented reality language as a first step towards practical modelling and simulation of multi-level living systems. Initial work focuses on the design and implementation of this visual language and calculus (VLC) and its graphical user interface. The results will be integrated within the current methodology and practices of theoretical biology and (personalized) medicine to deepen and to enhance the holistic understanding of life.",industry
10.1016/j.simpat.2015.07.004,Journal,Simulation Modelling Practice and Theory,scopus,2017-12-01,sciencedirect,A reinforcement learning methodology for a human resource planning problem considering knowledge-based promotion,https://api.elsevier.com/content/abstract/scopus_id/84939200331,"This paper addresses a combined problem of human resource planning (HRP) and production-inventory control for a high-tech industry, wherein the human resource plays a critical role. The main characteristics of this resource are the levels of “knowledge” and the learning process. The learning occurs during the production process in which a worker can promote to the upper knowledge level. Workers in upper levels have more productivity in the production. The objective is to maximize the expected profit by deciding on the optimal numbers of workers in various knowledge levels to fulfill both production and training requirement. As taking an action affects next periods’ decisions, the main problem is to find the optimal hiring policy of non-skilled workers in long-time horizon. Thus, we develop a reinforcement learning (RL) model to obtain the optimal decision for hiring workers under the demand uncertainty. The proposed interval-based policy of our RL model, in which for each state there are multiple choices, makes it more flexible. We also embed some managerial issues such as layoff and overtime-working hours into the model. To evaluate the proposed methodology, stochastic dynamic programming (SDP) and a conservative method implemented in a real case study are used. We study all these methods in terms of four criteria: average obtained profit, average obtained cost, the number of new-hired workers, and the standard deviation of hiring policies. The numerical results confirm that our developed method end up with satisfactory results compared to two other approaches.",industry
10.1016/j.engappai.2017.06.014,Journal,Engineering Applications of Artificial Intelligence,scopus,2017-11-01,sciencedirect,A configurable partial-order planning approach for field level operation strategies of PLC-based industry 4.0 automated manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85030713189,"The machine and plant automation domain is faced with an ever increasing demand for ensuring the adaptability of manufacturing facilities in context of Industry 4.0. Field level automation software plays a dominant role in strengthening the overall flexibility of manufacturing resources. Classical programming approaches based typically on signal-oriented languages result in disproportionate effort for ensuring necessary flexibility. To address this challenge, a novel approach based on artificial intelligence planning techniques is presented which is able to handle domain specific requirements while facilitating efficient, scalable problem solving. Throughout this article, a discussion of specific requirements on automated planning techniques for field level automation software in the machine and plant automation domain with respect to Industry 4.0 is provided. An intensive study on existing works and their drawbacks towards addressing these requirements is presented. The proposed configurable partial-order planning approach is based upon a combination of an adapted goal-based planning formulation and its reformulation by means of linear programming techniques. It is shown that the proposed approach is able to efficiently solve large planning problems by exhibiting positive scalability characteristics which indicates its applicability for real-size plants.",industry
10.1016/j.cie.2017.09.016,Journal,Computers and Industrial Engineering,scopus,2017-11-01,sciencedirect,Smart operators in industry 4.0: A human-centered approach to enhance operators’ capabilities and competencies within the new smart factory context,https://api.elsevier.com/content/abstract/scopus_id/85029476237,"As the Industry 4.0 takes shape, human operators experience an increased complexity of their daily tasks: they are required to be highly flexible and to demonstrate adaptive capabilities in a very dynamic working environment. It calls for tools and approaches that could be easily embedded into everyday practices and able to combine complex methodologies with high usability requirements. In this perspective, the proposed research work is focused on the design and development of a practical solution, called Sophos-MS, able to integrate augmented reality contents and intelligent tutoring systems with cutting-edge fruition technologies for operators’ support in complex man-machine interactions. After establishing a reference methodological framework for the smart operator concept within the Industry 4.0 paradigm, the proposed solution is presented, along with its functional and non-function requirements. Such requirements are fulfilled through a structured design strategy whose main outcomes include a multi-layered modular solution, Sophos-MS, that relies on Augmented Reality contents and on an intelligent personal digital assistant with vocal interaction capabilities. The proposed approach has been deployed and its training potentials have been investigated with field experiments. The experimental campaign results have been firstly checked to ensure their statistical relevance and then analytically assessed in order to show that the proposed solution has a real impact on operators’ learning curves and can make the difference between who uses it and who does not.",industry
10.1016/j.knosys.2017.07.007,Journal,Knowledge-Based Systems,scopus,2017-10-01,sciencedirect,Fit evaluation of virtual garment try-on by learning from digital pressure data,https://api.elsevier.com/content/abstract/scopus_id/85022033035,"Presently, garment fit evaluation mainly focuses on real try-on, and rarely deals with virtual try-on. With the rapid development of E-commerce, there is a profound growth of garment purchases through the internet. In this context, fit evaluation of virtual garment try-on is vital in the clothing industry. In this paper, we propose a Naive Bayes-based model to evaluate garment fit. The inputs of the proposed model are digital clothing pressures of different body parts, generated from a 3D garment CAD software; while the output is the predicted result of garment fit (fit or unfit). To construct and train the proposed model, data on digital clothing pressures and garment real fit was collected for input and output learning data respectively. By learning from these data, our proposed model can predict garment fit rapidly and automatically without any real try-on; therefore, it can be applied to remote garment fit evaluation in the context of e-shopping. Finally, the effectiveness of our proposed method was validated using a set of test samples. Test results showed that digital clothing pressure is a better index than ease allowance to evaluate garment fit, and machine learning-based garment fit evaluation methods have higher prediction accuracies.",industry
10.1016/j.eneco.2017.06.020,Journal,Energy Economics,scopus,2017-08-01,sciencedirect,"Composite forecasting approach, application for next-day electricity price forecasting",https://api.elsevier.com/content/abstract/scopus_id/85024479867,"Accurate forecasting of electricity prices can provide significant benefits to energy suppliers when allocating their assets and to energy consumers for defining an optimal portfolio. There are numerous methods that efficiently support the forecasting of time series, such as electricity prices, which have high volatility. However, the performance of these approaches varies depending on data sets and operational conditions. In this work, the concept of composite forecasting is presented and implemented in a retrospective study, in real industrial forecasting conditions to show the potential of forecast performance improvement and comparable high consistency of a forecast performance across different ‘Day Peak’ and ‘Day Base’ electricity price data sets for different seasons. As individual methods support vector regression, artificial neural networks and ridge regression are implemented. The forecast performances of these methods are evaluated and compared with their forecast combination using different error measures. The results show that composite forecasting processes with ‘inverse root mean squared error’ combination approach can generate, on average, a more accurate and robust forecast than using an individual methods or other combination schemas.",industry
10.1016/j.mineng.2017.02.013,Journal,Minerals Engineering,scopus,2017-08-01,sciencedirect,Designing gold extraction processes: Performance study of a case-based reasoning system,https://api.elsevier.com/content/abstract/scopus_id/85014728806,"This paper presents a method for externalising and formalising knowledge involving the selection of hydrometallurgical process flowsheets for gold extraction from ores. A case-based reasoning (CBR) system was built using an open source software myCBR 3.0. The aim of the systems is to recommend flowsheet alternatives for processing a potential gold ore deposit. Nine attributes: Ore type, Gold ore grade, Gold distribution, Gold grain size, Sulfide present, Arsenic sulfide, Copper sulfide, Iron sulfide and Clay present were modelled and several literature sources of actual gold mines and processes were used for acquiring cases for the system. After preliminary testing, functional evaluation of the built CBR system was carried out by using five real mining projects as test cases. Additionally, human experts in the field of gold hydrometallurgy were interviewed to demonstrate the benefits of the CBR system as it holds no human biases towards any processing techniques. It was found that the suggestions of the CBR system provided useful information and direction for further process design and performed well compared to the interviewed human experts, thus confirming that the system is of practical relevance to the process engineer designing an industrial gold processing plant. The current model was found to be a functioning basis for further development through additional attributes, adjusted attribute weighting and increased number of cases.",industry
10.1016/j.ifacol.2017.08.986,Conference Proceeding,IFAC-PapersOnLine,scopus,2017-07-01,sciencedirect,Using data mining methods for manufacturing process control,https://api.elsevier.com/content/abstract/scopus_id/85031805594,"The Industry 4.0 concept assumes that modern manufacturing systems generate huge amounts of data that must be collected, stored, managed and analysed. The case study is focused on predicting the manufacturing process behaviour according to production data. The paper presents the way of gaining knowledge about the future behaviour of manufacturing system by data mining predictive tasks. The proposed simulation model of the real manufacturing process was designed to obtain the data necessary for the control process. The predictions of the manufacturing process behaviour were implemented varying the input parameters using selected methods and techniques of data mining. The predicted process behaviour was verified using the simulation model.
                  The authors analysed different methods. The neural network method was selected for deploying new data by PMML files in the final phases. The objectives of the research are to design and verify the data mining tools in order to support the manufacturing system control by aiming at improving the decisionmaking process. Based on the prediction of the goal production outcomes, the actual control strategies can be precisely modified. Then they can be used in real manufacturing system without risks.",industry
10.1016/j.ifacol.2017.08.902,Conference Proceeding,IFAC-PapersOnLine,scopus,2017-07-01,sciencedirect,A Networked Production System to Implement Virtual Enterprise and Product Lifecycle Information Loops,https://api.elsevier.com/content/abstract/scopus_id/85031797675,"This paper is aimed at considering supply chain and related data management within an integrated vision of the product lifecycle management (PLM) implemented through the unified approach which is proper to the Industry 4.0 initiative. In particular, with the proposed manufacturing system architecture, decision support tools can use a unified repository fed by a factory replication application, powered by data from the field, even from remote production units. Such data allow to monitor the behaviour of the digital twin of the real machine and produces a digital twin of the real product, incorporating its actual characteristics measured by means of suitable acquiring systems (in the treated example: a 3D laser scanner). Moreover, it is provided a description of the plant technological subsystems that allow to share designing and manufacturing activities across multiple similar units located in remote areas. In this context of virtual enterprise, the supply chain management results as a key factor in enabling a cooperative approach.",industry
10.1016/j.infsof.2017.03.003,Journal,Information and Software Technology,scopus,2017-07-01,sciencedirect,Uncertainty-wise evolution of test ready models,https://api.elsevier.com/content/abstract/scopus_id/85015382293,"Context
                  Cyber-Physical Systems (CPSs), when deployed for operation, are inherently prone to uncertainty. Considering their applications in critical domains (e.g., healthcare), it is important that such CPSs are tested sufficiently, with the explicit consideration of uncertainty. Model-based testing (MBT) involves creating test ready models capturing the expected behavior of a CPS and its operating environment. These test ready models are then used for generating executable test cases. It is, therefore, necessary to develop methods that can continuously evolve, based on real operational data collected during the operation of CPSs, test ready models and uncertainty captured in them, all together termed as Belief Test Ready Models (BMs)
               
                  Objective
                  Our objective is to propose a model evolution framework that can interactively improve the quality of BMs, based on operational data. Such BMs are developed by one or more test modelers (belief agents) with their assumptions about the expected behavior of a CPS, its expected physical environment, and potential future deployments. Thus, these models explicitly contain subjective uncertainty of the test modelers.
               
                  Method
                  We propose a framework (named as UncerTolve) for interactively evolving BMs (specified with extended UML notations) of CPSs with subjective uncertainty developed by test modelers. The key inputs of UncerTolve include initial BMs of CPSs with known subjective uncertainty and real data collected from the operation of CPSs. UncerTolve has three key features: 1) Validating the syntactic correctness and conformance of BMs against real operational data via model execution, 2) Evolving objective uncertainty measurements of BMs via model execution, and 3) Evolving state invariants (modeling test oracles) and guards of transitions (modeling constraints for test data generation) of BMs with a machine learning technique.
               
                  Results
                  As a proof-of-concept, we evaluated UncerTolve with one industrial CPS case study, i.e., GeoSports from the healthcare domain. Using UncerTolve, we managed to evolve 51% of belief elements, 18% of states, and 21% of transitions as compared to the initial BM developed in an industrial setting.
               
                  Conclusion
                  
                     UncerTolve can successfully evolve model elements of the initial BM, in addition to objective uncertainty measurements using real operational data. The evolved model can be used to generate additional test cases covering evolved model elements and objective uncertainty. These additional test cases can be used to test the current and future deployments of a CPS to ensure that it will handle uncertainty gracefully during its operations.",industry
10.1016/j.engappai.2016.08.019,Journal,Engineering Applications of Artificial Intelligence,scopus,2017-06-01,sciencedirect,GPU-based parallel optimization of immune convolutional neural network and embedded system,https://api.elsevier.com/content/abstract/scopus_id/84995489085,"Up to now, the image recognition system has been utilized more and more widely in the security monitoring, the industrial intelligent monitoring, the unmanned vehicle, and even the space exploration. In designing the image recognition system, the traditional convolutional neural network has some defects such as long training time, easy over-fitting and high misclassification rate. In order to overcome these defects, we firstly used the immune mechanism to improve the convolutional neural network and put forward a novel immune convolutional neural network algorithm, after we analyzed the network structure and parameters of the convolutional neural network. Our algorithm not only integrated the location data of the network nodes and the adjustable parameters, but also dynamically adjusted the smoothing factor of the basis function. In addition, we utilized the NVIDIA GPU (Graphics Processing Unit) to accelerate the new immune convolutional neural network (ICNN) in parallel computing and built a real-time embedded image recognition system for this ICNN. The immune convolutional neural network algorithm was improved with CUDA programming and was tested with the sample data in the GPU-based environment. The GPU-based implementation of the novel immune convolutional neural network algorithm was made with the cuDNN, which was designed by NVIDIA for GPU-based accelerating of DNNs in machine learning. Experimental results show that our new immune convolutional neural network has higher recognition rate, more stable performance and faster computing speed than the traditional convolutional neural network.",industry
10.1016/j.neucom.2016.09.005,Journal,Neurocomputing,scopus,2017-05-10,sciencedirect,Wood moisture content prediction using feature selection techniques and a kernel method,https://api.elsevier.com/content/abstract/scopus_id/84996497604,"Wood is a renewable, abundant bio-energy and environment friendly resource. Woody biomass Moisture Content (
                        MC
                     ) is a key parameter for controlling the biofuel product qualities and properties. In this paper, we are interested in predicting 
                        MC
                      from data. The input impedance of half-wave dipole antenna when buried in the wood pile varies according to the permittivity of wood. Hence, the measurement of reflection coefficient, that gives information about the input impedance, depends directly on the 
                        MC
                      of wood. The relationship between the reflection coefficient measurements and the 
                        MC
                      is studied. Based upon this relationship, 
                        MC
                      predictive models that use machine learning techniques and feature selection methods are proposed. Numerical experiments using real world data show the relevance of the proposed approach that requires a limited computational power. Therefore, a real-time implementation for industrial processes is feasible.",industry
10.1016/j.jmsy.2017.02.007,Journal,Journal of Manufacturing Systems,scopus,2017-04-01,sciencedirect,Framework and development of fault detection classification using IoT device and cloud environment,https://api.elsevier.com/content/abstract/scopus_id/85014081409,"While Cyber-physical system (CPS) is considered as a key foundation for cyber manufacturing, many related frameworks and applications have been provided. This research suggests a new and effective CPS architecture for supporting multi-sites and multi-products manufacturing. As target processes, the manufacturing processes for vehicles’ High Intensity Discharge (HID) headlight and cable modules are considered. These modules are manufactured with several multi-manufacturing sites consisting of internal manufacturing tasks and intermediate outsourcing processes. In addition, they produce multiple types of HID cable modules with different components. These issues make it difficult to improve the qualities of the overall processes and to control those considering overall manufacturing plants and processes. In order to overcome these limitations, this research provides an Internet of Things (IoT) embedded cloud control architecture. The mixed flow issues are overcome with the cloud control server with the suggested framework. The developed IoT device detects several system status and transmits the signals. The data is analyzed for the fault detection classification (FDC) mechanism using deep learning based analytics. Then, the cyber manufacturing based simulation is executed using the provided multi-products queueing network model. The estimated simulation results is used for generating dynamic manufacturing decisions reflecting the real-time changes of the production environment. The suggested framework and its implementations can be used for various industrial processes and applications.",industry
10.1016/j.jmsy.2017.02.011,Journal,Journal of Manufacturing Systems,scopus,2017-04-01,sciencedirect,A fog computing-based framework for process monitoring and prognosis in cyber-manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85013912214,"Small- and medium-sized manufacturers, as well as large original equipment manufacturers (OEMs), have faced an increasing need for the development of intelligent manufacturing machines with affordable sensing technologies and data-driven intelligence. Existing monitoring systems and prognostics approaches are not capable of collecting the large volumes of real-time data or building large-scale predictive models that are essential to achieving significant advances in cyber-manufacturing. The objective of this paper is to introduce a new computational framework that enables remote real-time sensing, monitoring, and scalable high performance computing for diagnosis and prognosis. This framework utilizes wireless sensor networks, cloud computing, and machine learning. A proof-of-concept prototype is developed to demonstrate how the framework can enable manufacturers to monitor machine health conditions and generate predictive analytics. Experimental results are provided to demonstrate capabilities and utility of the framework such as how vibrations and energy consumption of pumps in a power plant and CNC machines in a factory floor can be monitored using a wireless sensor network. In addition, a machine learning algorithm, implemented on a public cloud, is used to predict tool wear in milling operations.",industry
10.1016/j.flowmeasinst.2016.10.001,Journal,Flow Measurement and Instrumentation,scopus,2017-04-01,sciencedirect,Intelligent recognition of gas-oil-water three-phase flow regime and determination of volume fraction using radial basis function,https://api.elsevier.com/content/abstract/scopus_id/85000420282,"The problem of how to accurately measure the flow rate of oil–gas–water mixtures in a pipeline remains one of the key challenges in the petroleum industry. This paper proposes a new methodology for identifying flow regimes and predicting volume fractions in gas-oil-water multiphase systems using dual energy fan-beam gamma-ray attenuation technique and artificial neural networks. The novelty of this study in comparison with previous works, is using just 4 extracted features (photo peaks of 241Am and 137Cs in 2 detectors) from the gamma ray spectrums instead of using the whole gamma ray spectrum, which reduces the undesired noises and also improves the speed of recognition in real situations. Radial basis function was used for developing the neural network model in MATLAB software in order to classify the flow patterns (annular, stratified and homogenous) and predict the value of volume fractions. The ideal and static theoretical models for flow regimes have been developed using MCNP-X code. The proposed networks could correctly recognize all the three different flow regimes and also determine volume fractions with mean absolute error of less than 5.68% according to the recognized regime.",industry
10.1016/j.compositesb.2016.12.050,Journal,Composites Part B: Engineering,scopus,2017-03-01,sciencedirect,Digitisation of manual composite layup task knowledge using gaming technology,https://api.elsevier.com/content/abstract/scopus_id/85009923562,"Increased market demand for composite products and shortage of expert laminators is compelling the composite industry to explore ways to acquire layup skills from experts and transfer them to novices and eventually to machines. There is a lack of holistic methods in literature for capturing composite layup skills especially involving complex moulds. This research aims to develop an informatics-based method, enabled by consumer-grade gaming technology and machine learning, to capture and digitise manufacturing task knowledge from skill-intensive hand layup. The digitisation is underpinned by the proposed human-workpiece interaction theory and implemented to automatically extract and decode key knowledge constituents such as layup strategies, ply manipulation techniques, motion mechanics and problem-solving during hand layup, collectively categorised as layup skills. The significance of this research is its potential to facilitate cost-effective transfer of skills from experts to novices, real-time automated supervision of hand layup and automation of layup tasks in the future.",industry
10.1016/j.jenvman.2016.10.056,Journal,Journal of Environmental Management,scopus,2017-02-01,sciencedirect,Improving the efficiency of dissolved oxygen control using an on-line control system based on a genetic algorithm evolving FWNN software sensor,https://api.elsevier.com/content/abstract/scopus_id/85006851639,"This work proposes an on-line hybrid intelligent control system based on a genetic algorithm (GA) evolving fuzzy wavelet neural network software sensor to control dissolved oxygen (DO) in an anaerobic/anoxic/oxic process for treating papermaking wastewater. With the self-learning and memory abilities of neural network, handling the uncertainty capacity of fuzzy logic, analyzing local detail superiority of wavelet transform and global search of GA, this proposed control system can extract the dynamic behavior and complex interrelationships between various operation variables. The results indicate that the reasonable forecasting and control performances were achieved with optimal DO, and the effluent quality was stable at and below the desired values in real time. Our proposed hybrid approach proved to be a robust and effective DO control tool, attaining not only adequate effluent quality but also minimizing the demand for energy, and is easily integrated into a global monitoring system for purposes of cost management.",industry
10.1016/j.simpat.2016.08.007,Journal,Simulation Modelling Practice and Theory,scopus,2017-02-01,sciencedirect,Intelligent simulation: Integration of SIMIO and MATLAB to deploy decision support systems to simulation environment,https://api.elsevier.com/content/abstract/scopus_id/84996798684,"Discrete-event simulation is a decision support tool which enables practitioners to model and analyze their own system behavior. Although simulation packages are capable of mimicking most tasks in a real-world system, there are some decision-making activities, which are beyond the reach of simulation packages. The Application Programmers Interface (API) of SIMIO provides a wide range of opportunities for researchers to develop their own logic and apply it during the simulation run. This paper illustrates how to deploy MATLAB, as a computational tool coupled with SIMIO, as a simulation package by using a new user-defined step instance named “CallMATLAB”. A manufacturing system case study is introduced where the CallMATLAB step instance is used to create an Iterative Optimization-based Simulation (IOS) model. This model is created to evaluate the performance of different optimizers. The benefits of this hybridization for other industries, including healthcare systems, supply chain management systems, and project management problems are discussed.",industry
10.1016/j.promfg.2017.07.167,Journal,Procedia Manufacturing,scopus,2017-01-01,sciencedirect,Towards Robust Early Stage Data Knowledge-based Inference Engine to Support Zero-defect Strategies in Manufacturing Environment,https://api.elsevier.com/content/abstract/scopus_id/85029884694,"Decision Support Systems are considered as a robust technology able to provide an advantage to several manufacturing companies. As part of the Z-Fact0r EU project, an autonomous and self-adjusted inference engine; namely the Early Stage-Decision Support System (ES-DSS) will be deployed. The scope is to facilitate real-time inspection, condition monitoring and control - diagnosis at the shop-floor, utilizing continuously mine multiple data streams and run the suitable models to monitor operations and quality performance, to classify products on the basis of quality metrics, as well to predict occurrence of defects and deviations from production and quality requirements.",industry
10.1016/j.procir.2017.03.093,Conference Proceeding,Procedia CIRP,scopus,2017-01-01,sciencedirect,Cyber-Physical Manufacturing Metrology Model (CPM<sup>3</sup>) for Sculptured Surfaces - Turbine Blade Application,https://api.elsevier.com/content/abstract/scopus_id/85028681766,"Cyber-Physical Manufacturing (CPM) and digital manufacturing represent the key elements for implementation of Industry 4.0 framework. Worldwide, Industry 4.0 becomes national research strategy in the field of engineering for the following ten years. The International Conference USA-EU-Far East-Serbia Manufacturing Summit was held from 31st May to 2nd June 2016 in Belgrade, Serbia. The result of the conference was the development of Industry 4.0 Model for Serbia as a framework for New Industrial Policy – Horizon 2020/2030.
                  Implementation of CPM in manufacturing systems generates “smart factory”. Products, resources, and processes within smart factory are realized and controlled through CPM model. This leads to significant advantages with respect to high product/process quality, real-time applications, savings in resources consumption, as well as, lower costs in comparison with classical manufacturing systems. Smart factory is designed in accordance with sustainable and service-oriented best business practices/models. It is based on optimization, flexibility, self-adaptability and learning, fault tolerance, and risk management. Complete manufacturing digitalization and digital factory are the key elements of Industry 4.0 Program.
                  In collaborative research, which we carry out in the field of quality control and manufacturing metrology at University of Belgrade, Faculty of Mechanical Engineering in Serbia and at Department of Mechanical Engineering, University of Texas, Austin in USA, three research areas are defined: (а) Digital manufacturing – towards Cloud Manufacturing Systems (as a basis for CPS), in which quality and metrology represent integral parts of process optimization based on Taguchi model, and (б) Cyber-Physical Quality Model (CPQM) – our approach, in which we have developed and tested intelligent model for prismatic parts inspection planning on CMM (Coordinate Measuring Machine). The third research area directs our efforts to the development of framework for Cyber-Physical Manufacturing Metrology Model (CPM3). CPM3 framework will be based on integration of digital product metrology information through metrology features recognition, and generation of global/local inspection plan for free-form surfaces; we will illustrate our approach using turbine blade example. This paper will present recent results of our research on CPM3.",industry
10.1016/j.procir.2017.03.115,Conference Proceeding,Procedia CIRP,scopus,2017-01-01,sciencedirect,Dynamic Analysis of Intelligent Coil Leveling Machine for Cyber-physical Systems Implementation,https://api.elsevier.com/content/abstract/scopus_id/85028650941,"In manufacturing industry, wider range variants and personalized productions are becoming formidable challenges that needed to be for smart manufacturing. In smart manufacturing, machines are connected cooperatively to seamlessly and quickly adjust production setting to reach market requirements. Furthermore, real-time production data visualization and evaluation are the keys to increase manufacturing productivity, efficiency, and flexibility. This integrated research is aimed to develop an intelligent coil leveling machine through dynamic analysis of real-time machine sensors network for cyber-physical systems implementation in smart manufacturing. In this proposed intelligent coil leveling machine, intelligent sensors network is embedded in the machine to allow real-time monitoring of the machine through feedback controlled system and cloud network to ensure optimized production with optimal machine setting instantly. Intelligent sensors network of the proposed coil leveling machine such as leveling roller indentation, leveling force, and coil curvature has been completed. Preliminary real-time dynamic monitoring of the leveling rollers and coil curvature has been accomplished. Following, real-time dynamic analysis is performed to demonstrate the implementation of the cyber-physical systems where machine learning intelligence can be achieved. Lastly, real-time cloud network monitoring are implemented to allow users to collect manufacturing data online. Through this research, conventional leveling machine can be transformed in which machine setting configurations can be adjusted to the production line through virtual cyber-physical system. Production data can be visualized and evaluated in real-time with precise and intelligent production strategies to ensure customer's requirements and to enhance production efficiency and flexibility in smart manufacturing of sheet metal coil.",industry
10.1016/j.procir.2017.03.125,Conference Proceeding,Procedia CIRP,scopus,2017-01-01,sciencedirect,Ant Colony Optimization Algorithms to Enable Dynamic Milkrun Logistics,https://api.elsevier.com/content/abstract/scopus_id/85028644167,"Flexibility in combination with high capacities are the main reasons for milkruns being one of the most popular intralogistics solutions. In most cases they are only used for static routes to always deliver the same material to the same stations. However, in the context of Industry 4.0, milkrun logistic also has become very popular for use cases where different materials have to be delivered to different stations in little time, so routes cannot be planned in advance anymore. As loading and unloading the milkrun requires a significant amount of time, beside the routing problem itself, both driving and loading times have to be taken into account. Especially in scenarios where high flexibility is required those times will vary significantly and thus are a crucial factor for obtaining the optimal solution. Although containing stochastic components, those times can be predicted by considering the optimal point of time for delivery. In consequence, the best tradeoff between short routes and optimal delivery times is in favor of the shortest route. To solve this optimization problem a biology-inspired method – the ant colony optimization algorithm – has been enhanced to obtain the best solution regarding the above-mentioned aspects. A manufacturing scenario was used to prove the ability of the algorithm in real world problems. It also demonstrates the ability to adapt to changes in manufacturing systems very quickly by dynamically modelling and simulating the processes in intralogistics. The paper describes the ant colony optimization algorithm with the necessary extensions to enable it for milkrun logistic problems. Additionally the implemented software environment to apply the algorithm in practice is explained.",industry
10.1016/j.petrol.2016.11.033,Journal,Journal of Petroleum Science and Engineering,scopus,2017-01-01,sciencedirect,A hybrid particle swarm optimization and support vector regression model for modelling permeability prediction of hydrocarbon reservoir,https://api.elsevier.com/content/abstract/scopus_id/85028257367,"The significance of accurate permeability prediction cannot be over-emphasized in oil and gas reservoir characterization. Support vector machine regression (SVR), a computational intelligence technique, has been very successful in the estimation of permeability and has been widely deployed due to its unique features. However, careful selection of SVR hyper-parameters is highly essential to its optimum performance and this task is traditionally done using trial and error approach (TE-SVR) which takes a lot of time and do not guarantee optimal selection of the hyper-parameters. In this work, the performance of particle swarm optimization (PSO) technique, a heuristic optimization technique, is investigated for the optimal selection of SVR hyper-parameters for the first time in modelling and characterization of hydrocarbon reservoir. The technique is capable of automatic selection of the optimum combination of SVR hyper-parameters resulting in higher predictive accuracy and generalization ability of the developed model. The resulting PSO-SVR model is compared to SVR models whose parameters are obtained through random search (RAND-SVR) and trial and error approach (TE-SVR). The comparison is done using real-life industrial datasets obtained during petroleum exploration from four distinct oil wells located in a Middle Eastern oil and gas field. Simulation results indicate that the PSO-SVR model outperforms all the other models. Error reduction of 15.1%, 26.15%, 12.32% and 7.1% are recorded for PSO-SVR model compared to ordinary SVR (TE-SVR) in well-A, well-B, well-C and well-D, respectively. Also, reduction of 12.8%, 23.97%, 2.51% and 0.11 are recorded when PSO-SVR and RAND-SVR results are compared in the respective wells. Furthermore, the results show the potential of the application of heuristics algorithms, such as PSO, in the optimization of computational intelligence techniques employed in hydrocarbon reservoir characterizations. Therefore, PSO technique is proposed for the optimization of SVR hyper-parameters in permeability prediction and reservoir characterization based on its superior performance over the commonly employed optimization techniques.",industry
10.1016/j.promfg.2017.07.091,Conference Proceeding,Procedia Manufacturing,scopus,2017-01-01,sciencedirect,Machine Learning-based CPS for Clustering High throughput Machining Cycle Conditions,https://api.elsevier.com/content/abstract/scopus_id/85023607399,"Cyber-physical systems (CPS) have opened up a wide range of opportunities in terms of performance analysis that can be applied directly to the machine tool industry and are useful for maintenance systems and machine designers. High-speed communication capabilities enable the data to be gathered, pre-processed and processed for the purpose of machine diagnosis. This paper describes a complete real-world CPS implementation cycle, ranging from machine data acquisition to processing and interpretation. In fact, the aim of this paper is to propose a CPS for machine component knowledge discovery based on clustering algorithms using real data from a machining process. Therefore, it compares three clustering algorithms –k-means, hierarchical agglomerative and Gaussian mixture models– in terms of their contribution to spindle performance knowledge during high throughput machining operation.",industry
10.1016/j.promfg.2017.04.039,Journal,Procedia Manufacturing,scopus,2017-01-01,sciencedirect,Digital Twin as Enabler for an Innovative Digital Shopfloor Management System in the ESB Logistics Learning Factory at Reutlingen - University,https://api.elsevier.com/content/abstract/scopus_id/85020859111,"Technologies for mapping the “digital twin” have been under development for approximately 20 years. Nowadays increasingly intelligent, individualized products encourages companies to respond innovatively to customer requirements and to handle the rising product variations quickly.
                  An integrated engineering network, spanning across the entire value chain, is operated to intelligently connect various company divisions, and to generate a business ecosystem for products, services and communities. The conditions for the digital twin are thereby determined in which the digital world can be fed into the real, and the real world back into the digital to deal such intelligent products with rising variations.
                  The term digital twin can be described as a digital copy of a real factory, machine, worker etc., that is created and can be independently expanded, automatically updated as well as being globally available in real time. Every real product and production site is permanently accompanied by a digital twin. First prototypes of such digital twins already exist in the ESB Logistics Learning Factory on a cloud- and app-based software that builds on a dynamic, multidimensional data and information model. A standardized language of the robot control systems via software agents and positioning systems has to be integrated. The aspect of the continuity of the real factory in the digital factory as an economical means of ensuring continuous actuality of digital models looks as the basis of changeability.
                  For the indoor localization sensor combinations that in addition to the hardware already contain the software required for the sensor data fusion should be used. Processing systems, scenario-live-simulations and digital shop floor management results in a mandatory procedural combination. Essential to the digital twin is the ability to consistently provide all subsystems with the latest state of all required information, methods and algorithms.",industry
10.1016/j.procir.2017.01.047,Conference Proceeding,Procedia CIRP,scopus,2017-01-01,sciencedirect,Using Graph-based Design Languages to Enhance the Creation of Virtual Commissioning Models,https://api.elsevier.com/content/abstract/scopus_id/85020027557,"‘Industrie 4.0’ based production systems are likely to change the way future products are manufactured. As information technology gains influence on these systems there is a chance of higher flexibility due to decentralized logic and artificial intelligence. All this leads to a higher complexity and also indeterminism is feasible. Therefore simulation technologies will become a mandatory requirement, especially virtual commissioning will get necessary as the amount of software is rising.
                  A lot of manpower is required to establish and maintain a virtual commissioning system as it needs a large database of standard components. Therefore in most cases small- and mid-sized companies are forced to avoid such technologies. Using graph-based design languages to create virtual commissioning models can help to solve this problem. The basic principle is to shape an abstract model of a production system which will then be individually built within the domain specific tools. One of these should be a virtual commissioning tool to evaluate the functionality of the built model. If a change in the design is necessary, the new virtual commissioning model can be regenerated automatically. This approach is even more reasonable, if graph-based design languages are used throughout the whole product life cycle.",industry
10.1016/j.procs.2017.01.213,Conference Proceeding,Procedia Computer Science,scopus,2017-01-01,sciencedirect,Hybrid Agents Implementation for the Control of the Construction Company,https://api.elsevier.com/content/abstract/scopus_id/85016095509,"Planning the project duration together with separate works is an essential element of managing the construction. The final duration depends on multiple factors, including the funds, customer requests, and capabilities of the construction company. In order to avoid additional costs in penalties or additional expenses, the management needs to estimate the real construction duration in advance, before the contract is signed. Further on, these terms need to be monitored both in whole and for the specific jobs in order to be able to edit further stages with regard of the remaining time, resources and used resources ratio. The development of a decision support system for the construction company is a pressing problem due to the growing demand in decision making persons’ labor automation in planning and monitoring the construction processes. The paper presents the model and the application experience for such a system.",industry
10.1016/j.rser.2016.11.046,Journal,Renewable and Sustainable Energy Reviews,scopus,2017-01-01,sciencedirect,Comparison of different discharge strategies of grid-connected residential PV systems with energy storage in perspective of optimal battery energy storage system sizing,https://api.elsevier.com/content/abstract/scopus_id/85006456716,"The paper presents a yearly comparison of different residential self-consumption-reducing discharge strategies for grid connected residential PV systems with the Battery Energy Storage System (BESS). Altogether, three discharge strategies are taken into consideration; base case, adaptive algorithm and an energy-market-oriented remote-controlled strategy. All of the presented strategies are strictly limited to available residential load reduction based on the current BESS regulations. Furthermore, the simulations are run on real-world measurement data. The adaptive “grid-friendly” discharge algorithm utilizes global optima and a moving-average calculation to maximize the self-energy consumption in regards to the peak grid load periods. On the other hand a remote-controlled discharge scenario is taken into consideration to maximize self-consumption and to decrease the grid load when the intraday energy prices are the highest, if there is opportunity to do so. The system size optimization equations are based on a hidden layer feedforward neural network system. The main goal of the network is to predict the corresponding equations for the optimization software. Based on the neural network results, an “easy-to-use” BESS-size economic optimization web-based application has been developed to demonstrate the feasibility of the different discharge methodologies and to make easier different manufactures’ BESS system comparisons.",industry
10.1016/j.paerosci.2016.10.001,Journal,Progress in Aerospace Sciences,scopus,2017-01-01,sciencedirect,An evolutionary outlook of air traffic flow management techniques,https://api.elsevier.com/content/abstract/scopus_id/85006413338,"In recent years Air Traffic Flow Management (ATFM) has become pertinent even in regions without sustained overload conditions caused by dense traffic operations. Increasing traffic volumes in the face of constrained resources has created peak congestion at specific locations and times in many areas of the world. Increased environmental awareness and economic drivers have combined to create a resurgent interest in ATFM as evidenced by a spate of recent ATFM conferences and workshops mediated by official bodies such as ICAO, IATA, CANSO the FAA and Eurocontrol. Significant ATFM acquisitions in the last 5 years include South Africa, Australia and India. Singapore, Thailand and Korea are all expected to procure ATFM systems within a year while China is expected to develop a bespoke system. Asia-Pacific nations are particularly pro-active given the traffic growth projections for the region (by 2050 half of all air traffic will be to, from or within the Asia-Pacific region). National authorities now have access to recently published international standards to guide the development of national and regional operational concepts for ATFM, geared to Communications, Navigation, Surveillance/Air Traffic Management and Avionics (CNS+A) evolutions. This paper critically reviews the field to determine which ATFM research and development efforts hold the best promise for practical technological implementations, offering clear benefits both in terms of enhanced safety and efficiency in times of growing air traffic. An evolutionary approach is adopted starting from an ontology of current ATFM techniques and proceeding to identify the technological and regulatory evolutions required in the future CNS+A context, as the aviation industry moves forward with a clearer understanding of emerging operational needs, the geo-political realities of regional collaboration and the impending needs of global harmonisation.",industry
10.1016/j.ijpe.2016.10.021,Journal,International Journal of Production Economics,scopus,2017-01-01,sciencedirect,Single-hidden layer neural networks for forecasting intermittent demand,https://api.elsevier.com/content/abstract/scopus_id/84994731834,"Managing intermittent demand is a vital task in several industrial contexts, and good forecasting ability is a fundamental prerequisite for an efficient inventory control system in stochastic environments. In recent years, research has been conducted on single-hidden layer feedforward neural networks, with promising results. In particular, back-propagation has been adopted as a gradient descent-based algorithm for training networks. However, when managing a large number of items, it is not feasible to optimize networks at item level, due to the effort required for tuning the parameters during the training stage. A simpler and faster learning algorithm, called the extreme learning machine, has been therefore proposed in the literature to address this issue, but it has never been tried for forecasting intermittent demand. On the one hand, an extensive comparison of single-hidden layer networks trained by back-propagation is required to improve our understanding of them as predictors of intermittent demand. On the other hand, it is also worth testing extreme learning machines in this context, because of their lower computational complexity and good generalisation ability.
                  In this paper, neural networks trained by back-propagation and extreme learning machines are compared with benchmark neural networks, as well as standard forecasting methods for intermittent demand on real-time series, by combining different input patterns and architectures. A statistical analysis is then conducted to validate the best performance through different aggregation levels. Finally, some insights for practitioners are presented to improve the potential of neural networks for implementation in real environments.",industry
10.1016/j.energy.2016.09.096,Journal,Energy,scopus,2016-12-01,sciencedirect,Generation of realistic scenarios for multi-agent simulation of electricity markets,https://api.elsevier.com/content/abstract/scopus_id/84988734772,"Most market operators provide daily data on several market processes, including the results of all market transactions. The use of such data by electricity market simulators is essential for simulations quality, enabling the modelling of market behaviour in a much more realistic and efficient way. RealScen (Realistic Scenarios Generator) is a tool that creates realistic scenarios according to the purpose of the simulation: representing reality as it is, or on a smaller scale but still as representative as possible. This paper presents a novel methodology that enables RealScen to collect real electricity markets information and using it to represent market participants, as well as modelling their characteristics and behaviours. This is done using data analysis combined with artificial intelligence. This paper analyses the way players' characteristics are modelled, particularly in their representation in a smaller scale, simplifying the simulation while maintaining the quality of results. A study is also conducted, comparing real electricity market values with the market results achieved using the generated scenarios. The conducted study shows that the scenarios can fully represent the reality, or approximate it through a reduced number of representative software agents. As a result, the proposed methodology enables RealScen to represent markets behaviour, allowing the study and understanding of the interactions between market entities, and the study of new markets by assuring the realism of simulations.",industry
10.1016/j.epsr.2016.07.018,Journal,Electric Power Systems Research,scopus,2016-12-01,sciencedirect,Classification for consumption data in smart grid based on forecasting time series,https://api.elsevier.com/content/abstract/scopus_id/84981303127,"One of the most important tasks of present day smart grid implementations is to classify different types of consumers (households, office buildings and industrial plants) because they may be served by the power supplier with different parameters, rates, contracts.
                  In this paper, we propose a novel classification scheme for smart grid systems where the collected data are processed in order to increase the efficiency of electricity transportation as well as demand-supply management. The new scheme is based on forecasting the consumption time series obtained from a smart meter. Class assignment is determined using the forecast error. Different linear and nonlinear methods were tested based on the corresponding assumptions on the statistical behavior of the underlying consumption time series.
                  Performance tests were carried out with simulations in order to demonstrate the capabilities and to compare the achieved performance of the proposed scheme with existing solutions. The simulations have been executed using (i) artificially generated consumption data, which data came from a bottom-up semi-Markov model and (ii) real, measured power consumption data as well. The parameters of the model have been validated on real data. The numerical results have demonstrated that our method can better model and classify the consumption patterns of office-buildings than the existing methods. As a result, the proposed method may prove to be a promising classification tool.",industry
10.1016/j.eswa.2016.06.043,Journal,Expert Systems with Applications,scopus,2016-11-30,sciencedirect,Data quality assessment of maintenance reporting procedures,https://api.elsevier.com/content/abstract/scopus_id/84977274884,"Today’s largest and fastest growing companies’ assets are no longer physical, but rather digital (software, algorithms...). This is all the more true in the manufacturing, and particularly in the maintenance sector where quality of enterprise maintenance services are closely linked to the quality of maintenance data reporting procedures. If quality of the reported data is too low, it can results in wrong decision-making and loss of money. Furthermore, various maintenance experts are involved and directly concerned about the quality of enterprises’ daily maintenance data reporting (e.g., maintenance planners, plant managers...), each one having specific needs and responsibilities. To address this Multi-Criteria Decision Making (MCDM) problem, and since data quality is hardly considered in existing expert maintenance systems, this paper develops a maintenance reporting quality assessment (MRQA) dashboard that enables any company stakeholder to easily – and in real-time – assess/rank company branch offices in terms of maintenance reporting quality. From a theoretical standpoint, AHP is used to integrate various data quality dimensions as well as expert preferences. A use case describes how the proposed MRQA dashboard is being used by a Finnish multinational equipment manufacturer to assess and enhance reporting practices in a specific or a group of branch offices.",industry
10.1016/j.resconrec.2016.03.012,Journal,"Resources, Conservation and Recycling",scopus,2016-11-01,sciencedirect,Implementation of OPTIMASS to optimise municipal wastewater sludge processing chains: Proof of concept,https://api.elsevier.com/content/abstract/scopus_id/85028239611,"In sludge management, sludge is increasingly perceived as a marketable product rather than as a waste material. This awareness in combination with the variety of factors influencing the optimal management strategy and disposal route, introduces the need to optimise the sludge treatment throughout the whole chain instead of only minimising its production. In this paper, OPTIMASS, a mixed integer linear programming model to optimise strategic and tactical decisions in biomass-based supply chains, is proposed in order to meet this need. The applicability of OPTIMASS is illustrated through its implementation with a view to minimise the overall global warming impact of a real municipal wastewater sludge processing chain in “region X”. A first scenario addresses the optimisation of the allocation and treatment of municipal wastewater sludge within the current network. Second, OPTIMASS is used to identify the optimal location(s) for new drying facilities in this chain. Finally, the effect on the optimal chain of changes in municipal wastewater sludge production and of changes in global warming impact of the cement industry as a disposal route is evaluated.
                  The analysis reveals that municipal wastewater sludge processing chains can be considered to be instances of the generic biomass-based supply chain and that the OPTIMASS tool can be applied to support strategic and tactical decisions for optimising sludge management in case new technologies, new treatment facility locations, new disposal options, etc. are at stake. The validity of the OPTIMASS approach is confirmed by the close correspondence between its outcome and the results of a decision support system, specifically developed for the municipal wastewater sludge processing chain.",industry
10.1016/j.cie.2016.07.031,Journal,Computers and Industrial Engineering,scopus,2016-11-01,sciencedirect,TIMSPAT – Reachability graph search-based optimization tool for colored Petri net-based scheduling,https://api.elsevier.com/content/abstract/scopus_id/84991205081,"The combination of Petri net (PN) modeling with AI-based heuristic search (HS) algorithms (PNHS) has been successfully applied as an integrated approach to deal with scheduling problems that can be transformed into a search problem in the reachability graph. While several efficient HS algorithms have been proposed albeit using timed PN, the practical application of these algorithms requires an appropriate tool to facilitate its development and analysis. However, there is a lack of tool support for the optimization of timed colored PN (TCPN) models based on the PNHS approach for schedule generation. Because of its complex data structure, TCPN-based scheduling has often been limited to simulation-based performance analysis only. Also, it is quite difficult to evaluate the strength and tractability of algorithms for different scheduling scenarios due to the different computing platforms, programming languages and data structures employed. In this light, this paper presents a new tool called TIMSPAT, developed to overcome the shortcomings of existing tools. Some features that distinguish this tool are the collection of several HS algorithms, XML-based model integration, the event-driven exploration of the timed state space including its condensed variant, localized enabling of transitions, the introduction of static place, and the easy-to-use syntax statements. The tool is easily extensible and can be integrated as a component into existing PN simulators and software environments. A comparative study is performed on a real-world eyeglass production system to demonstrate the application of the tool for scheduling purposes.",industry
10.1016/j.csi.2016.03.003,Journal,Computer Standards and Interfaces,scopus,2016-11-01,sciencedirect,Intelligent software product line configurations: A literature review,https://api.elsevier.com/content/abstract/scopus_id/84964669931,"A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product.",industry
10.1016/j.knosys.2016.07.022,Journal,Knowledge-Based Systems,scopus,2016-10-15,sciencedirect,Software test quality rating: A paradigm shift in swarm computing for software certification,https://api.elsevier.com/content/abstract/scopus_id/84979704090,"Recently, software quality issues have emerged to be recognized as a fundamental point as we actualize an extensive growth of organizations involved in software industries. Still, these organizations cannot ensure the quality of their products; therefore abandoning customers in uncertainties. Software certification is the branch of quality by means that quality requires to be measured prior to certification admitting process. However, creating an official certification model is difficult due to the deficiency of data in the domain of software engineering. This research participates in solving the problem of assessing software quality by introducing a model that handles a fuzzy inference engine to mix both of the processes–driven and application-driven quality assurance procedures. The fundamental purpose of the suggested model is to enhance the compactness and the interpretability of the system's fuzzy rules via engaging an ant colony optimization algorithm (ACO), which attempts to discover a good rule description by a set of compound rules initially represented with traditional single rules. The proposed model is a fitting one that can be seen as practicing certification models that have already been created from software quality domain data and modifying them to a context-specific data. The model has been tested by a case study and the results have confirmed feasibility and practicality of the model in a real environment.",industry
10.1016/j.jclepro.2016.05.091,Journal,Journal of Cleaner Production,scopus,2016-10-01,sciencedirect,Developing an ant colony approach for green closed-loop supply chain network design: a case study in gold industry,https://api.elsevier.com/content/abstract/scopus_id/84988844589,"The forward/reverse logistics network design is an important and strategic issue due to its effects on efficiency and responsiveness of a supply chain. In practice, it is needed to formulate and solve real problems through efficient algorithms in a reasonable time. Hence, this paper tries to cover real case problem with a multi-objective model and an integrated forward/reverse logistics network design. Further, the model is customized and implemented for a case study in gold industry where the reverse logistics play crucial role. A new solution approach is applied for the proposed 7-layer network of the case study and the solutions are achieved in order solve the current difficulties of the investigated supply chain. This paper seeks to address how a multi objective logistics model in the gold industry can be created and solved through an efficient meta-heuristic algorithm. A green approach based on the CO2 emission is considered in the network design approach. The developed model includes four echelons in the forward direction and three echelons in the reverse. First, an integer linear programming model is developed to minimize costs and emissions. Then, in order to solve the model, an algorithm based on ant colony optimization is developed. The performance of the proposed algorithm has been compared with the optimum solutions of the LINGO software through various numerical examples based on the random data and real-world instances. The evaluation studies demonstrate that the proposed model is practical and applicable and the developed algorithm is reliable and efficient. The results prove the managerial implications of the model and the solution approach in terms of presenting appropriate modifications to the mangers of the selected supply chain. Further, a Taguchi-based parameter setting is undertaken to ensure using the appropriate parameters for the algorithm.",industry
10.1016/j.adhoc.2016.06.011,Journal,Ad Hoc Networks,scopus,2016-10-01,sciencedirect,Feature selection for performance characterization in multi-hop wireless sensor networks,https://api.elsevier.com/content/abstract/scopus_id/84977657790,"Current trends in Wireless Sensor Networks are faced with the challenge of shifting from testbeds in controlled environments to real-life deployments, characterized by unattended and long-term operation. The network performance in such settings depends on various factors, ranging from the operational space, the behavior of the protocol stack, the intra-network dynamics, and the status of each individual node. As such, characterizing the network’s high-level performance based exclusively on link-quality estimation, can yield episodic snapshots on the performance of specific, point-to-point links. The objective of this work is to provide an integrated framework for the unsupervised selection of the dominant features that have crucial impact on the performance of end-to-end links, established over a multi-hop topology. Our focus is on compressing the original feature vector of network parameters, by eliminating redundant network attributes with predictable behavior. The proposed approach is implemented alongside different cases of protocol stacks and evaluated on data collected from real-life deployments in rural and industrial environments. Discussions on the efficacy of the proposed scheme, and the dominant network characteristics per deployment are offered.",industry
10.1016/j.ijpe.2016.06.005,Journal,International Journal of Production Economics,scopus,2016-09-01,sciencedirect,Hybrid flow shop batching and scheduling with a bi-criteria objective,https://api.elsevier.com/content/abstract/scopus_id/84975859979,"This paper addresses the hybrid flow shop batching and scheduling problem where sequence-dependent family setup times are present and the objective is to simultaneously minimize the weighted sum of the total weighted completion time and total weighted tardiness. In particular, it disregards the group technology assumptions by allowing for the possibility of splitting pre-determined groups of jobs into inconsistent batches in order to improve the operational efficiency. A benchmark of small size problems is considered to show the benefits of batching on group scheduling. Since the problem is strongly NP-hard, several algorithms based upon tabu search are developed at three levels, which move back and forth between batching and scheduling phases. Two algorithms incorporate tabu search into the framework of path-relinking to exploit the information on good solutions. These tabu search/path-relinking algorithms comprise several distinguishing features including two relinking procedures to effectively construct paths and the stage-based improvement procedure to consider the move interdependency. The best tabu search algorithm as a local search algorithm is compared to a population-based algorithm, and the superiority of the former over the latter is shown using a statistical experiment. The initial solution finding mechanism is implemented to trigger the search into the solution space. The efficiency and effectiveness of the best algorithm is verified with the help of the results found by CPLEX. The results show that the best algorithm, based on tabu search/path relinking and the stage-based improvement procedure, could find solutions at least as good as CPLEX, but in drastically shorter computational time. In order to reflect the real industry requirements, dynamic machine availability times, dynamic job release times, machine eligibility and machine capability for processing jobs, desired lower bounds on batch sizes, and job skipping are considered.",industry
10.1016/j.jpowsour.2016.05.092,Journal,Journal of Power Sources,scopus,2016-08-30,sciencedirect,Prognostics of Proton Exchange Membrane Fuel Cells stack using an ensemble of constraints based connectionist networks,https://api.elsevier.com/content/abstract/scopus_id/84975104897,"Proton Exchange Membrane Fuel Cell (PEMFC) is considered the most versatile among available fuel cell technologies, which qualify for diverse applications. However, the large-scale industrial deployment of PEMFCs is limited due to their short life span and high exploitation costs. Therefore, ensuring fuel cell service for a long duration is of vital importance, which has led to Prognostics and Health Management of fuel cells. More precisely, prognostics of PEMFC is major area of focus nowadays, which aims at identifying degradation of PEMFC stack at early stages and estimating its Remaining Useful Life (RUL) for life cycle management. This paper presents a data-driven approach for prognostics of PEMFC stack using an ensemble of constraint based Summation Wavelet- Extreme Learning Machine (SW-ELM) models. This development aim at improving the robustness and applicability of prognostics of PEMFC for an online application, with limited learning data. The proposed approach is applied to real data from two different PEMFC stacks and compared with ensembles of well known connectionist algorithms. The results comparison on long-term prognostics of both PEMFC stacks validates our proposition.",industry
10.1016/j.cviu.2016.03.018,Journal,Computer Vision and Image Understanding,scopus,2016-07-01,sciencedirect,"Wize Mirror-a smart, multisensory cardio-metabolic risk monitoring system",https://api.elsevier.com/content/abstract/scopus_id/84963813154,"In the recent years personal health monitoring systems have been gaining popularity, both as a result of the pull from the general population, keen to improve well-being and early detection of possibly serious health conditions and the push from the industry eager to translate the current significant progress in computer vision and machine learning into commercial products. One of such systems is the Wize Mirror, built as a result of the FP7 funded SEMEOTICONS (SEMEiotic Oriented Technology for Individuals CardiOmetabolic risk self-assessmeNt and Self-monitoring) project. The project aims to translate the semeiotic code of the human face into computational descriptors and measures, automatically extracted from videos, multispectral images, and 3D scans of the face. The multisensory platform, being developed as the result of that project, in the form of a smart mirror, looks for signs related to cardio-metabolic risks. The goal is to enable users to self-monitor their well-being status over time and improve their life-style via tailored user guidance. This paper is focused on the description of the part of that system, utilising computer vision and machine learning techniques to perform 3D morphological analysis of the face and recognition of psycho-somatic status both linked with cardio-metabolic risks. The paper describes the concepts, methods and the developed implementations as well as reports on the results obtained on both real and synthetic datasets.",industry
10.1016/j.jobe.2016.04.010,Journal,Journal of Building Engineering,scopus,2016-06-01,sciencedirect,"Modeling and simulation controlling system of HVAC using fuzzy and predictive (radial basis function, RBF) controllers",https://api.elsevier.com/content/abstract/scopus_id/84965095816,"Heating, ventilating and air conditioning (HVAC) systems are used in buildings, industry and agriculture to provide thermal and humidity comfort. Modeling of HVAC system can help to design precise controlling systems. In this study, a HVAC system had been modeled using MATLAB simulation software that had been developed using a fuzzy controlling system and radial basis function (RBF) model of artificial neural network (ANN) as a predictive control system. Results of the modeled systems were extracted and compared with actual system. In order to compare results of the modeled and actual systems, comparing parameters, such as mean absolute error (MAE), root mean square error (RMSE), mean absolute percentage/relative error (MAPE) and coefficient of Pearson correlation (r) were applied. The results indicated that, the modeled systems was accurately controlling the system and the difference between real and modeled system was also close. In the results as a whole, the predictive controller (RBF network) has the best performance compared to fuzzy model.",industry
10.1016/j.epsr.2016.03.012,Journal,Electric Power Systems Research,scopus,2016-06-01,sciencedirect,Metalearning to support competitive electricity market players' strategic bidding,https://api.elsevier.com/content/abstract/scopus_id/84962522494,"Electricity markets are becoming more competitive, to some extent due to the increasing number of players that have moved from other sectors to the power industry. This is essentially resulting from incentives provided to distributed generation. Relevant changes in this domain are still occurring, such as the extension of national and regional markets to continental scales. Decision support tools have thereby become essential to help electricity market players in their negotiation process. This paper presents a metalearner to support electricity market players in bidding definition. The proposed metalearner uses a dynamic artificial neural network to create its own output, taking advantage on several learning algorithms already implemented in ALBidS (Adaptive Learning strategic Bidding System). The proposed metalearner considers different weights for each strategy, based on their individual performance. The metalearner's performance is analysed in scenarios based on real electricity markets data using MASCEM (Multi-Agent Simulator for Competitive Electricity Markets). Results show that the proposed metalearner is able to provide higher profits to market players when compared to other current methodologies and that results improve over time, as consequence of its learning process.",industry
10.1016/j.ins.2016.01.001,Journal,Information Sciences,scopus,2016-05-01,sciencedirect,Solving integrated process planning and scheduling problem with constructive meta-heuristics,https://api.elsevier.com/content/abstract/scopus_id/84957879888,"For product manufacturing, process planning is to select a series of manufacturing processes according to the product design specification, and scheduling is to allocate manufacturing resources such as machines and tools to these processes. It is a common problem that the process plan and the schedule are not able to cope with the changes in real time manufacturing. Integrated process planning and scheduling (IPPS) is to conduct the process planning and scheduling functions concurrently, with the aim to improve the dynamic responsiveness of the production schedule. This paper investigates the formulation and implementation of constructive meta-heuristics for solving IPPS problems. To begin with, a model representation is established to express IPPS problems with AND/OR graphs. With this model representation, a generic framework is proposed for implementing constructive meta-heuristics in the solution model. The generic framework provides a common procedure for the constructive meta-heuristics, which encapsulates the calculation of the search frontier and state transitions, and provides two interfaces for accommodating different constructive search algorithms. Ant colony optimization (ACO), a commonly-used algorithm which possesses all typical characteristics of constructive meta-heuristics, is adopted as a representative example for illustrating the implementation. Experiments and tests are conducted to validate the proposed system. The single objective minimizing the makespan is set for evaluating the performance of the proposed system. Experimental results of the benchmark problems have shown the effectiveness and high performance of the proposed approach based on the integration of the generic framework and ACO strategy.",industry
10.1016/B978-0-08-100571-2.00008-7,Book,Information Systems for the Fashion and Apparel Industry,scopus,2016-04-08,sciencedirect,Intelligent demand forecasting systems for fast fashion,https://api.elsevier.com/content/abstract/scopus_id/84979900385,"Sales forecasting in the fashion industry has been a very challenging issue for decades. Recently, the concept of fast fashion has emerged as a successful strategy for retailers. In terms of sales forecasting, this concept involves new approaches to deal with specific features such as the limited amount of historical data and shortened time for the computation. The literature review of existing methods in this domain shows that many models have been proposed. They are mainly based on artificial intelligence techniques. Among these techniques, we focus on the two models that arise as references for long-term and short-term forecasts to develop a two-stage sales forecasting system. Associated with a store replenishment model, which is inspired from a method implemented in a famous fast fashion brand, we propose to simulate our two-stage forecasting system on real data to evaluate the real benefits of advanced forecasting techniques for fast fashion retailing.",industry
10.1016/j.ymssp.2015.09.025,Journal,Mechanical Systems and Signal Processing,scopus,2016-03-01,sciencedirect,Classification of acoustic emission signals using wavelets and Random Forests: Application to localized corrosion,https://api.elsevier.com/content/abstract/scopus_id/84961163563,"This paper aims to propose a novel approach to classify acoustic emission (AE) signals deriving from corrosion experiments, even if embedded into a noisy environment. To validate this new methodology, synthetic data are first used throughout an in-depth analysis, comparing Random Forests (RF) to the k-Nearest Neighbor (k-NN) algorithm. Moreover, a new evaluation tool called the alter-class matrix (ACM) is introduced to simulate different degrees of uncertainty on labeled data for supervised classification. Then, tests on real cases involving noise and crevice corrosion are conducted, by preprocessing the waveforms including wavelet denoising and extracting a rich set of features as input of the RF algorithm. To this end, a software called RF-CAM has been developed. Results show that this approach is very efficient on ground truth data and is also very promising on real data, especially for its reliability, performance and speed, which are serious criteria for the chemical industry.",industry
10.1016/j.mechatronics.2015.06.015,Journal,Mechatronics,scopus,2016-03-01,sciencedirect,Symbolic discrete-time planning with continuous numeric action parameters for agent-controlled processes,https://api.elsevier.com/content/abstract/scopus_id/84939200231,"In industrial domains such as manufacturing control, a trend away from centralized planning and scheduling towards more flexible distributed agent-based approaches could be observed over recent years. To be of practical relevance, the local control mechanisms of the autonomous agents must be able to dependably adhere and dynamically adjust to complex numeric goal systems like business key performance indicators in an economically beneficial way. However, planning with numeric state variables and objectives still poses a challenging task within the field of artificial intelligence (AI).
                  In this article, a new general-purpose AI planning approach is presented that operates in two stages and extends existing domain-independent modeling formalisms like PDDL with continuous (i.e., infinite-domain) numeric action parameters, which are currently still unsupported by state-of-the-art AI planners. In doing so, it enables the solution of mathematical optimization problems at the action level of the planning tasks, which are inherent to many real-world control problems. To deal with certain difficulties concerning reliable and fast detection of action applicability that arise when planning with real-valued action parameters, the implemented planner allows resorting to an adjustable “satisficing” strategy by means of partial execution and subsequent repair of infeasible plans over the course of time. The functioning of the system is evaluated in a multi-agent simulation of a shop floor control scenario with focus on the effects the possible problem cases and different degrees of satisficing have on attained plan quality and total planning time. As the results demonstrate the basic practicability of the approach for the given setting, this contribution constitutes an important step towards the effective and dependable integration of complex numeric goal systems and non-linear multi-criteria optimization tasks into autonomous agent-controlled industrial processes in a reusable, domain-independent way.",industry
10.1016/j.seppur.2015.12.056,Journal,Separation and Purification Technology,scopus,2016-02-29,sciencedirect,Rapid cultivation of aerobic granule for the treatment of solvent recovery raffinate in a bench scale sequencing batch reactor,https://api.elsevier.com/content/abstract/scopus_id/84954169665,"Aerobic granular sludge (AGS) was cultivated in a bench scale sequencing batch reactor within 21days. Strategy of the rapid startup was inoculated with part of mature AGS during cultivation, while aerobic biological selector was implemented for the inhibiting outgrowth of filamentous bacteria and fast selection of zoogloea bacteria. Then, the cultivated AGS was employed for the treatment of solvent recovery raffinate. Stable AGS was successfully domesticated after 55days under strategy of gradually increase the proportion of real wastewater in influent. The domesticated AGS was orange, irregular shape, smooth and compact. SVI, SV30/SV5, MLVSS/MLSS, EPS, PN/PS, average particle size, granulation rate, (SOUR)H and (SOUR)N of AGS were 19.06mL/g, 0.97, 0.55, 30.05mg/g MLVSS, 1.10, 1.28mm, 98.87%, 32.47 and 7.97mg O2/hgVSS respectively. Finally, COD, TIN, NH4
                     +–N and TP of the effluent were lower than 25.9mg/L, 1.64mg/L, 1.13mg/L and 0.21mg/L, and their removal rate was more than 98.43%, 97.12%, 98.02% and 98.09% respectively. Thus, COD, TP removal, nitrification and denitrification were realized in a single bioreactor. The result indicated that the feasibility of AGS for high C/N ratio industrial wastewater treatment.",industry
10.1016/j.ifacol.2016.11.160,Conference Proceeding,IFAC-PapersOnLine,scopus,2016-01-01,sciencedirect,Neural networks as a diagnosing tool for industrial level measurement through non-contacting radar type and support to the decision for its better application,https://api.elsevier.com/content/abstract/scopus_id/85006454620,"The aim of this study was to develop an analysis tool based on artificial neural networks (ANN) to detect level measurement problems with free wave propagation radars. The trend of using this type of radar has been growing in the last ten years mainly because of its easy installation on the top of tanks and reservoirs, and for its low rate maintenance comparing to other level measurement technologies. For the experiments, a Rosemount radar was used and the training of the neural network was based on the data from the software Radar Master. Therefore, some network topologies in different scenarios were tested and it was possible to demonstrate the efficiency of the ANN with accuracy rate between 94.44 to 100% for the first experiment with networks using 10, 20 or 50 neurons in the hidden layer. This technique was applied in a real industrial application, a sugar and ethanol mill, and accuracy rate was about 87,0 to 96,1%. This methodology can be applied to asset management software for diagnosis report or troubleshooting which would increase the level measurement reliability and plant safety.",industry
10.1016/j.procir.2016.06.096,Conference Proceeding,Procedia CIRP,scopus,2016-01-01,sciencedirect,Transfer of Model of Innovative Smart Factory to Croatian Economy Using Lean Learning Factory,https://api.elsevier.com/content/abstract/scopus_id/84999791899,"Croatia's manufacturing industry faces many problems and obstacles that have a large impact on its competitiveness. Insufficiently educated and unskilled personnel, particularly in the production and management fields, are decreasing competitiveness that is necessary for survival in the global market. Objective of project Innovative Smart Enterprise is to establish a special learning environment in one Laboratory asLean Learning Factory, i.e. simulation of a real factory through specialized equipment. The Lean Learning Factory's mission is to integrate needed knowledge into the engineering curriculum. Therefore, Lean Learning Factory at University of Split is in continuous developing process to support practice-based engineering curriculum with possibility of learning necessary tools and methods, using didactic games or real life products and equipment. Solution proposal for best balance between toys and real products consider design and production line development for product Karet. It is a traditional and original product from Croatia, so it will raise enthusiasmin learning process in both students and industry employees. Two assembly lines will be developed, one traditionally equipped and one intelligent, networked, flexible, and fully improved by Lean tools. By deeper analysis of both assembly lines, hybrid assembly lines could be designed, to balance on one side assembly tact time according to customer demand and total cost of installation and running on the other side. Methods and tools adapted and implemented, in both design and analysis process for optimization of this hybrid assembly line would be scaled and adjusted for industry use as part as knowledge transfer from university to enterprises.",industry
10.1016/j.proeng.2016.07.416,Conference Proceeding,Procedia Engineering,scopus,2016-01-01,sciencedirect,Automated Detection of Faults in Wastewater Pipes from CCTV Footage by Using Random Forests,https://api.elsevier.com/content/abstract/scopus_id/84997822163,"Sewer systems require regular inspection in order to ensure their satisfactory condition. As most sewer networks consist of pipes too small for engineers to traverse, CCTV footage is used to record the interior of these pipes. This footage is manually analysed by qualified engineers, to determine the condition of the pipe and the presence of any faults. We propose a methodology, which automatically detects faults within the CCTV footage. This has the potential to dramatically reduce the time required to process the large volume of CCTV footage produced during a survey. The proposed methodology first characterises localised regions of each video frame using multiscale GIST features. Extremely randomised trees are then used to learn a classifier that distinguishes between frames showing a fault and normal frames. The technique is tested on 670 video segments from real sewer inspections of a variety of pipes, supplied by Wessex Water. Detection performance is assessed by plotting receiver operating characteristics and quantifying the area under the curve. Preliminary results indicate high detection accuracy of 88% and an area under the ROC curve of 96%. The machine learning used reduces the footage to a selection of frames containing faults, which can be quickly identified (whether by an engineer or another piece of software), showing promise for use in industrial wastewater network surveys.",industry
10.1016/j.ifacol.2016.07.167,Conference Proceeding,IFAC-PapersOnLine,scopus,2016-01-01,sciencedirect,A proposal for teaching SCADA systems using Virtual Industrial Plants in Engineering Education,https://api.elsevier.com/content/abstract/scopus_id/84994803392,"The main objectives of SCADA (Supervisory Control And Data Acquisition) systems are the supervisory analysis of the system, control algorithms validation, and data acquisition. These systems are normally implemented according to the international standards: UNE-EN ISO 9241, ISAIOI-Human-Machine Interfaces, ISA S5, and in the case treated in this paper The Spanish Royal Decree 488/1997. This paper presents a software architecture for the development of educational laboratories, through industrial virtual plants which models and logic are implemented in Matlab® and used within LabVIEW® through an appropriate protocol. Lab VIEW® from National Instruments, a specific purpose software for this kind of applications, was used, since it allows us to provide a friendly interface, to perform communications, data acquisition and the information management. In addition, to illustrate the use of the proposed architecture, different virtual industrial plants for students of different Bachelor and Master degrees in engineering at the University of Almeria have been developed. This paper shows the different virtual industrial plants that have been developed using SCADA systems to facilitate students’ learning of basic concepts and techniques for an Industrial Informatics course.",industry
10.1016/B978-0-444-63428-3.50407-0,Book Series,Computer Aided Chemical Engineering,scopus,2016-01-01,sciencedirect,Process Integration: Pinch Analysis and Mathematical Programming - Directions for Future Development,https://api.elsevier.com/content/abstract/scopus_id/84994259521,"Numerous studies have been performed process systems engineering field for improving the efficiency of supplying and using energy, water and other resources and consequently for reducing the emissions of greenhouse gases, volatile organic compounds and other pollutants, accumulating a significant body of methods, applications and results. It has become apparent that the resource inputs and effluents of industrial processes and the other units including the business centres, civic objects and even agricultural plants can and are often connected with each other. Most industrial plants and the other units throughout the world still use more energy and water than necessary, they are proven cases in the range 20 – 30 %, emitting too large volumes of Greenhouse Gases and other pollutants.
                  Water-saving measures and the reuse of water may reduce groundwater consumption by as much as 25 – 30 %. Usually reducing resource consumption is achieved by increasing internal recycling and the reuse of energy and material streams. Projects for improving process resource efficiencies can be very beneficial and also potentially improve the public perception of the companies.
                  Motivating, launching and carrying out such projects, however, involve appropriate optimisation, based on adequate process models, applied within the framework of appropriate resource minimisation strategies and procedures. Process Integration supporting process design, integration and optimisation has been around for nearly 45 years. It has been closely related to the development of process systems engineering, as well as utilising mathematical modelling and information technology.
                  In the broader sense Process Integration methods can be classified into those relying on process based insight and targeting on the one hand, mainly employing targeting, heuristics and artificial intelligence—AI. On the other hand are the methods employing detailed mathematical models usually implemented as algebraic models with embedded superstructures in the case of process network synthesis. The methods relying on thermodynamic insights have been first published in the early 1980-s (Linnhoff and Flower, 1978) as well as those using mathematical programming—MP (Papoulias and Grossmann, 1983). There can also be a combined approach (Klemeš and Kravanja, 2013).
                  On the one hand, the concept relying on thermodynamic and/or physical insights using the well-known Pinch Analysis has been the more widely accepted in both academia and industry. Process Integration has thus converged towards two schools of thought, the thermodynamic based (Pinch) and the mathematically based MP, each having its own advantages and drawbacks. The thermodynamic school has mostly preceded that of the MP in generating ideas based on engineering creativity. The MP school has enacted its ideas and described them as explicit mathematical models for solving advanced PI problems.
                  The collaboration between both approaches has been widening, taking from each other the more applicable parts. Its development has been accelerating as the combined methodology has been able to provide answers and support for important issues regarding economic development—energy, water and resources better utilisation and savings. This contribution is targeted towards a short overview of recent achievements and future challenges.",industry
10.1016/j.procir.2015.12.071,Conference Proceeding,Procedia CIRP,scopus,2016-01-01,sciencedirect,Enhancing Constraint Propagation in ACO-based Schedulers for Solving the Job Shop Scheduling Problem,https://api.elsevier.com/content/abstract/scopus_id/84968773482,"Increasing number of variants lead to growing complexity in planning processes in production. Not only is the initial planning a tremendous task if there is a huge variety of products but also reacting to changes becomes more frequent and more demanding. Many algorithms being able to solve the static problem need to perform a full recalculation if there is disturbance in production which makes them too time consuming for instant reactions to changes in production.
                  Ant Colony Optimization (ACO) has proven its potentials in solving the theoretical Job Shop Scheduling Problem offering the advantage of not needing an entire recalculation in the case of changes. But when using the algorithm for calculation in real time scenarios with returning data from production plants several restrictions have to be fulfilled. The reaction to those restriction is currently not sufficiently provided by implementations of the ACO which prevents the use in practical applications. These restrictions are modelled as constraints that can for example involve the reaction to disturbances like failures or manual changes. But also considering transportation times or providing the possibility to realize batch processes is discussed. There are different possibilities to realize the reactions to restrictions in ACO, but in this paper they are modelled as constraints affecting the ACO during optimization. The constraint propagation is implemented by restricting the selection of succeeding edges, an approach that only has little impact on computational performance.
                  In this paper the concept of constraining the Ant Colony Optimization in Job Shop Scheduling is being introduced and explained. Subsequently the demand for additional constraints is presented and enhancements to the existing approach are defined and commented theoretically.",industry
10.1016/j.eswa.2015.07.016,Journal,Expert Systems with Applications,scopus,2016-01-01,sciencedirect,Applying supplier selection methodologies in a multi-stakeholder environment: A case study and a critical assessment,https://api.elsevier.com/content/abstract/scopus_id/84944345232,"In the contemporary global market, supplier selection represents a crucial process for enhancing firms’ competitiveness. In firms operating in low-complexity sectors, supplier selection generally leverages on few significant variables (price, delivery time, quality) and it is often left to the buyers’ experience. On the other hand, in industries characterised by remarkable product complexity, supplier selection systems gain the characteristics of a multi-stakeholder and multi-criteria problem, which needs to be theoretically formalised and realistically adapted to specific contexts.
                  An increasing number of researches have been devoted to the development of different methodologies to cope with this problem. Nevertheless, while the number of applications is growing, there is little empirical evidence of the practical usefulness of such tools, that are mainly tested on numerical examples or computational experiments and focused on a dyadic version of the problem, overlooking the wider set of actors involved in the decision-making problem. The result is a clear dichotomy between academic theory and business practice.
                  Therefore, the paper contributes to understand the above dichotomy by evaluating the applicability to real-world multi-stakeholder problems of the two main approaches proposed in the literature to deal with supplier selection, the analytic hierarchic process (AHP) and the fuzzy set theory (FST). Based on an industrial case study, a thorough discussion is developed, dealing with the issues arising during the implementation and practical functioning of such decision support systems, also providing provide practical guidelines and managerial implications.",industry
10.1016/j.ultsonch.2015.07.022,Journal,Ultrasonics Sonochemistry,scopus,2016-01-01,sciencedirect,"Impact of ultrasound on solid-liquid extraction of phenolic compounds from maritime pine sawdust waste. Kinetics, optimization and large scale experiments",https://api.elsevier.com/content/abstract/scopus_id/84938390300,"Maritime pine sawdust, a by-product from industry of wood transformation, has been investigated as a potential source of polyphenols which were extracted by ultrasound-assisted maceration (UAM). UAM was optimized for enhancing extraction efficiency of polyphenols and reducing time-consuming. In a first time, a preliminary study was carried out to optimize the solid/liquid ratio (6g of dry material per mL) and the particle size (0.26cm2) by conventional maceration (CVM). Under these conditions, the optimum conditions for polyphenols extraction by UAM, obtained by response surface methodology, were 0.67W/cm2 for the ultrasonic intensity (UI), 40°C for the processing temperature (T) and 43min for the sonication time (t). UAM was compared with CVM, the results showed that the quantity of polyphenols was improved by 40% (342.4 and 233.5mg of catechin equivalent per 100g of dry basis, respectively for UAM and CVM). A multistage cross-current extraction procedure allowed evaluating the real impact of UAM on the solid–liquid extraction enhancement. The potential industrialization of this procedure was implemented through a transition from a lab sonicated reactor (3L) to a large scale one with 30L volume.",industry
10.1016/j.simpat.2015.05.011,Journal,Simulation Modelling Practice and Theory,scopus,2015-11-01,sciencedirect,A flexible framework for accurate simulation of cloud in-memory data stores,https://api.elsevier.com/content/abstract/scopus_id/84947019341,"In-memory (transactional) data stores, also referred to as data grids, are recognized as a first-class data management technology for cloud platforms, thanks to their ability to match the elasticity requirements imposed by the pay-as-you-go cost model. On the other hand, determining how performance and reliability/availability of these systems vary as a function of configuration parameters, such as the amount of cache servers to be deployed, and the degree of in-memory replication of slices of data, is far from being a trivial task. Yet, it is an essential aspect of the provisioning process of cloud platforms, given that it has an impact on the amount of cloud resources that are planned for usage. To cope with the issue of predicting/analysing the behavior of different configurations of cloud in-memory data stores, in this article we present a flexible simulation framework offering skeleton simulation models that can be easily specialized in order to capture the dynamics of diverse data grid systems, such as those related to the specific (distributed) protocol used to provide data consistency and/or transactional guarantees. Besides its flexibility, another peculiar aspect of the framework lies in that it integrates simulation and machine-learning (black-box) techniques, the latter being used to capture the dynamics of the data-exchange layer (e.g. the message passing layer) across the cache servers. This is a relevant aspect when considering that the actual data-transport/networking infrastructure on top of which the data grid is deployed might be unknown, hence being not feasible to be modeled via white-box (namely purely simulative) approaches. We also provide an extended experimental study aimed at validating instances of simulation models supported by our framework against execution dynamics of real data grid systems deployed on top of either private or public cloud infrastructures. Particularly, our validation test-bed has been based on an industrial-grade open-source data grid, namely Infinispan by JBoss/Red-Hat, and a de-facto standard benchmark for NoSQL platforms, namely YCSB by Yahoo. The validation study has been conducted by relying on both public and private cloud systems, scaling the underlying infrastructure up to 100 (resp. 140) Virtual Machines for the public (resp. private) cloud case. Further, we provide some experimental data related to a scenario where our framework is used for on-line capacity planning and reconfiguration of the data grid system.",industry
10.1016/j.compbiomed.2015.07.015,Journal,Computers in Biology and Medicine,scopus,2015-11-01,sciencedirect,"Implementation of a web based universal exchange and inference language for medicine: Sparse data, probabilities and inference in data mining of clinical data repositories",https://api.elsevier.com/content/abstract/scopus_id/84941884468,"We extend Q-UEL, our universal exchange language for interoperability and inference in healthcare and biomedicine, to the more traditional fields of public health surveys. These are the type associated with screening, epidemiological and cross-sectional studies, and cohort studies in some cases similar to clinical trials. There is the challenge that there is some degree of split between frequentist notions of probability as (a) classical measures based only on the idea of counting and proportion and on classical biostatistics as used in the above conservative disciplines, and (b) more subjectivist notions of uncertainty, belief, reliability, or confidence often used in automated inference and decision support systems. Samples in the above kind of public health survey are typically small compared with our earlier “Big Data” mining efforts. An issue addressed here is how much impact on decisions should sparse data have. We describe a new Q-UEL compatible toolkit including a data analytics application DiracMiner that also delivers more standard biostatistical results, DiracBuilder that uses its output to build Hyperbolic Dirac Nets (HDN) for decision support, and HDNcoherer that ensures that probabilities are mutually consistent. Use is exemplified by participating in a real word health-screening project, and also by deployment in a industrial platform called the BioIngine, a cognitive computing platform for health management.",industry
10.1016/j.eswa.2015.04.036,Journal,Expert Systems with Applications,scopus,2015-07-28,sciencedirect,Clustering and visualization of failure modes using an evolving tree,https://api.elsevier.com/content/abstract/scopus_id/84937967581,"Despite the popularity of Failure Mode and Effect Analysis (FMEA) in a wide range of industries, two well-known shortcomings are the complexity of the FMEA worksheet and its intricacy of use. To the best of our knowledge, the use of computation techniques for solving the aforementioned shortcomings is limited. As such, the idea of clustering and visualization pertaining to the failure modes in FMEA is proposed in this paper. A neural network visualization model with an incremental learning feature, i.e., the evolving tree (ETree), is adopted to allow the failure modes in FMEA to be clustered and visualized as a tree structure. In addition, the ideas of risk interval and risk ordering for different groups of failure modes are proposed to allow the failure modes to be ordered, analyzed, and evaluated in groups. The main advantages of the proposed method lie in its ability to transform failure modes in a complex FMEA worksheet to a tree structure for better visualization, while maintaining the risk evaluation and ordering features. It can be applied to the conventional FMEA methodology without requiring additional information or data. A real world case study in the edible bird nest industry in Sarawak (Borneo Island) is used to evaluate the usefulness of the proposed method. The experiments show that the failure modes in FMEA can be effectively visualized through the tree structure. A discussion with FMEA users engaged in the case study indicates that such visualization is helpful in comprehending and analyzing the respective failure modes, as compared with those in an FMEA table. The resulting tree structure, together with risk interval and risk ordering, provides a quick and easily understandable framework to elucidate important information from complex FMEA forms; therefore facilitating the decision-making tasks by FMEA users. The significance of this study is twofold, viz., the use of a computational visualization approach to tackling two well-known shortcomings of FMEA; and the use of ETree as an effective neural network learning paradigm to facilitate FMEA implementations. These findings aim to spearhead the potential adoption of FMEA as a useful and usable risk evaluation and management tool by the wider community.",industry
10.1016/j.measurement.2015.06.004,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2015-07-06,sciencedirect,A signal pre-processing algorithm designed for the needs of hardware implementation of neural classifiers used in condition monitoring,https://api.elsevier.com/content/abstract/scopus_id/84934767009,"Gearboxes have a significant influence on the durability and reliability of a power transmission system. Currently, extensive research studies are being carried out to increase the reliability of gearboxes working in the energy industry, especially with a focus on planetary gears in wind turbines and bucket wheel excavators. In this paper, a signal pre-processing algorithm designed for condition monitoring of planetary gears working in non-stationary operation is presented. The algorithm is dedicated for hardware implementation on Field Programmable Gate Arrays (FPGAs). The purpose of the algorithm is to estimate the features of a vibration signal that are related to failures, e.g. misalignment and unbalance. These features can serve as the components of an input vector for a neural classifier. The approach proposed here has several important benefits: it is resistant to small speed fluctuations up to 7%, it can be performed in real-time conditions and its implementation does not require many resources of FPGAs.",industry
10.1016/j.ijfoodmicro.2015.03.010,Journal,International Journal of Food Microbiology,scopus,2015-07-02,sciencedirect,A strategy to establish food safety model repositories,https://api.elsevier.com/content/abstract/scopus_id/84926308733,"Transferring the knowledge of predictive microbiology into real world food manufacturing applications is still a major challenge for the whole food safety modelling community. To facilitate this process, a strategy for creating open, community driven and web-based predictive microbial model repositories is proposed. These collaborative model resources could significantly improve the transfer of knowledge from research into commercial and governmental applications and also increase efficiency, transparency and usability of predictive models. To demonstrate the feasibility, predictive models of Salmonella in beef previously published in the scientific literature were re-implemented using an open source software tool called PMM-Lab. The models were made publicly available in a Food Safety Model Repository within the OpenML for Predictive Modelling in Food community project. Three different approaches were used to create new models in the model repositories: (1) all information relevant for model re-implementation is available in a scientific publication, (2) model parameters can be imported from tabular parameter collections and (3) models have to be generated from experimental data or primary model parameters. All three approaches were demonstrated in the paper. The sample Food Safety Model Repository is available via: http://sourceforge.net/projects/microbialmodelingexchange/files/models and the PMM-Lab software can be downloaded from http://sourceforge.net/projects/pmmlab/. This work also illustrates that a standardized information exchange format for predictive microbial models, as the key component of this strategy, could be established by adoption of resources from the Systems Biology domain.",industry
10.1016/j.ifacol.2015.08.045,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-07-01,sciencedirect,Downhole pressure estimation using committee machines and neural networks,https://api.elsevier.com/content/abstract/scopus_id/84992511260,"In gas-lifted oil wells the monitoring of downhole pressure plays an important role. However, the permanent downhole gauge (PDG) sensor often fails. Because maintenance or replacement of PDGs is usually unfeasible, soft-sensors are promising alternatives to monitor the downhole pressure in the case of sensor failure. In this paper, a data-driven soft-sensor is implemented to estimate the downhole pressure using committee machines composed by finite impulse response (FIR) neural networks. Experimental results in three real datasets of the same oil well indicate that the identified soft-sensor is able to predict the downhole pressure with satisfactory accuracy. The model input variables were selected by statistical tests which increased insight concerning such variables. Committee machines outperformed single-model soft-sensors on experimental data.",industry
10.1016/j.jal.2014.11.005,Journal,Journal of Applied Logic,scopus,2015-06-01,sciencedirect,Implementation and testing of a soft computing based model predictive control on an industrial controller,https://api.elsevier.com/content/abstract/scopus_id/84922903117,"This work presents a real time testing approach of an Intelligent Multiobjective Nonlinear-Model Predictive Control Strategy (iMO-NMPC). The goal is the testing and analysis of the feasibility and reliability of some Soft Computing (SC) techniques running on a real time industrial controller. In this predictive control strategy, a Multiobjective Genetic Algorithm is used together with a Recurrent Artificial Neural Network in order to obtain the control action at each sampling time. The entire development process, from the numeric simulation of the control scheme to its implementation and testing on a PC-based industrial controller, is also presented in this paper. The computational time requirements are discussed as well. The obtained results show that the SC techniques can be considered also to tackle highly nonlinear and coupled complex control problems in real time, thus optimising and enhancing the response of the control loop. Therefore this work is a contribution to spread the SC techniques in on-line control applications, where currently they are relegated mainly to be used off-line, as is the case of optimal tuning of control strategies.",industry
10.1016/j.bios.2014.07.084,Journal,Biosensors and Bioelectronics,scopus,2015-05-05,sciencedirect,"Lytic enzymes as selectivity means for label-free, microfluidic and impedimetric detection of whole-cell bacteria using ALD-Al<inf>2</inf>O<inf>3</inf> passivated microelectrodes",https://api.elsevier.com/content/abstract/scopus_id/84922271489,"Point-of-care (PoC) diagnostics for bacterial detection offer tremendous prospects for public health care improvement. However, such tools require the complex combination of the following performances: rapidity, selectivity, sensitivity, miniaturization and affordability. To meet these specifications, this paper presents a new selectivity method involving lysostaphin together with a CMOS-compatible impedance sensor for genus-specific bacterial detection. The method enables the sample matrix to be directly flown on the polydopamine-covered sensor surface without any pre-treatment, and considerably reduces the background noise. Experimental proof-of-concept, explored by simulations and confirmed through a setup combining simultaneous optical and electrical real-time monitoring, illustrates the selective and capacitive detection of Staphylococcus epidermidis in synthetic urine also containing Enterococcus faecium. While providing capabilities for miniaturization and system integration thanks to CMOS compatibility, the sensors show a detection limit of ca. 108 (CFU/mL).min in a 1.5μL microfluidic chamber with an additional setup time of 50min. The potentials, advantages and limitations of the method are also discussed.",industry
10.1016/j.compind.2015.05.001,Journal,Computers in Industry,scopus,2015-05-04,sciencedirect,Artificial cognitive control with self-x capabilities: A case study of a micro-manufacturing process,https://api.elsevier.com/content/abstract/scopus_id/84955410573,"Nowadays, even though cognitive control architectures form an important area of research, there are many constraints on the broad application of cognitive control at an industrial level and very few systematic approaches truly inspired by biological processes, from the perspective of control engineering. Thus, our main purpose here is the emulation of human socio-cognitive skills, so as to approach control engineering problems in an effective way at an industrial level. The artificial cognitive control architecture that we propose, based on the shared circuits model of socio-cognitive skills, seeks to overcome limitations from the perspectives of computer science, neuroscience and systems engineering. The design and implementation of artificial cognitive control architecture is focused on four key areas: (i) self-optimization and self-leaning capabilities by estimation of distribution and reinforcement-learning mechanisms; (ii) portability and scalability based on low-cost computing platforms; (iii) connectivity based on middleware; and (iv) model-driven approaches. The results of simulation and real-time application to force control of micro-manufacturing processes are presented as a proof of concept. The proof of concept of force control yields good transient responses, short settling times and acceptable steady-state error. The artificial cognitive control architecture built into a low-cost computing platform demonstrates the suitability of its implementation in an industrial setup.",industry
10.1016/j.ifacol.2015.06.036,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-05-01,sciencedirect,Dexrov: Dexterous undersea inspection and maintenance in presence of communication latencies,https://api.elsevier.com/content/abstract/scopus_id/84992521813,"Underwater inspection and maintenance (e.g. in the oil & gas industry) are demanding and costly activities for which ROV based setups are often deployed in addition or in substitution to deep divers - contributing to operations risks and costs cutting. However the operation of a ROV requires significant off-shore dedicated manpower to handle and operate the robotic platform. In order to reduce the burden of operations, DexROV proposes to work out more cost effective and time efficient ROV operations, where manned support is in a large extent delocalized onshore (i.e. from a ROV control center), possibly at a large distance from the actual operations, relying on satellite communications. The proposed scheme also makes provision for advanced dexterous manipulation capabilities, exploiting human expertise when deemed useful. The outcomes of the project will be integrated and evaluated in a series of tests and evaluation campaigns, culminating with a realistic deep sea (1,300 meters) trial.",industry
10.1016/j.ifacol.2015.06.159,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-05-01,sciencedirect,A benchmark dataset for depth sensor based activity recognition in a manufacturing process,https://api.elsevier.com/content/abstract/scopus_id/84953879013,Algorithms for automated recognition of human activities are crucial for supporting the next generation of process measures in manufacturing. While there is active research underway for many sensor systems and algorithms they will need to be tested in real-world conditions in order to mature and become robust or generalized enough for broad deployment in industry. In this paper we present a case study and dataset from a real-world setting along with three performance measures for six common classifiers. The intent is to provide a dataset and baseline performance level metrics so that others may compare their activity recognition algorithms to a common standard.,industry
10.1016/j.ifacol.2015.06.228,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-05-01,sciencedirect,Multicast dataset synchronization and agent negotiation in distributed manufacturing control systems,https://api.elsevier.com/content/abstract/scopus_id/84953870369,"Multi agent systems represent an elegant approach for the control architecture of manufacturing systems. Distributed control architectures have the potential to achieve greater flexibility by being capable of local decision making based on real time reasoning. One of the main challenges of these distributed architectures is represented by the capability to synchronize the production data across all execution points in a reliable and consistent fashion. In this context, this paper aims to resolve the problems associated with real time production data synchronization in distributed multi-agent control systems by proposing a common dataset synchronized across all agent entities using multicast network communication. On top of this common dataset approach, an agent negotiation mechanism is proposed that addresses the operation sequencing and resource allocation in decentralized operation model. The pilot implementation is using JADE multi agent platform and JGroups for real time data synchronization and NetLogo for abstract representation of the simulation system. Experimental results gathered from the pilot implementation are discussed.",industry
10.1016/j.isatra.2014.11.011,Journal,ISA Transactions,scopus,2015-05-01,sciencedirect,Online monitoring and control of particle size in the grinding process using least square support vector regression and resilient back propagation neural network,https://api.elsevier.com/content/abstract/scopus_id/84929271125,"Particle size soft sensing in cement mills will be largely helpful in maintaining desired cement fineness or Blaine. Despite the growing use of vertical roller mills (VRM) for clinker grinding, very few research work is available on VRM modeling. This article reports the design of three types of feed forward neural network models and least square support vector regression (LS-SVR) model of a VRM for online monitoring of cement fineness based on mill data collected from a cement plant. In the data pre-processing step, a comparative study of the various outlier detection algorithms has been performed. Subsequently, for model development, the advantage of algorithm based data splitting over random selection is presented. The training data set obtained by use of Kennard–Stone maximal intra distance criterion (CADEX algorithm) was used for development of LS-SVR, back propagation neural network, radial basis function neural network and generalized regression neural network models. Simulation results show that resilient back propagation model performs better than RBF network, regression network and LS-SVR model. Model implementation has been done in SIMULINK platform showing the online detection of abnormal data and real time estimation of cement Blaine from the knowledge of the input variables. Finally, closed loop study shows how the model can be effectively utilized for maintaining cement fineness at desired value.",industry
10.1016/j.asoc.2015.03.034,Journal,Applied Soft Computing Journal,scopus,2015-04-30,sciencedirect,A genetic algorithm based decision support system for the multi-objective node placement problem in next wireless generation network,https://api.elsevier.com/content/abstract/scopus_id/84929176273,"The node placement problem involves positioning and configuring infrastructure for wireless networks. Applied to next generation networks, it establishes a new wireless architecture able to integrate heterogeneous components that can collaborate and exchange data. Furthermore, the heterogeneity of wireless networks makes the problem more intractable. This paper presents a novel multi-objective node placement problem that optimizes concurrently four objectives: maximizing communication coverage, minimizing the active structures’ costs, maximizing of the total capacity bandwidth and minimizing the noise level in the network. Known to be 
                        NP
                     -hard, the problem can be approached by applying heuristics mainly for large problem instances. As the number of nodes to place is not determined beforehand; we propose to apply a multi-objective variable-length genetic algorithm (VLGA) that simultaneously searches for the optimal number, positions and nature of heterogeneous nodes and communication devices. The performance of the VLGA is highlighted through the implementation of a decision support system (DSS) applied to the surveillance maritime problem using real data instances. We compare the ability of the proposed algorithm with an existing multi-objective model from the literature in order to validate its effectiveness in dealing with heterogeneous components. The results show that the proposed model well fits the network architecture constraints with a better balance between the objectives applied to the surveillance problem.",industry
10.1016/j.mee.2015.01.018,Journal,Microelectronic Engineering,scopus,2015-04-20,sciencedirect,An FPGA based human detection system with embedded platform,https://api.elsevier.com/content/abstract/scopus_id/84922572817,"Focusing on the computing speed of the practical machine learning based human detection system at the testing (detecting) stage to reach the real-time requirement in an embedded platform, the idea of iterative computing HOG with FPGA circuit design is proposed. The completed HOG accelerator contains gradient calculation circuit module and histogram accumulation circuit module. The linear SVM classification algorithm producing a number of necessary weak classifiers is combined with Adaboost algorithm to establish a strong classifier. The human detection is successfully implemented on a portable embedded platform to reduce the system cost and size. Experimental result shows that the performance error of accuracy appears merely about 0.1–0.4% in comparison between the presented FPGA based HW/SW co-design and the PC based pure software. Meanwhile, the computing speed achieves the requirement of a real-time embedded system, 15fps.",industry
10.1016/B978-0-12-800341-1.00001-2,Book,Industrial Agents: Emerging Applications of Software Agents in Industry,scopus,2015-03-12,sciencedirect,Software Agent Systems,https://api.elsevier.com/content/abstract/scopus_id/84944408386,"Agents and multi-agent systems are one of the most fascinating topics in computer science. They attracted and unified not only researchers from nearly all computer science areas but also researchers from other core disciplines such as psychology, sociology, biology, or control engineering. In the meantime, agent-based systems successfully prove their usefulness in many different real-life application areas, especially industrial ones. This is a clear sign that this discipline has become mature. This chapter presents a comprehensive state-of-the-art introduction into advanced software agents and multi-agent systems. Properties and types of agents and multi-agent systems are discussed, which include precise definitions of both. A successful cooperation between agents is only possible if they can communicate in an efficient and semantically meaningful way. Thus, relevant communication strategies are discussed. Agent-based applications can be very powerful, complex systems. Their development can profit a lot from adequate support tools. Different development support options and environments are discussed in some detail. Due to their nature, multi-agent systems are excellent candidates for the realization of comprehensive simulations, especially if the individuality and uniqueness of components of the simulation environment play an important role. The second part of the chapter addresses supporting technologies and concepts. Ontologies, self-organization and emergence, and swarm intelligence and stigmergy are introduced and discussed in some detail.",industry
10.1016/j.isatra.2014.09.019,Journal,ISA Transactions,scopus,2015-03-01,sciencedirect,Soft sensor for real-time cement fineness estimation,https://api.elsevier.com/content/abstract/scopus_id/84926259783,"This paper describes the design and implementation of soft sensors to estimate cement fineness. Soft sensors are mathematical models that use available data to provide real-time information on process variables when the information, for whatever reason, is not available by direct measurement. In this application, soft sensors are used to provide information on process variable normally provided by off-line laboratory tests performed at large time intervals. Cement fineness is one of the crucial parameters that define the quality of produced cement. Providing real-time information on cement fineness using soft sensors can overcome limitations and problems that originate from a lack of information between two laboratory tests. The model inputs were selected from candidate process variables using an information theoretic approach. Models based on multi-layer perceptrons were developed, and their ability to estimate cement fineness of laboratory samples was analyzed. Models that had the best performance, and capacity to adopt changes in the cement grinding circuit were selected to implement soft sensors. Soft sensors were tested using data from a continuous cement production to demonstrate their use in real-time fineness estimation. Their performance was highly satisfactory, and the sensors proved to be capable of providing valuable information on cement grinding circuit performance. After successful off-line tests, soft sensors were implemented and installed in the control room of a cement factory. Results on the site confirm results obtained by tests conducted during soft sensor development.",industry
10.1016/j.compag.2014.12.010,Journal,Computers and Electronics in Agriculture,scopus,2015-02-01,sciencedirect,A Decision Support System to design modified atmosphere packaging for fresh produce based on a bipolar flexible querying approach,https://api.elsevier.com/content/abstract/scopus_id/84921031926,"To design new packaging for fresh food, stakeholders of the food chain express their needs and requirements, according to some goals and objectives. These requirements can be gathered into two groups: (i) fresh food related characteristics and (ii) packaging intrinsic characteristics. Modified Atmosphere Packaging (MAP) is an efficient way to delay senescence and spoilage and thus to extend the very short shelf life of respiring products such as fresh fruits and vegetables. Consequently, packaging O2/CO2 permeabilities must fit the requirements of fresh fruits and vegetable as predicted by virtual MAP simulating tools. Beyond gas permeabilities, the choice of a packaging material for fresh produce includes numerous other factors such as the cost, availability, potential contaminants of raw materials, process ability, and waste management constraints. For instance, the user may have the following multi-criteria query for his/her product asking for a packaging with optimal gas permeabilities that guarantee product quality and optionally a transparent packaging material made from renewable resources with a cost for raw material less than 3€/kg. To help stakeholders taking a rational decision based on the expressed needs, a new multi-criteria Decision Support System (DSS) for designing biodegradable packaging for fresh produce has been built. In this paper we present the functional specification, the software architecture and the implementation of the developed tool. This tool includes (i) a MAP simulation module combining mass transfer models and respiration of the food, (ii) a multi-criteria flexible querying module which handles imprecise, uncertain and missing data stored in the database. We detail its operational functioning through a real life case study to determine the most satisfactory materials for apricots packaging.",industry
10.1016/j.promfg.2015.07.372,Journal,Procedia Manufacturing,scopus,2015-01-01,sciencedirect,Case Study: Use of Online Tools in the Classroom and their Impact on Industrial Design Pedagogy,https://api.elsevier.com/content/abstract/scopus_id/85009959445,"Industrial Design education is going through a rapid evolution with more Design students making use of internet resources and tools such as crowdsourcing, 3D printing services, and other web-based tools to validate their ideas more quickly. The popularity of online 3D printing services such as Shapeways and Sculpteo accelerate the design process and learning. These services allow the designer to “print” virtually in any material such as plastics or metals. The impact of this new technology and other new web-based tools is significant not only in the industry but in the classroom as well. Current Industrial Design pedagogy is still partially based on technology, materials and processes that were developed a century ago. For example, pencils and paper are still the primary idea development tool. Books and magazines used to be the primary research tool but already have been surpassed by the Internet. Computer technology has improved significantly since the appearance of the first PCs, Macs and CNC machines. With all these advances in technology, one aspect of Industrial Design education that needs to be re-visited is the pedagogy of Design Drafting in this new age of online 3D printing services. The traditional Design pedagogy that was based on the development of different skills or competencies in separate courses or classes have not changed significantly in the last 40 years, 3D printing technology could potentially change this situation. Some new academic papers discuss this newer trend in Industrial Design schools but very few provide examples on how they implemented the new Internet-based 3D printing services in their curriculum. Industrial Design schools need to adapt quickly to the new reality, embracing Internet resources and online tools as core skills that every designer must have. This paper will discuss one case in particular where student projects were developed using online 3D printing services.",industry
10.1016/j.bica.2015.04.008,Journal,Biologically Inspired Cognitive Architectures,scopus,2015-01-01,sciencedirect,Automatic navigation of wall following mobile robot using Adaptive Resonance Theory of Type-1,https://api.elsevier.com/content/abstract/scopus_id/84960798237,"The automatic navigation of wall following robot is playing important role in various real world tasks such as underwater exploration, unmanned flight, and in automotive industries based on its computational complexity. In this work, a novel navigation approach based on biologically inspired neural network, known as “Adaptive Resonance Theory-1” which was proposed by Carpenter and Grossberg, has been implemented and investigated for navigation of wall following mobile robots. The proposed navigation algorithm is successfully tested with three sensor reading datasets obtained from clockwise navigation of SCITOS G5 mobile robot. Test decision accuracy (%), and simulation time were used as performance analysis parameters for the proposed algorithm and it has been found that the present work can achieve 99.59% of maximum decision accuracy.",industry
10.1016/j.procs.2015.07.575,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,A Decision Support System for Estimating Growth Parameters of Commercial Fish Stock in Fisheries Industries,https://api.elsevier.com/content/abstract/scopus_id/84948408653,"In this paper we present a Decision Support System (DSS) to estimate the values of the growth parameters from the yield effort data of a harvested population. The software is developed using Visual C# programming language in windows form application. Several softwares are used to check the validity of the multiple linear regression computational output in the DSS. The multiple linear regression computation is done using Ordinary Least Square (OLS) method and its computation comes from the discretization of two most known growth models, namely Logistic growth model and Gompertz growth model. We discretize the models in terms of yield effort variables and used the resulting equations as the bases for computing the intrinsic growth rate r and the carrying capacity K parameters which are the main ingredients in the MSY formula. Those models are also used as two selection tools placed in the main menu of the software. We use an existing published data as an example to estimate the growth parameters. We also compute the Maximum Sustainable Yield (MSY) for each model which represents the maximum amount of allowable biomass extracted from the fish population without harming the sustainability of the fisheries. Technically it suggests the value of the maximum sustainable yield as a decision to the harvester in managing the population and is often represented in a function of the growth parameters of the harvested population. Knowing the right growth parameters of the population is critical in determining the MSY, since the MSY is not stable. Hence, using an inaccurate values of growth parameters is likely detrimental in applying the MSY to the real fisheries. Our DSS has a contribution as a tool in reducing the error in calculating the growth parameters and the MSY. We found that the results are in agreement with the known literatures.",industry
10.1016/j.jmsy.2014.06.004,Journal,Journal of Manufacturing Systems,scopus,2015-01-01,sciencedirect,"A toolbox for the design, planning and operation of manufacturing networks in a mass customisation environment",https://api.elsevier.com/content/abstract/scopus_id/84942294486,"The task of design, planning and operation of manufacturing networks is becoming more and more challenging for companies, as globalisation, mass customisation and the turbulent economic landscape create demand volatility, uncertainties and high complexity. In this context, this paper investigates the performance of decentralised manufacturing networks through a set of methods developed into a software framework in a toolbox approach. The Tabu Search and Simulated Annealing metaheuristic methods are used together with an Artificial Intelligence method, called Intelligent Search Algorithm. A multi-criteria decision making procedure is carried out for the evaluation of the quality of alternative manufacturing network configurations using multiple conflicting criteria including dynamic complexity, reliability, cost, time, quality and environmental footprint. A comparison of the performance of each method based on the quality of the solutions that it provided is carried out. The statistical design of experiments robust engineering technique is used for the calibration of the adjustable parameters of the methods. Moreover, the impact of demand fluctuation to the operational performance of the alternative networks, expressed thorough a dynamic complexity indicator, is investigated through simulation. The developed framework is validated through a real life case, with data coming from the CNC machine building industry.",industry
10.1016/B978-0-444-63578-5.50097-9,Book Series,Computer Aided Chemical Engineering,scopus,2015-01-01,sciencedirect,Data Analysis and Modelling of a Fluid Catalytic Cracking Unit (FCCU) for an Implementation of Real Time Optimization,https://api.elsevier.com/content/abstract/scopus_id/84940474774,"In the Fluid Catalytic Cracking Units (FCCU) large hydrocarbon molecules are cracked into smaller molecules, generating high value products such as diesel, gasoline and useful petrochemical olefins. The control of these units is fundamental to maintain a satisfactory operation. Hence, the Real Time Optimization has proved an interesting strategy. A dynamic simulation of a FCCU was developed using a phenomenological industrial validated model. A Dynamic Neural Network (DNN) was trained with data from the FCCU model and gross and systematic errors were added to employ this system as a virtual plant. Data from this virtual plant were used to study strategies of online data processing, considering steady state identification (SSI) and gross error detection (GED), in order to eliminate measurement noise, as the initial steps into an RTO implementation.",industry
10.1016/j.future.2014.11.015,Journal,Future Generation Computer Systems,scopus,2015-01-01,sciencedirect,On-line failure prediction in safety-critical systems,https://api.elsevier.com/content/abstract/scopus_id/84917709364,"In safety-critical systems such as Air Traffic Control system, SCADA systems, Railways Control Systems, there has been a rapid transition from monolithic systems to highly modular ones, using off-the-shelf hardware and software applications possibly developed by different manufactures. This shift increased the probability that a fault occurring in an application propagates to others with the risk of a failure of the entire safety-critical system. This calls for new tools for the on-line detection of anomalous behaviors of the system, predicting thus a system failure before it happens, allowing the deployment of appropriate mitigation policies.
                  The paper proposes a novel architecture, namely CASPER, for online failure prediction that has the distinctive features to be (i) black-box: no knowledge of applications internals and logic of the system is required (ii) non-intrusive: no status information of the components is used such as CPU or memory usage; The architecture has been implemented to predict failures in a real Air Traffic Control System. CASPER exhibits high degree of accuracy in predicting failures with low false positive rate. The experimental validation shows how operators are provided with predictions issued a few hundred of seconds before the occurrence of the failure.",industry
10.1016/j.ijpe.2014.09.004,Journal,International Journal of Production Economics,scopus,2015-01-01,sciencedirect,An RFID-based intelligent decision support system architecture for production monitoring and scheduling in a distributed manufacturing environment,https://api.elsevier.com/content/abstract/scopus_id/84915733989,"Global manufacturing companies have some pressing needs to improve production visibility and decision-making performance by implementing effective production monitoring and scheduling. This paper proposes a radio frequency identification (RFID)-based intelligent decision support system architecture to handle production monitoring and scheduling in a distributed manufacturing environment. A pilot implementation of the architecture is reported in a distributed clothing manufacturing environment. RFID and cloud technologies were integrated for real-time and remote production capture and monitoring. Intelligent optimization techniques were also implemented to generate effective production scheduling solutions. A prototype system with remote monitoring and production scheduling functions was developed and implemented in a distributed manufacturing environment, which demonstrated the effectiveness of the architecture. The proposed architecture has good extensibility and scalability, which can easily be integrated with production decision-making as well as production and logistics operations in the supply chain. Lastly, this paper discusses the difficulties encountered and lessons learned during system implementation and the managerial implications of the proposed architecture.",industry
10.1016/j.asoc.2014.10.018,Journal,Applied Soft Computing Journal,scopus,2015-01-01,sciencedirect,Performance assessment of heat exchanger using intelligent decision making tools,https://api.elsevier.com/content/abstract/scopus_id/84912132496,"Process and manufacturing industries today are under pressure to deliver high quality outputs at lowest cost. The need for industry is therefore to implement cost savings measures immediately, in order to remain competitive. Organizations are making strenuous efforts to conserve energy and explore alternatives. This paper explores the development of an intelligent system to identify the degradation of heat exchanger system and to improve the energy performance through online monitoring system. The various stages adopted to achieve energy performance assessment are through experimentation, design of experiments and online monitoring system. Experiments are conducted as per full factorial design of experiments and the results are used to develop artificial neural network models. The predictive models are used to predict the overall heat transfer coefficient of clean/design heat exchanger. Fouled/real system value is computed with online measured data. Overall heat transfer coefficient of clean/design system is compared with the fouled/real system and reported. It is found that neural net work model trained with particle swarm optimization technique performs better comparable to other developed neural network models. The developed model is used to assess the performance of heat exchanger with the real/fouled system. The performance degradation is expressed using fouling factor, which is derived from the overall heat transfer coefficient of design system and real system. It supports the system to improve the performance by asset utilization, energy efficient and cost reduction in terms of production loss. This proposed online energy performance system is implemented into the real system and the adoptability is validated.",industry
10.1016/j.jss.2014.07.038,Journal,Journal of Systems and Software,scopus,2014-11-01,sciencedirect,A learning-based module extraction method for object-oriented systems,https://api.elsevier.com/content/abstract/scopus_id/84908163044,"Developers apply object-oriented (OO) design principles to produce modular, reusable software. Therefore, service-specific groups of related software classes called modules arise in OO systems. Extracting the modules is critical for better software comprehension, efficient architecture recovery, determination of service candidates to migrate legacy software to a service-oriented architecture, and transportation of such services to cloud-based distributed systems. In this study, we propose a novel approach to automatic module extraction to identify services in OO software systems. In our approach, first we create a weighted and directed graph of the software system in which vertices and edges represent the classes and their relations, respectively. Then, we apply a clustering algorithm over the graph to extract the modules. We calculate the weight of an edge by considering its probability of being within a module or between modules. To estimate these positional probabilities, we propose a machine-learning-based classification system that we train with data gathered from a real-world OO reference system. We have implemented an automatic module extraction tool and evaluated the proposed approach on several open-source and industrial projects. The experimental results show that the proposed approach generates highly accurate decompositions that are close to authoritative module structures and outperforms existing methods.",industry
10.1016/j.mejo.2013.12.006,Journal,Microelectronics Journal,scopus,2014-03-01,sciencedirect,Enhancing confidence in indirect analog/RF testing against the lack of correlation between regular parameters and indirect measurements,https://api.elsevier.com/content/abstract/scopus_id/84897670549,"The greedy specification testing remains mandatory for analog and radio frequency (RF) integrated circuits because of the accuracy of the sorting based on these measurements. Unfortunately, to be implemented, this kind of testing method often incurs very high costs (expensive instruments, long test time…). A common approach, in the literature, is the so-called indirect/alternate test strategy. This strategy consists in deriving targeted specifications from low-cost Indirect Measurements (IMs). During the industrial test phase, the estimation of regular specifications using IMs is based on a correlation model that has been built previously, during a training phase. Despite the substantial test cost reduction offered by this strategy, its deployment in industry is limited, mainly because of a lack of confidence in the accuracy of estimations made by the correlation model. A solution to increase the confidence in the estimation of specifications using the indirect approach is to implement redundancy in the prediction phase. In this paper, we demonstrate that the redundancy implementation brings more than identifying rare misjudged circuits from a high-correlated model. Indeed redundancy massively increases the accuracy despite of the lack of accurate models that have been assumed in previous implementations of redundant indirect testing. This approach is illustrated on a real case study for which we have experimental measurements on a set of 10,000 devices.",industry
10.1016/j.patcog.2013.09.007,Journal,Pattern Recognition,scopus,2014-03-01,sciencedirect,The cluster assessment of facial attractiveness using fuzzy neural network classifier based on 3D Moiré features,https://api.elsevier.com/content/abstract/scopus_id/84888386508,"Facial attractiveness has long been argued upon varied emphases by philosophers, artists, psychologists and biologists. A number of studies empirically investigated how facial attractiveness was influenced by 2D facial characteristics, such as symmetry, averageness and golden ratio. However, few implementations of facial beauty assessment were based on 3D facial features. The purpose of this paper is to propose a novel cluster assessment system for facial attractiveness that is characterized by the incorporation of 3D geometric Moiré features with an adjusted fuzzy neural network (FNN). We first extract 3D facial features from images acquired by a 3dMD scanner. Seven Moiré features are employed to represent a 3D facial image. The FNN classifier, taking the Moiré features as the parameters, is then trained and validated against independently conducted attractiveness ratings. A number of diverse referees were invited and offered their attractiveness ratings over a five-item Likert scale for 100 female facial images. The proposed assessment presents a high accuracy rate of 90%, and the area under curve (AUC) computed from the receiver operating characteristic (ROC) curve is 0.95. The results show that the perceptions of facial attractiveness are essentially consensus among raters, and can be mathematically modeled through supervised learning techniques. The high accuracy achieved proves that the proposed FNN classifier can serve as a general, automated and human-like judgment tool for objective classification of female facial attractiveness, and thus has potential applications to the entertainment industry, cosmetic industry, virtual media, and plastic surgery.",industry
10.1016/j.enbuild.2014.08.004,Journal,Energy and Buildings,scopus,2014-01-01,sciencedirect,Neural network model ensembles for building-level electricity load forecasts,https://api.elsevier.com/content/abstract/scopus_id/84907570987,"The future power grid is expected to provide unprecedented flexibility in how energy is generated, distributed, and managed, which increasingly necessitates an ability to perform accurate short-term small-scale electricity load and generation forecasting, e.g., at the level of individual buildings or sites. In this paper, we present a novel building-level neural network-based ensemble model for day-ahead electricity load forecasting and show that it outperforms the previously established best performing model, SARIMA, by up to 50%, in the context of load data from half a dozen operational commercial and industrial sites. In addition, we show a straightforward, automated way to select model parameters, making our model practical for use in real deployments.",industry
10.1016/j.anifeedsci.2014.04.017,Journal,Animal Feed Science and Technology,scopus,2014-01-01,sciencedirect,Keeping under control a liquid feed fermentation process for pigs: A reality scale pilot based study,https://api.elsevier.com/content/abstract/scopus_id/84903816625,"An original and fully automated liquid feeding pilot has been designed and implemented to monitor and optimize the fermentation process of liquid feed for pigs at a pre-industrial scale. The installation was designed and instrumented to continuously record the temperature, pH and redox potential (E
                     
                        h
                     ) during the fermentation course of wheat flour based feed mixed with water in a 1:2.5 (w:w) ratio. Single and multiple batches experiments were carried-out with feed inoculation achieved by leftover or with a selected culture of lactic acid producing bacteria (LAB). Physicochemical and microbiological characteristics of the fermentation process which include lactic and acetic acids and ethanol concentrations, enumerations of lactic acid producing bacteria, yeasts, total coliforms and Escherichia coli, were monitored and analyzed as a function of the main feed control factors: incubation time, operating temperature, feed time schedule and percentage of leftover. From batch experiments, it was observed that increasing the operating temperature from 15 to 30°C, accelerates the rate of fermentation by reducing about 5–6-folds the process latency and the duration to reach a pH value of 4.0 which is considered as optimal to achieve biosafety. Nevertheless, this does not prevent the blooming of coliforms as their counts increases from 4 to 6 log10
                     CFU/mL within 24h. In opposite, multiple batches are proved to be effective in both accelerating the fermentation rates and reducing the survival of Coliform bacteria in fermented liquid feed (FLF). Feed fermented at 25°C during 24-h cycles with a 22% leftover ensures the prominence of LAB strains over yeasts with a population level that stabilizes at around 9 log10
                     CFU/mL (vs. 7 log10
                     CFU/mL for single batches), a lactic acid production up to 35g/kg dry matter (DM) and a pH value between 5 and 3.5 throughout the period. Concomitantly, total Coliforms number decreases from 7.5 to 2.2 log10
                     CFU/mL within 72h whereas E. coli became undetectable beyond 48h. Addition of a starter culture (Pediococcus acidilactici, Bactocell®) at 9 log10
                     CFU/kg DM at the initial stage of FLF production reduces 25–35 times the total coliforms and E. coli counts. No significant differences in the amounts of organic compounds produced by the microflora as compared to the control FLF after 80h nor in the microbial levels are observed. It is concluded that sequences of fermentation cycles allows, in a given temperature range, establishing a positive, robust, microbial ecosystem.",industry
10.1016/j.powtec.2014.05.051,Journal,Powder Technology,scopus,2014-01-01,sciencedirect,"Soft sensing of particle size in a grinding process: Application of support vector regression, fuzzy inference and adaptive neuro fuzzy inference techniques for online monitoring of cement fineness",https://api.elsevier.com/content/abstract/scopus_id/84902965121,"Use of soft sensors for online particle size monitoring in a grinding process is a viable alternative since physical sensors for the same are not available for many such processes. Cement fineness is an important quality parameter in the cement grinding process. However, very few studies have been done for soft sensing of cement fineness in the grinding process. Moreover, most of the grinding process modeling approaches have been reported for ball mills and rarely any modeling of vertical roller mill is available. In this research, modeling of vertical roller mill used for clinker grinding has been done using support vector regression (SVR), fuzzy inference and adaptive neuro fuzzy inference(ANFIS) techniques since these techniques have not yet been largely explored for particle size soft sensing. The modeling has been done by collection of the real industrial data from a cement grinding process followed by data cleaning and a structured method of dividing the data into training and validation data sets using the Kennard–Stone subset selection algorithm. Optimum SVR hyper parameters were determined using a combined approach of analytical method and grid search plus cross validation. The models were developed using MATLAB from the training data and were tested with the validation data. Results reveal that the proposed ANFIS model of the clinker grinding process shows much superior performance compared with the other types of model. The ANFIS model was implemented in the SIMULINK environment for real-time monitoring of cement fineness from the knowledge of input variables and the model computation time was determined. It is observed that the model holds good promise to be implemented online for real-time estimation of cement fineness which will certainly help the plant operators in maintaining proper cement quality and in reducing losses.",industry
10.1016/j.eswa.2013.08.003,Journal,Expert Systems with Applications,scopus,2014-01-01,sciencedirect,Intelligent business processes composition based on multi-agent systems,https://api.elsevier.com/content/abstract/scopus_id/84888360250,"This paper proposes a novel model for automatic construction of business processes called IPCASCI (Intelligent business Processes Composition based on multi-Agent systems, Semantics and Cloud Integration). The software development industry requires agile construction of new products able to adapt to the emerging needs of a changing market. In this context, we present a method of software component reuse as a model (or methodology), which facilitates the semi-automatic reuse of web services on a cloud computing environment, leading to business process composition. The proposal is based on web service technology, including: (i) Automatic discovery of web services; (ii) Semantics description of web services; (iii) Automatic composition of existing web services to generate new ones; (iv) Automatic invocation of web services. As a result of this proposal, we have presented its implementation (as a tool) on a real case study. The evaluation of the case study and its results are proof of the reliability of IPCASCI.",industry
10.1016/j.asoc.2013.05.017,Journal,Applied Soft Computing Journal,scopus,2014-01-01,sciencedirect,A hybrid noise suppression filter for accuracy enhancement of commercial speech recognizers in varying noisy conditions,https://api.elsevier.com/content/abstract/scopus_id/84888294149,"Commercial speech recognizers have made possible many speech control applications such as wheelchair, tone-phone, multifunctional robotic arms and remote controls, for the disabled and paraplegic. However, they have a limitation in common in that recognition errors are likely to be produced when background noise surrounds the spoken command, thereby creating potential dangers for the disabled if recognition errors exist in the control systems. In this paper, a hybrid noise suppression filter is proposed to interface with the commercial speech recognizers in order to enhance the recognition accuracy under variant noisy conditions. It intends to decrease the recognition errors when the commercial speech recognizers are working under a noisy environment. It is based on a sigmoid function which can effectively enhance noisy speech using simple computational operations, while a robust estimator based on an adaptive-network-based fuzzy inference system is used to determine the appropriate operational parameters for the sigmoid function in order to produce effective speech enhancement under variant noisy conditions. The proposed hybrid noise suppression filter has the following advantages for commercial speech recognizers: (i) it is not possible to tune the inbuilt parameters on the commercial speech recognizers in order to obtain better accuracy; (ii) existing noise suppression filters are too complicated to be implemented for real-time speech recognition; and (iii) existing sigmoid function based filters can operate only in a single-noisy condition, but not under varying noisy conditions. The performance of the hybrid noise suppression filter was evaluated by interfacing it with a commercial speech recognizer, commonly used in electronic products. Experimental results show that improvement in terms of recognition accuracy and computational time can be achieved by the hybrid noise suppression filter when the commercial recognizer is working under various noisy environments in factories.",industry
10.1016/j.advengsoft.2013.09.003,Journal,Advances in Engineering Software,scopus,2014-01-01,sciencedirect,Software architecture knowledge for intelligent light maintenance,https://api.elsevier.com/content/abstract/scopus_id/84885359031,"The maintenance management plays an important role in the monitoring of business activities. It ensures a certain level of services in industrial systems by improving the ability to function in accordance with prescribed procedures. This has a decisive impact on the performance of these systems in terms of operational efficiency, reliability and associated intervention costs. To support the maintenance processes of a wide range of industrial services, a knowledge-based component is useful to perform the intelligent monitoring. In this context we propose a generic model for supporting and generating industrial lights maintenance processes. The modeled intelligent approach involves information structuring and knowledge sharing in the industrial setting and the implementation of specialized maintenance management software in the target information system. As a first step we defined computerized procedures from the conceptual structure of industrial data to ensure their interoperability and effective use of information and communication technologies in the software dedicated to the management of maintenance (E-candela). The second step is the implementation of this software architecture with specification of business rules, especially by organizing taxonomical information of the lighting systems, and applying intelligence-based operations and analysis to capitalize knowledge from maintenance experiences. Finally, the third step is the deployment of the software with contextual adaptation of the user interface to allow the management of operations, editions of the balance sheets and real-time location obtained through geolocation data. In practice, these computational intelligence-based modes of reasoning involve an engineering framework that facilitates the continuous improvement of a comprehensive maintenance regime.",industry
10.1016/j.ultras.2013.07.018,Journal,Ultrasonics,scopus,2014-01-01,sciencedirect,Ultrasonic sensor based defect detection and characterisation of ceramics,https://api.elsevier.com/content/abstract/scopus_id/84884211045,"Ceramic tiles, used in body armour systems, are currently inspected visually offline using an X-ray technique that is both time consuming and very expensive. The aim of this research is to develop a methodology to detect, locate and classify various manufacturing defects in Reaction Sintered Silicon Carbide (RSSC) ceramic tiles, using an ultrasonic sensing technique. Defects such as free silicon, un-sintered silicon carbide material and conventional porosity are often difficult to detect using conventional X-radiography. An alternative inspection system was developed to detect defects in ceramic components using an Artificial Neural Network (ANN) based signal processing technique. The inspection methodology proposed focuses on pre-processing of signals, de-noising, wavelet decomposition, feature extraction and post-processing of the signals for classification purposes. This research contributes to developing an on-line inspection system that would be far more cost effective than present methods and, moreover, assist manufacturers in checking the location of high density areas, defects and enable real time quality control, including the implementation of accept/reject criteria.",industry
10.1016/j.sna.2013.09.021,Journal,"Sensors and Actuators, A: Physical",scopus,2013-11-14,sciencedirect,Feasibility study of Hierarchical Temporal Memories applied to welding diagnostics,https://api.elsevier.com/content/abstract/scopus_id/84887294247,"Defect classification in on-line welding quality monitoring systems is an active area of research with a significant relevance to several industrial sectors where welding processes are extensively employed. Approaches based on some artificial intelligence implementations, like Artificial Neural Networks or Fuzzy Logic have been attempted, but their impact in real industrial scenarios is nowadays rather modest. In this paper a new approach based on Hierarchical Temporal Memories and the acquired plasma spectra is explored and analyzed by means of several arc-welding experimental tests. Results show the ability of the proposed solution to perform a suitable classification among several weld perturbations. The search for an optimal configuration of the algorithm and the usefulness of both spatial (spectral) and temporal identification of patterns will be also discussed, and the results will be compared with those provided by a solution based on feature selection and neural networks, exhibiting the better performance of the HTM model in terms of performance and handling of the input data.",industry
10.1016/j.fluid.2013.08.018,Journal,Fluid Phase Equilibria,scopus,2013-11-05,sciencedirect,Utilization of support vector machine to calculate gas compressibility factor,https://api.elsevier.com/content/abstract/scopus_id/84884180985,"The compressibility factor (Z-factor) is considered as a very important parameter in the petroleum industry because of its broad applications in PVT characteristics. In this study, a meta-learning algorithm called Least Square Support Vector Machine (LSSVM) was developed to predict the compressibility factor. In addition, the proposed technique was examined with previous models, exhibiting an R
                     2 and an MSE of 0.999 and 0.000014, respectively. A significant drawback in the conventional LSSVM is the determination of optimal parameters to attain desired output with a reasonable accuracy. To eliminate this problem, the current study introduced coupled simulated annealing (CSA) algorithm to develop a new model, known as CSA-LSSVM. The proposed algorithm included 4756 datasets to validate the effectiveness of the CSA-LSSVM model using statistical criteria. The new technique can be utilized in chemical and petroleum engineering software packages where the most accurate value of Z-factor is required to predict the behavior of real gas, significantly affecting design aspects of equipment involved in gas processing plants.",industry
10.1016/j.jprocont.2013.09.014,Journal,Journal of Process Control,scopus,2013-10-28,sciencedirect,A multilayer-perceptron based method for variable selection in soft sensor design,https://api.elsevier.com/content/abstract/scopus_id/84886080853,"The paper proposes a new method for variable selection for prediction settings and soft sensors applications. The new variable selection method is based on the multi-layer perceptron (MLP) neural network model, where the network is trained a single time, maintaining low computational cost. The proposed method was successfully applied, and compared with four state-of-the-art methods in one artificial dataset and three real-world datasets, two publicly available datasets (Box–Jenkins gas furnace and gas mileage), and a dataset of a problem where the objective is to estimate the fluoride concentration in the effluent of a real urban water treatment plant (WTP). The proposed method presents similar or better approximation performance when compared to the other four methods. In the experiments, among all the five methods, the proposed method selects the lowest number of variables and variables-delays pairs to achieve the best solution. In soft sensors applications having a lower number of variables is a positive factor for decreasing implementation costs, or even making the soft sensor feasible at all.",industry
10.1016/j.neucom.2013.02.039,Journal,Neurocomputing,scopus,2013-10-22,sciencedirect,Point and prediction interval estimation for electricity markets with machine learning techniques and wavelet transforms,https://api.elsevier.com/content/abstract/scopus_id/84881221196,"A growing number of countries all over the world are switching over to deregulated or the market structure of electricity sector with a view to enhance productivity, efficiency and to lower the prices. Barring a few cases, the deregulated structure is doing quite well in most of the countries. However a persistent issue that plagues the involved parties such as producers, traders, retailers etc., is the uncertainty that prevails in the system. Due to a number of known, unknown factors, the electricity prices exhibit fluctuating characteristics which is difficult to control as well as predict. Several forecasting techniques have been developed and successfully implemented for existing markets around the world with comparable performance. However, the uncertainty aspect of the point forecasts has not been analyzed significantly. In this work, an attempt is made to quantify such uncertainties existing in the market using statistical techniques like prediction intervals. Hybrid models using neural networks and Extreme Learning machines with wavelets as preprocessors are developed and applied for point as well as prediction interval forecasting for Ontario Electricity Market, PJM Day-Ahead and Real time markets.",industry
10.1016/j.engappai.2013.04.006,Journal,Engineering Applications of Artificial Intelligence,scopus,2013-09-01,sciencedirect,Post-design analysis for building and refining AI planning systems,https://api.elsevier.com/content/abstract/scopus_id/84880771590,"The growth of industrial applications of artificial intelligence has raised the need for design tools to aid in the conception and implementation of such complex systems. The design of automated planning systems faces several engineering challenges including the proper modeling of the domain knowledge: the creation of a model that represents the problem to be solved, the world that surrounds the system, and the ways the system can interact with and change the world in order to solve the problem. Knowledge modeling in AI planning is a hard task that involves acquiring the system requirements and making design decisions that can determine the behavior and performance of the resulting system. In this paper we investigate how knowledge acquired during a post-design phase of modeling can be used to improve the prospective model. A post-design framework is introduced which combines a knowledge engineering tool and a virtual prototyping environment for the analysis and simulation of plans. This framework demonstrates that post-design analysis supports the discovery of missing requirements and can guide the model refinement cycle. We present three case studies using benchmark domains and eight state-of-the-art planners. Our results demonstrate that significant improvements in plan quality and an increase in planning speed of up to three orders of magnitude can be achieved through a careful post-design process. We argue that such a process is critical for the deployment of AI planning technology in real-world engineering applications.",industry
10.1016/j.jsr.2013.04.005,Journal,Journal of Safety Research,scopus,2013-06-24,sciencedirect,Transferability and robustness of real-time freeway crash risk assessment,https://api.elsevier.com/content/abstract/scopus_id/84879088842,"Introduction
                  This study examines the data from single loop detectors on northbound (NB) US-101 in San Jose, California to estimate real-time crash risk assessment models.
               
                  Method
                  The classification tree and neural network based crash risk assessment models developed with data from NB US-101 are applied to data from the same freeway, as well as to the data from nearby segments of the SB US-101, NB I-880, and SB I-880 corridors. The performance of crash risk assessment models on these nearby segments is the focus of this research.
               
                  Results
                  The model applications show that it is in fact possible to use the same model for multiple freeways, as the underlying relationships between traffic data and crash risk remain similar.
               
                  Impact on Industry
                  The framework provided here may be helpful to authorities for freeway segments with newly installed traffic surveillance apparatuses, since the real-time crash risk assessment models from nearby freeways with existing infrastructure would be able to provide a reasonable estimate of crash risk. The robustness of the model output is also assessed by location, time of day, and day of week. The analysis shows that on some locations the models may require further learning due to higher than expected false positive (e.g., the I-680/I-280 interchange on US-101 NB) or false negative rates. The approach for post-processing the results from the model provides ideas to refine the model prior to or during the implementation.",industry
10.1016/j.neucom.2012.04.033,Journal,Neurocomputing,scopus,2013-06-03,sciencedirect,Applying soft computing techniques to optimise a dental milling process,https://api.elsevier.com/content/abstract/scopus_id/84875966713,"This study presents a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. This novel intelligent procedure is based on the following phases. Firstly, a neural model extracts the internal structure and the relevant features of the data set representing the system. Secondly, the dynamic system performance of different variables is specifically modelled using a supervised neural model and identification techniques. This constitutes the model for the fitness function of the production process, using relevant features of the data set. Finally, a genetic algorithm is used to optimise the machine parameters from a non parametric fitness function. The proposed novel approach was tested under real dental milling processes using a high-precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study.",industry
10.1016/j.engappai.2012.11.009,Journal,Engineering Applications of Artificial Intelligence,scopus,2013-05-01,sciencedirect,An intelligent system for wafer bin map defect diagnosis: An empirical study for semiconductor manufacturing,https://api.elsevier.com/content/abstract/scopus_id/84876945059,"Wafer bin maps (WBMs) that show specific spatial patterns can provide clue to identify process failures in the semiconductor manufacturing. In practice, most companies rely on experienced engineers to visually find the specific WBM patterns. However, as wafer size is enlarged and integrated circuit (IC) feature size is continuously shrinking, WBM patterns become complicated due to the differences of die size, wafer rotation, the density of failed dies and thus human judgments become inconsistent and unreliable. To fill the gaps, this study aims to develop a knowledge-based intelligent system for WBMs defect diagnosis for yield enhancement in wafer fabrication. The proposed system consisted of three parts: graphical user interface, the WBM clustering solution, and the knowledge database. In particular, the developed WBM clustering approach integrates spatial statistics test, cellular neural network (CNN), adaptive resonance theory (ART) neural network, and moment invariant (MI) to cluster different patterns effectively. In addition, an interactive converse interface is developed to present the possible root causes in the order of similarity matching and record the diagnosis know-how from the domain experts into the knowledge database. To validate the proposed WBM clustering solution, twelve different WBM patterns collected in real settings are used to demonstrate the performance of the proposed method in terms of purity, diversity, specificity, and efficiency. The results have shown the validity and practical viability of the proposed system. Indeed, the developed solution has been implemented in a leading semiconductor manufacturing company in Taiwan. The proposed WBM intelligent system can recognize specific failure patterns efficiently and also record the assignable root causes verified by the domain experts to enhance troubleshooting effectively.",industry
10.1016/j.robot.2012.12.005,Journal,Robotics and Autonomous Systems,scopus,2013-05-01,sciencedirect,A survey of bio-inspired robotics hands implementation: New directions in dexterous manipulation,https://api.elsevier.com/content/abstract/scopus_id/84875695547,"Recently, significant advances have been made in ROBOTICS, ARTIFICIAL INTELLIGENCE and other COGNITIVE related fields, allowing to make much sophisticated biomimetic robotics systems. In addition, enormous number of robots have been designed and assembled, explicitly realize biological oriented behaviors. Towards much skill behaviors and adequate grasping abilities (i.e. ARTICULATION and DEXTEROUS MANIPULATION), a new phase of dexterous hands have been developed recently with biomimetically oriented and bio-inspired functionalities. In this respect, this manuscript brings a detailed survey of biomimetic based dexterous robotics multi-fingered hands. The aim of this survey, is to find out the state of the art on dexterous robotics end-effectors, known in literature as (ROBOTIC HANDS) or (DEXTEROUS MULTI-FINGERED) robot hands. Hence, this review finds such biomimetic approaches using a framework that permits for a common description of biological and technical based hand manipulation behavior. In particular, the manuscript focuses on a number of developments that have been taking place over the past two decades, and some recent developments related to this biomimetic field of research. In conclusions, the study found that, there are rich research efforts in terms of KINEMATICS, DYNAMICS, MODELING and CONTROL methodologies. The survey is also indicating that, the topic of biomimetic inspired robotics systems make significant contributions to robotics hand design, in four main directions for future research. First, they provide a genuine world test of models of biologically inspired hand designs and dexterous manipulation behaviors. Second, they provide novel manipulation articulations and mechanisms available for industrial and domestic uses, most notably in the field of human like hand design and real world applications. Third, this survey has also indicated that, there are quite large number of attempts to acquire biologically inspired hands. These attempts were almost successful, where they exposed more novel ideas for further developments. Such inspirations were directed towards a number of topics related (HAND MECHANICS AND DESIGN), (HAND TACTILE SENSING), (HAND FORCE SENSING), (HAND SOFT ACTUATION) and (HAND CONFIGURATION AND TOPOLOGY). FOURTH, in terms of employing AI related sciences and cognitive thinking, it was also found that, rare and exceptional research attempts were directed towards the employment of biologically inspired thinking, i.e. (AI, BRAIN AND COGNITIVE SCIENCES) for hand upper control and towards much sophisticated dexterous movements. Throughout the study, it has been found there are number of efforts in terms of mechanics and hand designs, tactical sensing, however, for hand soft actuation, it seems this area of research is still far away from having a realistic muscular type fingers and hand movements.",industry
10.1016/j.compind.2012.11.005,Journal,Computers in Industry,scopus,2013-04-01,sciencedirect,IMAQCS: Design and implementation of an intelligent multi-agent system for monitoring and controlling quality of cement production processes,https://api.elsevier.com/content/abstract/scopus_id/84875245342,"In cement plant, since all processes are chemical and irreversible, monitoring and control is a critical factor. If the process is not controlled at any stage, the final product can be damaged or lost. Thus, in such environments, considering the quality of the product at each state is essential. Also, to control the process, communication among different parts of production line is essential. The wasted time in production line has a direct effect on process correction time and cement production performance. Here, a model of a new intelligent multi-agent quality control system (IMAQCS) for controlling the quality of cement production processes is suggested. This model, using of rule-based artificial intelligence technique, concentrates on relationship between departments in cement production line to monitor multi-attribute quality factors. With the presence of agents for controlling the quality of cement processes, real-time analyzing and decision making in a fault condition will be provided. In order to validate the proposed model, IMAQCS is deployed in real plants of a cement industries complex in Iran. The ability of the system in the process production environment is assessed. The effectiveness and efficiency of the system are demonstrated by reducing the process correction time and increasing the cement production performance. Finally, this system can effectively impact on factory resources and cost saving.",industry
10.1016/j.epsr.2013.01.011,Journal,Electric Power Systems Research,scopus,2013-02-21,sciencedirect,FPGA-based neural network harmonic estimation for continuous monitoring of the power line in industrial applications,https://api.elsevier.com/content/abstract/scopus_id/84873945333,"Manufacturing cells are present in almost all the industrial sector. Unfortunately, all machine tools into the manufacturing cell are connected to the same power line, implying that their operation adds nonlinear loads such as harmonics and interharmonics that affect the general machine-tool condition. A novel NN-based methodology for harmonic monitoring through time in transient and stationary signals that satisfies the IEC61000-4-7 standard is presented, as well as its implementation into a field programmable gate array (FPGA) for continuous and online monitoring. The proposed method and the developed instrument have been tested in a real manufacturing cell.",industry
10.1533/9780857093967.1.208,Book,Joining Textiles: Principles and Applications,scopus,2013-01-01,sciencedirect,Intelligent sewing systems for garment automation and robotics,https://api.elsevier.com/content/abstract/scopus_id/84903011824,"Sewing machine interactions at different speeds have been used to construct qualitative rules mapping fabric properties to optimum sewing machine settings for intelligent sewing machines. the inference procedures of fuzzy logic have been implemented in a neural network to allow for optimisation of output membership functions and, subsequently, self-learning. the technique is successfully applied to develop intelligent sewing machines and further implemented in textile and garment manufacturing. An intelligent manufacturing environment has been put forward in which fabric properties predict the sewability of any fabric, determine the minimum change of fabric properties required, and control in real time the stitching of a garment by using the feedback closed loop of the Neuro-Fuzzy model. the system has been successfully tried in an industrial setting. optimum settings were achieved under static and dynamic machine conditions, including for the properties of difficult fabrics and compensation for mishandling by the operator over the speed range of the sewing machine.",industry
10.1016/j.procir.2013.09.042,Conference Proceeding,Procedia CIRP,scopus,2013-01-01,sciencedirect,An enabling digital foundation towards smart machining,https://api.elsevier.com/content/abstract/scopus_id/84886789550,"Today's major challenges for manufacturing companies in the aerospace and automotive industries are clear: global cooperation with multiple supply chain partners, production optimization, management and tracking of information so as to meet new requirements in terms of traceability, security and sustainability. The need for a data exchange standard that allows disparate entities and their associated devices in a manufacturing system to share data seamlessly is clearly obvious. And the first expected impact is the ‘next generation’ smart controller that could really enable an intelligent machining process based on real-time monitoring and diagnosis, self-learning decision and adaptive optimization. The four-year project titled FoFdation envisions a ‘Digital and Smart Factory’ architecture and implementation. This has the potential to achieve significant benefits in earlier visibility of manufacturing issues, faster production ramp-up time, faster time to volume production and subsequently shorter time to market, reduced manufacturing costs and improved product quality, as well as sustainability objectives like low energy consumption and waste reduction. The present paper describes the on-going work with specific focus on the definition and implementation of the FoFdation Smart Machine Controller (SMC) in an adaptable architecture that satisfies both commercial and open source CNC controllers. It highlights the project's end use validation framework as well as sets a strong Manufacturing Information System foundation on which process optimization and control as well as sustainable practices can be based. It presents the general vision of the target solution for the SMC developed in the FoFdation project. It is based on efforts past and present both by academia and industry in various capacities and proposes tentative implementations based on the STEP-NC standard to define the machine controller of the future.",industry
10.3182/20130825-4-US-2038.00105,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2013-01-01,sciencedirect,An on-line training simulator built on dynamic simulations of crushing plants,https://api.elsevier.com/content/abstract/scopus_id/84885800887,"Crushing plants are widely used around the world as a pre-processing step in the mineral and mining industries or as standalone processing plants for final products in the aggregates industry. Despite automation and different types of advanced model predictive control, many the processes are still managed by operators. The skill of the operators influences the process performance and thus production yield. Therefore, it is important to train the operators so they know how to behave in different situations and to make them able to operate the process in the best possible way.
                  Different types of models for crushers and other production units have been developed during the years and the latest improvement is the addition of dynamic behavior which gives the crushing plants a time dependent behavior and performance. This can be used as a simulator for operators training. By connecting an Internet based Human Machine Interface (WebHMI) to a dynamic simulator with the models incorporated, an on-line training environment for operators can be achieved.
                  In this paper, a dynamic crushing plant simulator implemented in MATLAB/SIMULINK has been connected to a WebHMI. The WebHMI is accessible via the Internet, thus creating a realistic control room for operators’ training. In the created training environment, the operators can be trained under realistic conditions. Simple training scenarios and how they could be simulated are discussed. Apart from the increased level of knowledge and experience among the operators, the time aspect is an important factor. While a real crushing plant is still being built, the operators to be can already be trained, saving a lot of the commissioning and ramp up time.",industry
10.3182/20130828-3-UK-2039.00025,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2013-01-01,sciencedirect,A new extensive source for web-based control education - Contlab.eu,https://api.elsevier.com/content/abstract/scopus_id/84885206555,"Modern technologies allow to create a networked control system with off-the-shelf mobile devices. As such, there is the possibility of having the role of who offers and who uses a “remote” laboratory played by the same people. Extending recently published ideas, the paper presents a first nucleus of functionalities allowing one to create process simulators and controllers which run on a mobile application, and then share them with others. Some words are also spent on some of the possibilities opened by the proposal, sketching out some interesting didactic activities to propose to the students.",industry
10.1016/j.asoc.2012.05.031,Journal,Applied Soft Computing Journal,scopus,2013-01-01,sciencedirect,Optimal design of laser solid freeform fabrication system and real-time prediction of melt pool geometry using intelligent evolutionary algorithms,https://api.elsevier.com/content/abstract/scopus_id/84881665462,"With the rapid growth of laser applications and the introduction of high efficiency lasers (e.g. fiber lasers), laser material processing has gained increasing importance in a variety of industries. Among the applications of laser technology, laser cladding has received significant attention due to its high potential for material processing such as metallic coating, high value component repair, prototyping, and even low-volume manufacturing. In this paper, two optimization methods have been applied to obtain optimal operating parameters of Laser Solid Freeform Fabrication Process (LSFF) as a real world engineering problem. First, Particle Swarm Optimization (PSO) algorithm was implemented for real-time prediction of melt pool geometry. Then, a hybrid evolutionary algorithm called Self-organizing Pareto based Evolutionary Algorithm (SOPEA) was proposed to find the optimal process parameters. For further assurance on the performance of the proposed optimization technique, it was compared to some well-known vector optimization algorithms such as Non-dominated Sorting Genetic Algorithm (NSGA-II) and Strength Pareto Evolutionary Algorithm (SPEA 2). Thereafter, it was applied for simultaneous optimization of clad height and melt pool depth in LSFF process. Since there is no exact mathematical model for the clad height (deposited layer thickness) and the melt pool depth, the authors developed two Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to estimate these two process parameters. Optimization procedure being done, the archived non-dominated solutions were surveyed to find the appropriate ranges of process parameters with acceptable dilutions. Finally, the selected optimal ranges were used to find a case with the minimum rapid prototyping time. The results indicate the acceptable potential of evolutionary strategies for controlling and optimization of LSFF process as a complicated engineering problem.",industry
10.1016/j.riai.2013.05.002,Journal,RIAI - Revista Iberoamericana de Automatica e Informatica Industrial,scopus,2013-01-01,sciencedirect,Identification and wavenet control of AC motor,https://api.elsevier.com/content/abstract/scopus_id/84880211449,"En el presente artículo se muestra un esquema de identificación y control que sintoniza en línea las ganancias proporcional, integral y derivativa de un controlador PID discreto aplicado a un sistema dinámico SISO. Esto se logra empleando una red neuronal de base radial con funciones de activación wavelet hijas Morlet (wavenet) adicionalmente en cascada un filtro de respuesta infinita al impulso (IIR). Dicho esquema es aplicado en tiempo real para controlar la velocidad de un motor de inducción de CA trifásico del tipo jaula de ardilla (MIJA) alimentado con un variador de frecuencia trifásico, de esta forma se muestra cómo este esquema de identificación y control en línea, puede ser implementado en este tipo de plantas que son ampliamente utilizadas en la industria, sin la necesidad de obtener los parámetros del modelo matemático del conjunto variador de frecuencia-motor de inducción trifásico. Se presentan los resultados obtenidos en simulación numérica y experimentales, empleando para esto la plataforma de LabVIEW.
               
                  This paper presents a control scheme to tune online the proportional, integral and derivative gains of a discrete PID controller, through the identification and control of a SISO stable and minimum phase dynamic system. This is accomplished using a radial basis network neural with daughter Morlet wavelets activation functions in cascaded with an infinite impulse response (IIR) filter. This scheme is applied in real time to control the speed of an AC three-phase induction motor supplied with a three-phase inverter. So in this way we show how the identification and control scheme can be implemented in this type of plants that are widely used in industry, without the need of mathematical model parameters of the induction motor. We present numerical simulation and experimental results.",industry
10.1016/j.compchemeng.2012.06.021,Journal,Computers and Chemical Engineering,scopus,2012-12-20,sciencedirect,SmartGantt - An interactive system for generating and updating rescheduling knowledge using relational abstractions,https://api.elsevier.com/content/abstract/scopus_id/84869501412,"Generating and updating rescheduling knowledge that can be used in real time has become a key issue in reactive scheduling due to the dynamic and uncertain nature of industrial environments and the emergent trend towards cognitive systems in production planning and execution control. Disruptive events have a significant impact on the feasibility of plans and schedules. In this work, the automatic generation and update through learning of rescheduling knowledge using simulated transitions of abstract schedule states is proposed. An industrial example where a current schedule must be repaired in response to unplanned events such as the arrival of a rush order, raw material delay, or an equipment failure which gives rise to the need for rescheduling is discussed. A software prototype (SmartGantt) for interactive schedule repair in real-time is presented. Results demonstrate that responsiveness is dramatically improved by using relational reinforcement learning and relational abstractions to develop a repair policy.",industry
10.1016/j.dss.2012.08.006,Journal,Decision Support Systems,scopus,2012-12-01,sciencedirect,Sales forecasting for computer wholesalers: A comparison of multivariate adaptive regression splines and artificial neural networks,https://api.elsevier.com/content/abstract/scopus_id/84868667879,"Artificial neural networks (ANNs) have been found to be useful for sales/demand forecasting. However, one of the main shortcomings of ANNs is their inability to identify important forecasting variables. This study uses multivariate adaptive regression splines (MARS), a nonlinear and non-parametric regression methodology, to construct sales forecasting models for computer wholesalers. Through the outstanding variable screening ability of MARS, important sales forecasting variables for computer wholesalers can be obtained to enable them to make better sales management decisions. Two sets of real sales data collected from Taiwanese computer wholesalers are used to evaluate the performance of MARS. The experimental results show that the MARS model outperforms backpropagation neural networks, a support vector machine, a cerebellar model articulation controller neural network, an extreme learning machine, an ARIMA model, a multivariate linear regression model, and four two-stage forecasting schemes across various performance criteria. Moreover, the MARS forecasting results provide useful information about the relationships between the forecasting variables selected and sales amounts through the basis functions, important predictor variables, and the MARS prediction function obtained, and hence they have important implications for the implementation of appropriate sales decisions or strategies.",industry
10.1016/j.ymssp.2012.01.021,Journal,Mechanical Systems and Signal Processing,scopus,2012-07-01,sciencedirect,FPGA-based entropy neural processor for online detection of multiple combined faults on induction motors,https://api.elsevier.com/content/abstract/scopus_id/84860217701,"For industry, a faulty induction motor signifies production reduction and cost increase. Real-world induction motors can have one or more faults present at the same time that can mislead to a wrong decision about its operational condition. The detection of multiple combined faults is a demanding task, difficult to accomplish even with computing intensive techniques. This work introduces information entropy and artificial neural networks for detecting multiple combined faults by analyzing the 3-axis startup vibration signals of the rotating machine. A field programmable gate array implementation is developed for automatic online detection of single and combined faults in real time.",industry
10.1016/j.cirp.2012.03.065,Journal,CIRP Annals - Manufacturing Technology,scopus,2012-04-23,sciencedirect,Decision support systems for effective maintenance operations,https://api.elsevier.com/content/abstract/scopus_id/84861592241,"To compete successfully in the market place, leading manufacturing companies are pursuing effective maintenance operations. Existing computerized maintenance management systems (CMMS) can no longer meet the needs of dynamic maintenance operations. This paper describes newly developed decision support tools for effective maintenance operations: (1) data-driven short-term throughput bottleneck identification, (2) estimation of maintenance windows of opportunity, (3) prioritization of maintenance tasks, (4) joint production and maintenance scheduling systems, and (5) maintenance staff management. Mathematical algorithms and simulation tools are utilized to illustrate the concepts of these decision support systems. Results from real implementations in automotive manufacturing are presented to demonstrate the effectiveness of these tools.",industry
10.1016/j.jmsy.2011.09.002,Journal,Journal of Manufacturing Systems,scopus,2012-04-01,sciencedirect,Intelligent evaluation of supplier bids using a hybrid technique in distributed supply chains,https://api.elsevier.com/content/abstract/scopus_id/84858340427,"The main idea of this research is to devise the smart module to pick the best supplier bid(s) automatically. The hybrid model is composed of three useful tools: fuzzy logic, AHP, and QFD. The approach has been carefully implemented and verified via a real-world case study in a medium-to-large industry manufacturing vehicle tires and other rubber products. A collection of 12 assessment criteria classified into two categories have been considered. Eight factors are derived from customer suggestions and the other four are design specifications required to manufacture the product. The main outcomes are: a hybrid autonomous model to evaluate supplier bids without direct human intervention; devising a hybrid three-module method and overcoming complexity of computations in resulting algorithm by means of agents; outlining the best criteria to assess suppliers; evaluating the suppliers based on voice of customer during all stages of the process; and discussing analysis, design, and implementation issues of the evaluation agent. The paper includes implications for development of an integrated total system for supply chain coordination. The most important advantages of this work over earlier researches on supplier selection are: implementation of an autonomous assessment mechanism using intelligent agents for the first time, making the best out of three widely applied methodologies all at once, evaluation process mainly based on features of customer order, coordination of supply job based on a bidding system, and portal-mediated operation and control.",industry
10.1016/j.conengprac.2011.06.009,Journal,Control Engineering Practice,scopus,2012-04-01,sciencedirect,Data reconciliation and optimal management of hydrogen networks in a petrol refinery,https://api.elsevier.com/content/abstract/scopus_id/84857191932,"This paper describes the main problems associated to the management of hydrogen networks in petrol refineries and presents an approach to deal with them with the aim of operating the installation in the most profitable way. In particular, the problems of data reconciliation, economic optimization and interaction with the underlying basic control system are reviewed. The paper provides also a proposal for the implementation of the system and illustrates the approach with results obtained using real data from an industrial site.",industry
10.1016/j.eswa.2011.11.112,Journal,Expert Systems with Applications,scopus,2012-04-01,sciencedirect,A multi-agent-based decision support system for bankruptcy contagion effects,https://api.elsevier.com/content/abstract/scopus_id/84855900478,"With the increasing interdependence of marketing participants, distress experienced by a specific entity may cause other connecting firms to encounter financial difficulties, leading to a negative impact on their stock valuations. At the same time, individual investors have a great need to gain relevant information for portfolio risk management. The monitoring vision cannot be limited to investors’ portfolios but must take into account any potential candidates affected. Based on the ontological knowledge model of inter-firm relationships, the proposed multi-agent decision support system continuously observes real-time news reports and forecasts their potential impact on the corresponding stock price. After identifying relating companies for which significant market reactions can be expected, a wireless push-based message service promptly supplies information. A case study is used to illustrate the multi-agent-based decision support system (MAB-DSS) implementation and its use. The example shows that the MAB-DSS can automate the solution for intricate and dynamic valuation effects among interdependent firms and provide constructive advice for individual investors.",industry
10.1016/j.proeng.2012.07.193,Conference Proceeding,Procedia Engineering,scopus,2012-01-01,sciencedirect,Modelling and simulation for Industrial DC Motor using Intelligent control,https://api.elsevier.com/content/abstract/scopus_id/84877113112,"This paper presents an overview of Proportional Integral control (PI) and Artificial Intelligent control (AI) algorithms. AI and PI controller are analyzed using Matlab [Simulink] software. The DC motor is an attractive piece of equipment in many industrial applications requiring variable speed and load characteristics due to its ease of controllability. The main objective of this paper illustrates how the speed of the DC motor can be controlled using different controllers. The simulation results demonstrate that the responses of DC motor with an AI control which is Fuzzy Logic Control shows satisfactory well damped control performance. The results shows that Industrial DC Motor model develop using its physical parameters and controlled with an AI controller give better response, it means it can used as a controller to the real time DC Motor",industry
10.1016/j.compeleceng.2012.05.013,Journal,Computers and Electrical Engineering,scopus,2012-01-01,sciencedirect,Automatic network intrusion detection: Current techniques and open issues,https://api.elsevier.com/content/abstract/scopus_id/84866355973,"Automatic network intrusion detection has been an important research topic for the last 20years. In that time, approaches based on signatures describing intrusive behavior have become the de-facto industry standard. Alternatively, other novel techniques have been used for improving automation of the intrusion detection process. In this regard, statistical methods, machine learning and data mining techniques have been proposed arguing higher automation capabilities than signature-based approaches. However, the majority of these novel techniques have never been deployed on real-life scenarios. The fact is that signature-based still is the most widely used strategy for automatic intrusion detection. In the present article we survey the most relevant works in the field of automatic network intrusion detection. In contrast to previous surveys, our analysis considers several features required for truly deploying each one of the reviewed approaches. This wider perspective can help us to identify the possible causes behind the lack of acceptance of novel techniques by network security experts.",industry
10.3182/20120403-3-DE-3010.00066,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,A model-based approach for timing analysis of industrial automation systems,https://api.elsevier.com/content/abstract/scopus_id/84866095348,"This paper presents a temporal characterization for automation systems. The final goal is to achieve a whole model in which a schedulability analysis could be applied in order to assure that this kind of systems meet timing non-functional requirements of the application. This work is performed in the context of a Model Driven Development approach. The definition of three Domain Specific Models: control specification, and hardware and software architectures, is the base for the whole model. In particular, the software domain model uses the XML interface defined by PLCopen for expressing IEC 61131-3 automation projects. The information contained in the model of the application is processed to generate the temporal model of the automation system. A specific transformation of this model makes possible to carry out a schedulability analysis of the system. In particular, this is achieved by generating the specific input model of the well-known Modelling and Analysis Suite for Real-Time Applications, MAST.",industry
10.3182/20120403-3-DE-3010.00010,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,RodosVisor - An object-oriented and customizable hypervisor: The CPU virtualization,https://api.elsevier.com/content/abstract/scopus_id/84866091325,"RodosVisor is an object-oriented and bare-metal virtual machine monitor (VMM) or hypervisor designed for the aerospace industry, mainly to provide time and spatial separation to the NetworkCentric core avionics machine, Montenegro and Dittrich (2009). The NetworkCentric core avionics machine consists of several harmonized components working together to implement dependable computing in a simple way, with computing units managed by the local real-time operating system RODOS. To support partitioned software architectures such as AIR, Rufino et al. (2009), and MILS, DeLong, R. (2007), RodosVisor adapted the Popek and Goldberg's fidelity, efficiency and resource control virtualization requirements, Popek and Goldberg (1974), to the space application domain by extending them with extra ones, like timing determinism, reactivity and improved dependability. Another distinctive RodosVisor feature is the customized design based on generative programming techniques, such as aspect oriented programming and template meta-programming.",industry
10.3182/20120403-3-DE-3010.00082,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,Web monitoring system and gateway for serial communication PLC,https://api.elsevier.com/content/abstract/scopus_id/84866084543,"An industrial process requires interacting with the rest of the plant, being able to exchange data with other devices and monitoring systems in order to optimize production, reporting information and providing control capabilities to distant users. Internet, and, especially web browsers are an excellent tool to provide information for remote users, allowing not only monitoring but also controlling the industrial process as an SCADA software or HMI system. The proposed system does not need specific proprietary software and its associated license costs. In this work, a webserver system is implemented under a Freescale microcontroller, acting as a gateway for a simple PLC with single RS232 communication capabilities. The webserver is modular, providing independent access to single I/O PLC locations. Different webpage design can offer different monitoring capabilities by combining the required I/O modules according to the required application without any change in the microcontroller programming. This capability is close to SCADA software or industrial HMI systems where custom screens can be made. This proposal offers a low cost and flexible monitoring solution to old or basic industrial processes controlled by PLC with low communication capabilities. Using a web browser, the system can be monitored from any internet capable device: PC, tablet, smartphone, etc. An example for a pneumatic PID levitation control system is given.",industry
10.1016/j.autcon.2011.05.018,Journal,Automation in Construction,scopus,2012-01-01,sciencedirect,Simulation and analytical techniques for construction resource planning and scheduling,https://api.elsevier.com/content/abstract/scopus_id/81355138778,"To date, few construction methods or models in the literature have discussed about helping the project managers decide the near-optimum distributions of manpower, material, equipment and space according to their project objectives and project constraints. Thus, the traditional scheduling methods or models often result in a “seat-of-the-pants” style of management, rather than decision making based on an analysis of real data. This paper presents an intelligent scheduling system (ISS) that can help the project managers to find the near-optimum schedule plan according to their project objectives and project constraints. ISS uses simulation techniques to distribute resources and assign different levels of priorities to different activities in each simulation cycle to find the near-optimal solution. ISS considers and integrates most of the important construction factors (schedule, cost, space, manpower, equipment and material) simultaneously in a unified environment, which makes the resulting schedule that will be closer to optimal. Furthermore, ISS allows for what-if analyses of possible scenarios, and schedule adjustments based on unforeseen conditions (change orders, late material delivery, etc.). Finally, two sample applications and one real-world construction project are utilized to illustrate and compare the effectiveness of ISS with two widely used software packages, Primavera Project Planner and Microsoft Project.",industry
10.1016/j.wasman.2011.07.018,Journal,Waste Management,scopus,2011-12-01,sciencedirect,A web-based Decision Support System for the optimal management of construction and demolition waste,https://api.elsevier.com/content/abstract/scopus_id/80054853534,"Wastes from construction activities constitute nowadays the largest by quantity fraction of solid wastes in urban areas. In addition, it is widely accepted that the particular waste stream contains hazardous materials, such as insulating materials, plastic frames of doors, windows, etc. Their uncontrolled disposal result to long-term pollution costs, resource overuse and wasted energy. Within the framework of the DEWAM project, a web-based Decision Support System (DSS) application – namely DeconRCM – has been developed, aiming towards the identification of the optimal construction and demolition waste (CDW) management strategy that minimises end-of-life costs and maximises the recovery of salvaged building materials. This paper addresses both technical and functional structure of the developed web-based application. The web-based DSS provides an accurate estimation of the generated CDW quantities of twenty-one different waste streams (e.g. concrete, bricks, glass, etc.) for four different types of buildings (residential, office, commercial and industrial). With the use of mathematical programming, the DeconRCM provides also the user with the optimal end-of-life management alternative, taking into consideration both economic and environmental criteria. The DSS’s capabilities are illustrated through a real world case study of a typical five floor apartment building in Thessaloniki, Greece.",industry
10.1016/j.asoc.2011.05.011,Journal,Applied Soft Computing Journal,scopus,2011-12-01,sciencedirect,Credit risk evaluation using neural networks: Emotional versus conventional models,https://api.elsevier.com/content/abstract/scopus_id/80053571498,"Credit scoring and evaluation is one of the key analytical techniques in credit risk evaluation which has been an active research area in financial risk management. Artificial neural networks (NNs) have been considered to be accurate tools for credit analysis among others in the credit industry. Lately, emotional neural networks (EmNNs) have been suggested and applied successfully for pattern recognition. In this paper we investigate the efficiency of EmNNs and compare their performance to conventional NNs when applied to credit risk evaluation. In total 12 neural networks; based equally on emotional and conventional neural models; are arbitrated under three learning schemes to classify whether a credit application is approved or declined. The learning schemes differ in the ratio of training-to-validation data used during training and testing the neural networks. The emotional and conventional neural models are trained using real world credit application cases from the Australian credit approval datasets which has 690 cases; each case with 14 numerical attributes; based on which an application is accepted or rejected. The performance of the 12 neural networks will be evaluated using certain criteria. Experimental results suggest that both emotional and conventional neural models can be used effectively for credit risk evaluations, however the emotional models outperform their conventional counterparts in decision making speed and accuracy, thus, making them ideal for implementation in fast automatic processing of credit applications.",industry
10.1016/j.ijthermalsci.2011.06.020,Journal,International Journal of Thermal Sciences,scopus,2011-12-01,sciencedirect,A simplified method to evaluate the energy performance of CO<inf>2</inf> heat pump units,https://api.elsevier.com/content/abstract/scopus_id/80052736833,"The prediction of the performances of CO2 transcritical heat pumps demands accurate calculation methods, where a particular effort is devoted to the gas cooler modelling, as the correlation between high pressure and gas cooler outlet temperature strongly affects the cycle performance. The above-mentioned methods require a large amount of input data and calculation power. As a consequence they are often useless for the full characterisation of heat pumps which are sold on the market.
                  A simplified numerical method for the performance prediction of vapour compression heat pumps working in a transcritical cycle is presented, based only on performance data at the nominal rating conditions. The proposed procedure was validated against experimental data of two different tap water heat pumps. For the considered units, simulation results are in good agreement with the experimental ones. The deviations range from −6.4% to +1.7% and from −3.8% to +5.8% for the COPH
                      of the air/water heat pump and the water/water heat pump, respectively. The heating capacity deviations stayed within −5.5% and +1.7% range and within −5.0% and +7.9% range for the same units.
                  The proposed mathematical model appears to be a reliable tool to be used by the refrigeration industry or to be implemented into dynamic building-plant energy simulation codes. Finally, it represents a useful instrument for the definition of tailored approximated optimal high pressure curve considering the operating characteristics of the specific CO2 transcritical unit. It could also be implemented on board of a real unit control system where it could be used as model coupled to computational intelligence algorithms for pressure optimisation.",industry
10.1016/j.neucom.2011.06.027,Journal,Neurocomputing,scopus,2011-11-01,sciencedirect,Neural network based controller for Cr<sup>6+</sup>-Fe<sup>2+</sup> batch reduction process,https://api.elsevier.com/content/abstract/scopus_id/80053311549,An automated pilot plant has been designed and commissioned to carry out online/real-time data acquisition and control for the Cr6+–Fe2+ reduction process. Simulated data from the Cr6+–Fe2+ model derived are validated with online data and laboratory analysis using ICP-AES analysis method. The distinctive trend or patterns exhibited in the ORP profiles for the non-equilibrium model derived have been utilized to train neural network-based controllers for the process. The implementation of this process control is to ensure sufficient Fe2+ solution is dosed into the wastewater sample in order to reduce all Cr6+–Cr3+. The neural network controller has been utilized to compare the capability of set-point tracking with a PID controller in this process. For this process neural network-based controller dosed in less Fe2+ solution compared to the PID controller which hence reduces wastage of chemicals. Industrial Cr6+ wastewater samples obtained from an electro-plating factory has also been tested on the pilot plant using the neural network-based controller to determine its effectiveness to control the reduction process for a real plant. The results indicate the proposed controller is capable of fully reducing the Cr6+–Cr3+ in the batch treatment process with minimal dosage of Fe2+.,industry
10.1016/j.envsoft.2011.04.002,Journal,Environmental Modelling and Software,scopus,2011-10-01,sciencedirect,Application of the Analytic Hierarchy Process and the Analytic Network Process for the assessment of different wastewater treatment systems,https://api.elsevier.com/content/abstract/scopus_id/79957865817,"Multicriteria analyses (MCAs) are used to make comparative assessments of alternative projects or heterogeneous measures and allow several criteria to be taken into account simultaneously in a complex situation. The paper shows the application of different MCA techniques to a real decision problem concerning the choice of the most sustainable wastewater treatment (WWT) technology, namely Anaerobic digestion, Phytoremediation and Composting, for small cheese factories. Particularly, the Analytic Hierarchy Process (AHP) and its recent implementation, the Analytic Network Process (ANP), have been considered for prioritizing the different technologies. The models enable all the elements of the decision process to be considered, namely environmental aspects, technological factors and economic costs, and to compare them to find the best alternative. The AHP and ANP techniques are applied through specific software packages with user-friendly interfaces called Expertchoice and Superdecision, respectively. A comparison of the merits obtained from the different models shows that Phytoremediation results as the most sustainable WWT technology for small cheese factories and that the use of the ANP method, which allows more sophisticated analysis to be made, succeeds in offering better results.",industry
10.1016/j.eswa.2011.04.012,Journal,Expert Systems with Applications,scopus,2011-09-15,sciencedirect,Fast defect detection in homogeneous flat surface products,https://api.elsevier.com/content/abstract/scopus_id/79958015915,"This paper introduces a novel hybrid approach for both defect detection and localization in homogeneous flat surface products. Real time defect detection in industrial products is a challenging problem. Fast production speeds and the variable nature of production defects complicate the process of automating the defect detection task. Speeding up the detection process is achieved in this paper by implementing a hybrid approach that is based on the statistical decision theory, multi-scale and multi-directional analysis and a neural network implementation of the optimal Bayesian classifier. The coefficient of variation is first used as a homogeneity measure for approximate defect localization. Second, features are extracted from the log Gabor filter bank response to accurately localize and detect the defect while reducing the complexity of Gabor based inspection approaches. A probabilistic neural network (PNN) is used for fast defect classification based on the maximum posterior probability of the Log-Gabor based statistical features. Experimental results show a major performance enhancement over existing defect detection approaches.",industry
10.1016/j.ces.2011.03.041,Journal,Chemical Engineering Science,scopus,2011-08-01,sciencedirect,Successive approximate model based multi-objective optimization for an industrial straight grate iron ore induration process using evolutionary algorithm,https://api.elsevier.com/content/abstract/scopus_id/79958723405,"Multi-objective optimization of any complex industrial process using first principle computationally expensive models often demands a substantially higher computation time for evolutionary algorithms making it less amenable for real time implementation. A combination of the above-mentioned first principle model and approximate models based on artificial neural network (ANN) successively learnt in due course of optimization using the data obtained from first principle models can be intelligently used for function evaluation and thereby reduce the aforementioned computational burden to a large extent. In this work, a multi-objective optimization task (simultaneous maximization of throughput and Tumble index) of an industrial iron ore induration process has been studied to improve the operation of the process using the above-mentioned metamodeling approach. Different pressure and temperature values at different points of the furnace bed, grate speed and bed height have been used as decision variables whereas the bounds on cold compression strength, abrasion index, maximum pellet temperature and burn-through point temperature have been treated as constraints. A popular evolutionary multi-objective algorithm, NSGA II, amalgamated with the first principle model of the induration process and its successively improving approximation model based on ANN, has been adopted to carry out the task. The optimization results show that as compared to the PO solutions obtained using only the first principle model, (i) similar or better quality PO solutions can be achieved by this metamodeling procedure with a close to 50% savings in function evaluation and thereby computation time and (ii) by keeping the total number of function evaluations same, better quality PO solutions can be obtained.",industry
10.1016/j.eswa.2011.01.051,Journal,Expert Systems with Applications,scopus,2011-08-01,sciencedirect,Recommendation system for localized products in vending machines,https://api.elsevier.com/content/abstract/scopus_id/79953690190,"This paper proposes a framework of localized product recommendation system for automatic vending machines applications. The goal is to offer suitable recommendations of localized products to customers in distinct locations. We develop a hybrid technique that combines a meta-heuristic approach, clustering technique, classification, and statistical method. In the approach, an intelligent system is implemented to analyze product attributes and determine localized products based on the transaction data. To prove the feasibility and effectiveness of proposed approach, we implemented the system in several automatic vending machines owned by an information service company of Taiwan. Nine machines were selected and compared from two locations: living lab by Institute for Information Industry of Taiwan at Song-shan District and business office building at Nei-hu District in Taipei. The real life experiments showed that the profit of vending machine increases after applying our system.",industry
10.1016/j.eswa.2011.01.081,Journal,Expert Systems with Applications,scopus,2011-07-01,sciencedirect,Expert system for analysis of quality in production of electronics,https://api.elsevier.com/content/abstract/scopus_id/79952444562,"Quality issues have become increasingly important in the production of electronics, especially when dealing with electronic products not assimilated to the mainstream of consumer electronics, but rather to the group of industrial electronic devices and machinery designed to last for years or even decades. In this paper, an intelligent optimization and modeling system for electronics production is demonstrated. The system exploits real production data and can be used to diagnose and optimize the manufacturing processes. It contains three modules consisting of appropriate mathematical tools specifically tailored to each task: (1) preprocessing, (2) variable selection, and (3) optimization modules. Moreover, concrete examples are presented from the latter two modules, by using a wave soldering process as a case study. Currently, the system works on the Matlab platform, but can be programmed into standalone software and automated in the future. The results illustrate that the system can offer an efficient tool for diagnostics and process optimization in the electronics industry.",industry
10.1016/j.jchromb.2011.03.059,Journal,Journal of Chromatography B: Analytical Technologies in the Biomedical and Life Sciences,scopus,2011-06-01,sciencedirect,Influence of different spacer arms on Mimetic Ligand <sup>™</sup> A2P and B14 membranes for human IgG purification,https://api.elsevier.com/content/abstract/scopus_id/79955910197,"Microporous membranes are an attractive alternative to circumvent the typical drawbacks associated to bead-based chromatography. In particular, the present work intends to evaluate different affinity membranes for antibody capture, to be used as an alternative to Protein A resins. To this aim, two Mimetic Ligands™ A2P and B14, were coupled onto different epoxide and azide group activated membrane supports using different spacer arms and immobilization chemistries. The spacer chemistries investigated were 1,2-diaminoethane (2LP), 3,6-dioxa-1,8-octanedithiol (DES) and [1,2,3] triazole (TRZ). These new mimetic membrane materials were investigated by static and by dynamic binding capacity studies, using pure polyclonal human immunoglobulin G (IgG) solutions as well as a real cell culture supernatant containing monoclonal IgG1. The best results were obtained by combining the new B14 ligand with a TRZ-spacer and an improved Epoxy 2 membrane support material. The new B14-TRZ-Epoxy 2 membrane adsorbent provided binding capacities of approximately 3.1mg/mL, besides (i) a good selectivity towards IgG, (ii) high IgG recoveries of above 90%, (iii) a high Pluronic-F68 tolerance and (iv) no B14-ligand leakage under harsh cleaning-in-place conditions (0.6M sodium hydroxide). Furthermore, foreseeable improvements in binding capacity will promote the implementation of membrane adsorbers in antibody manufacturing.",industry
10.1016/j.matdes.2011.01.058,Journal,Materials and Design,scopus,2011-06-01,sciencedirect,A hybrid of back propagation neural network and genetic algorithm for optimization of injection molding process parameters,https://api.elsevier.com/content/abstract/scopus_id/79953161387,"This paper presents a hybrid optimization method for optimizing the process parameters during plastic injection molding (PIM). This proposed method combines a back propagation (BP) neural network method with an intelligence global optimization algorithm, i.e. genetic algorithm (GA). A multi-objective optimization model is established to optimize the process parameters during PIM on the basis of the finite element simulation software Moldflow, Orthogonal experiment method, BP neural network as well as Genetic algorithm. Optimization goals and design variables (process parameters during PIM) are specified by the requirement of manufacture. A BP artificial neural network model is developed to obtain the mathematical relationship between the optimization goals and process parameters. Genetic algorithm is applied to optimize the process parameters that would result in optimal solution of the optimization goals. A case study of a plastic article is presented. Warpage as well as clamp force during PIM are investigated as the optimization objectives. Mold temperature, melt temperature, packing pressure, packing time and cooling time are considered to be the design variables. The case study demonstrates that the proposed optimization method can adjust the process parameters accurately and effectively to satisfy the demand of real manufacture.",industry
10.1016/j.aca.2011.01.041,Journal,Analytica Chimica Acta,scopus,2011-03-18,sciencedirect,Biodiesel classification by base stock type (vegetable oil) using near infrared spectroscopy data,https://api.elsevier.com/content/abstract/scopus_id/79952487365,"The use of biofuels, such as bioethanol or biodiesel, has rapidly increased in the last few years. Near infrared (near-IR, NIR, or NIRS) spectroscopy (>4000cm−1) has previously been reported as a cheap and fast alternative for biodiesel quality control when compared with infrared, Raman, or nuclear magnetic resonance (NMR) methods; in addition, NIR can easily be done in real time (on-line). In this proof-of-principle paper, we attempt to find a correlation between the near infrared spectrum of a biodiesel sample and its base stock. This correlation is used to classify fuel samples into 10 groups according to their origin (vegetable oil): sunflower, coconut, palm, soy/soya, cottonseed, castor, Jatropha, etc. Principal component analysis (PCA) is used for outlier detection and dimensionality reduction of the NIR spectral data. Four different multivariate data analysis techniques are used to solve the classification problem, including regularized discriminant analysis (RDA), partial least squares method/projection on latent structures (PLS-DA), K-nearest neighbors (KNN) technique, and support vector machines (SVMs). Classifying biodiesel by feedstock (base stock) type can be successfully solved with modern machine learning techniques and NIR spectroscopy data. KNN and SVM methods were found to be highly effective for biodiesel classification by feedstock oil type. A classification error (E) of less than 5% can be reached using an SVM-based approach. If computational time is an important consideration, the KNN technique (E
                     =6.2%) can be recommended for practical (industrial) implementation. Comparison with gasoline and motor oil data shows the relative simplicity of this methodology for biodiesel classification.",industry
10.1016/j.asoc.2010.09.007,Journal,Applied Soft Computing Journal,scopus,2011-03-01,sciencedirect,Forecasting stock markets using wavelet transforms and recurrent neural networks: An integrated system based on artificial bee colony algorithm,https://api.elsevier.com/content/abstract/scopus_id/78751613501,"This study presents an integrated system where wavelet transforms and recurrent neural network (RNN) based on artificial bee colony (abc) algorithm (called ABC-RNN) are combined for stock price forecasting. The system comprises three stages. First, the wavelet transform using the Haar wavelet is applied to decompose the stock price time series and thus eliminate noise. Second, the RNN, which has a simple architecture and uses numerous fundamental and technical indicators, is applied to construct the input features chosen via Stepwise Regression-Correlation Selection (SRCS). Third, the Artificial Bee Colony algorithm (ABC) is utilized to optimize the RNN weights and biases under a parameter space design. For illustration and evaluation purposes, this study refers to the simulation results of several international stock markets, including the Dow Jones Industrial Average Index (DJIA), London FTSE-100 Index (FTSE), Tokyo Nikkei-225 Index (Nikkei), and Taiwan Stock Exchange Capitalization Weighted Stock Index (TAIEX). As these simulation results demonstrate, the proposed system is highly promising and can be implemented in a real-time trading system for forecasting stock prices and maximizing profits.",industry
10.1016/j.cma.2010.11.014,Journal,Computer Methods in Applied Mechanics and Engineering,scopus,2011-02-01,sciencedirect,Multi-objective optimization of turbomachinery using improved NSGA-II and approximation model,https://api.elsevier.com/content/abstract/scopus_id/78650612499,"Coupled optimization methods based on multi-objective genetic algorithms and approximation models are widely used in engineering optimizations. In the present paper, a similar framework is proposed for the aerodynamic optimization of turbomachinery by coupling the well known multi-objective genetic algorithm—NSGA-II and back propagation neural network. The verification results of mathematical problems show that the coupled method with the origin NSGA-II cannot get the real Pareto front due to the prediction error of BPNN. A modified crowding distance is proposed in cooperation with a coarse-to-fine approaching strategy based on the iterations between NSGA-II and BPNN. The results of mathematical model problems show the effect of these improving strategies. An industrial application case is implemented on a transonic axial compressor. The optimization objectives are to maximize efficiencies of two working points and to minimize the variation of the choked mass flow. CFD simulation is employed to provide the performance evaluation of initial training samples for BPNN. The optimized results are compared with optimization results of a single objective optimization based on weighting function. The comparison shows that the present framework can provide not only better solutions than the single objective optimization, but also various alternative solutions. The increase of computational costs is acceptable especially when approximation models are used.",industry
10.1016/j.procs.2011.08.020,Conference Proceeding,Procedia Computer Science,scopus,2011-01-01,sciencedirect,Model development of a virtual learning environment to enhance lean education,https://api.elsevier.com/content/abstract/scopus_id/84856459118,"Modern day industry is becoming leaner by the day. This demands engineers with an in-depth understanding of lean philosophies. Current methods for teaching lean include hands-on projects and simulation. However, simulation games available in the market lack simplicity, ability to store the results, and modeling power. The goal of this research is to develop a virtual simulation platform which would enable students to perform various experiments by applying lean concepts. The design addresses these deficiencies through the use of VE-Suite, a virtual engineering software. The design includes user-friendly dialogue boxes, graphical models of machines, performance display gauges, and an editable layout. The platform uses laws of operations management such as Little's law, economic order quantity (EOQ) models, and cycle time. These laws enable students to implement various lean concepts such as pull system, just-in-time (JIT), single piece flow, single minute exchange of dies (SMED), kaizen, kanban, U-layout, by modifying the process parameters such as process times, setup times, layout, number, and placement of machines. The simulation begins with a traditional push type mass production line and the students improve the line by implementing lean techniques. Thus, students experience the advantages of lean real time while facing the real life problems encountered in implementing it.",industry
10.1016/j.jngse.2011.08.002,Journal,Journal of Natural Gas Science and Engineering,scopus,2011-01-01,sciencedirect,An implementation of a distributed artificial intelligence architecture to the integrated production management,https://api.elsevier.com/content/abstract/scopus_id/84855498917,"The oil production process is highly complex and requires the combination of several disciplines and technological tools for its management. System integration and the automation of the workflows required to develop oil production operations are two main problems nowadays at the oil and gas production industry. This work approaches these problems through the implementation of distributed artificial intelligence architecture, designed for the automated production management. The architecture comprises a standardized schema to access information sources, a production ontological framework and an intelligent workflow mechanism based on multi-agent systems and electronic institution. Our architecture present several novelties: the incorporation of the semantic integration, the extension of the agents theory through the electronic institutions paradigm to solve the real-time decision problems typical in the industry, the Holon-Agent hybrid model used to make more feasible its implementation, among others. An oil production management case study is presented in order to demonstrate the applicability of the proposed architecture.",industry
10.1016/j.isatra.2011.06.006,Journal,ISA Transactions,scopus,2011-01-01,sciencedirect,A hybrid intelligent controller for a twin rotor MIMO system and its hardware implementation,https://api.elsevier.com/content/abstract/scopus_id/80052415887,"This paper presents a fuzzy PID control scheme with a real-valued genetic algorithm (RGA) to a setpoint control problem. The objective of this paper is to control a twin rotor MIMO system (TRMS) to move quickly and accurately to the desired attitudes, both the pitch angle and the azimuth angle in a cross-coupled condition. A fuzzy compensator is applied to the PID controller. The proposed control structure includes four PID controllers with independent inputs in 2-DOF. In order to reduce total error and control energy, all parameters of the controller are obtained by a RGA with the system performance index as a fitness function. The system performance index utilized the integral of time multiplied by the square error criterion (ITSE) to build a suitable fitness function in the RGA. A new method for RGA to solve more than 10 parameters in the control scheme is investigated. For real-time control, Xilinx Spartan II SP200 FPGA (Field Programmable Gate Array) is employed to construct a hardware-in-the-loop system through writing VHDL on this FPGA.",industry
10.1016/j.csda.2010.06.014,Journal,Computational Statistics and Data Analysis,scopus,2011-01-01,sciencedirect,Robust weighted kernel logistic regression in imbalanced and rare events data,https://api.elsevier.com/content/abstract/scopus_id/77956394683,"Recent developments in computing and technology, along with the availability of large amounts of raw data, have contributed to the creation of many effective techniques and algorithms in the fields of pattern recognition and machine learning. The main objectives for developing these algorithms include identifying patterns within the available data or making predictions, or both. Great success has been achieved with many classification techniques in real-life applications. With regard to binary data classification in particular, analysis of data containing rare events or disproportionate class distributions poses a great challenge to industry and to the machine learning community. This study examines rare events (REs) with binary dependent variables containing many more non-events (zeros) than events (ones). These variables are difficult to predict and to explain as has been evidenced in the literature. This research combines rare events corrections to Logistic Regression (LR) with truncated Newton methods and applies these techniques to Kernel Logistic Regression (KLR). The resulting model, Rare Event Weighted Kernel Logistic Regression (RE-WKLR), is a combination of weighting, regularization, approximate numerical methods, kernelization, bias correction, and efficient implementation, all of which are critical to enabling RE-WKLR to be an effective and powerful method for predicting rare events. Comparing RE-WKLR to SVM and TR-KLR, using non-linearly separable, small and large binary rare event datasets, we find that RE-WKLR is as fast as TR-KLR and much faster than SVM. In addition, according to the statistical significance test, RE-WKLR is more accurate than both SVM and TR-KLR.",industry
10.1016/j.robot.2010.08.004,Journal,Robotics and Autonomous Systems,scopus,2010-12-31,sciencedirect,Open-ended evolution as a means to self-organize heterogeneous multi-robot systems in real time,https://api.elsevier.com/content/abstract/scopus_id/78649913375,"This work deals with the application of multi-robot systems to real tasks and, in particular, their coordination through interaction based control systems. Within this field, the practical solutions that have been implemented in real robots mainly use strongly coordinated architectures and assignment strategies because of reliability and fault tolerance issues when addressing problems in reality. Emergent approaches have also been proposed with limited success, basically due to the unpredictability of the behaviors obtained. Here, an emergent approach, called r-ASiCo, is presented containing a procedure to produce predictable solutions and thus avoiding the typical problems associated with these techniques. The r-ASico algorithm is the real time version of the Asynchronous Situated Co-evolution algorithm (ASiCo), which exploits natural open-ended evolution to generate emergent complex collective behaviors and deals with systems made up of a huge number of elements and nonlinear interactions. The goal of r-ASiCo is to design the global behavior desired for the robot team as a collective entity and allow the emergence of behaviors through the interaction of the team members using social rules they learn to implement. To this end, r-ASiCo manages a series of features that are inherent to natural evolution based methods such as energy exchange and mating selection procedures, together with a technique to guide the evolution towards a design objective, the principled evaluation function selection procedure. Hence, this paper presents the components and operation of r-ASiCo and illustrates its application through a collective cleaning task example. It was implemented using 8 e-puck robots in two different real scenarios and its results complemented with those of a 30 e-puck case. The results show the capabilities of r-ASiCo to create a self-organized and adaptive multi-robot system configuration that is tolerant to environmental changes and to failures within the robot team.",industry
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry
10.1016/j.simpat.2010.04.010,Journal,Simulation Modelling Practice and Theory,scopus,2010-10-01,sciencedirect,Revisiting state space exploration of timed coloured petri net models to optimize manufacturing system's performance,https://api.elsevier.com/content/abstract/scopus_id/77955305732,"Due to constant fluctuations in market demands, nowadays scheduling of flexible manufacturing systems is taking great importance to improve competitiveness. Coloured Petri Nets (CPN) is a high level modelling formalism which have been widely used to model and verify systems, allowing representing not only the system’s dynamic behaviour but also the information flow. One approach that focuses in performance optimization of industrial systems is the one that uses the CPN formalism extended with time features (Timed Coloured Petri Nets) and explores all the possible states of the model (state space) looking for states of particular interest under industrial scope. Unfortunately, using the time extension, the state space becomes awkward for most industrial problems, reason why there is a recognized need of approaches that could tackle optimization problems such as the scheduling of manufacturing activities without simplifying any important aspect of the real system. In this paper a timed state space approach for properties verification and systems optimization is presented together with new algorithms in order to get better results when time is used as a cost function for optimizing the makespan of manufacturing systems. A benchmarking example of a job-shop is modelled in CPN formalism to illustrate the improvements that can be achieved with the proposed implementations.",industry
10.1016/j.jfoodeng.2010.02.027,Journal,Journal of Food Engineering,scopus,2010-07-01,sciencedirect,Nonlinear predictive control based on artificial neural network model for industrial crystallization,https://api.elsevier.com/content/abstract/scopus_id/77950043186,"This paper illustrates the benefits of a nonlinear model based predictive control (NMPC) strategy for setpoint tracking control of an industrial crystallization process. A neural networks model is used as internal model to predict process outputs. An optimization problem is solved to compute future control actions taking into account real-time control objectives. Furthermore, a more suitable output variable is used for process control: the mass of crystals in the solution is used instead of the traditional electrical conductivity. The performance of the NMPC implementation is assessed via simulation results based on industrial data.",industry
10.1016/j.engappai.2010.02.007,Journal,Engineering Applications of Artificial Intelligence,scopus,2010-06-01,sciencedirect,A knowledge-based architecture for distributed fault analysis in power networks,https://api.elsevier.com/content/abstract/scopus_id/77950510844,"Power industry around the world is facing several changes since deregulation with constant pressure put on improving security, reliability and quality of the power supply. Computational fault analysis and diagnosis of power networks have been active research topics with several theories and algorithms proposed. This paper proposes a distributed diagnostic algorithm for fault analysis in power networks. Distributed architecture for power network fault analysis (DAPFA) is an intelligent, model-based diagnostic algorithm that incorporates a hierarchical power network representation and model. The architecture is based on the industry’s substation automation implementation standards. The structural and functional model is a multi-level representation with each level depicting a more complex grouping of components than its predecessor in the hierarchy. The distributed functional representation contains the behavioral knowledge related to the components of that level in the structural model.
                  The diagnostic algorithm of DAPFA is designed to perform fault analysis in pre-diagnostic and diagnostic levels. Pre-diagnostic phase provides real-time analysis while the diagnostic phase provides the final diagnostic analysis. The diagnostic algorithm incorporates knowledge-based and model-based reasoning mechanisms with one of the model levels represented as a network of neural nets. The relevant algorithms and techniques are discussed. The resulting system has been implemented on a New Zealand sub-system and the results are analyzed.",industry
10.1016/j.matcom.2010.01.002,Journal,Mathematics and Computers in Simulation,scopus,2010-05-01,sciencedirect,Flow regimes identification and liquid-holdup prediction in horizontal multiphase flow based on neuro-fuzzy inference systems,https://api.elsevier.com/content/abstract/scopus_id/77953134320,"Numerous techniques have been used to identify flow regimes and liquid holdup in horizontal multiphase flow, but often neither perform well nor very accurate. Recently, neuro-fuzzy inference systems learning scheme have been gaining popularity in its capability for solving both prediction and classification problems. It is a hybrid intelligent systems scheme that is able to forecast an output in the uncertainty situations. This paper investigates the capabilities of neuro-fuzzy TypeI in identifying flow regimes and forecasting liquid holdup in horizontal multiphase flow. The performance of neuro-fuzzy modeling scheme is implemented using different real-world industry databases. Comparative studies were carried out to compare neuro-fuzzy systems performance with the most popular existing approaches in identifying flow regimes and predict liquid holdup in horizontal multiphase flow. Results show that neuro-fuzzy is flexible, reliable, outperforms the existing techniques and show bright future capabilities in solving different oil and gas industry problems, namely, rock mechanics properties, water saturation, faceis classification, and distinct bioinformatics applications.",industry
10.1016/S1684-1182(10)60014-X,Journal,"Journal of Microbiology, Immunology and Infection",scopus,2010-04-01,sciencedirect,Fast Diagnosis and Quantification for Porcine Circovirus Type 2 (PCV-2) Using Real-Time Polymerase Chain Reaction,https://api.elsevier.com/content/abstract/scopus_id/77951778976,"Background/Purpose
                  The postweaning multisystemic wasting syndrome, caused by the porcine circovirus type 2 (PCV-2), is a major disease that poses a significant threat to the global swine industry. The purpose of this study was to establish a real-time polymerase chain reaction (PCR) method for the quantification of PCV-2 and to enable the rapid differentiation of porcine circoviruses type 1 and 2 (PCV-1 and PCV-2). Such a method would significantly speed up the process of clinical diagnosis, and could also be used to study the pathogenic mechanisms of diseases associated with PCV-2.
               
                  Methods
                  Multiplex real-time PCR, together with LightCycler PCR data analysis software, was used for the quantification of PCV-2, and for the rapid differentiation of PCV-1 and PCV-2. A 263-bp DNA fragment was amplified from the 3′ end of the open reading frame-2 of PCV-2 by nested PCR, and its DNA sequence was verified as having 100% identity with a PCV-2 standard (NCBI accession number: AF055394). The 263-bp DNA fragment was cloned into the pGEM-T easy vector, and the recombinant plasmid was serially diluted and quantified using real-time PCR. A standard curve was then constructed for quantification of the PCV-2 levels in field samples. The differentiation of PCV-1 and PCV-2 was carried out by analyzing the melting temperatures of the genotype-specific PCR products.
               
                  Results
                  To quantify the PCV-2 levels in field samples, a standard curve (1 × 102 −1 × 109 copies/μL) was constructed. PCV-2 concentrations as low as 1 × 102 copies/mL could be detected in specimens taken from the lymph nodes or infected tissues in samples of PCV-2-infected pigs. The diagnosis of PCV-1 and PCV-2 infections and the quantification of the viral load in the field samples could be completed within 45 minutes after extracting the viral DNA using a commercial extraction kit.
               
                  Conclusion
                  This study demonstrate that real-time PCR is a clinically feasible method for the accurate quantification of PCV-2, and for the rapid differentiation of PCV-1 and PCV-2.",industry
10.1016/j.eswa.2009.09.057,Journal,Expert Systems with Applications,scopus,2010-04-01,sciencedirect,Rule based system for power quality disturbance classification incorporating S-transform features,https://api.elsevier.com/content/abstract/scopus_id/71349083572,"Detection and classification of power quality (PQ) disturbances in real-time is an important consideration to electric utilities and many industrial customers so that diagnosis and mitigation of such disturbances can be implemented quickly. This paper presents the design and development of a rule based system for intelligent classification of PQ disturbances using the S-transform features. A hardware system has been designed using advanced digital signal processor to provide fast data capture and processing of signals using the S-transform analysis. Distinct features of various disturbances are extracted from the S-transform analysis in which these features are used to formulate rules. A rule-based expert system is developed to automate the process of classifying the various types of disturbances. The disturbance classification results prove that the developed rule based system is more accurate than the neural network in classifying PQ disturbances such as voltage sag, swell, impulsive transient, notching and interruption.",industry
10.1016/j.cej.2010.01.018,Journal,Chemical Engineering Journal,scopus,2010-03-01,sciencedirect,"Industrial batch dryer data mining using intelligent pattern classifiers: Neural network, neuro-fuzzy and Takagi-Sugeno fuzzy models",https://api.elsevier.com/content/abstract/scopus_id/76449108616,"This contribution describes the pattern recognition based data analysis of an existing industrial batch dryer, and the comparison of three artificial intelligence techniques suited to perform classification tasks: neural networks trained using the Levenberg–Marquardt and the Levenberg–Marquardt method with Bayesian regularization, the neuro-fuzzy model based on clustering and grid partition, and the Takagi–Sugeno fuzzy models. The classifiers are used to quantify the dryer batch time and its variation during a certain production period, thus the motivation behind the work is genuine. The presented pattern recognition method implements a supervised learning approach and is based on pressure measurement profiles recorded by the plant data management software—the PI System from OSIsoft.
                  It is found that the neural networks trained with the Bayesian regularization have shown the most robust classification performance with respect to separation threshold selection. Furthermore, it is concluded that the application of artificial intelligence techniques in real chemical manufacturing facilities is feasible and provides useful information for process performance monitoring purposes. The pattern recognition findings presented in this paper are not case specific and can be directly used for the monitoring of a large variety of drying processes since the pressure profile features – vacuum check, pressure decrease, vacuum break – do not depend on the chemicals which are dried. Since the development of the artificial intelligent classifiers is presented in detail and step by step, this work may be interesting as a pattern recognition tutorial for chemical engineers.",industry
10.3182/20100831-4-fr-2021.00069,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2010-01-01,sciencedirect,Synthetic target systems in control education: Lessons teachers are learning from students,https://api.elsevier.com/content/abstract/scopus_id/84901939930,"In modern Programmable Logic Controllers (PLCs) programming education and training, software packages emulating industrial plants are replacing physical target systems. Whilst this approach is indubitably cost and safety effective, it is important to understand how educationally effective it is, and to what extent can it replace training on real plants. The paper analyses this problem from the feedback that authors got from HMS research and their students in a more than ten years build up experience of control education based on both real and synthetic systems. It concludes that synthetic plants are indeed effective in many training scenarios, but real target systems are still irreplaceable for a cluster of applications. Moreover, despite computer games technology is making synthetic target systems very appealing, both educators and simulation software developers recognize that there is still room for improvement. As such, seizing recent advances in computer technology, software developers are preparing a new generation of synthetic plants.",industry
10.1016/j.robot.2010.04.001,Journal,Robotics and Autonomous Systems,scopus,2010-01-01,sciencedirect,Visual servoing of redundant manipulator with Jacobian matrix estimation using self-organizing map,https://api.elsevier.com/content/abstract/scopus_id/80052724345,"Vision based redundant manipulator control with a neural network based learning strategy is discussed in this paper. The manipulator is visually controlled with stereo vision in an eye-to-hand configuration. A novel Kohonen’s self-organizing map (KSOM) based visual servoing scheme has been proposed for a redundant manipulator with 7 degrees of freedom (DOF). The inverse kinematic relationship of the manipulator is learned using a Kohonen’s self-organizing map. This learned map is shown to be an approximate estimate of the inverse Jacobian, which can then be used in conjunction with the proportional controller to achieve closed loop servoing in real-time. It is shown through Lyapunov stability analysis that the proposed learning based servoing scheme ensures global stability. A generalized weight update law is proposed for KSOM based inverse kinematic control, to resolve the redundancy during the learning phase. Unlike the existing visual servoing schemes, the proposed KSOM based scheme eliminates the computation of the pseudo-inverse of the Jacobian matrix in real-time. This makes the proposed algorithm computationally more efficient. The proposed scheme has been implemented on a 7 DOF PowerCube™ robot manipulator with visual feedback from two cameras.",industry
10.1016/j.eswa.2010.04.092,Journal,Expert Systems with Applications,scopus,2010-01-01,sciencedirect,A Bayesian petrophysical decision support system for estimation of reservoir compositions,https://api.elsevier.com/content/abstract/scopus_id/77957846821,"The exploration for oil and gas requires real-time petrophysical expertise to interpret measurement data acquired in boreholes and to recommend further steps. High time pressure and the far reaching nature of these decisions, as well as the limited opportunity to gain in depth petrophysical experience suggests that a decision support system that can aid the petrophysicist will be very useful.
                  In this paper we describe a Bayesian approach for obtaining compositional estimates that combines expert knowledge with information obtained from measurements. We define a prior model for the compositional volume fractions and observation models for each of the measurement tools. Both prior and observation models are based on domain expertise. These models are combined in a joint probability model. To deal with the nonlinearities in the model, Bayesian inference is implemented by using the hybrid Monte Carlo algorithm.
                  In the system, tool measurement values can entered and the posterior probability distribution of the compositional fractions can be obtained by applying Bayes’ rule. Bayesian inference is also used for optimal tool selection, using conditional entropy to select the most informative tool to obtain better estimates of the reservoir.
                  Reliability and consistency of the method is demonstrated by inference on synthetically generated data.",industry
10.1016/j.apenergy.2009.10.026,Journal,Applied Energy,scopus,2010-01-01,sciencedirect,A new spinning reserve requirement forecast method for deregulated electricity markets,https://api.elsevier.com/content/abstract/scopus_id/77950860487,"Ancillary services are necessary for maintaining the security and reliability of power systems and constitute an important part of trade in competitive electricity markets. Spinning Reserve (SR) is one of the most important ancillary services for saving power system stability and integrity in response to contingencies and disturbances that continuously occur in the power systems. Hence, an accurate day-ahead forecast of SR requirement helps the Independent System Operator (ISO) to conduct a reliable and economic operation of the power system. However, SR signal has complex, non-stationary and volatile behavior along the time domain and depends greatly on system load. In this paper, a new hybrid forecast engine is proposed for SR requirement prediction. The proposed forecast engine has an iterative training mechanism composed of Levenberg–Marquadt (LM) learning algorithm and Real Coded Genetic Algorithm (RCGA), implemented on the Multi-Layer Perceptron (MLP) neural network. The proposed forecast methodology is examined by means of real data of Pennsylvania–New Jersey–Maryland (PJM) electricity market and the California ISO (CAISO) controlled grid. The obtained forecast results are presented and compared with those of the other SR forecast methods.",industry
10.1016/j.ces.2009.11.003,Journal,Chemical Engineering Science,scopus,2010-01-01,sciencedirect,Neurocontrol of a multi-effect batch distillation pilot plant based on evolutionary reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/76049117435,"The time cost of first-principles dynamic modelling and the complexity of nonlinear control strategies may limit successful implementation of advanced process control. The maximum return on fixed capital within the processing industries is thus compromised. This study introduces a neurocontrol methodology that uses partial system identification and symbiotic memetic neuro-evolution (SMNE) for the development of neurocontrollers. Partial system identification is achieved using singular spectrum analysis (SSA) to extract state variables from time series data. The SMNE algorithm uses a symbiotic evolutionary algorithm and particle swarm optimisation to learn optimal neurocontroller weights from the partially identified system within a reinforcement learning framework. A multi-effect batch distillation (MEBAD) pilot plant was constructed to demonstrate the real world application of the neurocontrol methodology, motivated by the nonsteady state operation and nonlinear process interaction between multiple distillation columns. Multi-loop proportional integral (PI) control was implemented as a reduced model, reflecting an approach involving no modelling or significant controller tuning. Rapid multiple input multiple out nonlinear controller development was achieved using SSA and the SMNE algorithm, demonstrating comparable time and cost to implementation in relation to the reduced model. The optimal neurocontroller reduced the batch time and therefore the energy consumption by 45% compared to conventional multi-loop SISO PI control.",industry
10.1016/j.desal.2009.02.065,Journal,Desalination,scopus,2009-11-30,sciencedirect,Factorial experimental design for biosorption of iron and zinc using Typha domingensis phytomass,https://api.elsevier.com/content/abstract/scopus_id/71249156043,"Typha domingensis phytomass was used as a biosorbent for metal ions removal from wastewater. A full 23 factorial design of experiments was used to obtain the best conditions of biosorption of Fe3+ and Zn2+ from water solutions. The three factors considered were temperature, pH, and biosorbent dosage. Two levels for each factor were used; pH (2.5 and 6.0), temperature (25 and 45°C), and phytomass loading weight (0.5 and 1g/50ml). Batch experiments were carried out using 50ml solutions containing 10mg/l Fe3+ and 4mg/l Zn2+ simulating the concentration of those metals in a real wastewater effluent. The removal percentages of iron and zinc after 120min of contact time were then evaluated. The results were analyzed statistically using the Minitab 15 statistical software to determine the most important factors affecting the metals removal efficiency. The pH was found to be the most significant factor for the two studied metal ions.",industry
10.1016/j.ejor.2008.11.024,Journal,European Journal of Operational Research,scopus,2009-11-16,sciencedirect,"Tactical level planning in float glass manufacturing with co-production, random yields and substitutable products",https://api.elsevier.com/content/abstract/scopus_id/67349125429,"We investigate tactical level planning problems in float glass manufacturing. Float glass manufacturing is a process that has some unique properties such as uninterruptible production, random yields, partially controllable co-production compositions, complex relationships in sequencing of products, and substitutable products. Furthermore, changeover times and costs are very high, and production speed depends significantly on the product mix. These characteristics render measurement and management of the production capacity difficult. The motivation for this study is a real life problem faced at Trakya Cam in Turkey. Trakya Cam has multiple geographically separated production facilities. Since transportation of glass is expensive, logistics costs are high. In this paper, we consider multi-site aggregate planning, and color campaign duration and product mix planning. We develop a decision support system based on several mixed integer linear programming models in which production and transportation decisions are made simultaneously. The system has been fully implemented, and has been in use at Trakya Cam since 2005.",industry
10.1016/j.cie.2009.06.009,Journal,Computers and Industrial Engineering,scopus,2009-11-01,sciencedirect,Using Minimum Quantization Error chart for the monitoring of process states in multivariate manufacturing processes,https://api.elsevier.com/content/abstract/scopus_id/71849108091,"The need for multivariate statistical process control (MSPC) becomes more important as several variables should be monitored simultaneously. MSPC is implemented using a variety of techniques including neural networks (NNs). NNs have excellent noise tolerance in real time, requiring no hypothesis on statistical distribution of monitored processes. This feature makes NNs promising tools used for monitoring process changes. However, major NNs applied in SPC are based on supervised learning, which limits their wide applications. In the paper, a Self-Organizing Map (SOM)-based process monitoring approach is proposed for enhancing the monitoring of manufacturing processes. It is capable to provide a comprehensible and quantitative assessment value for current process state, which is achieved by the Minimum Quantization Error (MQE) calculation. Based on these MQE values over time series, an MQE chart is developed for monitoring process changes. The performance of MQE chart is analyzed in a bivariate process under the assumption that the predictable abnormal patterns are not available. The performance of MQE is further evaluated in a semiconductor batch manufacturing process. The experimental results indicate that MQE charts can become an effective monitoring and analysis tool for MSPC.",industry
10.1016/j.biotechadv.2009.05.003,Journal,Biotechnology Advances,scopus,2009-11-01,sciencedirect,Advances in on-line monitoring and control of mammalian cell cultures: Supporting the PAT initiative,https://api.elsevier.com/content/abstract/scopus_id/70349949073,"In recent years, much attention has been directed towards the development of global methods for on-line process monitoring, especially since the Food and Drug Administration (FDA) launched the Process Analytical Technology (PAT) guidance, stimulating biopharmaceutical companies to update their monitoring tools to ensure a pre-defined final product quality. The ideal technologies for biopharmaceutical processes should operate in situ, be non-invasive and generate on-line information about multiple key bioprocess and/or metabolic variables. A wide range of spectroscopic techniques based on in situ probes have already been tested in mammalian cell cultures, such as near infrared (NIR), mid infrared (MIR), 2D fluorescence and dielectric capacitance spectroscopy; similarly, the electronic nose technique based on chemical array sensors has been tested for in situ off-gas analysis of mammalian cell cultures. All these methods provide series of spectra, from which meaningful information must be extracted. In this sense, data mining techniques such as principal components regression (PCR), partial least squares (PLS) or artificial neural networks (ANN) have been applied to handle the dense flow of data generated from the real-time process analyzers. Furthermore, the implementation of feedback control methods would help to improve process performance and ultimately ensure reproducibility. This review discusses the suitability of several spectroscopic techniques coupled with chemometric methods for improved monitoring and control of mammalian cell processes.",industry
10.1016/j.engappai.2009.03.001,Journal,Engineering Applications of Artificial Intelligence,scopus,2009-06-01,sciencedirect,An implementing framework for holonic manufacturing control with multiple robot-vision stations,https://api.elsevier.com/content/abstract/scopus_id/67349215793,"The paper describes a holonic control architecture and implementing issues for agile job shop assembly with networked intelligent robots, based on the dynamic simulation of material processing and transportation. The holarchy was defined considering the PROSA reference architecture relative to which in-line vision-based quality control was added by help of feature-based descriptions of the material flow. Two solutions for production planning are proposed: a knowledge-based algorithm using production rules, and an OO resolved scheduling rate planner (RSRP) based on variable-timing simulation. Failure- and recovery-management are developed as generic scenarios embedding the CNP mechanism into production self-rescheduling. Aggregate Order Holon execution is realized by OPC-based PLC software integration and event-driven product transportation. The holonic control of multiple networked robot-vision stations also features tolerance to station computer- (IBM PC-type), station controller- (robot controller), quality control- (machine vision) and communication- (LAN) failure. Fault tolerance and high availability at shop-floor level are provided due to the multiple physical communication capabilities of the robot controllers, to their multiple-axis multitasking operating capability, and to hardware redundancy of single points of failure (SPOF). Implementing solutions and experiments are reported for a 6-station robot-vision assembly cell with twin-track closed-loop pallet transportation system and product-racking RD/WR devices. Future developments will consider manufacturing integration at enterprise level.",industry
10.1016/j.commatsci.2008.04.030,Journal,Computational Materials Science,scopus,2009-03-01,sciencedirect,Hybrid intelligent approach for modeling and optimization of semiconductor devices and nanostructures,https://api.elsevier.com/content/abstract/scopus_id/59749102668,"In this work, we present a hybrid intelligent approach for parameter extraction and design optimization of semiconductor nanoscale devices and nanostructures. Based on evolutionary algorithms, numerical methods, neural network scheme and parallel computing technique, the optimization methodology is developed and successfully implemented. In the hybrid approach, an evolutionary algorithm, such as genetic algorithm or particle swarm optimization, firstly searches the entire problem space to get a set of roughly estimated solutions. The numerical method, such as Levenberg–Marquardt method, then performs a local optima search and sets the local optima as the suggested values for the genetic algorithm to perform further optimizations. Meanwhile, the neural network is applied to investigate the influence of parameters on the optimized functions which thus guides the evolutionary direction of genetic algorithm. For solving real world problems, all functional blocks are performed under a PC-based Linux cluster system with message-passing interface libraries. This hybrid intelligent approach has experimentally been implemented and validated for different applications in semiconductor nanodevices and nanostructures. For semiconductor nanodevice parameter extraction, this approach shows its capability to automatically extract a set of global parameters among sixteen 90nm complementary metal oxide semiconductor (CMOS) devices. Compared with the measured current–voltage (I–V) curves of fabricated CMOS samples, the optimized I–V results are within 3% of accuracy. The computational examinations including sensitivity, convergence property, and parallelization are discussed. For parameter extraction of organic light emitting diode (OLED), the approach also achieves good accuracy for red, green, blue OLEDs. For the third and fourth applications, optimal structure design of silicon photonic taper waveguide and photonic crystal are further advanced by integrating a simulation-based technique in the developed system. All of these experiments demonstrate interesting results and validate the optimization methodology. The concept of hybrid intelligent approach may benefit modeling and optimization in diverse science and engineering problems.",industry
10.1016/j.engappai.2008.05.009,Journal,Engineering Applications of Artificial Intelligence,scopus,2009-02-01,sciencedirect,Identifying source(s) of out-of-control signals in multivariate manufacturing processes using selective neural network ensemble,https://api.elsevier.com/content/abstract/scopus_id/58249092619,"In multivariate statistical process control (MSPC), most multivariate quality control charts are shown to be effective in detecting out-of-control signals based upon an overall statistic. But these charts do not relieve the need for pinpointing source(s) of the out-of-control signals. Neural networks (NNs) have excellent noise tolerance and high pattern identification capability in real time, which have been applied successfully in MSPC. This study proposed a selective NN ensemble approach DPSOEN, where several selected NNs are jointly used to classify source(s) of out-of-control signals in multivariate processes. The immediate location of the abnormal source(s) can greatly narrow down the set of possible assignable causes, facilitating rapid analysis and corrective action by quality operators. The performance of DPSOEN is analyzed in multivariate processes. It shows improved generalization performance that outperforms those of single NNs and Ensemble All approach. The investigation proposed a heuristic approach for applying the DPSOEN-based model as an effective and useful tool to identify abnormal source(s) in bivariate statistical process control (SPC) with potential application for MSPC in general.",industry
10.1016/j.compchemeng.2008.05.019,Journal,Computers and Chemical Engineering,scopus,2009-01-13,sciencedirect,ANN-based soft-sensor for real-time process monitoring and control of an industrial polymerization process,https://api.elsevier.com/content/abstract/scopus_id/57049112694,"This paper presents the development and the industrial implementation of a virtual sensor (soft-sensor) in the polyethylene terephthalate (PET) production process. This soft-sensor, based on a feed-forward artificial neural network (ANN), was primarily used to provide on-line estimates of the PET viscosity, which is necessary for process control purposes. The ANN-based soft-sensor (ANN-SS) was also used for providing redundant measurements of the viscosity that could be compared to the results obtained from the process viscometer. It was shown that the proposed ANN-SS was able to adequately infer the polymer viscosity, in such a way so as this soft-sensor could be used in the real-time process control strategy. The proposed control system has successfully been applied in servo and regulatory problems, thus allowing an effective and feasible operation of the industrial plant.",industry
10.3182/20090603-3-RU-2001.0447,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2009-01-01,sciencedirect,Telematics application to optimize operation process of municipal heat and power plant,https://api.elsevier.com/content/abstract/scopus_id/79960930362,"Municipal heat and power plant is a complex and huge industrial system. The main elements of the system are the heating boilers. To perform the operation process of the boilers efficiently it is necessary to monitor a wide range of operation parameters in a real time. Great number of the parameters, short response time and big distance between the controlled objects are the main reasons for telematic systems implementation. But, the basic conditions of telematic system application are: measurement instruments and proper control algorithm. Nowadays, heating boilers are equipped with measurement systems by the producer. The heating boilers are very expensive devices whose operation phase is very long. Therefore, in many municipal heat and power plants, the production process is carried out using old type heating boilers. In such cases telematic systems should operate in spite of limited measurement vector. To do this, special control procedures ought to be implemented. In the paper, the heating boiler control algorithms are presented. Described algorithms are used in case of real industrial objects where a set of monitored parameters is not sufficient for executing full automatic control. An expert system dedicated to support the operation process of heating boiler is also presented. Because of limited information about the heating boiler operation, the process is controlled approximately. To deal with the indicated problem, the idea of fuzzy sets implementation is also described. The presented method of control process fuzzification can increase the quality of heating boiler operation. It will make the implementation of power industry telematics more efficient.",industry
10.3182/20090603-3-RU-2001.0280,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2009-01-01,sciencedirect,Multi-agent reinforcement learning for adaptive scheduling: Application to multi-site company,https://api.elsevier.com/content/abstract/scopus_id/79960899247,"In recent years, most companies have resorted to multi-site organization in an effort to improve their competitiveness and to adapt to current conditions. In this article, we propose a model for adaptive scheduling in multi-site companies. We adopt a multi-agent approach in which intelligent agents have reactive learning ability. This allows them to make accurate short-term decisions. Our model is implemented on a 3-tier architecture that ensures the security of the data exchanged between the various company sites. Experimentations on a real case study demonstrate the applicability and the effectiveness of our model concerning both optimality and reactivity.",industry
10.1016/j.mechatronics.2009.02.007,Journal,Mechatronics,scopus,2009-01-01,sciencedirect,Neural network based design of fault-tolerant controllers for automated sequential manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/67349155544,"This paper presents a novel application of recurrent neural network (RRN) to fault-tolerant control (FTC) of automated sequential manufacturing systems (ASMS) subject to sensor faults. Two RRNs are employed: the first one acts as an I/O relations recognizer and is able to detect faulty sensors and the latter is used as an inverse model of the AMSM to compute the desired control action in a faulty case according to nominal specifications. The learning process of these networks is carried out based on training data generated from the healthy manufacturing system controlled by a programmable logic controller (PLC). Design of the proposed fault-tolerant control system (FTCS) scheme is based on utilizing the two RNNs, a reconfigurable controller and a fault decision subsystem. The design procedure of the proposed FTCS is introduced. The proposed FTCS has been implemented and tested experimentally for a benchmark industrial ASMS subject to single or multiple faulty sensors. Experimental results show the effectiveness of the procedure for a real simple plant. In addition, the results prove these features of the proposed FTCS: (a) effectively improving the faulty control system behaviors, (b) accomplishing its proper functionality in handling single and multiple sensor faults, (c) identifying the sensor faults, and (d) being advantageous in reducing the complexity of the hardware redundancy.",industry
10.3168/jds.2008-1163,Journal,Journal of Dairy Science,scopus,2009-01-01,sciencedirect,"Prediction of coagulation properties, titratable acidity, and pH of bovine milk using mid-infrared spectroscopy",https://api.elsevier.com/content/abstract/scopus_id/58349117919,"This study investigated the potential application of mid-infrared spectroscopy (MIR 4,000–900cm−1) for the determination of milk coagulation properties (MCP), titratable acidity (TA), and pH in Brown Swiss milk samples (n=1,064). Because MCP directly influence the efficiency of the cheese-making process, there is strong industrial interest in developing a rapid method for their assessment. Currently, the determination of MCP involves time-consuming laboratory-based measurements, and it is not feasible to carry out these measurements on the large numbers of milk samples associated with milk recording programs. Mid-infrared spectroscopy is an objective and nondestructive technique providing rapid real-time analysis of food compositional and quality parameters. Analysis of milk rennet coagulation time (RCT, min), curd firmness (a30, mm), TA (SH°/50mL; SH°=Soxhlet-Henkel degree), and pH was carried out, and MIR data were recorded over the spectral range of 4,000 to 900cm−1. Models were developed by partial least squares regression using untreated and pretreated spectra. The MCP, TA, and pH prediction models were improved by using the combined spectral ranges of 1,600 to 900cm−1, 3,040 to 1,700cm−1, and 4,000 to 3,470cm−1. The root mean square errors of cross-validation for the developed models were 2.36min (RCT, range 24.9min), 6.86mm (a30, range 58mm), 0.25 SH°/50mL (TA, range 3.58 SH°/50mL), and 0.07 (pH, range 1.15). The most successfully predicted attributes were TA, RCT, and pH. The model for the prediction of TA provided approximate prediction (R2
                     =0.66), whereas the predictive models developed for RCT and pH could discriminate between high and low values (R2
                     =0.59 to 0.62). It was concluded that, although the models require further development to improve their accuracy before their application in industry, MIR spectroscopy has potential application for the assessment of RCT, TA, and pH during routine milk analysis in the dairy industry. The implementation of such models could be a means of improving MCP through phenotypic-based selection programs and to amend milk payment systems to incorporate MCP into their payment criteria.",industry
10.1016/j.jchromb.2008.11.006,Journal,Journal of Chromatography B: Analytical Technologies in the Biomedical and Life Sciences,scopus,2009-01-01,sciencedirect,Application of solid-phase microextraction and gas chromatography-mass spectrometry for measuring chemicals in saliva of synthetic leather workers,https://api.elsevier.com/content/abstract/scopus_id/57549092683,"Saliva is of interest as a diagnostic aid for oral and systemic diseases, to monitor therapeutic drugs, and detect illicit drug abuse. It is also attractive for biological monitoring of exposure to hazardous solvents. The major advantage of this indicator over other biological monitoring targets is that the saliva is noninvasive and less confidential in comparison with blood and urine. Salivary analysis is generally acceptable by study subjects and can be applied to investigation of a wide variety of compounds. However, very few studies have been conducted on the saliva matrix to monitor exposure to hazardous solvents. The aim of this study is to establish an analytical method, headspace solid-phase microextraction (HS-SPME) followed by gas chromatography–mass spectrometry (GC–MS), by which the saliva matrix can be monitored for multiple compounds with various polarities, such as methyl ethyl ketone (MEK), isopropyl alcohol (IPA), and N,N-dimethyl formamide (DMF) (common solvents used in synthetic leather manufacture), as well as acetone (ACE) and N-methyl formamide (NMF) (metabolites of IPA and DMF, respectively). We studied this technique as an alternative biological monitoring method for investigating exposure to hazardous solvents. A Carboxen/Polydimethylsiloxane (CAR/PDMS 75μm) fiber coating was employed for this study, and various extraction and desorption parameters were evaluated. The extraction efficiency and reproducibility of analyses was improved by pre-incubation. The limits of detection were 0.004, 0.003, 0.006, 0.05, and 0.10μg/mL for ACE, MEK, IPA, DMF, and NMF, respectively. Method validation was performed on standards spiked in blank saliva, and a correlation was made between HS-SPME and traditional solvent pretreatment methods. It was found that correlation coefficients (r) were greater than 0.996 for each analyte, with no significant differences (p
                     >0.05) between two methods. However, the SPME method achieved lower limits of detection, with good accuracy (recovery 95.3–109.2%) and precision (1.17–8.22% CV) for both intra- and inter-assay, when quality control samples were analyzed for all five compounds. The partition coefficient for each compound between the headspace of the saliva sample and the CAR/PDMS fiber coating was 90.9, 170.1, 36.4, 3.70 and 0.92 for ACE, MEK, IPA, DMF and NMF, respectively. Real sample analyses were performed on workers in a synthetic leather factory. In summary, the SPME method is a highly versatile and flexible technique for chemical measurement, and we demonstrate its application for monitoring biological exposure to hazardous solvents. Saliva monitoring using sensitive SPME approaches for determining workplace exposure should prove useful as an alternative exposure monitoring method.",industry
10.1016/j.eswa.2008.02.053,Journal,Expert Systems with Applications,scopus,2009-01-01,sciencedirect,Software reliability identification using functional networks: A comparative study,https://api.elsevier.com/content/abstract/scopus_id/56349084084,"Software engineering development has gradually become essential element in different aspects of the daily life and an important factor in numerous critical real-industry applications, such as, nuclear plants, medical monitoring control, real-time military, bioinformatics, oil and gas industry, and air traffic control. This paper proposes a functional network as a novel computational intelligence scheme for tracking and predicting the software reliability. Several applications are presented to illustrate this new intelligent system framework models. To demonstrate the usefulness of functional networks and the existing data mining schemes, we briefly describe the learning algorithm of functional networks associativity model in predicting the software reliability. Comparative studies will be carried out to compare the performance of functional networks with the most popular existing data mining techniques, such as, statistical regression multilayer feed forward neural networks, and support vector machines. The results show that the performance of functional networks is more reliable, stable, accurate, and outperforms other techniques.",industry
10.1016/j.eswa.2007.09.058,Journal,Expert Systems with Applications,scopus,2009-01-01,sciencedirect,Knowledge acquisition for decision support systems on an electronic assembly line,https://api.elsevier.com/content/abstract/scopus_id/53849097356,"Increasing global competition has made many manufacturing companies recognize that competitive manufacturing in terms of low cost and high quality is crucial for success. Real-time process control and production optimization are, however, extremely challenging areas because manufacturing processes are getting ever more complex and involve many different parameters. This is a major problem when building decision support systems especially in electronics manufacturing. Although problem-solving is a knowledge intensive activity undertaken by people on the production floor, it is quite common to have large databases and run blindly feature extraction and data mining methods. Performance of these methods could, however, be drastically increased when combined with knowledge or expertise of the process.
                  This paper describes how defect-related knowledge on an electronic assembly line can be integrated in the decision making process at an operational and organizational level. It focuses in particular on the efficient acquisition of shallow knowledge concerning everyday human interventions on the production lines, as well as on the factory-wide sharing of the resulting information for an improved defect management. Software with dedicated interfaces has been developed using a knowledge representation that supports portability and flexibility of the system. Semi-automatic knowledge acquisition from the production floor and generation of comprehensive reports for the quality department resulted in an improvement of the usability, usage, and usefulness of the decision support system.",industry
10.1016/j.petrol.2008.08.002,Journal,Journal of Petroleum Science and Engineering,scopus,2008-12-01,sciencedirect,Integration of seismic attributes and production data for infill drilling strategies - A virtual intelligence approach,https://api.elsevier.com/content/abstract/scopus_id/57049110413,"Field development strategies are at the forefront of common engineering practices in the oil and gas industry. Reservoir simulation is the most commonly applied methodology to generate an optimum field development plan. However, reservoir simulation can be an energy and cost intensive method that often relies on rather subjective assumption of input parameters, due to lack of accurate field data. In this paper, a new approach using Artificial Neural Network (ANN) technology is proposed to predict individual well performances and accordingly develop infill drilling strategies. Due to its predictive capabilities, ANN is used as a tool to construct a correlation for production prediction. Seismic attributes, which capture heterogeneity of the reservoir geology, and completion information are used as network inputs. In calculating the interference effects, the geometry of the flow system under consideration was used together with the geometric location and the starting production schedule of each well within the system. The method was successfully implemented on a case study of the 19N 94W Township of the Wamsutter field in Wyoming using actual seismic attributes, completion information, well configuration, and production data. Production predictions were generated by the network for all locations at which seismic attributes were available. More promising locations were then selected for infill drilling purposes based on predicted productions at these locations. The predicted initial rate and 10-yr cumulative production were considered in the selection of infill drilling locations with high productivity potential. Results from this work show that the ANN was able to map the relationship between production, completion information, interference effects, and reservoir characteristics captured in seismic attributes. The proposed methodology allowed the construction of spatial maps of gas production, revealing new sweet spots which could not be identified from the existing production history alone. The production maps derived from the ANN predictions contain important heterogeneous features associated with reservoir properties reflected in seismic data. Even though well interference was initially thought to have a limited effect on well performance for the case study presented, the incorporation of well interference parameters in the network design improved production predictions, suggesting that well interference has a more significant impact on well performance than originally anticipated.",industry
10.1016/j.ijmachtools.2008.06.003,Journal,International Journal of Machine Tools and Manufacture,scopus,2008-11-01,sciencedirect,A new approach to identifying the elastic behaviour of a manufacturing machine,https://api.elsevier.com/content/abstract/scopus_id/49949095540,"The dynamic solicitations imposed on modern production machines make the rigid body hypothesis obsolete in mechanical design. To increase productivity while ensuring effector trajectory quality, numerical control of the machine has to integrate the elastic deformations of machine parts. To reach this objective, the elastic behaviour of the machine has to be accurately predicted by a simple but efficient elastic model. This model should involve low-cost computation to be implemented in a real-time control loop, and also necessitates an accurate identification process to attain the required precision.
                  The paper proposes to model the elastic behaviour of the machine using an efficient mass–spring–damper model. The main contribution lies in a new approach to accurately identifying the elastic model parameters.
                  Instead of using modal analysis, the new approach is based on time domain identification using a specific metrological device. The measurement system developed and described in this paper is based on combined computer vision and inertial sensor data.
                  First, the displacements of points of interest on the machine part are tracked and measured by a standard digital camera (0.8Mega pixels and 30frames/s). A subpixelar detector allows us to reach a level of accuracy equal to 30μm (maximum error) for a displacement up to 300mm away from the sensor, but 30Hz image sampling does not provide enough information to get all details of the overall movement.
                  So secondly, accelerometers are added to get the required information to over-sample the vision data without additional error; by doing so the effective sampling frequency increases from 30Hz to 15kHz.
                  The proposed identification method is applied to a real industrial system; the results obtained in elastic deformation prediction are quite satisfactory and validate the new approach proposed in this paper.",industry
10.1016/j.eswa.2007.08.079,Journal,Expert Systems with Applications,scopus,2008-11-01,sciencedirect,Adaptive burn-in time decision system based on pattern recognition for intelligent reliability control,https://api.elsevier.com/content/abstract/scopus_id/48949098603,"Most semiconductor companies usually screen out early field failures of semiconductor device by conducting burn-in test for all manufactured devices. Burn-in is a production process whereby all manufactured devices operated under accelerated stresses for constant periods of time and accordingly crucial in productivity, on-time delivery and quality of semiconductor device. Many researches on the determination of optimal burn-in time and schedule of its operations have been conducted. Most of them, however, had some limitations to apply to real world because of their complexity and practical difficulties. Our study aims at providing an easy, efficient and more practical alternative approach. We present a multi-agent-based system, called adaptive burn-in time decision system (ABITDS) that predict the reliability level of a newly manufactured semiconductor lot on the basis of fail patterns of previously manufactured lots and then make an adaptive decision of burn-in time according to the predicted reliability level. The ABITDS uses SOM (Self-Organizing Map) neural network to firstly extract the patterns of defective chips within the wafer, wafer bin map patterns, of previously manufactured lots with good, normal and bad reliability level and predict the reliability levels of newly manufactured lots by measuring the similarity degrees between their wafer bin map patterns and the extracted ones. We implemented a web-based ABITDS prototype and validated the effectiveness of our approach through its application to real data of a semiconductor company.",industry
10.1016/j.micpro.2008.04.002,Journal,Microprocessors and Microsystems,scopus,2008-10-01,sciencedirect,A tunable high-performance architecture for enhancement of stream video captured under non-uniform lighting conditions,https://api.elsevier.com/content/abstract/scopus_id/54549122634,"A novel architecture for performing hue-saturation-value (HSV) domain enhancement of digital color images captured under non-uniform lighting conditions is proposed in this paper for video streaming applications. The approach promotes log-domain computation to eliminate all multiplications, divisions and exponentiations utilizing the compact high-speed logarithmic estimation modules. An optimized quadrant symmetric architecture is incorporated into the design of homomorphic filter for the enhancement of intensity value. Efficient modules are also presented for conversion between RGB and HSV color spaces with tunable H and S components in HSV for more flexible color rendering. The design is able to bring out details hidden in shadow regions of the image and preserve the bright parts with adjustable vividness and color shift for improvement of visual quality while maintaining its consistency. It is capable of producing 187.86 million outputs per second (MOPs) on Xilinx’s Virtex II XC2V2000-4ff896 field programmable gate array (FPGA) at a clock frequency of 187.86MHz. It can process over 179.1 (1024×1024) frames per second, which is very suitable for high definition videos, and consumes approximately 70.7% and 76.8% less hardware resource with 127% and 280% performance boost when compared to the designs with machine learning algorithm in [M.Z. Zhang, M.J. Seow, V.K. Asari, A high performance architecture for color image enhancement using a machine learning approach, International Journal of Computational Intelligence Research – Special Issue on Advances in Neural Networks 2(1) (2006) 40–47], and with separated dynamic and contrast enhancements in [H.T. Ngo, M.Z. Zhang, L. Tao, V.K. Asari, Design of a high performance architecture for real-time enhancement of video stream captured in extremely low lighting environment, International Journal of Embedded Systems: Special Issue on Media and Stream Processing, in press], respectively. This approach also provide 83.4 times performance gain with more consistent fidelity in the results compared to some DSP based implementations (256×256 frame size) [G.D. Hines, Z. Rahman, D.J. Jobson, G.A. Woodell, DSP implementation of the retinex image enhancement algorithm, visual information processing XIII, in: Proceedings of the SPIE, vol. 5438, 2004, pp. 13–24; G.D. Hines, Z. Rahman, D.J. Jobson, G.A. Woodell, Single-scale retinex using digital signal processors, in: Proceedings of the Global Signal Processing Conference, September 2004, pp. 1–6] under the reflectance-illuminance category of image enhancement models.",industry
10.1016/j.apmr.2007.11.030,Journal,Archives of Physical Medicine and Rehabilitation,scopus,2008-05-01,sciencedirect,Development of a Wheelchair Virtual Driving Environment: Trials With Subjects With Traumatic Brain Injury,https://api.elsevier.com/content/abstract/scopus_id/42649105111,"Spaeth DM, Mahajan H, Karmarkar A, Collins D, Cooper RA, Boninger ML. Development of a wheelchair virtual driving environment: trials with subjects with traumatic brain injury.
               
                  Objective
                  To develop and test a wheelchair virtual driving environment that can provide quantifiable measures of driving ability, offer driver training, and measure the performance of alternative controls.
               
                  Design
                  A virtual driving environment was developed. The wheelchair icon is displayed in a 2-dimensional, bird's eye view and has realistic steering and inertial properties. Eight subjects were recruited to test the virtual driving environment. They were clinically evaluated for range of motion, muscle strength, and visual field function. Driving capacity was assessed by a brief trial with an actual wheelchair. During virtual trials, subjects were seated in a stationary wheelchair; a standard motion sensing joystick (MSJ) was compared with an experimental isometric joystick by using a repeated-measures design.
               
                  Setting
                  Subjects made 2 laboratory visits. The first visit included clinical evaluation, tuning the isometric joystick, familiarization with virtual driving environment, and 4 driving tasks. The second visit included 40 trials with each joystick.
               
                  Participants
                  Subjects (n=8; 7 men, 1 woman) with a mean age of 22.65±2y and traumatic brain injury, both ambulatory and nonambulatory, were recruited.
               
                  Interventions
                  The MSJ used factory settings. A tuning program customized the isometric joystick transfer functions during visit 1. During the second visit, subjects performed 40 trials with each joystick.
               
                  Main Outcome Measure
                  The root mean square error (RMSE) was defined as the average deviation from track centerline (in meters) and speed (in m/s).
               
                  Results
                  Data analysis from the first 8 subjects showed no statistically significant differences between joysticks. RMSE averaged .12 to .21m; speed averaged .75m/s. For all tasks and joysticks, driving in reverse resulted in a higher RMSE and more virtual collisions than forward driving. RMSE rates were greater in left and right turns than straight and docking tasks.
               
                  Conclusions
                  Testing with instrumented real wheelchairs can validate the virtual driving environment and assess whether virtual driving skills transfer to actual driving.",industry
10.1016/j.snb.2007.12.032,Journal,"Sensors and Actuators, B: Chemical",scopus,2008-04-14,sciencedirect,Preemptive identification of optimum fermentation time for black tea using electronic nose,https://api.elsevier.com/content/abstract/scopus_id/41449097471,"During black tea manufacturing, tealeaves pass through the fermentation process, when the grassy smell is transformed into a floral smell. Optimum fermentation is extremely crucial in deciding the final quality of finished tea and it is very important to terminate the fermentation process at the right time. Present day industry practice for monitoring of fermentation is purely subjective and is carried out by experienced personnel. In this paper, a study has been made on real-time smell monitoring of black tea during the fermentation process using electronic nose as well as prediction of the correct fermentation time. The study has been implemented in two steps. First, for prediction of optimum fermentation time, five different time-delay neural networks (TDNNs), named as multiple-time-delay neural networks (m-TDNN), have been used. During the second study, we have investigated the possibility of existence of different smell stages during the fermentation runs of black tea processing using self-organizing map (SOM), and then used three TDNNs for different smell stages. The results show excellent promise for the instrument to be used by the industry.",industry
10.1016/j.jmatprotec.2007.05.044,Journal,Journal of Materials Processing Technology,scopus,2008-01-21,sciencedirect,A systematic solution methodology for inferential multivariate modelling of industrial grinding process,https://api.elsevier.com/content/abstract/scopus_id/36549063418,"The need for precision components and parts in manufacturing industries has bought an increase in the need for finishing operations that can satisfy this demand. In addition, there is a continuous demand for hard and tough materials that can withstand varying stress conditions to ensure prolonged service life of components and parts. The need to process these materials economically so as to meet stringent product quality requirements (generally expressed as composite of a family of properties, so-called multiple response characteristics) has become a real challenge for researchers and practitioners in manufacturing industries. Grinding has the potential to meet these critical needs for accurate and economic means of finishing parts, and generate the required surface topography. Despite this importance and popularity, grinding still remains one of the most difficult and least-understood processes due to lack of adequate inferential mechanistic and analytical multivariate models, for varied industrial situations. In this context, data-driven inferential linear or nonlinear multiple statistical regression, and artificial neural network modelling have become increasingly popular techniques for complex industrial grinding processes. Unfortunately, these techniques are either proposed and implemented in isolation or presented as a comparative evaluation grinding case study. A systematic solution methodology for inferential multivariate modelling, which addresses the different phases, starting from preliminary linear random x-case multivariate regression model, hypothesis testing of influence of addition of higher-order nonlinear terms to the adequate linear model (or presence of nonlinearity), and subsequent selection of a suitable nonlinear artificial neural network-based multivariate model, is lacking. In view of the above-mentioned conditional requirements, this paper attempts to provide a systematic methodology to develop a multivariate linear regression model, hypothesis testing for the influence of nonlinear terms to linear model, and accordingly selection of a suitable artificial neural network-based inferential model with improved prediction accuracy and control of grinding behaviour. The methodology suggests the use of various statistical techniques, such as Q–Q (quantile–quantile) plotting, data transformation, data standardization, outlier detection test, model adequacy test, model cross-validation and generalization. The suitability of the recommended methodology is illustrated with the help of an engine cylinder liner grinding (honing) case example, in a leading automotive manufacturing unit in India.",industry
10.1016/j.energy.2008.03.004,Journal,Energy,scopus,2008-01-01,sciencedirect,A new approach to ensure successful implementation of sustainable demand side management (DSM) in South African mines,https://api.elsevier.com/content/abstract/scopus_id/45849141587,"Demand side management (DSM) is seen as a short-term solution to the imminent problem of electricity supply shortages in South Africa. DSM aims to reduce peak loads with immediate results in a short time. The mining sector in South Africa is a large energy user with pumping one of the largest consuming systems. Therefore, DSM potential (load shift) should be investigated on these pumping systems.
                  For sustainable load shift, a system is required that simulates, optimises and controls the actual on-site situation. As no such equipment that performs all these processes could be found for deep South African mines, it was developed by HVAC International (Pty) Ltd. It is called the Real-time Energy Management System (REMS). With this system, maximum results can be obtained on a sustainable basis.
                  In this study, four similar DSM projects were investigated. These are described as case studies at gold mines in the Free State Province. For each of these studies a different new innovation was implemented. The innovations described include the adaptation of REMS to handle multi-level intricate pump systems, mines without any instrumentation and control infrastructure, as well as Three Chamber Piped Feeder Systems (3CPFSs).",industry
10.1016/j.asoc.2007.02.015,Journal,Applied Soft Computing Journal,scopus,2008-01-01,sciencedirect,Dynamic data mining technique for rules extraction in a process of battery charging,https://api.elsevier.com/content/abstract/scopus_id/40649097043,"Battery charging controllers design and application is a growing industry direction. Fast and efficient charging of battery packs is a problem which is difficult and often expensive to solve using conventional techniques. The majority of existing works on intelligent charging systems are based on expert knowledge and heuristics. Not all features of the desired charging behavior can be attained by the hard-wired logic implemented by expert generated rules. Because the battery charging is a highly dynamic process and the chemical technology a battery uses varies significantly for different battery types, data mining technique can be of real importance for extracting the charging rules from the large databases, especially when the charging logic is to be continuously changed during the life of the battery dependent on the type and characteristics of the battery and utilization conditions. In this paper we use soft computing-based data mining technique for extraction of control rules for effective and fast battery charging process. The obtained rules were used for NiCd battery charging. The comparative performance evaluation was done among the existing charging control methods and the proposed system, which demonstrated a significant increase of performance (minimum charging time and minimum overheating) using the soft computing-based approach.",industry
10.1016/j.epsr.2006.12.005,Journal,Electric Power Systems Research,scopus,2008-01-01,sciencedirect,ATC enhancement using TCSC via artificial intelligent techniques,https://api.elsevier.com/content/abstract/scopus_id/35148890919,Procurement of optimum available transfer capability (ATC) in the restructured electricity industry is a crucial challenge with regards to open access to transmission network. This paper presents an approach to determine the optimum location and optimum capacity of TCSC in order to improve ATC as well as voltage profile. Real genetic algorithm (RGA) associated with analytical hierarchy process (AHP) and fuzzy sets are implemented as a hybrid heuristic technique in this paper to optimize such a complicated problem. The effectiveness of the proposed methodology is examined through different case studies.,industry
10.1016/j.simpat.2007.04.011,Journal,Simulation Modelling Practice and Theory,scopus,2007-09-01,sciencedirect,Grey-box modeling of a motorcycle shock absorber for virtual prototyping applications,https://api.elsevier.com/content/abstract/scopus_id/34547773158,"There is an increasing use of virtual prototyping tools in the motorcycle industry, aimed at reducing the development time of new models and speeding up performance optimization, by providing the designer with an in-laboratory virtual test track. Virtual prototyping software are multibody simulation software, which require the availability of models of all the vehicle components. The choice of the model is then of paramount importance, since it heavily affects the accuracy and reliability of the simulation results. Conventional models (like linear models) are often inadequate to describe the behavior of complex nonlinear components, so that it is necessary to appeal to different modeling approaches. This is actually the case when dealing with motorcycle suspension systems, given that their most critical part, the shock absorber, exhibits nonlinear and time-variant behavior.
                  In this paper, a grey-box model of a racing motorcycle mono-tube shock absorber is proposed, which consists of a nonlinear parametric model and a black-box, neural-network-based model. The absorber model has been implemented in a numerical simulation environment, and validated against experimental test data. The results of the validation show that the model is able to reproduce the real behavior of the shock absorber with an accuracy that matches or even beats that of other models previously presented in the literature. The interfacing of the proposed model to the ADAMS virtual prototyping environment is also discussed.",industry
10.1016/j.cor.2005.05.019,Journal,Computers and Operations Research,scopus,2007-04-01,sciencedirect,Using mega-trend-diffusion and artificial samples in small data set learning for early flexible manufacturing system scheduling knowledge,https://api.elsevier.com/content/abstract/scopus_id/33748743279,"Neural networks are widely utilized to extract management knowledge from acquired data, but having enough real data is not always possible. In the early stages of dynamic flexible manufacturing system (FMS) environments, only a litter data is obtained, and this means that the scheduling knowledge is often unreliable. The purpose of this research is to utilize data expansion techniques for an obtained small data set to improve the accuracy of machine learning for FMS scheduling. This research proposes a mega-trend-diffusion technique to estimate the domain range of a small data set and produce artificial samples for training the modified backpropagation neural network (BPNN). The tool used is the Pythia software. The results of the FMS simulation model indicate that learning accuracy can be significantly improved when the proposed method is applied to a very small data set.",industry
10.1016/j.conengprac.2006.06.005,Journal,Control Engineering Practice,scopus,2007-02-01,sciencedirect,Prioritised A* search in real-time elevator dispatching,https://api.elsevier.com/content/abstract/scopus_id/33750177574,"Under the typical operating conditions of an elevator system, there is insufficient time to consider all dispatching alternatives and the major elevator companies normally adopt empirical techniques to reduce complexity and achieve acceptable performance. The current work has been able to demonstrate that in practical circumstances an optimal solution to the real-time elevator-dispatching problem can be obtained. The elevator dispatching problem is formulated as a heuristic search and is implemented using a novel extension of the popular A* search, termed prioritised A*, that retains the desirable admissibility and monotonicity of A*. PA* makes best use of the limited time available by ensuring the dispatcher considers the most important aspect of the problem first, namely to give each elevator its first assignment. In a manufacturing process, this is equivalent to ensuring that each machine is immediately given its first job, while the determination of the detailed order of the remaining jobs is refined later. This study has obtained access to extensive data records collected from installed elevator systems and their analysis has led to the identification of new passenger models able to deliver suitable high quality predictive data to improve the operations of the dispatcher.",industry
10.1016/B978-044452206-1/50034-3,Book,Parallel Computational Fluid Dynamics 2005,scopus,2006-12-01,sciencedirect,Numerical simulation of transonic flows by a double loop flexible evolution,https://api.elsevier.com/content/abstract/scopus_id/78651562872,"The chapter presents a methodology that uses evolutionary algorithms to obtain fluid velocities for potential flows inside a nozzle. A genetic algorithm with real encoding which, with the technique of partial sampling of the solution nodes, presents a number of advantages over other evolutionary algorithms is proposed in the chapter. These include the avoidance of a rigid connectivity to discretize the domain, thus making it a meshless method. The chapter also proposes a double-loop strategy with EAs which enables improvement of the methodology. The amount of computer storage is low and convergence behavior is good as the qualitative characteristics of the solution are taken into account in the algorithm. The artificial intelligence based on evolutionary algorithms can bring together lessons learned by developers and decision tools into advanced software developments, which can facilitate their use to solve complex challenging problems in industry.",industry
10.1016/j.compchemeng.2006.05.005,Journal,Computers and Chemical Engineering,scopus,2006-11-01,sciencedirect,An integrated framework for on-line supervised optimization,https://api.elsevier.com/content/abstract/scopus_id/33750367584,We present a finite element numerical study of heat transfer in lid driven channels with fully developed axial flow for non-Newtonian power law fluids. The effect of channel aspect ratio and material properties on temperature distribution and wall heat transfer are studied. The results show that in comparison with Newtonian fluids the shear thinning property of the fluids acts to reduce the local viscous dissipative heating and as a result the axial local fluid temperature is reduced. Applications of the results to scraped-surface heat exchanger design and operation are recommended.,industry
10.1016/j.ijmachtools.2005.10.002,Journal,International Journal of Machine Tools and Manufacture,scopus,2006-10-01,sciencedirect,Integrated machining error compensation method using OMM data and modified PNN algorithm,https://api.elsevier.com/content/abstract/scopus_id/33746476938,"This paper presents an integrated machining error compensation method based on polynomial neural network (PNN) approach and inspection database of on-machine-measurement (OMM) system. To improve the accuracy of the OMM system, geometric errors of the CNC machining center and probing errors are compensated. Machining error distributions of a specimen workpiece are measured to obtain error compensation parameters. To efficiently analyze the machining errors, two machining error parameters, Werr
                      and Derr
                     , are defined. Subsequently, these parameters can be modeled using the PNN approach, which is used to determine machining errors for the considered cutting conditions. Consequently, by using an iterative algorithm, tool path can be corrected to effectively reduce machining errors in the end-milling process. Required programs are developed using Ch language, and modified termination method are applied to reduce computation times. Experiments are carried out to validate the approaches proposed in this paper. The proposed integrated machining error compensation method can be effectively implemented in a real machining situation, producing much fewer errors.",industry
10.1016/j.compind.2006.02.011,Journal,Computers in Industry,scopus,2006-08-01,sciencedirect,SIMAP: Intelligent System for Predictive Maintenance. Application to the health condition monitoring of a windturbine gearbox,https://api.elsevier.com/content/abstract/scopus_id/33746237617,"SIMAP is the abbreviated name for the Intelligent System for Predictive Maintenance. It is a software application addressed to the diagnosis in real-time of industrial processes. It takes into account the information coming in real-time from different sensors and other information sources and tries to detect possible anomalies in the normal behaviour expected of the industrial components. The incipient detection of anomalies allows for an early diagnosis and the possibility to plan effective maintenance actions. Also, the continuous monitoring performed allows for an estimation in a qualitative form of the health condition of the components. SIMAP is a general tool oriented to the diagnosis and maintenance of industrial processes, however the first experience of its application has been at a windfarm. In this real case, SIMAP is able to optimize and to dynamically adapt a maintenance calendar for a monitored windturbine according to the real needs and operating life of it as well as other technical and economical criteria. In particular this paper presents the application of SIMAP to the health condition monitoring of a windturbine gearbox as an example of its capabilities and main features.",industry
10.1016/j.snb.2005.12.065,Journal,"Sensors and Actuators, B: Chemical",scopus,2006-07-28,sciencedirect,A novel chemical detector using cermet sensors and pattern recognition methods for toxic industrial chemicals,https://api.elsevier.com/content/abstract/scopus_id/33646858326,"The development and evaluation of a novel gas sensing system intended for use on US Navy vessels is presented. The purpose of this sensor is to provide real-time detection and quantification of a wide range of known toxic chemicals including toxic industrial chemicals (TICs) and combustible or corrosive gases. The sensor system incorporates an array of ceramic-metal (cermet) gas microsensors with multivariate pattern recognition techniques and represents a rugged, light-weight, and low-cost solution to analysis problems that would otherwise need to be addressed with multiple conventional electrochemical sensors. The smart microsensor arrays are being developed by combining cermet electrochemical sensors utilizing cyclic voltammetry, with intelligent firmware and software to drive the sensors and analyze the data. The chemical microsensor architecture may be modified for detection selectivity of a variety of chemical species. The microsensor arrays have potential application for monitoring hazardous chemicals in the part-per-million to part-per-billion range in a variety of internal and external environments. The arrays sense analytes using pattern recognition techniques to determine the presence of vapors of interest. A test demonstrator has been developed with a four-sensor array, readout electronics, and system control software. The four-sensor array was exposed to 15 test vapors. The 15-analyte sources, including two blood agents, 10 TICs and three simulants were generated at five different concentrations in humid air. The cermet sensor array provided unique responses for the various analytes tested. Similar analyte types produced similar results. The sensitivity is sufficient to detect all the analytes at their respective exposure limits. A variety of feature selection and pattern recognition methods are being investigated for robust detection.",industry
10.1016/j.simpat.2005.10.012,Journal,Simulation Modelling Practice and Theory,scopus,2006-07-01,sciencedirect,A virtual shop modeling system for industrial fabrication shops,https://api.elsevier.com/content/abstract/scopus_id/33744546468,"Industrial fabrication is both a production system with a high product mix and a project-based industry. The accuracy of short-term project-based planning, such as estimating and project scheduling, is extremely important for a project’s success. This paper proposes an integrated modeling system that explicitly models the product mix in order to improve the planning accuracy. Automated process planning and a processing time estimation method were implemented through the integration of simulation with such modeling methods as computer aided process planning and artificial neural networks. Two case studies are presented to demonstrate the capability of the proposed system.",industry
10.1016/j.jmatprotec.2005.04.041,Journal,Journal of Materials Processing Technology,scopus,2006-06-01,sciencedirect,An intelligent system for monitoring and optimization of ball-end milling process,https://api.elsevier.com/content/abstract/scopus_id/33646812230,"The paper presents an intelligent system for on-line monitoring and optimization of the cutting process on the model of the ball-end milling. An intelligent system for monitoring and optimization in ball-end milling is developed both in hardware and software. It is based on a PC, which is connected to the CNC main processor module through a serial-port so that control and communication can be realised. The monitoring system is based on LabVIEW software, the data acquisition system and the measuring devices (sensors) for the cutting force measuring. The system collects the variables of the cutting process by means of sensors. The measured values are delivered to the computer program through the data acquisition system for data processing and analysis. The optimization technique is based on genetic algorithms for the determination of the cutting conditions in machining operations. In metal cutting processes, cutting conditions have an influence on reducing the production cost and time and deciding the quality of a final product. Experimental results show that the proposed genetic algorithm-based procedure for solving the optimization problem is effective and efficient, and can be integrated into a real-time intelligent manufacturing system for solving complex machining optimization problems.",industry
10.1016/j.fm.2005.05.003,Journal,Food Microbiology,scopus,2006-05-01,sciencedirect,"Development of a real-time PCR assay targeting the sporulation gene, spo0A, for the enumeration of thermophilic bacilli in milk powder",https://api.elsevier.com/content/abstract/scopus_id/26044470853,"Thermophilic bacilli, such as Anoxybacillus, Geobacillus and Bacillus, are common contaminants growing within the processing lines of milk powder producing factories. These contaminants are used as indicator organisms for plant hygiene and specification limits based on their numbers have been implemented to ensure milk powder quality. In this study, we present a SYBR Green-based real-time PCR assay for the rapid detection and enumeration of these thermophilic bacilli in milk powder using the spo0A sporulation gene as quantification target. With this method the detection of thermophilic bacilli in milk powder can be accomplished within 1h. The detection limit for reconstituted and inoculated milk was 80vegetative cfuml−1 and 640sporesml−1, respectively.",industry
10.1016/j.engappai.2004.12.003,Journal,Engineering Applications of Artificial Intelligence,scopus,2005-08-01,sciencedirect,NEFCLASS-based neuro fuzzy controller for SRM drive,https://api.elsevier.com/content/abstract/scopus_id/18144374571,"Switched reluctance motor (SRM) is increasingly employed in industrial applications where variable speed is required because of their simple construction, ease of maintenance, low cost and high efficiency. However, the SRM performance often degrades for the machine parameter variations. The SRM converter is difficult to control due to its nonlinearities and parameter uncertainties. In this paper, to overcome this problem, a neuro fuzzy controller (NFC) is proposed. Heuristic rules are derived with the membership functions of the fuzzy variables tuned by a neural network (NN). The algorithm is implemented on a digital signal processor (TMS320F240) allowing great flexibility for various real time applications. Experimental results demonstrate the effectiveness of the NFC with various working conditions of the SRM.",industry
10.1016/j.ins.2004.09.011,Journal,Information Sciences,scopus,2005-05-13,sciencedirect,On the design of intelligent robotic agents for assembly,https://api.elsevier.com/content/abstract/scopus_id/16344396133,"Robotic agents can greatly be benefited from the integration of perceptual learning in order to monitor and adapt to changing environments. To be effective in complex unstructured environments, robots have to perceive the environment and adapt accordingly. In this paper it is discussed a biology inspired approach based on the adaptive resonance theory (ART) and implemented on an KUKA KR15 industrial robot during real-world operations (e.g. assembly operations). The approach intends to embed naturally the skill learning capability during manufacturing operations (i.e., within a flexible manufacturing system).
                  The integration of machine vision and force sensing has been useful to demonstrate the usefulness of the cognitive architecture to acquire knowledge and to effectively use it to improve its behaviour. Practical results are presented, showing that the robot is able to recognise a given component and to carry out the assembly. Adaptability is validated by using different component geometry during assemblies and also through skill learning which is shown by the robot’s dexterity.",industry
10.1016/j.asoc.2004.07.001,Journal,Applied Soft Computing Journal,scopus,2005-01-01,sciencedirect,Application of internal model control methods to industrial combustion,https://api.elsevier.com/content/abstract/scopus_id/10844285344,"Most practical systems are inherently non-linear to some extent in their behaviour and for their cost effective, smooth and safe operation, optimised control systems based on the non-linear models are required. To this end many useful techniques such as the stochastic modelling, sliding mode control and adaptive identification and control have been proposed in the literature. However, the high cost of implementation, the inability to capture imprecision with the required level of tolerance, and the in-flexibility against distortions in the operating variables, make them less attractive. To this end new artificial intelligence based techniques such as fuzzy logic, neural networks and probabilistic reasoning, are becoming more and more popular. Among these techniques neural networks have an edge over the others, mainly because of their ability to process large amount of available data, subsequent to the development of some interpretable models for solving engineering problems. Moreover, the ability to capture the non-linearities of a real system accurately and the versatility in being able to accommodate with ease, the various conventional and advanced strategies within their structures, make them much more attractive. The problem becomes more computationally worse and uncontrollable when inverse of the system does not exist. This problem is resolved when neural network based techniques such as internal model control (IMC) are applied to the real systems.
                  This paper outlines the application of neural networks based IMC methods for estimation/control of important input and output variables of a 0.5MW laboratory scale industrial furnace. The application involves inputs such as the airflow rate, swirl number and momentum ratio. The outputs include emission levels of oxides of nitrogen especially nitric oxide. The response to step and staircase inputs has been analysed. The results have been compared with standard linear quadratic controller. The control output of the IMC methods has resulted in almost similar steady state error performance to the linear quadratic regulator. Although the development process of the IMC method might take longer time because of the training and data arrangement but has the capability of readjustment after being developed.",industry
10.1016/j.compedu.2003.12.001,Journal,Computers and Education,scopus,2004-12-01,sciencedirect,Virtual reality simulations and animations in a web-based interactive manufacturing engineering module,https://api.elsevier.com/content/abstract/scopus_id/3042807121,"This paper presents a web-based interactive teaching package that provides a comprehensive and conducive yet dynamic and interactive environment for a module on automated machine tools in the Manufacturing Division at the National University of Singapore. The use of Internet technologies in this teaching tool makes it possible to conjure visualisations that cannot be achieved using traditional teaching materials such as transparencies. Virtual reality simulations and animations were developed and appropriately placed in the teaching materials to enhance the student understanding of complex concepts. This is especially useful in teaching automated machine tools, which deals primarily with the numerical control (NC) of the motions of automated machine tools. These virtual reality simulations and animations provide the capability of training students in NC programming and operations without the need to work on actual NC machines in the laboratory. The simulations are suitably placed in the package to engage the students and enhance their concentration, while at the same time generate interactions. Customised question types were also designed and implemented with a tutorial monitoring application.",industry
10.1016/j.envsoft.2003.10.003,Journal,Environmental Modelling and Software,scopus,2004-08-01,sciencedirect,Modelling SO<inf>2</inf> concentration at a point with statistical approaches,https://api.elsevier.com/content/abstract/scopus_id/3342982389,"In this paper, the results obtained by inter-comparing several statistical techniques for modelling SO2 concentration at a point such as neural networks, fuzzy logic, generalised additive techniques and other recently proposed statistical approaches are reported. The results of the inter-comparison are the fruits of collaboration between some of the partners of the APPETISE project funded under the Framework V Information Societies and Technologies (IST) programme. Two different cases for study were selected: the Siracusa industrial area, in Italy, where the pollution is dominated by industrial emissions and the Belfast urban area, in the UK, where domestic heating makes an important contribution. The different kinds of pollution (industrial/urban) and different locations of the areas considered make the results more general and interesting. In order to make the inter-comparison more objective, all the modellers considered the same datasets. Missing data in the original time series was filled by using appropriate techniques. The inter-comparison work was carried out on a rigorous basis according to the performance indices recommended by the European Topic Centre on Air and Climate Change (ETC/ACC). The targets for the implemented prediction models were defined according to the EC normative relating to limit values for sulphur dioxide. According to this normative, three different kinds of targets were considered namely daily mean values, daily maximum values and hourly mean values. The inter-compared models were tested on real cases of poor air quality. In the paper, the inter-compared techniques are ranked in terms of their capability to predict critical episodes. A ranking in terms of their predictability of the three different targets considered is also proposed. Several key issues are illustrated and discussed such as the role of input variable selection, the use of meteorological data, and the use of interpolated time series. Moreover, a novel approach referred to as the technique of balancing the training pattern set, which was successfully applied to improve the capability of ANN models to predict exceedences is introduced. The results show that there is no single modelling approach, which generates optimum results in terms of the full range of performance indices considered. In view of the implementation of a warning system for air quality control, approaches that are able to work better in the prediction of critical episodes must be preferred. Therefore, the artificial neural network prediction models can be recommended for this purpose. The best forecasts were achieved for daily averages of SO2 while daily maximum and hourly mean values are difficult to predict with acceptable accuracy.",industry
10.1016/j.engappai.2004.03.001,Journal,Engineering Applications of Artificial Intelligence,scopus,2004-04-01,sciencedirect,Artificial neural networks and neuro-fuzzy systems for modelling and controlling real systems: A comparative study,https://api.elsevier.com/content/abstract/scopus_id/2942564462,"This article presents a comparison of artificial neural networks and neuro-fuzzy systems applied for modelling and controlling a real system. The main objective is to model and control the temperature inside of a kiln for the ceramic industry. The details of all system components are described. The steps taken to arrive at the direct and inverse models using the two architectures: adaptive neuro fuzzy inference system and feedforward neural networks are described and compared. Finally, real-time control results using internal model control strategy are presented.
                  Using available Matlab software for both algorithms, the objective is to show the implementation steps for modelling and controlling a real system. Finally, the performances of the two solutions were compared through different parameters for a specific real didactic case.",industry
10.1016/j.engappai.2003.11.008,Journal,Engineering Applications of Artificial Intelligence,scopus,2004-02-01,sciencedirect,Incorporating fuzzy approaches for production planning in complex industrial environments: The roll shop case,https://api.elsevier.com/content/abstract/scopus_id/17544390830,"Operation of complex shops needs specific, sophisticated procedures in order to guarantee competitive plant performance. In this paper we present a hierarchy of models for roll shop departments in the steel industry, focusing on the calculation of the priority of the rolls to produce. A fuzzy-based model was developed and implemented in a real environment, allowing the simulation of expert behaviour, considering the characteristics of an environment with imprecise information. A description of the model and implementation experiences in a real shop are reported.",industry
10.1016/j.compind.2003.06.002,Journal,Computers in Industry,scopus,2004-02-01,sciencedirect,Web-based search system of pattern recognition for the pattern of industrial component by an innovative technology,https://api.elsevier.com/content/abstract/scopus_id/0347593788,"The real-time system uses a recurrent neural network (RNN) with associative memory for training and recognition. This study attempts to use associative memory to apply pattern recognition (PR) technology to the real-time pattern recognition of engineering components in a web-based recognition system with a client–server network structure. Remote engineers can draw the shape of the engineering components using the browser, and the recognition system then searches the component database via the Internet. Component patterns are stored in the database system considered here. Moreover, the data fields of each component pattern contain the properties and specifications of that pattern, except in the case of engineering components. The database system approach significantly improves recognition system capacity. The recognition system examined here employs parallel computing, which increases system recognition rate. The recognition system used in this work is an Internet based client–server network structure. The final phase of the system recognition applies database matching technology to processing recognition, and can solve the problem of spurious states. The system considered here is implemented on the Yang-Fen Automation Electrical Engineering Company as a case study. The experiment is continued for 4 months, and engineers are also used to operating the web-based pattern recognition system. Therefore, the cooperative plan described above is analyzed and discussed here. Finally, these papers propose the tradition methods compare with the innovative methods.",industry
10.1016/s1474-6670(17)31023-6,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2004-01-01,sciencedirect,Real-Time requirements in diagnostic systems,https://api.elsevier.com/content/abstract/scopus_id/85064459122,"While artificial intelligence methodologies are being applied towards increasingly realistic domains that require timely responses, real-time systems are coming to incorporate decision-making tools that require more intelligent capabilities. This paper’ describes a distributed multi agent architecture considering industrial demands for realtime diagnostic and decision support systems. The correctness of the behavior of such a system depends on the results of the computation as well as on the time at which the results can be provided. Especially in distributed systems, a proper handling of real-time requirements together with a deterministic behavior of all parts is extremely important to guarantee reliable and accurate data processing.",industry
10.1016/j.cie.2004.05.005,Journal,Computers and Industrial Engineering,scopus,2004-01-01,sciencedirect,Application of neural networks to heuristic scheduling algorithms,https://api.elsevier.com/content/abstract/scopus_id/3242706865,"This paper considers the use of artificial neural networks (ANNs) to model six different heuristic algorithms applied to the n job, m machine real flowshop scheduling problem with the objective of minimizing makespan. The objective is to obtain six ANN models to be used for the prediction of the completion times for each job processed on each machine and to introduce the fuzziness of scheduling information into flowshop scheduling. Fuzzy membership functions are generated for completion, job waiting and machine idle times. Different methods are proposed to obtain the fuzzy parameters. To model the functional relation between the input and output variables, multilayered feedforward networks (MFNs) trained with error backpropagation learning rule are used. The trained network is able to apply the learnt relationship to new problems. In this paper, an implementation alternative to the existing heuristic algorithms is provided. Once the network is trained adequately, it can provide an outcome (solution) faster than conventional iterative methods by its generalizing property. The results obtained from the study can be extended to solve the scheduling problems in the area of manufacturing.",industry
10.1016/j.cie.2004.05.002,Journal,Computers and Industrial Engineering,scopus,2004-01-01,sciencedirect,Functional evaluation of an event detection ensemble to detect anomalous system behavior,https://api.elsevier.com/content/abstract/scopus_id/3242702363,"Surveillance systems are established to manage the complexity of ensuring processes of interest behave as expected. They are formed in response to public demand that systems—natural and human-made—be predictable, managed, and under control. This paper examines the functionality of an event detection ensemble used to detect anomalous conditions a system of interest may exhibit. The event detection ensemble consists of an agent and its associated sensors, models, and detectors. In addition, the subsystem is presented in the broader context of a generalized surveillance design framework. A logical organization of components is presented as well as a demonstration implementation scheme. The event detection ensemble is evaluated for functionality using two hypothetical test cases representing real-world applications. Event signatures based upon prediction, extrapolation, and domain discrepancies are characterized and simulated. In addition, a radial basis neural network is employed to create a model capable of distinguishing these types of discrepancies. Finally, application and benefits of this design approach is discussed with respect to designing surveillance systems for human-made and natural systems. Discussion includes how this design framework can be applied to detect events in a wide array of industrial engineering applications.",industry
10.1016/S0895-7177(03)00120-1,Journal,Mathematical and Computer Modelling,scopus,2003-05-15,sciencedirect,The application of intelligent agent technology to simulation,https://api.elsevier.com/content/abstract/scopus_id/0038686924,"We have been successfully applying intelligent agent technology to several real world simulation problems such as semiconductor manufacturing processes, train operations, and electric motor assembly lines.
                  Agent technology is a methodology to realize an autonomous decentralized system with cooperative interactions among agents that model each element of the system. An intelligent agent has problem solving and learning skills, as well as the knowledge for that purpose. An agent is characterized as having autonomy, sociality, reactivity, and proactiveness.
                  In the development of a software system using agent technology, each element of the system is described in an independent and modular program code. This method makes addition, change, and deletion of an element much easier than the case of conventional programming. In almost any real world system, each element works mostly in an independent and parallel manner yet with interactions with each other, and matches well with the concept of an agent.
                  We have chosen a platform called the PIM, parallel inference machine, that describes and executes multiple agents in the independent and parallel manner. It makes the simulation application software development for a real world system much more straightforward than conventional computing platforms.
                  This paper describes the agent technology, its application to real world simulation and the platform “PIM”. It describes in some detail actual simulator application examples: semiconductor process system management, train operation management, and electric motor assembly line change.",industry
10.1016/S0305-0548(02)00044-8,Journal,Computers and Operations Research,scopus,2003-05-01,sciencedirect,Using MLP networks to design a production scheduling system,https://api.elsevier.com/content/abstract/scopus_id/0037405170,"This paper investigates the application of artificial neural networks to the problem of job shop scheduling with a scope of a deterministic time-varying demand pattern over a fixed planning horizon. The purpose of the research is to design and develop a job shop scheduling system (a scheduling software) that can generate effective job shop schedules using the multi-layered perceptron (MLP) networks. The contributions of this study include designing, developing, and implementing a production activity scheduling system using the MLP networks; developing a method for organizing sample data using a denotation bit to indicate processing sequence and processing time of a job simultaneously; using the back-propagation training process to control local minimal solutions; and developing a heuristics to improve and revise the initial production schedule. The proposed production activity schedule system is tested in a real production environment and illustrated in the paper with a sample case.",industry
10.1016/B978-075067495-9/50008-2,Book,Techniques for Adaptive Control,scopus,2003-01-01,sciencedirect,"Knowledgescape, an objectoriented real-time adaptive modeling and optimization expert control system for the process industries",https://api.elsevier.com/content/abstract/scopus_id/84904029450,"This chapter discusses KnowledgeScape, an object-oriented real-time expert control system for the process industries that have built-in adaptive modeling and optimization capabilities. The primary use of KnowledgeScape is the online continuous monitoring of plant performance and the calculation of new process setpoints, which maintain and optimize performance as feed and operating conditions vary. This model-based optimization ability adds true artificial intelligence to KnowledgeScape in that it creates the basis for learning about the process. It also discusses about the intelligent software objects, artificial intelligence tools, and process control and their use in KnowledgeScape. This chapter intents to present how neural networks are used within KnowledgeScape to provide learning and predictive capabilities; the implementation of genetic algorithms within Knowledgescape; and the Crisp Rules; Fuzzy Rules relating to KnowledgeScape. The sequence of steps needed to successfully configure and implement a KnowledgeScape system is summarized. This chapter concludes that a careful reflection of the descriptions of functions of KnowledgeScape suggests that its design can be used in a much broader sense. Specifically, KnowledgeScape can be used to embed intelligence in any process.",industry
10.1016/S1077-2014(03)00035-4,Journal,Real-Time Imaging,scopus,2003-01-01,sciencedirect,SVM approximation for real-time image segmentation by using an improved hyperrectangles-based method,https://api.elsevier.com/content/abstract/scopus_id/0142123423,"A real-time implementation of an approximation of the support vector machine (SVM) decision rule is proposed. This method is based on an improvement of a supervised classification method using hyperrectangles, which is useful for real-time image segmentation. The final decision combines the accuracy of the SVM learning algorithm and the speed of a hyperrectangles-based method. We review the principles of the classification methods and we evaluate the hardware implementation cost of each method. We present the combination algorithm, which consists of rejecting ambiguities in the learning set using SVM decision, before using the learning step of the hyperrectangles-based method. We present results obtained using Gaussian distribution and give an example of image segmentation from an industrial inspection problem. The results are evaluated regarding hardware cost as well as classification performances.",industry
10.1016/S0957-4158(03)00042-4,Journal,Mechatronics,scopus,2003-01-01,sciencedirect,Mechatronic design,https://api.elsevier.com/content/abstract/scopus_id/0041509193,"Mechatronic design is the integrated design of a mechanical system and its embedded control system. In order to make proper choices early in the design stage, tools are required that support modelling and simulation of physical systems––together with the controllers––with parameters that are directly related to the real-world system. Such software tools are becoming available now. Components in various physical domains (e.g. mechanical or electrical) can easily be selected from a library and combined into a ‘process’ that can be controlled by block-diagram-based (digital) controllers. A few examples will be discussed that show the use of such a tool in various stages of the design. The examples include a typical mechatronic system with a flexible transmission, a mobile robot, and an industrial linear motor with a neural-network-based learning feed-forward controller that compensates for cogging.",industry
10.1016/S0360-8352(03)00039-1,Journal,Computers and Industrial Engineering,scopus,2003-01-01,sciencedirect,A fuzzy neural network approach to machine condition monitoring,https://api.elsevier.com/content/abstract/scopus_id/0038789161,"This paper is focused on the implementation of a predictive neural network for use as an operator's aid in the diagnosis of faults with high prediction accuracy in an automated manufacturing environment In order to evaluate the performance of the model, the network has been tested using both simulated time series and real time machine vibration data collected in lab experiments.",industry
10.1016/s0141-9331(02)00069-8,Journal,Microprocessors and Microsystems,scopus,2002-12-20,sciencedirect,Real-time implementation of a dynamic fuzzy neural networks controller for a SCARA,https://api.elsevier.com/content/abstract/scopus_id/0037147579,"This paper presents the design, development and implementation of a Dynamic Fuzzy Neural Networks (D-FNNs) Controller suitable for real-time industrial applications. The unique feature of the D-FNNs controller is that it has dynamic self-organising structure, fast learning speed, good generalisation and flexibility in learning. The approach of rapid prototyping is employed to implement the D-FNNs controller with a view of controlling a Selectively Compliance Assembly Robot Arm (SCARA) in real time. Simulink, an iterative software for simulating dynamic systems, is used for modelling, simulation and analysis of the dynamic system. The D-FNNs controller was implemented through Real-Time Workshop (RTW). RTW generates C-codes from the Simulink block diagrams and in turn, the generated codes (object codes) are downloaded to the dSPACE DS1102 floating-point processor, together with the supporting files, for execution. The performance of the D-FNNs controller was found to be superior and it matches favourably with the simulation results.",industry
10.1016/S0924-0136(02)00201-7,Journal,Journal of Materials Processing Technology,scopus,2002-06-20,sciencedirect,AI-based condition monitoring of the drilling process,https://api.elsevier.com/content/abstract/scopus_id/0037142641,"With increasing competitive pressures, manufacturing systems in the automotive industry are being driven more and more aggressively. The pressures imposed on the processes and lack of system ‘slack’ have led to increased use of tool condition monitoring (TCM) systems. In parallel, there has been wide-ranging research in academia. However, a closer examination shows that there has been very little migration of this research into industrial practice. Furthermore, the success of industrially deployed monitoring systems has been poor. It has been suggested that a significant factor behind both these phenomenon has been the ‘difficult’ environment in which such systems must operate; an environment where they are subject to many stochastic influences, ranging from ambient conditions, to user input, to workpiece consistency.
                  Neural networks (NNs) have found increasing favour in manufacturing systems research because of their ability to perform robustly in noisy environments. Almost all the applications of this technology in TCM have been in the detection/prediction of tool wear. From an academic standpoint, it may be speculated that the lack of focus on breakage and missing tool detection has been due to the relatively trivial nature of detecting such anomalies in the laboratory environment. However, detection in the production environment is compromised by a wide range of factors, which can give rise to false alarms when such strategies are transported from laboratory conditions. In this paper, data from a real manufacturing process is used to demonstrate the potential application of NNs to the task of anomaly detection in the production environment.",industry
10.3182/20020721-6-es-1901.01502,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2002-01-01,sciencedirect,Neural network applications for model based fault detection with parity equations,https://api.elsevier.com/content/abstract/scopus_id/84866936261,"The rising complexity of modern automotive engines with an increasing number of actuators and sensors to minimise emissions and fuel consumption and to maximise engine driveability require a detailed supervision for fault detection and on-board diagnosis. The European Community Directive 98/69/EC requires on-board diagnosis for spark ignition engines and will require it for diesel engines as of January 2003, mainly to prevent excessive emissions. Beside this regulation it is also in the interest of the automobile manufactures to establish capable diagnosis systems for maintenance, repair and the benefit of their customers. This paper will describe applications of neural networks for modelling complex fluid- and thermodynamics with unknown physical model structure. Reference models, which describe the fault free process, are set up and identified with the special neural network LOLIMOT (Local-Linear-Model-Tree). Fault detection algorithms, which employ the method of parity equations, were successfully implemented and tested in real time with a 2 litre diesel engine and a Rapid Control Prototyping System. Measurements of online fault detection are shown for several built-in faults in the intake system of this diesel engine.",industry
10.1016/S0160-791X(02)00038-6,Journal,Technology in Society,scopus,2002-01-01,sciencedirect,Data mining techniques for customer relationship management,https://api.elsevier.com/content/abstract/scopus_id/0036863466,"Advancements in technology have made relationship marketing a reality in recent years. Technologies such as data warehousing, data mining, and campaign management software have made customer relationship management a new area where firms can gain a competitive advantage. Particularly through data mining—the extraction of hidden predictive information from large databases—organizations can identify valuable customers, predict future behaviors, and enable firms to make proactive, knowledge-driven decisions. The automated, future-oriented analyses made possible by data mining move beyond the analyses of past events typically provided by history-oriented tools such as decision support systems. Data mining tools answer business questions that in the past were too time-consuming to pursue. Yet, it is the answers to these questions make customer relationship management possible. Various techniques exist among data mining software, each with their own advantages and challenges for different types of applications. A particular dichotomy exists between neural networks and chi-square automated interaction detection (CHAID). While differing approaches abound in the realm of data mining, the use of some type of data mining is necessary to accomplish the goals of today’s customer relationship management philosophy.",industry
10.1016/S1359-835X(01)00013-6,Journal,Composites - Part A: Applied Science and Manufacturing,scopus,2001-12-01,sciencedirect,"Intelligent model-based control of preform permeation in liquid composite molding processes, with online optimization",https://api.elsevier.com/content/abstract/scopus_id/0035546371,"Manufacturing of quality products via liquid molding processes such as Resin Transfer Molding (RTM), calls for a precise control of resin progression through fibrous preforms during mold fill. Lack of an effective process control leads to formation of dry spots and voids that are detrimental to product quality. This study presents the use of physics-based process simulations in real-time, towards a generalized process control. The implementation of process simulations for on-line model-predictive control requires that the simulation time scales be less than the time scales of the process. An artificial neural network trained using data from numerical process models is used to provide rapid, real-time process simulations for the model-based control. A simulated annealing algorithm, working interactively with the neural network process model, is used to derive optimal control decisions rapidly and on-the-fly. The controller performance is systematically demonstrated for several processing scenarios.",industry
10.1016/S0950-7051(01)00151-4,Journal,Knowledge-Based Systems,scopus,2001-11-01,sciencedirect,Specifying fault tolerance in mission critical intelligent systems,https://api.elsevier.com/content/abstract/scopus_id/0035505448,"Real time intelligent systems are being increasingly used in mission critical applications in domains like military, aerospace, process control industry and medicine. Despite this vast potential, the major concern about deploying mission critical intelligent systems is their dependability. Dependability encompasses such notions as reliability, safety, security, maintainability and portability. A major concern about mission critical intelligent systems is their performance in the presence of failures. Intelligent systems are characterized by often non-existent, imprecise or rapidly changing specifications. This makes the task of characterizing an intelligent system's performance in the presence of failures much more difficult. In this paper, we characterize the failures that are likely in a mission critical intelligent system. We propose an extended I/O automata model to capture these failure specifications. We further demonstrate how these specifications can be realized in a real time expert system by structuring the knowledge base. This formalism can also be used to specify the fault tolerant properties of the underlying hardware and software over which the intelligent system resides. Thus we have an unified formalism to specify fault tolerance properties in hardware, system software and the intelligent system. This will enable us to reason about the performance of the entire system inclusive of all its components in an uniform manner.",industry
10.1016/S0952-1976(01)00033-1,Journal,Engineering Applications of Artificial Intelligence,scopus,2001-10-01,sciencedirect,Intelligent control of a rotary kiln fired with producer gas generated from biomass,https://api.elsevier.com/content/abstract/scopus_id/0035494049,"During the past decade, the academic world has been extremely active in developing new algorithms and theories in the field of artificial intelligence (AI) and intelligent systems. In most cases, however, emphasis has been placed more on theoretical frameworks and mathematical bases than on what the individual AI techniques could offer and on how different techniques could be applied to solve real industrial-scale problems. The reputation of intelligent systems has consequently suffered from an inability to transfer new and sophisticated techniques to industrial applications with identifiable benefits. As a result, although a wide range of intelligent control techniques has been available already for many years, most of the applications in the process industry are based on more conventional techniques. Recently, as awareness of intelligent systems has grown, industrial problems and implementations have fortunately received increasing attention. In this paper, an intelligent supervisory-level system implemented at one of the major Finnish pulp mills to control a lime kiln fired with producer gas generated from biomass is presented. First, the major results of a field study are summarised, with special attention paid to burnt lime quality aspects. Next, a novel linguistic equations approach, which provides flexible methods for both modelling and control, is briefly described. The overall structure and main functions of the developed control system are then described with the main emphasis on the control of temperature and lime quality. Finally, the results obtained during the extended testing period of the system are presented and discussed.",industry
10.1016/S0141-9331(01)00104-1,Journal,Microprocessors and Microsystems,scopus,2001-04-20,sciencedirect,QoS management in programmable networks through mobile agents,https://api.elsevier.com/content/abstract/scopus_id/0035917663,"In delivering multimedia services, quality of service represents a crucial commitment to be satisfied. Very often it has been considered only from a theoretical point of view, leaving any implementation details out of the discussion, mainly for the lack of concrete possibilities to execute its control and management effectively. Recent technological developments in the networking and distributed programming fields are now opening new challenging scenarios towards the negotiation and guarantee of QoS in the delivery of multimedia services through the network. Active or programmable networks are becoming a reality, and the migration of software components among network nodes seems to be the direction pursued by most of network manufactures. Mobile software agents represent a very attractive approach to the distributed control of computer networks and a valid alternative to the implementation of strategies for the management of QoS. In this paper, we present our approach to QoS management through mobile agents. The potentiality of this approach is shown through two application examples. The first one focuses on resource reservation through RSVP in an int-serv scenario, while the second one shows how to provide QoS to aggregated traffic flowing through a virtual network.",industry
10.1016/S0167-7799(00)01528-6,Journal,Trends in Biotechnology,scopus,2001-02-01,sciencedirect,Multivariate statistical monitoring of batch processes: An industrial case study of fermentation supervision,https://api.elsevier.com/content/abstract/scopus_id/0035253422,"This article describes the development of Multivariate Statistical Process Control (MSPC) procedures for monitoring batch processes and demonstrates its application with respect to industrial tylosin biosynthesis. Currently, the main fermentation phase is monitored using univariate statistical process control principles implemented within the G2 real-time expert system package. This development addresses integrating various process stages into a monitoring system and observing interactions among individual variables through the use of multivariate projection methods. The benefits of this approach will be discussed from an industrial perspective.",industry
10.1006/rtim.2001.0231,Journal,Real-Time Imaging,scopus,2001-01-01,sciencedirect,Real-time vision-based system for textile fabric inspection,https://api.elsevier.com/content/abstract/scopus_id/0035678020,"This paper presents an automatic vision-based system for quality control of web textile fabrics. The general hardware and software platform developed to solve this problem is presented and a powerful algorithm for defect inspection is proposed. Based on the improved binary, textural and neural network algorithms the proposed method gives good results in the detection of many types of fabric defects under real industrial conditions, where the presence of many types of noise is an inevitable phenomenon. A high detection rate with good localization accuracy, low rate of false alarms, compatibility with standard inspection tools and low price are the main advantages of the proposed system as well as the overall inspection approach.",industry
10.1016/S0360-8352(00)00078-4,Journal,Computers and Industrial Engineering,scopus,2001-01-01,sciencedirect,"Comparison of constraint logic programming and distributed problem solving: A case study for interactive, efficient and practicable job-shop scheduling",https://api.elsevier.com/content/abstract/scopus_id/0035247584,"The job-shop scheduling issue is more and more described not only in terms of efficiency (e.g. Makespan), but also in terms of interactivity and practicability. The aim of this paper is to evaluate the ability of two approaches inherited from artificial intelligence domain to contribute to the solving of this issue: Constraint Logic Programming (CLP) on the one hand, and, on the other hand, Distributed Problem Solving (DPS) inherited from Distributed Artificial Intelligence (DAI). This analysis is achieved through the use of two specific computerised tools: Constraint Handling In Prolog (CHIP) as a CLP software and the Distributed Production Scheduling System (DPSS) as a distributed problem solving system. These tools are then evaluated in terms of interactivity, efficiency and practicability. Interactivity is discussed according to qualitative points of view such as the ability to provide efficient decision support, a set of alternative solutions and the possibility to parameterise the algorithms. Efficiency is described in terms of optimality or sub-optimality by the analysis of the Makespan criterion vs. fixed computation time. Practicability is associated to the industrial viability of the methods: ability to cope with real industrial case study or ability to face real industrial contexts. Evaluation is then performed through a multiple criteria analysis. This analysis is achieved given an increasing number of operations to perform.
                  The results highlight the high complementary level of these two approaches, allowing us to provide a framework for a joint integration, which shall be optimised when taking into account the assets of each approach according to the three evaluation criteria.",industry
10.1016/S0019-0578(00)00047-1,Journal,ISA Transactions,scopus,2001-01-01,sciencedirect,Winner take-all experts network for sensor validation,https://api.elsevier.com/content/abstract/scopus_id/0035039968,"The validation of sensor measurements has become an integral part of the operation and control of modern industrial equipment. The sensor under harsh environment must be shown to consistently provide the correct measurements. Analysis of the validation hardware or software should trigger an alarm when the sensor signals deviate appreciably from the correct values. Neural network based models can be used to on-line estimate critical sensor values when neighboring sensor measurements are used as inputs. The underlying assumption is that the neighboring sensors share an analytical relationship. The discrepancy between the measured and predicted sensor values may then be used as an indicator for sensor health. The proposed Winner Take All Experts (WTAE) network based on a ‘divide and conquer’ strategy significantly reduces the computational time required to train the neural network. It employs a growing fuzzy clustering algorithm to divide a complicated problem into a series of simpler sub-problems and assigns an expert to each of them locally. After the sensor approximation, the outputs from the estimator and the real sensor readings are compared both in the time domain and the frequency domain. Three fault indicators are used to provide analytical redundancy to detect the sensor failure. In the decision stage, the intersection of three fuzzy sets accomplishes a decision level fusion, which indicates the confidence level of the sensor health. Two data sets, the Spectra Quest Machinery Fault Simulator data set and the Westland vibration data set, were used in simulations to demonstrate the performance of the proposed WTAE network. The simulation results show the proposed WTAE is competitive with or even superior to the existing approaches.",industry
10.1016/S0098-1354(00)00542-1,Journal,Computers and Chemical Engineering,scopus,2000-07-15,sciencedirect,On-line training method of ANN in DMS for side draw quality of refinery fractionator,https://api.elsevier.com/content/abstract/scopus_id/0034660705,"The dynamic on-line monitoring system (DMS) based on on-line training of artificial neural network (ANN) has been successfully implemented in a refinery for real-time, on-line and dynamic estimation of quality indexes of fractionator side draw. To improve the predicted accuracy of DMS and fit the new production cases, a novel ANN training method, on-line training method, has been proposed. It can save a lot of time and effort in contrast to the conventional off-line training for ANN. It also improves the reliability of DMS.",industry
10.1016/S0305-0548(99)00118-5,Journal,Computers and Operations Research,scopus,2000-06-01,sciencedirect,PREFDIS: A multicriteria decision support system for sorting decision problems,https://api.elsevier.com/content/abstract/scopus_id/0034009314,"This paper, following the methodological framework of multicriteria decision aid (MCDA), presents the PREFDIS (PREFerence DIScrimination) multicriteria decision support system to study sorting decision problems. The main characteristic and a major advantage of the system is the incorporation into its model base of four MCDA methods originating from the preference-disaggregation approach, namely the UTADIS method (UTilités Additives DIScriminantes) and three of its variants, referred to as UTADIS I, UTADIS II and UTADIS III. Using these methods, the decision maker (DM) can develop interactively powerful additive utility models to sort a set of alternatives into two or more predefined classes as accurately as possible, based on different sorting techniques. Furthermore, the system provides enriched preference modeling capabilities, including the modeling of non-monotone preferences. The friendly window-based user interface of the system enables the decision maker/user to take full advantage of the capabilities of the system in order to make effective real-time decisions.
               
                  Scope and purpose
                  The sorting problem refers to the assignment of a finite set of alternatives (actions, objects) to predefined ordered classes. Several real-world decision problems are addressed through the sorting approach, including financial decision-making problems, environmental decisions, marketing decisions, and even medical decisions (medical diagnosis). For several decades the sorting (discrimination) among two or more sets of objects has been studied from the multivariate statistical point of view. Recently, the possibilities of new approaches such as expert systems, neural networks, mathematical programming, multicriteria decision aid (MCDA), etc., have been explored, in order to study the sorting problem within a more flexible framework and to develop sorting models with higher discriminating and predicting ability. This paper presents the PREFDIS (PREFerence DISiscrimination) multicriteria decision support system for the study of sorting decision problems. The system incorporating four MCDA sorting methods enables the decision maker to develop interactively, in real time, additive utility models to sort a set of alternatives into two or more predefined classes.",industry
10.1016/S0952-1976(00)00052-X,Journal,Engineering Applications of Artificial Intelligence,scopus,2000-01-01,sciencedirect,Real-time integrated process supervision,https://api.elsevier.com/content/abstract/scopus_id/0034508997,"This paper presents the use of a micro-controller-based integrated process supervision (IPS) system as a real-time platform for investigative work in structuring expert control. Two different control approaches, based on classical and artificial intelligence techniques, were integrated within IPS and serve as practical examples of the structured approach to expert control. The IPS is a refinement of the expert control architecture. It allows the integration of several control techniques in a single generic framework. Specifically, the paper presents the extensive experimental results derived from a micro-controller-based implementation of IPS on the real-time control of a typical industrial heat-exchanger process. The classical approach, based on auto-tuning techniques, was implemented under the IPS framework. Three auto-tuning techniques, namely Ziegler–Nichols tuning, amplitude tuning and phase tuning were incorporated. In addition, neural-network-based control techniques using the modified cerebellar model articulation controller (MCMAC) were also seamlessly incorporated within the IPS scheme. The real-time experimental results using the IPS architecture significantly demonstrated the effectiveness of IPS in handling varying operating conditions. Furthermore, the inclusion of both AI and classical control techniques within a common supervisory framework adequately shows the generality of the architecture.",industry
10.1016/S0957-4158(99)00058-6,Journal,Mechatronics,scopus,2000-01-01,sciencedirect,Analysis and real-time implementation of a radial-basis-function neural-network compensator for high-performance robot manipulators,https://api.elsevier.com/content/abstract/scopus_id/0034135293,"System performance of robot manipulators with nonadaptive controllers might degrade significantly in the presence of structured or unstructured uncertainties. In order to improve the system performance, a novel radial-basis-function (RBF) neural-network (NN) compensator is proposed. With the RBF NN compensator introduced, the system errors and the NN weights with large dispersion in the initial NN weights are guaranteed to be bounded in the Lyapunov sense. The NN weights of the RBF NN compensator are adaptively tuned. Several software-based controllers, including the computed-torque control (CTC) and a few RBF NN schemes, are implemented in an industrial manipulator in real time. Experimental results are obtained to demonstrate the relative effectiveness of the proposed controllers in improving the tracking performance of the robot manipulators associated with structured or unstructured uncertainties.",industry
10.1205/026387600527554,Journal,Chemical Engineering Research and Design,scopus,2000-01-01,sciencedirect,MIMO soft sensors for estimating product quality with on-line correction,https://api.elsevier.com/content/abstract/scopus_id/0033947033,"The main difficulties of on-line quality control are the availability of on-line product quality measurements. Soft-sensing techniques supply attractive and efficient methods to deal with these difficulties. Soft sensors refer to the modelling approaches to estimating hard-to-measure process variables (e.g. quality variables) from other easy-to-measure variables (e.g. temperature, pressure and flowrate measurements). At present, much more research is concerned with multi-input single-output (MISO) systems than with MIMO systems in the field of soft-sensing modelling. In this paper, some MIMO soft-sensing techniques are studied for estimating multiple product quality variables simultaneously in a hydrocracking fractionator. RBF and fuzzy ARTMAP networks are used to build the models and the latter is shown to be more suitable for MIMO soft-sensing modelling. The issues of data pretreatment and on-line correction, which are very important for the industrial implementation of MIMO soft sensors, are discussed in detail. A useful method using a multivariable fuzzy PID (MFPID) on-line correction algorithm is proposed for the MIMO soft sensors enabling them to adapt with the fluctuation of process operating conditions and uncertain system disturbances. The real application results show that the proposed methods are effective for MIMO soft-sensing modelling and have great promise in industrial process applications.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/j.eswa.2021.116203,Journal,Expert Systems with Applications,scopus,2022-03-15,sciencedirect,Semantic segmentation based stereo visual servoing of nonholonomic mobile robot in intelligent manufacturing environment,https://api.elsevier.com/content/abstract/scopus_id/85119331942,"In the interest of developing an intelligent manufacturing environment with an agile, efficient, and optimally utilized transportation system, mobile robots need to achieve a certain level of autonomy as they play an important role in carrying out transportation tasks. Bearing this in mind, in the paper we propose a novel stereo visual servoing method for nonholonomic mobile robot control based on semantic segmentation. Semantic segmentation provides a rich body of information required for an adequate decision-making process in a clustered, dynamic, and ever-changing manufacturing environment. The innovative idea behind the new visual servoing system is to utilize semantic information of the scene for visual servoing, as well as for other mobile robot tasks, such as obstacle avoidance, scene understanding, and simultaneous localization and mapping. Semantic segmentation is carried out by exploiting fully convolutional neural networks. The new visual servoing algorithm utilizes an intensity-based image registration procedure, which results in the image transformation matrix. The transformation matrix encompasses the relations of images taken at the current and desired pose, and that information is directly used for visual servoing. The developed algorithm is deployed on our own developed wheeled differential drive mobile robot RAICO (Robot with Artificial Intelligence based COgnition). The experimental evaluation is carried out in the 3D simulation environment and in the laboratory model of the real manufacturing environment. The experimental results show that the accuracy of the proposed approach is improved when compared to the state-of-the-art approaches while being robust to the partial occlusions of the scene and illumination changes.",industry
10.1016/j.ssci.2021.105529,Journal,Safety Science,scopus,2022-02-01,sciencedirect,A novel decision support system for managing predictive maintenance strategies based on machine learning approaches,https://api.elsevier.com/content/abstract/scopus_id/85118705579,"Nowadays, the industrial environment is characterised by growing competitiveness, short response times, cost reduction and reliability of production to meet customer needs. Thus, the new industrial paradigm of Industry 4.0 has gained interest worldwide, leading many manufacturers to a significant digital transformation. Digital technologies have enabled a novel approach to decision-making processes based on data-driven strategies, where knowledge extraction relies on the analysis of a large amount of data from sensor-equipped factories. In this context, Predictive Maintenance (PdM) based on Machine Learning (ML) is one of the most prominent data-driven analytical approaches for monitoring industrial systems aiming to maximise reliability and efficiency. In fact, PdM aims not only to reduce equipment failure rates but also to minimise operating costs by maximising equipment life. When considering industrial applications, industries deal with different issues and constraints relating to process digitalisation. The main purpose of this study is to develop a new decision support system based on decision trees (DTs) that guides the decision-making process of PdM implementation, considering context-aware information, quality and maturity of collected data, severity, occurrence and detectability of potential failures (identified through FMECA analysis) and direct and indirect maintenance costs. The decision trees allow the study of different scenarios to identify the conditions under which a PdM policy, based on the ML algorithm, is economically profitable compared to corrective maintenance, considered to be the current scenario. The results show that the proposed methodology is a simple and easy way to implement tool to support the decision process by assessing the different levels of occurrence and severity of failures. For each level, savings and the potential costs have been evaluated at leaf nodes of the trees aimed at defining the most suitable maintenance strategy implementation. Finally, the proposed DTs are applied to a real industrial case to illustrate their applicability and robustness.",industry
10.1016/j.eswa.2021.116045,Journal,Expert Systems with Applications,scopus,2022-02-01,sciencedirect,POSIMNET-R: An immunologic resilient approach to position routers in Industrial Wireless Sensor Networks,https://api.elsevier.com/content/abstract/scopus_id/85117584055,"Industry 4.0 has increased the interest in employing Industrial Wireless Sensor Network (IWSN) technologies in industrial automation. The advantages range from ease of installation and maintenance to reduced deployment time and infrastructure costs. However, industrial automation has critical requirements regarding network infrastructure, such as reliability and failure tolerance. Therefore, it is imperative to have an adequate placement of sensor and router nodes, to obtain a network with multiple paths, allowing the data to reach management systems within a reasonable time, even in the event of failures. The placement of router nodes has to consider latency, network lifespan, connectivity, and failure tolerance aspects in a possibly hostile environment, with classified areas and obstacles such as silos, tanks and buildings. We present a new approach, called POSIMNET-R, to place IWSN routing nodes in an industrial configuration, which circumvents forbidden areas and obstacles, based on Artificial Immunological Networks. The resulting network offers low failure rates and path redundancy criteria. The results have shown that POSIMNET-R was capable of providing a reliable network with multiple paths and resilience of the used routers equal to 81.50% in the basic case study and 73.66% in the real case scenario.",industry
10.1016/j.ress.2021.108119,Journal,Reliability Engineering and System Safety,scopus,2022-02-01,sciencedirect,Prognostics and Health Management (PHM): Where are we and where do we (need to) go in theory and practice,https://api.elsevier.com/content/abstract/scopus_id/85117331443,"We are performing the digital transition of industry, living the 4th industrial revolution, building a new World in which the digital, physical and human dimensions are interrelated in complex socio-cyber-physical systems. For the sustainability of these transformations, knowledge, information and data must be integrated within model-based and data-driven approaches of Prognostics and Health Management (PHM) for the assessment and prediction of structures, systems and components (SSCs) evolutions and process behaviors, so as to allow anticipating failures and avoiding accidents, thus, aiming at improved safe and reliable design, operation and maintenance.
                  There is already a plethora of methods available for many potential applications and more are being developed: yet, there are still a number of critical problems which impede full deployment of PHM and its benefits in practice. In this respect, this paper does not aim at providing a survey of existing works for an introduction to PHM nor at providing new tools or methods for its further development; rather, it aims at pointing out main challenges and directions of advancements, for full deployment of condition-based and predictive maintenance in practice.",industry
10.1016/j.future.2021.08.030,Journal,Future Generation Computer Systems,scopus,2022-02-01,sciencedirect,A wearable-based posture recognition system with AI-assisted approach for healthcare IoT,https://api.elsevier.com/content/abstract/scopus_id/85115908462,"Human posture recognition is a challenging task in the medical healthcare industry, when pursuing intelligence, accuracy, security, privacy, and efficiency, etc. Currently, the main posture recognition methods are captured-behaviors-based visual image analysis and wearable devices-based signal analysis. However, these methods suffer from issues such as high misjudgment rate, high-cost and low-efficiency. To address these issues, we propose a collaborative AI-IoT-based solution (namely, WMHPR) that embeds with advanced AI-assisted approach. In WMHPR, we propose the multi-posture recognition (MPR), an offline algorithm is implemented on wearable hardware, to identify posture based on multi-dimensions data. Meanwhile, an AI-based algorithm running on the cloud server (online), named Cascade-AdaBoosting-CART (CACT), is proposed to further enhance the reliability and accuracy of MPR. We recruit 20 volunteers for real-life experiments to evaluate the effectiveness, and the results show our solution is significantly outstanding in terms of accuracy and reliability while comparing with other typical algorithms.",industry
10.1016/j.comcom.2021.10.036,Journal,Computer Communications,scopus,2022-01-15,sciencedirect,LSTM-MFCN: A time series classifier based on multi-scale spatial–temporal features,https://api.elsevier.com/content/abstract/scopus_id/85119299619,"Time series classification (TSC) task attracts huge interests, since they correspond to the real-world problems in a wide variety of fields, such as industry monitoring. Deep learning methods, especially CNN and FCN, shows competitive performance in TSC task by their virtue of good adaption for raw time series and self-adapting extraction of features. Then various variants of CNN are proposed so as to make further breakthrough by the better perception to characteristics of data. Among them, LSTM-FCN and GRU-FCN who learn spatial and temporal features simultaneously are the most remarkable ones, achieving state of the art results. Therefore, inspired by their success and in consideration of the discriminative features implied in time series are diverse in size, a multimodal network LSTM-MFCN composed of multi-scale FCN (MFCN) and LSTM are proposed in this work. The gate-based network LSTM naturally fits to various terms time dependencies, and FCN with multi-scale sets of filters are capable to perceive spatial features of different range from time series curves. Besides, dilation convolution is deployed to build multi-scale receptive fields in larger level without increasing the parameters to be trained. The full perception of large multi-scale spatial–temporal features lead LSTM-MFCN to possess comprehensive and thorough grasp to time series, thus achieve even better accuracies. Finally, two representative architectures are presented specifically and their experiments on UCR datasets reveals the effectiveness and superiority of proposed LSTM-MFCN.",industry
10.1016/j.energy.2021.122359,Journal,Energy,scopus,2022-01-15,sciencedirect,Fuzzy inference system application for oil-water flow patterns identification,https://api.elsevier.com/content/abstract/scopus_id/85117714992,"Prediction of oil-water two-phase flow pattern provides an effective solution for reducing oil production costs. In this research, the fuzzy inference system (FIS) is utilized to predict fluid flow patterns and establish a new adaptable prediction model. This paper takes No. 10 industrial white oil and tap water as the research objects to simulate fluids, and analyzes the changes of the pipeline angle, the total flow of oil-water two-phase flow and the convective pattern of water cut. A data set containing 60 samples was used to create the model, and the Mamdani fuzzy model was established using MATLAB software. The results show that compared with the BP neural network algorithm, the model set forth in the present paper has higher accuracy and reliability, and can achieve real-time monitoring and effectively reduce errors, especially in the case of decision-making. In addition, the fuzzy model is demonstrated that in the entire production logging process of non-vertical wells, the use of a fuzzy inference system to predict fluid flow patterns can greatly save production costs while ensuring the safe operation of production equipment.",industry
10.1016/j.cose.2021.102500,Journal,Computers and Security,scopus,2022-01-01,sciencedirect,AntiViruses under the microscope: A hands-on perspective,https://api.elsevier.com/content/abstract/scopus_id/85118529412,"AntiViruses (AVs) are the main defense line against attacks for most users and much research has been done about them, especially proposing new detection procedures that work in academic prototypes. However, as most current and commercial AVs are closed-source solutions, in practice, little is known about their real internals: information such as what is a typical AV database size, the detection methods effectively used in each operation mode, and how often on average the AVs are updated are still unknown. This prevents research work from meeting the industrial practices more thoroughly. To fill this gap, in this work, we systematize the knowledge about AVs. To do so, we first surveyed the literature and identified existing knowledge gaps in AV internals’ working. Further, we bridged these gaps by analyzing popular (Windows, Linux, and Android) AV solutions to check their operations in practice. Our methodology encompassed multiple techniques, from tracing to fuzzing. We detail current AV’s architecture, including their multiple components, such as browser extensions and injected libraries, regarding their implementation, monitoring features, and self-protection capabilities. We discovered, for instance, a great disparity in the set of API functions hooked by the distinct AV’s libraries, which might have a significant impact in the viability of academically-proposed detection models (e.g., machine learning-based ones).",industry
10.1016/j.compind.2021.103556,Journal,Computers in Industry,scopus,2022-01-01,sciencedirect,C-Ports: A proposal for a comprehensive standardization and implementation plan of digital services offered by the “Port of the Future”,https://api.elsevier.com/content/abstract/scopus_id/85118477493,"In this paper we address the topic of a possible path to standardize the ICT services expected to be delivered by the so-called “Port of the Future”. How the most relevant technologies and Information Systems are used by the Port Communities for their businesses is discussed together with a detailed analysis of the on-going actions carried on by Standard Setting Organizations. Considering the examples given by the C-ITS Platform and the C-Roads programme at EU level, a proposal of contents to be considered in a comprehensive standardization action is given. The innovation services are therefore grouped into four bundles: (i) Vessel & Marine Navigation, (ii) e-Freight & (Intermodal) Logistics, (iii) Passenger Transport, (iv) Environmental sustainability. The standardized version of these applications will be finally labeled as C-Port services. Alongside the standardization plan, a proposal for ranking the ports on the basis of a specially-defined C-Port vector is discussed with the purpose of addressing the well-known lack of consensus around the mathematical definition of the Smart Port Index. Considering the good practice and the background offered by the Port of Livorno in terms of innovation actions, the prospected final user applications are then labeled as Day 1, Day 1.5, and Day 2 services in consideration of the technical and commercial gaps to be filled. As a case study about the evolution in the C-Port vector experienced by the Port of Livorno in the last years will also be discussed.",industry
10.1016/j.knosys.2021.107607,Journal,Knowledge-Based Systems,scopus,2021-12-25,sciencedirect,Adaptive multi-objective service composition reconfiguration approach considering dynamic practical constraints in cloud manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85117828056,"Dynamic uncertainty factors such as equipment faults are common in practically implemented cloud manufacturing (CMfg) environments, often causing the manufacturing service to be invalidated. In that case, efficient reconfiguration of the original service composition under practical constraints is critical; however, existing research scarcely focuses on it. This paper proposes a dynamic service composition reconfiguration model to bridge the gap by considering practical constraints (DSCRPC) in a real-life cloud manufacturing environment. Based on the constraints considered in this study, the DSCRPC model redefines three objectives: time (T*), cost (C*), and product service quality (Q*S*). To optimize the DSCRPC model, this study developed an adaptive multi-population multi-objective whale optimization algorithm (AMPOWOA) based on the Pareto strategy. The algorithm adopts four balancing strategies and adaptively optimizes and adjusts the key parameters under various balancing strategies through well-designed reinforcement learning models. Finally, we conduct numerical experiments and actual application case tests to compare the performances of AMPOWOA and other algorithms (MOWOA, MOHHO, NSGA-II). The results show that DSCRPC can continuously tackle the cloud manufacturing service composition (CMSC) reconfiguration issue with constraints until an order is completed. Moreover, AMPOWOA is superior to the other algorithms optimizing the DSCRPC model. This significantly enhances the robustness of service composition reconfiguration in real-life CMfg.",industry
10.1016/j.apenergy.2021.117857,Journal,Applied Energy,scopus,2021-12-15,sciencedirect,A hybrid deep learning-based online energy management scheme for industrial microgrid,https://api.elsevier.com/content/abstract/scopus_id/85115173233,"The fluctuations in electricity prices and intermittency of renewable energy systems necessitate the adoption of online energy management schemes in industrial microgrids. However, it is challenging to design effective and optimal online rolling horizon energy management strategies that can deliver assured optimality, subject to the uncertainties of volatile electricity prices and stochastic renewable resources. This paper presents an adaptable online energy management scheme for industrial microgrids that minimizes electricity costs while meeting production requirements by repeatedly solving an optimization problem over a moving control window, taking advantage of forecasted future prices and renewable energy profiles implemented by a hybrid deep learning model. The predicted values over the control horizon are assumed to be uncertain, and a multivariate Gaussian distribution is used to handle the variations in electricity prices and renewable resources around their predicted nominal values. Simulation results under different scenarios using real-world data verify the effectiveness of the proposed online energy management scheme, assessed by the corresponding gaps with respect to several selected benchmark strategies and the ideal boundaries of the best and worst known solutions. Furthermore, the robustness of the scheme is verified by considering severe errors in forecasted electricity prices and renewable profiles.",industry
10.1016/j.apenergy.2021.117733,Journal,Applied Energy,scopus,2021-12-15,sciencedirect,Controlling distributed energy resources via deep reinforcement learning for load flexibility and energy efficiency,https://api.elsevier.com/content/abstract/scopus_id/85114713033,"Behind-the-meter distributed energy resources (DERs), including building solar photovoltaic (PV) technology and electric battery storage, are increasingly being considered as solutions to support carbon reduction goals and increase grid reliability and resiliency. However, dynamic control of these resources in concert with traditional building loads, to effect efficiency and demand flexibility, is not yet commonplace in commercial control products. Traditional rule-based control algorithms do not offer integrated closed-loop control to optimize across systems, and most often, PV and battery systems are operated for energy arbitrage and demand charge management, and not for the provision of grid services. More advanced control approaches, such as MPC control have not been widely adopted in industry because they require significant expertise to develop and deploy. Recent advances in deep reinforcement learning (DRL) offer a promising option to optimize the operation of DER systems and building loads with reduced setup effort. However, there are limited studies that evaluate the efficacy of these methods to control multiple building subsystems simultaneously. Additionally, most of the research has been conducted in simulated environments as opposed to real buildings. This paper proposes a DRL approach that uses a deep deterministic policy gradient algorithm for integrated control of HVAC and electric battery storage systems in the presence of on-site PV generation. The DRL algorithm, trained on synthetic data, was deployed in a physical test building and evaluated against a baseline that uses the current best-in-class rule-based control strategies. Performance in delivering energy efficiency, load shift, and load shed was tested using price-based signals. The results showed that the DRL-based controller can produce cost savings of up to 39.6% as compared to the baseline controller, while maintaining similar thermal comfort in the building. The project team has also integrated the simulation components developed during this work as an OpenAIGym environment and made it publicly available so that prospective DRL researchers can leverage this environment to evaluate alternate DRL algorithms.",industry
10.1016/j.ijpe.2021.108296,Journal,International Journal of Production Economics,scopus,2021-12-01,sciencedirect,An integrated Delphi-MCDM-Bayesian Network framework for production system selection,https://api.elsevier.com/content/abstract/scopus_id/85114948077,"Several attempts are needed to choose the most compatible production system for achieving the desired manufacturing outputs. The significant role of manufacturing strategy deployment is selecting the production system best suited for a manufacturing firm. The appropriately chosen production system (strategic process choice) facilitates a firm to produce “order winning” outputs and provides a production competence to achieve business success. This research presents a novel framework to determine the compatible production system for a manufacturing firm. An integrated three-stage Delphi-MCDM-Bayesian Network (BN) framework has been proposed. The process choice criteria (PCC) considered for deciding production systems are identified through an in-depth literature review and then validated by experts through a Delphi method in the first stage. It resulted in the determination of twenty-six PCC. In the second stage, the multi-criteria decision-making (MCDM) based voting analytical hierarchy process (VAHP) method is adopted to determine each criterion's relative importance for a firm. The relative weights obtained are then used as input for the machine learning (ML) technique- Bayesian network (BN) in the third stage. The BN model quantifies the selection probability of production systems. The proposed Delphi-MCDM-BN framework is demonstrated using a real-life case of a “hydraulic and pneumatic valve” manufacturing firm to select a suitable production system. The three-stage framework is a novel contribution to the literature, which can be used by researchers, practitioners, and manufacturing strategists to choose an appropriate production system for any manufacturing firm.",industry
10.1016/j.asoc.2021.107859,Journal,Applied Soft Computing,scopus,2021-12-01,sciencedirect,Securing Smart Cities using LSTM algorithm and lightweight containers against botnet attacks,https://api.elsevier.com/content/abstract/scopus_id/85114806873,"Smart Cities contains millions of IoT sensors supporting critical applications such as Smart Transport, Buildings, Intelligent Vehicles, and Logistics. A central administrator appointed by the government manages and maintains the security of each node. Smart City relies upon millions of sensors that are heterogeneous and do not support standard security architecture. Different manufacturers have weak protection protocols for their products and do not update their firmware upon newly identified operating systems’ vulnerabilities. Adversaries using brute force methods exploit the lack of inbuilt security systems on IoT devices to grow their bot network. Smart cities require a standard framework combining soft computing and Deep Learning (DL) for device fleet management and complete control of sensor operating systems for absolute security. This paper presents a real-world application for IoT fleet management security using a lightweight container-based botnet detection (C-BotDet) framework. Using a three-phase approach, the framework using Artificial Intelligence detects compromised IoT devices sending malicious traffic on the network. Balena Cloud revokes API keys and prevents a compromised device from infecting other devices to form a more giant botnet. VPN (Virtual Private Network) prevents inter-device communication and routes all malicious traffic through an external server. The framework quickly updates the standard Linux-based operating system IoT device fleet without relying on different manufacturers to update their system security individually. The simulation and analysis of the C-BotDet framework are presented in a practical working environment to demonstrate its implementation feasibility.",industry
10.1016/j.livsci.2021.104700,Journal,Livestock Science,scopus,2021-11-01,sciencedirect,A review of deep learning algorithms for computer vision systems in livestock,https://api.elsevier.com/content/abstract/scopus_id/85118744270,"In livestock operations, systematically monitoring animal body weight, biometric body measurements, animal behavior, feed bunk, and other difficult-to-measure phenotypes is manually unfeasible due to labor, costs, and animal stress. Applications of computer vision are growing in importance in livestock systems due to their ability to generate real-time, non-invasive, and accurate animal-level information. However, the development of a computer vision system requires sophisticated statistical and computational approaches for efficient data management and appropriate data mining, as it involves massive datasets. This article aims to provide an overview of how deep learning has been implemented in computer vision systems used in livestock, and how such implementation can be an effective tool to predict animal phenotypes and to accelerate the development of predictive modeling for precise management decisions. First, we reviewed the most recent milestones achieved with computer vision systems and the respective deep learning algorithms implemented in Animal Science studies. Then, we reviewed the published research studies in Animal Science which used deep learning algorithms as the primary analytical strategy for image classification, object detection, object segmentation, and feature extraction. The great number of reviewed articles published in the last few years demonstrates the high interest and rapid development of deep learning algorithms in computer vision systems across livestock species. Deep learning algorithms for computer vision systems, such as Mask R-CNN, Faster R-CNN, YOLO (v3 and v4), DeepLab v3, U-Net and others have been used in Animal Science research studies. Additionally, network architectures such as ResNet, Inception, Xception, and VGG16 have been implemented in several studies across livestock species. The great performance of these deep learning algorithms suggests an improved predictive ability in livestock applications and a faster inference. However, only a few articles fully described the deep learning algorithms and their implementation. Thus, information regarding hyperparameter tuning, pre-trained weights, deep learning backbone, and hierarchical data structure were missing. We summarized peer-reviewed articles by computer vision tasks (image classification, object detection, and object segmentation), deep learning algorithms, animal species, and phenotypes including animal identification and behavior, feed intake, animal body weight, and many others. Understanding the principles of computer vision and the algorithms used for each application is crucial to develop efficient systems in livestock operations. Such development will potentially have a major impact on the livestock industry by predicting real-time and accurate phenotypes, which could be used in the future to improve farm management decisions, breeding programs through high-throughput phenotyping, and optimized data-driven interventions.",industry
10.1016/j.jmapro.2021.09.048,Journal,Journal of Manufacturing Processes,scopus,2021-11-01,sciencedirect,Joint active search and neuromorphic computing for efficient data exploitation and monitoring in additive manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85117322411,"The recent integration of imaging technology with additive manufacturing (AM) leads to the plethora of in-process and high-dimensional data. Machine learning (ML) methods have been implemented to improve understanding of defect formation in AM-built parts and controlling process variability in real-time. However, modern ML methods, in particular deep neural networks, are empowered by massive high-quality labeled data, which are limited in AM due to the following reasons: First, large data labeling is often tedious, costly, and requires substantial human efforts with considerable expertise. Second, the performance of the learning methods depends to a great extent on the presence of positive data instances (i.e., defective) as they are more informative for monitoring. Third, the rare positives result in a severe imbalanced dataset poses critical challenges in training ML methods designed with the assumption that the input contains an equal number of instances from each class. In this research, we propose novel annotation and learning with limited number of data through the integration of active search and hyperdimensional computing (HDC). The active search is developed to benefit from a single bandit model to learn about the data distribution (exploration) while sampling from the regions potentially containing more positives (exploitation). HDC is introduced as an alternative computing method that mimics important brain functionalities and encodes data with high-dimensional vectors, thereby enabling single-pass learning with just a few samples. Experimental results on a real-world case study of drag link joint build show the proposed model locates the rare positives thoroughly and detects lack of fusion defects with the accuracy of 89.58%, in 3.221 ± 0.029 second training time and with only 66 sample data. The joint active search and neuromorphic computing framework is shown to have strong potentials for general applications in a diverse set of domains with in-situ imaging data.",industry
10.1016/j.addma.2021.102328,Journal,Additive Manufacturing,scopus,2021-11-01,sciencedirect,In situ infrared temperature sensing for real-time defect detection in additive manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85115355988,"Melt pool temperature is a critical parameter for the majority of additive manufacturing processes. Monitoring of the melt pool temperature can facilitate the real-time detection of various printing defects such as voids, over-extrusion, filament breakage, clogged nozzle, etc. that occur either naturally or as the result of malicious hacking activity. This study uses an in situ, multi-sensor approach for monitoring melt pool temperature in which non-contact infrared temperature sensors with customized field of view move along with the extruder of a fused deposition modeling-based printer and sense melt pool temperature from a very short working distance regardless of its X-Y translational movements. A statistical method for defect detection is developed and utilized to identify temperature deviations caused by intentionally implemented defects. Effective detection for multiple defect types and sizes is demonstrated using both a simple L-shaped test geometry and a more complex industry standard test article. Strengths and limitations of this approach are presented, and the potential for expansion via more advanced data analysis techniques such as machine learning are discussed.",industry
10.1016/j.psj.2021.101437,Journal,Poultry Science,scopus,2021-11-01,sciencedirect,Pharmacokinetic/pharmacodynamic profiles of baicalin against Mycoplasma gallisepticum in an in vivo infection model,https://api.elsevier.com/content/abstract/scopus_id/85115144118,"Mycoplasma gallisepticum (
                        M. gallisepticum
                     ), a devastating avian pathogen that commonly causes chronic respiratory disease in chicken, is responsible for tremendous economic losses to the poultry industry. Baicalin is the main constituent of Scutellaria baicalensis that shows potential therapeutic effects against M. gallisepticum. However, the pharmacokinetic/pharmacodynamics (PK/PD) profiles of baicalin against M. gallisepticum are not well understood. The main objective of the present study was to determine the relationship between the PK/PD index and efficacy of baicalin in the M. gallisepticum infection model in chickens. The experiments were carried out on 10-day-old chickens that were challenged with M. gallisepticum in the bilateral air sacs. While, baicalin was orally administrated once in a day for 3 consecutive days, started from d 3 postinfection. Ultra-performance liquid chromatography (UPLC) was used to evaluate the PK parameters of baicalin at doses of 200, 400, and 600 mg/kg in M. gallisepticum-infected chickens. Real-time PCR (RT-PCR) was used for the quantitative detection of M. gallisepticum in lungs. The PK and PD data were fitted to WinNonlin software to evaluate the PK/PD profiles of baicalin against M. gallisepticum. The minimum inhibitory concentration (MIC) of baicalin against M. gallisepticum strain Rlow was 31.25 µg/mL. The in vivo data suggested that baicalin concentration in the lung tissues was higher than plasma (1.21–1.73 times higher). The ratios of AUC24h/MIC of baicalin against bacteriostatic, bactericidal, and eradication were 0.62, 1.33, and 1.49 h, respectively. In conclusion, these results provided potential reference for future clinical dose selection of baicalin and evaluation of susceptibility breakpoints.",industry
10.1016/j.jmbbm.2021.104728,Journal,Journal of the Mechanical Behavior of Biomedical Materials,scopus,2021-11-01,sciencedirect,What can artificial intelligence and machine learning tell us? A review of applications to equine biomechanical research,https://api.elsevier.com/content/abstract/scopus_id/85112485329,"Artificial intelligence (AI) and machine learning (ML) are fascinating interdisciplinary scientific domains where machines are provided with an approximation of human intelligence. The conjecture is that machines are able to learn from existing examples, and employ this accumulated knowledge to fulfil challenging tasks such as regression analysis, pattern classification, and prediction. The horse biomechanical models have been identified as an alternative tool to investigate the effects of mechanical loading and induced deformations on the tissues and structures in humans. Many reported investigations into bone fatigue, subchondral bone damage in the joints of both humans and animals, and identification of vital parameters responsible for retaining integrity of anatomical regions during normal activities in all species are heavily reliant on equine biomechanical research. Horse racing is a lucrative industry and injury prevention in expensive thoroughbreds has encouraged the implementation of various measurement techniques, which results in massive data generation. ML substantially accelerates analysis and interpretation of data and provides considerable advantages over traditional statistical tools historically adopted in biomechanical research. This paper provides the reader with: a brief introduction to AI, taxonomy and several types of ML algorithms, working principle of a feedforward artificial neural network (ANN), and, a detailed review of the applications of AI, ML, and ANN in equine biomechanical research (i.e. locomotory system function, gait analysis, joint and bone mechanics, and hoof function). Reviewing literature on the use of these data-driven tools is essential since their wider application has the potential to: improve clinical assessments enabling real-time simulations, avoid and/or minimize injuries, and encourage early detection of such injuries in the first place.",industry
10.1016/j.asoc.2021.107784,Journal,Applied Soft Computing,scopus,2021-11-01,sciencedirect,Towards learning behavior modeling of military logistics agent utilizing profit sharing reinforcement learning algorithm,https://api.elsevier.com/content/abstract/scopus_id/85112396580,"Agent-based modeling has become a beneficial tool in describing the complex and intelligent decision-making behaviors of military logistics entities, which is essential in exploring military logistics system. A challenging task in this field is the learning behavior modeling of military logistics agents. Profit sharing (PS) reinforcement learning algorithm is a representative exploitation-oriented method describing empirical reinforcement learning mechanism, and has been successfully applied to a variety of real-world problems. However, constructing the learning behavior model of military logistics agents is difficult by merely using the original PS algorithm. This difficulty is due to the actual characteristics of equipment support operations and military requirements, such as experience sharing, cooperative action, and hierarchical control. To address this issue, we propose an improved PS algorithm by introducing cooperative task reward correction parameters, experience sharing learning function, and superior command controlled function. We use the research methodology centering on the basic process of the improved PS algorithm as basis to construct the architecture of the learning behavior model of military logistics agents and its corresponding model of elements. Furthermore, we design the implementation algorithm of the learning behavior model. Lastly, we conduct a case study of a tactical military industrial logistics simulation system, thereby verifying the feasibility and effectiveness of the learning behavior model. We find that the improved PS algorithm and corresponding learning behavior model have more advantages than the original PS algorithm.",industry
10.1016/j.aquaeng.2021.102192,Journal,Aquacultural Engineering,scopus,2021-11-01,sciencedirect,"An integrated framework of sensing, machine learning, and augmented reality for aquaculture prawn farm management",https://api.elsevier.com/content/abstract/scopus_id/85112001426,"The rapid growth of prawn farming on an international scale will play an important role in meeting the protein requirements of an expanding global population. Efficient management of the commercial ponds for healthy production of prawns is the key mantra of success in this industry. It is a necessity to maintain the water quality parameters in these ponds within specific ranges to create an ideal environment of optimal growth of healthy prawns. The current practice of water quality data collection and their usage for decision making on most farms is not efficient and does not take full advantage of the latest technologies. The research presented in this paper aimed at addressing this problem by systematic investigation and development of an integrated framework where (i) modern sensors were investigated for their suitability and deployed for continuous monitoring of the water quality variables in prawn ponds; (ii) novel machine learning models were investigated based on collected data and deployed to accurately forecast pond status over next 24 h. This provides farmers insight into upcoming situations and take necessary measures to avoid catastrophic situations; and (iii) augmented reality-based visualisation methods were investigated for improved data capture process and efficient decision making through real-time interactive interfaces. The paper presents the integrated framework as well as the details of sensing, machine learning, and augmented reality components. We found that (i) YSI EXO2 Multi-Sonde is the best sensor for continuous monitoring of prawn ponds; (ii) ForecastNet (our developed machine learning model) provides best forecasting results with symmetric mean absolute percentage error of 6.1 %, 9.6 %, and 8.5 % for dissolved oxygen, pH, and temperature; and (iii) augmented reality-based interactive interface achieves accuracy as high as 89.2 % for management decisions with at least 41 % less time. The experience of the project as presented in this paper can act as a guide for researchers as well as prawn farmers to take advantage of latest sensors, machine learning algorithms and augmented reality tools.",industry
10.1016/j.talanta.2021.122608,Journal,Talanta,scopus,2021-11-01,sciencedirect,"PIXE based, Machine-Learning (PIXEL) supported workflow for glass fragments classification",https://api.elsevier.com/content/abstract/scopus_id/85109431731,"This paper presents a structured workflow for glass fragment analysis based on a combination of Elemental Analysis using PIXE and Machine Learning tools, with the ultimate goal of standardizing and helping forensic efforts. The proposed workflow was implemented on glass fragments received from the Israeli DIFS (Israeli Police Force's Division of Identification and Forensic Sciences) that were collected from various vehicles, including glass fragments from different manufacturers and years of production. We demonstrate that this workflow can produce models with high (>80%) accuracy in identifying glass fragment's origins and provide a test-case demonstrating how the model can be applied in real-life forensic events. We provide a standard, reproducible methodology that can be used in many forensic domains beyond glass fragments, for example, Gun Shot Residue, flammable liquids, illegal substances, and more.",industry
10.1016/j.renene.2021.05.155,Journal,Renewable Energy,scopus,2021-11-01,sciencedirect,A deep learning approach towards the detection and recognition of opening of windows for effective management of building ventilation heat losses and reducing space heating demand,https://api.elsevier.com/content/abstract/scopus_id/85107941088,"Building ventilation accounts for up to 30% of the heat loss in commercial buildings and 25% in industrial buildings. To effectively aid the reduction of energy consumption in the building sector, the development of demand-driven control systems for heating ventilation and air-conditioning (HVAC) is necessary. In countries with temperate climates such as the UK, many buildings depend on natural ventilation strategies such as openable windows, which are useful for reducing overheating prevalence during the summer. The manual opening and adjustment of windows by occupants, particularly during the heating season, can lead to substantial heat loss and consequent energy consumption. This could also result in the unnecessary or over ventilation of the space, or the fresh air is more than what is required to ensure adequate air quality. Furthermore, energy losses build up when windows are left open for extended periods. Hence, it is important to develop control strategies that can detect and recognise the period and amount of window opening in real-time and at the same time adjust the HVAC systems to minimise energy wastage and maintain indoor environment quality and thermal comfort. This paper presents a vision-based deep learning framework for the detection and recognition of manual window operation in buildings. A trained deep learning model is deployed into an artificial intelligence-powered camera. To assess the proposed strategy's capabilities, building energy simulation was used with various operation profiles of the opening of the windows based on various scenarios. Initial experimental tests were conducted within a university lecture room with a south-facing window. Deep learning influenced profile (DLIP) was generated via the framework, which uses real-time window detection and recognition data. The generated DLIP were compared with the actual observations, and the initial detection results showed that the method was capable of identifying windows that were opened and had an average accuracy of 97.29%. The results for the three scenarios showed that the proposed strategy could potentially be used to help adjust the HVAC setpoint or alert the occupants or building managers to prevent unnecessary heating demand. Further developments include enhancing the framework ability to detect multiple window opening types and sizes and the detection accuracy by optimising the model.",industry
10.1016/j.ymssp.2021.107915,Journal,Mechanical Systems and Signal Processing,scopus,2021-11-01,sciencedirect,Machine learning based frequency modelling,https://api.elsevier.com/content/abstract/scopus_id/85103975336,"Detection of cracks in structures has always been an important research topic in the industrial domain closely associated with aerospace, mechanical, marine and civil engineering. The presence of the cracks alters the dynamic response properties. Hence, it becomes crucial to locate these cracks in the structures to avoid any catastrophic failures and maintain structural integrity and performance. The study's objective is to propose two distinct statistical procedures for conducting the machine learning experiment for modelling the frequency and show the effect of experiment design on the results. In the study, the predictive performance of machine learning models and their ensembles is compared within each experiment design and between two experimental designs for the task of prediction of first six natural frequencies of a fixed ended cracked beam. The study highlights the significance of more than one experimental design to reduce the confirmation bias in the research and discusses the proposed methods' generalizability over the different modelling constraints and modelling parameters. The study also discusses a real-world implementation of the learned machine learning models from the perspective of Bayesian optimization.",industry
10.1016/j.jhlste.2020.100275,Journal,"Journal of Hospitality, Leisure, Sport and Tourism Education",scopus,2021-11-01,sciencedirect,Industry 4.0 technologies in tourism education: Nurturing students to think with technology,https://api.elsevier.com/content/abstract/scopus_id/85092173436,"The Industry 4.0 revolution is bringing major transformations in the tourism systems design suitable for technologically oriented consumers. Indeed, methods and technologies introduced by Big Data, Automation, Virtual and augmented reality, Robotics and ICT well fit with the Tourism 4.0 paradigm. However, tourism students are not yet trained on techniques, issues and methods related to the Industry 4.0 framework.
                  Hence, relying on a careful examination of the literature on tourism market trends linked to the offer of innovative technological services, we identified conceptual, methodological, technological and practical skills to be developed in an academic curriculum for Tourism Science students. Learning path were focused on: i) processes of data acquisition from social media, ii) data analysis using Machine Learning techniques and iii) data design into significant elements useful to implement communication systems in the tourism field.
               
                  Results
                  showed that the most of participants achieved a medium-high evaluation for the implementation of the communication systems, applying appropriately techniques and tools learned along the course. Furthermore, the high percentage of students satisfaction registered in relation to the course, revealed that students enjoyed this experience. Outcomes reflects the acquisition and the awareness of those skills that will enable students to be conscious protagonists of their role in tourism 4.0.",industry
10.1016/j.probengmech.2021.103173,Journal,Probabilistic Engineering Mechanics,scopus,2021-10-01,sciencedirect,Machine learning based digital twin for stochastic nonlinear multi-degree of freedom dynamical system,https://api.elsevier.com/content/abstract/scopus_id/85117922944,"The potential of digital twin technology is immense, specifically in the infrastructure, aerospace, and automotive sector. However, practical implementation of this technology is not at an expected speed, specifically because of lack of application-specific details. In this paper, we propose a novel digital twin framework for stochastic nonlinear multi-degree of freedom (MDOF) dynamical systems. The proposed digital twin has four modules — (a) a physics-based nominal model, (b) a data collection module, (c) algorithm for real-time update of the digital twin and (d) module for predicting future state. The modules for real-time update and prediction are based on the so-called gray-box modeling approach, and utilizes both physics based and data driven frameworks; this enables the proposed digital twin to generalize and predict future responses. The gray box modeling framework used within the digital twin is developed by coupling Bayesian filtering and machine learning algorithm. Although, the proposed digital twin can be used with any machine learning regression algorithm, we have used Gaussian process in this study. Performance of the proposed approach is illustrated using two examples. Results obtained indicate the applicability and excellent performance of the proposed digital twin framework.",industry
10.1016/j.jocs.2021.101443,Journal,Journal of Computational Science,scopus,2021-10-01,sciencedirect,Towards versatile conversations with data-driven dialog management and its integration in commercial platforms,https://api.elsevier.com/content/abstract/scopus_id/85115363526,"Conversational interfaces have recently become a ubiquitous element in both the personal sphere by easing access to services, and industrial environments by the automation of services, improved customer support and its corresponding cost savings. However, designing the dialog model used by these interfaces to decide system responses is still a hard-to-accomplish task for complex conversational interactions. This paper describes a data-driven dialog management technique, which provides flexibility to develop, deploy and maintain this module. Various configurations for classification algorithms are assessed with two dialog corpora of different application domains, size, dimensionalities and set of possible system responses. The results of the evaluation show satisfactory accuracy and coherence rates in both tasks. As a proof of concept, our proposal has also been integrated with DialogFlow, a platform provided by Google to design conversational user interfaces. Our proposal has been assessed with a real use case, proving that it can be deployed in conjunction with commercial platforms, obtaining satisfactory results for the objective and subjective assessments completed.",industry
10.1016/j.techfore.2021.120986,Journal,Technological Forecasting and Social Change,scopus,2021-10-01,sciencedirect,Big data and firm marketing performance: Findings from knowledge-based view,https://api.elsevier.com/content/abstract/scopus_id/85113910474,"A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications. This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms’ strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms’ activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.",industry
10.1016/j.asoc.2021.107702,Journal,Applied Soft Computing,scopus,2021-10-01,sciencedirect,OrbitNet: A new CNN model for automatic fault diagnostics of turbomachines,https://api.elsevier.com/content/abstract/scopus_id/85111487515,"Unplanned outage due to faults in a high-fidelity turbomachine such as steam turbine and centrifugal compressor often results in the reduced reliability and productivity of a factory while increasing its maintenance costs. Shaft orbit images generated from turbomachine vibration signals have been used to diagnose component faults. However, the existing methods were developed mostly by either using features extracted from orbits or utilizing simulation data which may produce inaccurate results in practical applications due to system complexity and data uncertainties. This paper presents a novel deep learning convolution neural network methodology for accurately automatic diagnostics of multiple faults in general rotating machines by adeptly integrating advanced signal processing with orbit images augmentation, considering the high non-linearity and uncertainty of sensed vibration signals. Environmental noise in vibration signals are filtered through the integration of multiresolution discrete wavelet packet transform and Bayesian hypothesis testing-based automatic thresholding. Shaft orbit images generated from the cleansed vibration data are augmented to increase their representativity and generalization. A novel multi-layer convolutional neural network model, OrbitNet, is specially designed to improve its generality and robustness while avoid possible overfitting in fault identification of various turbomachines. The proposed model retains the pattern information in the axis trajectory to the greatest extent, with the ability of accurately capturing features of various faults in different turbomachines. A generic implementation procedure is proposed for automatic fault diagnosis of rotating machinery based on the presented methodology. A comparison study is conducted to demonstrate the effectiveness and feasibility of the proposed methodology by using the sensed vibration signals collected from three real-world centrifugal compressors, two steam turbines and one generator with four different fault modes including imbalance, friction, misalignment and oil whirl.",industry
10.1016/j.conengprac.2021.104903,Journal,Control Engineering Practice,scopus,2021-10-01,sciencedirect,Synthesizing labeled data to enhance soft sensor performance in data-scarce regions,https://api.elsevier.com/content/abstract/scopus_id/85111246946,"Quality variables are key indicators of the operating performance in industrial processes. Because they are difficult to measure, soft sensor models can be adopted to predict them timely. For accurate prediction, sufficient training data are necessary to construct a good soft sensor model. In practical industrial processes, however, data labeled with quality variables are usually deficient in the desired region. Particularly, when the process is just switched to a new mode, available data in this new mode are initially quite a few. In this paper, a novel data synthesis method based on the regressor-embedded semi-supervised variational autoencoder (RSSVAE) model is proposed to generate synthetic labeled data when the original labeled data are inadequate. The proposed model utilizes not only the original data in the data-scarce region but also the data in other regions, which share some common information with the scarce data. Meanwhile, data synthesis and model correction mechanism are implemented iteratively to avoid model biases. Once the synthetic labeled data of the data-scarce region are acquired, they are combined with the original labeled data to establish a local soft sensor and predict the quality variables of the unlabeled data. Finally, a real ammonia synthesis process is introduced to demonstrate the effectiveness of the proposed method.",industry
10.1016/j.asoc.2021.107644,Journal,Applied Soft Computing,scopus,2021-10-01,sciencedirect,Production scheduling in industrial mining complexes with incoming new information using tree search and deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85109174667,"Industrial mining complexes have implemented digital technologies and advanced sensors to monitor and gather real-time data about their different operational aspects, starting from the supply of materials from the mineral deposits involved to the products provided to customers. However, technologies are not available to respond in real-time to the incoming new information to adapt the short-term production schedule of a mining complex. A short-term production schedule determines the daily/weekly/monthly sequence of extraction, the destination of materials and utilization of processing streams. This paper presents a novel self-learning artificial intelligence algorithm for mining complexes that learns, from its own experience, to adapt the short-term production scheduling decisions by responding to incoming new information. The algorithm plays the game of short-term production scheduling on its own using a Monte Carlo tree search to train a deep neural network agent that adapts the short-term production schedule with incoming new information. The deep neural network agent evaluates the short-term production scheduling decisions and, in parallel, performs searches using the Monte Carlo tree search to generate experiences. The experiences are then used to train the agent. The agent improves the strength of the tree search, which results in an even stronger self-play to generate better experiences. An application of the proposed algorithm at a real-world copper mining complex shows its exceptional performance to adapt the 13-week short-term production schedule almost in real-time. The adapted production schedule successfully meets the different production requirements and makes better use of the processing capabilities, while also increasing copper concentrate production by 7% and cash flows by 12% compared to the initial production schedule. A video of the proposed algorithm can be found at https://youtu.be/_gSbzxMc_W8.",industry
10.1016/j.cofs.2021.03.014,Journal,Current Opinion in Food Science,scopus,2021-10-01,sciencedirect,Novel digital technologies implemented in sensory science and consumer perception,https://api.elsevier.com/content/abstract/scopus_id/85104656313,"New and emerging digital technologies have been implemented in sensory science, which minimize subjectivity and biases in data acquisition and interpretation compared to traditional methods. These technologies have enabled the incorporation of physiological and emotional responses of panelists elicited by food, beverage, and packaging stimuli through accurate and unbiased information from different sensor technologies. This review focused on recent advances of digital technologies used for sensory science, such as (i) software for sensory science, (ii) integration of biometrics to assess physiological and emotional responses of panelists, (iii) incorporation of virtual, augmented, and mixed reality, and (iv) sensor technology (electronic noses and tongues) for sensory analysis. Rapid data acquisition and results’ interpretation could open the way to automation and implementation of Artificial Intelligence that could revolutionize the food and beverage industries. It also presents a proposed framework for integrating and implementing digital technologies through the food chain from farm/manufacturing facilities to the palate.",industry
10.1016/j.rcim.2021.102176,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2021-10-01,sciencedirect,Robotic grasping: from wrench space heuristics to deep learning policies,https://api.elsevier.com/content/abstract/scopus_id/85104603575,"The robotic grasping task persists as a modern industry problem that seeks autonomous, fast implementation, and efficient techniques. Domestic robots are also a reality demanding a delicate and accurate human–machine interaction, with precise robotic grasping and handling. From decades ago, with analytical heuristics, to recent days, with the new deep learning policies, grasping in complex scenarios is still the aim of several works’ that propose distinctive approaches. In this context, this paper aims to cover recent methodologies’ development and discuss them, showing state-of-the-art challenges and the gap to industrial applications deployment. Given the complexity of the related issue associated with the elaborated proposed methods, this paper formulates some fair and transparent definitions for results’ assessment to provide researchers with a clear and standardised idea of the comparison between the new proposals.",industry
10.1016/j.knosys.2021.107261,Journal,Knowledge-Based Systems,scopus,2021-09-27,sciencedirect,Federated conditional generative adversarial nets imputation method for air quality missing data,https://api.elsevier.com/content/abstract/scopus_id/85111226892,"The air quality is a topic of extreme concern that attracts a lot of attention in the world. Many intelligent air quality monitoring networks have been deployed in various places, especially in big cities. These monitoring networks collect air quality data with some missing data for some reasons which pose an obstacle for air quality publishing and studies. Generative adversarial nets (GAN) methods have achieved state-of-the-art performance in missing data imputation. GAN-based imputation method needs enough training data while one monitoring network has just a few and poor quality monitoring data and these data sets do not meet the independent identical distribution (IID) condition. Therefore, one monitoring network side needs to utilize more monitoring data from other sides as far as possible. However, in the real world, these air quality monitoring networks are owned by different organizations — companies, the government even some secret units. Many of them cannot share detailed monitoring data due to security, privacy, and industrial competition. In this paper, it is the first time to propose a conditional GAN imputation method under a federated learning framework to solve the data sets that come from diverse data-owners without sharing. Furthermore, we improve the vanilla conditional GAN performance with Wasserstein distance and “Hint mask” trick. The experimental results show that our GAN-based imputation methods can achieve the best performance. And our federated GAN imputation method outperforms the GAN imputation method trained locally for each participant which means our imputation model can work. Our proposed federated GAN method can benefit model quality by increasing access to air quality data through private multi-institutional collaborations. We further investigate the effects of data geographical distribution across collaborating participants on model quality and, interestingly, we find that the GAN training process with a federated learning framework performs more stable.",industry
10.1016/j.pmcj.2021.101445,Journal,Pervasive and Mobile Computing,scopus,2021-09-01,sciencedirect,Towards generating a reliable device-specific identifier for IoT devices,https://api.elsevier.com/content/abstract/scopus_id/85111971982,"A significant number of IoT devices are being deployed in the wild, mostly in remote locations and in untrusted conditions. This could include monitoring an electronic perimeter fence or a critical infrastructure such as telecom and power grids. Such applications rely on the fidelity of data reported from the IoT devices, and hence it is imperative to identify the trustworthiness of the remote device before taking decisions. Existing approaches use a secret key usually stored in volatile or non-volatile memory for creating an encrypted digital signature. However, these techniques are vulnerable to malicious attacks and have significant computation and energy overhead. This paper presents a novel device-specific identifier, IoT-ID that captures the device characteristics and can be used towards device identification. IoT-ID is based on physically unclonable functions (PUFs), that exploit variations in the manufacturing process to derive a unique fingerprint for integrated circuits. In this work, we design novel PUFs for Commercially Off the Shelf (COTS) components such as clock oscillators and ADC, to derive IoT-ID for a device. Hitherto, system component PUFs are invasive and rely on additional dedicated hardware circuitry to create a unique fingerprint. A highlight of our PUFs is doing away with special hardware. IoT-ID is non-invasive and can be invoked using simple software APIs running on COTS components. IoT-ID has the following key properties viz., constructability, real-time, uniqueness, and reproducibility, making them robust device-specific identifiers.
                  We present detailed experimental results from our live deployment of 50 IoT devices running over a month. Our edge machine learning algorithm has 100% accuracy in uniquely identifying the 50 devices in our deployment and can run locally on the resource-constrained IoT device. We show the scalability of IoT-ID with the help of numerical analysis on 1000s of IoT devices. Further, we discuss approaches to evaluate and improve the reliability of the IoT-ID.
                        1
                     
                     
                        1
                        This manuscript is an extension of the paper ‘IoT-ID: A Novel Device-Specific Identifier Based on Unique Hardware Fingerprints’ Vaidya et al. (2020) published in 2020 IEEE/ACM Fifth International Conference on Internet-of-Things Design and Implementation (IoTDI).",industry
10.1016/j.ijcip.2021.100424,Journal,International Journal of Critical Infrastructure Protection,scopus,2021-09-01,sciencedirect,Industrial intrusion detection based on the behavior of rotating machine,https://api.elsevier.com/content/abstract/scopus_id/85111013464,"In this study, a new industrial intrusion detection method is introduced for the control system of rotating machines as critical assets in many industries. Data tampering is a major attack on the control systems which disrupts the functionality of the asset. Hence, our objective is to detect data manipulations in the system. We use the behavior of the rotating machine to propose new industrial intrusion detection for the control system of the rotating machine by machine learning techniques. The behavior is elicited by the data of sensors under all the conditions of the rotating machine operation. In this work, the nonlinear regression, novelty detection, outlier detection, and classification approaches are implemented to create behavioral model. On each implementation, online data are compared with the real data of behavior prediction model during the operation of the rotating machine to detect any abnormality. According to our experimental results, the accuracy of the behavioral models created by the One-classSVM novelty detection, k- Nearest Neighbor (kNN) outlier detection, decision tree classifier, k-Neighbors classifier, random forest classifier, and AdaBoost classifier is obtained as 0.98, 0.994, 0.999, 0.999, 0.999, and 0.999, respectively. The results indicate that the proposed industrial intrusion detection method is able to detect the data tampering attacks on the control system of the rotating machines very accurately.",industry
10.1016/j.segan.2021.100511,Journal,"Sustainable Energy, Grids and Networks",scopus,2021-09-01,sciencedirect,Multi-interval programming based scheduling of appliances with user preferences and dynamic pricing in residential area,https://api.elsevier.com/content/abstract/scopus_id/85110609599,"In industrial and commercial sectors, numerous countries had successfully implemented the dynamic pricing as a solution to the problem of high power demand in peak hours. But, an extensive use of real-time pricing in the residential electricity sector is hugely missing. In order to boost the efficiency of electricity market by demand response, real-time pricing needs to be implemented into residential sector also. In this paper the proposed algorithm is implemented for residential consumers of different categories with real time pricing data of ComEd, Northern Illinois Power Company, and Alactra Utilities Corporation. The proposed algorithm incorporates single interval and multi interval programming for different power pricing schemes. The proposed algorithm is suggested using metaheuristic optimization techniques viz. cuckoo search (CS), adaptive cuckoo search (ACS) and Hybrid GA–PSO for the optimum scheduling of residential appliances. The objective of this paper is to minimize the monthly electricity bill cost as well as peak demand under uncertain electricity prices. The comparative analysis of optimal solutions obtained by various artificial intelligence techniques validates the high performance of proposed algorithm. It facilitates both the residential consumer and utilities with benefits.",industry
10.1016/j.ijcip.2021.100452,Journal,International Journal of Critical Infrastructure Protection,scopus,2021-09-01,sciencedirect,Adversarial attacks and mitigation for anomaly detectors of cyber-physical systems,https://api.elsevier.com/content/abstract/scopus_id/85107972529,"The threats faced by cyber-physical systems (CPSs) in critical infrastructure have motivated research into a multitude of attack detection mechanisms, including anomaly detectors based on neural network models. The effectiveness of anomaly detectors can be assessed by subjecting them to test suites of attacks, but less consideration has been given to adversarial attackers that craft noise specifically designed to deceive them. While successfully applied in domains such as images and audio, adversarial attacks are much harder to implement in CPSs due to the presence of other built-in defence mechanisms such as rule checkers (or invariant checkers). In this work, we present an adversarial attack that simultaneously evades the anomaly detectors and rule checkers of a CPS. Inspired by existing gradient-based approaches, our adversarial attack crafts noise over the sensor and actuator values, then uses a genetic algorithm to optimise the latter, ensuring that the neural network and the rule checking system are both deceived. We implemented our approach for two real-world critical infrastructure testbeds, successfully reducing the classification accuracy of their detectors by over 50% on average, while simultaneously avoiding detection by rule checkers. Finally, we explore whether these attacks can be mitigated by training the detectors on adversarial samples.",industry
10.1016/j.asoc.2021.107574,Journal,Applied Soft Computing,scopus,2021-09-01,sciencedirect,Nonlinear-based Chaotic Harris Hawks Optimizer: Algorithm and Internet of Vehicles application,https://api.elsevier.com/content/abstract/scopus_id/85107718159,"Harris Hawks Optimizer (HHO) is one of the many recent algorithms in the field of metaheuristics. The HHO algorithm mimics the cooperative behavior of Harris Hawks and their foraging behavior in nature called surprise pounce. HHO benefits from a small number of controlling parameters setting, simplicity of implementation, and a high level of exploration and exploitation. To alleviate the drawbacks of this algorithm, a modified version called Nonlinear based Chaotic Harris Hawks Optimization (NCHHO) is proposed in this paper. NCHHO uses chaotic and nonlinear control parameters to improve HHO’s optimization performance. The main goal of using the chaotic maps in the proposed method is to improve the exploratory behavior of HHO. In addition, this paper introduces a nonlinear control parameter to adjust HHO’s exploratory and exploitative behaviors. The proposed NCHHO algorithm shows an improved performance using a variety of chaotic maps that were implemented to identify the most effective one, and tested on several well-known benchmark functions. The paper also considers solving an Internet of Vehicles (IoV) optimization problem that showcases the applicability of NCHHO in solving large-scale, real-world problems. The results demonstrate that the NCHHO algorithm is very competitive, and often superior, compared to the other algorithms. In particular, NCHHO provides 92% better results in average to solve the uni-modal and multi-modal functions with problem dimension sizes of D = 30 and 50, whereas, with respect to the higher dimension problem, our proposed algorithm shows 100% consistent improvement with D = 100 and 1000 compared to other algorithms. In solving the IoV problem, the success rate was 62.5%, which is substantially better in comparison with the state-of-the-art algorithms. To this end, the proposed NCHHO algorithm in this paper demonstrates a promising method to be widely used by different applications, which brings benefits to industries and businesses in solving their optimization problems experienced daily , such as resource allocation, information retrieval, finding the optimal path for sending data over networks, path planning, and so many other applications.",industry
10.1016/j.jss.2021.110993,Journal,Journal of Systems and Software,scopus,2021-09-01,sciencedirect,Automated defect prioritization based on defects resolved at various project periods,https://api.elsevier.com/content/abstract/scopus_id/85107687766,"Defect prioritization is mainly a manual and error-prone task in the current state-of-the-practice. We evaluated the effectiveness of an automated approach that employs supervised machine learning. We used two alternative techniques, namely a Naive Bayes classifier and a Long Short-Term Memory model. We performed an industrial case study with a real project from the consumer electronics domain. We compiled more than 15,000 issues collected over 3 years. We could reach an accuracy level up to 79.36% and we had 3 observations. First, Long Short-Term Memory model has a better accuracy when compared with a Naive Bayes classifier. Second, structured features lead to better accuracy compared to textual descriptions. Third, accuracy is not improved by considering increasingly earlier defects as part of the training data. Increasing the size of the training data even decreases the accuracy compared to the results, when we use data only regarding the recently resolved defects.",industry
10.1016/j.jnca.2021.103116,Journal,Journal of Network and Computer Applications,scopus,2021-09-01,sciencedirect,Traffic Engineering in Hybrid Software Defined Network via Reinforcement Learning,https://api.elsevier.com/content/abstract/scopus_id/85107660469,"The emergence of Software Defined Network (SDN) provides a centralized and flexible approach to route network flows. Due to the technical and economic challenges in upgrading to a fully SDN-enabled network, hybrid SDN, with a partial deployment of SDN switches in a traditional network, has been a prevailing network architecture. Meanwhile, Traffic Engineering (TE) in the hydbrid SDN has attracted wide attentions from academia and industry. Previous studies on TE in the hybrid SDN are either traffic-oblivious or time-consuming, which causes routing schemes failed in responding to the dynamically-changing traffic rapidly and intelligently. Therefore, in this paper, we propose a Reinforcement Learning (RL) based method, which learns a traffic-splitting agent to address the dynamically-changing traffic and achieve the link load balancing in the hybrid SDN. Specifically, to rapidly and intelligently determine a routing scheme to the new traffic demands, a traffic-splitting agent is designed and learnt offline by exploiting the RL algorithm to establish the direct relationship between traffic demands and traffic-splitting policies. Once the traffic-splitting agent is learnt, the effective traffic-splitting policies, which are used to determine the traffic-splitting ratios on SDN switches, can be generated rapidly. Additionally, to meet the interactive requirements for learning a traffic-splitting agent, a reasonable simulation environment is proposed to be constructed to avoid routing loops when traffic-splitting policies are taken. Extensive evaluations on different topologies and real traffic demands demonstrate that the proposed method achieves the comparable network performance and performs superiorities in rapidly generating the satisfying routing schemes.",industry
10.1016/j.scs.2021.103009,Journal,Sustainable Cities and Society,scopus,2021-09-01,sciencedirect,Applying machine learning in intelligent sewage treatment: A case study of chemical plant in sustainable cities,https://api.elsevier.com/content/abstract/scopus_id/85106305327,"Nowadays, sewage treatment in sustainable cities attracts more researchers both from academic and industrial communities. Especially, since industrial sewage is normally highly toxic, which could cause serious pollution in a city and lead to health problems of residents, it is critical to monitor and predictably maintain sewage treatment facilities in cities. This paper presents an intelligent sewage treatment system based on machine learning and Internet of Things sensors to assist to manage the sewage treatment in a fine chemical plant. The implemented system has operated for twenty months, acquired multi-dimension data such as temperatures in different treatment processes, operation parameters of devices, and real-time Chemical Oxygen Demand (COD). Since the change trend of outflow COD is highly related to operation status, this paper innovatively uses different types of temperature and water inflow data as model inputs and applies three algorithms to make prediction, which are Support Vector Regression (SVR), Long Short-Term Memory (LSTM) neural network, and Gated Recurrent Unit (GRU) neural network. The experimental results show that GRU model performs better (MAPE = 10.18%, RMSE = 35.67, MAE = 31.16) than LSTM and SVR. This study can be extended to various sewage treatment scenarios in sustainable cities.",industry
10.1016/j.energy.2021.120700,Journal,Energy,scopus,2021-09-01,sciencedirect,Nonlinear generalized predictive controller based on ensemble of NARX models for industrial gas turbine engine,https://api.elsevier.com/content/abstract/scopus_id/85105736036,"New design and operation of modern gas turbine engines (GTEs) are becoming more and more complex where several limitations and control modes should be fulfilled at the same time to accomplish a safe and ideal performance for the engine. For this purpose, a constrained multi-input multi-output (MIMO) non-linear model predictive controller (NMPC) based on neural network model is designed to fulfill the control requirements of a Siemens SGT-A65 three-spool aero-derivative gas turbine engine (ADGTE) used for power generation. However, the implementation of NMPC in real time has two challenges: Firstly, the design of an accurate non-linear model, which can run many times faster than real time. Secondly, the usage of a rapid and reliable optimization algorithm to solve the optimization problem in real time. To solve these issues, the constrained MIMO NMPC is created based on the generalized predictive control (GPC) algorithm as a result of its clarity, ease of use, and capacity to deal with problems in one algorithm. In addition, seven ensembles of eight multi-input single-output (MISO) non-linear autoregressive network with exogenous inputs (NARX) models are used as a base model for the GPC controller to predict the future process outputs. Estimation of free and forced responses of the GPC based on the neural network (NN) model of the plant each sampling time without performing instantaneous linearization is proposed in this study, which reduces the NMPC optimization problem to a linear optimization problem at each sampling step. In addition, the Hildreth's quadratic programming algorithm is used to solve the quadratic optimization problem within the NMPC controller, which offers ease of use and reliability in real time applications. To demonstrate the performance of the NNGPC controller developed in this study, we have compared the performance of the neural network generalized predictive control (NNGPC) controller to the existing controller of the SGT-A65 engine. The simulation results show that the NNGPC has demonstrated output responses with less oscillatory behavior and smoother control actions to the sudden variation in the electric load disturbance than those observed in the existing min-max controller. However, the min-max controller has faster response than that of the NNGPC controller.",industry
10.1016/j.asoc.2021.107465,Journal,Applied Soft Computing,scopus,2021-09-01,sciencedirect,Click-event sound detection in automotive industry using machine/deep learning,https://api.elsevier.com/content/abstract/scopus_id/85105315919,"In the automotive industry, despite the robotic systems on the production lines, factories continue employing workers in several custom tasks getting for semi-automatic assembly operations. Specifically, the assembly of electrical harnesses of engines comprises a set of connections between electrical components. Despite the task is easy to perform, employees tend not to notice that a few components are not being connected properly due to physical fatigue provoked by repetitive tasks. This yields a low quality of the assembly production line and possible hazards. In this work, we propose a sound detection system based on machine/deep learning (ML/DL) approaches to identify click sounds produced when electrical harnesses are connected. The purpose of this system is to count the number of connections properly made and to feedback to the employees. We collect and release a public dataset of 25,000 click sounds of 25 ms length at 22 kHz during three months of assembly operations in an automotive production line located in Mexico. Then, we design an ML/DL-based methodology for click sound detection of assembled harnesses under real conditions of a noisy environment (noise level ranging from 
                        
                           −
                           16
                           .
                           67
                        
                      dB to 
                        
                           −
                           12
                           .
                           87
                        
                      dB) including other machinery sounds. Our best ML/DL model (i.e., a combination between five acoustic features and an optimized convolutional neural network) is able to detect click sounds in a real assembly production line with an accuracy of 
                        
                           94
                           .
                           55
                           ±
                           0
                           .
                           83
                        
                      %. To the best of our knowledge, this is the first time a click sounds detection system in assembling electrical harnesses of engines for giving feedback to the workers is proposed and implemented in a real-world automotive production line. We consider this work valuable for the automotive industry on how to apply ML/DL approaches for improving the quality of semi-automatic assembly operations.",industry
10.1016/j.simpa.2021.100081,Journal,Software Impacts,scopus,2021-08-01,sciencedirect,OpenICS: Open image compressive sensing toolbox and benchmark[Formula presented],https://api.elsevier.com/content/abstract/scopus_id/85115856142,"The real-world application of image compressive sensing is largely limited by the lack of standardization in implementation and evaluation. To address this limitation, we present OpenICS, an image compressive sensing toolbox that implements multiple popular image compressive sensing algorithms into a unified framework with a standardized user interface. Furthermore, a corresponding benchmark is also proposed to provide a fair and complete evaluation of the implemented algorithms. We hope this work can serve the growing research community of compressive sensing and the industry to facilitate the development and application of image compressive sensing.",industry
10.1016/j.cag.2021.04.035,Journal,Computers and Graphics (Pergamon),scopus,2021-08-01,sciencedirect,DIMNet: Dense implicit function network for 3D human body reconstruction,https://api.elsevier.com/content/abstract/scopus_id/85105858338,"In recent years, with the improvement of artificial intelligence technology, it has become possible to reconstruct high-precision 3D human body models based on ordinary RGB images. The current 3D human body reconstruction technology requires complex external equipment to scan all angles of the human body, which is complicated to be implemented and cannot be popularized. In order to solve this problem, this paper applies deep learning models on reconstructing 3D human body based on monocular images. First of all, this paper uses Stacked Hourglass network to perform convolution operations on monocular images collected from different views. Then Multi-Layer Perceptrons (MLPs) are used to decode the encoded high-level images. The feature codes in the two views(main and side) are fused, and the interior and exterior points are classified by the fusion features, so as to obtain the corresponding 3D occupancy field. At last, the Marching Cube algorithm is used for 3D reconstruction with a specific threshold and then we use Laplace smoothing algorithm to remove artifacts. This paper proposes a dense sampling strategy based on the important joint points of the human body, which has a certain optimization effect on the realization of high-precision 3D reconstruction. The performance of the proposed scheme has been validated on the open source datasets, MGN dataset and the THuman dataset, provided by Tsinghua University. The proposed scheme can reconstruct features such as clothing folds, color textures, and facial details,and has great potential to be applied in different applications.",industry
10.1016/j.eswa.2021.114820,Journal,Expert Systems with Applications,scopus,2021-08-01,sciencedirect,Machine Learning for industrial applications: A comprehensive literature review,https://api.elsevier.com/content/abstract/scopus_id/85102967505,"Machine Learning (ML) is a branch of artificial intelligence that studies algorithms able to learn autonomously, directly from the input data. Over the last decade, ML techniques have made a huge leap forward, as demonstrated by Deep Learning (DL) algorithms implemented by autonomous driving cars, or by electronic strategy games. Hence, researchers have started to consider ML also for applications within the industrial field, and many works indicate ML as one the main enablers to evolve a traditional manufacturing system up to the Industry 4.0 level. Nonetheless, industrial applications are still few and limited to a small cluster of international companies. This paper deals with these topics, intending to clarify the real potentialities, as well as potential flaws, of ML algorithms applied to operation management. A comprehensive review is presented and organized in a way that should facilitate the orientation of practitioners in this field. To this aim, papers from 2000 to date are categorized in terms of the applied algorithm and application domain, and a keyword analysis is also performed, to details the most promising topics in the field. What emerges is a consistent upward trend in the number of publications, with a spike of interest for unsupervised and especially deep learning techniques, which recorded a very high number of publications in the last five years. Concerning trends, along with consolidated research areas, recent topics that are growing in popularity were also discovered. Among these, the main ones are production planning and control and defect analysis, thus suggesting that in the years to come ML will become pervasive in many fields of operation management.",industry
10.1016/j.eswa.2021.114753,Journal,Expert Systems with Applications,scopus,2021-08-01,sciencedirect,A league-winner algorithm for defect classification in an industrial web inspection system,https://api.elsevier.com/content/abstract/scopus_id/85102641846,"This paper presents a modification to be added to multiclass classifiers, that improves their performance when classifying, in this case, defects appearing in polyethylene films. It aims to classify a new defect by confronting every defect type against each of the other types. In a simplified way, the type that results winner in more matches is the type that the defect belongs to. Different ways of implementing neural networks have been tested, using Gradient Descent and techniques for backpropagation. These techniques have been formally and understandably explained. In addition, a method based on decision trees has been included for comparison. Different issues related to the practical implementation of the detection and identification system within an installed production chain are addressed. The resulting system has been incorporated as a real inspection automatism in a polyethylene manufacturing line, and trained with defects previously obtained from the same line.",industry
10.1016/j.jclepro.2021.127385,Journal,Journal of Cleaner Production,scopus,2021-07-25,sciencedirect,Improving degradation of real wastewaters with self-heating magnetic nanocatalysts,https://api.elsevier.com/content/abstract/scopus_id/85105709482,"Industrial effluents contain a wide range of organic pollutants that present harmful effects on the environment and deprived communities with no access to clean water. As this organic matter is resistant to conventional treatments, Advanced Oxidation Processes (AOPs) have emerged as a suitable option to counteract these environmental challenges. Engineered iron oxide nanoparticles have been widely tested in AOPs catalysis, but their full potential as magnetic induction self-heating catalysts has not been studied yet on real and highly contaminated industrial wastewaters. In this study we have designed a self-heating catalyst with a finely tuned structure of small cores (10 nm) aggregates to develop multicore particles (40 nm) with high magnetic moment and high colloidal stability. This nanocatalyst, that can be separated by magnetic harvesting, is able to increase reaction temperatures (up to 90 °C at 1 mg/mL suspension in 5 min) under the action of alternating magnetic fields. This efficient heating was tested in the degradation of a model compound (methyl orange) and real wastewaters, such as leachate from a solid landfill (LIX) and colored wastewater from a textile industry (TIW). It was possible to increase reaction rates leading to a reduction of the chemical oxygen demand of 50 and 90%, for TIW and LIX. These high removal and degradation ability of the magnetic nanocatalyst was sustained with the formation of strong reactive oxygen species by a Fenton-like mechanism as proved by electron paramagnetic resonance. These findings represent an important advance for the industrial implementation of a scalable, non-toxic, self-heating catalysts that can certainly enhance AOP for wastewater treatment in a more sustainable and efficient way.",industry
10.1016/j.procs.2021.06.013,Conference Proceeding,Procedia Computer Science,scopus,2021-07-01,sciencedirect,Mathematical model of chemical process prediction for industrial safety risk assessment,https://api.elsevier.com/content/abstract/scopus_id/85112600838,The article presents a mathematical model of the functioning of the technological process of styrene production using neural network technologies. The use of a direct propagation neural network with a single hidden layer trained on an experimental sample is considered. An algorithm for forming a neural network is proposed. The model is implemented as a software module. The results of predicting the process of chemical production of styrene based on real data and recommendations for using the developed model in the process of assessing the industrial safety of particularly dangerous production processes are presented.,industry
10.1016/j.jmsy.2021.04.005,Journal,Journal of Manufacturing Systems,scopus,2021-07-01,sciencedirect,LearningADD: Machine learning based acoustic defect detection in factory automation,https://api.elsevier.com/content/abstract/scopus_id/85106283308,"Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061 s in 99 % probability.",industry
10.1016/j.tifs.2021.04.042,Journal,Trends in Food Science and Technology,scopus,2021-07-01,sciencedirect,Efficient extraction of deep image features using convolutional neural network (CNN) for applications in detecting and analysing complex food matrices,https://api.elsevier.com/content/abstract/scopus_id/85105814254,"Background
                  The development of techniques and methods for rapidly and reliably detecting and analysing food quality and safety products is of significance for the food industry. Traditional machine learning algorithms based on handcrafted features normally have poor performance due to their limited representation capacity for complex food characteristics. Recently, the convolutional neural network (CNN) emerges as an effective and potential tool for feature extraction, which is considered the most popular architecture of deep learning and has been increasingly applied for the detection and analysis of complex food matrices.
               
                  Scope and approach
                  In the current review, the structure of CNN, the method of feature extraction based on 1-D, 2-D and 3-D CNN models, and multi-feature aggregation methods are introduced. Applications of CNN as a depth feature extractor for detecting and analyzing complex food matrices are discussed, including meat and aquatic products, cereals and cereal products, fruits and vegetables, and others. In addition, data sources, model architecture and overall performance of CNN with other existing methods are compared, and trends of future studies on applying CNN for food detection and analysis are also highlighted.
               
                  Key findings and conclusions
                  CNN combined with nondestructive detection techniques and computer vision system show great potential for effectively and efficiently detecting and analysing complex food matrices, and the features based on CNN show better performance and outperform the features handcrafted or those extracted by machine learning algorithms. Although there still remains some challenges in using CNN, it is expected that CNN models will be deployed on mobile devices for real-time detection and analysis of food matrices in future.",industry
10.1016/j.ins.2021.01.013,Journal,Information Sciences,scopus,2021-07-01,sciencedirect,Attributed community search based on effective scoring function and elastic greedy method,https://api.elsevier.com/content/abstract/scopus_id/85101624086,"In recent years, with the proliferation of rich attribute information available for entities in real-world networks and the increasing demand for more personalized community searches, attributed community search (ACS), an upgraded version of the community search problem, has attracted great attention from the both academic and industry areas. Some algorithms have been proposed to solve this novel research problem. However, they have a deficiency in evaluating the quality of the attributed community structure, which may mislead them and discover less valuable structures. In this paper, we make up for this defect, and propose the SFEG algorithm to better solve the ACS problem. SFEG designs a more effective scoring function to measure the quality of the discovered attributed community structure, and presents an elastic greedy optimization method to quickly maximize the function value to determine the target community with a specific meaning. The extensive experiments conducted on the attributed graph datasets with ground-truth communities show that our algorithm significantly outperforms the state-of-the-art.",industry
10.1016/j.ymssp.2020.107510,Journal,Mechanical Systems and Signal Processing,scopus,2021-06-16,sciencedirect,Metric-based meta-learning model for few-shot fault diagnosis under multiple limited data conditions,https://api.elsevier.com/content/abstract/scopus_id/85100211264,"The real-world large industry has gradually become a data-rich environment with the development of information and sensor technology, making the technology of data-driven fault diagnosis acquire a thriving development and application. The success of these advanced methods depends on the assumption that enough labeled samples for each fault type are available. However, in some practical situations, it is extremely difficult to collect enough data, e.g., when the sudden catastrophic failure happens, only a few samples can be acquired before the system shuts down. This phenomenon leads to the few-shot fault diagnosis aiming at distinguishing the failure attribution accurately under very limited data conditions. In this paper, we propose a new approach, called Feature Space Metric-based Meta-learning Model (FSM3), to overcome the challenge of the few-shot fault diagnosis under multiple limited data conditions. Our method is a mixture of general supervised learning and episodic metric meta-learning, which will exploit both the attribute information from individual samples and the similarity information from sample groups. The experiment results demonstrate that our method outperforms a series of baseline methods on the 1-shot and 5-shot learning tasks of bearing and gearbox fault diagnosis across various limited data conditions. The time complexity and implementation difficulty have been analyzed to show that our method has relatively high feasibility. The feature embedding is visualized by t-SNE to investigate the effectiveness of our proposed model.",industry
10.1016/j.chemolab.2021.104314,Journal,Chemometrics and Intelligent Laboratory Systems,scopus,2021-06-15,sciencedirect,A scalable approach for the efficient segmentation of hyperspectral images,https://api.elsevier.com/content/abstract/scopus_id/85105360467,"The number of applications of hyperspectral imaging (HSI) is steadily increasing, as technology evolves and cameras become more affordable. However, the volume of data in a hyperspectral image is large (order of Gigabytes) and standard off-the-shelf algorithms for multi-channel image analysis cannot be readily applied, due to the prohibitive computational time and large memory requirements. Therefore, new scalable approaches are required to perform hyperspectral image analysis. In this article we address an efficient methodology for conducting Unsupervised Image Segmentation – one of the basic and most fundamental image analysis operations. In the methodology proposed, unsupervised segmentation is conducted after transforming the spectral and spatial dimensions of the raw hyperspectral image into a more compact representation using multivariate and multiresolution techniques. The clusters identified in the compact image representation are then used to train a discriminative classifier. The classifier is then adapted and transferred for application to the raw image, where it will efficiently label all the original pixels. With the proposed methodology, the computational expensive operations (unsupervised clustering and classifier learning) are minimized, whereas the efficient implementation of the classifier guarantees the analysis at the native resolution. The effectiveness of the proposed methodology was tested on a real case study considering an industrial hyperspectral image capturing the reflectance spectrum for several objects made of different unknown materials. A significant reduction in the computational cost was achieved without compromising the quality of the unsupervised segmentation, demonstrating the potential of the proposed approach.",industry
10.1016/j.addma.2021.101961,Journal,Additive Manufacturing,scopus,2021-06-01,sciencedirect,Deep representation learning for process variation management in laser powder bed fusion,https://api.elsevier.com/content/abstract/scopus_id/85105695571,"Laser Powder Bed Fusion (LPBF) is an additive manufacturing process where laser power is applied to fuse the spread powder and fabricate industrial parts in a layer by layer fashion. Despite its great promise in fabrication flexibility, print quality has long been a major barrier for its widespread implementation. Traditional offline post-manufacturing inspections to detect the defects in finished products are expensive and time-consuming and thus cannot be applied in real-time monitoring and control. In-situ monitoring methods by relying on the in-process sensor data, on the other hand, can provide viable alternatives to aid with the online detection of anomalies during the process. Given the crucial importance of melt pool characteristics to the quality of final products, this paper provides a framework to process the melt pool images by a configuration of Convolutional Auto-Encoder (CAE) neural networks. The network’s corresponding bottleneck layer learns a deep yet low-dimensional representation from melt pools while preserving the spatial correlation and complex features intrinsic in the images. As opposed to the manual annotation of data by X-ray imaging or destructive tests, an agglomerative clustering algorithm is applied to these representations to automatically extract the anomalies and annotate the data accordingly. A control charting scheme based on Hotelling’s T
                     2 and S
                     2 statistics is then developed to monitor the process’s stability by keeping track of the learned representations and residuals obtained from the reconstruction of original images. Testing the proposed methodology on the collected data from an experimental build demonstrates that the method can extract a set of complex features that are inextricable otherwise by using hand-crafted feature engineering methods. Moreover, through extensive numerical studies, it is shown that the proposed feature extraction and statistical process monitoring scheme is capable of detecting the anomalies in real-time with accuracy and F
                     1 score of about 95% and 82%, respectively.",industry
10.1016/j.compeleceng.2021.107121,Journal,Computers and Electrical Engineering,scopus,2021-06-01,sciencedirect,Efficient neural networks for edge devices,https://api.elsevier.com/content/abstract/scopus_id/85103242184,"Due to limited computation and storage resources of industrial internet of things (IoT) edge devices, many emerging intelligent industrial IoT applications based on deep neural networks (DNNs) heavily depend on cloud computing for computation and storage. However, cloud computing faces technical issues in long latency, poor reliability, and weak privacy, resulting in the need for on-device computation and storage. On-device computation is essential for many time-critical industrial IoT applications, which require real-time data processing. In this paper, we review three major research areas for on-device computation, specifically quantization, pruning, and network architecture design. The three techniques could enable a DNN model to be deployed on edge devices for real-time computation and storage, mainly due to the reduction of computation and space complexity. More importantly, these techniques could make DNNs applicable to industrial IoT devices.",industry
10.1016/j.renene.2021.03.008,Journal,Renewable Energy,scopus,2021-06-01,sciencedirect,Intelligent energy management based on SCADA system in a real Microgrid for smart building applications,https://api.elsevier.com/content/abstract/scopus_id/85102248554,"Energy management is one of the main challenges in Microgrids (MGs) applied to Smart Buildings (SBs). Hence, more studies are indispensable to consider both modeling and operating aspects to utilize the upcoming results of the system for the different applications. This paper presents a novel energy management architecture model based on complete Supervisory Control and Data Acquisition (SCADA) system duties in an educational building with an MG Laboratory (Lab) testbed, which is named LAMBDA at the Electrical and Energy Engineering Department of the Sapienza University of Rome. The LAMBDA MG Lab simulates in a small scale a SB and is connected with the DIAEE electrical network. LAMBDA MG is composed of a Photovoltaic generator (PV), a Battery Energy Storage System (BESS), a smart switchboard (SW), and different classified loads (critical, essential, and normal) some of which are manageable and controllable (lighting, air conditioning, smart plugs operating into the LAB). The aim of the LAMBDA implementation is making the DIAEE smart for energy saving purposes. In the LAMBDA Lab, the communication architecture consists in a complex of master/slave units and actuators carried out by two main international standards, Modbus (industrial serial standard for electrical and technical monitoring systems) and Konnex (an open standard for commercial and domestic building automation). Making the electrical department smart causes to reduce the required power from the main grid. Hence, to achieve the aims, results have been investigated in two modes. Initially, the real-time mode based on the SCADA system, which reveals real daily power consumption and production of different sources and loads. Next, the simulation part is assigned to shows the behavior of the main grid, loads and BESS charging and discharging based on energy management system. Finally, the proposed model has been examined in different scenarios and evaluated from the economic aspect.",industry
10.1016/j.cja.2020.09.011,Journal,Chinese Journal of Aeronautics,scopus,2021-06-01,sciencedirect,Framework and development of data-driven physics based model with application in dimensional accuracy prediction in pocket milling,https://api.elsevier.com/content/abstract/scopus_id/85097765922,"In the manufacturing of thin wall components for aerospace industry, apart from the side wall contour error, the Remaining Bottom Thickness Error (RBTE) for the thin-wall pocket component (e.g. rocket shell) is of the same importance but overlooked in current research. If the RBTE reduces by 30%, the weight reduction of the entire component will reach up to tens of kilograms while improving the dynamic balance performance of the large component. Current RBTE control requires the off-process measurement of limited discrete points on the component bottom to provide the reference value for compensation. This leads to incompleteness in the remaining bottom thickness control and redundant measurement in manufacturing. In this paper, the framework of data-driven physics based model is proposed and developed for the real-time prediction of critical quality for large components, which enables accurate prediction and compensation of RBTE value for the thin wall components. The physics based model considers the primary root cause, in terms of tool deflection and clamping stiffness induced Axial Material Removal Thickness (AMRT) variation, for the RBTE formation. And to incorporate the dynamic and inherent coupling of the complicated manufacturing system, the multi-feature fusion and machine learning algorithm, i.e. kernel Principal Component Analysis (kPCA) and kernel Support Vector Regression (kSVR), are incorporated with the physics based model. Therefore, the proposed data-driven physics based model combines both process mechanism and the system disturbance to achieve better prediction accuracy. The final verification experiment is implemented to validate the effectiveness of the proposed method for dimensional accuracy prediction in pocket milling, and the prediction accuracy of AMRT achieves 0.014 mm and 0.019 mm for straight and corner milling, respectively.",industry
10.1016/j.apenergy.2021.116688,Journal,Applied Energy,scopus,2021-05-15,sciencedirect,Advanced price forecasting in agent-based electricity market simulation,https://api.elsevier.com/content/abstract/scopus_id/85102042154,"Machine learning and agent-based modeling are two popular tools in energy research. In this article, we propose an innovative methodology that combines these methods. For this purpose, we develop an electricity price forecasting technique using artificial neural networks and integrate the novel approach into the established agent-based electricity market simulation model PowerACE. In a case study covering ten interconnected European countries and a time horizon from 2020 until 2050 at hourly resolution, we benchmark the new forecasting approach against a simpler linear regression model as well as a naive forecast. Contrary to most of the related literature, we also evaluate the statistical significance of the superiority of one approach over another by conducting Diebold–Mariano hypothesis tests. Our major results can be summarized as follows. Firstly, in contrast to real-world electricity price forecasts, we find the naive approach to perform very poorly when deployed model-endogenously (mean absolute percentage error 0.40–0.53). Secondly, although the linear regression performs reasonably well (mean absolute percentage error 0.17–0.32), it is outperformed by the neural network approach (mean absolute percentage error 0.17–0.21). Thirdly, the use of an additional classifier for outlier handling substantially improves the forecasting accuracy, particularly for the linear regression approach. Finally, the choice of the model-endogenous forecasting method has a clear impact on simulated electricity prices. This latter finding is particularly crucial since these prices are a major results of electricity market models.",industry
10.1016/j.comnet.2021.107955,Journal,Computer Networks,scopus,2021-05-08,sciencedirect,Elastic Computing Resource Virtualization Method for a Service-centric Industrial Internet of Things,https://api.elsevier.com/content/abstract/scopus_id/85102477910,"The industrial Internet of Things (IIoT) enables the interconnection of machines, devices, resources, and computing technologies to improve the reliability of manufacturing services. The role of Software-Defined Networks (SDNs) and Network Function Virtualization (NFV) are exploited in the IIoT environment to ensure effective management and computing resource utilization. Based on the SDN and NFV paradigms, this article introduces a novel elastic computing resource virtualization (ECRV) method to improve the flexibility of resource management in the IIoT. The need for virtualization is obtained by identifying the control and process platforms used in industrial task management. Support vector machine-based classification learning is used to achieve balanced identification, and prevents unnecessary distribution of limited resources, Support vector machine helps to retain flexibility in task control processes that use available industrial resources. By separating the process and control platforms, service dissemination is improved and backlogs in task processing are decreased. The proposed method could provide flexible virtualization and reduces the service response time and task failure.",industry
10.1016/j.eti.2021.101527,Journal,Environmental Technology and Innovation,scopus,2021-05-01,sciencedirect,Barriers to the digitalisation and innovation of Australian Smart Real Estate: A managerial perspective on the technology non-adoption,https://api.elsevier.com/content/abstract/scopus_id/85104052467,"The real estate sector brings a fortune to the global economy. But, presently, this sector is regressive and uses traditional methods and approaches. Therefore, it needs a technological transformation and innovation in line with the Industry 4.0 requirements to transform into smart real estate. However, it faces the barriers of disruptive digital technology (DDT) adoption and innovation that need effective management to enable such transformation. These barriers present managerial challenges that affect DDT adoption and innovation in smart real estate. The current study assesses these DDTs adoption and innovation barriers facing the Australian real estate sector from a managerial perspective. Based on a comprehensive review of 72 systematically retrieved and shortlisted articles, we identify 21 key barriers to digitalisation and innovation. The barriers are grouped into the technology-organisation-external environment (TOE) categories using a Fault tree. Data is collected from 102 real estate and property managers to rate and rank the identified barriers. The results show that most of the respondents are aware of the DDTs and reported AI (22.5% of respondents), big data (12.75%) and VR (12.75%) as the most critical technologies not adopted so far due to costs, organisation policies, awareness, reluctance, user demand, tech integration, government support and funding. Overall, the highest barrier (risk) scores are observed for high costs of software and hardware (T1), high complexity of the selected technology dissemination system (T2) and lack of government incentives, R&D support, policies, regulations and standards (E1). Among the TOE categories, as evident from the fault tree analysis, the highest percentage of failure to adopt the DDT is attributed to E1 in the environmental group. For the technological group, the highest failure reason is attributed to T2. And for the organisational group, the barrier with the highest failure chances for DDT adoption is the lack of organisational willingness to invest in digital marketing (O4). These barriers must be addressed to pave the way for DDT adoption and innovation in the Australian real estate sector and move towards smart real estate.",industry
10.1016/j.eswa.2020.114399,Journal,Expert Systems with Applications,scopus,2021-05-01,sciencedirect,A reinforcement learning-based algorithm for the aircraft maintenance routing problem,https://api.elsevier.com/content/abstract/scopus_id/85098682553,"With recent developments in the airline industry worldwide, the competition among the industry has increased largely with many key players in the market. In order to generate profits, the industry has paid much attention to generate optimal routes that are maintenance feasible. The main aim of operational aircraft maintenance routing problem (OAMRP) is to generate these optimal routes for each aircraft that are maintenance feasible and follow the constraints defined by the Federal Aviation Administration (FAA). In this paper, the OAMRP is studied with two main objectives. First, to propose a formulation of a network flow-based Integer Linear Programming (ILP) framework for the OAMRP that considers three main maintenance constraints simultaneously: maximum flying-hour, limit on the number of take-offs between two consecutive maintenance checks and the work-force capacity. Second, to develop a new reinforcement learning-based algorithm which can be used to solve the problem, quickly and efficiently, as compared to commonly available optimization software. Finally, the evaluation of the proposed algorithm on real case datasets obtained from a major airline located in the Middle East verifies that the algorithm generates high-quality solutions quickly for both medium and large-scale flight schedule dataset.",industry
10.1016/j.jmsy.2021.02.012,Journal,Journal of Manufacturing Systems,scopus,2021-04-01,sciencedirect,Robust diagnosis with high protection to gas turbine failures identification based on a fuzzy neuro inference monitoring approach,https://api.elsevier.com/content/abstract/scopus_id/85101807612,"Modern industry requires the development of new monitoring and diagnostic procedures, which enable the detection, localization, and isolation of faults. For sustainable solutions in terms of operational safety and availability, while bringing out zero accidents, zero downtime, and zero faults, for a trend acting on environmental issues. Towards this development, this work proposes solutions for the monitoring of gas turbines and their real-time implementation, in order to approximate and predict the degradation of the components of this system, by an approach of faults detection and isolation, based on an adaptive neural-fuzzy inference system. This will develop a reliable approach to maintain and monitor gas turbines, in case of failure or accident to prevent in real-time and makes it possible to achieve high power with efficiency and small footprint with High performance by operating this rotating machine. However, the application of the Adaptive Neuro-Fuzzy Inference System Observer-Based Approach, makes it possible to increase the life of the examined turbine and keep better reliability for their monitoring system and satisfy the techno-economic and environmental performance impacts. For the purpose of controlling failures and the occurrence of turbine system malfunctions, and avoiding their consequences on the safety and productivity of the installation.",industry
10.1016/j.enconman.2021.113856,Journal,Energy Conversion and Management,scopus,2021-04-01,sciencedirect,The mutual benefits of renewables and carbon capture: Achieved by an artificial intelligent scheduling strategy,https://api.elsevier.com/content/abstract/scopus_id/85101129959,"Renewable power and carbon capture are key technologies to transfer the power industry into low carbon generation. Renewables have been developed fast, however, the intermittent nature has imposed higher requirement for the flexibility of the power grid. Retrofitting carbon capture technologies to existing fossil-fuel fired power plants is an important solution to avoid the “lock-in” of emissions, but the high operating costs hinders their large scale application. The coexistence of renewable power and carbon capture opens up a new avenue that the deployment of carbon capture can provide additional flexibility for better accommodation of renewable power while excess renewables can be used to reduce the operating costs of carbon capture. To this end, this paper proposes an artificial intelligence based optimal scheduling strategy for the power plant-carbon capture system in the context of renewable power penetration to show that the mutual benefits between carbon capture and renewable power can be achieved when the carbon capture process is made fully adjustable. An artificial intelligent deep belief neural network is used to reflect the complex interactions between carbon, heat and electricity within the power plant carbon capture system. Multiple operating goals are considered in the scheduling such as minimizing the operating costs, renewable power curtailment and carbon emission, and the particle swarm heuristic optimization is employed to find the optimal solution. The impacts of carbon capture constraint mode, carbon emission penalty coefficient, carbon dioxide production constraints and renewable power installed capacity are investigated to provide broader insight on the potential benefit of carbon capture in future low-carbon energy system. A case study using real world data of weather condition and load demand shows that renewable power curtailment can be reduced by 51% with the integration of post-combustion capture systems and 35% of total carbon emission are captured by the use of excess renewable power through optimal scheduling. This paper points out a new way of using artificial intelligent technologies to coordinate the couplings between carbon and electricity for efficient and environmentally friendly operation of future low-carbon energy system.",industry
10.1016/j.asoc.2020.107069,Journal,Applied Soft Computing,scopus,2021-04-01,sciencedirect,Deep learning feature exploration for Android malware detection,https://api.elsevier.com/content/abstract/scopus_id/85098947132,"Android mobile devices and applications are widely deployed and used in industry and smart city. Malware detection is one of the most powerful and effective approaches to guarantee security of Android systems, especially for industrial platform and smart city. Recently, researches using machine learning-based techniques for Android malware detection increased rapidly. Nevertheless, most of the appeared approaches have to perform feature analysis and selection, so-called feature engineering, which is time-consuming and relies on artificial experience. To solve the inefficiency problem of feature engineering, we propose TC-Droid, an automatic framework for Android malware detection based on text classification method. The core idea of TC-Droid is derived from the field of text classification. TC-Droid feeds on the text sequence of APPs analysis reports generated by AndroPyTool, applies a convolutional neural network (CNN) to explore significant information (or knowledge) under original report text, instead of manual feature engineering. In an evaluation with different number of real-world samples, TC-Droid outperforms state-of-the-art model (Drebin) and several classic models (NB, LR, KNN, RF) as well. With multiple experimental settings and corresponding comparisons, TC-Droid achieves effective and flexible performance in Android malware detection task.",industry
10.1016/j.neucom.2020.10.097,Journal,Neurocomputing,scopus,2021-03-21,sciencedirect,3D-RVP: A method for 3D object reconstruction from a single depth view using voxel and point,https://api.elsevier.com/content/abstract/scopus_id/85097471582,"Three-dimensional object reconstruction technology has a wide range of applications such as augment reality, virtual reality, industrial manufacturing and intelligent robotics. Although deep learning-based 3D object reconstruction technology has developed rapidly in recent years, there remain important problems to be solved. One of them is that the resolution of reconstructed 3D models is hard to improve because of the limitation of memory and computational efficiency when deployed on resource-limited devices. In this paper, we propose 3D-RVP to reconstruct a complete and accurate 3D geometry from a single depth view, where R, V and P represent Reconstruction, Voxel and Point, respectively. It is a novel two-stage method that combines a 3D encoder-decoder network with a point prediction network. In the first stage, we propose a 3D encoder-decoder network with residual learning to output coarse prediction results. In the second stage, we propose an iterative subdivision algorithm to predict the labels of adaptively selected points. The proposed method can output high-resolution 3D models by increasing a small number of parameters. Experiments are conducted on widely used benchmarks of a ShapeNet dataset in which four categories of models are selected to test the performance of neural networks. Experimental results show that our proposed method outperforms the state-of-the-arts, and achieves about 
                        
                           2.7
                           %
                        
                      improvement in terms of the intersection-over-union metric.",industry
10.1016/j.fbp.2020.12.009,Journal,Food and Bioproducts Processing,scopus,2021-03-01,sciencedirect,Study of Galactooligosaccharides production from dairy waste by FTIR and chemometrics as Process Analytical Technology,https://api.elsevier.com/content/abstract/scopus_id/85099356128,"Galactooligosaccharides (GOS) production from whey, a relevant by-product of dairy industry, answers to the Circular Economy principle of extending the life cycle of products. Indeed, it allows the reuse of dairy waste to produce prebiotics to be used in functional food preparations. For this purpose, the effective monitoring of GOS production should be performed in real time and by environmentally friendly techniques. Thus, FTIR spectroscopy, combined with different chemometric approaches, has been tested to assess a Process Analytical Technology to follow GOS production from cheese whey. Partial Least Square regression models were reliable for lactose, glucose and galactose determination (Root Mean Square Error of Prediction of 21.9, 11.1 and 12.4 mg mL−1, respectively). Furthermore, Multivariate Curve Resolution – Alternating Least Square models were proposed to describe trends of the reaction components along the process being an interesting alternative to chromatographic determinations. The real time implementation of the proposed approach will provide the dairy industry with a reliable and green Process Analytical Technology for dairy waste reallocation, avoiding sample pre-processing, large use of organic solvents and long times of analysis.",industry
10.1016/j.future.2020.10.031,Journal,Future Generation Computer Systems,scopus,2021-03-01,sciencedirect,DISCERNER: Dynamic selection of resource manager in hyper-scale cloud-computing data centres,https://api.elsevier.com/content/abstract/scopus_id/85096157784,"Data centres constitute the engine of the Internet, and run a major portion of large web and mobile applications, content delivery and sharing platforms, and Cloud-computing business models. The high performance of such infrastructures is therefore critical for their correct functioning. This work focuses on the improvement of data-centre performance by dynamically switching the main data-centre governance software system: the resource manager. Instead of focusing on the development of new resource-managing models as soon as new workloads and patterns appear, we propose DISCERNER, a decision-theory model that can learn from numerous data-centre execution logs to determine which existing resource-managing model may optimise the overall performance for a given time period. Such a decision-theory system employs a classic machine-learning classifier to make real-time decisions based on past execution logs and on the current data-centre operational situation. A set of extensive and industry-guided experiments has been simulated by a validated data-centre simulation tool. The results obtained show that the values of key performance indicators may be improved by at least 20% in realistic scenarios.",industry
10.1016/j.future.2020.10.018,Journal,Future Generation Computer Systems,scopus,2021-03-01,sciencedirect,Large-scale online multi-view graph neural network and applications,https://api.elsevier.com/content/abstract/scopus_id/85095762066,"Recently popularized Graph Neural Network (GNN) has been attaching great attention along with its successful industry applications. This paper focuses on two challenges traditional GNN frameworks face: (i) most of them are transductive and mainly concentrate on homogeneous networks considering single typed nodes and edges; (ii) they are difficult to handle the real-time changing network structures as well as scale to big graph data. To address these issues, a novel attention-based Heterogeneous Multi-view Graph Neural Network (aHMGNN) solution is introduced. aHMGNN models a more intricate heterogeneous multi-view network, where various node and edge types co-exist and each of these objects also contain specific attributes. It is end-to-end, and two stages are designed for node embeddings learning and multi-typed node and edge representations fusion, respectively. Experimental studies on large-scale spam detection and link prediction tasks clearly verify the efficiency and effectiveness of our proposed aHMGNN. Furthermore, we have implemented our approach in one of the largest e-commerce platforms which further verifies that aHMGNN is arguably promising and scalable in real-world applications.",industry
10.1016/j.knosys.2020.106679,Journal,Knowledge-Based Systems,scopus,2021-02-15,sciencedirect,Federated learning for machinery fault diagnosis with dynamic validation and self-supervision,https://api.elsevier.com/content/abstract/scopus_id/85098734354,"Intelligent data-driven machinery fault diagnosis methods have been successfully and popularly developed in the past years. While promising diagnostic performance has been achieved, the existing methods generally require large amounts of high-quality supervised data for training, which are mostly difficult and expensive to collect in real industries. Therefore, it is motivated that the distributed data of multiple clients can be integrated and exploited to build a powerful data-driven model. However, that basically requires data sharing among different users, and is not preferred in most industrial cases due to potential conflict of interests. In order to address the data island problem, a federated learning method for machinery fault diagnosis is proposed in this paper. Model training is locally implemented within each participated client, and a self-supervised learning scheme is proposed to enhance the learning performance. The server aggregates the locally updated models in each training round under the dynamic validation scheme, and a global fault diagnosis model can be established. Only the models are mutually communicated rather than the data, which ensures data privacy among different clients. The experiments on two datasets suggest the proposed method offers a promising approach on confidential decentralized learning.",industry
10.1016/j.abb.2020.108730,Journal,Archives of Biochemistry and Biophysics,scopus,2021-02-15,sciencedirect,Artificial intelligence in the early stages of drug discovery,https://api.elsevier.com/content/abstract/scopus_id/85098095696,"Although the use of computational methods within the pharmaceutical industry is well established, there is an urgent need for new approaches that can improve and optimize the pipeline of drug discovery and development. In spite of the fact that there is no unique solution for this need for innovation, there has recently been a strong interest in the use of Artificial Intelligence for this purpose. As a matter of fact, not only there have been major contributions from the scientific community in this respect, but there has also been a growing partnership between the pharmaceutical industry and Artificial Intelligence companies. Beyond these contributions and efforts there is an underlying question, which we intend to discuss in this review: can the intrinsic difficulties within the drug discovery process be overcome with the implementation of Artificial Intelligence? While this is an open question, in this work we will focus on the advantages that these algorithms provide over the traditional methods in the context of early drug discovery.",industry
10.1016/j.patter.2020.100195,Journal,Patterns,scopus,2021-02-12,sciencedirect,Topic classification of electric vehicle consumer experiences with transformer-based deep learning,https://api.elsevier.com/content/abstract/scopus_id/85100638713,"The transportation sector is a major contributor to greenhouse gas (GHG) emissions and is a driver of adverse health effects globally. Increasingly, government policies have promoted the adoption of electric vehicles (EVs) as a solution to mitigate GHG emissions. However, government analysts have failed to fully utilize consumer data in decisions related to charging infrastructure. This is because a large share of EV data is unstructured text, which presents challenges for data discovery. In this article, we deploy advances in transformer-based deep learning to discover topics of attention in a nationally representative sample of user reviews. We report classification accuracies greater than 91% (F1 scores of 0.83), outperforming previously leading algorithms in this domain. We describe applications of these deep learning models for public policy analysis and large-scale implementation. This capability can boost intelligence for the EV charging market, which is expected to grow to US$27.6 billion by 2027.",industry
10.1016/j.ijepes.2021.107505,Journal,International Journal of Electrical Power and Energy Systems,scopus,2021-02-01,sciencedirect,Global sensitivity analysis for a real-time electricity market forecast by a machine learning approach: A case study of Mexico,https://api.elsevier.com/content/abstract/scopus_id/85113278481,"The study presents the hybridization of global sensitivity analysis with data-driven techniques to evaluate the Mexican electricity market interaction and assess the impact of individual parameters concerning locational marginal prices. The study case pertains to Yucatan, Mexico's electricity grid and market characteristics. A comparison of three artificial intelligence techniques in the electricity market is presented to forecast electricity prices in real-time market conditions. The study contemplates exogenous input parameters classified as regional, operational, meteorological, and economic indicators. A sensitivity analysis was carried out to the model with the best performance of the Artificial Intelligence techniques. The results showed that the impact of the variables fluctuates according to market and consumption conditions. In this study, the most relevant variables were electricity generation (17.06%), fossil fuel costs (natural gas 12.54% and diesel 8.63%), load zone (11.17%), and the day of the year (8.51%). From the qualitative point of view, the complex behavior of the parameters was analyzed; moreover, the quantitative results weighted the relevance of the variables in the Locational Marginal Prices. The meteorological and economic parameters allow assessing the environment where it interacts and serves as an instrument for decision-making in the planning of the energy sector. The presented methodology can be implemented as an alternative tool for market participants to analyze electricity prices.",industry
10.1016/j.jmapro.2020.12.050,Journal,Journal of Manufacturing Processes,scopus,2021-02-01,sciencedirect,Online tool condition monitoring for ultrasonic metal welding via sensor fusion and machine learning,https://api.elsevier.com/content/abstract/scopus_id/85099501543,"In ultrasonic metal welding (UMW), tool wear significantly affects the weld quality and tool maintenance constitutes a substantial part of production cost. Thus, tool condition monitoring (TCM) is crucial for UMW. Despite extensive literature focusing on TCM for other manufacturing processes, limited studies are available on TCM for UMW. Existing TCM methods for UMW require offline high-resolution measurement of tool surface profiles, which leads to undesirable production downtime and delayed decision-making. This paper proposes a completely online TCM system for UMW using sensor fusion and machine learning (ML) techniques. A data acquisition (DAQ) system is designed and implemented to obtain in-situ sensing signals during welding processes. A large feature pool is then extracted from the sensing signals. A subset of features are selected and subsequently used by ML-based classification models. A variety of classification models are trained, validated, and tested using experimental data. The best-performing classification models can achieve close to 100% classification accuracy for both training and test datasets. The proposed TCM system not only provides real-time TCM for UMW but also can support optimal decision-making in tool maintenance. The TCM system can be extended to predict remaining useful life (RUL) of tools and integrated with a controller to adjust welding parameters accordingly.",industry
10.1016/j.micpro.2020.103628,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Consumer decision-making and smart logistics planning based on FPGA and convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85097718891,"In the fourth Industrial Revolution, cost-effective planning and rational management were the key to the success of the revolution. This paper mainly studies the development and application of models in machine learning technology. The abnormal activities monitored in real time are rectified so that the customer's electronic orders can be displayed through the support of big data, thus laying the foundation for the development of intelligent logistics. Under the data system, an exception model is created and classified and regressed. In this model, the security and stability of customer orders in the network can be automatically detected, and the abnormal data can be analyzed and evaluated. Unusual circumstances of this kind need to be in an intelligent logistics environment, and delivery tasks must be called intuitive for special care. Early detection of abnormal order events is expected to improve the accuracy of delivery planning. To enable new technical solutions, the logistics industry and economic decision-makers often lack the IT background and expertise needed to start developing new systems and technical solutions. Evaluate the benefits of using. Implementation and integration complexity is seen as one of the three major obstacles to the success of the IoT above. This is by hindering long-term investment in new technologies from slowing down digitization.",industry
10.1016/j.micpro.2020.103594,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Enterprise financial risk management platform based on 5 G mobile communication and embedded system,https://api.elsevier.com/content/abstract/scopus_id/85097578772,"5 G technology has been applied to the financial sector. As a mortgage and supervision of sensors, network cameras, mobile device to improve the financial performance of real-time data generated by the data, bank loan credit risk management has been used. There are many risks of financial credit in modern society, the most important of which is the financial danger on mobile Internet. In the case of mobile phone payment popularity, financial risk has also greatly improved. This makes the traditional statistics and models can not fully meet the needs of the development of modern society. Bank credit risk has also improved to some extent. Therefore, there is a practical need for a more robust risk prediction model of artificial intelligence to predict the default behavior with good accuracy and competency-based big data analytics. This paper presents data mining method optimization and 5 G mobile communications and embedded systems commercial banks, based on financial risk management. It is safe to protect personal privacy, consider these requirements 5 G system design. To successfully connect to the ability to make money, telecom service providers need to ask their players to match their products and the industry. In addition to simple connections, they need a unified high-level function, such as coordination of network resources, analytical capabilities, and automated business and operations. Mob ileum provides Business Assurance Analytics to improve and develop a strong customer value proposition during 5 G technologies deployment. Experimental results show that the risk management models have fast convergence, powerful forecasting capabilities, and effectively perform screening default behavior. Simultaneously, distributed significant data clusters to achieve significantly reduce the processing time model training and testing.",industry
10.1016/j.apenergy.2020.116049,Journal,Applied Energy,scopus,2021-02-01,sciencedirect,Adaptive prognostics in a controlled energy conversion process based on long- and short-term predictors,https://api.elsevier.com/content/abstract/scopus_id/85097470918,"The pulp and paper industry is a fundamental sector of the economy of many countries. However, this sector requires real collaboration and initiatives from stakeholders to reduce its significant consumption of energy and emission of greenhouse gases. Heat exchangers are examples of equipment in pulp mills that are subjected to undesirable and complex phenomena such as evolution of fouling over time, which leads to inefficiency in terms of energy consumption and unplanned shutdowns, resulting in ineffective maintenance strategies and production costs. Therefore, there is a clear need to develop an accurate predictive maintenance tool that helps mill operators avoid such situations. It is necessary for that tool to effectively track the fouling evolution level and, based on it, deploy a reliable prognostics approach to estimate more accurately the time-to-clean of this equipment. This study presents a new hybrid prognostics approach for fouling prediction in heat exchangers. The proposed approach relies on the fusion of information of different prediction horizons to estimate the time-to-clean. Employing long short-term memory, it allows adaptation of long-term predictions by accurate short-term predictions using multiple non-linear auto-regressive exogenous models. This fusion not only captures the changes in degradation trend over time, but also ensures a good accuracy of prognostics results in both the short- and long-term horizons for planning maintenance actions. The effectiveness of the proposed approach was successfully proven on real industrial data collected from a pulp mill heat exchanger located in Canada.",industry
10.1016/j.apenergy.2020.116297,Journal,Applied Energy,scopus,2021-02-01,sciencedirect,An Echo State Network for fuel cell lifetime prediction under a dynamic micro-cogeneration load profile,https://api.elsevier.com/content/abstract/scopus_id/85097452614,"Improving Proton Exchange Membrane Fuel Cell durability is a key that paves the way to its large scale industrial deployment. During the last five years, the prognostics discipline emerged as an interesting field for Proton Exchange Membrane Fuel Cell state of health prediction and lifetime estimation. The information provided by the prognostic module is crucial for optimizing the control strategy to extend the fuel cell lifetime. In this paper, an approach based on Echo State Network for fuel cell prognostics under a variable load is developed. The novelty of this paper is to perform prognostics under a variable load profile without prior knowledge of this latter. Two solutions are developed in this work. The first one consists of evaluating the remaining useful lifetime under a repeated load cycle. The second one is based on using Markov chains to generate estimations of the future load profile, allowing thus to overcome the need of real future load profile prior knowledge. Both proposed solutions give accurate prediction results of proton exchange membrane fuel cell remaining useful lifetime, with low uncertainties.",industry
10.1016/j.micpro.2020.103579,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Development of cultural tourism platform based on FPGA and convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85097346419,"Data mining can be described as a typical analysis of large datasets to investigate early unknown types, styles, and interpersonal relationships to generate the right decision information. It improves their markets and today to maintain control over whether these companies are forced into the data mining tools and technologies they use to develop and manage tourism products and services in the market. It is falling out of the favorable situation of the travel and tourism industry. Objective work is to provide and display its application in data mining and tourism. Advances in mobile technology provide an opportunity to obtain real-time information of travelers, such as time and space behavior, at the destination they visit. This study analyzed a large-scale mobile phone data set to capture the mobile phone traces of international tourists who visited South Korea. We adopt the trajectory data mining method to understand tourism activities’ spatial structure in three different destinations. The research reveals tourist destinations and multiple “hot spots” (or popular areas) that interact spatially in these places through spatial cluster analysis and sequential pattern mining. Therefore, this article provides the planning of spatial model destinations to integrate important tourism influences, which is based on tourism design. The proposed system is modelled in Field Programmable Gate Array (FPGA) using Xilinx software.",industry
10.1016/j.micpro.2020.103318,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Enterprise financial cost management platform based on FPGA and neural network,https://api.elsevier.com/content/abstract/scopus_id/85094858518,"At present, the domestic costs of most construction companies are relatively scattered with the cost data of various business agents. Unless it is controlled by an experienced manager, decision-makers cannot have the real-time dynamic cost of a project. In the information age, it is of vital importance to use the information to control the cost of construction projects dynamically. Cost management and the establishment of an information platform are ways to control the platform integrated cost data, operators, computer software and hardware, and corresponding method information, and its core is cost data information. The place for financial cost analysis and decision making is a conceptually rich field where information is a commercial product which is complicated, extensive, and invaluable. In this model, first,a set of extracts from the macro-credit feature space is designed and then, FPGA and neural network (FPGA, NN) models for credit evaluation is built based on these indicators, eventually it is applied scientifically, and reasonably, practically. Several state credit metrics are randomly selected. Our model shows applications that are both practical and competent. Using this model, authorities can analyze local credit conditions, allowing investors to make wise decisions to invest while saving on operating and credit costs. Most importantly, this model can help impulsive local government leaders, businesses, and even everyone to enhance competitiveness and capacities of attractive regions, thereby foster a good atmosphere for a credit culture.",industry
10.1016/j.micpro.2020.103301,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,IoT enabled cancer prediction system to enhance the authentication and security using cloud computing,https://api.elsevier.com/content/abstract/scopus_id/85094168107,"In recent days, Internet of Things, Cloud Computing, Deep learning, Machine learning and Artificial Intelligence are considered to be an emerging technologies to solve variety of real world problems. These techniques are importantly applied in various fields such as healthcare systems, transportation systems, agriculture and smart cities to produce fruitful results for number of issues in today's environment. This research work focuses on one such application in the field of IoT together with cloud computing. More number of sensors that are deployed in human body is used to collect patient related data such as deviation in body temperature and others which leads to variation in blood cells that turned to be cancerous cells. Main intention of this work is design a cancer prediction system using Internet of Things upon extracting the details of blood results to test whether it is normal or abnormal. In addition to this, encryption is done on the blood results of cancer affected patient and store it in cloud for quick reference through Internet for the doctor or healthcare nurse to handle the patient data secretly. This research work concentrates on enhancing the health care computations and processing. It provides a framework to enhance the performance of the existing health care industry across the globe. As the entire medical data has to be saved in cloud, the traditional medical treatment limitations can be overcome. Encryption and decryption is done using AES algorithm in order to provide authentication and security in handling cancer patients. The main focus is to handle healthcare data effectively for the patient when they are away from the home town since the needed cancer treatment details are stored in cloud. The task completion time is greatly reduce from 400 to 160  by using VMs. CloudSim gives an adaptable simulation structure that empowers displaying and reproduced results.",industry
10.1016/j.isatra.2020.08.024,Journal,ISA Transactions,scopus,2021-02-01,sciencedirect,Data-driven adaptive modeling method for industrial processes and its application in flotation reagent control,https://api.elsevier.com/content/abstract/scopus_id/85089898141,"In real industrial processes, new process “excitation” patterns that largely deviate from previously collected training data will appear due to disturbances caused by process inputs. To reduce model mismatch, it is important for a data-driven process model to adapt to new process “excitation” patterns. Although efforts have been devoted to developing adaptive process models to deal with this problem, few studies have attempted to develop an adaptive process model that can incrementally learn new process “excitation” patterns without performance degradation on old patterns. In this study, efforts are devoted to enabling data-driven process models with incremental learning ability. First, a novel incremental learning method is proposed for process model updating. Second, an adaptive neural network process model is developed based on the novel incremental learning method. Third, a nonlinear model predictive control based on the adaptive process model is implemented and applied for flotation reagent control. Experiments based on historical data provide evidence that the newly developed adaptive process model can accommodate new process “excitation” patterns and preserve its performance on old patterns. Furthermore, industry experiments carried out in a real-world lead–zinc froth flotation plant provide industrial evidence and show that the newly designed controller is promising for practical flotation reagent control.",industry
10.1016/j.rcim.2020.102029,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2021-02-01,sciencedirect,Towards manufacturing robotics accuracy degradation assessment: A vision-based data-driven implementation,https://api.elsevier.com/content/abstract/scopus_id/85088120602,"In this manuscript we report on a vision-based data-driven methodology for industrial robot health assessment. We provide an experimental evidence of the usefulness of our methodology on a system comprised of a 6-axis industrial robot, two monocular cameras and five binary squared fiducial markers. The fiducial marker system permits to accurately track the deviation of the end-effector along a fixed non-trivial trajectory. Moreover, we monitor the trajectory deflection using three gradually increasing weights attached to the end-effector. When the robot is loaded with the maximum allowed payload, a deviation of 0.77mm is identified in the Z-coordinate of the end-effector. Tracing trajectory information, we train five supervised learning regression models. Such models are afterwards used to predict the deviation of the end-effector, using the pose estimation provided by the visual tracking system. As a result of this study, we show that this procedure is a stable, robust, rigorous and reliable tool for robot trajectory deviation estimation and it even allows to identify the mechanical element producing non-kinematic errors.",industry
10.1016/j.apenergy.2021.118127,Journal,Applied Energy,scopus,2021-01-01,sciencedirect,Data-driven control of room temperature and bidirectional EV charging using deep reinforcement learning: Simulations and experiments,https://api.elsevier.com/content/abstract/scopus_id/85118721393,"The control of modern buildings is a complex multi-loop problem due to the integration of renewable energy generation, storage devices, and electric vehicles (EVs). Additionally, it is a complex multi-criteria problem due to the need to optimize overall energy use while satisfying users’ comfort. Both conventional rule-based (RB) controllers, which are difficult to apply in multi-loop settings, and advanced model-based controllers, which require an accurate building model, cannot fulfil the requirements of the building automation industry to solve this problem optimally at low development and commissioning costs. This work presents a fully data-driven pipeline to obtain an optimal control policy from historical building and weather data, thus avoiding the need for complex physics-based modelling. We demonstrate the potential of this method by jointly controlling a room temperature and an EV to minimize the cost of electricity while retaining the comfort of the occupants. We model the room temperature with a recurrent neural network and use it as a simulation environment to learn a deep reinforcement learning (DRL) control policy. It achieves on average 17% energy savings and 19% better comfort satisfaction than a standard RB room temperature controller. When a bidirectional EV is connected additionally and a two-tariff electricity pricing is applied, it successfully leverages the battery and decreases the overall cost of electricity. Finally, we deployed it on a real building, where it achieved up to 30% energy savings while maintaining similar comfort levels compared to a conventional RB room temperature controller.",industry
10.1016/j.promfg.2021.06.086,Conference Proceeding,Procedia Manufacturing,scopus,2021-01-01,sciencedirect,Pervasive environmental sensing for Industry 4.0 as an educational tool,https://api.elsevier.com/content/abstract/scopus_id/85117930435,"The reduced cost of implementing pervasive industrial sensing networks enables universities to incorporate these tools in engineering curricula. They provide engineering students from increasingly computerized backgrounds, such as mechanical and automotive engineering, the opportunity to work alongside students from technical schools who bring different skill sets than what students may be used to, synthesize historical data, and drive the sensing system’s physical system design and implementation. This paper outlines this convergent curriculum’s initial implementation stage, including the wireless environmental sensing Internet of Things (IoT) network, focusing on laboratory environmental sensing. Students placing many sensors around the lab and on equipment generates a wealth of real-time and historical data for use in the classroom and provides them a tangible example of learning to measure the world around them. This setup parallels the current varied Industry 4.0 state of the manufacturing industry, where Big Data exists but is underutilized, and where additional sensors and intelligent machine data streams are added each year. Students in each class are given a defined portion of a broader roadmap to a fully instrumented and intelligent laboratory environment. In the first step, student-programmed environmental sensors were placed around the lab and provide temperature, humidity, pressure, and gas mixture measures every five minutes. Classroom use of the aggregated data includes visualizing the laboratory and essential equipment’s current status using a Microsoft PowerBI dashboard and historical data visualization and analysis through trend forecasting and outlier detection in Python JupyterLab notebooks. The IoT system’s installation also provided an infrastructure for further study of future student-designed IoT projects.",industry
10.1016/j.orp.2021.100204,Journal,Operations Research Perspectives,scopus,2021-01-01,sciencedirect,A review of approximate dynamic programming applications within military operations research,https://api.elsevier.com/content/abstract/scopus_id/85117385700,"Sequences of decisions that occur under uncertainty arise in a variety of settings, including transportation, communication networks, finance, defence, etc. The classic approach to find an optimal decision policy for a sequential decision problem is dynamic programming; however its usefulness is limited due to the curse of dimensionality and the curse of modelling, and thus many real-world applications require an alternative approach. Within operations research, over the last 25 years the use of Approximate Dynamic Programming (ADP), known as reinforcement learning in many disciplines, to solve these types of problems has increased in popularity. These efforts have resulted in the successful deployment of ADP-generated decision policies for driver scheduling in the trucking industry, locomotive planning and management, and managing high-value spare parts in manufacturing. In this article we present the first review of applications of ADP within a defence context, specifically focusing on those which provide decision support to military or civilian leadership. This article’s main contributions are twofold. First, we review 18 decision support applications, spanning the spectrum of force development, generation, and employment, that use an ADP-based strategy and for each highlight how its ADP algorithm was designed, evaluated, and the results achieved. Second, based on the trends and gaps identified we discuss five topics relevant to applying ADP to decision support problems within defence: the classes of problems studied; best practices to evaluate ADP-generated policies; advantages of designing policies that are incremental versus complete overhauls when compared to currently practiced policies; the robustness of policies as scenarios change, such as a shift from high to low intensity conflict; and sequential decision problems not yet studied within defence that may benefit from ADP.",industry
10.1016/j.procs.2021.08.095,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,SODA: A real-time simulation framework for object detection and analysis in smart manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85116946450,"For modern manufacturing firms, automation has already become a norm but constantly needs to be improved as firms still face strong demand to increase their productivity. This can be achieved by reducing dependability on manpower, reaching lean and even unmanned production and this is where some of the standards of Industry 4.0 come in useful, not to mention: Machine Vision, Image Recognition or Machine Learning. In our paper, we present SODA – our approach to build a flexible ML and AI enabled framework for object detection, analysis, and simulation. The framework is designed to support a development process of solutions requiring real-time analysis of images of different types of moving objects on a conveyor belt. In our work we discuss architectural challenges of the developed framework as well as the basic components of the system. We do also provide information on how to use the framework and present a sample implementation of an actual system employing some of the machine learning methods.",industry
10.1016/j.procs.2021.09.013,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Exploiting supervised machine learning for driver detection in a real-world environment,https://api.elsevier.com/content/abstract/scopus_id/85116888149,"The proliferation of info-entertainment systems in today’s vehicles has provided a really cheap and easy-to-deploy platform with the ability to gather information about the vehicle under analysis. Ultra-response connectivity networks with a latency below 10 milliseconds are providing the perfect infrastructure in which this information can be sent to improve safety and security. With the purpose of providing an architecture to increase safety and security in an automotive context, we in this paper propose a method for detecting the driver in real-time exploiting supervised machine learning techniques. The experimental analysis performed on real-world data shows that the proposed method obtains encouraging results.",industry
10.1016/j.procs.2021.09.233,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,A note on the applications of artificial intelligence in the hospitality industry: Preliminary results of a survey,https://api.elsevier.com/content/abstract/scopus_id/85116885410,"Intelligent technologies are widely implemented in different areas of modern society but specific approaches should be applied in services. Basic relationships refer to supporting customers and people responsible for services offering for these customers. The aim of the paper is to analyze and evaluate the state-of-the art of artificial intelligence (AI) applications in the hospitality industry. Our findings show that the major deployments concern in-person customer services, chatbots and messaging tools, business intelligence tools powered by machine learning, and virtual reality & augmented reality. Moreover, we performed a survey (n = 178), asking respondents about their perceptions and attitudes toward AI, including its implementation within a hotel space. The paper attempts to discuss how the hotel industry can be motivated by potential customers to apply selected AI solutions. In our opinion, these results provide useful insights for understanding the phenomenon under investigation. Nevertheless, since the results are not conclusive, more research is still needed on this topic. Future studies may concern both qualitative and quantitative methods, devoted to developing models that: a) quantify the potential benefits and risks of AI implementations, b) determine and evaluate the factors affecting the AI adoption by the customers, and c) measure the user (guest) experience of the hotel services, fueled by AI-based technologies.",industry
10.1016/j.matpr.2021.03.109,Conference Proceeding,Materials Today: Proceedings,scopus,2021-01-01,sciencedirect,Real-time applications and novel manufacturing strategies of incremental forming: An industrial perspective,https://api.elsevier.com/content/abstract/scopus_id/85114180763,"Incremental Sheet-Metal Forming (ISMF) is a flexible and evolving metal forming technology for rapid free-form prototyping and small-batch metal components manufacturing. The end product has evolved by means of localized deformation in addition bi-axial stretching during that deforming tool squeezed on blank with predefined process variables. Owing to a unique process advantages and low manufacturing cost, its market requirement continuous enlargement and the process gradually transforms from prototyping to real-time manufacturing perspective. Over the preceding decades, ISMF technology has been adequately established in research and development, although it is less explored in the real-time industrial environment. The main intention of this exploration is to bring-forth insight into potential applications such as aviation, automotive, bio-medical, research and concept development through implementation of ISMF. Further, component evaluation performed to establish a convenient and feasible solution from deep-drawing and hydro-forming. For customized part forming, conventional forming process seems to be insufficient. Due to industrial transformation, dependent on cost-effectiveness, even prototyping and low-volume manufactured components relying on superior quality. Although, understanding the effect and influence of process variable, which needs the data analysis with implementing the optimization models and Artificial neural-network (ANN) model. These types of analysis majorly focus on monitoring and predict target values at each cycle and also reconfigure to optimistic or organize the iterative method for describing the appropriate process guidelines. Further, recent advances in ISMF process variants are explored, while looking at the benefits of ISMF for real-time part production. ISMF continues to mature into technology for production applications, while exploring the potential field to transform the way sheet components are fabricated in the new-era of digital manufacturing. This study will, in turn, enhance the capabilities of ISMF technology, which has grown significantly over the preceding decades, allowing technology adopters to innovate new design principle and achieve greater production flexibility.",industry
10.1016/j.dss.2021.113653,Journal,Decision Support Systems,scopus,2021-01-01,sciencedirect,AI-based industrial full-service offerings: A model for payment structure selection considering predictive power,https://api.elsevier.com/content/abstract/scopus_id/85114151068,"Artificial Intelligence and servitization reshape the way that manufacturing companies derive value. Aiming to sustain competitive advantage and intensify customer loyalty, full-service providers offer the use of their products as a service to achieve continuous revenues. For this purpose, companies implement AI classification algorithms to enable high levels of service at controllable costs. However, traditional asset sellers who become service providers require previously atypical payment structures, as classic payment methods involving a one-time fee for production costs and profit margins are unsuitable. In addition, a low predictive power of the implemented classification algorithm can lead to misclassifications, which diminish the achievable level of service and the intended net present value of the resultant service. While previous works focus solely on the costs of such misclassifications, our decision model highlights implications for payment structures, service levels, and – ultimately – the net present value of such data-driven service offerings. Our research suggests that predictive power can be a major factor in selecting a suitable payment structure and the overall design of service level agreements. Therefore, we compare common payment structures for data-driven services and investigate their relationship to predictive power. We develop our model using a design science methodology and iteratively evaluate our results using a four-step approach that includes interviews with industry experts and the application of our model to a real-world use case. In summary, our research extends the existing knowledge of servitization and data-driven services in the manufacturing industry through a quantitative decision model.",industry
10.1016/B978-0-323-88506-5.50132-7,Book Series,Computer Aided Chemical Engineering,scopus,2021-01-01,sciencedirect,Implementation of first-principles surface interactions in a hybrid machine learning assisted modelling of flocculation,https://api.elsevier.com/content/abstract/scopus_id/85110537894,"Machine learning algorithms are drawing attention for modelling processes in the chemical and biochemical industries. Due to a lack of fundamental understanding of complex processes and a lack of reliable real-time measurement methods in bio-based manufacturing, machine learning approaches have become more important. Hybrid modelling approaches that combine detailed process understanding with machine learning can provide an opportunity to integrate prior process knowledge with various measurement data for efficient modelling of the (bio) chemical processes. In this study, the application of a hybrid modelling framework that combines various first-principles models with machine learning algorithms is demonstrated through a laboratory-scale case of flocculation of silica particles in water. Since flocculation is a process that occurs across length- and time scales, an integrated hybrid multi-scale modelling framework can improve the phenomenological understanding of the process. The first-principles models utilized in this study are molecular scale particle surface interaction models such as combined with a larger-scale population balance model.",industry
10.1016/B978-0-323-88506-5.50161-3,Book Series,Computer Aided Chemical Engineering,scopus,2021-01-01,sciencedirect,Artificial Intelligence Based Prediction of Exergetic Efficiency of a Blast Furnace,https://api.elsevier.com/content/abstract/scopus_id/85110444162,"The iron melting furnaces are the most energy-consuming equipment of the iron and steel industry. The energy efficiency of the furnace is affected by process conditions such as the inlet temperature, velocity of the charge, and its composition. Hence, optimum values of these process conditions are vital in the efficient operation of the furnace. Computational methods have been very helpful in the optimum design and operation of process equipment. In this study, a first principle (FP) model was developed for an iron-making furnace to visualize its internal dynamics. To minimize the large computational time required for the FP-based analysis, a data-based model, i.e., Artificial Neural Networks (ANN), is developed using data extracted from the FP model. The ANN model was developed using data sets comprised of the values of temperature of the charge and gasses, velocity, concentration of the oxygen, pressure, airflow directions, energy and exergy profiles, and overall exergy efficiency of the furnace along with its height. The ANN model was highly accurate in prediction and is suitable for real-time implementation in a steel manufacturing plant.",industry
10.1016/B978-0-323-88506-5.50144-3,Book Series,Computer Aided Chemical Engineering,scopus,2021-01-01,sciencedirect,Machine learning-based approach to identify the optimal design and operation condition of organic solvent nanofiltration (OSN),https://api.elsevier.com/content/abstract/scopus_id/85110354404,"Organic solvent nanofiltration (OSN) is one of the most anticipated separation technologies that provides wide-ranged industrial applications such as solvent recovery, solute concentration, and diluent separation. Despite of technical merits of the OSN technology, the numerous characteristics and perplexing nonlinearity on the OSN system have been a critical obstacle for understanding the governing principles, thereby prohibiting practical deployments. Recently, machine learning (ML) based approaches have been widely used for the modelling, discovery and optimization of complex design problems in chemical engineering area such as catalysis, electrochemistry and physicochemical systems. Therefore, this study aims to develop a new ML-based approach for modelling and optimizing the design scheme and operating condition of the OSN system. By collecting commercial OSN data through literatures reviews, the major descriptors for the prediction of the OSN membrane, such as MWCO, solute mole weight, solute concentration, solvent parameter, temperature, pressure, flux, were defined. We then screened noises and outliers of the collected data to ensure a high and consistent density and uniqueness. Support vector machine (SVM) was implemented as a prediction models to simulate the OSN performance and identify the optimal conditions as well as the process scheme. As a result, the optimal operation strategies (i.e., pressure, temperature and solvent and solvent types) were analyzed to meet the targeted specification of the OSN system (mass flux and rejection rate). The proposed ML-based approach can promote a real-world OSN application by reducing a number of time-consuming and expensive experiments for establishing OSN design and operation strategy.",industry
10.1016/B978-0-323-88506-5.50194-7,Book Series,Computer Aided Chemical Engineering,scopus,2021-01-01,sciencedirect,Attack Detection Using Unsupervised Learning Algorithms in Cyber-Physical Systems,https://api.elsevier.com/content/abstract/scopus_id/85110277992,"Cyber-Physical Systems (CPS) are collections of physical and computer components that are integrated with each other to operate a process safely and efficiently. Examples of CPS include industrial control systems, water systems, robotics systems, smart grid, etc. However, the security aspect of CPS is still a concern that makes them vulnerable to cyber attacks on the control elements, network or physical systems. The work reported here is an attempt towards detecting cyber attacks and improving process monitoring in CPS; using unsupervised machine learning anomaly detection algorithms such as one-class SVM, isolation forest, elliptic envelope. These algorithms are evaluated using the dataset of a real Water Distribution Plant (WADI) built at the iTrust centre at Singapore University of Technology and Design for cyber security research. For modelling purposes, process 1 and 2 of the aforementioned plant were taken into consideration because the implemented attacks were closely related to only these sub-processes. The result of the experiment shows that one-class SVM is found to be the most effective algorithm in determining anomalies for this particular dataset.",industry
10.1016/j.isatra.2021.06.010,Journal,ISA Transactions,scopus,2021-01-01,sciencedirect,A real-world application of Markov chain Monte Carlo method for Bayesian trajectory control of a robotic manipulator,https://api.elsevier.com/content/abstract/scopus_id/85108508566,"Reinforcement learning methods are being applied to control problems in robotics domain. These algorithms are well suited for dealing with the continuous large scale state spaces in robotics field. Even though policy search methods related to stochastic gradient optimization algorithms have become a successful candidate for coping with challenging robotics and control problems in recent years, they may become unstable when abrupt variations occur in gradient computations. Moreover, they may end up with a locally optimal solution. To avoid these disadvantages, a Markov chain Monte Carlo (MCMC) algorithm for policy learning under the RL configuration is proposed. The policy space is explored in a non-contiguous manner such that higher reward regions have a higher probability of being visited. The proposed algorithm is applied in a risk-sensitive setting where the reward structure is multiplicative. Our method has the advantages of being model-free and gradient-free, as well as being suitable for real-world implementation. The merits of the proposed algorithm are shown with experimental evaluations on a 2-Degree of Freedom robot arm. The experiments demonstrate that it can perform a thorough policy space search while maintaining adequate control performance and can learn a complex trajectory control task within a small finite number of iteration steps.",industry
10.1016/j.cirp.2021.04.046,Journal,CIRP Annals,scopus,2021-01-01,sciencedirect,Semi-Double-loop machine learning based CPS approach for predictive maintenance in manufacturing system based on machine status indications,https://api.elsevier.com/content/abstract/scopus_id/85108064671,"The paper presents two original and innovative contributions: 1) the model of machine learning (ML) based approach for predictive maintenance in manufacturing system based on machine status indications only, and 2) semi-Double-loop machine learning based intelligent Cyber-Physical System (I-CPS) architecture as a higher-level environment for ML based predictive maintenance execution. Considering only the machine status information provides rapid and very low investment-based implementation of an advanced predictive maintenance paradigm, especially important for SMEs. The model is validated in real-life situations, exploring different learning algorithms and strategies for learning maintenance predictive models. The findings show very high level of prediction accuracy.",industry
10.1016/j.procir.2021.05.031,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,Artificial intelligence enhanced interaction in digital twin shop-floor,https://api.elsevier.com/content/abstract/scopus_id/85107885361,"As an enabling technology for smart manufacturing, digital twin has been widely applied in manufacturing shop-floor. A great deal of research focuses on the key issues in implementing digital twin shop-floor (DTS), including scheduling, production planning, fault diagnosis and prognostics. However, DTS puts forward higher requirements in terms of real-time interaction. Artificial intelligence (AI), as an effective approach to improve the intelligence of the physical shop-floor, provides a new method to meet the above requirements. In this paper, a framework of AI-enhanced DTS in interaction is proposed. AI-enhanced DTS improves the real-time interaction through predictive control. The implementation mechanism of AI-enhanced interaction in DTS is also presented in detail. Enabling technologies for interaction in DTS are introduced at last.",industry
10.1016/j.procs.2021.03.074,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Requirements towards optimizing analytics in industrial processes,https://api.elsevier.com/content/abstract/scopus_id/85106735396,"Modern production systems are composed of complex manufacturing processes with highly technology specific cause-effect relationships. Developments in sensor technology and computational science allow for data-driven decision making that facilitate effcient and objective production management. However, process data may only be beneficial if it is enriched with meta information and process expertise, reduced to relevant information and modelling results interpreted correctly. The importance of data integration in the heterogeneous industrial environment rises at the same momentum as new metrology techniques are deployed. In this paper, we focus on optimizing analytics, containing data-driven decision making for predictive quality and maintenance. We summarize key requirements for data analytics and machine learning application in industrial processes. With a use case from automotive component manufacturing we characterize industrial production, categorize process data and put requirements in context to a real-world example.",industry
10.1016/j.procs.2021.02.026,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,DDNN based data allocation method for IIoT,https://api.elsevier.com/content/abstract/scopus_id/85104871428,"With the complete application of artificial intelligence in the field of industrial production and manufacturing and the rapid development of edge computing, industrial processing sites often need to deploy machine learning tasks at edges and terminals. We propose a data allocation method based on Distributed Deep Neural Networks (DDNN) framework, which allocates data to edge servers or stays locally for processing. DDNN divides deep learning tasks and deploys pre-trained shallow neural networks and deep neural networks at local or edges, respectively. However, all data is processed locally, and the failure is sent to the edge server or the cloud. It will lead to excessive pressure on local terminal equipment and long-term idle edge servers, which cannot meet industrial production’s real-time requirements on user privacy and time-sensitive tasks. In this paper, the complexity and inference error rate of machine learning model, the data processing speed of local equipment and edge server, and the transmission time are comprehensively considered to establish the system model. A joint optimization problem is proposed to minimize the total data processing delay. The optimal solution is derived analytically, and the optimal data allocation methhod is given. Simulation experiments are designed to verify the method’s effectiveness and study the influence of key parameters on the allocation method.",industry
10.1016/j.procs.2021.03.075,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Input doubling method based on SVR with RBF kernel in clinical practice: Focus on small data,https://api.elsevier.com/content/abstract/scopus_id/85104314419,"In recent years, machine-learning-based approaches have become of considerable interest to the efficient processing of short or limited data samples. Its so-called small data approach. This is due to the significant growth of new intellectual analysis tasks in various industries, which are characterized by limited historical data. These include Materials Science, Economics, Medicine, and so on. An effective processing of short datasets is especially acute in medicine. Insufficient number of vectors, significant gaps in the data collected during the supervision of patient’s treatment or rehabilitation, reduces the effectiveness or prevents effective intellectual analysis based on them. This paper presents a new approach to processing short medical data samples. The basis of the developed method is SVR with RBF kernel. The algorithmic implementation of the method in both operation modes is described. Experimental modeling on a real short data set (Trabecular bone data) is conducted. It contained only 35 observations. A comparison of the method with a number of existing machine learning methods is conducted. It is experimental established the highest accuracy of the method among those considered. The developed method has potential opportunities for wide application in various fields of medicine.",industry
10.1016/j.procir.2021.01.128,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,A Machine Vision-based Cyber-Physical Production System for Energy Efficiency and Enhanced Teaching-Learning Using a Learning Factory,https://api.elsevier.com/content/abstract/scopus_id/85102656008,"Machine vision (MV) can help in achieving real-time data analysis in a manufacturing environment. This can be implemented in any industry to achieve real-time monitoring of workpieces for geometric defects and material irregularities. Identification of defects, sorting of workpieces based on their physical parameters, and analysis of process abnormalities can be achieved by using the real-time data from simple and cost-effective raspberry pi with camera and open source machine learning platform TensorFlow to run convolutional neural network (CNN) model. The proposed cyber-physical production system enables to develop a MV based system for data acquisition integrating physical entities of learning factory (LF) with the cyber world. Nowadays, LFs are widely used to train the workforce for developing competencies for emerging technologies and challenges faced due to technological advancements in Industry 4.0. This paper demonstrates the application of a cost-effective MV system in a learning factory environment to achieve real-time data acquisition and energy efficiency. The proposed low-cost machine vision is found to detect geometric irregularities, colours and surface defects. The simple cost effective MV system has enhanced the energy efficiency and reduced the total carbon footprint by 18.37 % and 78.83 % depending upon the location of MV system along the flow. The teaching-learning experience is also enhanced through action-based learning strategies. This not only ensures less rework, better control, unbiased decisions, 100% quality assurance but also the need of workers/operators can be reduced.",industry
10.1016/j.procir.2021.01.115,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,Development of a Decision Support System for 3D Printing Processes based on Cyber Physical Production Systems,https://api.elsevier.com/content/abstract/scopus_id/85102637852,"3D printing, an additive manufacturing (AM) technology, potentially provides sustainability advantages such as less waste generation, lightweight geometries, reduced material and energy consumption, lower inventory waste, etc. This paper proposes a decision support system for the 3D printing process based on Cyber Physical Production System (CPPS). The user is enabled to dynamically assess the carbon footprint based on the energy and material usage for their 3D printed object. A CPPS framework for the environmental sustainability of the 3D printing process is presented, which supports the derivation of improved strategies for product design and production. A physical world for 3D printing is used with the internet of things (IoT) devices like sensor node, webcam, smart plugs, and raspberry pi to host printer Management Software (PMS) for real-time monitoring and control of material and energy consumption during the printing process. Experiments have been conducted based on Taguchi L9 orthogonal array with polylactic Acid (PLA) as a filament material to estimate the product-related manufacturing energy consumption with the carbon footprint. The proposed framework can be effectively used by the users to supports the decision-making process for saving resources and energy; and minimizing the effect on the environment.",industry
10.1016/j.procir.2021.01.010,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,Analysis of Barriers to Industry 4.0 adoption in Manufacturing Organizations: An ISM Approach,https://api.elsevier.com/content/abstract/scopus_id/85102622489,"Industry 4.0 has enabled technological integration of cyber physical systems and internet based communication in manufacturing value creation processes. As of now, many people use it as a collective term for advanced technologies, i.e. advanced robotics, artificial intelligence, machine learning, big data analytics, cloud computing, smart sensors, internet of things, augmented reality, etc. This substantially improves flexibility, quality, productivity, cost, and customer satisfaction by transforming existing centralized manufacturing systems towards digital and decentralized one. Despite having potential benefits of industry 4.0, the organizations are facing typical obstacles and challenges in adopting new technologies and successful implementation in their business models. This paper aims to identify potential barriers which may hinder the implementation of industry 4.0 in manufacturing organizations. The identified barriers, through comprehensive literature review and on the basis of opinions collected from industry experts, are: poor value-chain integration, cyber-security challenges, uncertainty about economic benefits, lack of adequate skills in workforce, high investment requirements, lack of infrastructure, jobs disruptions, challenges in data management and data quality, lack of secure standards and norms, and resistance to change. Interpretive Structural Modeling (ISM) is used to establish relationships among these barriers to develop a hierarchical model and MICMAC analysis for further classification of identified barriers for better understanding. An analysis of driving and dependence of the barriers may help in clear understanding of these for successful implementation of Industry 4.0 practices in the organizations.",industry
10.1016/j.procs.2021.01.348,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Procedure model for the development and launch of intelligent assistance systems,https://api.elsevier.com/content/abstract/scopus_id/85101779152,"The paper analyses the current state of knowledge on approaches for the practical implementation of machine learning based assistance systems for production planning and control.
                  A concept of a procedure model for application-oriented projects in the field of industrial series production is proposed. It focusses on order sequencing and machine allocation in a real time production environment. As part of an application-oriented research project, a use case is referenced. In this paper, a first conceptual approach is presented, using the example of an industrial production of printed circuit boards.
                  In the following steps, practical suitability is checked on the basis of the practical reference, conclusions are drawn and the methodology will be developed further. The aim is a generally valid procedure model for industrial series production.",industry
10.1016/j.jtice.2021.01.007,Journal,Journal of the Taiwan Institute of Chemical Engineers,scopus,2021-01-01,sciencedirect,On the evaluation of solubility of hydrogen sulfide in ionic liquids using advanced committee machine intelligent systems,https://api.elsevier.com/content/abstract/scopus_id/85099564514,"Ionic Liquids (ILs) are increasingly emerging as new innovating green solvents with great importance from academic, industrial, and environmental perspectives. This surge of interest in considering ILs in various applications is owed to their attractive properties. Involvements in the gas sweetening and the reduction of the amounts of sour and acid gasses are among the most promising applications of ILs. In this study, new advanced committee machine intelligent systems (CMIS) were introduced for predicting the solubility of hydrogen sulfide (H2S) in various ILs. The implemented CMIS models were gained by linking robust data-driven techniques, namely multilayer perceptron (MLP) and cascaded forward neural network (CFNN) beneath rigorous schemes using group method of data handling (GMDH) and genetic programming (GP). The proposed paradigms were developed using an extensive database encompassing 1243 measurements of H2S solubility in 33 ILs. The performed comprehensive error investigation revealed that the newly implemented paradigms yielded very satisfactory prediction performance. Besides, it was found that CMIS-GP provided more accurate estimations of H2S solubility in ILs compared with both the other intelligent models and the best-prior paradigms. In this regard, the developed CMIS-GP exhibited overall average absolute relative deviation (AARD) and coefficient of determination (R2) values of 2.3767% and 0.9990, respectively. Lastly, the trend analyses demonstrated that the tendencies of CMIS-GP predictions were in excellent accordance with the real variations of H2S solubility in ILs with respect to pressure and temperature.",industry
10.1016/j.aei.2021.101246,Journal,Advanced Engineering Informatics,scopus,2021-01-01,sciencedirect,"A systematic literature review on intelligent automation: Aligning concepts from theory, practice, and future perspectives",https://api.elsevier.com/content/abstract/scopus_id/85099458674,"With the recent developments in robotic process automation (RPA) and artificial intelligence (AI), academics and industrial practitioners are now pursuing robust and adaptive decision making (DM) in real-life engineering applications and automated business workflows and processes to accommodate context awareness, adaptation to environment and customisation. The emerging research via RPA, AI and soft computing offers sophisticated decision analysis methods, data-driven DM and scenario analysis with regard to the consideration of decision choices and provides benefits in numerous engineering applications. The emerging intelligent automation (IA) – the combination of RPA, AI and soft computing – can further transcend traditional DM to achieve unprecedented levels of operational efficiency, decision quality and system reliability. RPA allows an intelligent agent to eliminate operational errors and mimic manual routine decisions, including rule-based, well-structured and repetitive decisions involving enormous data, in a digital system, while AI has the cognitive capabilities to emulate the actions of human behaviour and process unstructured data via machine learning, natural language processing and image processing. Insights from IA drive new opportunities in providing automated DM processes, fault diagnosis, knowledge elicitation and solutions under complex decision environments with the presence of context-aware data, uncertainty and customer preferences. This sophisticated review attempts to deliver the relevant research directions and applications from the selected literature to the readers and address the key contributions of the selected literature, IA’s benefits, implementation considerations, challenges and potential IA applications to foster the relevant research development in the domain.",industry
10.1016/j.matdes.2020.109201,Journal,Materials and Design,scopus,2021-01-01,sciencedirect,Online prediction of mechanical properties of hot rolled steel plate using machine learning,https://api.elsevier.com/content/abstract/scopus_id/85092064894,"In industrial steel plate production, process parameters and steel grade composition significantly influence the microstructure and mechanical properties of the steel produced. But determining the exact relationship between process parameters and mechanical properties is a challenging process. This work aimed to devise a deep learning model, to predict mechanical properties of industrial steel plate including yield strength (YS), ultimate tensile strength (UTS), elongation (EL), and impact energy (Akv); based on the process parameters as well as composition of raw steel, and apply it online to a real steel manufacturing plant. An optimal deep neural network (DNN) model was formulated with 27 inputs parameters, 2 hidden layers each having 200 nodes and 4 output parameters (27 × 200 × 200 × 4) with an initial learning rate 0.0001, using Adam optimizer and subjected to Z pre-processing method, to yield an accurate model with R2 = 0.907. The tuned DNN model, had a root mean square error of 21.06 MPa, 16.67 MPa, 2.36%, and 39.33 J, and root mean square percentage error of 4.7%, 2.9%, 7.7%, and 16.2%, for YS, UTS, EL and Akv respectively. Through comparative analysis, it was found that the accuracy of DNN model was higher than other classic machine learning algorithms. To interpret the model assumptions and findings, several local linear models were devised and analyzed to establish the link between process parameters and mechanical properties. Finally the tuned DNN model was deployed in the real-steel plant for online monitoring and control of steel mechanical properties, and to guide the production of targeted steel plates with tailored mechanical properties.",industry
10.1016/bs.adcom.2020.08.013,Book Series,Advances in Computers,scopus,2021-01-01,sciencedirect,Empowering digital twins with blockchain,https://api.elsevier.com/content/abstract/scopus_id/85090745248,"A digital twin is an exact digital/logical/cyber/virtual representation/replica of any tangible physical system or process. And the digital twin runs on a competent IT infrastructure (say, cloud centers). In essence, a digital twin is typically a software program that takes various real-world data about a ground-level physical system as prospective inputs and produces useful outputs in the form of insights. The outputs generally are the value-adding and decision-enabling predictions or simulations of how that physical system will act on those inputs. These help in quickly and easily realizing highly optimized and organized products with less cost and risk.
                  The manufacturing industry had embraced the digital twin technology long time back to be modern in their operations, outputs, and offerings. The distinct contributions of the digital twin paradigm, since then, have gone up significantly with the seamless synchronization with a number of pioneering technologies such as the Internet of Things (IoT), artificial intelligence (AI), big and streaming data analytics, data lakes, software-defined cloud environments, blockchain, etc. With the concept of cyber physical systems (CPS) is being adopted and adapted widely and wisely, complicated yet sophisticated electronics devices at the ground level are being blessed with their corresponding digital twins. The digital twins enable data scientists and system designers to optimize a number of things including process excellence, knowledge discovery and dissemination in time, better system design, robust verification and validation, etc. In the recent past, with the flourishing of the blockchain technology, the scope for digital twins has gone up remarkably. This unique combination is bound to produce additional competencies and fresh use cases for enterprises. This chapter is to explain how they integrate and initiate newer opportunities to be grabbed and gained for a better tomorrow.",industry
10.1016/j.jclepro.2020.124022,Journal,Journal of Cleaner Production,scopus,2021-01-01,sciencedirect,Artificial intelligence in nuclear industry: Chimera or solution?,https://api.elsevier.com/content/abstract/scopus_id/85090601822,"Nuclear industry is in crisis and innovation is the central theme of its survival in future. Artificial intelligence has made a quantum leap in last few years. This paper comprehensively analyses recent advancement in artificial intelligence for its applications in nuclear power industry. A brief background of machine learning techniques researched and proposed in this domain is outlined. A critical assessment of various nuances of artificial intelligence for nuclear industry is provided. Lack of operational data from real power plant especially for transients and accident scenario is a major concern regarding the accuracy of intelligent systems. There is no universally agreed opinion among researchers for selecting the best artificial intelligence techniques for a specific purpose as intelligent systems developed by various researchers are based on different data set. Interlaboratory work frame or round-robin programme to develop the artificial intelligent tool for any specific purpose, based on the same data base, can be crucial in claiming the accuracy and thus the best technique. The black box nature of artificial techniques also poses a serious challenge for its implementation in nuclear industry, as it makes them prone to fooling.",industry
10.1016/j.jobe.2020.101601,Journal,Journal of Building Engineering,scopus,2021-01-01,sciencedirect,Trainingless multi-objective evolutionary computing-based nonintrusive load monitoring: Part of smart-home energy management for demand-side management,https://api.elsevier.com/content/abstract/scopus_id/85087827958,"Electricity is the most widely used form of energy in modern society. One method of satisfying the continuously increasing industrial, commercial, and residential electrical-energy demands of consumers in smart grids is to use an Internet-of-things (IoT) service-oriented electrical-energy management system (EMS) to intrusively monitor and manage electrical loads, which can effectively react to demand-response schemes for demand-side management (DSM). Nonintrusive load monitoring (NILM), a viable cost-effective load disaggregation technique, has recently gained considerable attention as a nonintrusive alternative to EMS in the research field of smart grids. This paper presents a smart IoT-oriented home EMS founded on trainingless multi-objective evolutionary computing-based NILM for DSM in a smart grid. Evolutionary computing-based NILM is considered and addressed as a multi-objective combinatorial optimization problem. The proposed NILM technique can determine the electrical appliances based on their individual electrical characteristics extracted from composite electrical-load consumption with no intrusive deployment of smart plugs or power meters. A fully nonintrusive NILM alternative is considered and proposed. In addition, this alternative is different from conventional NILM because conventional NILM considers artificial intelligence including artificial neural networks (NNs) and deep NN as load classifiers of NILM where training and retraining stages and a hyperparameter tuning procedure are required. The proposed smart IoT-oriented home EMS was experimentally investigated with the trainingless multi-objective evolutionary computing-based NILM in a real house environment. The experimental results confirm that the proposed methodology is feasible.",industry
10.1016/j.jmsy.2020.06.012,Journal,Journal of Manufacturing Systems,scopus,2021-01-01,sciencedirect,"A digital twin to train deep reinforcement learning agent for smart manufacturing plants: Environment, interfaces and intelligence",https://api.elsevier.com/content/abstract/scopus_id/85087690907,"Filling the gaps between virtual and physical systems will open new doors in Smart Manufacturing. This work proposes a data-driven approach to utilize digital transformation methods to automate smart manufacturing systems. This is fundamentally enabled by using a digital twin to represent manufacturing cells, simulate system behaviors, predict process faults, and adaptively control manipulated variables. First, the manufacturing cell is accommodated to environments such as computer-aided applications, industrial Product Lifecycle Management solutions, and control platforms for automation systems. Second, a network of interfaces between the environments is designed and implemented to enable communication between the digital world and physical manufacturing plant, so that near-synchronous controls can be achieved. Third, capabilities of some members in the family of Deep Reinforcement Learning (DRL) are discussed with manufacturing features within the context of Smart Manufacturing. Trained results for Deep Q Learning algorithms are finally presented in this work as a case study to incorporate DRL-based artificial intelligence to the industrial control process. As a result, developed control methodology, named Digital Engine, is expected to acquire process knowledges, schedule manufacturing tasks, identify optimal actions, and demonstrate control robustness. The authors show that integrating a smart agent into the industrial platforms further expands the usage of the system-level digital twin, where intelligent control algorithms are trained and verified upfront before deployed to the physical world for implementation. Moreover, DRL approach to automated manufacturing control problems under facile optimization environments will be a novel combination between data science and manufacturing industries.",industry
10.1016/j.jclepro.2020.123365,Journal,Journal of Cleaner Production,scopus,2020-12-20,sciencedirect,An active preventive maintenance approach of complex equipment based on a novel product-service system operation mode,https://api.elsevier.com/content/abstract/scopus_id/85089891280,"The product-service system (PSS) business model has received increasing attention in equipment maintenance studies, as it has the potential to provide high value-added services for equipment users and construct ethical principles for equipment providers to support the implementation of circular economy. However, the PSS providers in equipment industry are facing many challenges when implementing Industry 4.0 technologies. One important challenge is how to fully collect and analyse the operational data of different equipment and diverse users in widely varied conditions to make the PSS providers create innovative equipment management services for their customers. To address this challenge, an active preventive maintenance approach for complex equipment is proposed. Firstly, a novel PSS operation mode was developed, where complex equipment is offered as a part of PSS and under exclusive control by the providers. Then, a solution of equipment preventive maintenance based on the operation mode was designed. A deep neural network was trained to predict the remaining effective life of the key components and thereby, it can pre-emptively assess the health status of equipment. Finally, a real-world industrial case of a leading CNC machine provider was developed to illustrate the feasibility and effectiveness of the proposed approach. Higher accuracy for predicting the remaining effective life was achieved, which resulted in predictive identification of the fault features, proactive implementation of the preventive maintenance, and reduction of the PSS providers’ maintenance costs and resource consumption. Consequently, the result shows that it can help PSS providers move towards more ethical and sustainable directions.",industry
10.1016/j.oceaneng.2020.108261,Journal,Ocean Engineering,scopus,2020-12-15,sciencedirect,Real-time data-driven missing data imputation for short-term sensor data of marine systems. A comparative study,https://api.elsevier.com/content/abstract/scopus_id/85093700362,"In the maritime industry, sensors are utilised to implement condition-based maintenance (CBM) to assist decision-making processes for energy efficient operations of marine machinery. However, the employment of sensors presents several challenges including the imputation of missing values. Data imputation is a crucial pre-processing step, the aim of which is the estimation of identified missing values to avoid under-utilisation of data that can lead to biased results. Although various studies have been developed on this topic, none of the studies so far have considered the option of imputing incomplete values in real-time to assist instant data-driven decision-making strategies. Hence, a methodological comparative study has been developed that examines a total of 20 widely implemented machine learning and time series forecasting algorithms. Moreover, a case study on a total of 7 machinery system parameters obtained from sensors installed on a cargo vessel is utilised to highlight the implementation of the proposed methodology. To assess the models’ performance seven metrics are estimated (Execution time, MSE, MSLE, RMSE, MAPE, MedAE, Max Error). In all cases, ARIMA outperforms the remaining models, yielding a MedAE of 0.08 r/min and a Max Error of 2.4 r/min regarding the main engine rotational speed parameter.",industry
10.1016/j.eswa.2020.113653,Journal,Expert Systems with Applications,scopus,2020-12-15,sciencedirect,Cost-sensitive learning classification strategy for predicting product failures,https://api.elsevier.com/content/abstract/scopus_id/85088008188,"In the current era of Industry 4.0, sensor data used in connection with machine learning algorithms can help manufacturing industries to reduce costs and to predict failures in advance. This paper addresses a binary classification problem found in manufacturing engineering, which focuses on how to ensure product quality delivery and at the same time to reduce production costs. The aim behind this problem is to predict the number of faulty products, which in this case is extremely low. As a result of this characteristic, the problem is reduced to an imbalanced binary classification problem. The authors contribute to imbalanced classification research in three important ways. First, the industrial application coming from the electronic manufacturing industry is presented in detail, along with its data and modelling challenges. Second, a modified cost-sensitive classification strategy based on a combination of Voronoi diagrams and genetic algorithm is applied to tackle this problem and is compared to several base classifiers. The results obtained are promising for this specific application. Third, in order to evaluate the flexibility of the strategy, and to demonstrate its wide range of applicability, 25 real-world data sets are selected from the KEEL repository with different imbalance ratios and number of features. The strategy, in this case implemented without a predefined cost, is compared with the same base classifiers as those used for the industrial problem.",industry
10.1016/j.ijpx.2020.100058,Journal,International Journal of Pharmaceutics: X,scopus,2020-12-01,sciencedirect,Deep convolutional neural networks: Outperforming established algorithms in the evaluation of industrial optical coherence tomography (OCT) images of pharmaceutical coatings,https://api.elsevier.com/content/abstract/scopus_id/85096171040,"This paper presents a novel evaluation approach for optical coherence tomography (OCT) image analysis of pharmaceutical solid dosage forms based on deep convolutional neural networks (CNNs). As a proof of concept, CNNs were applied to image data from both, in- and at-line OCT implementations, monitoring film-coated tablets as well as single- and multi-layered pellets. CNN results were compared against results from established algorithms based on ellipse-fitting, as well as to human-annotated ground truth data. Performance benchmarks used include, efficiency (computation speed), sensitivity (number of detections from a defined test set) and accuracy (deviation from the reference method). The results were validated by comparing the output of several algorithms to data manually annotated by human experts and microscopy images of cross-sectional cuts of the same dosage forms as a reference method. In order to guarantee comparability for all results, the algorithms were executed on the same hardware. Since modern OCT systems must operate under real-time conditions in order to be implemented in-line into manufacturing lines, the necessary steps are discussed on how to achieve this goal without sacrificing the algorithmic performance and how to tailor a deep CNN to cope with the high amount of image noise and alterations in object appearance. The developed deep learning approach outperforms static algorithms currently available in pharma applications with respect to performance benchmarks, and represents the next level in real time evaluation of challenging industrial OCT image data.",industry
10.1016/j.compind.2020.103329,Journal,Computers in Industry,scopus,2020-12-01,sciencedirect,A Middleware Platform for Intelligent Automation: An Industrial Prototype Implementation,https://api.elsevier.com/content/abstract/scopus_id/85092922057,"The development of dynamic data-based Decision Support Systems (DSSs) along with the increasing availability of data in the industry, makes real-time data acquisition and management a challenge. Intelligent automation appears as a holistic combination of automation with analytics and decisions made by artificial intelligence, delivering smart manufacturing and mass customization while improving resource efficiency. However, challenges towards the development of intelligent automation architectures include the lack of interoperability between systems, complex data preparation steps, and the inability to deal with both high-frequency and high-volume data in a timely fashion. This paper contributes to industrial frameworks focused on the development of standardized system architectures for Industry 4.0, closing the gap between generic architectures and physical realizations. It proposes a platform for intelligent automation relying on a gateway or middleware between field devices, enterprise databases, and DSSs in real-time scenarios. This is achieved by providing the middleware interoperability, determinism, and automatic data structuring over an industrial communication infrastructure such as the OPC UA Standard over Time Sensitive Networks (TSN). Cloud services and database warehousing used to address some of the challenges are handled using fog computing and a multi-workload database. This paper presents an implementation of the platform in the pharmaceutical industry, providing interoperability and real-time reaction capability to changes to an industrial prototype using dynamic scheduling algorithms.",industry
10.1016/j.autcon.2020.103354,Journal,Automation in Construction,scopus,2020-12-01,sciencedirect,Real-time online detection of trucks loading via genetic neural network,https://api.elsevier.com/content/abstract/scopus_id/85091689126,"This article focuses on real-time online detection of trucks loading via genetic neural network. Firstly, according to the state structure of the truck and the deployment of the sensor in the monitoring system, a mathematical model that magnetic sensors detecting the weight of the truck is established, it provides a theoretical basis for the calculation of the compensator deviation. Secondly, a feedback compensator for disturbance signals is designed by genetic neural network in the load monitoring system. Thirdly, the stability of the control system is analyzed by the Lyapunov stability theory. Fourthly, a real-time monitoring system is proposed for the loading of trucks. Finally, a complete experiment is processed to in-depth discussion and analysis. Field experiments showed that this scheme solves the problem of real-time load detection of trucks, it proposes a monitoring system for transportation in the construction industry.",industry
10.1016/j.autcon.2020.103387,Journal,Automation in Construction,scopus,2020-12-01,sciencedirect,Virtual prototyping- and transfer learning-enabled module detection for modular integrated construction,https://api.elsevier.com/content/abstract/scopus_id/85090569290,"Modular integrated construction is one of the most advanced off-site construction technologies and involves the repetitive process of installing prefabricated prefinished volumetric modules. Automatic detection of location and movement of modules should facilitate progress monitoring and safety management. However, automatic module detection has not been implemented previously. Hence, virtual prototyping and transfer-learning techniques were combined in this study to develop a module-detection model based on mask regions with convolutional neural network (Mask R-CNN). The developed model was trained with datasets comprising both virtual and real images, and it was applied to two modular construction projects for automatic progress monitoring. The results indicate the effectiveness of the developed model in module detection. The proposed method using virtual prototyping and transfer learning not only facilitates the development of automation in modular construction, but also provides a new approach for deep learning in the construction industry.",industry
10.1016/j.ssci.2020.104967,Journal,Safety Science,scopus,2020-12-01,sciencedirect,Risk assessment by failure mode and effects analysis (FMEA) using an interval number based logistic regression model,https://api.elsevier.com/content/abstract/scopus_id/85090003051,"In order to reduce risks of failure, industries use a methodology called Failure Mode and Effects Analysis (FMEA) in terms of the Risk Priority Number (RPN). The RPN number is a product of ordinal scale variables, severity (S), occurrence (O) and detection (D) and product of such ordinal variables is debatable. The three risk attributes (S, O, and D) are generally given equal weightage, but this assumption may not be suitable for real-world applications. Apart from severity, occurrence, and detection, the presence of other risk attributes may also influence the risk of failure and hence should be considered for achieving a holistic approach towards mitigating failure modes. This paper proposes a systematic approach for developing a standard equation for RPN measure, using the methodology of interval number based logistic regression. Instead of utilizing RPN in product form for each failure, this method is benefited from decisions based on probability of risk of failure, 
                        
                           '
                           P
                           '
                        
                      which is more realistic in practical applications. A case study is presented to illustrate the application of the proposed methodology in finding the risk of failure of high capacity submersible pumps in the power plant. The developed logistic regression model (logit model) using R software helped in generating the probability of risk of failure equation for predicting the failures. The model showed the correct classification rate to be 77.47%. The Receiver Operating Characteristic (ROC) curve showed the logit-model to be 81.98% accurate with an optimal cut-off value of 0.56.",industry
10.1016/j.epsr.2020.106742,Journal,Electric Power Systems Research,scopus,2020-12-01,sciencedirect,Learning model of generator from terminal data,https://api.elsevier.com/content/abstract/scopus_id/85089545570,"Assuming that a generator is monitored by the system operator via a PMU device positioned at the generator’s terminal bus, we pose and resolve the question of the real-time, data-driven and automatic monitoring of the generator’s performance. We establish regimes of optimal performance for four complementary techniques ranging from the computationally light (a) Vector Auto-Regressive Model, suitable for normal, linear or almost linear regime, via (b) Long-Short-Term-Memory and (c) Neural ODE Deep Learning models, appropriate to monitor mildly nonlinear regimes, and finally to the (d) physics-informed model. For example, the physics-informed model is capable of fast identification of nonlinear transients and providing interpretable results, suitable, in particular, for corrective actions. The conclusions are reached in the result of validating the models on synthetic data generated in a realistic setting from an open-source, state-of-the-art modeling software. Advanced analysis is followed by a summary and conclusion suitable for the next step - validation of the hierarchy of the suggested data-driven schemes in the industry setting.",industry
10.1016/j.jclepro.2020.123125,Journal,Journal of Cleaner Production,scopus,2020-12-01,sciencedirect,A systematic literature review on machine tool energy consumption,https://api.elsevier.com/content/abstract/scopus_id/85088635681,"Energy efficiency has become an integral part of the metal manufacturing industries as a means to improve economic and environmental performance, and increase competitiveness. Machine tools are not only the major energy consumer in the manufacturing industry but also have very low efficiency. Therefore, the analysis of energy consumption by the machine tools is primarily important to understand their complex and dynamic energy consumption behavior. This will lead to the development of better corrective measures. Literature review helps in identifying and assessing the existing knowledge to recognize the future research areas for fostering the research interest on the specific topic. In this review article, the reference literature is identified using a systematic methodology followed by descriptive and content analysis to understand the evolution of research in machining energy. The review focuses on four machining energy aspects – classification, modelling, improvement strategies, and efficiency evaluation. A six level hierarchical model is proposed for better understanding of machining energy classification. The literature review shows that the research in this field intensified after 2009. It is observed that the research focus has shifted towards micro level classification of machining energy including transient states. More detailed and accurate energy consumption models are developed in recent years with increased use of soft computational methods. Real time energy data monitoring and its use for online optimization of machining processes is witnessed. The use of micro analysis, energy benchmarking and standardization of energy assessment indices require more research. Deployment of machining energy models for improving the sustainability of machine tools; data analytics and AI applications; and integration with industry 4.0 are new research opportunities in the field.",industry
10.1016/j.joes.2020.03.003,Journal,Journal of Ocean Engineering and Science,scopus,2020-12-01,sciencedirect,Developing a predictive maintenance model for vessel machinery,https://api.elsevier.com/content/abstract/scopus_id/85086839612,"The aim of maintenance is to reduce the number of failures in equipment and to avoid breakdowns that may lead to disruptions during operations. The objective of this study is to initiate the development of a predictive maintenance solution in the shipping industry based on a computational artificial intelligence model using real-time monitoring data. The data analysed originates from the historical values from sensors measuring the vessel´s engines and compressors health and the software used to analyse these data was R. The results demonstrated key parameters held a stronger influence in the overall state of the components and proved in most cases strong correlations amongst sensor data from the same equipment. The results also showed a great potential to serve as inputs for developing a predictive model, yet further elements including failure modes identification, detection of potential failures and asset criticality are some of the issues required to define prior designing the algorithms and a solution based on artificial intelligence. A systematic approach using big data and machine learning as techniques to create predictive maintenance strategies is already creating disruption within the shipping industry, and maritime organizations need to consider how to implement these new technologies into their business operations and to improve the speed and accuracy in their maintenance decision making.",industry
10.1016/j.ejor.2020.05.010,Journal,European Journal of Operational Research,scopus,2020-12-01,sciencedirect,Data-driven optimization model customization,https://api.elsevier.com/content/abstract/scopus_id/85086372620,"When embedded in software-based decision support systems, optimization models can greatly improve organizational planning. In many industries, there are classical models that capture the fundamentals of general planning decisions (e.g., designing a delivery route). However, these models are generic and often require customization to truly reflect the realities of specific operational settings. Yet, such customization can be an expensive and time-consuming process. At the same time, popular cloud computing software platforms such as Software as a Service (SaaS) are not amenable to customized software applications. We present a framework that has the potential to autonomously customize optimization models by learning mathematical representations of customer-specific business rules from historical data derived from model solutions and implemented plans. Because of the wide-spread use in practice of mixed integer linear programs (MILP) and the power of MILP solvers, the framework is designed for MILP models. It uses a common mathematical representation for different optimization models and business rules, which it encodes in a standard data structure. As a result, a software provider employing this framework can develop and maintain a single code-base while meeting the needs of different customers. We assess the effectiveness of this framework on multiple classical MILPs used in the planning of logistics and supply chain operations and with different business rules that must be observed by implementable plans. Computational experiments based on synthetic data indicate that solutions to the customized optimization models produced by the framework are regularly of high-quality.",industry
10.1016/j.patter.2020.100107,Journal,Patterns,scopus,2020-11-13,sciencedirect,Wiz: A Web-Based Tool for Interactive Visualization of Big Data,https://api.elsevier.com/content/abstract/scopus_id/85097417500,"In an age of information, visualizing and discerning meaning from data is as important as its collection. Interactive data visualization addresses both fronts by allowing researchers to explore data beyond what static images can offer. Here, we present Wiz, a web-based application for handling and visualizing large amounts of data. Wiz does not require programming or downloadable software for its use and allows scientists and non-scientists to unravel the complexity of data by splitting their relationships through 5D visual analytics, performing multivariate data analysis, such as principal component and linear discriminant analyses, all in vivid, publication-ready figures. With the explosion of high-throughput practices for materials discovery, information streaming capabilities, and the emphasis on industrial digitalization and artificial intelligence, we expect Wiz to serve as an invaluable tool to have a broad impact in our world of big data.",industry
10.1016/j.jclepro.2020.122870,Journal,Journal of Cleaner Production,scopus,2020-11-10,sciencedirect,Enhancing the adaptability: Lean and green strategy towards the Industry Revolution 4.0,https://api.elsevier.com/content/abstract/scopus_id/85088397153,"Industry 4.0 has brought forth many advantages and challenges for the industry players. Many organizations are strategizing to take advantage of this industrial paradigm shift, thus improving the sustainability of the enterprise. However, there are many factors such as talent development, machinery advancement and infrastructure development which involve huge investment that need to be considered. This paper presents an enhanced adaptive model for the implementation of the lean and green (L&G) strategy in processing sectors to solve dynamic industry problems associated with Industry 4.0. A feature of this enhanced adaptive model is that it combines experts’ experience and operational data as input in dealing with real industry application. A lean and green index is coupled in the model to serve as a benchmark and process improvement tracking indicator. This allows the industrialists to set a lean and green index (LGI) target for effective process improvement. From this integrated model, an ensemble of backpropagation optimizers is then used to identify the best-optimized strategy. This ensemble optimizer is formulated to perform operation improvement and update the targeted LGI automatically when a higher index is achieved for continuous improvement. A case study on a combined heat and power plant is performed and reflects an improvement of 18.25% on the LGI. This work serves as a practical transition strategy for the industrialist desiring to improve the sustainability of the facility with Industry 4.0 elements at minimum investment cost.",industry
10.1016/j.cie.2020.106868,Journal,Computers and Industrial Engineering,scopus,2020-11-01,sciencedirect,Simulation in industry 4.0: A state-of-the-art review,https://api.elsevier.com/content/abstract/scopus_id/85091194972,"Simulation is a key technology for developing planning and exploratory models to optimize decision making as well as the design and operations of complex and smart production systems. It could also aid companies to evaluate the risks, costs, implementation barriers, impact on operational performance, and roadmap toward Industry 4.0. Although several advances have been made in this domain, studies that systematically characterize and analyze the development of simulation-based research in Industry 4.0 are scarce. Therefore, this study aims to investigate the state-of-the-art research performed on the intersecting area of simulation and the field of Industry 4.0. Initially, a conceptual framework describing Industry 4.0 in terms of enabling technologies and design principles for modeling and simulation of Industry 4.0 scenarios is proposed. Thereafter, literature on simulation technologies and Industry 4.0 design principles is systematically reviewed using the preferred reporting items for systematic reviews and meta-analyses (PRISMA) methodology. This study reveals an increasing trend in the number of publications on simulation in Industry 4.0 within the last four years. In total, 10 simulation-based approaches and 17 Industry 4.0 design principles were identified. A cross-analysis of concepts and evaluation of models’ development suggest that simulation can capture the design principles of Industry 4.0 and support the investigation of the Industry 4.0 phenomenon from different perspectives. Finally, the results of this study indicate hybrid simulation and digital twin as the primary simulation-based approaches in the context of Industry 4.0.",industry
10.1016/j.infsof.2020.106368,Journal,Information and Software Technology,scopus,2020-11-01,sciencedirect,Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions,https://api.elsevier.com/content/abstract/scopus_id/85087690796,"Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems.
                  
                     Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges.
                  
                     Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment.
                  
                     Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions.
                  
                     Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research.",industry
10.1016/j.petrol.2020.107509,Journal,Journal of Petroleum Science and Engineering,scopus,2020-11-01,sciencedirect,"Design and construction of the knowledge base system for geological outfield cavities classifications: An example of the fracture-cavity reservoir outfield in Tarim basin, NW China",https://api.elsevier.com/content/abstract/scopus_id/85087076723,"Tahe oilfield, located in NW Tarim Basin, is one of the largest and most difficult fracture cavity reservoirs in the world. Different fracture cavities, different generated mechanisms, and different oil production capacities. In order to study the significant parameters that can characterize the categories of facture-cavity. This research adopted outfield manual measurement, 3D digital modeling technique to obtain characterization parameters. According to experienced geological survey, typical outcrops were selected, then scanned by UAV (Unmanned aerial vehicle). Consequently, 3D digital models, including real coordinates and parameter information, were established by Agisoft Photoscan. Through geological testing results, various combination characteristic patterns of relative categories were analyzed. By using digital measure tool, combined with manually measured data, the parameters were extracted from the 3D digital model (DM). Then an initial geological database was established. For furtherly analyzing the database, the mathematic statistics methods of multiple linear regression (MLR), neural network technique (NNT) and discriminative classification technique (DCT) were applied. Using software of SPSS statistics 17.0, more than 200 groups of geological data (various categories of fracture-cavity) were optimally processed. Consequently, the significant characteristic parameters were interpreted to determine diverse categories. The results showed that: (1) cavity width, height, fracture length and cavity aspect ratio were significant parameters to classify runoff cavity categories. (2) Fault-controlled cavities could be accurately classified by fracture length and fracture density. (3) The main cavity categories could be distinguished by cavity width, cavity height and fracture density. Performances of the approach have been examined with 10 percentages of the samples, and a good agreement performed in the simulated results, and anastomosis rate was more than 80%. The researched results have critical guiding significance to evaluate types of fracture-cavity, develop and explore of fracture-cavity reservoirs. The construction technique of knowledge base can be applied for diverse fracture-cavity reservoirs in the various formations in different areas in the world.",industry
10.1016/j.measurement.2020.108043,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2020-11-01,sciencedirect,Smart frost measurement for anti-disaster intelligent control in greenhouses via embedding IoT and hybrid AI methods,https://api.elsevier.com/content/abstract/scopus_id/85086577761,"A novel Agro-industrial IoT (AIIoT) technology and architecture for intelligent frost forecasting in greenhouses via hybrid Artificial Intelligence (AI), is reported. The Internet of Things (IoT) allows the objects interconnection on the physical world using sensors and actuators via the Internet. The smart system was designed and implemented through a climatological station equipped with Artificial Neural Networks (ANN) and a fuzzy associative memory (FAM) for ecological control of the anti-frost disaster irrigation. The ANN forecasts the inside temperature of the greenhouses and the fuzzy control predicts the cropland temperatures for the activation of five output levels of the water pump. The results were compared to a Fourier-statistical analysis of hourly data, showing that the ANN models provide a temperature prediction with effectiveness higher than 90%, as compared to monthly data model. Moreover, results of this process were validated through the determination of the coefficient of variance analysis method (
                        
                           
                              
                                 R
                              
                              2
                           
                        
                     ).",industry
10.1016/j.measurement.2020.108052,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2020-11-01,sciencedirect,Deep learning-based prognostic approach for lithium-ion batteries with adaptive time-series prediction and on-line validation,https://api.elsevier.com/content/abstract/scopus_id/85086367941,"Prognostics for lithium-ion batteries is very critical in many industrial applications, and accurate prediction of battery state of health (SOH) is of great importance for health management. This paper proposes a novel deep learning-based prognostic method for lithium-ion batteries with on-line validation. An effective variant of recurrent neural network, i.e. long short-term memory structure, is used with variable input dimension, that facilitates network training with additional labeled samples. Adaptive time-series predictions are carried out for prognostics. An on-line validation method is further proposed for parameter optimization in real time based on the available system information, which allows for continuous model improvement. Experiments on a popular lithium-ion battery dataset are implemented to validate the effectiveness and superiority of the proposed method. The experimental results show the prognostic performances are promising both for the multi-steps-ahead predictions and long-horizon SOH estimations.",industry
10.1016/j.micpro.2020.103227,Journal,Microprocessors and Microsystems,scopus,2020-10-01,sciencedirect,Fog Computing-inspired Smart Home Framework for Predictive Veterinary Healthcare,https://api.elsevier.com/content/abstract/scopus_id/85089574391,"Domestic Pet Care has been an important domain in the healthcare industry. In the presented study, a comprehensive framework of the Smart VetCare system for the health monitoring of domestic pets has been presented. The work is focused on the remote surveillance of domestic animals’ health conditions inside the home environment using IoMT Technology. Specifically, pet health is analyzed for vulnerability in the ambient home environment and ubiquitous activities over a fog computing platform of FogBus. Furthermore, a temporal data granule is formulated and the Probability of Health Vulnerability (PoHV) is defined for determining the health severity of the animal. Additionally, the Temporal Sensitivity Measure (TSM) is defined for real-time pet healthcare analysis, which is visualized using the Self Organized Mapping (SOM) Technique. For validation purposes, the framework is deployed in the smart home environment using 12 IoMT WiSense Nodes and Health Sensor belt for monitoring a domestic dog of American Bully breed over the dynamic resource management platform of FogBus and iFogSim simulator. Based on the comparison with numerous state-of-the-art techniques, the proposed framework can register a better precision value (94.78%), accuracy value (95.38%), sensitivity value (93.71%), and f-measure value (94.41%).",industry
10.1016/j.nucengdes.2020.110817,Journal,Nuclear Engineering and Design,scopus,2020-10-01,sciencedirect,Machine learning enabled advanced manufacturing in nuclear engineering applications,https://api.elsevier.com/content/abstract/scopus_id/85089553568,"Advanced manufacturing has gained tremendous interest in both research and industry in the past few years. Over nearly the same period of time, machine learning (ML) has made phenomenal advancements, finding its way into many aspects of manufacturing. For the nuclear engineering field, the adoption of advanced manufacturing is a compelling argument due to the ambitious challenges the field faces. The combination of advanced manufacturing with ML holds great potential in the nuclear engineering field, and even further development is needed to accelerate their deployment towards real-world applications. This review paper seeks to detail several key aspects of ML enabled advanced manufacturing that are used or could prove useful to nuclear applications ranging from radiation detector materials to reactor parts fabrication. The applications covered here include new material extrapolation, manufacturing defect detection, and additive manufacturing parameters’ optimization.",industry
10.1016/j.aei.2020.101136,Journal,Advanced Engineering Informatics,scopus,2020-10-01,sciencedirect,Ensemble deep learning based semi-supervised soft sensor modeling method and its application on quality prediction for coal preparation process,https://api.elsevier.com/content/abstract/scopus_id/85087393963,"Coal preparation is the most effective and economical technique to reduce impurities and improve the product quality for run-of-mine coal. The timely and accurate prediction for key quality characteristics of separated coal plays a significant role in condition monitoring and production control. However, these quality characteristics are usually difficult to directly measure online in industrial practices Although some computation intelligence based soft sensor modeling methods have been developed and reported in existing research for these quality variables estimation, some problems still exist, i.e., manual feature extraction, considerable unlabeled data, temporal dynamic behavior in data, which will influence the accuracy and efficiency for established soft sensor model. To address above-mentioned problem and develop an more excellent quality prediction model for coal preparation process, a novel deep learning based semi-supervised soft sensor modeling approach is proposed which combining the advantage of unsupervised deep learning technique (i.e., Stacked Auto-Encoder (SAE)) with the advantage of supervised deep bidirectional recurrent learner (i.e., Bidirectional Long Short-Term Memory (BLSTM)). More specifically, the unsupervised SAE networks are implemented to learn the representative features hidden in all available input data (labeled and unlabeled samples) and store them as context vector. Then, partial context vector with corresponding labels and the quality variable measure value at previous time are concatenated to form a new merged input feature vector. After that, the temporal and dynamic features are further extracted from the new merged input feature vector via BLSTM networks. Subsequently, the fully connected layers (FCs) are exploited to learn the higher-level features from the last hidden layer of the BLSTM. Lastly, the learned output features by FCs are fed into a supervised liner regression layer to predict the coal quality metrics. Meanwhile, to avoid over-fitting, some regularization techniques are utilized and discussed in proposed network. The application in ash content estimation for a real dense medium coal preparation process and some comparison experiment result demonstrate that the effectiveness and priority of proposed soft sensor modeling approach.",industry
10.1016/j.patrec.2020.06.028,Journal,Pattern Recognition Letters,scopus,2020-10-01,sciencedirect,On the use of a full stack hardware/software infrastructure for sensor data fusion and fault prediction in industry 4.0,https://api.elsevier.com/content/abstract/scopus_id/85087339064,"Aspects related to prognostics are becoming a crucial part in the industrial sector. In this sense, Industry 4.0 is considered as a new paradigm that leverages on the IoT to propose increasingly more solutions to provide an estimate on the working conditions of an industrial plant. However, in context like the industrial sector where the number and heterogeneity of sensors can be very large, and the time requirements are very stringent, emerges the challenge to design effective infrastructures to interact with these complex systems. In this paper, we propose a full stack hardware/software infrastructure to collect, manage, and analyze the data gathered from a set of heterogeneous sensors attached to a real scale replica industrial plant available in our laboratory. On top of the proposed infrastructure we designed and implemented a fault prediction algorithm which exploits sensors data fusion with the aim to assess the working conditions of the industrial plant. The result section shows the obtained results in terms of accuracy from testing our proposed model and provides a comparison with a traditional Deep Neural Network (DNN) topology.",industry
10.1016/j.ins.2020.05.028,Journal,Information Sciences,scopus,2020-10-01,sciencedirect,Input selection methods for data-driven Soft sensors design: Application to an industrial process,https://api.elsevier.com/content/abstract/scopus_id/85086080455,"Soft Sensors (SSs) are inferential models which are widely used in industry. They are generally built through data-driven approaches that exploit industry historical databases. Selection of input variables is one of the most critical issues in SSs design. This paper aims at highlighting difficulties arising from the implementation of data-driven input selection methods when solving real-world case studies. A procedure is, therefore, proposed for input selection, based on both data-driven and expert-driven input selection methods. The procedure allows designing SSs with good prediction accuracy and a low number of inputs.
                  The design of an SS for a real-world industrial process is used. The results reported show that the selection methods proposed in literature do not give consistent results when applied to the considered case study. The key role for plant expert knowledge emerges, outlining the opportunity of judicious use of automatic data-driven procedures.",industry
10.1016/j.knosys.2020.106178,Journal,Knowledge-Based Systems,scopus,2020-09-27,sciencedirect,Deep learning-based unsupervised representation clustering methodology for automatic nuclear reactor operating transient identification,https://api.elsevier.com/content/abstract/scopus_id/85087409980,"Transient identification of condition monitoring data in nuclear reactor is important for system health assessment. Conventionally, the operating transients are correlated with the pre-designed ones by human operators during operations. However, due to necessary conservatism and significant differences between the operating and pre-designed transients, it has been less effective to manually identify transients, that usually contribute to different system degradation modes. This paper proposes a deep learning-based unsupervised representation clustering method for automatic transient pattern recognition based on the on-site condition monitoring data. Sample entropy is used as indicator for transient extraction, and a pre-training stage is implemented using an auto-encoder architecture for learning high-level features. An iterative representation clustering algorithm is further proposed to enhance the clustering effects, where a novel distance metric learning strategy is integrated. Experiments on a real-world nuclear reactor condition monitoring dataset validate the effectiveness and superiority of the proposed method, which provides a promising tool for transient identification in the real industrial scenarios. This study offers a new perspective in exploring unlabeled data with deep learning, and the end-to-end implementation scheme facilitates applications in the real nuclear industry.",industry
10.1016/j.cjche.2020.06.015,Journal,Chinese Journal of Chemical Engineering,scopus,2020-09-01,sciencedirect,Deep learning technique for process fault detection and diagnosis in the presence of incomplete data,https://api.elsevier.com/content/abstract/scopus_id/85089986909,"In modern industrial processes, timely detection and diagnosis of process abnormalities are critical for monitoring process operations. Various fault detection and diagnosis (FDD) methods have been proposed and implemented, the performance of which, however, could be drastically influenced by the common presence of incomplete or missing data in real industrial scenarios. This paper presents a new FDD approach based on an incomplete data imputation technique for process fault recognition. It employs the modified stacked autoencoder, a deep learning structure, in the phase of incomplete data treatment, and classifies data representations rather than the imputed complete data in the phase of fault identification. A benchmark process, the Tennessee Eastman process, is employed to illustrate the effectiveness and applicability of the proposed method.",industry
10.1016/j.robot.2020.103578,Journal,Robotics and Autonomous Systems,scopus,2020-09-01,sciencedirect,Real-time topological localization using structured-view ConvNet with expectation rules and training renewal,https://api.elsevier.com/content/abstract/scopus_id/85086575996,"Mobile service robots possess high potential of providing numerous assistances in the working areas. In an attempt to develop a mobile service robot which is dynamically balanced for faster movement and taller manipulation capability, we designed and prototyped J4.alpha, which is intended for swift navigation and nimble manipulation. Previously, we devised a pure visual method based on a supervised deep learning model for real-time recognition of nodal locations. Four low-resolution RGB cameras are installed around J4.alpha to capture the surrounding visual features for training and detection. As the method is developed for ease of implementation, fast real-time application, accurate detection, and low cost, we further improve the accuracy and the practicality of the method in this study. Specifically, a set of expectation rules are introduced to reject outlier detections, and a scheme of training renewal is devised to effectively react to environmental modifications. In our previous tests, precision and recall rates of the location coordinate detection by the ConvNet models were generally between 0.78 and 0.91; by introducing the expectation rules, precision and recall are improved by approximately 10%. A large scale field test is also carried out here for both corridor and factory scenarios; the performance of the proposed method was tested for detection accuracy and verified for 2 m and 0.5 m nodal intervals. The scheme of training renewal designed for capturing and reflecting environmental modifications was also proved to be effective.",industry
10.1016/j.scs.2020.102252,Journal,Sustainable Cities and Society,scopus,2020-09-01,sciencedirect,A deep learning-based IoT-oriented infrastructure for secure smart City,https://api.elsevier.com/content/abstract/scopus_id/85085594643,"In recent years, the Internet of Things (IoT) infrastructures are developing in various industrial applications in sustainable smart cities and societies such as smart manufacturing, smart industries. The Cyber-Physical System (CPS) is also part of IoT-oriented infrastructure. CPS has gained considerable success in industrial applications and critical infrastructure with a distributed environment. This system aims to integrate the physical world to computational facilities as cyberspace. However, there are many challenges, such as security and privacy, centralization, communication latency, scalability in such an environment. To mitigate these challenges, we propose a Deep Learning-based IoT-oriented infrastructure for a secure smart city where Blockchain provides a distributed environment at the communication phase of CPS, and Software-Defined Networking (SDN) establishes the protocols for data forwarding in the network. A deep learning-based cloud is utilized at the application layer of the proposed infrastructure to resolve communication latency and centralization, scalability. It enables cost-effective, high-performance computing resources for smart city applications such as the smart industry, smart transportation. Finally, we evaluated the performance of our proposed infrastructure. We compared it with existing methods using quantitative analysis and security and privacy analysis with different measures such as scalability and latency. The evaluation of our implementation results shows that performance is improved.",industry
10.1016/j.compind.2020.103226,Journal,Computers in Industry,scopus,2020-09-01,sciencedirect,Perspective on holonic manufacturing systems: PROSA becomes ARTI,https://api.elsevier.com/content/abstract/scopus_id/85085261123,"Looking back at 30 years of research into holonic manufacturing systems, these explorations made a lasting scientific contribution to the overall architecture of intelligent manufacturing systems. Most notably, holonic architectures are defined in terms of their world-of-interest (Van Brussel et al., 1998). They do not have an information layer, a communication layer, etc. Instead, they have components that relate to real-world assets (e.g. machine tools) and activities (e.g. assembly). And, they mirror and track the structure of their world-of-interest, which allows them to scale and adapt accordingly.
                  This research has wandered around, at times learning from its mistakes, and progressively carved out an invariant structure while it translated and applied scientific insights from complex-adaptive systems theory (e.g. autocatalytic sets) and from bounded rationality (e.g. holons). This paper presents and discusses the outcome of these research efforts.
                  At the top level, the holonic structure distinguishes intelligent beings (or digital twins) from intelligent agents. These digital twins inherit the consistency from reality, which they mirror. They are intelligent beings when they reflect what exists in the world without imposing artificial limitations in this reality. Consequently, a conflict with a digital twin is a conflict with reality.
                  In contrast, intelligent agents typically transform NP-hard challenges into computations with low-polynomial complexity. Unavoidably, this involves arbitrariness (e.g. don’t care choices). Likewise, relying on case-specific properties, to ensure an outcome in polynomial time, usually renders the validity of an agent’s choices both short-lived and situation-dependent. Here, intelligent agents create conflicts by imposing limitations of their own making in their world-of-interest.
                  Real-world smart systems are aggregates comprising both intelligent beings and intelligent agents. They are performers. Inside these performers, digital twins may constitute the foundations, supporting walls, support beams and pillars because these intelligent beings are protected by their real-world counterpart. Further refining the top-level of this architecture, a holonic structure enables these digital twins to shadow their real-world counterpart whenever it changes, adapts and evolves.
                  In contrast, the artificial limitations, imposed by the intelligent agents, cannot be allowed to build up inertia, which would hamper the undoing of arbitrary or case-specific limitations. To this end, performers explicitly manage the rights over their assets. Revoking such rights from a limitation-imposing agent will free the assets. This will be at the cost of reduced services from the agent. When other service providers rely on this agent, their services may be affected as well; that’s how the inertia builds up and how harmful legacy is created. Thus, the services of digital twins are to be preferred over the services of an intelligent agent by developers of holonic manufacturing systems.
                  Finally, digital twins corresponding to the decision making in the world-of-interest (a non-physical asset) allow to mirror the world-of-interest in a predictive mode (in addition to track and trace). It allows to generate short-term forecasts while preserving the benefits of intelligent beings. These twins are the intentions of the decision-making intelligent agents. Evidently, when intentions change, the forecasts needs to be regenerated (i.e. tracking the corresponding reality by the twin). This advanced feature can be deployed in a number of configurations (cf. annex).",industry
10.1016/j.compind.2020.103244,Journal,Computers in Industry,scopus,2020-09-01,sciencedirect,Machine learning for predictive scheduling and resource allocation in large scale manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85084401966,"The digitalization processes in manufacturing enterprises and the integration of increasingly smart shop floor devices and software control systems caused an explosion in the data points available in Manufacturing Execution Systems. The degree in which enterprises can capture value from big data processing and extract useful insights represents a differentiating factor in developing controls that optimize production and protect resources. Machine learning and Big Data technologies have gained increased traction being adopted in some critical areas of planning and control. Cloud manufacturing allows using these technologies in real time, lowering the cost of implementing and deployment. In this context, the paper offers a machine learning approach for reality awareness and optimization in cloud.
                  Specifically, the paper focuses on predictive production planning (operation scheduling, resource allocation) and predictive maintenance. The main contribution of this research consists in developing a hybrid control solution that uses Big Data techniques and machine learning algorithms to process in real time information streams in large scale manufacturing systems, focusing on energy consumptions that are aggregated at various layers. The control architecture is distributed at the edge of the shop floor for data collecting and format transformation, and then centralized at the cloud computing platform for data aggregation, machine learning and intelligent decisions. The information is aggregated in logical streams and consolidated based on relevant metadata; a neural network is trained and used to determine possible anomalies or variations relative to the normal patterns of energy consumption at each layer. This novel approach allows for accurate forecasting of energy consumption patterns during production by using Long Short-term Memory neural networks and deep learning in real time to re-assign resources (for batch cost optimization) and detect anomalies (for robustness) based on predicted energy data.",industry
10.1016/j.neucom.2020.02.109,Journal,Neurocomputing,scopus,2020-08-04,sciencedirect,Tracking control of redundant mobile manipulator: An RNN based metaheuristic approach,https://api.elsevier.com/content/abstract/scopus_id/85082490397,"In this paper, we propose a topology of Recurrent Neural Network (RNN) based on a metaheuristic optimization algorithm for the tracking control of mobile-manipulator while enforcing nonholonomic constraints. Traditional approaches for tracking control of mobile robots usually require the computation of Jacobian-inverse or linearization of its mathematical model. The proposed algorithm uses a nature-inspired optimization approach to directly solve the nonlinear optimization problem without any further transformation. First, we formulate the tracking control as a constrained optimization problem. The optimization problem is formulated on position-level to avoid the computationally expensive Jacobian-inversion. The nonholonomic limitation is ensured by adding equality constraints to the formulated optimization problem. We then present the Beetle Antennae Olfactory Recurrent Neural Network (BAORNN) algorithm to solve the optimization problem efficiently using very few mathematical operations. We present a theoretical analysis of the proposed algorithm and show that its computational cost is linear with respect to the degree of freedoms (DOFs), i.e., O(m). Additionally, we also prove its stability and convergence. Extensive simulation results are prepared using a simulated model of IIWA14, a 7-DOF industrial-manipulator, mounted on a differentially driven cart. Comparison results with particle swarm optimization (PSO) algorithm are also presented to prove the accuracy and numerical efficiency of the proposed controller. The results demonstrate that the proposed algorithm is several times (around 75 in the worst case) faster in execution as compared to PSO, and suitable for real-time implementation. The tracking results for three different trajectories; circular, rectangular, and rhodonea paths are presented.",industry
10.1016/j.heliyon.2020.e04667,Journal,Heliyon,scopus,2020-08-01,sciencedirect,Effects of mobile augmented reality apps on impulse buying behavior: An investigation in the tourism field,https://api.elsevier.com/content/abstract/scopus_id/85089806662,"Many of today's online services are designed specifically to encourage impulse buying. Moreover, many studies have shown that with the assistance of Mobile Augmented Reality, retailers have the potential to significantly improve their sales. However, the effects of Mobile AR on consumer impulse buying behavior have yet to be examined, particularly in the tourism field. Consequently, the present study integrates the Technology Acceptance Model (TAM), Stimulus-Organism-Response (SOR) framework, and flow theory to examine the effects of Mobile AR apps on tourist impulse buyingbehavior. The research model is implemented using an online questionnaire, with the results analyzed by Partial-Least-Squares Structural Equation Modeling (PLS-SEM) approach. The results obtained from 479 valid samples show that the characteristics of Mobile AR apps play an important role in governing tourist behavior in making unplanned purchases. In particular, as the utility, ease-of-use, and interactivity of the apps increase, the perceived enjoyment and satisfaction of the user also increase and give rise to a stronger impulse buying behavior. The results also reveal a mediating effect of the flow experience on the relationship between the perceived ease of use of the Mobile AR app and the user satisfaction in using the app. Overall, the findings presented in this study provide a useful source of reference for Mobile AR app developers, retailers, and tourism marketers in better understanding users' preferences for Mobile AR apps and strengthening their impulse buying behavior in the tourism context as a result.",industry
10.1016/j.aei.2020.101101,Journal,Advanced Engineering Informatics,scopus,2020-08-01,sciencedirect,Predictive model-based quality inspection using Machine Learning and Edge Cloud Computing,https://api.elsevier.com/content/abstract/scopus_id/85084733420,"The supply of defect-free, high-quality products is an important success factor for the long-term competitiveness of manufacturing companies. Despite the increasing challenges of rising product variety and complexity and the necessity of economic manufacturing, a comprehensive and reliable quality inspection is often indispensable. In consequence, high inspection volumes turn inspection processes into manufacturing bottlenecks.
                  In this contribution, we investigate a new integrated solution of predictive model-based quality inspection in industrial manufacturing by utilizing Machine Learning techniques and Edge Cloud Computing technology. In contrast to state-of-the-art contributions, we propose a holistic approach comprising the target-oriented data acquisition and processing, modelling and model deployment as well as the technological implementation in the existing IT plant infrastructure. A real industrial use case in SMT manufacturing is presented to underline the procedure and benefits of the proposed method. The results show that by employing the proposed method, inspection volumes can be reduced significantly and thus economic advantages can be generated.",industry
10.1016/j.physa.2019.124049,Journal,Physica A: Statistical Mechanics and its Applications,scopus,2020-08-01,sciencedirect,Fast Super-Paramagnetic Clustering,https://api.elsevier.com/content/abstract/scopus_id/85078038012,"We map stock market interactions to spin models to recover their hierarchical structure using a simulated annealing based Super-Paramagnetic Clustering (SPC) algorithm. This is directly compared to a modified implementation of a maximum likelihood approach we call fast Super-Paramagnetic Clustering (f-SPC). The methods are first applied to standard toy test-case problems, and then to a data-set of 447 stocks traded on the New York Stock Exchange (NYSE) over 1249 days. The signal to noise ratio of stock market correlation matrices is briefly considered. Our result recover approximately clusters representative of standard economic sectors and mixed ones whose dynamics shine light on the adaptive nature of financial markets and raise concerns relating to the effectiveness of industry based static financial market classification in the world of real-time data analytics. A key result is that we show that f-SPC maximum likelihood solutions converge to ones found within the Super-Paramagnetic Phase where the entropy is maximum, and those solutions are qualitatively better for high dimensionality data-sets.",industry
10.1016/j.measurement.2020.107768,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2020-07-15,sciencedirect,"Intelligent fault diagnosis of rotating machinery via wavelet transform, generative adversarial nets and convolutional neural network",https://api.elsevier.com/content/abstract/scopus_id/85082880587,"The fault detection of rotating machinery systems especially its typical components such as bearings and gears is of special importance for maintaining machine systems working normally and safely. However, due to the change of working conditions, the disturbance of environment noise, the weakness of early features and various unseen compound failure modes, it is quite hard to achieve high-accuracy intelligent failure monitoring task of rotating machinery using existing intelligent fault diagnosis approaches in real industrial applications. In the paper, a novel and high-accuracy fault detection approach named WT-GAN-CNN for rotating machinery is presented based on Wavelet Transform (WT), Generative Adversarial Nets (GANs) and convolutional neural network (CNN). The proposed WT-GAN-CNN approach includes three parts. To begin with, WT is employed for extracting time-frequency image features from one-dimension raw time domain signals. Secondly, GANs are used to generate more training image samples. Finally, the built CNN model is used to accomplish the fault detection of rotating machinery by the original training time-frequency images and the generated fake training time-frequency images. Two experiment studies are implemented to assess the effectiveness of our proposed approach and the results demonstrate it is higher in testing accuracy than other intelligent failure detection approaches in the literatures even in the interference of strong environment noise or when working conditions are changed. Furthermore, its result in the stability of testing accuracy is also quite excellent.",industry
10.1016/j.telpol.2020.101960,Journal,Telecommunications Policy,scopus,2020-07-01,sciencedirect,Innovation ecosystems theory revisited: The case of artificial intelligence in China,https://api.elsevier.com/content/abstract/scopus_id/85083340447,"Beyond the mainstream discussion on the key role of China in the global AI landscape, the knowledge about the real performance and future perspectives of the AI ecosystem in China is still limited. This paper evaluates the status and prospects of China's AI innovation ecosystem by developing a Triple Helix framework particularized for this case. Based on an in-depth qualitative study and on interviews with experts, the analysis section summarizes the way in which the AI innovation ecosystem in China is being built, which are the key features of the three spheres of the Triple Helix -governments, industry and academic/research institutions-as well as the dynamic context of the ecosystem through the identification of main aspects related to the flows of skills, knowledge and funding and the interactions among them. Using this approach, the discussion section illustrates the specificities of the AI innovation ecosystem in China, its strengths and its gaps, and which are its prospects. Overall, this revisited ecosystem approach permits the authors to address the complexity of emerging environments of innovation to draw meaningful conclusions which are not possible with mere observation. The results show how a favourable context, the broad adoption rate and the competition for talent and capital among regional-specialized clusters are boosting the advance of AI in China, mainly in the business to customer arena. Finally, the paper highlights the challenges ahead in the current implementation of the ecosystem that will largely determine the potential global leadership of China in this domain.",industry
10.1016/j.ins.2020.03.063,Journal,Information Sciences,scopus,2020-07-01,sciencedirect,Generating behavior features for cold-start spam review detection with adversarial learning,https://api.elsevier.com/content/abstract/scopus_id/85083304717,"Due to the wide applications, spam detection has long been a hot research topic in both academia and industry. Existing studies show that behavior features are effective in distinguishing the spam and legitimate reviews. However, it usually takes a long time to collect such features and thus is hard to apply them to cold-start spam review detection tasks. Recent advances leveraged the neural network to encode the various types of textual, behavior, and attribute information for this task. However, the inherent problem, i.e., lack of effective behavior features for new users who post just one review, is still unsolved.
                  In this paper, we exploit the generative adversarial network (GAN) for addressing this problem. The key idea is to generate synthetic behavior features (SBFs) for new users from their easily accessible features (EAFs). Specifically, we first select six well recognized real behavior features (RBFs) existing for regular users. We then train a GAN framework including a generator to generate SBFs from their EAFs including text, rating, and attribute features, and a discriminator to discriminate RBFs and SBFs. We design a new implementation of generator and discriminator for effective training. The trained GAN is finally applied to new users for generating synthetic behavior features. We conduct extensive experiments on two Yelp datasets. Experimental results demonstrate that our proposed framework significantly outperforms the state-of-the-art methods.",industry
10.1016/j.petrol.2020.107087,Journal,Journal of Petroleum Science and Engineering,scopus,2020-07-01,sciencedirect,Transformation of academic teaching and research: Development of a highly automated experimental sucker rod pumping unit,https://api.elsevier.com/content/abstract/scopus_id/85079611752,"Sucker rod pumps are one of the most popular solutions for artificial lift since their inception in the 19th century with minimum changes in design. Presently, companies are deploying digital technology in the field and, there has been a big push for a networked oilfield in recent years. This means technology is now able to control machines in remote places, evaluate their performances and control safety operating parameters. But these digital solutions are still not available in universities, causing a technological and technical gap for students and researchers.
                  This study presents a prototype of a new dedicated Interactive Digital Sucker Rod Pumping Unit (ID-SRP) system at the University of Oklahoma with representative operating conditions. The prototype mimics sucker rod pump working principles and also imitates different realistic rod string motions. The application and solutions are focused on providing authentic learning experiences for petroleum engineers. The system is also designed to address and optimize SRP well performance and safety through Model Predictive Controller (MPC) implementation and meeting industrial requirements. It connects the physical and virtual interaction with learning technologies. The objective is to bridge the tangible and the abstract for a better understanding of sucker rod concept and implement existing theories into the digital system. Additionally, it aids our future petroleum engineers on how to apply basic industry principles and upsurge their problem-solving skills.
                  The developed unit is capable of simulating any situations in real time and using Internet of Things (IoT) for data acquisition to create tailored diagnostic tools that students and laboratory staff can utilize. The software selected for the system is LabVIEW, which controls all the necessary equipment. This system can build personalized dynocard graphs, intake live data and export them to other programs live Excel, MATLAB, Python or any other programming languages.",industry
10.1016/j.eswa.2020.113251,Journal,Expert Systems with Applications,scopus,2020-07-01,sciencedirect,Integrating complex event processing and machine learning: An intelligent architecture for detecting IoT security attacks,https://api.elsevier.com/content/abstract/scopus_id/85079340111,"The Internet of Things (IoT) is growing globally at a fast pace: people now find themselves surrounded by a variety of IoT devices such as smartphones and wearables in their everyday lives. Additionally, smart environments, such as smart healthcare systems, smart industries and smart cities, benefit from sensors and actuators interconnected through the IoT. However, the increase in IoT devices has brought with it the challenge of promptly detecting and combating the cybersecurity attacks and threats that target them, including malware, privacy breaches and denial of service attacks, among others. To tackle this challenge, this paper proposes an intelligent architecture that integrates Complex Event Processing (CEP) technology and the Machine Learning (ML) paradigm in order to detect different types of IoT security attacks in real time. In particular, such an architecture is capable of easily managing event patterns whose conditions depend on values obtained by ML algorithms. Additionally, a model-driven graphical tool for security attack pattern definition and automatic code generation is provided, hiding all the complexity derived from implementation details from domain experts. The proposed architecture has been applied in the case of a healthcare IoT network to validate its ability to detect attacks made by malicious devices. The results obtained demonstrate that this architecture satisfactorily fulfils its objectives.",industry
10.1016/j.neucom.2020.01.083,Journal,Neurocomputing,scopus,2020-06-07,sciencedirect,Integrating adaptive moving window and just-in-time learning paradigms for soft-sensor design,https://api.elsevier.com/content/abstract/scopus_id/85079267624,"Most applications of soft sensors in process industries require learning from a stream of data, which may exhibit nonstationary dynamics, or concept drift. In this study, we develop a relevance vector machine (RVM) based novel adaptive learning algorithm called MWAdp-JITL, to meet the demands of continuous processes. The resulting algorithm combines active and passive learning: A moving window (MW) algorithm, which adapts the window size against virtual/real concept drifts, is coupled with a just-in-time learning (JITL) model, constructed using an appropriate region of historical data, and the ensemble weights of the MW and JITL models are adjusted for each query point. Tests on four real industrial datasets and a synthetic data, comprising various concept drift scenarios, show that MWAdp-JITL yields superior prediction accuracy and is generally more robust to changes in algorithm parameters compared to conventional adaptive learning methods and state-of-the-art algorithms from the literature. MWAdp-JITL complies with time limits of online prediction, and is applicable for high dimensional processes under various types of concept drifts. It is seen that MWAdp-JITL can successfully achieve a good balance in bias-variance tradeoff, justifying the use of only two exquisitely selected learners in ensemble learning.",industry
10.1016/j.cola.2020.100970,Journal,Journal of Computer Languages,scopus,2020-06-01,sciencedirect,"Visual Programming Environments for End-User Development of intelligent and social robots, a systematic review",https://api.elsevier.com/content/abstract/scopus_id/85085272330,"Robots are becoming interactive and robust enough to be adopted outside laboratories and in industrial scenarios as well as interacting with humans in social activities. However, the design of engaging robot-based applications requires the availability of usable, flexible and accessible development frameworks, which can be adopted and mastered by researchers and practitioners in social sciences and adult end users as a whole. This paper surveys Visual Programming Environments aimed at enabling a paradigm fostering the so-called End-User Development of applications involving robots with social capabilities. The focus of this article is on those Visual Programming Environments that are designed to support social research goals as well as to cater for professional needs of people not trained in more traditional text-based computer programming languages. This survey excludes interfaces aimed at supporting expert programmers, at allowing industrial robots to perform typical industrial tasks (such as pick and place operations), and at teaching children how to code. After having performed a systematic search, sixteen programming environments have been included in this survey. Our goal is two-fold: first, to present these software tools with their technical features and Authoring Artificial Intelligence modeling approaches, and second, to present open challenges in the development of Visual Programming Environments for end users and social researchers, which can be informative and valuable to the community. The results show that the most recent such tools are adopting distributed and Component-Based Software Engineering approaches and web technologies. However, few of them have been designed to enable the independence of end users from high-tech scribes. Moreover, findings indicate the need for (i) more objective and comparative evaluations, as well as usability and user experience studies with real end users; and (ii) validations of these tools for designing applications aimed at working “in-the-wild” rather than only in laboratories and structured settings.",industry
10.1016/j.asoc.2020.106208,Journal,Applied Soft Computing Journal,scopus,2020-06-01,sciencedirect,Dynamic scheduling for flexible job shop with new job insertions by deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85081140568,"In modern manufacturing industry, dynamic scheduling methods are urgently needed with the sharp increase of uncertainty and complexity in production process. To this end, this paper addresses the dynamic flexible job shop scheduling problem (DFJSP) under new job insertions aiming at minimizing the total tardiness. Without lose of generality, the DFJSP can be modeled as a Markov decision process (MDP) where an intelligent agent should successively determine which operation to process next and which machine to assign it on according to the production status of current decision point, making it particularly feasible to be solved by reinforcement learning (RL) methods. In order to cope with continuous production states and learn the most suitable action (i.e. dispatching rule) at each rescheduling point, a deep Q-network (DQN) is developed to address this problem. Six composite dispatching rules are proposed to simultaneously select an operation and assign it on a feasible machine every time an operation is completed or a new job arrives. Seven generic state features are extracted to represent the production status at a rescheduling point. By taking the continuous state features as input to the DQN, the state–action value (Q-value) of each dispatching rule can be obtained. The proposed DQN is trained using deep Q-learning (DQL) enhanced by two improvements namely double DQN and soft target weight update. Moreover, a “softmax” action selection policy is utilized in real implementation of the trained DQN so as to promote the rules with higher Q-values while maintaining the policy entropy. Numerical experiments are conducted on a large number of instances with different production configurations. The results have confirmed both the superiority and generality of DQN compared to each composite rule, other well-known dispatching rules as well as the stand Q-learning-based agent.",industry
10.1016/j.rcim.2019.101887,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2020-06-01,sciencedirect,Deep learning-based smart task assistance in wearable augmented reality,https://api.elsevier.com/content/abstract/scopus_id/85074770255,"Wearable augmented reality (AR) smart glasses have been utilized in various applications such as training, maintenance, and collaboration. However, most previous research on wearable AR technology did not effectively supported situation-aware task assistance because of AR marker-based static visualization and registration. In this study, a smart and user-centric task assistance method is proposed, which combines deep learning-based object detection and instance segmentation with wearable AR technology to provide more effective visual guidance with less cognitive load. In particular, instance segmentation using the Mask R-CNN and markerless AR are combined to overlay the 3D spatial mapping of an actual object onto its surrounding real environment. In addition, 3D spatial information with instance segmentation is used to provide 3D task guidance and navigation, which helps the user to more easily identify and understand physical objects while moving around in the physical environment. Furthermore, 2.5D or 3D replicas support the 3D annotation and collaboration between different workers without predefined 3D models. Therefore, the user can perform more realistic manufacturing tasks in dynamic environments. To verify the usability and usefulness of the proposed method, we performed quantitative and qualitative analyses by conducting two user studies: 1) matching a virtual object to a real object in a real environment, and 2) performing a realistic task, that is, the maintenance and inspection of a 3D printer. We also implemented several viable applications supporting task assistance using the proposed deep learning-based task assistance in wearable AR.",industry
10.1016/j.comcom.2020.04.053,Journal,Computer Communications,scopus,2020-05-15,sciencedirect,IOT and cloud computing based parallel implementation of optimized RBF neural network for loader automatic shift control,https://api.elsevier.com/content/abstract/scopus_id/85089243295,"One of the key issues in automatic shift control of V-type cyclical loaders is determining how to find the best gear for the current conditions according to a certain mapping relation, but this complex and nonlinear mapping is difficult to express by a mathematical relation. However, to solve such nonlinear problems, a radial basis function (RBF) neural network is the best choice. In this paper, a certain type of wheel loader is taken as the research object, and an RBF neural network algorithm based on an improved genetic algorithm (GA) optimization is proposed. The global search ability of the GA is improved by adaptively adjusting the crossover probability and mutation probability. The RBF neural network expansion coefficient is optimized by an improved GA. Using industrial IOT technology, an optimized RBF neural network based on Map-Reduce on a cloud computing cluster is designed. The diesel engine computer and transmission computer on the loader are integrated to achieve dual-processor distributed parallel data processing and calculation. Then the loader automatic variable speed control algorithm model of improved GA optimized RBF neural network based on IOT cloud computing is established. The network model is trained and simulated using real vehicle automatic shift test data. The simulation results show that the improved GA-RBF neural network algorithm can achieve a correct recognition rate of 97.92%. The error matrix norm reaches the minimum value when the algorithm is iterated to the 17th generation. The improved algorithm has the advantages of a high gear recognition rate, fast convergence speed and strong real-time shift performance and is an effective new shift control method. The test results show that the shift boost time is less than 0.15 s and has a certain gradient. Compared with the manual shift process performed in the past, some improvements are achieved in the optimal shift time, shift response speed and shift quality. Compared with the traditional single computer based on serial training RBF neural network learning algorithm, whether it is Great progress has been made in convergence speed, training time, recognition rate, and data processing capabilities. Through the simulation and test, the validity of the intelligent shift control method of the improved GA optimized RBF neural network based on IOT cloud computing is verified. It has better engineering application value.",industry
10.1016/j.microrel.2020.113640,Journal,Microelectronics Reliability,scopus,2020-05-01,sciencedirect,Two phase cooling with nano-fluid for highly dense electronic systems-on-chip – A pilot study,https://api.elsevier.com/content/abstract/scopus_id/85083093178,"In recent days, electronics gadgets need to design for higher functionalities with dense populated systems in order to meet the demands like lower in size, weight and power consumption. Even industrial electronic component and system design also prefer same slim fashion. On other hand the overheating of electronic components reduces its performance, life and by the way the reliability of such electronic product/system is greatly affected due to overheating. The conventional cooling methods failed to offer best performances. Hence this part of research proposed a effective two phase cooling technique with nano-fluid. The objective of this research is to maintain the maximum temperature at the junction and hot spots in order to break a new ground in the cooling of electronic systems. The Maximum permissible operating temperature for any commercial electronic applications is only upto 70 °C (equal to 343.15 K) and above which most of the inherent electronic circuits may malfunction and destroy the entire application.The HotSpot Simulator-6.0 software employed for establish, verify the simulated model and trial runs to answer many ‘what if’ questions. In the simulation, hottest spot has been found in Int_Reg region, where the steady temperature grows beyond the threshold temperature level. The temperature has to be decreased in order to provide reliable working environment. Hence, HFO 1234ze nano-fluid employed with flow rate of 1100 ml per minute. The nanofliuid minimizes the temperature of the simulated electronic circuit from 351.80 K to 326.86 K in UUT. The proposed two phase nano-fluid cooling system for 3-D Unit-Under Test (UUT) was verified and validated with real system and simulated for experiments. Thus, a high range of temperature difference from the initial and final steady state temperature has been evidently shown in the proposed two phase nano fluid cooling method. The system found outperforms as best of both the worlds. The nanofluid cooling system can be used in thermal-aware systems and highly dense systems to maintain the temperature not much than 343.15 K, even at full load conditions.",industry
10.1016/j.engappai.2020.103589,Journal,Engineering Applications of Artificial Intelligence,scopus,2020-05-01,sciencedirect,Deep reinforcement one-shot learning for artificially intelligent classification in expert aided systems,https://api.elsevier.com/content/abstract/scopus_id/85081990367,"In recent years there has been a sharp rise in applications, in which significant events need to be classified but only a few training instances are available. These are known as cases of one-shot learning. To handle this challenging task, organizations often use human analysts to classify events under high uncertainty. Existing algorithms use a threshold-based mechanism to decide whether to classify an object automatically or send it to an analyst for deeper inspection. However, this approach leads to a significant waste of resources since it does not take the practical temporal constraints of system resources into account. By contrast, the focus in this paper is on rigorously optimizing the resource consumption in the system which applies to broad application domains, and is of a significant interest for academic research, industrial developments, as well as society and citizens benefit. The contribution of this paper is threefold. First, a novel Deep Reinforcement One-shot Learning (DeROL) framework is developed to address this challenge. The basic idea of the DeROL algorithm is to train a deep-Q network to obtain a policy which is oblivious to the unseen classes in the testing data. Then, in real-time, DeROL maps the current state of the one-shot learning process to operational actions based on the trained deep-Q network, to maximize the objective function. Second, the first open-source software for practical artificially intelligent one-shot classification systems with limited resources is developed for the benefit of researchers and developers in related fields. Third, an extensive experimental study is presented using the OMNIGLOT dataset for computer vision tasks, the UNSW-NB15 dataset for intrusion detection tasks, and the Cleveland Heart Disease Dataset for medical monitoring tasks that demonstrates the versatility and efficiency of the DeROL framework.",industry
10.1016/j.ijmultiphaseflow.2019.103194,Journal,International Journal of Multiphase Flow,scopus,2020-05-01,sciencedirect,Bubble patterns recognition using neural networks: Application to the analysis of a two-phase bubbly jet,https://api.elsevier.com/content/abstract/scopus_id/85079560188,"Gas-liquid two-phase bubbly flows are found in different areas of science and technology such as nuclear energy, chemical industry, or piping systems. Optical diagnostics of two-phase bubbly flows with modern panoramic techniques makes it possible to capture simultaneously instantaneous characteristics of both continuous and dispersed phases with a high spatial resolution. In this paper, we introduce a novel approach based on neural networks to recognize bubble patterns in images and identify their geometric parameters. The originality of the proposed method consists in training of a neural network ensemble using synthetic images that resemble real photographs gathered in experiment. The use of neural networks in combination with automatically generated data allowed us to detect overlapping, blurred, and non-spherical bubbles in a broad range of volume gas fractions. Experiments on a turbulent bubbly jet proved that the implemented method increases the identification accuracy, reducing errors of various kinds, and lowers the processing time compared to conventional recognition methods. Furthermore, utilizing the new method of bubbles recognition, the primary physical parameters of a dispersed phase, such as bubble size distribution and local gas content, were calculated in a near-to-nozzle region of the bubbly jet. The obtained results and integral experimental parameters, especially volume gas fraction, are in good agreement with each other.",industry
10.1016/j.marpol.2020.103829,Journal,Marine Policy,scopus,2020-05-01,sciencedirect,Analyzing gaps in policy: Evaluation of the effectiveness of minimum landing size (MLS) regulations in Turkey,https://api.elsevier.com/content/abstract/scopus_id/85079518573,"The Mediterranean and Black Sea host the most intense overfishing and Turkey has the largest commercial fisheries in them (when both seas considered). However, the state of the Turkish fisheries is in critical condition as both the quality (i.e, in number of caught species, value and sizes of fish) and quantity of fisheries catches have been rapidly declining in recent decades. One pioneer fisheries management initiative thoroughly evaluated here pertains to minimum landing size (MLS) regulations for commercial taxa, with the aim of promoting stock sustainability by ensuring fish reproduce before they are caught. This study examines 29 taxa in relation to MLS by analyzing changes in catch per unit effort trends pre-and post MLS to gauge regulation effectiveness, changes to MLS regulations since implementation, and finally evaluates the Turkish MLS sizes in relation to Turkish maturity sizes, to provide advice for taxa requiring changes. It seems intensive fishing may have reduced the size at maturity for many species in Turkey, as they mature smaller here than the Mediterranean and global averages. Eleven taxa listed in MLS regulations are under the lengths of first maturity (Lmat) sizes in Turkish waters and need to be increased, especially that of bonito, hake, swordfish and bluefish (by 18 cm, 10 cm, 10 cm and 8 cm, respectively), while 16 taxa still require national studies to determine their Lmat sizes in Turkish waters. In conclusion, in Turkey, MLS regulations are completely ineffective due to a lack of monitoring and control for juvenile fish at landing sites, markets and processing plants, along with insufficient penalties for such infractions, yet, there remains plenty of room for improvement. To improve the state of the fisheries, MLS measures could be improved by increasing fines, monitoring and control, making some gear types more selective and use of real-time closures and no fishing zones to protect spawning and nursery habitats.",industry
10.1016/j.ress.2020.106821,Journal,Reliability Engineering and System Safety,scopus,2020-05-01,sciencedirect,Towards Efficient Robust Optimization using Data based Optimal Segmentation of Uncertain Space,https://api.elsevier.com/content/abstract/scopus_id/85078707908,"Performing multi-objective optimization under uncertainty is a common requirement in industries and academia. Robust optimization (RO) is considered as an efficient and tractable approach provided one has access to behavioral data for the uncertain parameters. However, solutions of RO may be far from the real solution and less reliable due to inability to map the uncertain space accurately, especially when the data appears discontinuous and scattered in the uncertain domain. Amalgamating machine learning algorithms with RO, this paper proposes a data-driven methodology, where a novel fuzzy clustering mechanism is implemented along-with boundary construction, to transcript the uncertain space such that the specific regions of uncertainty are identified. Subsequently, using intelligent Sobol sampling, samples are generated in the mapped uncertain regions. Results of two test cases are presented along with a comprehensive comparison study. Considered case-studies include highly nonlinear model for continuous casting process from steelmaking industries, where a multi-objective optimization problem under uncertainty is solved to balance the conflict between productivity and energy consumption. The Pareto-optimal solutions of the resulting RO problem are obtained through Non-Dominated Sorting Genetic Algorithm – II, and ~23–29% improvement is observed in the uncertain objective function. Further, the spread and diversity metrics are enhanced by ~10–95% as compared to those obtained using other standard uncertainty sets.",industry
10.1016/j.jss.2020.110519,Journal,Journal of Systems and Software,scopus,2020-05-01,sciencedirect,Traceability Link Recovery between Requirements and Models using an Evolutionary Algorithm Guided by a Learning to Rank Algorithm: Train control and management case,https://api.elsevier.com/content/abstract/scopus_id/85078162306,"Traceability Link Recovery (TLR) has been a topic of interest for many years within the software engineering community. In recent years, TLR has been attracting more attention, becoming the subject of both fundamental and applied research. However, there still exists a large gap between the actual needs of industry on one hand and the solutions published through academic research on the other.
                  In this work, we propose a novel approach, named Evolutionary Learning to Rank for Traceability Link Recovery (TLR-ELtoR). TLR-ELtoR recovers traceability links between a requirement and a model through the combination of evolutionary computation and machine learning techniques, generating as a result a ranking of model fragments that can realize the requirement.
                  TLR-ELtoR was evaluated in a real-world case study in the railway domain, comparing its outcomes with five TLR approaches (Information Retrieval, Linguistic Rule-based, Feedforward Neural Network, Recurrent Neural Network, and Learning to Rank). The results show that TLR-ELtoR achieved the best results for most performance indicators, providing a mean precision value of 59.91%, a recall value of 78.95%, a combined F-measure of 62.50%, and a MCC value of 0.64. The statistical analysis of the results assesses the magnitude of the improvement, and the discussion presents why TLR-ELtoR achieves better results than the baselines.",industry
10.1016/j.ejor.2019.10.015,Journal,European Journal of Operational Research,scopus,2020-05-01,sciencedirect,From one-class to two-class classification by incorporating expert knowledge: Novelty detection in human behaviour,https://api.elsevier.com/content/abstract/scopus_id/85074814083,"One-class classification is the standard procedure for novelty detection. Novelty detection aims to identify observations that deviate from a determined normal behaviour. Only instances of one class are known, whereas so called novelties are unlabelled. Traditional novelty detection applies methods from the field of outlier detection. These standard one-class classification approaches have limited performance in many real business cases. The traditional techniques are mainly developed for industrial problems such as machine condition monitoring. When applying these to human behaviour, the performance drops significantly. This paper proposes a method that improves existing approaches by creating semi-synthetic novelties in order to have labelled data for the two classes. Expert knowledge is incorporated in the initial phase of this data generation process. The method was deployed on a real-life test case where the goal was to detect fraudulent subscriptions to a telecom family plan. This research demonstrates that the two-class expert model outperforms a one-class model on the semi-synthetic dataset. In a next step the model was validated on a real dataset. A fraud detection team of the company manually checked the top predicted novelties. The results show that incorporating expert knowledge to transform a one-class problem into a two-class problem is a valuable method.",industry
10.1016/j.dss.2020.113266,Journal,Decision Support Systems,scopus,2020-04-01,sciencedirect,ForeSim-BI: A predictive analytics decision support tool for capacity planning,https://api.elsevier.com/content/abstract/scopus_id/85079423691,"This paper proposes a decision support tool for maintenance capacity planning of complex product systems. The tool – ForeSim-BI – addresses the problem faced by maintenance organizations in forecasting the workload of future maintenance interventions and in planning an adequate capacity to face that expected workload. Developed and implemented from a predictive analytics perspective in the particular context of a Portuguese aircraft maintenance organization, the tool integrates four main modules: (1) a forecasting module used to predict future and unprecedented maintenance workloads from historical data; (2) a Bayesian inference module used to transform prior workload forecasts, resulting from the forecasting module, into predictive forecasts after observations on the maintenance interventions being predicted become available; (3) a simulation module used to characterize the forecasted total workloads through sets of random variables, including maintenance work types, maintenance work phases, and maintenance work skills; and (4) a Bayesian network module used to combine the simulated workloads with historical data through probabilistic inference. A linear programming model is also developed to improve the efficiency of the decision-making process supported by Bayesian networks. The tool uses real industrial data, comprising 171 aircraft maintenance projects collected at the host organization, and is validated by comparing its results with real observations of a given maintenance intervention to which predictions were made and with a model simulating current forecasting practices employed in industry. Significantly more accurate forecasts have been obtained with the proposed tool, resulting in an important cost saving potential for maintenance organizations.",industry
10.1016/j.aei.2020.101052,Journal,Advanced Engineering Informatics,scopus,2020-04-01,sciencedirect,Deep learning-based method for vision-guided robotic grasping of unknown objects,https://api.elsevier.com/content/abstract/scopus_id/85079340469,"Nowadays, robots are heavily used in factories for different tasks, most of them including grasping and manipulation of generic objects in unstructured scenarios. In order to better mimic a human operator involved in a grasping action, where he/she needs to identify the object and detect an optimal grasp by means of visual information, a widely adopted sensing solution is Artificial Vision. Nonetheless, state-of-art applications need long training and fine-tuning for manually build the object’s model that is used at run-time during the normal operations, which reduce the overall operational throughput of the robotic system. To overcome such limits, the paper presents a framework based on Deep Convolutional Neural Networks (DCNN) to predict both single and multiple grasp poses for multiple objects all at once, using a single RGB image as input. Thanks to a novel loss function, our framework is trained in an end-to-end fashion and matches state-of-art accuracy with a substantially smaller architecture, which gives unprecedented real-time performances during experimental tests, and makes the application reliable for working on real robots. The system has been implemented using the ROS framework and tested on a Baxter collaborative robot.",industry
10.1016/j.autcon.2019.103062,Journal,Automation in Construction,scopus,2020-04-01,sciencedirect,Towards automated clash resolution of reinforcing steel design in reinforced concrete frames via Q-learning and building information modeling,https://api.elsevier.com/content/abstract/scopus_id/85078657649,"The design of reinforcing steel bars (rebars) is critical to reinforced concrete (RC) structures. Generally, a good number of rebars are required by a design code, particularly at member connections. As such, rebar clashes (i.e., collisions and congestions) would be inevitable. It would be impractical, labor-intensive, and error-prone to avoid all possible clashes manually or even using standard design software. The building information modeling (BIM) technology has been utilized by the present architecture, engineering, and construction (ACE) industry for clash-free rebar designs. However, most existing BIM-based approaches offer the clash resolution strategy for moving components with an optimization algorithm, and are only applicable to the RC structures with regular shapes. In particular, the optimized path of rebars cannot be adjusted to avoid the obstacles, thus limiting the practical applications. Furthermore, most existing studies lack the learning from design code and constructibility constraints to realize automatic and intelligent arrangement and adjustment of rebars for avoiding the obstacles encountered in complex RC joints and frame structures. Considering these shortcomings, the authors have recently proposed an immediate reward-based multi-agent reinforcement learning (MARL) system with BIM, towards automatic clash-free rebar designs of RC joints without clashes. However, as the immediate reward is required in the MARL system for guiding the learning of a rebar design, it will not succeed in clash-free rebar designs of complex RC structures where immediate reward is often unavailable. In this study, this study further extends the previous work with Q-learning (a model-free reinforcement learning algorithm) for more realistic path planning considering both immediate and delayed rewards in clash-free rebar designs for real-world RC structures. In particular, the rebar design problem is treated as a path-planning problem of multi-agent system, where each rebar is deemed as an intelligence reinforcement learning agent. Next, by employing the Q-learning as the reinforcement learning engine, the particular form of state, action, and immediate and delayed rewards for the reinforcement MARL for automatic rebar designs considering more actual constructible constraints and design codes can be developed. Comprehensive experiments on three typical beam-column joints and a two-story RC building frame were conducted to evaluate the efficiency of the proposed method. The study results of paths of rebar designs, success rates, and average time confirm that the proposed framework with MARL and BIM is effective and efficient.",industry
10.1016/j.talanta.2019.120664,Journal,Talanta,scopus,2020-04-01,sciencedirect,Modelling of bioprocess non-linear fluorescence data for at-line prediction of etanercept based on artificial neural networks optimized by response surface methodology,https://api.elsevier.com/content/abstract/scopus_id/85076829838,"In the last years, regulatory agencies in biopharmaceutical industry have promoted the design and implementation of Process Analytical Technology (PAT), which aims to develop rapid and high-throughput strategies for real-time monitoring of bioprocesses key variables, in order to improve their quality control lines. In this context, spectroscopic techniques for data generation in combination with chemometrics represent alternative analytical methods for on-line critical process variables prediction. In this work, a novel multivariate calibration strategy for the at-line prediction of etanercept, a recombinant protein produced in a mammalian cells-based perfusion process, is presented. For data generation, samples from etanercept processes were daily obtained, from which fluorescence excitation-emission matrices were generated in the spectral ranges of 225.0 and 495.0 nm and 250.0 and 599.5 nm for excitation and emission modes, respectively. These data were correlated with etanercept concentration in supernatant (measured by an off-line HPLC-based reference univariate technique) by implementing different chemometric strategies, in order to build predictive models. Partial least squares (PLS) regression evidenced a non-linear relation between signal and concentration when observing actual vs. predicted concentrations. Hence, a non-parametric approach was implemented, based on a multilayer perceptron artificial neural network (MLP). The MLP topology was optimized by means of the response surface methodology. The prediction performance of MLP model was superior to PLS, since the first is able to cope with non-linearity in calibration models, reaching percentage mean relative error in predictions of about 7.0% (against 12.6% for PLS). This strategy represents a fast and inexpensive approach for etanercept monitoring, which conforms the principles of PAT.",industry
10.1016/j.compag.2020.105284,Journal,Computers and Electronics in Agriculture,scopus,2020-03-01,sciencedirect,An experimental study of stunned state detection for broiler chickens using an improved convolution neural network algorithm,https://api.elsevier.com/content/abstract/scopus_id/85079902403,"Effective recognition method of broiler stunned state has always been an important issue in real industries. In recent years, recognition methods such as neural networks have been receiving increasing attention due to their great merits of high diagnostic accuracy and easy implementation. To improve the accuracy and efficiency of broiler stunned state recognition, an improved fast region-based convolutional neural network (You Only Look Once + Multilayer Residual Module (YOLO + MRM)) algorithm was proposed and applied to the recognition of three broiler stunned states: insufficient, appropriate and excessive stuns. The images were collected from a broiler-slaughtering line using a complementary metal-oxide semiconductor (CMOS) camera. The area of the head and wings of a broiler in the original image was marked according to the PASCAL VOC data format and the dataset of each broiler stunned state was obtained. The results showed that the YOLO + MRM algorithm achieved good performance with an accuracy of 96.77%. To compare YOLO + MRM with other models, similar experiments were conducted using a conventional back propagation neural network (BP-NN) classifier, as well as YOLO, and the recognition accuracies were 90.11% and 94.74%, respectively. YOLO + MRM can complete the detection task of more than 180,000 broilers per hour. Compared with the traditional method, little prior expertise on image recognition is required, the recognition accuracy and speed are improved obviously. This study has provided a foundation and highlighted the potential for automatically detecting the stunned state of broiler chickens, which is crucial for the success of an automatic electric stunning process in the poultry industry.",industry
10.1016/j.adhoc.2019.102047,Journal,Ad Hoc Networks,scopus,2020-03-01,sciencedirect,An intelligent Edge-IoT platform for monitoring livestock and crops in a dairy farming scenario,https://api.elsevier.com/content/abstract/scopus_id/85076174369,"Today’s globalized and highly competitive world market has broadened the spectrum of requirements in all the sectors of the agri-food industry. This paper focuses on the dairy industry, on its need to adapt to the current market by becoming more resource efficient, environment-friendly, transparent and secure. The Internet of Things (IoT), Edge Computing (EC) and Distributed Ledger Technologies (DLT) are all crucial to the achievement of those improvements because they allow to digitize all parts of the value chain, providing detailed information to the consumer on the final product and ensuring its safety and quality. In Smart Farming environments, IoT and DLT enable resource monitoring and traceability in the value chain, allowing producers to optimize processes, provide the origin of the produce and guarantee its quality to consumers. In comparison to a centralized cloud, EC manages the Big Data generated by IoT devices by processing them at the network edge, allowing for the implementation of services with shorter response times, and a higher Quality of Service (QoS) and security. This work presents a platform oriented to the application of IoT, Edge Computing, Artificial Intelligence and Blockchain techniques in Smart Farming environments, by means of the novel Global Edge Computing Architecture, and designed to monitor the state of dairy cattle and feed grain in real time, as well as ensure the traceability and sustainability of the different processes involved in production. The platform is deployed and tested in a real scenario on a dairy farm, demonstrating that the implementation of EC contributes to a reduction in data traffic and an improvement in the reliability in communications between the IoT-Edge layers and the Cloud.",industry
10.1016/j.future.2019.10.043,Journal,Future Generation Computer Systems,scopus,2020-03-01,sciencedirect,HealthFog: An ensemble deep learning based Smart Healthcare System for Automatic Diagnosis of Heart Diseases in integrated IoT and fog computing environments,https://api.elsevier.com/content/abstract/scopus_id/85074613864,"Cloud computing provides resources over the Internet and allows a plethora of applications to be deployed to provide services for different industries. The major bottleneck being faced currently in these cloud frameworks is their limited scalability and hence inability to cater to the requirements of centralized Internet of Things (IoT) based compute environments. The main reason for this is that latency-sensitive applications like health monitoring and surveillance systems now require computation over large amounts of data (Big Data) transferred to centralized database and from database to cloud data centers which leads to drop in performance of such systems. The new paradigms of fog and edge computing provide innovative solutions by bringing resources closer to the user and provide low latency and energy efficient solutions for data processing compared to cloud domains. Still, the current fog models have many limitations and focus from a limited perspective on either accuracy of results or reduced response time but not both. We proposed a novel framework called HealthFog for integrating ensemble deep learning in Edge computing devices and deployed it for a real-life application of automatic Heart Disease analysis. HealthFog delivers healthcare as a fog service using IoT devices and efficiently manages the data of heart patients, which comes as user requests. Fog-enabled cloud framework, FogBus is used to deploy and test the performance of the proposed model in terms of power consumption, network bandwidth, latency, jitter, accuracy and execution time. HealthFog is configurable to various operation modes which provide the best Quality of Service or prediction accuracy, as required, in diverse fog computation scenarios and for different user requirements.",industry
10.1016/j.compind.2019.103164,Journal,Computers in Industry,scopus,2020-02-01,sciencedirect,Integrating artificial intelligent techniques and continuous time simulation modelling. Practical predictive analytics for energy efficiency and failure detection,https://api.elsevier.com/content/abstract/scopus_id/85099790267,"Energy efficiency and reliability needs are growing in many economic sectors, where predictive analytics are becoming essential tools for these key variables forecasting.
                  When predicting these variables, in many occasions, the problem to simplify the prediction model format when dealing with similar systems, which are placed in different functional locations, is a very complex problem due to model unavoidable dependency on changing operating conditions (per time and location). So effort is placed in this paper to develop tools that can easily adapt prediction models’ structure to existing operating conditions, for a given time period and place where the asset is located. Furthermore, these tools may allow the model to be easily trained and tested for automated implementation within the plant’s remote surveillance system.
                  To this end, Artificial Intelligence (AI) techniques, and in particular artificial neural network (ANN) models, have been selected in this paper as prediction models, since their structure can be adapted to improve predictions accuracy and they can also learn from dynamic changes in environmental conditions.
                  To demonstrate the adaptability for prediction accuracy and self-learning capabilities of the model, we have implemented an ANN with a backpropagation algorithm as a continuous time simulation model, which is then implemented using Vensim simulation environment, to benefit of the outstanding software optimization features for fast training.
                  Using this model we provide predictions of asset degradation and operational risk under existing real time internal and locational variables. We can also dynamically release preventive maintenance activities. This prediction model is exemplified in an industrial case for failures in cryogenic pumps of LNG tanks.",industry
10.1016/j.comcom.2020.01.018,Journal,Computer Communications,scopus,2020-02-01,sciencedirect,Enhanced resource allocation in mobile edge computing using reinforcement learning based MOACO algorithm for IIOT,https://api.elsevier.com/content/abstract/scopus_id/85077781443,"The Mobile networks deploy and offers a multiaspective approach for various resource allocation paradigms and the service based options in the computing segments with its implication in the Industrial Internet of Things (IIOT) and the virtual reality. The Mobile edge computing (MEC) paradigm runs the virtual source with the edge communication between data terminals and the execution in the core network with a high pressure load. The demand to meet all the customer requirements is a better way for planning the execution with the support of cognitive agent. The user data with its behavioral approach is clubbed together to fulfill the service type for IIOT. The swarm intelligence based and reinforcement learning techniques provide a neural caching for the memory within the task execution, the prediction provides the caching strategy and cache business that delay the execution. The factors affecting this delay are predicted with mobile edge computing resources and to assess the performance in the neighboring user equipment. The effectiveness builds a cognitive agent model to assess the resource allocation and the communication network is established to enhance the quality of service. The Reinforcement Learning techniques Multi Objective Ant Colony Optimization (MOACO) algorithms has been applied to deal with the accurate resource allocation between the end users in the way of creating the cost mapping tables creations and optimal allocation in MEC.",industry
10.1016/j.cie.2019.106225,Journal,Computers and Industrial Engineering,scopus,2020-02-01,sciencedirect,Fuzzy possibility regression integrated with fuzzy adaptive neural network for predicting and optimizing electrical discharge machining parameters,https://api.elsevier.com/content/abstract/scopus_id/85076689961,"An electrical discharge machining (EDM) is one of the special production methods that are widely used in moldings, repairs and production of specific industrial components. Due to extensive production costs, optimal machining specifications are significant. Machining specifications are effective on output quality and thus attract more customers leading to higher profits. In this study, the impact of EDM parameters on surface roughness, material removal rate and electrode corrosion percentage have been investigated. In order to consider uncertainty of real production environments, the fuzzy theory is employed. Also, using the design of experiment (DOE) parameters calibration is performed and mathematical programming approach is applied for optimization purpose. The relationship between the machining parameters and the output process specification is examined by a fuzzy possibility regression model. Then, the mathematical relation of exact inputs and fuzzy outputs of the EDM process are extracted. The effectiveness of the three outputs is evaluated by interfacing models and fuzzy hypothesis testing. To determine the optimal levels of each output, a fuzzy adaptive neural network is used and appropriate models are prepared to be adapted with a fitted model of fuzzy possibility regression for comparison purposes. Validation tests imply the effectiveness of the proposed method. The integrated model is implemented in real case study. The results show that, fitted models can predict the material removal rate, surface fineness, and corrosion percentage of the electrode. The prediction accuracy of the proposed method is shown in comparison with the optimal fuzzy adaptive neural network outputs considering error value. Also, the proposed method is successful in identifying the optimal process parameters for EDM with reliable accuracy. The proposed integrated prediction and optimization model can be used as a calibration decision support in production systems to handle dynamic data structures and provide real time machining specifications to increase the output quality.",industry
10.1016/j.micpro.2019.102906,Journal,Microprocessors and Microsystems,scopus,2020-02-01,sciencedirect,Area and power efficient pipelined hybrid merged adders for customized deep learning framework for FPGA implementation,https://api.elsevier.com/content/abstract/scopus_id/85073599282,"With the rapid growth of deep learning and neural network algorithms, various fields such as communication, Industrial automation, computer vision system and medical applications have seen the drastic improvements in recent years. However, deep learning and neural network models are increasing day by day, while model parameters are used for representing the models. Although the existing models use efficient GPU for accommodating these models, their implementation in the dedicated embedded devices needs more optimization which remains a real challenge for researchers. Thus paper, carries an investigation of deep learning frameworks, more particularly as review of adders implemented in the deep learning framework. A new pipelined hybrid merged adders (PHMAC) optimized for FPGA architecture which has more efficient in terms of area and power is presented. The proposed adders represent the integration of the principle of carry select and carry look ahead principle of adders in which LUT is re-used for the different inputs which consume less power and provide effective area utilization. The proposed adders were investigated on different FPGA architectures in which the power and area were analyzed. Comparison of the proposed adders with the other adders such as carry select adders (CSA), carry look ahead adder (CLA), Carry skip adders and Koggle Stone adders has been made and results have proved to be highly vital into a 50% reduction in the area, power and 45% when compared with above mentioned traditional adders.",industry
10.1016/j.rcim.2019.101847,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2020-02-01,sciencedirect,Trajectory smoothing method using reinforcement learning for computer numerical control machine tools,https://api.elsevier.com/content/abstract/scopus_id/85070511935,"Tool-path codes output by computer-aided manufacturing software for high-speed machining are composed of discontinuous G01 line segments. The discontinuity of these tool movements causes computer numerical control (CNC) inefficiency. To achieve high-speed continuous motion, corner smoothing algorithms based on pre-planning methods are widely used. However, it is difficult to optimize smoothing trajectories in real-time systems. To obtain smooth trajectories efficiently, this paper proposes a neural network-based direct trajectory smoothing method. An intelligent neural network agent outputs servo commands directly based on the current tool path and running state in every cycle. To achieve direct control, motion feature and reward models were built, and reinforcement learning was used to train the neural network parameters without additional experimental data. The proposed method provides higher cutting efficiency than the local and global smoothing algorithms. Given its simple structure and low computational demands, it can easily be applied to real-time CNC systems.",industry
10.1016/j.ejor.2019.07.057,Journal,European Journal of Operational Research,scopus,2020-02-01,sciencedirect,Automating the planning of container loading for Atlas Copco: Coping with real-life stacking and stability constraints,https://api.elsevier.com/content/abstract/scopus_id/85070381903,"The Atlas Copco☆ distribution center in Allen, TX, supplies spare parts and consumables to mining and construction companies across the world. For some customers, packages are shipped in sea containers. Planning how to load the containers is difficult due to several factors: heterogeneity of the packages with respect to size, weight, stackability, positioning and orientation; the set of packages differs vastly between shipments; it is crucial to avoid cargo damage. Load plan quality is ultimately judged by shipping operators.
                  This container loading problem is thus rich with respect to practical considerations. These are posed by the operators and include cargo and container stability as well as stacking and positioning constraints. To avoid cargo damage, the stacking restrictions are modeled in detail. For solving the problem, we developed a two-level metaheuristic approach and implemented it in a decision support system. The upper level is a genetic algorithm which tunes the objective function for a lower level greedy-type constructive placement heuristic, to optimize the quality of the load plan obtained.
                  The decision support system shows load plans on the forklift laptops and has been used for over two years. Management has recognized benefits including reduction of labour usage, lead time, and cargo damage risk.",industry
10.1016/j.neucom.2019.09.082,Journal,Neurocomputing,scopus,2020-01-29,sciencedirect,A scalable and reconfigurable in-memory architecture for ternary deep spiking neural network with ReRAM based neurons,https://api.elsevier.com/content/abstract/scopus_id/85073152550,"Neuromorphic computing using post-CMOS technologies is gaining increasing popularity due to its promising potential to resolve the power constraints in Von-Neumann machine and its similarity to the operation of the real human brain. In this work, we propose a scalable and reconfigurable architecture that exploits the ReRAM-based neurons for deep Spiking Neural Networks (SNNs). In prior publications, neurons were implemented using dedicated analog or digital circuits that are not area and energy efficient. In our work, for the first time, we address the scaling and power bottlenecks of neuromorphic architecture by utilizing a single one-transistor-one-ReRAM (1T1R) cell to emulate the neuron. We show that the ReRAM-based neurons can be integrated within the synaptic crossbar to build extremely dense Process Element (PE)–spiking neural network in memory array–with high throughput. We provide microarchitecture and circuit designs to enable the deep spiking neural network computing in memory with an insignificant area overhead. Simulation results on MNIST and CIFAR-10 datasets with spiking Resnet (SResnet) and spiking Squeezenet (SSqueez) show that compared to the baseline CPU only solution, our proposed architecture achieves energy saving between 1222 ×  and 1853 ×  and speed improvement between 791 ×  to 1120 ×.",industry
10.1016/j.jclepro.2019.118788,Journal,Journal of Cleaner Production,scopus,2020-01-20,sciencedirect,Rapid evaluation of micro-scale photovoltaic solar energy systems using empirical methods combined with deep learning neural networks to support systems’ manufacturers,https://api.elsevier.com/content/abstract/scopus_id/85073926377,"Solar energy is becoming one of the most attractive renewable sources. In many cases, due to a wide range of financial or installation limitations, off-grid small scale micro power panels are favoured as modular systems to power lighting in gardens or to be integrated together to power small devices such as mobile phone chargers and distributed smart city facilities and services. Manufacturers and systems’ integrators have a wide range of options of micro-scale photo voltaic panels to choose from. This makes the selection of the right panel a challenging task and risky investment. To address this and to help manufacturers, this paper suggests and evaluates a novel approach based on integrating empirical lab-testing with short-term real data and neural networks to assess the performance of micro-scale photovoltaic panels and their suitability for a specific application in specific environment. The paper outlines the combination of lab testing power output under seasonal and hourly conditions during the year combined with environmental and operating conditions such as temperature, dust accumulation and tilt angle performance. Based on the lab results, a short in-situ experimental work is implemented and the performance over the year in the selected location in Kuwait is evaluated using deep learning neural networks. The findings of this approach are compared with simulation and long-term real data. The results show a maximum error of 23% of the neural network output when compared with the actual data, and a correlation values with previous work within 87.3% and 91.9% which indicate that the proposed approach could provide an experimental rapid and accurate assessment of the expected power output. Hence, supporting the rapid decision-making process for manufacturers and reducing investment risks.",industry
10.1016/j.fuel.2019.116250,Journal,Fuel,scopus,2020-01-15,sciencedirect,"On the development of experimental methods to determine the rates of asphaltene precipitation, aggregation, and deposition",https://api.elsevier.com/content/abstract/scopus_id/85072862208,"Despite the efforts throughout the last few decades, asphaltene deposition remains as one of the greatest challenges in the petroleum industry. In this work, we present a comprehensive series of experimental studies to better understand the asphaltene precipitation, aggregation, and deposition mechanisms. Here, we introduce a simple method to determine the amount of precipitated asphaltene using NIR spectroscopy measurements without the implementation of calibration curves. Moreover, the kinetics of asphaltene precipitation and aggregation is simultaneously investigated by a newly developed, fast, and reliable NIR spectroscopy technique. In the new method, only less than 2 ml of sample is required for each experiment. In addition, unlike gravimetric techniques, less time consuming and labor-intensive measurements can be performed. In addition, the temperature can be controlled; hence, experiments can be conducted to evaluate the effects of temperature and the driving force on the kinetics of asphaltene precipitation and aggregation. Subsequently, the quantified precipitated asphaltene amount can be used to calibrate the precipitation and aggregation kinetic parameters of the asphaltene deposition model. The results obtained from the kinetics experiments facilitate in establishing a function to scale the precipitation kinetic parameter from laboratory-scale experiments to real field high-pressure high-temperature conditions. Additionally, a multi-section stainless steel packed bed column is proposed to study asphaltene deposition at high temperature and under dynamic conditions. In these experiments, the amount of deposited asphaltene is directly quantified. The results from the packed bed column deposition tests can be used to calibrate the deposition kinetic parameter of the asphaltene deposition model.",industry
10.1016/j.neucom.2019.09.004,Journal,Neurocomputing,scopus,2020-01-02,sciencedirect,Digital neuromorphic real-time platform,https://api.elsevier.com/content/abstract/scopus_id/85072526243,"Hardware implementations of spiking neural networks in portable devices can improve many applications of robotics, neurorobotics or prosthetic fields in terms of power consumption, high-speed processing and learning mechanisms. Analog and digital platforms have been previously proposed to run these networks. Analog designs are closer to biology since they implement the original mathematical model. However, digital platforms are, to some extent, abstractions of this model so far. In this paper, a full digital platform to design, implement and run real-time analog-like spiking neural networks is presented. Specifically, we present the design and implementation of digital circuits to run real-time biologically plausible spiking neural networks on a Field Programmable Gate Array (FPGA). The circuit designed for the neuron implements the Leaky Integrate and Fire (LIF) model. The synapsis implemented is a bi-exponential current-based one. The synaptic circuit design consists of one static memory with the baseline current and a dynamic memory which stores the updated contribution over time of each pre-synaptic connection. All the parameters of both the neuron and the synapse are configurable. The results of the circuits are validated by running the same experiments on the Brian simulator. The circuits, which are totally original and independent of the technology, use only 136 slice registers of hardware resources. Thus, these designs allow the scale of the network. These circuits aim to be the basis of the spiking neural networks on digital devices. This platform allows the user to first simulate their network within the Brian simulator and then, confidently, move to the hardware platform replicating the same performance or even replace their analog platform with the digital one.",industry
10.1016/j.ifacol.2020.12.2866,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Reinforcement learning for dual-resource constrained scheduling,https://api.elsevier.com/content/abstract/scopus_id/85107805245,"This paper proposes using reinforcement learning to solve scheduling problems where two types of resources of limited availability must be allocated. The goal is to minimize the makespan of a dual-resource constrained flexible job shop scheduling problem. Efficient practical implementation is very valuable to industry, yet it is often only solved combining heuristics and expert knowledge. A framework for training a reinforcement learning agent to schedule diverse dual-resource constrained job shops is presented. Comparison with other state-of-the-art approaches is done on both simpler and more complex instances that the ones used for training. Results show the agent produces competitive solutions for small instances that can outperform the implemented heuristic if given enough time. Other extensions are needed before real-world deployment, such as deadlines and constraining resources to work shifts.",industry
10.1016/j.ifacol.2020.12.2856,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,A deep learning unsupervised approach for fault diagnosis of household appliances,https://api.elsevier.com/content/abstract/scopus_id/85107800132,Fault detection and fault diagnosis are crucial subsystems to be integrated within the control architecture of modern industrial processes to ensure high quality standards. In this paper we present a two-stage unsupervised approach for fault detection and diagnosis in household appliances. In particular a suitable testing procedure has been implemented on a real industrial production line in order to extract the most meaningful features that allow to efficiently classify different types of fault by consecutively exploiting deep autoencoder neural network and k-means or hierarchical clustering techniques.,industry
10.1016/j.ifacol.2020.12.2855,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Fault prediction as a service in the smart factory: Addressing common challenges for an effective implementation,https://api.elsevier.com/content/abstract/scopus_id/85107753365,"Fault prediction in manufacturing systems has consistently been an important theme in engineering research. Data-driven methods to deliver this service are gaining momentum due to developments regarding information and communication technologies. Particularly, fault prediction may be interpreted as a supervised learning classification problem, in which algorithms trained by operational data gathered from the shop-floor are capable of informing managers whether a machine might enter in a failure state or not. Despite the relevance of this approach, implementations are hindered by several challenges. In this work, we review approaches aimed to deal with four of these challenges, namely: limited amount of training data, unbalanced training data sets, uncertainty regarding which variables should be monitored, and uncertainty regarding how exactly historical data should be employed in the algorithm’s training. To deal with training sets with limited size, learning procedures observed to perform well with a lower volume of training data can be used, such as the Random Forests technique. Alternatively, transfer learning techniques can be utilized to adapt models trained in a virtual domain with abundant synthetic data to the real manufacturing system domain. To deal with unbalance among classification classes, cost-sensitive learning methods can be employed to alter the penalties incurred when misclassifications occurs in the minority class. Alternatively, resampling methods can be applied before learning occurs. Lastly, both the decisions regarding which variables to track, and to what extent historical data should be included in the training process, can be addressed through the use of specific feature selection methods.",industry
10.1016/j.ifacol.2020.12.299,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Artificial intelligence platform proposal for paint structure quality prediction within the industry 4.0 concept,https://api.elsevier.com/content/abstract/scopus_id/85103128034,"This article provides an artificial intelligence platform proposal for paint structure quality prediction using Big Data analytics methodologies. The whole proposal fits into the current trends that are outlined in the Industry 4.0 concept. The painting process is very complex, producing huge volumes of data, but the main problem is that the data comes from different data sources, often heterogeneous, and it is necessary to propose a way to collect and integrate them into a common repository. The motivation for this work were the industry requirements to solve specific problems that cannot be solved by standard methods but require a sophisticated and holistic approach. It is the application of artificial intelligence that suggests a solution that is not otherwise visible, and the use of standard methods would not give any satisfactory results. The result is the design of an artificial intelligence platform that has been deployed in a real manufacturing process, and the initial results confirm the correctness and validity of this step. We also present a data collection and integration architecture, which is an integral part of every big data analytics solution, and a principal component analysis that was used to reduce the dimensionality of the large number of production process data.",industry
10.1016/j.matpr.2020.08.718,Conference Proceeding,Materials Today: Proceedings,scopus,2020-01-01,sciencedirect,Hybrid clustering algorithm for an efficient brain tumor segmentation,https://api.elsevier.com/content/abstract/scopus_id/85102451494,"This work describes the data mining methods, techniques and algorithms used for implementation. It is an emerging field of IT industry and research. There are many other fields such as Artificial Intelligence, Machine Learning, Deep Learning, Virtualization, Visualization, Parallel Computing and Image Processing. The human internal Brain can be seen or visualized by the Magnetic Resonance Imaging scan or Computerized Tomography scan. The MRI image is scanned and will be taken as input for processing. The MRI scan is more advantageous and more comfortable than CT scan for diagnosis. MRI scan provides detailed picture of organs. It does not affect the human health and body condition. It doesn't use any radiation. It is purely based on the magnetic field and radio waves. LIPC technique makes the training samples from the patients and arranges them into different group of classes used to construct different dictionaries. Image segmentation is a technique of dividing an image into different multiple portions, which is used to spot out objects and boundaries in images. There are many image segmentation techniques applicable for image processing. No acceptable method is available for solving all kinds of segmentation problem. Every method has merits and demerits. So, choosing good method is the challenging task. The hybrid clustering method is proposed in this work. The k-means algorithm and fuzzy c-means algorithm is proposed for brain tumor segmentation. The algorithm is implemented in synthetic and real time dataset. From the experimental results, this method provides better results in the form of accuracy.",industry
10.1016/j.procir.2020.03.134,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Reconstructing CNC platform for EDM machines towards smart manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85102047186,"CNC (computer numerical control) systems play an ultimately important role for controlling EDM (electrical discharge machining) machine tools and their machining processes. Till now, existing CNC systems do not offer sufficient openness that supports researchers and engineers to expend its capabilities and functionalities in response to the increasing demands of smart manufacturing; on the other hand, transforming an EDM machine made by small and medium-sized machine tool builders, into a smart manufacturing system has never been an easy job. To address the issues and overcome the difficulties which block the way towards smart manufacturing, this paper proposes an open architecture CNC platform for EDM machine tools. This platform utilizes the state-of-the-art technologies in implementation of the hardware and software without compromising with the constraints of obsolete techniques. For demonstrating the unique capabilities, the generalized unit arc length increment (GUALI) interpolation method and the Digitizer/Player system architecture are adopted. To exhibit the feasibilities of the newly developed platform, three kinds of EDM machine tools are applied associated with advanced functionalities such as machining process adaptive control, applications of machine learning, 6-axis EDM of shrouded turbine blisks etc. In addition, a small-scale smart manufacturing unit for drilling film cooling holes of turbine blades is built up into real production by applying the new CNC system and related software applications. From the practitioner’s viewpoint, openness and standardization are the keys that enable the people from academia and industry bringing in their domain knowledge to enrich the smart manufacturing ecosystem.",industry
10.1016/j.procir.2020.07.006,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Operator support in human-robot collaborative environments using AI enhanced wearable devices,https://api.elsevier.com/content/abstract/scopus_id/85100836551,"Nowadays, in order to cover the needs of market for product mass customization, industries have started to move to hybrid production cells, involving both robots and human operators. Research has been done during previous years to promote and improve the collaboration between humans and robots, trying to address topics such as safety, awareness and cognitive support in form of Augmented Reality based instructions. Results of previous research show bottlenecks related to the way of interaction of the operators with such supportive systems though. Direct interaction approach with the use of push buttons or indirect-gesture based interaction, which are most often adopted by the researchers, require operators to constantly occupy their hands performing the relevant button presses or gestures. Moreover, previous approaches are hardware dependent and need a lot of customization to work with different hardware. This work tries to address these bottlenecks proposing the usage of wearable devices enhanced with AI in order to support the interaction of human operators with robots in human-robot collaborative environments in a seamless and non-intrusive way, wrapped around a framework called “Operator Support Module” (OSM). Among others, OSM supports a variety of hardware to easily fit in various industrial scenarios. Two case studies will be presented to demonstrate the approach.",industry
10.1016/j.promfg.2020.11.012,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Application of machine learning and vision for real-time condition monitoring and acceleration of product development cycles,https://api.elsevier.com/content/abstract/scopus_id/85100766330,"Development work within an experimental environment, in which certain properties are investigated and optimized, requires many test runs and is therefore often associated with long execution times, costs and risks. This can affect product, material and technology development in industry and research. New digital driver technologies offer the possibility to automate complex manual work steps in a cost-effective way, to increase the relevance of the results and to accelerate the processes many times over. In this context, this article presents a low-cost, modular and open-source machine vision system for test execution and evaluates it on the basis of a real industrial application. For this purpose a methodology for the automated execution of the load intervals, the process documentation and for the evaluation of the generated data by means of machine learning to classify wear levels. The software and the mechanical structure are designed to be adaptable to different conditions, components and for a variety of tasks in industry and research. The mechanical structure is required for tracking the test object and represents a motion platform with independent positioning by machine vision operators or machine learning. An evaluation of the state of the test object is performed by the transfer learning after the initial documentation run. The manual procedure for classifying the visually recorded data on the state of the test object is described for the training material. This leads to an increased resource efficiency on the material as well as on the personnel side since on the one hand the significance of the tests performed is increased by the continuous documentation and on the other hand the responsible experts can be assigned time efficiently. The presence and know-how of the experts are therefore only required for defined and decisive events during the execution of the experiments. Furthermore, the generated data are suitable for later use as an additional source of data for predictive maintenance of the developed object.",industry
10.1016/j.procs.2020.10.091,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Towards smart manufucturing: Implementation and benefits,https://api.elsevier.com/content/abstract/scopus_id/85099879698,"Production activities are generating a large amount of data in different types (i.e., text, images), that is not well exploited. This data can be translated easily to knowledge that can help to predict all the risks that can impact the business, solve problems, promote efficiency of the manufacturing to the maximum, make the production more flexible and improving the quality of making smart decisions, however, implementing the Smart Manufacturing(SM) concept provides this opportunity supported by the new generation of the technologies. Internet Of Things (IoT) for more connectivity and getting data in real time, Big Data to store the huge volume of data and Deep Learning algorithms(DL) to learn from the historical and real time data to generate knowledge, that can be used, predict all the risks, problem solving, and better decision-making.
                  In this paper, we will introduce SM and the main technologies to success the implementation, the benefits, and the challenges.",industry
10.1016/j.promfg.2020.10.053,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Enabling real-time quality inspection in smart manufacturing through wearable smart devices and deep learning,https://api.elsevier.com/content/abstract/scopus_id/85099870958,"In this paper, we present a novel method for utilising wearable devices with Convolutional Neural Networks (CNN) trained on acoustic and accelerometer signals in smart manufacturing environments in order to provide real-time quality inspection during manual operations. We show through our framework how recorded or streamed sound and accelerometer data gathered from a wrist-attached device can classify certain user actions as successful or unsuccessful. The classification is designed with a Deep CNN model trained on Mel-frequency Cepstral Coefficients (MFCC) from the acoustic input signals. The wearable device provides feedback on three different modalities: audio, visual and haptic; thus ensuring the worker’s awareness at all time. We validate our findings through deployments of the complete AI-enabled device in production facilities of Mercedes-Benz AG. From the conducted experiments it is concluded that the use of acoustic and accelerometer data is valuable to train a classifier with the purpose of action examination during industrial assembly operations, and provides an intuitive interface for ensuring continued and improved quality inspection.",industry
10.1016/j.promfg.2020.10.126,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Simulation-as-a-service for reinforcement learning applications by example of heavy plate rolling processes,https://api.elsevier.com/content/abstract/scopus_id/85099821374,"In the production industry, the digital transformation enables a significant optimization potential. The concept of reinforcement learning offers a suitable approach to train agents on learning control strategies, further advancing automation. While applications training directly on real-world processes are rare due to economical and safety constraints, simulations offer a way to develop and evaluate agents prior to deployment. With the rise of service-based business models, the simulation owner and the machine learning expert are likely to be different stakeholders in a joint project. Due to different requirements for both simulations and reinforcement-learning agents, the stakeholders may be reluctant or unable to grant full access to the respective software. This poses a serious impediment to the potential of the digital transformation. In this paper, a distributed architecture is proposed, which allows the remote training of reinforcement learning agents on a simulation. It is shown that this architecture allows the cooperation between two stakeholders by exposing a suitable technical interface to the simulation. The proposed architecture is implemented for a simulation of the multi-step metal forming process of heavy plate rolling. Furthermore, the implemented architecture is used to successfully train a reinforcement-learning agent on the task of designing optimal parameter schedules.",industry
10.1016/j.promfg.2020.05.123,Conference Proceeding,,scopus,2020-01-01,sciencedirect,Integrated tool condition monitoring systems and their applications: A comprehensive review,https://api.elsevier.com/content/abstract/scopus_id/85095576577,"In conventional metal cutting, different tool wear modes, and their individual deterioration rates play vital roles in overall production performance. For a given tool (i.e., geometry or materials), many shop floors still follow a standard rule by pre-setting a tool life, which is conservative but not realistic. Premature failure of a tool can cause unexpected machine downtime and material losses, while another tool could serve beyond that pre-set life. As a result, optimized tool life and productivity cannot be achieved. Moreover, nowadays, there is an increased demand of process monitoring and optimization on the unmanned and the semi-automated shop floors.
                  Tool condition monitoring (TCM) systems for process improvement and optimization have been in research for several decades. Both offline and online TCM systems are invented and discussed. A wide range of original publications are reported focusing on different sub-topics, e.g., specific machining process-based TCM methods, measurement or signal acquisition methods, processing methods, and classifiers. With the recent evolution of smart sensors in the era of Industry 4.0, development of online TCM systems received much attention to the researchers. Accordingly, research on some sub-topics also gets motivated into different directions, such as, feasibility of power or current sensors, machine vision technique, and combination of multi-sensors. Thus, from the industrial viewpoint, the current state of implementation of the proposed TCM systems for (near) real-time process monitoring and control needs to be clear. This paper presents the state-of-the-art of the TCM systems covering three major machining operations, discusses their application feasibility in industry environments, and states some current TCMS implementations. Challenges being faced by the industry are concluded, along with direction and suggestions for future researches.",industry
10.1016/j.promfg.2020.05.140,Conference Proceeding,,scopus,2020-01-01,sciencedirect,Development of real-time diagnosis framework for angular misalignment of robot spot-welding system based on machine learning,https://api.elsevier.com/content/abstract/scopus_id/85095131276,"This paper focuses on the real-time online monitoring and diagnosis framework for the angular misalignment of the robot spot-welding system, which can result in significant quality degradation of a weld nugget such as porosity. The data-driven approach is applied by installing the voltage and current sensors, collecting the associated mass data and processing them under normal and abnormal (angular misalignment) conditions. Two categories of features are extracted from the dynamic resistance (DR) and the voltage and current ones that are decomposed by wavelet transform (WT). The DR features are extracted from the DR profile and some critical features are selected by a t-test methodology. In the case of the WT-based features, the critical ones are selected by a max-relevance and min-redundancy (mRMR) and a sequential backward selection (SBS) wrapper. Consequently, three types of critical feature sets, such as DR features, WT features, and hybrid features combining those, are prepared to train machine learning-based models. Support vector machine (SVM) and probabilistic neural network (PNN) are applied to establish the diagnosis models, and the diagnostic accuracy and robustness are evaluated. Finally, the software for the on-line monitoring and diagnosis for angular misalignment of robot spot-welding system is developed and demonstrates its real-time applicability in an industrial site.",industry
10.1016/j.promfg.2020.05.146,Conference Proceeding,,scopus,2020-01-01,sciencedirect,One-shot recognition of manufacturing defects in steel surfaces,https://api.elsevier.com/content/abstract/scopus_id/85095111982,"Quality control is an essential process in manufacturing to make the product defect-free as well as to meet customer needs. The automation of this process is important to maintain high quality along with the high manufacturing throughput. With recent developments in deep learning and computer vision technologies, it has become possible to detect various features from the images with near-human accuracy. However, many of these approaches are data intensive. Training and deployment of such a system on manufacturing floors may become expensive and time-consuming. The need for large amounts of training data is one of the limitations of the applicability of these approaches in real-world manufacturing systems. In this work, we propose the application of a Siamese convolutional neural network to do one-shot recognition for such a task. Our results demonstrate how one-shot learning can be used in quality control of steel by identification of defects on the steel surface. This method can significantly reduce the requirements of training data and can also be run in real-time.",industry
10.1016/j.procir.2020.04.158,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Image processing based on deep neural networks for detecting quality problems in paper bag production,https://api.elsevier.com/content/abstract/scopus_id/85092428222,"It is critical for manufacturers to identify quality issues in production and prevent defective products being delivered to customers. We investigate the use of deep neural networks to perform automatic quality inspections based on image processing to eliminate the current manual inspection. A deep neural network was implemented in a real-world industrial case study, and its ability to detect quality problems was evaluated and analyzed. The results show that the network has an accuracy of 94.5%, which is considered good in comparison to the 70–80% accuracy of a trained human inspector.",industry
10.1016/j.procir.2020.04.135,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Application of Artificial Intelligence to an Electrical Rewinding Factory Shop,https://api.elsevier.com/content/abstract/scopus_id/85091693237,"The evolution of artificial intelligence (AI) and big data resulted in the full potential realization of technologies through convergence. Tremendous acceptance, adoption and implementation of the United Nations Sustainable Development Goals (SDG) Agenda 2030, has resulted in original equipment manufacturers (OEM) developing various designs of rotary machines in a bid to improve energy efficiency, with more improvements expected in the coming decade. An effective technique to manage energy efficiency in the smart grid is through integration of demand side management, inclusive of optimization of rewinding of an electric motor in a machine shop. This paper aims to conceptualize application of AI and augmented reality (AR) towards process visibility of remanufacturing rotary machine stators by robotic vision. SLT is the triangulation methodology used in laser scanning for 3D modelling, and instantaneous condition assessment of the core. A pre-defined robotic path is used towards identification of features for range image acquisition. Therefore, the potential of industry 4.0 in resuscitation of end-of-life products through service remanufacturers by RE in a rewinding shop are presented.",industry
10.1016/j.procs.2020.05.068,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Adoption of the conceive-design-implement-operate approach to the third year project in a team-based design-build environment,https://api.elsevier.com/content/abstract/scopus_id/85089026312,"The high-quality engineering education is one of the challenges in the 21stcentury, where the teaching-learning process will be enhanced by integrating and involving the students in the teaching process and the course will be delivered in an interesting and engaging way. So in the process of reforming the engineering education, the Department of Mechanical Engineering of RIT, Rajaramnagar has initiated Conceive, Design, Implement and Operate (CDIO) approach to produce the 21st-century engineers. CDIO approach caters an engineering education that stresses fundamentals set in the context of real-world systems and products. It provides a universal structure for a strong engineering education integrating an entire set of graduate attributes. Developing undergraduate students into successful engineers requires integration of technical knowledge and soft skills. As a part of CDIO implementation, the mechanical program has been modified to provide integrated team-based project learning. The paper presents the implementation methodology of the concept with an emphasis on the Third year design projects. As per the CDIO standards and syllabus, the program has been modified and the required facilities were made available in the college campus such as Workspaces and laboratories. The structured research-driven approach is provided to monitor and review the implementation of the CDIO principles and standards. It is observed that CDIO provides integrated learning to develop deep learning of technical knowledge whilst simultaneously develops personal, interpersonal, process, product, and system developing skills. The primary outcome of the CDIO is the students are exposed to the work environment used in the industry for the product and process development and the secondary outcome is that it provides a useful tool for devolvement and assessment of the skill set of the students. Finally, the effect of CDIO initiative for project enrichment is discussed and future plan is presented.",industry
10.1016/j.procs.2020.04.199,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Perspective Vehicle License Plate Transformation using Deep Neural Network on Genesis of CPNet,https://api.elsevier.com/content/abstract/scopus_id/85086630682,"Recent development in vehicular industries and increased number of cars in modern society leads the people to pay more attention on Vehicle License Plate Recognition (V-LPR). V-LPR plays a major role in traffic related application such as road traffic monitoring, vehicle parking lots access control etc. Existing state of the art V-LPR systems in real world deployment works under restricted conditions, such as static illumination, fixed background etc. Most of them fails to work when any of the above given conditions are violated. Hence to address this issue, a novel V-LPR system is designed using modern deep learning framework called ""Capsule Network"". The proposed system is robust and works fine in any condition. Further, the proposed method aims to improve the processing time by integrating the segmentation process within the CN framework which involves the training and recognizing of entire license plate cropped region. Moreover, the feature extraction is performed by CN framework over a segmented alphanumeric character. Finally, Data augmentation technique is also used as a supplement to the CN framework to strengthen the process of training with various orientations like rotation, shift and flip for improving the recognition task.",industry
10.1016/j.procs.2020.03.027,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Strategic zoning approach for urban areas: Towards a shared transportation system,https://api.elsevier.com/content/abstract/scopus_id/85085571988,"Investigating downstream freight demand is a prerequisite to accomplishing the overall strategic implementation of transportation systems. Machine learning has recently become widely applied in order to support decision-making in several logistic operational levels: travel/arrival time prediction, occupancy forecasting of logistic spaces, route optimization and so on. Nevertheless, strategic decision-making often overlooks flow tendencies forecasting. Targeting this perspective, the present paper aims at proposing an urban zoning approach based on time series forecasting of supply chain demand through clustering customers. To conduct our approach, we have selected a set of machine learning algorithms that are believed to be robust according to the literature and the achieved accuracy benchmarks. Considering real-life data-based computational results, a number of analytical insights are illustrated.",industry
10.1016/j.procs.2020.03.044,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,An artificial intelligence based crowdsensing solution for on-demand accident scene monitoring,https://api.elsevier.com/content/abstract/scopus_id/85085566175,"Road traffic crashes have a devastating impact on societies by claiming more than 1.35 million lives each year and causing up to 50 million injuries. Improving the efficiency of emergency management systems constitutes a key measure to reduce road traffic deaths and injuries. In this work, we propose a comprehensive crowdsensing-based solution for the real-time collection and the analysis of accident scene intelligence as a means to improve the efficiency of the emergency response process and help reduce road fatalities. The solution leverages sensory, mobile, and web technologies for the real-time monitoring of accident scenes, and employs Artificial Intelligence for the automatic analysis of the accident scene data, to allow the automatic generation of accident intelligence reports. Police officers and rescue teams can use those reports for fast and accurate situational assessment and effective response to emergencies. The proposed system was fully implemented and its operation was successfully tested using a variety of scenarios. This work gives interesting insights into the possibility of leveraging crowdsensing and artificial intelligence for offering emergency situational awareness and improving the efficiency of emergency response operations.",industry
10.1016/j.procs.2020.03.004,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Activity Recognition in Smart Homes using UWB Radars,https://api.elsevier.com/content/abstract/scopus_id/85085563629,"In the last decade, smart homes have transitioned from a potential solution for aging-in-place to a real set of technologies being deployed in the real-world. This technological transfer has been mostly supported by simple, commercially available sensors such as passive infrared and electromagnetic contacts. On the other hand, many teams of research claim that the sensing capabilities are still too low to offer accurate, robust health-related monitoring and services. In this paper, we investigate the possibility of using Ultra-wideband (UWB) Doppler radars for the purpose of recognizing the ongoing ADLs in smart homes. Our team found out that with simple configuration and classical features engineering, a small set of UWB radars could reasonably be used to recognize ADLs in a realistic home environment. A dataset was built from 10 persons performing 15 different ADLs in a 40 square meters apartment with movement on the other side of the wall. Random Forest was able to attain 80% accuracy with an F1-Score of 79%, and a Kappa of 77%. Those results indicate the use of Doppler radars can be a good research avenue for smart homes.",industry
10.1016/j.procs.2020.03.036,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Air Quality Forecasting using LSTM RNN and Wireless Sensor Networks,https://api.elsevier.com/content/abstract/scopus_id/85085553433,"In the past few decades, many urban areas around the world have suffered from severe air pollution and the health hazards that come with it, making gathering real-time air quality and air quality forecasting very important to take preventive and corrective measures. This paper proposes a scalable architecture to monitor and gather real-time air pollutant concentration data from various places and to use this data to forecast future air pollutant concentrations. Two sources are used to collect air quality data. The first being a wireless sensor network that gathers and sends pollutant concentrations to a server, with its sensor nodes deployed in various locations in Bengaluru city in South India. The second source is the real-time air quality data gathered and made available by the Government of India as a part of its Open Data initiative. Both sources provide average concentrations of various air pollutants on an hourly basis. Due to its proven track record of success with time-series data, a Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) model was chosen to perform the task of air quality forecasting. This paper critically analyses the performance of the model in two regions that exhibit a significant difference in temporal variations in air quality. As these variations increase, the model suffers performance degradation necessitating adaptive modelling.",industry
10.1016/j.promfg.2020.04.017,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,PPE compliance detection using artificial intelligence in learning factories,https://api.elsevier.com/content/abstract/scopus_id/85085527469,"This project demonstrates the application of Artificial Intelligence (AI) and machine vision for the identification of Personal Protective Equipment (PPE), particularly safety glasses in zones of the Learning Factory, where safety risks exist. The objective is to design and implement an automated system for ensuring the safety of personnel when they are in the vicinity of machinery that presents potential risks to the eyes. Microsoft Azure Custom Vision AI and Intelligent AI Services, in conjunction with low-cost vision devices with lightweight onboard AI capability, provide a platform for a deep learning neural network model using publicly available images under the Creative Commons License. A combination of cloud-based and on-premises AI is used in this proof of concept system to provide a real-time vision-based safety system capable of detecting and recording potential safety breaches, promoting compliance, and ultimately preventing accidents before they happen. This system can be used to initiate different control actions in the event of safety violations and can detect multiple forms of protective wear. The flexibility of the system offers multiple benefits to learning factories and manufacturing organizations such as improved user safety, reduced insurance costs, and better detection and recording of safety violations. The hybrid AI architecture approach allows for flexibility in training and deployment based on the capability of local computing resources.",industry
10.1016/j.promfg.2020.04.082,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Integrating virtual and physical production processes in learning factories,https://api.elsevier.com/content/abstract/scopus_id/85085519100,"Scaled learning factories are industrial learning environments that provide production systems and processes for learners on a model scale rather than using actual productive machines. This approach has benefits as for instance lower invest, increased approachability and higher safety levels. At the same time, constraints for implementation of actual production processes and required abstraction levels from industrial processes are limitations. To bridge the gap between benefits and limitation we propose the integration of virtual production processes in a prevalent physical learning factories. Resulting mixed reality solutions bear the potential to combine real and virtual objects at the same time and thus extend the physical model environment with virtually represented processes. Based on an initial analysis we develop a concept using spatial augmented reality and a game engine based simulation to realize a virtual integrated production process. The theoretical concept as well as the technical implementation is described. A first evaluation indicates a high rate of acceptance by trainees and illustrates the benefits for learning performance.",industry
10.1016/j.promfg.2020.04.055,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,From digital shop floor to real-time reporting: An IIoT based educational use case,https://api.elsevier.com/content/abstract/scopus_id/85085498833,"The Smart Production Lab (Lab)at the FH JOANNEUM in Kapfenberg, Austria, is a digital learning and research factory with an interdisciplinary focus on vertical and horizontal IT-integration. It is aiming at a higher transparency and productivity by applying latest digital technologies. The key technology is the Industrial Internet of Things (IIoT). Therefore, research driven IoT use cases are further developed such as hybrid IoT-concepts and architectures involving edge and cloud computing. State-of-the-Art use cases apply of-the-shelf technologies for ready-to-use implementations and teaching purposes. This paper introduces a case-based teaching concept in the area of IIoT. It provides students with a hands-on experience as well as deep insights in what is meant by modelling and implementing an IoT data flow from the shop floor to real-time reporting. For this purpose, on the operational technology (OT) layer IoT nodes were attached to the machinery in the Lab gathering and providing data for the IoT middleware layer, based on Open Platform Communication Unified Architecture (OPC UA). This central middleware-layer is represented by the open source platform Node-RED. In the respective use case the data is transformed in order to be stored in a NoSQL database, from where it can be accessed for real-time reporting either by cloud or on premise applications. The interdisciplinary nature of these use case consists of integrating the different aspects of a digital production, involving disciplines such as automation, digital retrofitting, operational technology, and informational technology. Thus, it provides students with a comprehensive understanding of the benefits and limitations of IIoT.",industry
10.1016/j.promfg.2020.04.037,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Implementing AR/MR - Learning factories as protected learning space to rise the acceptance for mixed and augmented reality devices in production,https://api.elsevier.com/content/abstract/scopus_id/85085498037,"When talking about digitization, changes in the way of working are inevitable: The implementation of intelligent machines or dealing with real-time data lead to new tasks supported by new technology. Also digital technologies such as Augmented and Mixed Reality (AR/MR) are pushing the market and setting new standards in collaboration, prototyping or maintenance. The correct handling of AR/MR devices requires a change in the employees’ behavior; changing working routines are followed by a new skill set and a change in the culture. The acceptance of employees can therefore be regarded as a critical success factor for the implementation of such technologies. Thus, the present paper answers the research question ‘what factors influence the employee’s acceptance of AR and MR data glasses in industry’. On the basis of a comprehensive literature analysis, an implementation workshop was developed and validated in cooperation with an industrial partner. The results were transformed into a workshop within the learning and research factory ‘Smart Production Lab’ to give employees and students the opportunity to train the handling of data glasses in a protected learning space in order to increase the acceptance for the technology.",industry
10.1016/j.promfg.2020.04.066,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,5G and AI technology application in the AMTC learning factory,https://api.elsevier.com/content/abstract/scopus_id/85085489730,"5G and AI (Artificial Intelligence) are changing industrial production and offer great potential for manufacturing enterprises. One of the effects resulting from the increasing quantity of production data is the increasing demands of transmission of large amounts of data, fast transmission speed, and rapid data analysis. However, merely relying on traditional communication technology and manual data processing does not lead to high transmission performance and low analysis time. It is essential to integrate 5G and AI technology to flexibly transmit large amounts of data and real-time data. To demonstrate the feasibility and potential of these two technologies, a concept was developed at the Advanced Manufacturing Technology Center (AMTC) at the Tongji University (Shanghai, China) and further implemented in the AMTC learning factory in cooperation with wbk of Karlsruhe Institute of Technology (Karlsruhe, Germany) and Ruhr-University Bochum (Bochum, Germany). This paper presents the learning factory design in detail, describing the concept design, training environment and training phases in the AMTC learning factory. It is followed by a case study consisting of specific examples of 5G and AI, implemented in the AMTC learning factory. The importance of integrated 5G and AI applications is pointed out and discussed.",industry
10.1016/j.promfg.2020.04.038,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Seamless data integration in the CAM-NC process chain in a learning factory,https://api.elsevier.com/content/abstract/scopus_id/85085473050,"The seamless data integration of different components in the CAM-NC process chain (tool management software, tool dispensing system, presetting machine and machine tool) is essential for maximizing the efficiency and minimizing the total error rate. This is done by entering the data into the system of the network only once. Then this data is available for all participants in this network at any time. This paper describes the aforementioned integration by using the example of creating a digital tool, which is used in a CAM simulation afterwards. Then the real set-up and machining process is discussed. The process chain explained in this paper was implemented at the smartfactory@tugraz - the Learning Factory at Graz University of Technology.",industry
10.1016/j.softx.2020.100419,Journal,SoftwareX,scopus,2020-01-01,sciencedirect,TWINKLE: A digital-twin-building kernel for real-time computer-aided engineering,https://api.elsevier.com/content/abstract/scopus_id/85079158568,"TWINKLE is a library for building families of solvers to perform Canonical Polyadic Decomposition (CPD) of tensors. The common characteristic of these solvers is that the data structure supporting the tuneable solution strategy is based on a Galerkin projection of the phase space. This allows processing and recovering tensors described by highly sparse and unstructured data. For achieving high performance, TWINKLE is written in C++ and uses the Armadillo open source library for linear algebra and scientific computing, based on LAPACK (Linear Algebra PACKage) and BLAS (Basic Linear Algebra Subprograms) routines. The library has been implemented keeping in mind its future extensibility and adaptability to fulfil the different users’ needs in academia and industry regarding Reduced Order Modelling (ROM) and data analysis by means of tensor decomposition. It is especially focused on post-processing data from Computer-Aided-Engineering (CAE) simulation tools.",industry
10.1016/j.aei.2020.101044,Journal,Advanced Engineering Informatics,scopus,2020-01-01,sciencedirect,IoT edge computing-enabled collaborative tracking system for manufacturing resources in industrial park,https://api.elsevier.com/content/abstract/scopus_id/85078852726,"In manufacturing industry, the movement of manufacturing resources in production logistics often affects the overall efficiency. This research is motivated by a world-leading air-conditioner manufacturer. In order to provide the right manufacturing resources for subsequent production steps, excessive time and human effort has been consumed in locating the manufacturing resources in a huge industrial park. The development of Internet of Things (IoT) has made a profound impact on establish smart manufacturing workshop and tracking applications, however a growing trend of data quantity that generated from massive, heterogeneous and bottomed manufacturing resources objects pose challenge to centralized decision. In this study, the concept of edge-computing deeply integrated in collaborative tracking purpose in virtue of IoT technology. An IoT edge computing enabled collaborative tracking architecture is developed to offload the computation pressure and realize distributed decision making. A supervised learning of genetic tracking method is innovatively presented to ensure tracking accuracy and effectiveness. Finally, the research output is developed and implemented in a real-life industrial park for verification. The results show that the proposed tracking method not only performs constant improving accuracy up to 96.14% after learning compared to other tracking method, but also ensure quick responsiveness and scalability.",industry
10.1016/j.aei.2020.101037,Journal,Advanced Engineering Informatics,scopus,2020-01-01,sciencedirect,A smart surface inspection system using faster R-CNN in cloud-edge computing environment,https://api.elsevier.com/content/abstract/scopus_id/85078666892,"Automated surface inspection has become a hot topic with the rapid development of machine vision technologies. Traditional machine vision methods need experts to carefully craft image features for defect detection. This limits their applications to wider areas. The emerging convolutional neural networks (CNN) can automatically extract features and yield good results in many cases. However, the CNN-based image classification methods are more suitable for flat surface texture inspection. It is difficult to accurately locate small defects in geometrically complex products. Furthermore, the computational power required in CNN algorithms is usually high and it is not efficient to be implemented on embedded hardware. To solve these problems, a smart surface inspection system is proposed using faster R-CNN algorithm in the cloud-edge computing environment. The faster R-CNN as a CNN-based object detection method can efficiently identify defects in complex product images and the cloud-edge computing framework can provide fast computation speed and evolving algorithm models. A real industrial case study is presented to illustrate the effectiveness of the proposed method. The results show that the proposed method can provide high detection accuracy within a short time.",industry
10.1016/j.aei.2019.101013,Journal,Advanced Engineering Informatics,scopus,2020-01-01,sciencedirect,Guidelines for applied machine learning in construction industry—A case of profit margins estimation,https://api.elsevier.com/content/abstract/scopus_id/85075778987,"The progress in the field of Machine Learning (ML) has enabled the automation of tasks that were considered impossible to program until recently. These advancements today have incited firms to seek intelligent solutions as part of their enterprise software stack. Even governments across the globe are motivating firms through policies to tape into ML arena as it promises opportunities for growth, productivity and efficiency. In reflex, many firms embark on ML without knowing what it entails. The outcomes so far are not as expected because the ML, as hyped by tech firms, is not the silver bullet. However, whatever ML offers, firms urge to capitalise it for their competitive advantage. Applying ML to real-life construction industry problems goes beyond just prototyping predictive models. It entails intensive activities which, in addition to training robust ML models, provides a comprehensive framework for answering questions asked by construction folks when intelligent solutions are getting deployed at their premises to substitute or facilitate their decision-making tasks. Existing ML guidelines used in the IT industry are vastly restricted to training ML models. This paper presents guidelines for Applied Machine Learning (AML) in the construction industry from training to operationalising models, which are drawn from our experience of working with construction folks to deliver Construction Simulation Tool (CST). The unique aspect of these guidelines lies not only in providing a novel framework for training models but also answering critical questions related to model confidence, trust, interpretability, bias, feature importance and model extrapolation capabilities. Generally, ML models are presumed black boxes; hence argued that nobody knows what a model learns and how it generates predictions. Even very few ML folks barely know approaches to answer questions asked by the end users. Without explaining the competence of ML, the broader adoption of intelligent solutions in the construction industry cannot be attained. This paper proposed a detailed process for AML to develop intelligent solutions in the construction industry. Most discussions in the study are elaborated in the context of profit margin estimation for new projects.",industry
10.1016/bs.adcom.2019.09.005,Book Series,Advances in Computers,scopus,2020-01-01,sciencedirect,Impact of cloud security in digital twin,https://api.elsevier.com/content/abstract/scopus_id/85073737509,"Digital Twin is a way to virtually represent or model a physical object using the real time data. This innovation sets up a way to deal with industries and organizations to supervise their products, consequently bridging the gap between design and implementations. As the name suggests, “Digital Twin” infers that a reproduction of the product is made in order to have a nearby relationship with the live item. The procedure of computerized twin begins by gathering real time data, processed data, and operational data and performs distinctive investigation which helps in anticipating the future. This additionally enhances the customer experiences by giving a digital feel of their product. The objective behind all these is the job of gathering information and putting them in a place, i.e., the cloud which could store exorbitant data. The user experience gets enhanced by the intervention of digital twin technology which could help in the successful working of the products geographically distributed. The impact of Internet of Things and Cloud Computing lifts up the digital twin.
                  The information gathered from the sources can be arranged in terms of utilization and prospect to change on a timely basis. These data, as they are stored require proper coordination and a legitimate use.
                  Digital Twin innovation assumes incredible opportunities in the field of manufacturing, healthcare, smart cities, automobile and so on. The effect of having a digital twin for the product makes it simple for activities and recognize the blemishes, if any happened. This approach can help reduce the workload and furthermore can get trained on the virtual machine without the need of a specific training.
                  With the most prevailing technologies of today, like Artificial Intelligence, Machine Learning and Internet of Things more prominent approach to train and monitor products, taking care of its own execution, collaborating to different frameworks, performing self-repairs are made possible. Hence the future is getting unfolded with the emerging DIGITAL TWIN era. The massive data utilized in the field of digital twin is prone to severe security breaches. Thus digital twin technology should be handled with extreme care so as to protect the data. Hence, this chapter identifies the ways and means of collecting, organizing and storing the data in a secured cloud environment. The data is filtered according to the use and priority and pushed into the cloud. It is determined to implement an exclusive algorithm for a secured cloud which would greatly benefit the users and the providers to handle and process it effectively.",industry
10.1016/j.vehcom.2019.100198,Journal,Vehicular Communications,scopus,2020-01-01,sciencedirect,In-vehicle network intrusion detection using deep convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85073150001,"The implementation of electronics in modern vehicles has resulted in an increase in attacks targeting in-vehicle networks; thus, attack detection models have caught the attention of the automotive industry and its researchers. Vehicle network security is an urgent and significant problem because the malfunctioning of vehicles can directly affect human and road safety. The controller area network (CAN), which is used as a de facto standard for in-vehicle networks, does not have sufficient security features, such as message encryption and sender authentication, to protect the network from cyber-attacks. In this paper, we propose an intrusion detection system (IDS) based on a deep convolutional neural network (DCNN) to protect the CAN bus of the vehicle. The DCNN learns the network traffic patterns and detects malicious traffic without hand-designed features. We designed the DCNN model, which was optimized for the data traffic of the CAN bus, to achieve high detection performance while reducing the unnecessary complexity in the architecture of the Inception-ResNet model. We performed an experimental study using the datasets we built with a real vehicle to evaluate our detection system. The experimental results demonstrate that the proposed IDS has significantly low false negative rates and error rates when compared to the conventional machine-learning algorithms.",industry
10.1016/j.eng.2019.02.013,Journal,Engineering,scopus,2019-12-01,sciencedirect,Artificial Intelligence in Steam Cracking Modeling: A Deep Learning Algorithm for Detailed Effluent Prediction,https://api.elsevier.com/content/abstract/scopus_id/85074530083,"Chemical processes can benefit tremendously from fast and accurate effluent composition prediction for plant design, control, and optimization. The Industry 4.0 revolution claims that by introducing machine learning into these fields, substantial economic and environmental gains can be achieved. The bottleneck for high-frequency optimization and process control is often the time necessary to perform the required detailed analyses of, for example, feed and product. To resolve these issues, a framework of four deep learning artificial neural networks (DL ANNs) has been developed for the largest chemicals production process—steam cracking. The proposed methodology allows both a detailed characterization of a naphtha feedstock and a detailed composition of the steam cracker effluent to be determined, based on a limited number of commercial naphtha indices and rapidly accessible process characteristics. The detailed characterization of a naphtha is predicted from three points on the boiling curve and paraffins, iso-paraffins, olefins, naphthenes, and aronatics (PIONA) characterization. If unavailable, the boiling points are also estimated. Even with estimated boiling points, the developed DL ANN outperforms several established methods such as maximization of Shannon entropy and traditional ANNs. For feedstock reconstruction, a mean absolute error (MAE) of 0.3 wt% is achieved on the test set, while the MAE of the effluent prediction is 0.1 wt%. When combining all networks—using the output of the previous as input to the next—the effluent MAE increases to 0.19 wt%. In addition to the high accuracy of the networks, a major benefit is the negligible computational cost required to obtain the predictions. On a standard Intel i7 processor, predictions are made in the order of milliseconds. Commercial software such as COILSIM1D performs slightly better in terms of accuracy, but the required central processing unit time per reaction is in the order of seconds. This tremendous speed-up and minimal accuracy loss make the presented framework highly suitable for the continuous monitoring of difficult-to-access process parameters and for the envisioned, high-frequency real-time optimization (RTO) strategy or process control. Nevertheless, the lack of a fundamental basis implies that fundamental understanding is almost completely lost, which is not always well-accepted by the engineering community. In addition, the performance of the developed networks drops significantly for naphthas that are highly dissimilar to those in the training set.",industry
10.1016/j.jmapro.2019.10.020,Journal,Journal of Manufacturing Processes,scopus,2019-12-01,sciencedirect,Data-driven smart manufacturing: Tool wear monitoring with audio signals and machine learning,https://api.elsevier.com/content/abstract/scopus_id/85074281429,"Tool wear in machining could result in poor surface finish, excessive vibration and energy consumption. Monitoring tool wear in real-time is crucial to improve manufacturing productivity and quality. While numerous sensor-based tool wear monitoring techniques have been demonstrated in laboratory environments, few tool wear monitoring systems have been deployed in factories because it is not realistic to install some of the important sensors such as dynamometers on manufacturing machines. To address this issue, a novel audio signal processing approach is introduced. This technique does not require expensive sensors but audio sensors only. A blind source separation method is used to separate source signals from noise. An extended principal component analysis is used for dimensionality reduction. Real-time multi-channel audio signals are collected during a set of milling tests under varying cutting conditions. The experimental data are used to develop and validate a predictive model. Experimental results have shown that the predictive model is capable of classifying tool wear conditions with high accuracy.",industry
10.1016/j.petrol.2019.106332,Journal,Journal of Petroleum Science and Engineering,scopus,2019-12-01,sciencedirect,Machine learning methods applied to drilling rate of penetration prediction and optimization - A review,https://api.elsevier.com/content/abstract/scopus_id/85070879413,"Drilling wells in challenging oil/gas environments implies in large capital expenditure on wellbore's construction. In order to optimize the drilling related operation, real-time decisions making have been put in place, so that prediction of rate of penetration (ROP) with accuracy is essential. Despite many efforts (theoretical and experimental) throughout the years, modeling the ROP as a mathematical function of some key variables is not so trivial, due to the highly non-linearity behavior experienced. Therefore, several researches in the recent years have been proposing to use data-driven models from artificial intelligence field for ROP prediction and optimization.
                  This paper presents an extensive review of the literature on ROP prediction, especially, with machine learning techniques, as well as how these models can be used to optimize the drilling activities. The ROP models are classified as traditional models (based on physics-models), statistical models (e.g. multiple regression), or machine learning methods. This review enables to see that machine learning techniques can potentially outperform in terms of ROP-prediction accuracy on top of traditional or statistical models. Throughout this work, an extensive analysis of different ways of obtaining ROP models is carried out, concluding with different strategies adopted in literature to perform data-driven model optimization.
                  Despite the saving potential which can be achieved with real-time optimization based on data-driven ROP models, it is noticeable that there is a lack of implementation of those techniques in the industry, as per literature review. To take a step forward in real implementations, the petroleum industry must be aware that yet no rule of thumb already exists on this specific area, but still, good and very reasonable results can be achieved by following the best practices identified in this review. In addition, the modern practices of machine learning provide promising guidelines for implementing projects in oil and gas industry.",industry
10.1016/j.sigpro.2019.06.019,Journal,Signal Processing,scopus,2019-12-01,sciencedirect,A generic parallel computational framework of lifting wavelet transform for online engineering surface filtration,https://api.elsevier.com/content/abstract/scopus_id/85068175044,"Nowadays, complex geometrical surface texture measurement and evaluation require advanced filtration techniques. Discrete wavelet transform (DWT), especially the second-generation wavelet (Lifting Wavelet Transform – LWT), is the most adopted one due to its unified and abundant characteristics in measured data processing, geometrical feature extraction, manufacturing process planning, and production monitoring. However, when dealing with varied complex functional surfaces, the computational payload for performing DWT in real-time often becomes a core bottleneck in the context of massive measured data and limited computational capacities. It is a more prominent problem for the areal surface texture filtration by using 2D DWT. To address the issue, this paper presents a generic parallel computational framework for lifting wavelet transform (GPCF-LWT) based on Graphics Process Unit (GPU) clusters and the Compute Unified Device Architecture (CUDA). Due to its cost-effective hardware design and the powerful parallel computing capacity, the proposed framework can support online (or near real-time) engineering surface filtration for micro- and nano-scale surface metrology through exploring a novel parallel method named LBB model, the improved algorithms of lifting scheme and three implementation optimizations on the heterogeneous multi-GPU systems. The innovative approach enables optimizations on individual GPU node through an overarching framework that is capable of data-oriented dynamic load balancing (DLB) driven by a fuzzy neural network (FNN). The paper concludes with a case study on filtering and extracting manufactured surface topographical characteristics from real surfaces. The experimental results have demonstrated substantial improvements on the GPCF-LWT implementation in terms of computational efficiency, operational robustness, and task generalization.",industry
10.1016/j.rcim.2019.05.008,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2019-12-01,sciencedirect,A real-time human-robot interaction framework with robust background invariant hand gesture detection,https://api.elsevier.com/content/abstract/scopus_id/85066259834,"In the light of factories of the future, to ensure productive and safe interaction between robot and human coworkers, it is imperative that the robot extracts the essential information of the coworker. We address this by designing a reliable framework for real-time safe human-robot collaboration, using static hand gestures and 3D skeleton extraction. OpenPose library is integrated with Microsoft Kinect V2, to obtain a 3D estimation of the human skeleton. With the help of 10 volunteers, we recorded an image dataset of alpha-numeric static hand gestures, taken from the American Sign Language. We named our dataset OpenSign and released it to the community for benchmarking. Inception V3 convolutional neural network is adapted and trained to detect the hand gestures. To augment the data for training the hand gesture detector, we use OpenPose to localize the hands in the dataset images and segment the backgrounds of hand images, by exploiting the Kinect V2 depth map. Then, the backgrounds are substituted with random patterns and indoor architecture templates. Fine-tuning of Inception V3 is performed in three phases, to achieve validation accuracy of 99.1% and test accuracy of 98.9%. An asynchronous integration of image acquisition and hand gesture detection is performed to ensure real-time detection of hand gestures. Finally, the proposed framework is integrated in our physical human-robot interaction library OpenPHRI. This integration complements OpenPHRI by providing successful implementation of the ISO/TS 15066 safety standards for “safety rated monitored stop” and “speed and separation monitoring” collaborative modes. We validate the performance of the proposed framework through a complete teaching by demonstration experiment with a robotic manipulator.",industry
10.1016/j.isatra.2018.12.025,Journal,ISA Transactions,scopus,2019-12-01,sciencedirect,Deep residual learning-based fault diagnosis method for rotating machinery,https://api.elsevier.com/content/abstract/scopus_id/85059116434,"Effective fault diagnosis of rotating machinery has always been an important issue in real industries. In the recent years, data-driven fault diagnosis methods such as neural networks have been receiving increasing attention due to their great merits of high diagnosis accuracy and easy implementation. However, it is mostly difficult to fully train a deep neural network since gradients in optimization may vanish or explode during back-propagation, which results in deterioration and noticeable variance in model performance. In fault diagnosis researches, larger data sequence of machinery vibration signal containing sufficient information is usually preferred and consequently, deep models with large capacity are generally adopted. In order to improve network training, a residual learning algorithm is proposed in this paper. The proposed architecture significantly improves the information flow throughout the network, which is well suited for processing machinery vibration signal with variable sequential length. Little prior expertise on fault diagnosis and signal processing is required, that facilitates industrial applications of the proposed method. Experiments on a popular rolling bearing dataset are implemented to validate the proposed method. The results of this study suggest that the proposed intelligent fault diagnosis method for rotating machinery offers a new and promising approach.",industry
10.1016/j.inffus.2018.11.020,Journal,Information Fusion,scopus,2019-12-01,sciencedirect,Data fusion based coverage optimization in heterogeneous sensor networks: A survey,https://api.elsevier.com/content/abstract/scopus_id/85059069923,"Sensor networks, as a promising network paradigm, have been widely applied in a great deal of critical real-world applications. A key challenge in sensor networks is how to improve and optimize coverage quality which is a fundamental metric to characterize how well a point or a region or a barrier can be sensed by the geographically deployed heterogeneous sensors. Because of the resource-limited, battery-powered and type-diverse features of the sensors, maintaining and optimizing coverage quality includes a significant amount of challenges in heterogeneous sensor networks. Many researchers from both academic and industrial communities have performed numerous significant works on coverage optimization problem in the past decades. Some of them also have surveyed the current models, theories and solutions on the problem of coverage optimization. However, most of the existing surveys and analytical studies ignore how to exploit data fusion and cooperation of the deployed sensors to enhance coverage performance. In this paper, we provide an insightful and comprehensive summarization and classification on the data fusion based coverage optimization problem and techniques. Aiming at overcoming the shortcomings existed in current solutions, we also discuss the future issues and challenges in this area and sketch a general research framework in the context of reinforcement learning.",industry
10.1016/j.eswa.2019.05.052,Journal,Expert Systems with Applications,scopus,2019-11-30,sciencedirect,Unsupervised collective-based framework for dynamic retraining of supervised real-time spam tweets detection model,https://api.elsevier.com/content/abstract/scopus_id/85067174995,"Twitter is one of the most popular social platforms. It has changed the way of communication and information dissemination through its real-time messaging mechanism. Recently, it has been used by researchers and industries as a new source of data for various intelligent systems, such as tweet sentiment analysis and recommendation systems, which require high data quality. However, due to its flexibility and popularity, Twitter has become the main target for spamming activities such as phishing legitimate users or spreading malicious software, which introduces new security issues and waste resources. Therefore, researchers have developed various machine-learning algorithms to reveal Twitter spam. However, as spammers have become smarter and more crafty, the characteristics of the spam tweets are varying over time making these methods inefficient to detect new spammers tricks and strategies. In addition, some of the employed methods (e.g. blacklisting) or spammer features (e.g. graph-based features) are extremely time-consuming, which hinders the ability to detect spammer activities in real-time. In this paper, we introduce a framework to deal with the volatility of the spam contents and new spamming patterns, called the spam drift. The framework combines the strength of unsupervised machine learning approach, which learns from unlabeled tweets, to retrain a real-time supervised tweet-level spam detection model in a batch mode. A set of experiments on a large-scale data set show the effectiveness of the proposed online unsupervised method in adaptively discovers and learns the patterns of new spam activities and achieve stable recall values reaching more than 95%. Although the average spam precision of our method is around 60%, the high spam recall values show the ability of our proposed method in reducing spam drift problems compared to traditional machine learning algorithms.",industry
10.1016/j.jclepro.2019.117870,Journal,Journal of Cleaner Production,scopus,2019-11-20,sciencedirect,"Digestate evaporation treatment in biogas plants: A techno-economic assessment by Monte Carlo, neural networks and decision trees",https://api.elsevier.com/content/abstract/scopus_id/85070258305,"Biogas production is one of the most promising pathways toward fully utilizing green energy within a circular economy. The anaerobic digestion process is the industry standard technology for biogas production due to its lowered energy consumption and its reliance on microbiology. Even in such an environmental-friendly process, liquid digestate is still produced from the remains of digested bio-feedstock and will require treatment. With unsuitable treatment procedure for liquid digestate, the mass of bio-feedstock can potentially escape the circular supply chain within the economy. This paper recommends the implementation of evaporator systems to provide a sustainable liquid digestate treating mechanism within the economy. Studied evaporator systems are represented by vacuum evaporation in combination with ammonia scrubber, stripping and reverse osmosis. Nevertheless, complex multi-dimensional decisions should be made by stakeholders before implementing such systems. Our work utilizes a novel techno-economics model to study the techno-economics robustness in implementing recent state-of-art vacuum evaporation systems with exploitation of waste heat from combined heat and power (CHP) units in biogas plants (BGP). To take into the account the stochasticity of the real world and robustness of the analysis, we used the Monte-Carlo simulation technique to generate more than 20,000 of different possibilities for the implementation of the evaporation system. Favourable decision pathways are then selected using a novel methodology which utilizes the artificial neural network and a hyper-optimized decision tree classifier. Two pathways that give the highest probability of providing a fast payback period are identified. Descriptive statistics are also used to analyse the distributions of decision parameters that lead to success in implementing the evaporator system. The results highlighted that integration of evaporation system are favourable when transport costs and incentives for CHP units are large and while feed-in tariffs for electricity production and specific investment costs are low. The result of this work is expected to pave the way for BGP stakeholders and decision makers in implementing liquid digestate treating technologies within the currently existing infrastructure.",industry
10.1016/j.compchemeng.2019.05.037,Journal,Computers and Chemical Engineering,scopus,2019-11-02,sciencedirect,Modern day monitoring and control challenges outlined on an industrial-scale benchmark fermentation process,https://api.elsevier.com/content/abstract/scopus_id/85071606321,"This paper outlines real-world control challenges faced by modern-day biopharmaceutical facilities through the extension of a previously developed industrial-scale penicillin fermentation simulation (IndPenSim). The extensions include the addition of a simulated Raman spectroscopy device for the purpose of developing, evaluating and implementation of advanced and innovative control solutions applicable to biotechnology facilities. IndPenSim can be operated in fixed or operator controlled mode and generates all the available on-line, off-line and Raman spectra for each batch. The capabilities of IndPenSim were initially demonstrated through the implementation of a QbD methodology utilising the three stages of the PAT framework. Furthermore, IndPenSim evaluated a fault detection algorithm to detect process faults occurring on different batches recorded throughout a yearly campaign. The simulator and all data presented here are available to download at www.industrialpenicillinsimulation.com and acts as a benchmark for researchers to analyse, improve and optimise the current control strategy implemented on this facility. Additionally, a highly valuable data resource containing 100 batches with all available process and Raman spectroscopy measurements is freely available to download. This data is highly suitable for the development of big data analytics, machine learning (ML) or artificial intelligence (AI) algorithms applicable to the biopharmaceutical industry.",industry
10.1016/j.cie.2019.106031,Journal,Computers and Industrial Engineering,scopus,2019-11-01,sciencedirect,Machine learning based concept drift detection for predictive maintenance,https://api.elsevier.com/content/abstract/scopus_id/85071975175,"In this work we present a machine learning based approach for detecting drifting behavior – so-called concept drifts – in continuous data streams. The motivation for this contribution originates from the currently intensively investigated topic Predictive Maintenance (PdM), which refers to a proactive way of triggering servicing actions for industrial machinery. The aim of this maintenance strategy is to identify wear and tear, and consequent malfunctioning by analyzing condition monitoring data, recorded by sensor equipped machinery, in real-time. Recent developments in this area have shown potential to save time and material by preventing breakdowns and improving the overall predictability of industrial processes. However, due to the lack of high quality monitoring data and only little experience concerning the applicability of analysis methods, real-world implementations of Predictive Maintenance are still rare. Within this contribution, we present a method, to detect concept drift in data streams as potential indication for defective system behavior and depict initial tests on synthetic data sets. Further on, we present a real-world case study with industrial radial fans and discuss promising results gained from applying the detailed approach in this scope.",industry
10.1016/j.enconman.2019.111932,Journal,Energy Conversion and Management,scopus,2019-11-01,sciencedirect,Cultural coyote optimization algorithm applied to a heavy duty gas turbine operation,https://api.elsevier.com/content/abstract/scopus_id/85070893013,"In the past decades, the quantity of researches regarding industrial gas turbines (GT) has increased exponentially in terms of number of publications and diversity of applications. The GTs offer high power output along with a high combined cycle efficiency and high fuel flexibility. As consequence, the energy efficiency, the pressure oscillations, the pollutant emissions and the fault diagnosis have become some of the recent concerns related to this type of equipment. In order to solve these GTs related problems and many other real-world engineering and industry 4.0 issues, a set of new technological approaches have been tested, such as the combination of Artificial Neural Networks (ANN) and metaheuristics for global optimization. In this paper, the recently proposed metaheuristic denoted Coyote Optimization Algorithm (COA) is applied to the operation optimization of a heavy duty gas turbine placed in Brazil and used in power generation. The global goal is to find the best valves setup to reduce the fuel consumption while coping with environmental and physical constraints from its operation. In order to treat it as an optimization problem, an integrated simulation model is implemented from original data-driven models and others previously proposed in literature. Moreover, a new version of the COA that links some concepts from Cultural Algorithms (CA) is proposed, which is validated under a set of benchmarks functions from the Institute of Electrical and Electronics Engineers (IEEE) Congress on Evolutionary Computation (CEC) 2017 and tested to the GT problem. The results show that the proposed Cultural Coyote Optimization Algorithm (CCOA) outperforms its counterpart for benchmark functions. Further, non-parametric statistical significance tests prove that the CCOA’s performance is competitive when compared to other state-of-the-art metaheuristics after a set of experiments for five case studies. In addition, the convergence analysis shows that the cultural mechanism employed in the CCOA has improved the COA balance between exploration and exploitation. As a result, the CCOA can improve the current GT operation significantly, reducing the fuel consumption up to 
                        
                           3.6
                           %
                        
                      meanwhile all constraints are accomplished.",industry
10.1016/j.engfracmech.2019.106642,Journal,Engineering Fracture Mechanics,scopus,2019-10-01,sciencedirect,Necking-induced fracture prediction using an artificial neural network trained on virtual test data,https://api.elsevier.com/content/abstract/scopus_id/85071523401,"The imperfection-based necking model by Marciniak and Kuczyński (MK) is frequently used for predicting the onset of localized necking under proportional and non-proportional loading, which can be considered a lower limit for the occurrence of fracture in a vehicle body structure subjected to crash loading. A large number of virtual imperfection lines at different orientation angles have to be analysed simultaneously in order to find the critical imperfection causing necking under arbitrary loading. This, and the continuous computation of a “distance to necking” quantity, representing a crucial output quantity for the simulation engineer, makes the model computationally expensive and limits industrial use in full-scale vehicle crash simulations.
                  In this work, an extended MK model is used for creating a virtual test data base under proportional and non-proportional loading for training of a computationally more efficient simple feed-forward neural network (NN). Both models are implemented in a User Material routine of an explicit crash code, where the predictions of the NN are in good agreement with the predictions of the MK reference model, however at a significantly reduced computational cost. Besides a pure numerical validation study, an experimental validation study has been performed, imposing biaxial tension loading followed by plane strain tension loading until necking using a special punch test apparatus. Whereas MK and NN are in good agreement with the experimental observations, the agreement of classical necking models, applied in conjunction with a linear damage accumulation (forming severity) concept was less accurate.",industry
10.1016/j.ibiod.2019.104744,Journal,International Biodeterioration and Biodegradation,scopus,2019-10-01,sciencedirect,Comparative evaluation of Pseudomonas species in single chamber microbial fuel cell with manganese coated cathode for reactive azo dye removal,https://api.elsevier.com/content/abstract/scopus_id/85069842075,"Microbial fuel cell (MFCs), distinguished by different strains of Pseudomonas species; Pseudomonas aeruginosa (MPEM-MFC I) and Pseudomonas fluorescens (MPEM-MFC II), was analyzed. Results have shown that, over a period of 360 h in the presence of 0.5 mM of model dye, MPEM MFC I produced the maximum power density of 2887 ± 13 μW m−2 (RO-16) and 1906 ± 7 μW m−2 (RB-5) compared with MPEM-MFC II with 1896 ± 15 μW m−2 (RO-16) and 1028 ± 9 μW m−2 (RB-5). Decolorization efficiency of MPEM-MFC I was 98 ± 1.2% (RO-16) and 95 ± 2% (RB-5). Total phenazine production in MPEM-MFC I was 12.3 ± 0.5 μg mL−1 higher than that of 8.9 ± 0.05 μg mL−1 (MPEM-MFC II) and its production have positive influence of electron shuttling that brought out high power output. Addition of phenazine externally reduced the dye degradation. Bioadhesion capability of P. aeruginosa on the anode reduced the internal resistance in MFCs. Thus the implementation of MFC is a most promising technology for the complete decolorization of reactive azo dyes and it has potential economic benefits in real-life industrial application.",industry
10.1016/j.future.2019.04.014,Journal,Future Generation Computer Systems,scopus,2019-10-01,sciencedirect,TIDE: Time-relevant deep reinforcement learning for routing optimization,https://api.elsevier.com/content/abstract/scopus_id/85065443852,"Routing optimization has been researched in network design for a long time, and plenty of optimization schemes have been proposed from both academia and industry. However, such schemes are either too complicated in applications or far from the optimal performance. In recent years, with the development of Software-defined Networking (SDN) and Artificial Intelligence (AI), AI-based methods of routing strategy are being considered. In this paper, we propose TIDE, an intelligent network control architecture based on deep reinforcement learning that can dynamically optimize routing strategies in an SDN network without human experience. TIDE is implemented and validated on a real network environment. Experiment result shows that TIDE can adjust the routing strategy dynamically according to the network condition and can improve the overall network transmitting delay by about 9% compared with traditional algorithms.",industry
10.1016/j.enbuild.2019.07.029,Journal,Energy and Buildings,scopus,2019-09-15,sciencedirect,Whole building energy model for HVAC optimal control: A practical framework based on deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85069552761,"Whole building energy model (BEM) is a physics-based modeling method for building energy simulation. It has been widely used in the building industry for code compliance, building design optimization, retrofit analysis, and other uses. Recent research also indicates its strong potential for the control of heating, ventilation and air-conditioning (HVAC) systems. However, its high-order nature and slow computational speed limit its practical application in real-time HVAC optimal control. Therefore, this study proposes a practical control framework (named BEM-DRL) that is based on deep reinforcement learning. The framework is implemented and assessed in a novel radiant heating system in an existing office building as a case study. The complete implementation process is presented in this study, including: building energy modeling for the novel heating system, multi-objective BEM calibration using the Bayesian method and the Genetic Algorithm, deep reinforcement learning training and simulation results evaluation, and control deployment. By analyzing the real-life control deployment data, it is found that BEM-DRL achieves 16.7% heating demand reduction with more than 95% probability compared to the old rule-based control. However, the framework still faces the practical challenges including building energy modeling of novel HVAC systems and multi-objective model calibration. Systematic study is also needed for the design of deep reinforcement learning training to provide a guideline for practitioners.",industry
10.1016/j.jnca.2019.06.003,Journal,Journal of Network and Computer Applications,scopus,2019-09-15,sciencedirect,MAPLE: A Machine Learning Approach for Efficient Placement and Adjustment of Virtual Network Functions,https://api.elsevier.com/content/abstract/scopus_id/85067443855,"As one of the many advantages of cloud computing, Network Function Virtualization (NFV) has revolutionized the network and telecommunication industry through enabling the migration of network functions from expensive dedicated hardware to software-defined components that run in the form of Virtual Network Functions (VNFs). However, with NFV comes numerous challenges related mainly to the complexity of deploying and adjusting VNFs in the physical networks, owing to the huge number of nodes and links in today's datacenters, and the inter-dependency among VNFs forming a certain network service. Several contributions have been made in an attempt to answer these challenges, where most of the existing solutions focus on the static placement of VNFs and overlook the dynamic aspect of the problem, which arises mainly due to the ever-changing resource availability in the cloud datacenters and the continuous mobility of the users. Few attempts have been lately made to incorporate the dynamic aspect to the VNF deployment solutions. The main problem of these approaches lies in their reactive readjustment scheme which determines the placement/migration strategy upon the receipt of a new request or the happening of a certain event, thus resulting in high setup latencies. In this paper, we take advantage of machine learning to reduce the complexity of the placement and readjustment processes through designing a cluster-based proactive solution. The solution consists of (1) an Integer Linear Programming (ILP) model that considers a tradeoff between the minimization of the latency, Service-Level Objective (SLO) violation cost, hardware utilization, and VNF readjustment cost, (2) an optimized k-medoids clustering approach which proactively partitions the substrate network into a set of disjoint on-demand clusters and (3) data-driven cluster-based placement and readjustment algorithms that capitalize on machine learning to intelligently eliminate some cost functions from the optimization problem to boost its feasibility in large-scale networks. Simulation results show that the proposed solution considerably reduces the readjustment time and decrease the hardware utilization compared to the K-means, original k-medoids and migration without clustering approaches.",industry
10.1016/j.ifacol.2019.11.102,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,Sustainable operations management for industry 4.0 and its social return,https://api.elsevier.com/content/abstract/scopus_id/85078948022,"In today’s industrial environment, where concepts of smart factories are consolidating their application in companies, it is still necessary to approach management decision making from a perspective that encompasses all aspects of sustainability without losing sight of the social return to which they must contribute. In order to obtain a reliable prediction, of the operation of a Sustainable Manufacturing System (SMS) and its Social Return (SR), this paper develops a methodology and procedures that allow predicting the system performance as a whole. This will allow us to assist management decision making in industries 4.0, supported by multi-criteria methods in knowledge management, simulation, value analysis and operational research by means of:
                  a) Study the economic, social and environmental impacts in the organization and management of the efficient operation of an SMS with the selection of strategies and alternatives in production chains to minimize and / or mitigate environmental and labor risks.
                  b) Encourage of industrial symbiosis or eco-industries networks that create opportunities increasing eco-efficiency and the positive social return of production systems.
                  This proposed methodology will facilitate changes in the structure of production systems in order to implement industry 4.0 paradigms through facilitator technologies such as simulation and virtual reality. This framework will allow Small and Medium Enterprises (SMEs) and other companies to address the decision-making activities that improve the economic-functional efficiency, which will lead to reduce the environmental impact and increase the positive social return of certain production strategies, considering working conditions.
                  The proposed approach went validated, in the area of the Euroregion Galicia North of Portugal, to favour the implementation of the decision-making through the Industry 4.0 Technologies.",industry
10.1016/j.ifacol.2019.11.172,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,Machine learning framework for predictive maintenance in milling,https://api.elsevier.com/content/abstract/scopus_id/85078904429,"In the Industry 4.0 era, artificial intelligence is transforming the manufacturing industry. With the advent of Internet of Things (IoT) and machine learning methods, manufacturing systems are able to monitor physical processes and make smart decisions through realtime communication and cooperation with humans, machines, sensors, and so forth. Artificial intelligence enables manufacturers to reduce equipment downtime, spot production defects, improve the supply chain, and shorten design times by using machine learning technologies which learn from experiences. One of the last application of these technologies is the development of Predictive Maintenance systems. Predictive maintenance combines Industrial IoT technologies with machine learning to forecast the exact time in which manufacturing equipment will need maintenance, allowing problems to be solved and adaptive decisions to be made in a timely fashion. This study will discuss the implementation of a milling Cutting-tool Predictive Maintenance solution (including Wear Monitoring), applied to a real milling data set as validation of the framework. More generally, this work provides a basic framework for creating a tool to monitor the wear level, preventing the breakdown, of a generic manufacturing tool, in order to improve human-machine interaction and optimize the production process.",industry
10.1016/j.ifacol.2019.11.385,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,Towards a data-driven predictive-reactive production scheduling approach based on inventory availability,https://api.elsevier.com/content/abstract/scopus_id/85078884096,"To survive in a competitive business environment, manufacturing systems require the proper deployment of advanced technologies coming from Industry 4.0. These technologies allow access to quasi-real-time data that provide a continuously updated picture of the production system, including the state of available inventory. Data-driven predictive-reactive production scheduling has the potential to support the anticipation and prompt reaction to overcome different kinds of disruptions that occur in production execution nowadays. This research paper aims to propose a conceptual model for a data-driven predictive-reactive production scheduling approach combining machine learning and simulation-based optimization, considering current inventory of raw material, work in process and final products inventory to characterize a job-shop production execution state. The approach supports decision-making in dynamic situations related to inventory availability that can affect production schedules.",industry
10.1016/j.ifacol.2019.11.465,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,"Integration of automatic generated simulation models, machine control projects and management tools to support whole life cycle of industrial digital twins",https://api.elsevier.com/content/abstract/scopus_id/85078871061,"The paper presents a framework of automatic generation of industrial digital twins. These digital twins will be suitable to support preliminary design phases of systems development, but also to support next phases of detailed designs implementation and systems running phases. These digital twin allow, from the preliminary designing phase, to generate a complete simulation of the target industrial system. But, at the same time, and without the need to develop and add any subsequent code, they should be a valuable support for the phases and tasks of exploitation: maintenance, machine or system learning, etc. The problem is that the requirements for first development phases are much more generic than those for later phases. For this reason, instead of incorporating specificities in the simulation system, the framework takes advantage of the applications which are being developed for the implementation of the real system. In these applications (the control program and the decisions and the high level management system), the specificities have had to be taken into account. The system has been specialized in industrial transportation and warehouse systems which, although have a finite number or building objects, they have an infinite set of final configurations, very different one from each other. The paper presents an evaluation of current simulation platforms suitable to be used as part of the framework, and the digital twin industrial system generation framework itself. An example of application is as well presented.",industry
10.1016/j.trip.2019.100028,Journal,Transportation Research Interdisciplinary Perspectives,scopus,2019-09-01,sciencedirect,Translation software: An alternative to transit data standards,https://api.elsevier.com/content/abstract/scopus_id/85075931935,"Data standardization is recognized in many disciplines as a critical aspect of data stewardship. Establishing and implementing data specifications increases the usefulness of data collection efforts and facilitates analysis techniques. With the advent of large quantities of machine-generated data, the use of standardized data formats feeds opportunities for visualization and advanced applications with machine-learning and Artificial Intelligence (AI). The transportation industry made substantial progress with data format specifications in the late 1990s, primarily for highway traffic. Unfortunately, establishing data standards has been an on-going challenge for the transit community. Archived Intelligent Transportation Systems (ITS) transit data (e.g., Automatic Vehicle Location (AVL), Automatic Passenger Counters (APCs), Automatic Fare Card (AFC)) still lack industry standards for data formats. Recent advancements in electronic transit scheduling (e.g., General Transit Feed Specifications (GTFS)) met a portion of this challenge with Open Data specifications. Now GTFS provides transit riders with agile information on services available at any location where the data is provided to developers of mobile device application (apps). Due to system and vendor limitations, the Metropolitan Transportation Authority (MTA), serving the New York City region, publishes its real-time subway system data in GTFS-R and its bus data in SIRI. This research develops an Application Programming Interface (API) to translate GTFS-R into SIRI to overcome the lack of standards making it possible to harmonize the subway and bus systems for the New York region. This solution offers the opportunity to develop a novel set of analytical tools, including pseudo-surveillance data for performance metrics.",industry
10.1016/j.jngse.2019.102933,Journal,Journal of Natural Gas Science and Engineering,scopus,2019-09-01,sciencedirect,Machine learning for surveillance of fluid leakage from reservoir using only injection rates and bottomhole pressures,https://api.elsevier.com/content/abstract/scopus_id/85068973220,"Carbon-neutral economies would require preventing the release of industrial-scale CO2 into the atmosphere by injecting into geologic formations. Large-scale injection of CO2 into deep reservoirs carries a potential for its undesired leakage into above zones, which can act as an obstacle to its large-scale implementation. Current methods for surveillance of CO2 leaks are costly and not very robust, especially the methods that simulate expected pressure behavior based on an assumed reservoir model.
                  This study proposes a machine learning method for surveillance of fluid leakage using deconvolution response function (a non-linear function of time varying bottomhole pressure and injection rates) from injection and monitoring wells as a measure of leakage that is simulated via multivariate linear regression of all the wells present in the reservoir. Leakage is detected by comparing “expected” (baseline without leaks) deconvolution response of all monitoring wells with their “observed” deconvolution response. Three key advantages of the proposed method are that it i) uses only injection rates and bottomhole pressure data (with no reservoir or geological model), ii) is independent of physical process parameterization uncertainties, and iii) applicable to both conventional and unconventional (e.g. fractured tight formations) reservoirs with any fluid (e.g. compressible, incompressible). The proposed method is first trained to learn well history with no leakage, followed by its validation after which it can be used to detect leakage by tracking a meaningful deviation error (at least twenty times the error of no leakage base scenario over same time period) between expected well response and observed well response at all monitoring wells. The well history required for the proposed method comes directly from measurements made at wells in a real field, but in absence of field data the proposed method is illustrated through well history simulated by reservoir simulations; no such numerical simulations are required for application of this method in a real world scenario with well measurements.",industry
10.1016/j.jss.2019.05.026,Journal,Journal of Systems and Software,scopus,2019-09-01,sciencedirect,Sentiment based approval prediction for enhancement reports,https://api.elsevier.com/content/abstract/scopus_id/85065795226,"The maintenance and evolution of the software application is a continuous phase in the industry. Users are frequently proposing enhancement requests for further functionalities. However, although only a small part of these requests are finally adopted, developers have to go through all of such requests manually, which is tedious and time consuming. To this end, in this paper we propose a sentiment based approach to predict how likely enhancement reports would be approved or rejected so that developers can first handle likely-to-be-approved requests. This could help the software applications to compete in the industry by upgrading their features in time as per user’s requirements. First, we preprocess enhancement reports using natural language preprocessing techniques. Second, we identify the words having positive and negative sentiments in the summary attribute of the enhancements reports and calculate the sentiment of each enhancement report. Finally, with the history data of real software application, we train a machine learning based classifier to predict whether a given enhancement report would be approved. The proposed approach has been evaluated with the history data from real software applications. The cross-application validation suggests that the proposed approach outperforms the state-of-the-art. The evaluation results suggest that the proposed approach increases the accuracy from 70.94% to 77.90% and improves the F-measure significantly from 48.50% to 74.53%.",industry
10.1016/j.compind.2019.04.016,Journal,Computers in Industry,scopus,2019-09-01,sciencedirect,A comparison of fog and cloud computing cyber-physical interfaces for Industry 4.0 real-time embedded machine learning engineering applications,https://api.elsevier.com/content/abstract/scopus_id/85065718296,"Industrial cyber-physical systems are the primary enabling technology for Industry 4.0, which combine legacy industrial and control engineering, with emerging technology paradigms (e.g. big data, internet-of-things, artificial intelligence, and machine learning), to derive self-aware and self-configuring factories capable of delivering major production innovations. However, the technologies and architectures needed to connect and extend physical factory operations to the cyber world have not been fully resolved. Although cloud computing and service-oriented architectures demonstrate strong adoption, such implementations are commonly produced using information technology perspectives, which can overlook engineering, control and Industry 4.0 design concerns relating to real-time performance, reliability or resilience. Hence, this research compares the latency and reliability performance of cyber-physical interfaces implemented using traditional cloud computing (i.e. centralised), and emerging fog computing (i.e. decentralised) paradigms, to deliver real-time embedded machine learning engineering applications for Industry 4.0. The findings highlight that despite the cloud’s highly scalable processing capacity, the fog’s decentralised, localised and autonomous topology may provide greater consistency, reliability, privacy and security for Industry 4.0 engineering applications, with the difference in observed maximum latency ranging from 67.7%–99.4%. In addition, communication failures rates highlighted differences in both consistency and reliability, with the fog interface successfully responding to 900,000 communication requests (i.e. 0% failure rate), and the cloud interface recording failure rates of 0.11%, 1.42%, and 6.6% under varying levels of stress.",industry
10.1016/j.eswa.2019.03.011,Journal,Expert Systems with Applications,scopus,2019-08-15,sciencedirect,Constraint learning based gradient boosting trees,https://api.elsevier.com/content/abstract/scopus_id/85063576343,"Predictive regression models aim to find the most accurate solution to a given problem, often without any constraints related to the model’s predicted values. Such constraints have been used in prior research where they have been applied to a subpopulation within the training dataset which is of greater interest and importance. In this research we introduce a new setting of regression problems, in which each instance can be assigned a different constraint, defined based on the value of the target (predicted) attribute. The new use of constraints is taken into account and incorporated into the learning process, and is also considered when evaluating the induced model. We propose two algorithms which are modifications to the regression boosting method. There are two advantages of the proposed algorithms: they are not dependent on the base learner used during the learning process, and they can be adopted by any boosting technique. We implemented the algorithms by modifying the gradient boosting trees (GBT) model, and we also introduced two measures for evaluating the models that were trained to solve the constraint problems. We compared the proposed algorithms to three baseline algorithms using four real-life datasets. Due to the algorithms’ focus on satisfying the constraints, in most cases the results showed significant improvement in the constraint-related measures, with just a minimal effect on the general prediction error. The main impact of the proposed approach is in its ability to derive a model with a higher level of assurance for specific cases of interest (i.e., the constrained cases). This is extremely important and has great significance in various use cases and expert and intelligent systems, particularly critical systems, such as critical healthcare systems (e.g., when predicting blood pressure or blood sugar level), safety systems (e.g., when aiming to estimate the distance of cars or airplanes from other objects), or critical industrial systems (e.g., require to estimate their usability along time). In each of these cases, there is a subpopulation of all instances that is of greater interest to the expert or system, and the sensitivity of the model’s error changes according to the real value of the predicted feature. For example, for a subpopulation of patients (e.g., patients under the age of eight, or patients known to be at risk), physicians often require a sensitive model that accurately predicts blood pressure values.",industry
10.1016/j.mfglet.2019.08.003,Journal,Manufacturing Letters,scopus,2019-08-01,sciencedirect,A self-aware and active-guiding training &amp; assistant system for worker-centered intelligent manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85070738030,"Training and on-site assistance is critical to help workers master required skills, improve worker productivity, and guarantee the product quality. Traditional training methods lack worker-centered considerations that are particularly in need when workers are facing ever-changing demands. In this study, we propose a worker-centered training & assistant system for intelligent manufacturing, which is featured with self-awareness and active-guidance. Multi-modal sensing techniques are applied to perceive each individual worker and a deep learning approach is developed to understand the worker’s behavior and intention. Moreover, an object detection algorithm is implemented to identify the parts/tools the worker is interacting with. Then the worker’s current state is inferred and used for quantifying and assessing the worker performance, from which the worker’s potential guidance demands are analyzed. Furthermore, onsite guidance with multi-modal augmented reality is provided actively and continuously during the operational process. Two case studies are used to demonstrate the feasibility and great potential of our proposed approach and system for applying to the manufacturing industry for frontline workers.",industry
10.1016/j.compind.2019.04.010,Journal,Computers in Industry,scopus,2019-08-01,sciencedirect,Managing workflow of customer requirements using machine learning,https://api.elsevier.com/content/abstract/scopus_id/85065732680,"Customer requirements – product specifications issued by the customer – organize the dialog between suppliers and customers and, hence, affect the dynamics of supply networks. These large and complex documents are frequently updated over time, while changes are seldom marked by the customers who issue the requirements. The lack of structure and defined responsibilities, thus, demands an expert to manually process the requirements. Here, the possibility to improve the usual workflow with machine learning algorithms is explored.
                  The whole requirements management process has two major bottlenecks, which can be automatized. The first one, detecting changes, can be accomplished via a document comparison tool. The second one, recognizing the responsibilities and assigning them to the right department, can be solved with standard machine learning algorithms. Here, such algorithms are applied to a dataset obtained from a global automotive industry supplier.
                  The proposed method improves the requirements management process by reducing an expert’s workload and thus decreasing the time for processing one document was reduced from 2 weeks to 1 h. Moreover, the method gives a high accuracy of department assignment and can self-improve once implemented into a requirements management system.
                  Although the machine learning methods are very popular nowadays, they are seldom used to improve business processes in real companies, especially in the case of processes that did not require digitalization in the past. Here we show, how such methods can solve some of the management problems and improve their workflow.",industry
10.1016/j.compind.2019.05.001,Journal,Computers in Industry,scopus,2019-08-01,sciencedirect,Industrial robot control and operator training using virtual reality interfaces,https://api.elsevier.com/content/abstract/scopus_id/85065132267,"Nowadays, we are involved in the fourth industrial revolution, commonly referred to as “Industry 4.0,” where cyber-physical systems and intelligent automation, including robotics, are the keys. Traditionally, the use of robots has been limited by safety and, in addition, some manufacturing tasks are too complex to be fully automated. Thus, human-robot collaborative applications, where robots are not isolated, are necessary in order to increase the productivity ensuring the safety of the operators with new perception systems for the robot and new interaction interfaces for the human. Moreover, virtual reality has been extended to the industry in the last years, but most of its applications are not related to robots. In this context, this paper works on the synergies between virtual reality and robotics, presenting the use of commercial gaming technologies to create a totally immersive environment based on virtual reality. This environment includes an interface connected to the robot controller, where the necessary mathematical models have been implemented for the control of the virtual robot. The proposed system can be used for training, simulation, and what is more innovative, for robot controlling in an integrated, non-expensive and unique application. Results show that the immersive experience increments the efficiency of the training and simulation processes, offering a cost-effective solution.",industry
10.1016/j.ece.2019.05.003,Journal,Education for Chemical Engineers,scopus,2019-07-01,sciencedirect,"Learning distillation by a combined experimental and simulation approach in a three steps laboratory: Vapor pressure, vapor-liquid equilibria and distillation column",https://api.elsevier.com/content/abstract/scopus_id/85066038830,"Distillation is one of the most important separation process in industrial chemistry. This operation is based on a deep knowledge of the fluid phase equilibria involved in the mixture to be separated. In particular, the most important aspects are the determination of the vapor pressures of the single compounds and the correct representation of the eventual not ideality of the mixture. Simulation science is a fundamental tool for managing these complex topics and chemical engineers students have to learn and to use it on real case-studies. To give to the students a complete overview of these complex aspects, a laboratory experience is proposed. Three different work stations were set up: i) determination of vapor pressure of two pure compounds; ii) the study of vapor-liquid equilibria of a binary mixture; iii) the use of a continuous multistage distillation column in dynamic and steady-state conditions. The simulation of all these activities by a commercial software, PRO II by AVEVA, allows to propose and verify the thermodynamic characteristics of the mixture and to correctly interpret the distillation column data. Moreover, the experimental plants and the data elaboration by classical equations are presented. The students are request to prepare a final report in which the description of the experimental plants and experimental procedure, the interpretation of the results and the simulation study are critically discussed in order to encourage them to reason and to acquire the concepts of the course.
                  Two different questionnaires each with 7 questions, for the course and for the laboratory, are proposed and analyzed. The final evaluation of the students was strongly positive both for the course as a whole and for the proposed laboratory activities.",industry
10.1016/j.psep.2019.05.016,Journal,Process Safety and Environmental Protection,scopus,2019-07-01,sciencedirect,An intelligent fire detection approach through cameras based on computer vision methods,https://api.elsevier.com/content/abstract/scopus_id/85065893982,"Fire that is one of the most serious accidents in petroleum and chemical factories, may lead to considerable production losses, equipment damages and casualties. Traditional fire detection was done by operators through video cameras in petroleum and chemical facilities. However, it is an unrealistic job for the operator in a large chemical facility to find out the fire in time because there may be hundreds of video cameras installed and the operator may have multiple tasks during his/her shift. With the rapid development of computer vision, intelligent fire detection has received extensive attention from academia and industry. In this paper, we present a novel intelligent fire detection approach through video cameras for preventing fire hazards from going out of control in chemical factories and other high-fire-risk industries. The approach includes three steps: motion detection, fire detection and region classification. At first, moving objects are detected through cameras by a background subtraction method. Then the frame with moving objects is determined by a fire detection model which can output fire regions and their locations. Since false fire regions (some objects similar with fire) may be generated, a region classification model is used to identify whether it is a fire region or not. Once fire appears in any camera, the approach can detect it and output the coordinates of the fire region. Simultaneously, instant messages will be immediately sent to safety supervisors as a fire alarm. The approach can meet the needs of real-time fire detection on the precision and the speed. Its industrial deployment will help detect fire at the very early stage, facilitate the emergency management and therefore significantly contribute to loss prevention.",industry
10.1016/j.cie.2019.04.054,Journal,Computers and Industrial Engineering,scopus,2019-07-01,sciencedirect,Agent-based modelling and heuristic approach for solving complex OEM flow-shop productions under customer disruptions,https://api.elsevier.com/content/abstract/scopus_id/85065083945,"The application of the agent-based simulation approach in the flow-shop production environment has recently gained popularity among researchers. The concept of agent and agent functions can help to automate a variety of difficult tasks and assist decision-making in flow-shop production. This is especially so in the large-scale Original Equipment Manufacturing (OEM) industry, which is associated with many uncertainties. Among these are uncertainties in customer demand requirements that create disruptions that impact production planning and scheduling, hence, making it difficult to satisfy demand in due time, in the right order delivery sequence, and in the right item quantities. It is however important to devise means of adapting to these inevitable disruptive problems by accommodating them while minimising the impact on production performance and customer satisfaction.
                  In this paper, an innovative embedded agent-based Production Disruption Inventory-Replenishment (PDIR) framework, which includes a novel adaptive heuristic algorithm and inventory replenishment strategy which is proposed to tackle the disruption problems. The capabilities and functionalities of agents are utilised to simulate the flow-shop production environment and aid learning and decision making. In practice, the proposed approach is implemented through a set of experiments conducted as a case study of an automobile parts facility for a real-life large-scale OEM. The results are presented in term of Key Performance Indicators (KPIs), such as the number of late/unsatisfied orders, to determine the effectiveness of the proposed approach. The results reveal a minimum number of late/unsatisfied orders, when compared with other approaches.",industry
10.1016/j.est.2019.04.015,Journal,Journal of Energy Storage,scopus,2019-06-01,sciencedirect,Comparison of a physical and a data-driven model of a Packed Bed Regenerator for industrial applications,https://api.elsevier.com/content/abstract/scopus_id/85064907454,"Thermal Energy Storage systems are promising technologies to match intermittent heat supply with demand and improve the energy efficiency of industrial processes. To optimally integrate these energy storage systems in industry, reliable and industrially applicable models are required. This work examines two different modeling approaches for a Sensible Thermal Energy Storage device, namely a Packed Bed Regenerator. A physical 1D-model using finite difference methods and a data-driven grey box model using Recurrent Neural Networks are described. Experimental data from a Packed Bed Regenerator test rig is used to create the data-driven model and to compare the results of both models with real measurements. A quantitative and qualitative comparison of the data-driven and the physical model is conducted. The results of the quantitative investigation show, that both models are able to capture the complex behavior of the Packed Bed Regenerator. With the qualitative analysis, the features of the different models are highlighted and advantages and limitations are discussed. Thus, it provides an orientation in the decision-making process for the choice of an appropriate modeling approach. The findings of this work can support the creation of physical, as well as data-driven models of sensible energy storage systems and strengthen their implementation to industrial processes. The generic grey box modeling approach and the findings of the qualitative comparison of the models can be also applied to other modeling tasks.",industry
10.1016/j.scitotenv.2019.02.213,Journal,Science of the Total Environment,scopus,2019-05-20,sciencedirect,Passive sampling of volatile organic compounds in industrial atmospheres: Uptake rate determinations and application,https://api.elsevier.com/content/abstract/scopus_id/85061829807,"This study describes the implementation of a passive sampling-based method followed by thermal desorption gas-chromatography-mass spectrometry (TD-GC–MS) for the monitoring of volatile organic compounds (VOCs) in industrial atmospheres. However, in order to employ passive sampling as a reliable sampling technique, a specific diffusive uptake rate is required for each compound. Accordingly, the aim of the present study was twofold. First, the experimental diffusive uptake rates of the target VOCs were determined under real industrial air conditions using Carbopack X thermal desorption tubes, and active sampling as reference method. The sampling campaigns carried out between October 2017 and May 2018 provided us of experimental diffusive uptake rates between 0.40 mL min−1 and 0.70 mL min−1 and stable over time (RSD % < 8%) for up to 41 VOCs. Secondly, the uptake rates obtained experimentally were applied for the determination of VOCs concentrations at 16 sampling sites in the North Industrial Complex of Tarragona. The results showed i-pentane, n-pentane and the compounds known as BTEX as the most representative ones. Moreover, some sporadic peaks of 1,3-butadiene, acrylonitrile, ethylbenzene and styrene resulting from certain industrial activities were detected.",industry
10.1016/j.engappai.2019.03.011,Journal,Engineering Applications of Artificial Intelligence,scopus,2019-05-01,sciencedirect,Distributed parallel deep learning of Hierarchical Extreme Learning Machine for multimode quality prediction with big process data,https://api.elsevier.com/content/abstract/scopus_id/85063385858,"In this work, the distributed and parallel Extreme Learning Machine (dp-ELM) and Hierarchical Extreme Learning Machine (dp-HELM) are proposed for multimode process quality prediction with big data. The efficient ELM algorithm is transformed into the distributed and parallel modeling form according to the MapReduce framework. Since the deep learning network structure of HELM is more accurate than the single layer of ELM in feature representation, the dp-HELM is further developed through decomposing the ELM-based Auto-encoders (ELM-AE) of deep hidden layers into a loop of MapReduce jobs. Additionally, the multimode issue is solved through the “divide and rule” strategy. The distributed and parallel K-means (dp-K-means) is utilized to divide the process modes, which are further trained in a synchronous parallel way by dp-ELM and dp-HELM. Finally, the Bayesian model fusion technique is utilized to integrate the local models for online prediction. The proposed algorithms are deployed on a Hadoop MapReduce computing cluster and the feasibility and efficiency are illustrated through building a real industrial quality prediction model with big process data.",industry
10.1016/j.jlp.2019.03.003,Journal,Journal of Loss Prevention in the Process Industries,scopus,2019-05-01,sciencedirect,A fuzzy expert system for mitigation of risks and effective control of gas pressure reduction stations with a real application,https://api.elsevier.com/content/abstract/scopus_id/85063113351,"Environmental changes and increased uncertainty due to technical damage, explosions and large fires have caused the risk of an inevitable element in the gas industry. This study purposes developing a new hybrid fuzzy expert system as a decision support system to mitigate the risk associated with gas transmission stations. The designed knowledge-based system combines the procedural and descriptive rules based on experts’ judgments to analyze the complex relationships between the different components of a gas pressure reduction station. The developed fuzzy expert system is coded in C language integrated production system (CLIPS) and is linked with MATLAB software for calling fuzzy functions. A real case study of gas pressure reduction stations in Iranian gas industry is conducted to validate the proposed expert system model. The expert system provides more than one thousand rules based on expert knowledge to prevent the pressure drop and the quality loss of gas or shutting off gas flow which accordingly increases gas flow stability. The proposed expert system could minimize the risk of hazardous scenarios, such as leakage and corrosion, in the gas industry and provide an acceptable precision in the provision of periodic control strategies and appropriate response under an emergency condition.",industry
10.1016/j.engappai.2019.02.019,Journal,Engineering Applications of Artificial Intelligence,scopus,2019-05-01,sciencedirect,A new hierarchical approach to requirement analysis of problems in automated planning,https://api.elsevier.com/content/abstract/scopus_id/85062901276,"The use of Knowledge Engineering (KE) processes to analyze and configure domains in automated planning is becoming more appealing since it was noticed that this issue could make a difference to solve real problems. The contrast between a generic domain independent approach, taken as canonical in AI, and alternative processes that include knowledge engineering – eventually adding specific knowledge – has been discussed by Computer and Engineering communities. A big impact has been noticed mainly in the early phase of requirement analysis when KE approach is normally introduced. Requirement analysis is responsible for carrying out the Knowledge modeling of both problem and work domains, which is a key issue to guide different planner algorithms to come out with efficient solutions. Also, there is the scalability issue that appear in most real problems. To face that, hierarchical methods played an important hole in the history of planning and inspired several solutions since the proposal of NONLIN in the 70’s. Since then, the idea of associating hierarchical relational nets with partial ordered actions has prevailed when large systems were considered. However, there is still a gap between the hierarchical approach and the state of art of requirements analysis to allow features anticipated by KE approach to really appear in the requirements of a planning process. This paper proposes a pathway to solve this gap starting with requirements elicitation represented first in the conventional semi-formal (diagrammatic) language – UML – that is translated to Hierarchical Petri Nets (HPNs) by a new enhanced algorithm. The proposed process was installed in a software tool – developed by one of the authors – that analyzes the performance of the KE planning model: itSIMPLE (Integrated Tools Software Interface for Modeling Planning Environment). This tool was initially designed to use classic Place/Transition nets and an old version of UML (2.1). It is now enhanced to use UML 2.4 and a hierarchical Petri Net extension, also developed by the authors. Realistic examples illustrate the process which is now being applied to larger problems related to the manufacturing of car sequencing domain, one of challenge of ROADEF 2005 (French Operations Research & Decision Support Society). Finally, we consider the possibility to introduce another approach to the KE process by using KAOS (Keep All Object Satisfied) to make the planning design more accurate.",industry
10.1016/j.ijheatmasstransfer.2018.12.170,Journal,International Journal of Heat and Mass Transfer,scopus,2019-05-01,sciencedirect,Visualization-based nucleate boiling heat flux quantification using machine learning,https://api.elsevier.com/content/abstract/scopus_id/85059864859,"Processes involving complex phenomena are ubiquitous in nature and industry, many of which are difficult to simulate computationally. Nucleate boiling heat transfer, for instance, has numerous practical applications, while the film boiling is an undesirable operation regime. So far, most correlations and computer simulations to quantify boiling heat transfer rely on direct measurement of thermohydraulic data, such as heater temperature, which is often invasive. Here it is demonstrated that neural network-based models can quantify heat transfer using only direct and indirect visual information of the boiling phenomenon, without any prior knowledge of the governing equations, which enables the non-intrusive measurement of heat flux based on boiling process imaging. It is shown that neural networks can encode bubble morphology and its correlation with heat flux returning errors as low as 7% when compared with precise experimental measurements, a significant improvement over current prediction methods of boiling heat transfer. Furthermore, it is shown that these systems may be implemented in inexpensive, compact computers, such as the Raspberry Pi, to infer heat flux in real time from visualization.",industry
10.1016/j.mfglet.2019.05.003,Journal,Manufacturing Letters,scopus,2019-04-01,sciencedirect,A blockchain enabled Cyber-Physical System architecture for Industry 4.0 manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85066168835,"Cyber-Physical Production Systems (CPPSs) are complex manufacturing systems which aim to integrate and synchronize machine world and manufacturing facility to the cyber computational space. However, having intensive interconnectivity and a computational platform is crucial for real-world implementation of CPPSs. In this paper, the potential impacts of blockchain technology in development and realization of real-world CPPSs are discussed. A unified three-level blockchain architecture is proposed as a guideline for researchers and industries to clearly identify the potentials of blockchain and adapt, develop, and incorporate this technology with their manufacturing developments towards Industry 4.0.",industry
10.1016/j.compag.2019.02.023,Journal,Computers and Electronics in Agriculture,scopus,2019-04-01,sciencedirect,Real-time nondestructive monitoring of Common Carp Fish freshness using robust vision-based intelligent modeling approaches,https://api.elsevier.com/content/abstract/scopus_id/85062029959,"In the current research, the potential of a novel method based on the artificial neural network was investigated to diagnose the freshness of common carp (Cyprinus carpio) during ice storage. Fish as an aquaculture product has high nutrients and low-fat content. So, people have consumed it as a safe and high-value foodstuff in their daily diet. Investigation of fish freshness is proposed as a significant issue in the aquaculture industry since fish spoils rapidly. The applied system of this study is comprised of the following steps: First, images of samples were captured and the pre-processing operation was done on the images. Then, particular channels including R, G, B, H, S, I, L*, a*, and b* were computed. Next, feature extraction was performed to obtain 6 types of texture features from each channel. Afterward, the hybrid Artificial Bee Colony-Artificial Neural Network (ABC-ANN) algorithm was applied to select the best features. Finally, the Support Vector Machine (SVM), K-Nearest Neighbor (K-NN) and Artificial Neural Network (ANN) algorithms as the most common methods were used to classify fish images. The best performance of the K-NN classifier was calculated in the k = 8 neighborhood size with the accuracy of 90.48. The best kernel function for the SVM algorithm was polynomial with C, sigma, and accuracy of 1, 2 and 91.52 percent, respectively. In this system, the input layer has consisted of 22 neurons based on the feature selection operation and 4 classes including most fresh, fresh, fairly fresh and spoiled have been used as the number of output layer. At the end, the best results of the MLP networks were achieved by LM learning algorithm and 6 neurons in the hidden layer with the 22–10–4 topology and accuracy of 93.01 percent. The achieved results demonstrate the high performance of the ANN classifier for evaluation of common carp freshness during ice storage as a rapid, accurate, non-destructive, real-time and automated method. It shows the potential of computer vision method in combination with artificial neural networks as an intelligent technique for evaluation of fish freshness.",industry
10.1016/j.compeleceng.2018.03.015,Journal,Computers and Electrical Engineering,scopus,2019-03-01,sciencedirect,BCI cinematics – A pre-release analyser for movies using H <inf>2</inf> O deep learning platform,https://api.elsevier.com/content/abstract/scopus_id/85046107707,"Entertainment industry has seen a phenomenal growth throughout the globe in recent times and movie industry enjoys a crucial role in the above emergence. A movie can capture the attention of a viewer and can trigger cognitive and emotional processes in the brain. In this article we assess the emotional outcome of the viewer while they watch the movie before its actual release that is, during its preview. Traditionally FMRI was used to assess the activity of brain but proved to be non-feasible and costly so we used EEG Sensors to monitor and record the functioning of the brain of movie viewer for further analysis. The collected data through EEG sensor were analysed using deep learning framework. H2O package of deep learning was employed to find high and low of different brain waves mapping to the emotions depicted in the every scene of the movie. Our proposed system named BCI cinematics obtained 85% accuracy and results were validated by obtaining the feedback from the stake holders. The outcome of this work will assist the creators to understand the emotional impact of movie over a normal viewer impartially thus enable them to modify certain scenes or change sequence of scenes and so on. When deployed in real time our system prove to be a cost saver for movie makers.",industry
10.1016/j.future.2018.02.011,Journal,Future Generation Computer Systems,scopus,2019-03-01,sciencedirect,Collaborative prognostics in Social Asset Networks,https://api.elsevier.com/content/abstract/scopus_id/85042391186,"With the spread of Internet of Things (IoT) technologies, assets have acquired communication, processing and sensing capabilities. In response, the field of Asset Management has moved from fleet-wide failure models to individualised asset prognostics. Individualised models are seldom truly distributed, and often fail to capitalise the processing power of the asset fleet. This leads to hardly scalable machine learning centralised models that often must find a compromise between accuracy and computational power. In order to overcome this, we present a novel theoretical approach to collaborative prognostics within the Social Internet of Things. We introduce the concept of Social Asset Networks, defined as networks of cooperating assets with sensing, communicating and computing capabilities. In the proposed approach, the information obtained from the medium by means of sensors is synthesised into a Health Indicator, which determines the state of the asset. The Health Indicator of each asset evolves according to an equation determined by a triplet of parameters. Assets are given the form of the equation but they ignore their parametric values. To obtain these values, assets use the equation in order to perform a non-linear least squares fit of their Health Indicator data. Using these estimated parameters, they are interconnected to a subset of collaborating assets by means of a similarity metric. We show how by simply interchanging their estimates, networked assets are able to precisely determine their Health Indicator dynamics and reduce maintenance costs. This is done in real time, with no centralised library, and without the need for extensive historical data. We compare Social Asset Networks with the typical self-learning and fleet-wide approaches, and show that Social Asset Networks have a faster convergence and lower cost. This study serves as a conceptual proof for the potential of collaborative prognostics for solving maintenance problems, and can be used to justify the implementation of such a system in a real industrial fleet.",industry
10.1016/j.enbuild.2018.12.034,Journal,Energy and Buildings,scopus,2019-02-15,sciencedirect,"IntelliMaV: A cloud computing measurement and verification 2.0 application for automated, near real-time energy savings quantification and performance deviation detection",https://api.elsevier.com/content/abstract/scopus_id/85059816255,"Energy conservation measures (ECMs) are implemented in all sectors with the objective of improving the efficiency with which energy is consumed. Measurement and verification (M&V) is required to verify the performance of every ECM to ensure its successful implementation and operation. The methodologies implemented to achieve this are currently evolving to a more dynamic state, known as measurement and verification 2.0, through the use of automated and advanced analytics. The primary barrier to the adoption of M&V 2.0 practices are the tools available to practitioners. This paper aims to populate the knowledge gap in the industrial buildings sector by presenting a novel cloud computing-based application, IntelliMaV, that applies advanced machine learning techniques on large datasets to automatically verify the performance of ECMs in near real-time. Additionally, a performance deviation detection system is incorporated, ensuring persistence of savings beyond the typical period of analysis in M&V.
                  IntelliMaV allows M&V practitioners to quantify energy savings with minimum levels of uncertainty by applying powerful analytics to data readily available in industrial facilities. The use of a cloud computing-based architecture reduces the resources required on-site and decreases the time required to train the baseline energy model through the use of parallel processing. The robust nature of the application ensures it is applicable across the broad spectrum of ECMs in the industrial buildings sector. A case study carried out in a large biomedical manufacturing facility demonstrates the ease of use of the application and the benefits realised through its adoption. The energy savings from an ECM were calculated to be 2,353,225 kWh/yr with 25.5% uncertainty at a 90% confidence interval.",industry
10.1016/j.oceaneng.2019.01.003,Journal,Ocean Engineering,scopus,2019-02-01,sciencedirect,Data management for structural integrity assessment of offshore wind turbine support structures: data cleansing and missing data imputation,https://api.elsevier.com/content/abstract/scopus_id/85061324147,"Structural Health Monitoring (SHM) and Condition Monitoring (CM) Systems are currently utilised to collect data from offshore wind turbines (OWTs), to enhance the accurate estimation of their operational performance. However, industry accepted practices for effectively managing the information that these systems provide have not been widely established yet. This paper presents a four-step methodological framework for the effective data management of SHM systems of OWTs and illustrates its applicability in real-time continuous data collected from three operational units, with the aim of utilising more complete and accurate datasets for fatigue life assessment of support structures. Firstly, a time-efficient synchronisation method that enables the continuous monitoring of these systems is presented, followed by a novel approach to noise cleansing and the posterior missing data imputation (MDI). By the implementation of these techniques those data-points containing excessive noise are removed from the dataset (Step 2), advanced numerical tools are employed to regenerate missing data (Step 3) and fatigue is estimated for the results of these two methodologies (Step 4). Results show that after cleansing, missing data can be imputed with an average absolute error of 2.1%, while this error is kept within the [+ 15.2%−11.0%] range in 95% of cases. Furthermore, only 0.15% of the imputed data fell outside the noise thresholds. Fatigue is found to be underestimated both, when data cleansing does not take place and when it takes place but MDI does not. This makes this novel methodology an enhancement to conventional structural integrity assessment techniques that do not employ continuous datasets in their analyses.",industry
10.1016/j.therap.2018.12.002,Journal,Therapie,scopus,2019-02-01,sciencedirect,"Early access to health products in France: Major advances of the French “Conseil stratégique des industries de santé” (CSIS) to be implemented (modalities, regulations, funding)",https://api.elsevier.com/content/abstract/scopus_id/85061149651,"In a context of perpetual evolution of treatments, access to therapeutic innovation is a major challenge for patients and the various players involved in the procedures of access to medicines. The revolutions in genomic and personalized medicine, artificial intelligence and biotechnology will transform the medicine of tomorrow and the organization of our health system. It is therefore fundamental that France prepares for these changes and supports the development of its companies in these new areas. The recent “Conseil stratégique des industries de santé” launched by Matignon makes it possible to propose a regulatory arsenal conducive to the implementation and diffusion of therapeutic innovations. In this workshop, we present a number of proposals, our approach having remained pragmatic with a permanent concern to be effective in the short term for the patients and to simplify the procedures as much as possible. This was achieved thanks to the participation in this workshop of most of the players involved (industrial companies, “Agence nationale de sécurité du médicament et des produits de santé”, “Haute Autorité de santé”, “Institut national du cancer”, “Les entreprises du médicament”, hospitals, “Observatoire du médicament, des dispositifs médicaux et de l’innovation thérapeutique”…). The main proposals tend to favor the implementation of clinical trials on our territory, especially the early phases, a wider access to innovations by favoring early access programs and setting up a process called “autorisation temporaire d’utilisation d’extension” (ATUext) that make it possible to prescribe a medicinal product even if the latter has a marketing authorisation in another indication. In addition, we propose a conditional reimbursement that will be available based on preliminary data but will require re-evaluation based on consolidated data from clinical trials and/or real-life data. Finally, in order to better carry out these assessments, with a view to access or care, we propose the establishment of partnership agreements with health agencies/hospitals in order to encourage the emergence of field experts, in order to prioritize an ascending expertise closer to patients’ needs and to real life.",industry
10.1016/j.cie.2018.08.018,Journal,Computers and Industrial Engineering,scopus,2019-02-01,sciencedirect,Ensemble-based big data analytics of lithofacies for automatic development of petroleum reservoirs,https://api.elsevier.com/content/abstract/scopus_id/85052098750,"Big data-driven ensemble learning is explored in this paper for quantitative geological lithofacies modeling, which is an integral and challenging part of petroleum reservoir development and characterization. Quantitative lithofacies modeling involves detection and recognition of underlying subsurface rock’s lithofacies. It requires real-time data acquisition, handling, storage, conditioning, analysis, and interpretation of raw sensory petroleum logging data. The real-time well-logs data collected from the sensor-based tools suffer from complications such as noise, nonlinearity, imbalance, and high-dimensionality which makes the prediction task more challenging. The existing literature on quantitative lithofacies modeling includes several data-driven techniques ranging from conventional well-logs to artificial intelligence (AI). Recently, multiple classifiers based Ensemble learners have been found to be more robust and reliable paradigms for detection and identification tasks in various machine learning applications, however, these are not well embraced in the petroleum industry. Ensemble methodology combines diverse expert’s opinions to obtain overall ensemble decision which in turn reduces the risk of a wrong decision. Thus, the uncertainties associated with complex reservoir data can be better handled by the use of Ensemble learners than the existing single learner based conventional models. Ensemble-based big data analytics, proposed in the paper, includes development and comparative performance testing of five popular ensemble methods (viz. Bagging, AdaBoost, Rotation forest, Random subspace, and DECORATE) for quantitative lithofacies modeling. Seven state-of-the-art base classifiers were used as members of different Ensemble learners for the analysis of Kansas (U.S.A.) oil-field data. The proposed techniques have been implemented on the widely used WEKA platform. The comparative performance analysis of the proposed techniques, presented in the paper, confirms its supremacy over the existing techniques used for quantitative lithofacies modeling.",industry
10.1016/j.matpr.2020.03.363,Conference Proceeding,Materials Today: Proceedings,scopus,2019-01-01,sciencedirect,Real-time Thermal Error Compensation Strategy for Precision Machine tools,https://api.elsevier.com/content/abstract/scopus_id/85085555603,"Present manufacturing trend is towards producing precision components with better accuracy. Machine errors like geometrical, thermal and process errors affect the component accuracy. Among these errors, thermal error contributes more than 50-60% of the total machining error. This paper mainly focuses on the development of a real-time thermal error compensation module for precision machine tools and talks about effective modeling of thermal errors, development of thermal error compensation model using feed-forward backpropagation neural network and also simplified model using regression analysis technique, algorithm development for real-time compensation and implementation of module onto the open architecture CNC controller. The developed module has been successfully tested on a Diamond Turning Machine (DTM) by machining the precision component and also verified the effectiveness of the module",industry
10.1016/j.promfg.2020.01.033,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Deep learning-based production forecasting in manufacturing: A packaging equipment case study,https://api.elsevier.com/content/abstract/scopus_id/85083533827,"We propose a Deep Learning (DL)-based approach for production performance forecasting in fresh products packaging. On the one hand, this is a very demanding scenario where high throughput is mandatory; on the other, due to strict hygiene requirements, unexpected downtime caused by packaging machines can lead to huge product waste. Thus, our aim is predicting future values of key performance indexes such as Machine Mechanical Efficiency (MME) and Overall Equipment Effectiveness (OEE). We address this problem by leveraging DL-based approaches and historical production performance data related to measurements, warnings and alarms. Different architectures and prediction horizons are analyzed and compared to identify the most robust and effective solutions. We provide experimental results on a real industrial case, showing advantages with respect to current policies implemented by the industrial partner both in terms of forecasting accuracy and maintenance costs. The proposed architecture is shown to be effective on a real case study and it enables the development of predictive services in the area of Predictive Maintenance and Quality Monitoring for packaging equipment providers.",industry
10.1016/j.promfg.2020.01.031,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,A deep learning approach for anomaly detection with industrial time series data: A refrigerators manufacturing case study,https://api.elsevier.com/content/abstract/scopus_id/85083532061,"In refrigerators production, vacuum creation is fundamental to guarantee the correct manufacturing of the product. Before inserting the refrigerant in the refrigerator cabinet, the vacuum is tested through a Pirani gauge that assesses the pressure within the cabinet. Such readings are used to evaluate the vacuum creation process and to verify if leakings are present. In this work, we employ a Deep Learning-based Anomaly Detection approach to associate an Anomaly Score to each pressure profile; this score can be exploited to optimize actions performed by human operators like more detailed inspections or unit exclusion from the downstream production stages. We propose a native time series-based approach based on Deep Learning and compare it with classic ones based on hand-craft features. The proposed approach is designed to be deployed in a Decision Support System for assisting human operators in the following testing operations, helping them in reducing evaluation bias and attention losses that are inevitable in production line environment. Moreover, costs associated with false positives (normally operating units detected as anomalous) and false negatives (undetected anomalies) are considered here to optimize decision making in a cost-reduction perspective. We also describe promising results obtained on real industrial data spanning on a 5-month period and consisting of thousands of tested household units.",industry
10.1016/j.promfg.2020.01.333,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Prognostic health management of production systems. New proposed approach and experimental evidences,https://api.elsevier.com/content/abstract/scopus_id/85082764769,"Prognostic Health Management (PHM) is a maintenance policy aimed at predicting the occurrence of a failure in components and consequently minimizing unexpected downtimes of complex systems. Recent developments in condition monitoring (CM) techniques and Artificial Intelligence (AI) tools enabled the collection of a huge amount of data in real-time and its transformation into meaningful information that will support the maintenance decision-making process. The emerging Cyber-Physical Systems (CPS) technologies connect distributed physical systems with their virtual representations in the cyber computational world. The PHM assumes a key role in the implementation of CPS in manufacturing contexts, since it allows to keep CPS and its machines in proper conditions. On the other hand, CPS-based PHM provide an efficient solution to maximize availability of machines and production systems. In this paper, evolving and unsupervised approaches for the implementation of PHM at a component level are described, which are able to process streaming data in real-time and with almost-zero prior knowledge about the monitored component. A case study from a real industrial context is presented. Different unsupervised and online anomaly detection methods are combined with evolving clustering models in order to detect anomalous behaviours in streaming vibration data and integrate the so-generated knowledge into supervised and adaptive models; then, the degradation model for each identified fault is built and the resulting RUL prediction model integrated into the online analysis. Supervised methods are applied to the same dataset, in batch mode, to validate the proposed procedure.",industry
10.1016/B978-0-12-409547-2.00449-2,Book,Encyclopedia of Analytical Science,scopus,2019-01-01,sciencedirect,Quality assurance | laboratory information management systems,https://api.elsevier.com/content/abstract/scopus_id/85079080236,"In today’s competitive laboratory environment, managers are under increased pressure to deliver high quality data, quickly, and cost effectively, with limited resources. This can only be achieved via laboratory automation. It is more critical than ever to ensure that the modern laboratory is equipped with the latest software Laboratory Information Management Software (LIMS) and tools together with automation technology to ensure that they remain competitive. LIMS together with laboratory automation (positive ID, Robotics, AI, etc.) imparts many benefits that include time savings, resource maximization, efficiency, quality improvements, along with cost reductions. For all businesses that rely on delivering high quality, reliable products, regardless of industry, defects can be responsible for huge losses, from laboratory/company reputation to associated costs of recalls and possibly lawsuits. Compared to manual procedures, automated tasks offer significant benefits which include, reproducibility, increased accuracy, speed (high throughput), enhanced communication, increased responsiveness, automated and effective reporting that results in higher customer satisfaction. Today, LIMS can be delivered on-demand via the Software as a Service (SaaS) model for organizations that either do not have the infrastructure to host the software or who find it more cost effective to utilize the tools hosted in the cloud, eliminating the need for an IT resource. Organizations that employ best practices and implement LIMS have all of their laboratory operational data in a centralized, secure database greatly facilitating access to real-time data, KPIs, document control, quality management as well as regulatory compliance.",industry
10.1016/j.procir.2019.05.017,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,The growing path in search of an industrial design identity,https://api.elsevier.com/content/abstract/scopus_id/85076752868,"Knowing that the education system must be reinvented periodically to face the changes of social and cultural paradigm, was reviewed the pedagogical organization of a set of disciplines of an industrial design course that were in operation for a decade. Thus, in view of the objective of restructuring the disciplinary group of industrial design, a new structure has been developed and implemented that could offer students the opportunity to explore problems and challenges that have real applications, increasing the possibility of acquiring competences effectively needed to practice the profession of designer.
                  This restructuring had as its starting point the concept of Project-based learning, which is designated as student-centered pedagogy that involves a dynamic classroom approach in which it is believed that students acquire a deeper knowledge through active exploration of real-world challenges and problems. Consequently, resulting in a learning process organized into levels with increasing degree of complexity. As well, different assimilations of markets and design scenarios.
                  Starting from the first year of the course, where students are still understanding the context of industrial design and its potentialities. At a time when their techniques, principles and methods are still very raw and basic. They are initiated in a LOW-ID and local industry context, to acquire basic skills. The second year allows embark on an intermediate level called MID-ID, with new skills in international brands approach. In the last year of the course the 3rd level is reached, HIGH-ID, with projects with the national industry.
                  The first year of implementation of this curriculum structure showed good results. Thus, favoring a solid interdisciplinary formation with, skills and competences that allow future designers to intervene creatively and competently in a variety of fields. This process allows to progress to the next academic degree to complete and validate the entire formation of the student.",industry
10.1016/j.procir.2019.03.212,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,Contribution to the development of a Digital Twin based on product lifecycle to support the manufacturing process,https://api.elsevier.com/content/abstract/scopus_id/85076726437,"The current manufacture challenges are closely linked to the aim of digitalizing the product, the process and the means of production. In such aspects, information about the production processes is available in real-time, allowing managers to act on digital models and, through them, apply decisions in real systems. Thus, having a mirror model or a Digital Twin enables real-time absorption, simulation and implementation of manufacturing variations from the real environment, allowing faster detection of physical problems, and faster production response. The Digital Twin is a virtual representation of the physical system, which is equipped with sensors and actuators and feed the digital system, where the monitoring of data and simulation of variations, for instance, take place. From the synchronized interactions of both components, it is possible to deliver the mentioned faster production responses. Brazilian and German universities joined efforts to develop a Digital Twin based on product lifecycle to support the Manufacturing Process to address these challenges. The proposed Digital Twin seeks to integrate the product twin and the twin of its development process. It shall represent the manufacturing process, enabling the monitoring and optimization of the real production process. The Digital Twin itself is addressed as a product inside the production system and, therefore, its development process will follow the product lifecycle perspective, from the conception and planning to its implementation and usage. The Digital Twin will be further improved with the introduction of Artificial Intelligence tools, characterizing a Smart Digital Twin of the Manufacturing Process. Thus, this paper aims to present the concepts of a research project that is being developed in a joint Brazilian-German Cooperative Research.",industry
10.1016/j.ifacol.2019.09.143,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,Machine Learning approaches for Anomaly Detection in Multiphase Flow Meters,https://api.elsevier.com/content/abstract/scopus_id/85076262725,"Multiphase Flow Meters (MPFM) are important metering tools in the oil and gas industry. A MPFM provides real-time measurements of gas, oil and water flows of a well without the need to separate the phases, a time-consuming procedure that has been classically adopted in the industry. Evaluating the composition of the flow is fundamental for the well management and productivity prediction; therefore, procedures for measuring quality assessment are of crucial importance. In this work we propose an Anomaly Detection approach to MPFM that is effectively able to hand the complexity and variability associated with MPFM data. The proposed approach is designed for embedded implementation and it exploits unsupervised Anomaly Detection approaches like Cluster Based Local Outlier Factor and Isolation Forest.",industry
10.1016/j.ifacol.2019.08.225,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,Curriculum change for graduate-level control engineering education at the Universidad Pontificia Bolivariana,https://api.elsevier.com/content/abstract/scopus_id/85076258553,"This paper addresses the graduate-level control engineering curriculum change performed at the Universidad Pontificia Bolivariana (UPB), Medellin, Colombia. New proposed methodologies include active learning activities using a new multipurpose experimental test bed that was developed with industrial components. The renovated graduate-level control engineering related courses include: Continuous Processes, Discrete Processes, Fuzzy Logic, Neural Networks and Genetic Algorithms, Linear Control, Nonlinear Control, and Optimal Estimation. The new experimental station was developed for teaching, research, and industrial training activities for the School of Engineering at the UPB. In this work, we report the use of the station in an Optimal Estimation course to replace a traditional homework/exams evaluation approach with an applied work that required independent study, the implementation of different observers in a real lab-scale industrial plant, and a paper-style written report. Increasing independent study activities resulted in academic discussions that are valuable for the learning process of the student. The use of the experimental station and the real comparison of estimation algorithms, implemented by using industrial controllers and high-level programming environments, provided the student skills that cannot be acquired by using only simulations in which real implementation restrictions/challenges do not appear. This work represents one of the first approaches for the implementation of the new curriculum model at the UPB for graduate education. The methodology used in the Optimal Estimation class promoted independent learning, critical thinking and writing skills through significant learning activities.",industry
10.1016/j.procs.2019.09.169,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,IAssistMe - Adaptable assistant for persons with eye disabilities,https://api.elsevier.com/content/abstract/scopus_id/85076257910,"Visually challenged people may experience certain difficulties in their daily interaction with technology. That is essentially because the main way to exchange and process information is by written text, images or videos. Since the basic purpose of innovation is to improve people’s lifestyle, in this paper we propose a system that can make technology accessible to a broader group. Our prototype is presented as a mobile application based on vocal interaction, which can help people facing visual disorders consult their personal agenda, create an event, invite other friends to attend it, check the weather in certain areas and many other day-to-day tasks. Regarding the implementation, the project consists of a mobile application that interacts with a cloud based system, which makes it reliable and low in latency due to the resource availability in multiple global regions, provided by the newly emerging platform used in building the infrastructure. The novelty of the system lays in the highly flexible serverless architecture [1] that is open to extension and closed to modification through the set of autonomous cloud processing methods that sustain the base of the functionality. This distributed processing approach guarantees that the user always receives a response from his personal assistant, either by using artificial intelligence context generated phrases, by real-time cloud function processing or by fallback to the training answers.",industry
10.1016/j.procs.2019.09.069,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,An Innovative Technology: Augmented Reality Based Information Systems,https://api.elsevier.com/content/abstract/scopus_id/85076255225,"In our generation the information systems evolve with new technologies: augmented reality (AR), IoT, artificial intelligence, blockchain etc. Anymore they perform information exchange by sensors. It is estimated that the systems will be in a state of extreme interaction and reach 50 billion devices connected in Internet in 2020. We know that everything around us will be in interaction and they will do everything without any need of human interference. For example, when our dishwasher is full, it will start to wash automatically, or when the run out of the gasoline, our car will drive to the nearest station, or even when a burglar is entered to our house, it will automatically be detected and be announced to the police office. In business life, the processes will be automatical in maximum level and this technology will increase productivity and efficiency. Next to mobile technology, it is thought that these new generation information systems (IS) will take the biggest place in our lives. AR also will be integrated to these systems to augment the information in real world. Humanity will augment its habitat in an innovative way thanks to these AR based IS. This paper surveys the current state-of-the-art AR systems related with aerospace & defense, industry, education, medical and gaming sectors. The connection of AR based IS and innovation is explained with a technological insight. In addition to international use cases HAVELSAN’s use cases are also given that are performed from the aspect of applied open innovation strategy. This strategy is addressed specific to the implemented activities of AR based IS.",industry
10.1016/j.promfg.2018.12.017,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,AI based injection molding process for consistent product quality,https://api.elsevier.com/content/abstract/scopus_id/85072584818,"In manufacturing processes, Injection Molding is widely used for producing plastic components with large lot size. So, continuous improvements in product quality consistency is crucial to maintaining a competitive edge in the injection molding industry. Various optimization techniques like ANN, GA, Iterative method, and simulation based are being used for optimization of Injection Molding process and obtaining optimal processing conditions. But still due to variation during molding cycles, quality failure occurs. As many constituents like process, Material, machine together yields product quality. This paper is focused on Real time AI based control of process parameters in injection molding cycle. Process parameters and their interrelationship with quality failure has been studied and later supposed to be used to generate algorithm for compensating the deviation of process parameters. Pressure and temperature sensor assisted monitoring system is used to collect data in real time and based on its comparison with the standard values an interrelationship is formed between parameters and plastic material properties. Algorithm generates new process parameter values to compensate the deviation and machine control follows the same. The entire process is supposed to be smart and automatic after being trained with AI and machine learning techniques. Simulation using Moldflow software and real industry collected data has been used for understanding whole molding process establishing relationship between failure and parameters. An automotive product in real industry is chosen for data acquisition, implementation and validation of entire AI based system.",industry
10.1016/j.promfg.2018.12.026,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Hybrid artificial intelligence system for the design of highly-automated production systems,https://api.elsevier.com/content/abstract/scopus_id/85072561400,"The automated design of production systems is a young field of research which has not been widely explored by industry nor research in recent decades. Currently, the effort spent in production system design is increasing significantly in automotive industry due to the number of product variants and product complexity. Intelligent methods can support engineers in repetitive tasks and give them more opportunity to focus on work which requires their core competencies. This paper presents a novel artificial intelligence methodology that automatically generates initial production system configurations based on real industrial scenarios in the automotive field of body-in-white production. The hybrid methodology reacts flexibly against data sets of different content and has been implemented in a software prototype.",industry
10.1016/j.procs.2019.04.090,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,Deep neural network method of recognizing the critical situations for transport systems by video images,https://api.elsevier.com/content/abstract/scopus_id/85071926362,"The deep neural network method of recognizing critical situations for transport systems according to video frames from the intelligent vehicles cameras is offered, that is effective in terms of accuracy and high-speed performance. Unlike the known solutions for the objects and normal or critical situations detection and recognition, it uses the classification with the subsequent reinforcement on the basis of several video stream frames and with the automatic annotation algorithm. The adapted architectures of neural networks are offered: the dual network to identify drivers and passengers according to the face image, the network with independent recurrent layers to classify situations according to the video fragment. The scheme of the intellectual distributed city system of transport safety using the cameras and on-board computers united in a single network is offered. Software modules in Python are developed and natural experiments are made. The possibility of the offered algorithms and programs in UGV or in the driver assistant systems implementation is shown with the illustrating examples in real-time.",industry
10.1016/j.promfg.2020.01.288,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Action recognition in manufacturing assembly using multimodal sensor fusion,https://api.elsevier.com/content/abstract/scopus_id/85070765380,"Production innovations are occurring faster than ever. Manufacturing workers thus need to frequently learn new methods and skills. In fast changing, largely uncertain production systems, manufacturers with the ability to comprehend workers’ behavior and assess their operation performance in near real-time will achieve better performance than peers. Action recognition can serve this purpose. Despite that human action recognition has been an active field of study in machine learning, limited work has been done for recognizing worker actions in performing manufacturing tasks that involve complex, intricate operations. Using data captured by one sensor or a single type of sensor to recognize those actions lacks reliability. The limitation can be surpassed by sensor fusion at data, feature, and decision levels. This paper presents a study that developed a multimodal sensor system and used sensor fusion methods to enhance the reliability of action recognition. One step in assembling a Bukito 3D printer, which composed of a sequence of 7 actions, was used to illustrate and assess the proposed method. Two wearable sensors namely Myo-armband captured both Inertial Measurement Unit (IMU) and electromyography (EMG) signals of assembly workers. Microsoft Kinect, a vision based sensor, simultaneously tracked predefined skeleton joints of them. The collected IMU, EMG, and skeleton data were respectively used to train five individual Convolutional Neural Network (CNN) models. Then, various fusion methods were implemented to integrate the prediction results of independent models to yield the final prediction. Reasons for achieving better performance using sensor fusion were identified from this study.",industry
10.1016/j.procir.2019.03.041,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,"Design, implementation and evaluation of reinforcement learning for an adaptive order dispatching in job shop manufacturing systems",https://api.elsevier.com/content/abstract/scopus_id/85068485505,"Modern production systems tend to have smaller batch sizes, a larger product variety and more complex material flow systems. Since a human oftentimes can no longer act in a sufficient manner as a decision maker under these circumstances, the demand for efficient and adaptive control systems is rising. This paper introduces a methodical approach as well as guideline for the design, implementation and evaluation of Reinforcement Learning (RL) algorithms for an adaptive order dispatching. Thereby, it addresses production engineers willing to apply RL. Moreover, a real-world use case shows the successful application of the method and remarkable results supporting real-time decision-making. These findings comprehensively illustrate and extend the knowledge on RL.",industry
10.1016/j.promfg.2019.03.047,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,A Practical Approach of Teaching Digitalization and Safety Strategies in Cyber-Physical Production Systems,https://api.elsevier.com/content/abstract/scopus_id/85065658005,"Digitalization strategies in cyber-physical production systems (CPPS) are one of the key factors of Industry 4.0. The topic not only addresses data preparation, real-time data processing, big data analytics, visualization and machine interface design but also cyber security and safety. Especially, unauthorized access to protected (personal or enterprise) data or unauthorized control of production facilities imply risks when it comes to digitalization. Because of the increased complexity of state-of-the-art technologies, educational institutions need to provide practice-oriented teaching methods in learning factories to help engineers of today understand the impact of those developments.
                  In the light of this fact, this paper presents a practical approach of teaching digitalization strategies in CPPS. Planning, implementing and impacts of digitalization strategies are taught on a use-case with human-robot-collaboration. The objective of the use-case is to realize a real-time obstacle avoidance approach for a collaborative application based on a local positioning system. Here, students not only learn how to model the kinematics of a robot and program a robot but also how to design machine interfaces for real-time data transfer and processing as well as impacts of digitalization on safety and security.
                  The implementation of the use-case is part of the TU Wien teaching portfolio and thus part of its learning factory, where students and apprentices have the possibility to experiment and gain experiences by deliberate error simulations.",industry
10.1016/j.procir.2019.02.101,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,Autonomous order dispatching in the semiconductor industry using reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85065424368,"Cyber Physical Production Systems (CPPS) provide a huge amount of data. Simultaneously, operational decisions are getting ever more complex due to smaller batch sizes, a larger product variety and complex processes in production systems. Production engineers struggle to utilize the recorded data to optimize production processes effectively because of a rising level of complexity. This paper shows the successful implementation of an autonomous order dispatching system that is based on a Reinforcement Learning (RL) algorithm. The real-world use case in the semiconductor industry is a highly suitable example of a cyber physical and digitized production system.",industry
10.1016/j.cirpj.2018.12.002,Journal,CIRP Journal of Manufacturing Science and Technology,scopus,2019-01-01,sciencedirect,"From factory floor to process models: A data gathering approach to generate, transform, and visualize manufacturing processes",https://api.elsevier.com/content/abstract/scopus_id/85058703955,"The need for tools to help guide decision making is growing within the manufacturing industry. The analysis performed by these tools will help operators and engineers to understand the behaviour of the manufacturing stations better and thereby take data-driven decisions to improve them. The tools use techniques borrowed from fields such as Data Analytics, BigData, Predictive Modelling, and Machine Learning. However, to be able to use these tools efficiently, data from the factory floor is required as input. This data needs to be extracted from two sources, the PLCs, and the robots. In practice, methods to extract usable data from robots are rather scarce. The present work describes an approach to capture data from robots, which can be applied to both legacy and current state-of-the-art manufacturing systems. The described approach is developed using Sequence Planner – a tool for modelling and analyzing production systems – and is currently implemented at an automotive company as a pilot project to visualize and examine the ongoing process. By exploiting the robot code structure, robot actions are converted to event streams that are abstracted into operations. We then demonstrate the applicability of the resulting operations, by visualizing the ongoing process in real-time as Gantt charts, that support the operators performing maintenance. And, the data is also analyzed off-line using process mining techniques to create a general model that describes the underlying behaviour existing in the manufacturing station. Such models are used to derive insights about relationships between different operations, and also between resources.",industry
10.1016/j.impact.2018.12.001,Journal,NanoImpact,scopus,2019-01-01,sciencedirect,SUNDS probabilistic human health risk assessment methodology and its application to organic pigment used in the automotive industry,https://api.elsevier.com/content/abstract/scopus_id/85058641247,"The increasing use of engineered nanomaterials (ENMs) in nano-enabled products (NEPs) has raised societal concerns about their possible health and ecological implications. To ensure a high level of human and environmental protection it is essential to properly estimate the risks of these new materials and to develop adequate risk management strategies. To this end, we propose a quantitative Human Health Risk Assessment (HHRA) methodology, which was developed in the European Seventh Framework research project SUN (Sustainable Nanotechnologies) and implemented in the web-based SUN Decision Support System (SUNDS). One of the major strengths of this probabilistic approach as compared to its deterministic alternatives is its ability to clearly communicate the uncertainties in the estimated risks in order to support better risk communication for more objective decision making by industries and regulators.
                  To demonstrate this methodology, we applied it in a real case study involving a nanoscale organic red pigment used in the automotive industry. Our analysis clearly showed that the main source of uncertainty was the extrapolation from (sub)acute in vivo toxicity data to long-term risk. This extrapolation was necessary due to a lack of (sub)chronic in vivo studies for the investigated nanomaterial. Despite the high uncertainty in the final results due to the conservative assumptions made in the risks assessment, the estimated risks are acceptable for all investigated exposure scenarios along the product lifecycle.",industry
10.1016/j.ijepes.2018.07.022,Journal,International Journal of Electrical Power and Energy Systems,scopus,2019-01-01,sciencedirect,Design and implementation of flexible Numerical Overcurrent Relay on FPGA,https://api.elsevier.com/content/abstract/scopus_id/85050986246,"This paper presents the contemporary design and implementation of an intelligent revelation in the field of the over-current relay to meet the challenges of the modern grid. The unique three-neuron single layered architecture of Artificial Neural Network (ANN) provides flexibility by exploiting its universal function approximation capabilities. The Unique Selling Proposition (USP) of the present development is the simple design of ANN, suitable for low-end, low-cost Field Programmable Gate Array (FPGA) implementation. The nano-scaled internal processing time for three-phase design, with the provision of remotely controlled adaptive relay settings, would definitely an innovative solution for grid connection of renewable energy sources. The proposed design of the universal over-current relay, confirmed by the real-time testing, is a true fusion of electrical power, communication and information technology to meet the global trend of the electrical power industries.",industry
10.1016/j.powtec.2018.08.064,Journal,Powder Technology,scopus,2018-11-01,sciencedirect,"Settling velocity of drill cuttings in drilling fluids: A review of experimental, numerical simulations and artificial intelligence studies",https://api.elsevier.com/content/abstract/scopus_id/85052516468,"In this paper, a comprehensive review of experimental, numerical and artificial intelligence studies on the subject of cuttings settling velocity in drilling muds made by researchers over the last seven decades is brought to the fore. In this respect, 91 experimental, 13 numerical simulations and 7 artificial intelligence researches were isolated, reviewed, tabulated and discussed. A comparison of the three methods and the challenges facing each of these methods were also reviewed. The major outcomes of this review include: (1) the unanimity among experimental researchers that mud rheology, particle size and shape and wall effect are major parameters affecting the settling velocity of cuttings in wellbores; (2) the prevalence of cuttings settling velocity experiments done with the mud in static conditions and the wellbore in the vertical configuration; (3) the extensive use of rigid particles of spherical shape to represent drill cuttings due to their usefulness in experimental visualization, particle tracking, and numerical implementation; (4) the existence of an artificial intelligence technique - multi-gene genetic programming (MGGP) which can provide an explicit equation that can help in predicting settling velocity; (5) the limited number of experimental studies factoring in the effect of pipe rotation and well inclination effects on the settling velocity of cuttings and (6) the most applied numerical method for determining settling velocity is the finite element method. Despite these facts, there is need to perform more experiments with real drill cuttings and factor in the effects of conditions such as drillstring rotation and well inclination and use data emanating therefrom to develop explicit models that would include the effects of these. It should be noted however, that the aim of this paper is not to create an encyclopaedia of particle settling velocity research, but to provide to the researcher with a basic, theoretical, experimental and numerical overview of what has so far been achieved in the area of cuttings settling velocity in drilling muds.",industry
10.1016/j.petrol.2018.06.072,Journal,Journal of Petroleum Science and Engineering,scopus,2018-11-01,sciencedirect,Data driven model for sonic well log prediction,https://api.elsevier.com/content/abstract/scopus_id/85050476760,"Near wellbore failure during the exploration of hydrocarbon reservoirs presents a serious concern to the oil and gas industry. To predict the probability of these undesirable phenomena, engineers study the mechanical rock properties of the formation such as Young's modulus, Bulk modulus, shear modulus and Poisson's ratio. Conventionally, these are measured indirectly using the established petro physical relationships from sonic wave velocities which can be obtained from sonic well logs. Unfortunately, reliable sonic well logs are not always available, due to poor borehole conditions (wash out), damaged tools and offset well data. Most offset well log data are not acquired with dipole sonic tools; they are acquired with a borehole compensated logging tool. This limits the application of acoustic measurements to estimate the mechanical rock properties.
                  In this study, a three-layer feedforward multilayered perceptron artificial neural network model is presented. This model aims to estimate compressional wave transit time and shear wave transit time using real gamma ray and formation density logs. The validation of the model is confirmed by using an oil and gas offshore shaley sandstone reservoir located in West Africa. The results of the validation show that the model presented in this study can be used to determine the sanding potential of the formation without performing a compressive geoscientific analysis in the absence of sonic well logs. The developed model's effectiveness is tested by comparing the predicted results with results obtained from the measured well log. The paper provides a tool to give preliminary recommendations of the likelihood of the formation to produce sand. Implementation of the proposed model can serve as a cost-effective and reliable alternative for the oil and gas industry.",industry
10.1016/j.measurement.2018.05.099,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2018-11-01,sciencedirect,Parallel three-dimensional electrical capacitance data imaging using a nonlinear inversion algorithm and L<sup>p</sup> norm-based model regularization,https://api.elsevier.com/content/abstract/scopus_id/85049483981,"In order to improve image reconstructions, different classes of nonlinear inversion algorithms are developed and used in different research topics like imaging processes in oil industry or the characterization of complex porous media or multiphase flows. These algorithms are able to avoid local minima and to reach more adapted minima of a given misfit function between observed/measured and computed data. Techniques as different as electrical, ultrasound or potential methods, are used. We present here a nonlinear algorithm that allows us to produce permittivity images by using electrical capacitance tomography (ECT). ECT is a non-invasive technique to image non-conductive permittivity distributions and is used in many oil industry imaging applications such as multiphase flows in pipelines, fluidized bed reactors, mixing vessels, and tanks of phase separation. Even if the ECT technique provides low resolution reconstructions, it is cheap, robust and very fast when compared to other imaging tools. In this method one or more rings of electrodes excite a medium to be imaged at high frequencies, and more particularly at frequencies for which a static electrical potential field has fully developed. In many studies of other research groups only one ring of sources is introduced but the reconstruction accuracy was not totally satisfactory due to the 3D nature of the problem to be solved. Instead of using nonlinear stochastic algorithms like the simulated annealing (SA) technique that we optimized in previous studies to image permittivity distributions of granular or solid materials as well as real oil–gas or two-phase flows in 2D cylindrical vessel configurations, we propose here a new ECT inversion tool to image permittivities in a 3D cylindrical configuration. 3D stochastic optimization methods such as SA, neural networks, genetic algorithms can become computationally too prohibitive, and classical local or linear inversion methods excessively smooth images in many cases. Therefore, we propose here a 3D parallel inversion procedure with different numbers of rings and different 
                        
                           
                              
                                 L
                              
                              
                                 p
                              
                           
                        
                      norms, with
                        
                           1
                           <
                           p
                           ⩽
                           2
                        
                     , applied to the model regularization of the misfit function to increase the resolution of the models after inversion. We are able to better reconstruct two-phase and three-phase (oil, gas and solids) mixtures by combining 
                        
                           
                              
                                 L
                              
                              
                                 p
                              
                           
                        
                     -norm regularizations of the misfit function to minimize and several rings of electrodes. All these algorithms have been implemented in a more general parallel framework TOMOFAST-X designed for multi-physics joint inversion purposes, and could also be used in other fields of research such as larger-scale geophysical exploration for instance.",industry
10.1016/j.ssci.2018.06.012,Journal,Safety Science,scopus,2018-11-01,sciencedirect,Occupational health and safety in the industry 4.0 era: A cause for major concern?,https://api.elsevier.com/content/abstract/scopus_id/85049323662,"Real-time communication, Big Data, human–machine cooperation, remote sensing, monitoring and process control, autonomous equipment and interconnectivity are becoming major assets in modern industry. As the fourth industrial revolution or Industry 4.0 becomes the predominant reality, it will bring new paradigm shifts, which will have an impact on the management of occupational health and safety (OHS).
                  In the midst of this new and accelerating industrial trend, are we giving due consideration to changes in OHS imperatives? Are the OHS consequences of Industry 4.0 being evaluated properly? Do we stand to lose any of the gains made through proactive approaches? Are there rational grounds for major concerns? In this article, we examine these questions in order to raise consciousness with regard to the integration of OHS into Industry4.0.
                  It is clear that if the technologies driving Industry 4.0 develop in silos and manufacturers’ initiatives are isolated and fragmented, the dangers will multiply and the net impact on OHS will be negative. As major changes are implemented, previous gains in preventive management of workplace health and safety will be at risk. If we are to avoid putting technological progress and OHS on a collision course, researchers, field experts and industrialists will have to collaborate on a smooth transition towards Industry 4.0.",industry
10.1016/j.vetmic.2018.08.026,Journal,Veterinary Microbiology,scopus,2018-10-01,sciencedirect,Detection of non-notifiable H4N6 avian influenza virus in poultry in Great Britain,https://api.elsevier.com/content/abstract/scopus_id/85053845094,"A 12-month pilot project for notifiable avian disease (NAD) exclusion testing in chicken and turkey flocks in Great Britain (GB) offered, in partnership with industry, opportunities to carry out differential diagnosis in flocks where NAD was not suspected, and to identify undetected or undiagnosed infections. In May 2014, clinical samples received from a broiler breeder chicken premises that had been experiencing health and production problems for approximately one week tested positive by avian influenza (AI) real-time reverse transcription polymerase chain reaction (RRT-PCR). Following immediate escalation to an official, statutory investigation to rule out the presence of notifiable AI virus (AIV; H5 or H7 subtypes), a non-notifiable H4N6 low pathogenicity (LP) AIV was detected through virus isolation in embryonated specific pathogen free (SPF) fowls’ eggs, neuraminidase inhibition test, cleavage site sequencing and AIV subtype H4-specific serology. Premises movement restrictions were lifted, and no further disease control measures were implemented as per the United Kingdom (UK) legislation. Phylogenetic analysis of the haemagglutinin and neuraminidase genes of the virus revealed closest relationships to viruses from Mallard ducks in Sweden during 2007 and 2009. In June 2014, clinical suspicion of NAD was reported in a flock of free-range laying chickens elsewhere in GB, due to increasing daily mortality and reduced egg production over a five-day period. An H4N6 LPAIV with an intravenous pathogenicity index of 0.50 was isolated. This virus was genetically highly similar, but not identical, to the virus detected during May 2014. Full viral genome analyses showed characteristics of a strain that had not recently transferred from wild birds, implying spread within the poultry sector had occurred. A stalk deletion in the neuraminidase gene sequence indicated an adaptation of the virus to poultry. Furthermore, there was unexpected evidence of systemic spread of the virus on post-mortem. No other cases were reported. Infection with LPAIVs often result in variable clinical presentation in poultry, making detection of disease more difficult.",industry
10.1016/j.mfglet.2018.09.002,Journal,Manufacturing Letters,scopus,2018-10-01,sciencedirect,Industrial Artificial Intelligence for industry 4.0-based manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85053749537,"The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.",industry
10.1016/j.compind.2018.07.004,Journal,Computers in Industry,scopus,2018-10-01,sciencedirect,IDARTS – Towards intelligent data analysis and real-time supervision for industry 4.0,https://api.elsevier.com/content/abstract/scopus_id/85050319341,"The manufacturing industry represents a data rich environment, in which larger and larger volumes of data are constantly being generated by its processes. However, only a relatively small portion of it is actually taken advantage of by manufacturers. As such, the proposed Intelligent Data Analysis and Real-Time Supervision (IDARTS) framework presents the guidelines for the implementation of scalable, flexible and pluggable data analysis and real-time supervision systems for manufacturing environments. IDARTS is aligned with the current Industry 4.0 trend, being aimed at allowing manufacturers to translate their data into a business advantage through the integration of a Cyber-Physical System at the edge with cloud computing. It combines distributed data acquisition, machine learning and run-time reasoning to assist in fields such as predictive maintenance and quality control, reducing the impact of disruptive events in production.",industry
10.1016/j.cie.2018.07.016,Journal,Computers and Industrial Engineering,scopus,2018-10-01,sciencedirect,New decision support system for strategic planning in process industries: Computational results,https://api.elsevier.com/content/abstract/scopus_id/85049776857,"The impact of a Stochastic Linear Programming (SLP) based Decision Support System in a manufacturing company, such as an integrated aluminum plant, is measured by two important parameters, the VSS and EVPI. With the real data of an integrated steel plant in India, we demonstrate that SLP based DSS can be very effective in managing demand uncertainty and performing futuristic integrated planning, and their financial impact can be in millions of dollars. A two stage stochastic programming model with recourse is implementedin the DSS here. A set of experiments is conducted. Real data from an aluminum company is used to validate the system. The importance of SLP based DSS can be realized from the fact that the value of the stochastic solution (VSS) is USD 3.58 million with 30% demand variability and equally likely demand distribution. The VSS as a percentage of Expectation of Expected Value (EEV) ranges from 0.90% to 18.93% across experiments.",industry
10.1016/j.jpdc.2018.04.005,Journal,Journal of Parallel and Distributed Computing,scopus,2018-10-01,sciencedirect,A malicious threat detection model for cloud assisted internet of things (CoT) based industrial control system (ICS) networks using deep belief network,https://api.elsevier.com/content/abstract/scopus_id/85047404138,"Internet of Things (IoT) devices are extensively used in modern industries combined with the conventional industrial control system (ICS) network through the industrial cloud to make the production data easily available to the corporate business management and easier control for highly profitable production systems. The different devices within the conventional ICS network originally manufactured to run on an isolated network and was not considered for the privacy and security of the control and production/architecture data being trafficked over the manufacturing plant to the corporate. Due to their extensive integration with the industrial cloud network over the internet, these ICS networks are exposed to a significant threat of malicious activities created by malicious software. Protecting ICS from such attacks requires continuous update of their database of anti-malware tools which requires efforts from manual experts on a regular basis. This limits real time protection of ICS.
                  Earlier work by Huda et al. (2017) based on a semi-supervised approach performed well. However training process of the semi-supervised-approach (Huda et al., 2017) is complex procedure which requires a hybridization of feature selection, unsupervised clustering and supervised training techniques. Therefore, it could be time consuming for ICS network for real time protection. In this paper, we propose an adaptive threat detection model for industrial cloud of things (CoT) based on deep learning. Deep learning has been used in many domain of pattern recognition and a popular approach for its simple training procedure. Most importantly, deep learning can learn the hidden patterns of the domain in an unsupervised manner which can avoid the requirements of huge expensive labeled data. We used this particular characteristic of deep learning to design our detection model.
                  Two different types of deep learning based detection models are proposed in this work. The first model uses a disjoint training and testing data for a deep belief network (DBN) and corresponding artificial neural network (ANN). In the second proposed detection model, DBN is trained using new unlabeled data to provide DBN with additional knowledge about the changes in the malicious attack patterns. Novelty of the proposed detection models is that the models are adaptive where training procedures is simpler than earlier work (Huda et al, 2017) and can adapt new malware behaviors from already available and cheap unlabeled data at the same time. This will avoid expensive manual labeling of new attacks and corresponding time complexity making it feasible for ICS networks. Performances of standard DBNs are sensitive to its configurations and values for the hyper-parameters including number of hidden nodes, learning rate and number epochs. Therefore proposed detection models find an optimal configuration by varying the structure of DBNs and other parameters. The proposed detection models are extensively tested on a real malware test bed. Experimental results show that the proposed approaches achieve higher accuracies than standard detection algorithms and obtain similar performances with earlier semi-supervised work (Huda et al., 2017) but provide a comparatively simplified training model.",industry
10.1016/j.apenergy.2018.06.040,Journal,Applied Energy,scopus,2018-09-15,sciencedirect,Optimal scheduling of a microgrid in a volatile electricity market environment: Portfolio optimization approach,https://api.elsevier.com/content/abstract/scopus_id/85048767400,"This paper proposes an optimal scheduling strategy for a microgrid participating in a volatile electricity market. The microgrid system includes photovoltaic generators, a wind turbine, a load, grid connection, and a battery storage system. An optimal microgrid operation is achieved by maximizing the utility function represented by the exponential rate of growth of the electricity market value through electricity transactions between the microgrid and main grid, on the premise of satisfying the power balance and generation limit of system components. The uncertainties occurring during the microgrid operation are represented by generator output, load demand, and electricity price fluctuation. The proposed strategy utilizes the Kelly Criterion, an optimal strategy that maximizes the growth rate of an asset’s net worth over repeated investments, coupled with an artificial neural network forecast of electricity price to deal with the volatile energy market. The proposed algorithm provides significant improvements in microgrid scheduling by eliminating the reliance on renewable generation and load forecasts, which makes it computationally inexpensive and thus feasible for real-time implementation. In representative case scenarios, using real-world tracers, we show that the algorithm has no dependency on meteorological forecasts and performs optimally in a volatile electricity market.",industry
10.1016/j.jisa.2018.05.002,Journal,Journal of Information Security and Applications,scopus,2018-08-01,sciencedirect,Identification of malicious activities in industrial internet of things based on deep learning models,https://api.elsevier.com/content/abstract/scopus_id/85047072990,"Internet Industrial Control Systems (IICSs) that connect technological appliances and services with physical systems have become a new direction of research as they face different types of cyber-attacks that threaten their success in providing continuous services to organizations. Such threats cause firms to suffer financial and reputational losses and the stealing of important information. Although Network Intrusion Detection Systems (NIDSs) have been proposed to protect against them, they have the difficult task of collecting information for use in developing an intelligent NIDS which can proficiently detect existing and new attacks. In order to address this challenge, this paper proposes an anomaly detection technique for IICSs based on deep learning models that can learn and validate using information collected from TCP/IP packets. It includes a consecutive training process executed using a deep auto-encoder and deep feedforward neural network architecture which is evaluated using two well-known network datasets, namely, the NSL-KDD and UNSW-NB15. As the experimental results demonstrate that this technique can achieve a higher detection rate and lower false positive rate than eight recently developed techniques, it could be implemented in real IICS environments.",industry
10.1016/j.neucom.2018.03.014,Journal,Neurocomputing,scopus,2018-06-14,sciencedirect,ACDIN: Bridging the gap between artificial and real bearing damages for bearing fault diagnosis,https://api.elsevier.com/content/abstract/scopus_id/85044327325,"Data-driven algorithms for bearing fault diagnosis have achieved much success. However, it is difficult and even impossible to collect enough data containing real bearing damages to train the classifiers, which hinders the application of these methods in industrial environments. One feasible way to address the problem is training the classifiers with data generated from artificial bearing damages instead of real ones. In this way, the problem changes to how to extract common features shared by both kinds of data because the differences between the artificial one and the natural one always baffle the learning machine. In this paper, a novel model, deep inception net with atrous convolution (ACDIN), is proposed to cope with the problem. The contribution of this paper is threefold. First and foremost, ACDIN improves the accuracy from 75% (best results of conventional data-driven methods) to 95% on diagnosing the real bearing faults when trained with only the data generated from artificial bearing damages. Second, ACDIN takes raw temporal signals as inputs, which means that it is pre-processing free. Last, feature visualization is used to analyze the mechanism behind the high performance of the proposed model.",industry
10.1016/j.enconman.2018.03.044,Journal,Energy Conversion and Management,scopus,2018-06-01,sciencedirect,Adaptive air-fuel ratio control of dual-injection engines under biofuel blends using extreme learning machine,https://api.elsevier.com/content/abstract/scopus_id/85044138298,"Dual-injection engines, which allow real-time control and injection of two different fuels, are capable of varying the ratio of biofuel blends at different engine operating conditions for optimal engine performance. However, while many experiments have been carried out on these engines to demonstrate their advantages, very few studies have focused on the corresponding air–fuel ratio (AFR) control strategy. In order to achieve stable engine operation, it is essential to maintain transient AFR during the change of fuel blend ratio. Therefore, this study proposes an adaptive controller for AFR control of dual-injection engines. The proposed controller is designed based on a recently developed machine learning method called extreme learning machine, and its stability is verified with Lyapunov analysis. Simulations have been performed on an industry-level engine simulation software to verify the controller. Since dual-injection engines are not available in the market, a spark-ignition engine has been retrofitted for dual-injection operation so that the proposed controller can be implemented and evaluated experimentally. Both simulation and experiment results show that the proposed controller can effectively regulate the AFR to desired level. The results also show that the proposed controller outperforms the engine built-in AFR controller, indicating its significance for dual-injection engines.",industry
10.1016/j.autcon.2018.01.003,Journal,Automation in Construction,scopus,2018-05-01,sciencedirect,Transfer learning and deep convolutional neural networks for safety guardrail detection in 2D images,https://api.elsevier.com/content/abstract/scopus_id/85041454603,"Safety has been a concern for the construction industry for decades. Unsafe conditions and behaviors are considered as the major causes of construction accidents. The current safety inspection of conditions and behaviors heavily rely on human efforts which are limited onsite. To improve the safety performance of the industry, a more efficient approach to identify the unsafe conditions on site is required to supplement the current manual inspection practice. A promising way to supplement the current manual safety inspection is automated and intelligent monitoring/inspection through information and sensing technologies, including localization techniques, environment monitoring, image processing and etc. To assess the potential benefits of contemporary technologies for onsite safety inspection, the authors focused on real-time guardrail detection, as unprotected edges are the ones cause for workers falling from heights.
                  In this paper, the authors developed a safety guardrail detection model based on convolutional neural network (CNN). An augmented data set is generated with the addition of background image to guardrail 3D models and used as training set. Transfer learning is utilized and the Visual Geometry Group architecture with 16 layers (VGG-16) model is adopted to construct the basic features extraction for the neural network. In the CNN implementation, 4000 augmented images were used to train the proposed model, while another 2000 images collected from real construction jobsites and 2000 images from Google were used to validate the proposed model. The proposed CNN-based guardrail detection model obtained a high accuracy of 96.5%. In addition, this study indicates that the synthetic images generated by augment technology can be used to create a large training dataset, and CNN-based image detection algorithm is a promising approach in construction jobsite safety monitoring.",industry
10.1016/j.neucom.2018.01.002,Journal,Neurocomputing,scopus,2018-04-12,sciencedirect,Robot manipulator control using neural networks: A survey,https://api.elsevier.com/content/abstract/scopus_id/85041636063,"Robot manipulators are playing increasingly significant roles in scientific researches and engineering applications in recent years. Using manipulators to save labors and increase accuracies are becoming common practices in industry. Neural networks, which feature high-speed parallel distributed processing, and can be readily implemented by hardware, have been recognized as a powerful tool for real-time processing and successfully applied widely in various control systems. Particularly, using neural networks for the control of robot manipulators have attracted much attention and various related schemes and methods have been proposed and investigated. In this paper, we make a review of research progress about controlling manipulators by means of neural networks. The problem foundation of manipulator control and the theoretical ideas on using neural network to solve this problem are first analyzed and then the latest progresses on this topic in recent years are described and reviewed in detail. Finally, toward practical applications, some potential directions possibly deserving investigation in controlling manipulators by neural networks are pointed out and discussed.",industry
10.1016/j.energy.2018.01.159,Journal,Energy,scopus,2018-04-01,sciencedirect,Multiobjective optimization of ethylene cracking furnace system using self-adaptive multiobjective teaching-learning-based optimization,https://api.elsevier.com/content/abstract/scopus_id/85041748366,"The ethylene cracking furnace system is crucial for an olefin plant. Multiple cracking furnaces are used to convert various hydrocarbon feedstocks to smaller hydrocarbon molecules, and the operational conditions of these furnaces significantly influence product yields and fuel consumption. This paper develops a multiobjective operational model for an industrial cracking furnace system that describes the operation of each furnace based on current feedstock allocations, and uses this model to optimize two important and conflicting objectives: maximization of key products yield, and minimization of the fuel consumed per unit ethylene. The model incorporates constraints related to material balance and the outlet temperature of transfer line exchanger. The self-adaptive multiobjective teaching-learning-based optimization algorithm is improved and used to solve the designed multiobjective optimization problem, obtaining a Pareto front with a diverse range of solutions. A real industrial case is investigated to illustrate the performance of the proposed model: the set of solutions returned offers a diverse range of options for possible implementation, including several solutions with both significant improvement in product yields and lower fuel consumption, compared with typical operational conditions.",industry
10.1016/j.measurement.2017.12.026,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2018-03-01,sciencedirect,NARX ANN-based instrument fault detection in motorcycle,https://api.elsevier.com/content/abstract/scopus_id/85039147782,"In the context of motorcycle, we can assist to an increasing interest toward semi-active suspension control systems able to improve both the comfort and the passenger’s safety in both racing and original equipment manufacturer applications. Such systems implement suitable strategies based on the measure of several quantities, among which the relative velocity of the wheels respect to the vehicle body with the aim of regulating in real-time the damping forces. The actual effectiveness of such strategy strongly depends on the reliability and accuracy of the data measured by the sensors involved in the control loop. Due to their simplicity and good performance in terms of linearity, the most used sensors for suspension displacement measurements are based on linear potentiometers but such kind of sensors suffer of wear and tear and aging higher than the other sensors involved in the control loop strategy. As a consequence, the fault detection of such sensor is strongly recommended to avoid wrong and in some cases dangerous suspension behaviors.
                  To this aim, in this paper a Fault Detection scheme for the rear suspension stroke sensor is designed and verified. The residual generation is based on the use of a Nonlinear Auto-Regressive with eXogenous inputs (NARX) network which is able to effectively take into account for the system nonlinearity. Experimental results have proven the good promptness and reliability of the scheme in detecting different kind of faults as “un-calibration faults” (e.g. due to slight variations of the input/output sensor curve), “hold-faults” (e.g. due to the breaking of the potentiometer cursor), “open circuit” and “short circuit” (e.g. due to electrical interruptions and short circuits, respectively).
                  In addition, to verify the feasibility of a real-time implementation on actual processing units employed in such context, the scheme has been successfully implemented on a microcontroller STM32 based on the general-purpose ARM-M4 architecture. The validation tests and analysis have shown that the proposed Instrument Fault Detection scheme could be successfully developed on these kind of architectures by assuring a real-time operating.",industry
10.1016/j.cose.2017.11.014,Journal,Computers and Security,scopus,2018-03-01,sciencedirect,Intelligent agents defending for an IoT world: A review,https://api.elsevier.com/content/abstract/scopus_id/85038807783,"Transition to the Internet of Things (IoT) is progressing without realization. In light of this securing traditional systems is still a challenging role requiring a mixture of solutions which may negatively impact, or simply, not scale to a desired operational level. Rule and signature based intruder detection remains prominent in commercial deployments, while the use of machine learning for anomaly detection has been an active research area. Behavior detection means have also benefited from the widespread use of mobile and wireless applications. For the use of smart defense systems we propose that we must widen our perspective to not only security, but also to the domains of artificial intelligence and the IoT in better understanding the challenges that lie ahead in hope of achieving autonomous defense. We investigate how intruder detection fits within these domains, particularly as intelligent agents. How current approaches of intruder detection fulfill their role as intelligent agents, the needs of autonomous action regarding compromised nodes that are intelligent, distributed and data driven. The requirements of detection agents among IoT security are vulnerabilities, challenges and their applicable methodologies. In answering aforementioned questions, a survey of recent research work is presented in avoiding refitting old solutions into new roles. This survey is aimed toward security researchers or academics, IoT developers and information officers concerned with the covered areas. Contributions made within this review are the review of literature of traditional and distributed approaches to intruder detection, modeled as intelligent agents for an IoT perspective; defining a common reference of key terms between fields of intruder detection, artificial intelligence and the IoT, identification of key defense cycle requirements for defensive agents, relevant manufacturing and security challenges; and considerations to future development. As the turn of the decade draws nearer we anticipate 2020 as the turning point where deployments become common, not merely just a topic of conversation but where the need for collective, intelligent detection agents work across all layers of the IoT becomes a reality.",industry
10.1016/j.microrel.2017.11.002,Journal,Microelectronics Reliability,scopus,2018-02-01,sciencedirect,Prognostics of aluminum electrolytic capacitors using artificial neural network approach,https://api.elsevier.com/content/abstract/scopus_id/85033701102,"In this work, an effort is being made to monitor the condition of in-circuit aluminum electrolytic capacitor using artificial neural network (ANN). Recent industrial surveys on the reliability of power electronic systems shows that most of faults occur due to the wear out of aluminum electrolytic capacitors and thermal stress is the major cause for its parametric degradation. The condition of target capacitors can be estimated by monitoring variation in equivalent series resistance (ESR) from the initial pristine state value. ANN is used to estimate ESR of pristine and weak target capacitors at the test conditions. The data set for training and testing of proposed back-propagation trained artificial neural network are experimentally obtained from the developed test bed. Using the test bed, target capacitors are subjected to different operating frequency and temperature in the output section of DC/DC buck converter circuit to determine the effect of variation in electrical and thermal stress on ESR value. After off-line training, the proposed ANN is implemented using National Instruments LabVIEW software. A low cost microcontroller is programmed for real time data acquisition of target capacitors and the serial transmission of acquired dataset to the LabVIEW software installed at host computer. The performance of the proposed method is evaluated in real time by comparing the resulting ESR with the experimental values of in-circuit target capacitors. The proposed ANN, once trained properly, can be used for different circuits and in different operating conditions because of its generalization capability.",industry
10.1016/j.neucom.2017.08.036,Journal,Neurocomputing,scopus,2018-01-31,sciencedirect,Data-driven model-free slip control of anti-lock braking systems using reinforcement Q-learning,https://api.elsevier.com/content/abstract/scopus_id/85029168035,"This paper proposes the design and implementation of a model-free tire slip control for a fast and highly nonlinear Anti-lock Braking System (ABS). A reinforcement Q-learning optimal control approach is inserted in a batch neural fitted scheme using two neural networks to approximate the value function and the controller, respectively. The transition samples required for learning high performance control can be collected by interacting with the process either by online exploiting the current iteration controller (or policy) under an ε-greedy exploration strategy, or by using data collected under any other controller that is capable of ensuring efficient exploration of the action-state space. Both approaches are highlighted in the paper. Fortunately, the ABS process fits this type of learning-by-interaction because it does not need an initial stabilizing controller. The validation case studies conducted on a real laboratory setup reveal that high control system performance can be achieved using the proposed approaches. Insightful comments on the observed control behavior are offered along with performance comparisons with several types of model-based and model-free controllers including relay, model-based optimal PI, an original model-free neural network state-feedback VRFT controller and a model-free neural network adaptive actor-critic one. With the ability to improve control performance starting from different supervisory controllers or to learn high performance controllers from scratch, the proposed Q-learning optimal control approach proves its performance in a wide operating range and is therefore recommended to its industrial application on ABS.",industry
10.1016/j.chemolab.2017.12.005,Journal,Chemometrics and Intelligent Laboratory Systems,scopus,2018-01-15,sciencedirect,A new reconstruction-based auto-associative neural network for fault diagnosis in nonlinear systems,https://api.elsevier.com/content/abstract/scopus_id/85037701407,"Auto-associative neural network (AANN) is a typical nonlinear principal component analysis method, which is widely used in industry for fault diagnosis purposes, especially in nonlinear systems. However, the basic AANN often suffers from “smearing effects” problems that may lead to misdiagnosis, particularly with regards to the complex faults involving multiple variables. In this work, a new reconstruction-based AANN (RBAANN) method is proposed to enhance the capacity of fault diagnosis. In RBAANN, a generic derivative equation is developed to investigate the effects of AANN model inputs on the prediction error between model inputs and outputs. Based on the derivative equation, the reconstruction-based index for single or multiple variables, which is defined as the minimum prediction error, is obtained by tuning the corresponding model inputs iteratively. However, without the prior knowledge of the real faulty variables, all the possible variable sets need to be evaluated by the reconstruction-based index, and this may result in an exhaustive search and cause a huge computational burden. Thus, a branch and bound algorithm is introduced into RBAANN to solve the variable selection problem. Finally, an efficient fault diagnosis strategy by integrating RBAANN and branch and bound algorithm (BAB-RBAANN) is implemented to further pinpoint the source of the detected faults. This BAB-RBAANN method can handle both single and multiple variable(s) faults for nonlinear systems without prior knowledge efficiently. The effectiveness of the proposed methods is evaluated on a validation example and an industrial example. Comparisons with other methods, including principal component analysis techniques, are also presented.",industry
10.1016/B978-0-12-813314-9.00011-6,Book,Computational Intelligence for Multimedia Big Data on the Cloud with Engineering Applications,scopus,2018-01-01,sciencedirect,Unsupervised anomaly detection for high dimensional data-An exploratory analysis,https://api.elsevier.com/content/abstract/scopus_id/85081928867,"Context: Anomaly detection is a crucial area engaging the attention of many researchers. It is a process of finding an unusual point or pattern in a given dataset. It is useful in many real time applications such as industry damage detection, detection of fraudulent usage of credit card, detection of failures in sensor nodes, detection of abnormal health and network intrusion detection. Algorithms proposed for anomaly detection in low dimensional data are not suitable for high dimensional data due to the well-known “dimensionality curses”.
               
                  Motivation: To tackle this issue, a plethora of algorithms dedicated to high dimensional data has been proposed. However, unsupervised algorithms have many problems and challenges, as there is no predefined data label to predict anomaly.
               
                  Objective: We aim at providing a complete view of unsupervised anomaly detection for high dimensional data which gives a clear perception of the concept.
               
                  Contribution: In this paper, existing algorithms and real time applications of unsupervised anomaly detection for high dimensional data have been studied. Evaluation measures, datasets and tools used by different authors have been discussed in detail. In addition, a hybrid framework of unsupervised anomaly detection algorithm called DBN–K means applied two different disease dataset is also proposed.
               
                  Future work: As future work, the proposed framework could be implemented and analyzed in other applications. High dimensional streaming data is another interesting area for further investigation, following this research work.",industry
10.1016/j.procir.2018.01.036,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,"Intuitive robot programming through environment perception, augmented reality simulation and automated program verification",https://api.elsevier.com/content/abstract/scopus_id/85061975291,"The increasing complexity of products and machines as well as short production cycles with small lot sizes present great challenges to production industry. Both, the programming of industrial robots in online mode using hand-held control devices or in offline mode using text-based programming requires specific knowledge of robotics and manufacturer-dependent robot control systems. In particular for small and medium-sized enterprises the machine control software needs to be easy, intuitive and usable without time-consuming learning steps, even for employees with no in-depth knowledge of information technology. To simplify the programming of application programs for industrial robots, we extended a cloud-based, task-oriented robot control system with environment perception and plausibility check functions. For the environment perception a depth camera and pointcloud processing hardware were installed. We detect objects located in the robot’s workspace by pointcloud processing with ROS and the PCL and add them to the augmented reality user interface of the robot control. The combination of process knowledge from task-oriented application programming and information about available workpieces from automated image processing enables a plausibility check and verification of the robot program before execution. After a robot program has been approved by the plausibility check, it is tested in an augmented reality simulation for collisions with the detected objects before deployment to the physical robot hardware. Experiments were carried out to evaluate the effectiveness of the developed extensions and confirmed their functionality.",industry
10.1016/j.procir.2018.09.067,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,A Conceptual Design for Smell Based Augmented Reality: Case Study in Maintenance Diagnosis,https://api.elsevier.com/content/abstract/scopus_id/85059916374,"The trend of Industry 4.0 encourages the next generation of manufacturing to be flexible, intelligent, and interoperable. The implementations of the Artificial Intelligence (AI) technology could potentially enhance maintenance in efficiency, and accuracy. However, it will not be a substitution to the human operator’s flexibility, decision-making and information received by the natural five senses. Augmented reality (AR) is commonly understood as a technology that overlays virtual information onto the existing environment to provide users a new and improved experience to assist their daily activities. However, AR can be used to enhance all human five senses rather than just overlay virtual imagery. In this paper, a design and a practical plan of smell augmentation for diagnosis is initialised, via a case study in maintenance. The aim of this paper is to evaluate the feasibilities, identify challenges, and summarise initial results of overlaying information through smell augmentations.",industry
10.1016/j.promfg.2018.04.009,Conference Proceeding,Procedia Manufacturing,scopus,2018-01-01,sciencedirect,Mixed Reality in Learning Factories,https://api.elsevier.com/content/abstract/scopus_id/85052906978,"Supported by rapid technological development, mixed reality (MR) applications are increasingly deployed in industrial practice. In manufacturing, MR can be utilized for information visualization, remote collaboration, human-machine-interfaces, design tools and education and training. This development makes new demands on learning factories in two major fields: One is the empowerment of users to work with MR in industrial applications. The second field is the utilization of the potential of MR for teaching and learning in learning factories. A great potential lies in the new possibilities of connecting digital content with the physical world. To analyze the potential applications of MR in learning factories in a structured way, an overview of potential MR applications based on the reality-virtuality continuum is presented with an analysis of case studies of applications in a learning factory including a mixed-reality-hackathon.",industry
10.1016/j.promfg.2018.04.026,Conference Proceeding,Procedia Manufacturing,scopus,2018-01-01,sciencedirect,Design and implementation of a low cost RFID track and trace system in a learning factory,https://api.elsevier.com/content/abstract/scopus_id/85052890798,"The factories of the future will make use of actuators, sensors and cyber-physical systems (CPS) to provide an environment in which human beings, machines, and resources will communicate as in a social network. In such a network, communication between various “objects” relay the current state of the physical world. Business decisions are made using the information and it is therefore critical that this information is accurate and in real-time. Information flow is a key enabler of such future factories. Industrial engineers, as designers and improvement agents of such factories of the future, will need to develop better skills in various aspects of data analytics and information communication technologies. This paper describes the development and implementation of a low cost RFID track and trace system (by students) for application in a Learning Factory for teaching undergraduate industrial engineering students key concepts related to Industry 4.0 and “smart factories”. The benefit of this system is not only a demonstrator to be used in the Learning Factory, but also can be used to teach students in a “learning by doing” fashion critical skills related to real time tracking in a manufacturing environment. The system also demonstrates potential low cost implementation of such technologies in SME’s.",industry
10.1016/j.ifacol.2018.08.421,Conference Proceeding,,scopus,2018-01-01,sciencedirect,A Multi Agent System architecture to implement Collaborative Learning for social industrial assets,https://api.elsevier.com/content/abstract/scopus_id/85052888258,"The ‘Industrial Internet of Things’ aims to connect industrial assets with one another and benefit from the data that is generated, and shared, among these assets. In recent years, the extensive instrumentation of machines and the advancements in Information Communication Technologies are re-shaping the role of assets in our industrial systems. An emerging concept here is that of ‘social assets’: assets that collaborate with each other in order to improve system optimisation. Cyber-Physical Systems (CPSs) are formed by embedding the assets with computers, or microcontrollers, which run real-time decision-making algorithms over the data originating from the asset. These are known as the ‘Digital Twins’ of the assets, and form the backbone of social assets. It is essential to have an architecture which enables a seamless integration of these technological advances for an industry. This paper proposes a Multi Agent System (MAS) architecture for collaborative learning, and presents the findings of an implementation of this architecture for a prognostics problem. Collaboration among assets is performed by calculating inter-asset similarity during operating condition to identify ‘friends’ and sharing operational data within these clusters of friends. The architecture described in this paper also presents a generic model for the Digital Twins of assets. Prognostics is demonstrated for the C-MAPSS turbofan engine degradation simulated data-set (Saxena and Goebel (2008)).",industry
10.1016/j.procs.2018.07.108,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Ambience Inhaling: Speech Noise Inhaler in Mobile Robots using Deep Learning,https://api.elsevier.com/content/abstract/scopus_id/85051344062,"Audio based, machine learning human-computer interface with speech recognition systems performs sensibly well with the human voice under clean ambience, but become frail in applied technological implementation involving real-life interface. In mobile robotic systems, the speech machines are normally retrained with new changing acoustic ambience conditions are to be met. To inhale, classify, and track the real-world ambience noise with the new changing acoustic condition, we introduce an Ambience Inhaling (AI) framework in this article. This framework of an AI is to seek out complete noise information from speech data, in contrast with noise-nature discovery. Our proposed framework uses a deep convolutional neural network (CNN) based learning for classification with speech spectrogram patch segments, including a hybrid Harold Hotelling's T-square algorithm with Bayesian statistics for segmentation analysis. We use a symposium presentation-ambience as a test platform. In the symposium presentation-ambience, noise modeling is done with n-gram language having the parameter of n = 2. The impulsive or short-term noise which is superimposed with long-term noise caused degradation in classification. This degradation caused the classification errors. The provision of decision was made. The Gaussian mixture model and hidden Markova model are used with noise-only and noisy speech respectively. Time and frequency pooling are used with spectrogram also. The classification scores of 62.26%, 65.89%, and 69.12% are achieved with 5, 10 and 15 CNN filters respectively. As a significance, an AI is efficient and innovative.",industry
10.1016/B978-0-444-64241-7.50087-2,Book Series,Computer Aided Chemical Engineering,scopus,2018-01-01,sciencedirect,Reinforcement Learning Applied to Process Control: A Van der Vusse Reactor Case Study,https://api.elsevier.com/content/abstract/scopus_id/85050599810,"With recent advances in industrial automation, data acquisition, and successful applications of Machine Learning methods to real-life problems, data-based methods can be expected to grow in use within the process control community in the near future. Model-based control methods rely on accurate models of the process to be effective. However, such models may be laborious to obtain and, even when available, the optimization problem underlying the online control problem may be too computationally demanding. Furthermore, the process degradation with time imposes that the model should be periodically updated to stay reliable. One way to address these drawbacks is through the merging of Reinforcement Learning (RL) techniques into the classical process control framework. In this work, a methodology to tackle the control of nonlinear chemical processes with RL techniques is proposed and tested on the wellknown benchmark problem of the non-isothermal CSTR with the Van de Vusse reaction. The controller proposed herein is based on the implementation of a policy that associates each state of the process to a certain control action. This policy is directly deduced from a measure of the expected performance gain, given by a value function dependent on the states and actions. In other words, in a given state, the action that provides the highest expected performance gain is chosen and implemented. The value function is approximated by a neural network that can be trained with pre-simulated data and adapted online with the continuous inclusion of new process data through the implementation of an RL algorithm. The results show that the proposed adaptive RLbased controller successfully manages to control and optimize the Van de Vusse reactor against unmeasured disturbances.",industry
10.1016/j.ifacol.2018.06.356,Conference Proceeding,,scopus,2018-01-01,sciencedirect,Design Principles Behind the Construction of an Autonomous Laboratory-Scale Drilling Rig,https://api.elsevier.com/content/abstract/scopus_id/85050080748,"In recent years, hot topics such as digitalization, machine learning, digital twin and big data have evolved from being envisions on the paper to state of art solutions, expected to revolutionize drilling efficiency in the industry. Drilling automation tomorrow is all about exploiting the current state of technologies available to the entire operation of drilling a well. Not only can drilling automation limit costs and reduce the risk to rig personnel and the environment, but they also give access to locations of considerable potential that previously have been regarded unsafe or uneconomical to operate in. There are however some challenges in keeping up with the ever-increasing pace of the development. For one, testing of novel and innovative solutions is often very expensive because of non-productive rig time during implementation, trial runs and data evaluation. Also, the modern technologies require extensive R&D before on-site testing can even commence. While on land-rigs, some of these costs and risks can be greatly minimized, many offshore solutions lack that luxury. This paper presents an overview of the design principles that go into the construction of a fully autonomous laboratory-scale drilling rig at the University of Stavanger. It aims at describing 1) the engineering principles involved to resemble full-scale drilling operations on the laboratory scale, 2) design considerations and components, 3) component requirements for the rig, 4) control system algorithms for real-time optimization of drilling parameters and detection and handling of drilling anomalies, 5) development of drilling models (drill string dynamics, bit-vibration, etc.) and 6) benefits and future work with the laboratory-scale system. Some of the concepts that are presented in this paper have yet to be implemented during 2018.",industry
10.1016/j.procir.2018.03.168,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,Reinforcement learning in real-time geometry assurance,https://api.elsevier.com/content/abstract/scopus_id/85049605436,"To improve the assembly quality during production, expert systems are often used. These experts typically use a system model as a basis for identifying improvements. However, since a model uses approximate dynamics or imperfect parameters, the expert advice is bound to be biased. This paper presents a reinforcement learning agent that can identify and limit systematic errors of an expert systems used for geometry assurance. By observing the resulting assembly quality over time, and understanding how different decisions affect the quality, the agent learns when and how to override the biased advice from the expert software.",industry
10.1016/j.procir.2018.03.022,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,Fostering Robust Human-Robot Collaboration through AI Task Planning,https://api.elsevier.com/content/abstract/scopus_id/85049587790,"Recent advances in Artificial Intelligence (AI) are facilitating the deployment of intelligent systems in manufacturing. In Human-Robot Collaboration (HRC), industrial robots offer accuracy and efficiency while humans guarantee both experience and specialized and not replaceable skills. The seamless coordination of such different abilities constitutes one of the current challenges. This paper presents a dynamic task sequencing system for robust HRC developed within a EU-funded project. The proposed solution uses AI techniques to deal with the temporal variance entailed by the active presence of humans as well as to dynamically adapt task plans according to actual behavior of the pair human-worker/robot. The tool has been deployed in a real pilot plant.",industry
10.1016/j.procs.2018.05.142,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Optimization of Software Testing,https://api.elsevier.com/content/abstract/scopus_id/85049103381,"The goal of any business is to satisfy the needs of its target customers, and IT industry is not an exception from that rule. Thus, the upgraded version of the V-model testing is supposed to deal with the weaknesses of the original version in question by combining it with the method known as agile testing. At the beginning of the report, hypothesis such as the strengths and weaknesses of the existing V-model testing via literature review and interviews with respective specialists in the sphere were analysed. Successively, the possible advantages of agile method of testing were then considered. Moreover, the report comes up with the ways in which the two models could be naturally combined to produce a much more effective one. Once the new model was presented, its strengths and weaknesses were assessed by the means of a case study analysis using metric and a data analysis through a survey were conducted to evaluate the credibility of the futurist model. Promptly, the research found that the suggested testing model provides better results than the common version of V-model testing. Firstly, a real case scenarios under metric evaluation of the models have indicated that the proposed model is better than the V-model, since it can handle the following aspects; reduced testing time, debugging, prioritization of requirements, easy mapping of roles and improved visibility of project resources. Secondly, a survey data analysis highlighted various advantages of the future model. The top priorities of the new model from the respondent’s perception were; the new model manages rapidly changing priorities, it accelerates time to market, it increases productivity and it improves quality.",industry
10.1016/j.procs.2018.05.113,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Real Time High Performance of Sliding Mode Controlled Induction Motor Drives,https://api.elsevier.com/content/abstract/scopus_id/85049099142,"Several industrial applications demand high performance speed functioning and require new control techniques so as to ensure a fast dynamic response. The present work investigates real time implementation and experimental sliding mode controlled (SMC) induction motor drives (IM). The strategy of sliding mode control is a powerful tool to ensure robustness. Nevertheless, the chattering phenomenon is a major disadvantage for non linear systems. For this purpose, two different types of analysis such as layer boundary methods are implemented in dSPACE 1104 controller board and compared between them in order to obtain the best method to reduce or eliminate chattering phenomenon. An experimental results using dSPACE 1104 based on TMS320F240 DSP are described in this work.",industry
10.1016/j.cirp.2018.04.041,Journal,CIRP Annals,scopus,2018-01-01,sciencedirect,Reinforcement learning for adaptive order dispatching in the semiconductor industry,https://api.elsevier.com/content/abstract/scopus_id/85045954603,"The digitalization of production systems tends to provide a huge amount of data from heterogeneous sources. This is particularly true for the semiconductor industry wherein real time process monitoring is inherently required to achieve a high yield of good parts. An application of data-driven algorithms in production planning to enhance operational excellence for complex semiconductor production systems is currently missing. This paper shows the successful implementation of a reinforcement learning-based adaptive control system for order dispatching in the semiconductor industry. Furthermore, a performance comparison of the learning-based control system with the traditionally used rule-based system shows remarkable results. Since a strict rulebook does not bind the learning-based control system, a flexible adaption to changes in the environment can be achieved through a combination of online and offline learning.",industry
10.1016/j.procir.2017.12.230,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,A Conceptual Model for Developing a Smart Process Control System,https://api.elsevier.com/content/abstract/scopus_id/85044679399,"Current Manufacturing Execution Systems (MES) are not supporting a full integration into overall processes across the supply chain. Thus, optimization is limited to single areas. The SemI40 project is aimed at developing an integrated concept of Smart Process Control System (SPCS), which enhances the overall agility and productivity. The system, therefore, autonomously acquires and interprets process data to allow product individual optimization and enhancing logistics management. It also provides full traceability across the supply chain in real-time and allows model based process simulation and decision making support. The concept is developed based on requirements elicitation in cooperation with industry partners and combines state-of-the-art technologies with current trends, like vertical integration, big data and machine learning.",industry
10.1016/j.mfglet.2017.12.013,Journal,Manufacturing Letters,scopus,2018-01-01,sciencedirect,Artificial neural network based framework for cyber nano manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85042371124,"Nanomanufacturing plays an important role for high performance products in several applications. The challenge for fabricating products with nanomaterials is the inability to interconnect and interface with nano/micro manufacturing equipment. This paper presents a framework for cyber nanomanufacturing. Input part designs of nano/micro scale components are evaluated with an artificial neural network (ANN) based smart agent to predict optimal nanomanufacturing processes. An internet-of-things (IoT) based cyber-interface simulator is implemented to simulate real-time machine availability. Further, an application program interface (API) is developed to integrate the ANN smart agent and IoT simulator outcomes to predict dynamic machine allocations in real-time.",industry
10.1016/j.addma.2017.11.009,Journal,Additive Manufacturing,scopus,2018-01-01,sciencedirect,Anomaly detection and classification in a laser powder bed additive manufacturing process using a trained computer vision algorithm,https://api.elsevier.com/content/abstract/scopus_id/85035797198,"Despite the rapid adoption of laser powder bed fusion (LPBF) Additive Manufacturing by industry, current processes remain largely open-loop, with limited real-time monitoring capabilities. While some machines offer powder bed visualization during builds, they lack automated analysis capability. This work presents an approach for in-situ monitoring and analysis of powder bed images with the potential to become a component of a real-time control system in an LPBF machine. Specifically, a computer vision algorithm is used to automatically detect and classify anomalies that occur during the powder spreading stage of the process. Anomaly detection and classification are implemented using an unsupervised machine learning algorithm, operating on a moderately-sized training database of image patches. The performance of the final algorithm is evaluated, and its usefulness as a standalone software package is demonstrated with several case studies.",industry
10.1016/j.ins.2017.09.027,Journal,Information Sciences,scopus,2018-01-01,sciencedirect,Incorporating negative information to process discovery of complex systems,https://api.elsevier.com/content/abstract/scopus_id/85029528097,"The discovery of a formal process model from event logs describing real process executions is a challenging problem that has been studied from several angles. Most of the contributions consider the extraction of a model as a one-class supervised learning problem where only a set of process instances is available. Moreover, the majority of techniques cannot generate complex models, a crucial feature in some areas like manufacturing. In this paper we present a fresh look at process discovery where undesired process behaviors can also be taken into account. This feature may be crucial for deriving process models which are less complex, fitting and precise, but also good on generalizing the right behavior underlying an event log. The technique is based on the theory of convex polyhedra and satisfiability modulo theory (SMT) and can be combined with other process discovery approach as a post processing step to further simplify complex models. We show in detail how to apply the proposed technique in combination with a recent method that uses numerical abstract domains. Experiments performed in a new prototype implementation show the effectiveness of the technique and the ability to be combined with other discovery techniques.",industry
10.1016/j.biosystems.2017.10.001,Journal,BioSystems,scopus,2017-12-01,sciencedirect,Towards a first implementation of the WLIMES approach in living system studies advancing the diagnostics and therapy in personalized medicine,https://api.elsevier.com/content/abstract/scopus_id/85033459793,"The goal of this paper is to advance an extensible theory of living systems using an approach to biomathematics and biocomputation that suitably addresses self-organized, self-referential and anticipatory systems with multi-temporal multi-agents. Our first step is to provide foundations for modelling of emergent and evolving dynamic multi-level organic complexes and their sustentative processes in artificial and natural life systems. Main applications are in life sciences, medicine, ecology and astrobiology, as well as robotics, industrial automation, man-machine interface and creative design. Since 2011 over 100 scientists from a number of disciplines have been exploring a substantial set of theoretical frameworks for a comprehensive theory of life known as Integral Biomathics. That effort identified the need for a robust core model of organisms as dynamic wholes, using advanced and adequately computable mathematics. The work described here for that core combines the advantages of a situation and context aware multivalent computational logic for active self-organizing networks, Wandering Logic Intelligence (WLI), and a multi-scale dynamic category theory, Memory Evolutive Systems (MES), hence WLIMES. This is presented to the modeller via a formal augmented reality language as a first step towards practical modelling and simulation of multi-level living systems. Initial work focuses on the design and implementation of this visual language and calculus (VLC) and its graphical user interface. The results will be integrated within the current methodology and practices of theoretical biology and (personalized) medicine to deepen and to enhance the holistic understanding of life.",industry
10.1016/j.simpat.2015.07.004,Journal,Simulation Modelling Practice and Theory,scopus,2017-12-01,sciencedirect,A reinforcement learning methodology for a human resource planning problem considering knowledge-based promotion,https://api.elsevier.com/content/abstract/scopus_id/84939200331,"This paper addresses a combined problem of human resource planning (HRP) and production-inventory control for a high-tech industry, wherein the human resource plays a critical role. The main characteristics of this resource are the levels of “knowledge” and the learning process. The learning occurs during the production process in which a worker can promote to the upper knowledge level. Workers in upper levels have more productivity in the production. The objective is to maximize the expected profit by deciding on the optimal numbers of workers in various knowledge levels to fulfill both production and training requirement. As taking an action affects next periods’ decisions, the main problem is to find the optimal hiring policy of non-skilled workers in long-time horizon. Thus, we develop a reinforcement learning (RL) model to obtain the optimal decision for hiring workers under the demand uncertainty. The proposed interval-based policy of our RL model, in which for each state there are multiple choices, makes it more flexible. We also embed some managerial issues such as layoff and overtime-working hours into the model. To evaluate the proposed methodology, stochastic dynamic programming (SDP) and a conservative method implemented in a real case study are used. We study all these methods in terms of four criteria: average obtained profit, average obtained cost, the number of new-hired workers, and the standard deviation of hiring policies. The numerical results confirm that our developed method end up with satisfactory results compared to two other approaches.",industry
10.1016/j.engappai.2017.06.014,Journal,Engineering Applications of Artificial Intelligence,scopus,2017-11-01,sciencedirect,A configurable partial-order planning approach for field level operation strategies of PLC-based industry 4.0 automated manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85030713189,"The machine and plant automation domain is faced with an ever increasing demand for ensuring the adaptability of manufacturing facilities in context of Industry 4.0. Field level automation software plays a dominant role in strengthening the overall flexibility of manufacturing resources. Classical programming approaches based typically on signal-oriented languages result in disproportionate effort for ensuring necessary flexibility. To address this challenge, a novel approach based on artificial intelligence planning techniques is presented which is able to handle domain specific requirements while facilitating efficient, scalable problem solving. Throughout this article, a discussion of specific requirements on automated planning techniques for field level automation software in the machine and plant automation domain with respect to Industry 4.0 is provided. An intensive study on existing works and their drawbacks towards addressing these requirements is presented. The proposed configurable partial-order planning approach is based upon a combination of an adapted goal-based planning formulation and its reformulation by means of linear programming techniques. It is shown that the proposed approach is able to efficiently solve large planning problems by exhibiting positive scalability characteristics which indicates its applicability for real-size plants.",industry
10.1016/j.cie.2017.09.016,Journal,Computers and Industrial Engineering,scopus,2017-11-01,sciencedirect,Smart operators in industry 4.0: A human-centered approach to enhance operators’ capabilities and competencies within the new smart factory context,https://api.elsevier.com/content/abstract/scopus_id/85029476237,"As the Industry 4.0 takes shape, human operators experience an increased complexity of their daily tasks: they are required to be highly flexible and to demonstrate adaptive capabilities in a very dynamic working environment. It calls for tools and approaches that could be easily embedded into everyday practices and able to combine complex methodologies with high usability requirements. In this perspective, the proposed research work is focused on the design and development of a practical solution, called Sophos-MS, able to integrate augmented reality contents and intelligent tutoring systems with cutting-edge fruition technologies for operators’ support in complex man-machine interactions. After establishing a reference methodological framework for the smart operator concept within the Industry 4.0 paradigm, the proposed solution is presented, along with its functional and non-function requirements. Such requirements are fulfilled through a structured design strategy whose main outcomes include a multi-layered modular solution, Sophos-MS, that relies on Augmented Reality contents and on an intelligent personal digital assistant with vocal interaction capabilities. The proposed approach has been deployed and its training potentials have been investigated with field experiments. The experimental campaign results have been firstly checked to ensure their statistical relevance and then analytically assessed in order to show that the proposed solution has a real impact on operators’ learning curves and can make the difference between who uses it and who does not.",industry
10.1016/j.knosys.2017.07.007,Journal,Knowledge-Based Systems,scopus,2017-10-01,sciencedirect,Fit evaluation of virtual garment try-on by learning from digital pressure data,https://api.elsevier.com/content/abstract/scopus_id/85022033035,"Presently, garment fit evaluation mainly focuses on real try-on, and rarely deals with virtual try-on. With the rapid development of E-commerce, there is a profound growth of garment purchases through the internet. In this context, fit evaluation of virtual garment try-on is vital in the clothing industry. In this paper, we propose a Naive Bayes-based model to evaluate garment fit. The inputs of the proposed model are digital clothing pressures of different body parts, generated from a 3D garment CAD software; while the output is the predicted result of garment fit (fit or unfit). To construct and train the proposed model, data on digital clothing pressures and garment real fit was collected for input and output learning data respectively. By learning from these data, our proposed model can predict garment fit rapidly and automatically without any real try-on; therefore, it can be applied to remote garment fit evaluation in the context of e-shopping. Finally, the effectiveness of our proposed method was validated using a set of test samples. Test results showed that digital clothing pressure is a better index than ease allowance to evaluate garment fit, and machine learning-based garment fit evaluation methods have higher prediction accuracies.",industry
10.1016/j.eneco.2017.06.020,Journal,Energy Economics,scopus,2017-08-01,sciencedirect,"Composite forecasting approach, application for next-day electricity price forecasting",https://api.elsevier.com/content/abstract/scopus_id/85024479867,"Accurate forecasting of electricity prices can provide significant benefits to energy suppliers when allocating their assets and to energy consumers for defining an optimal portfolio. There are numerous methods that efficiently support the forecasting of time series, such as electricity prices, which have high volatility. However, the performance of these approaches varies depending on data sets and operational conditions. In this work, the concept of composite forecasting is presented and implemented in a retrospective study, in real industrial forecasting conditions to show the potential of forecast performance improvement and comparable high consistency of a forecast performance across different ‘Day Peak’ and ‘Day Base’ electricity price data sets for different seasons. As individual methods support vector regression, artificial neural networks and ridge regression are implemented. The forecast performances of these methods are evaluated and compared with their forecast combination using different error measures. The results show that composite forecasting processes with ‘inverse root mean squared error’ combination approach can generate, on average, a more accurate and robust forecast than using an individual methods or other combination schemas.",industry
10.1016/j.mineng.2017.02.013,Journal,Minerals Engineering,scopus,2017-08-01,sciencedirect,Designing gold extraction processes: Performance study of a case-based reasoning system,https://api.elsevier.com/content/abstract/scopus_id/85014728806,"This paper presents a method for externalising and formalising knowledge involving the selection of hydrometallurgical process flowsheets for gold extraction from ores. A case-based reasoning (CBR) system was built using an open source software myCBR 3.0. The aim of the systems is to recommend flowsheet alternatives for processing a potential gold ore deposit. Nine attributes: Ore type, Gold ore grade, Gold distribution, Gold grain size, Sulfide present, Arsenic sulfide, Copper sulfide, Iron sulfide and Clay present were modelled and several literature sources of actual gold mines and processes were used for acquiring cases for the system. After preliminary testing, functional evaluation of the built CBR system was carried out by using five real mining projects as test cases. Additionally, human experts in the field of gold hydrometallurgy were interviewed to demonstrate the benefits of the CBR system as it holds no human biases towards any processing techniques. It was found that the suggestions of the CBR system provided useful information and direction for further process design and performed well compared to the interviewed human experts, thus confirming that the system is of practical relevance to the process engineer designing an industrial gold processing plant. The current model was found to be a functioning basis for further development through additional attributes, adjusted attribute weighting and increased number of cases.",industry
10.1016/j.ifacol.2017.08.986,Conference Proceeding,IFAC-PapersOnLine,scopus,2017-07-01,sciencedirect,Using data mining methods for manufacturing process control,https://api.elsevier.com/content/abstract/scopus_id/85031805594,"The Industry 4.0 concept assumes that modern manufacturing systems generate huge amounts of data that must be collected, stored, managed and analysed. The case study is focused on predicting the manufacturing process behaviour according to production data. The paper presents the way of gaining knowledge about the future behaviour of manufacturing system by data mining predictive tasks. The proposed simulation model of the real manufacturing process was designed to obtain the data necessary for the control process. The predictions of the manufacturing process behaviour were implemented varying the input parameters using selected methods and techniques of data mining. The predicted process behaviour was verified using the simulation model.
                  The authors analysed different methods. The neural network method was selected for deploying new data by PMML files in the final phases. The objectives of the research are to design and verify the data mining tools in order to support the manufacturing system control by aiming at improving the decisionmaking process. Based on the prediction of the goal production outcomes, the actual control strategies can be precisely modified. Then they can be used in real manufacturing system without risks.",industry
10.1016/j.ifacol.2017.08.902,Conference Proceeding,IFAC-PapersOnLine,scopus,2017-07-01,sciencedirect,A Networked Production System to Implement Virtual Enterprise and Product Lifecycle Information Loops,https://api.elsevier.com/content/abstract/scopus_id/85031797675,"This paper is aimed at considering supply chain and related data management within an integrated vision of the product lifecycle management (PLM) implemented through the unified approach which is proper to the Industry 4.0 initiative. In particular, with the proposed manufacturing system architecture, decision support tools can use a unified repository fed by a factory replication application, powered by data from the field, even from remote production units. Such data allow to monitor the behaviour of the digital twin of the real machine and produces a digital twin of the real product, incorporating its actual characteristics measured by means of suitable acquiring systems (in the treated example: a 3D laser scanner). Moreover, it is provided a description of the plant technological subsystems that allow to share designing and manufacturing activities across multiple similar units located in remote areas. In this context of virtual enterprise, the supply chain management results as a key factor in enabling a cooperative approach.",industry
10.1016/j.infsof.2017.03.003,Journal,Information and Software Technology,scopus,2017-07-01,sciencedirect,Uncertainty-wise evolution of test ready models,https://api.elsevier.com/content/abstract/scopus_id/85015382293,"Context
                  Cyber-Physical Systems (CPSs), when deployed for operation, are inherently prone to uncertainty. Considering their applications in critical domains (e.g., healthcare), it is important that such CPSs are tested sufficiently, with the explicit consideration of uncertainty. Model-based testing (MBT) involves creating test ready models capturing the expected behavior of a CPS and its operating environment. These test ready models are then used for generating executable test cases. It is, therefore, necessary to develop methods that can continuously evolve, based on real operational data collected during the operation of CPSs, test ready models and uncertainty captured in them, all together termed as Belief Test Ready Models (BMs)
               
                  Objective
                  Our objective is to propose a model evolution framework that can interactively improve the quality of BMs, based on operational data. Such BMs are developed by one or more test modelers (belief agents) with their assumptions about the expected behavior of a CPS, its expected physical environment, and potential future deployments. Thus, these models explicitly contain subjective uncertainty of the test modelers.
               
                  Method
                  We propose a framework (named as UncerTolve) for interactively evolving BMs (specified with extended UML notations) of CPSs with subjective uncertainty developed by test modelers. The key inputs of UncerTolve include initial BMs of CPSs with known subjective uncertainty and real data collected from the operation of CPSs. UncerTolve has three key features: 1) Validating the syntactic correctness and conformance of BMs against real operational data via model execution, 2) Evolving objective uncertainty measurements of BMs via model execution, and 3) Evolving state invariants (modeling test oracles) and guards of transitions (modeling constraints for test data generation) of BMs with a machine learning technique.
               
                  Results
                  As a proof-of-concept, we evaluated UncerTolve with one industrial CPS case study, i.e., GeoSports from the healthcare domain. Using UncerTolve, we managed to evolve 51% of belief elements, 18% of states, and 21% of transitions as compared to the initial BM developed in an industrial setting.
               
                  Conclusion
                  
                     UncerTolve can successfully evolve model elements of the initial BM, in addition to objective uncertainty measurements using real operational data. The evolved model can be used to generate additional test cases covering evolved model elements and objective uncertainty. These additional test cases can be used to test the current and future deployments of a CPS to ensure that it will handle uncertainty gracefully during its operations.",industry
10.1016/j.engappai.2016.08.019,Journal,Engineering Applications of Artificial Intelligence,scopus,2017-06-01,sciencedirect,GPU-based parallel optimization of immune convolutional neural network and embedded system,https://api.elsevier.com/content/abstract/scopus_id/84995489085,"Up to now, the image recognition system has been utilized more and more widely in the security monitoring, the industrial intelligent monitoring, the unmanned vehicle, and even the space exploration. In designing the image recognition system, the traditional convolutional neural network has some defects such as long training time, easy over-fitting and high misclassification rate. In order to overcome these defects, we firstly used the immune mechanism to improve the convolutional neural network and put forward a novel immune convolutional neural network algorithm, after we analyzed the network structure and parameters of the convolutional neural network. Our algorithm not only integrated the location data of the network nodes and the adjustable parameters, but also dynamically adjusted the smoothing factor of the basis function. In addition, we utilized the NVIDIA GPU (Graphics Processing Unit) to accelerate the new immune convolutional neural network (ICNN) in parallel computing and built a real-time embedded image recognition system for this ICNN. The immune convolutional neural network algorithm was improved with CUDA programming and was tested with the sample data in the GPU-based environment. The GPU-based implementation of the novel immune convolutional neural network algorithm was made with the cuDNN, which was designed by NVIDIA for GPU-based accelerating of DNNs in machine learning. Experimental results show that our new immune convolutional neural network has higher recognition rate, more stable performance and faster computing speed than the traditional convolutional neural network.",industry
10.1016/j.neucom.2016.09.005,Journal,Neurocomputing,scopus,2017-05-10,sciencedirect,Wood moisture content prediction using feature selection techniques and a kernel method,https://api.elsevier.com/content/abstract/scopus_id/84996497604,"Wood is a renewable, abundant bio-energy and environment friendly resource. Woody biomass Moisture Content (
                        MC
                     ) is a key parameter for controlling the biofuel product qualities and properties. In this paper, we are interested in predicting 
                        MC
                      from data. The input impedance of half-wave dipole antenna when buried in the wood pile varies according to the permittivity of wood. Hence, the measurement of reflection coefficient, that gives information about the input impedance, depends directly on the 
                        MC
                      of wood. The relationship between the reflection coefficient measurements and the 
                        MC
                      is studied. Based upon this relationship, 
                        MC
                      predictive models that use machine learning techniques and feature selection methods are proposed. Numerical experiments using real world data show the relevance of the proposed approach that requires a limited computational power. Therefore, a real-time implementation for industrial processes is feasible.",industry
10.1016/j.jmsy.2017.02.007,Journal,Journal of Manufacturing Systems,scopus,2017-04-01,sciencedirect,Framework and development of fault detection classification using IoT device and cloud environment,https://api.elsevier.com/content/abstract/scopus_id/85014081409,"While Cyber-physical system (CPS) is considered as a key foundation for cyber manufacturing, many related frameworks and applications have been provided. This research suggests a new and effective CPS architecture for supporting multi-sites and multi-products manufacturing. As target processes, the manufacturing processes for vehicles’ High Intensity Discharge (HID) headlight and cable modules are considered. These modules are manufactured with several multi-manufacturing sites consisting of internal manufacturing tasks and intermediate outsourcing processes. In addition, they produce multiple types of HID cable modules with different components. These issues make it difficult to improve the qualities of the overall processes and to control those considering overall manufacturing plants and processes. In order to overcome these limitations, this research provides an Internet of Things (IoT) embedded cloud control architecture. The mixed flow issues are overcome with the cloud control server with the suggested framework. The developed IoT device detects several system status and transmits the signals. The data is analyzed for the fault detection classification (FDC) mechanism using deep learning based analytics. Then, the cyber manufacturing based simulation is executed using the provided multi-products queueing network model. The estimated simulation results is used for generating dynamic manufacturing decisions reflecting the real-time changes of the production environment. The suggested framework and its implementations can be used for various industrial processes and applications.",industry
10.1016/j.jmsy.2017.02.011,Journal,Journal of Manufacturing Systems,scopus,2017-04-01,sciencedirect,A fog computing-based framework for process monitoring and prognosis in cyber-manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85013912214,"Small- and medium-sized manufacturers, as well as large original equipment manufacturers (OEMs), have faced an increasing need for the development of intelligent manufacturing machines with affordable sensing technologies and data-driven intelligence. Existing monitoring systems and prognostics approaches are not capable of collecting the large volumes of real-time data or building large-scale predictive models that are essential to achieving significant advances in cyber-manufacturing. The objective of this paper is to introduce a new computational framework that enables remote real-time sensing, monitoring, and scalable high performance computing for diagnosis and prognosis. This framework utilizes wireless sensor networks, cloud computing, and machine learning. A proof-of-concept prototype is developed to demonstrate how the framework can enable manufacturers to monitor machine health conditions and generate predictive analytics. Experimental results are provided to demonstrate capabilities and utility of the framework such as how vibrations and energy consumption of pumps in a power plant and CNC machines in a factory floor can be monitored using a wireless sensor network. In addition, a machine learning algorithm, implemented on a public cloud, is used to predict tool wear in milling operations.",industry
10.1016/j.flowmeasinst.2016.10.001,Journal,Flow Measurement and Instrumentation,scopus,2017-04-01,sciencedirect,Intelligent recognition of gas-oil-water three-phase flow regime and determination of volume fraction using radial basis function,https://api.elsevier.com/content/abstract/scopus_id/85000420282,"The problem of how to accurately measure the flow rate of oil–gas–water mixtures in a pipeline remains one of the key challenges in the petroleum industry. This paper proposes a new methodology for identifying flow regimes and predicting volume fractions in gas-oil-water multiphase systems using dual energy fan-beam gamma-ray attenuation technique and artificial neural networks. The novelty of this study in comparison with previous works, is using just 4 extracted features (photo peaks of 241Am and 137Cs in 2 detectors) from the gamma ray spectrums instead of using the whole gamma ray spectrum, which reduces the undesired noises and also improves the speed of recognition in real situations. Radial basis function was used for developing the neural network model in MATLAB software in order to classify the flow patterns (annular, stratified and homogenous) and predict the value of volume fractions. The ideal and static theoretical models for flow regimes have been developed using MCNP-X code. The proposed networks could correctly recognize all the three different flow regimes and also determine volume fractions with mean absolute error of less than 5.68% according to the recognized regime.",industry
10.1016/j.compositesb.2016.12.050,Journal,Composites Part B: Engineering,scopus,2017-03-01,sciencedirect,Digitisation of manual composite layup task knowledge using gaming technology,https://api.elsevier.com/content/abstract/scopus_id/85009923562,"Increased market demand for composite products and shortage of expert laminators is compelling the composite industry to explore ways to acquire layup skills from experts and transfer them to novices and eventually to machines. There is a lack of holistic methods in literature for capturing composite layup skills especially involving complex moulds. This research aims to develop an informatics-based method, enabled by consumer-grade gaming technology and machine learning, to capture and digitise manufacturing task knowledge from skill-intensive hand layup. The digitisation is underpinned by the proposed human-workpiece interaction theory and implemented to automatically extract and decode key knowledge constituents such as layup strategies, ply manipulation techniques, motion mechanics and problem-solving during hand layup, collectively categorised as layup skills. The significance of this research is its potential to facilitate cost-effective transfer of skills from experts to novices, real-time automated supervision of hand layup and automation of layup tasks in the future.",industry
10.1016/j.jenvman.2016.10.056,Journal,Journal of Environmental Management,scopus,2017-02-01,sciencedirect,Improving the efficiency of dissolved oxygen control using an on-line control system based on a genetic algorithm evolving FWNN software sensor,https://api.elsevier.com/content/abstract/scopus_id/85006851639,"This work proposes an on-line hybrid intelligent control system based on a genetic algorithm (GA) evolving fuzzy wavelet neural network software sensor to control dissolved oxygen (DO) in an anaerobic/anoxic/oxic process for treating papermaking wastewater. With the self-learning and memory abilities of neural network, handling the uncertainty capacity of fuzzy logic, analyzing local detail superiority of wavelet transform and global search of GA, this proposed control system can extract the dynamic behavior and complex interrelationships between various operation variables. The results indicate that the reasonable forecasting and control performances were achieved with optimal DO, and the effluent quality was stable at and below the desired values in real time. Our proposed hybrid approach proved to be a robust and effective DO control tool, attaining not only adequate effluent quality but also minimizing the demand for energy, and is easily integrated into a global monitoring system for purposes of cost management.",industry
10.1016/j.simpat.2016.08.007,Journal,Simulation Modelling Practice and Theory,scopus,2017-02-01,sciencedirect,Intelligent simulation: Integration of SIMIO and MATLAB to deploy decision support systems to simulation environment,https://api.elsevier.com/content/abstract/scopus_id/84996798684,"Discrete-event simulation is a decision support tool which enables practitioners to model and analyze their own system behavior. Although simulation packages are capable of mimicking most tasks in a real-world system, there are some decision-making activities, which are beyond the reach of simulation packages. The Application Programmers Interface (API) of SIMIO provides a wide range of opportunities for researchers to develop their own logic and apply it during the simulation run. This paper illustrates how to deploy MATLAB, as a computational tool coupled with SIMIO, as a simulation package by using a new user-defined step instance named “CallMATLAB”. A manufacturing system case study is introduced where the CallMATLAB step instance is used to create an Iterative Optimization-based Simulation (IOS) model. This model is created to evaluate the performance of different optimizers. The benefits of this hybridization for other industries, including healthcare systems, supply chain management systems, and project management problems are discussed.",industry
10.1016/j.promfg.2017.07.167,Journal,Procedia Manufacturing,scopus,2017-01-01,sciencedirect,Towards Robust Early Stage Data Knowledge-based Inference Engine to Support Zero-defect Strategies in Manufacturing Environment,https://api.elsevier.com/content/abstract/scopus_id/85029884694,"Decision Support Systems are considered as a robust technology able to provide an advantage to several manufacturing companies. As part of the Z-Fact0r EU project, an autonomous and self-adjusted inference engine; namely the Early Stage-Decision Support System (ES-DSS) will be deployed. The scope is to facilitate real-time inspection, condition monitoring and control - diagnosis at the shop-floor, utilizing continuously mine multiple data streams and run the suitable models to monitor operations and quality performance, to classify products on the basis of quality metrics, as well to predict occurrence of defects and deviations from production and quality requirements.",industry
10.1016/j.procir.2017.03.093,Conference Proceeding,Procedia CIRP,scopus,2017-01-01,sciencedirect,Cyber-Physical Manufacturing Metrology Model (CPM<sup>3</sup>) for Sculptured Surfaces - Turbine Blade Application,https://api.elsevier.com/content/abstract/scopus_id/85028681766,"Cyber-Physical Manufacturing (CPM) and digital manufacturing represent the key elements for implementation of Industry 4.0 framework. Worldwide, Industry 4.0 becomes national research strategy in the field of engineering for the following ten years. The International Conference USA-EU-Far East-Serbia Manufacturing Summit was held from 31st May to 2nd June 2016 in Belgrade, Serbia. The result of the conference was the development of Industry 4.0 Model for Serbia as a framework for New Industrial Policy – Horizon 2020/2030.
                  Implementation of CPM in manufacturing systems generates “smart factory”. Products, resources, and processes within smart factory are realized and controlled through CPM model. This leads to significant advantages with respect to high product/process quality, real-time applications, savings in resources consumption, as well as, lower costs in comparison with classical manufacturing systems. Smart factory is designed in accordance with sustainable and service-oriented best business practices/models. It is based on optimization, flexibility, self-adaptability and learning, fault tolerance, and risk management. Complete manufacturing digitalization and digital factory are the key elements of Industry 4.0 Program.
                  In collaborative research, which we carry out in the field of quality control and manufacturing metrology at University of Belgrade, Faculty of Mechanical Engineering in Serbia and at Department of Mechanical Engineering, University of Texas, Austin in USA, three research areas are defined: (а) Digital manufacturing – towards Cloud Manufacturing Systems (as a basis for CPS), in which quality and metrology represent integral parts of process optimization based on Taguchi model, and (б) Cyber-Physical Quality Model (CPQM) – our approach, in which we have developed and tested intelligent model for prismatic parts inspection planning on CMM (Coordinate Measuring Machine). The third research area directs our efforts to the development of framework for Cyber-Physical Manufacturing Metrology Model (CPM3). CPM3 framework will be based on integration of digital product metrology information through metrology features recognition, and generation of global/local inspection plan for free-form surfaces; we will illustrate our approach using turbine blade example. This paper will present recent results of our research on CPM3.",industry
10.1016/j.procir.2017.03.115,Conference Proceeding,Procedia CIRP,scopus,2017-01-01,sciencedirect,Dynamic Analysis of Intelligent Coil Leveling Machine for Cyber-physical Systems Implementation,https://api.elsevier.com/content/abstract/scopus_id/85028650941,"In manufacturing industry, wider range variants and personalized productions are becoming formidable challenges that needed to be for smart manufacturing. In smart manufacturing, machines are connected cooperatively to seamlessly and quickly adjust production setting to reach market requirements. Furthermore, real-time production data visualization and evaluation are the keys to increase manufacturing productivity, efficiency, and flexibility. This integrated research is aimed to develop an intelligent coil leveling machine through dynamic analysis of real-time machine sensors network for cyber-physical systems implementation in smart manufacturing. In this proposed intelligent coil leveling machine, intelligent sensors network is embedded in the machine to allow real-time monitoring of the machine through feedback controlled system and cloud network to ensure optimized production with optimal machine setting instantly. Intelligent sensors network of the proposed coil leveling machine such as leveling roller indentation, leveling force, and coil curvature has been completed. Preliminary real-time dynamic monitoring of the leveling rollers and coil curvature has been accomplished. Following, real-time dynamic analysis is performed to demonstrate the implementation of the cyber-physical systems where machine learning intelligence can be achieved. Lastly, real-time cloud network monitoring are implemented to allow users to collect manufacturing data online. Through this research, conventional leveling machine can be transformed in which machine setting configurations can be adjusted to the production line through virtual cyber-physical system. Production data can be visualized and evaluated in real-time with precise and intelligent production strategies to ensure customer's requirements and to enhance production efficiency and flexibility in smart manufacturing of sheet metal coil.",industry
10.1016/j.procir.2017.03.125,Conference Proceeding,Procedia CIRP,scopus,2017-01-01,sciencedirect,Ant Colony Optimization Algorithms to Enable Dynamic Milkrun Logistics,https://api.elsevier.com/content/abstract/scopus_id/85028644167,"Flexibility in combination with high capacities are the main reasons for milkruns being one of the most popular intralogistics solutions. In most cases they are only used for static routes to always deliver the same material to the same stations. However, in the context of Industry 4.0, milkrun logistic also has become very popular for use cases where different materials have to be delivered to different stations in little time, so routes cannot be planned in advance anymore. As loading and unloading the milkrun requires a significant amount of time, beside the routing problem itself, both driving and loading times have to be taken into account. Especially in scenarios where high flexibility is required those times will vary significantly and thus are a crucial factor for obtaining the optimal solution. Although containing stochastic components, those times can be predicted by considering the optimal point of time for delivery. In consequence, the best tradeoff between short routes and optimal delivery times is in favor of the shortest route. To solve this optimization problem a biology-inspired method – the ant colony optimization algorithm – has been enhanced to obtain the best solution regarding the above-mentioned aspects. A manufacturing scenario was used to prove the ability of the algorithm in real world problems. It also demonstrates the ability to adapt to changes in manufacturing systems very quickly by dynamically modelling and simulating the processes in intralogistics. The paper describes the ant colony optimization algorithm with the necessary extensions to enable it for milkrun logistic problems. Additionally the implemented software environment to apply the algorithm in practice is explained.",industry
10.1016/j.petrol.2016.11.033,Journal,Journal of Petroleum Science and Engineering,scopus,2017-01-01,sciencedirect,A hybrid particle swarm optimization and support vector regression model for modelling permeability prediction of hydrocarbon reservoir,https://api.elsevier.com/content/abstract/scopus_id/85028257367,"The significance of accurate permeability prediction cannot be over-emphasized in oil and gas reservoir characterization. Support vector machine regression (SVR), a computational intelligence technique, has been very successful in the estimation of permeability and has been widely deployed due to its unique features. However, careful selection of SVR hyper-parameters is highly essential to its optimum performance and this task is traditionally done using trial and error approach (TE-SVR) which takes a lot of time and do not guarantee optimal selection of the hyper-parameters. In this work, the performance of particle swarm optimization (PSO) technique, a heuristic optimization technique, is investigated for the optimal selection of SVR hyper-parameters for the first time in modelling and characterization of hydrocarbon reservoir. The technique is capable of automatic selection of the optimum combination of SVR hyper-parameters resulting in higher predictive accuracy and generalization ability of the developed model. The resulting PSO-SVR model is compared to SVR models whose parameters are obtained through random search (RAND-SVR) and trial and error approach (TE-SVR). The comparison is done using real-life industrial datasets obtained during petroleum exploration from four distinct oil wells located in a Middle Eastern oil and gas field. Simulation results indicate that the PSO-SVR model outperforms all the other models. Error reduction of 15.1%, 26.15%, 12.32% and 7.1% are recorded for PSO-SVR model compared to ordinary SVR (TE-SVR) in well-A, well-B, well-C and well-D, respectively. Also, reduction of 12.8%, 23.97%, 2.51% and 0.11 are recorded when PSO-SVR and RAND-SVR results are compared in the respective wells. Furthermore, the results show the potential of the application of heuristics algorithms, such as PSO, in the optimization of computational intelligence techniques employed in hydrocarbon reservoir characterizations. Therefore, PSO technique is proposed for the optimization of SVR hyper-parameters in permeability prediction and reservoir characterization based on its superior performance over the commonly employed optimization techniques.",industry
10.1016/j.promfg.2017.07.091,Conference Proceeding,Procedia Manufacturing,scopus,2017-01-01,sciencedirect,Machine Learning-based CPS for Clustering High throughput Machining Cycle Conditions,https://api.elsevier.com/content/abstract/scopus_id/85023607399,"Cyber-physical systems (CPS) have opened up a wide range of opportunities in terms of performance analysis that can be applied directly to the machine tool industry and are useful for maintenance systems and machine designers. High-speed communication capabilities enable the data to be gathered, pre-processed and processed for the purpose of machine diagnosis. This paper describes a complete real-world CPS implementation cycle, ranging from machine data acquisition to processing and interpretation. In fact, the aim of this paper is to propose a CPS for machine component knowledge discovery based on clustering algorithms using real data from a machining process. Therefore, it compares three clustering algorithms –k-means, hierarchical agglomerative and Gaussian mixture models– in terms of their contribution to spindle performance knowledge during high throughput machining operation.",industry
10.1016/j.promfg.2017.04.039,Journal,Procedia Manufacturing,scopus,2017-01-01,sciencedirect,Digital Twin as Enabler for an Innovative Digital Shopfloor Management System in the ESB Logistics Learning Factory at Reutlingen - University,https://api.elsevier.com/content/abstract/scopus_id/85020859111,"Technologies for mapping the “digital twin” have been under development for approximately 20 years. Nowadays increasingly intelligent, individualized products encourages companies to respond innovatively to customer requirements and to handle the rising product variations quickly.
                  An integrated engineering network, spanning across the entire value chain, is operated to intelligently connect various company divisions, and to generate a business ecosystem for products, services and communities. The conditions for the digital twin are thereby determined in which the digital world can be fed into the real, and the real world back into the digital to deal such intelligent products with rising variations.
                  The term digital twin can be described as a digital copy of a real factory, machine, worker etc., that is created and can be independently expanded, automatically updated as well as being globally available in real time. Every real product and production site is permanently accompanied by a digital twin. First prototypes of such digital twins already exist in the ESB Logistics Learning Factory on a cloud- and app-based software that builds on a dynamic, multidimensional data and information model. A standardized language of the robot control systems via software agents and positioning systems has to be integrated. The aspect of the continuity of the real factory in the digital factory as an economical means of ensuring continuous actuality of digital models looks as the basis of changeability.
                  For the indoor localization sensor combinations that in addition to the hardware already contain the software required for the sensor data fusion should be used. Processing systems, scenario-live-simulations and digital shop floor management results in a mandatory procedural combination. Essential to the digital twin is the ability to consistently provide all subsystems with the latest state of all required information, methods and algorithms.",industry
10.1016/j.procir.2017.01.047,Conference Proceeding,Procedia CIRP,scopus,2017-01-01,sciencedirect,Using Graph-based Design Languages to Enhance the Creation of Virtual Commissioning Models,https://api.elsevier.com/content/abstract/scopus_id/85020027557,"‘Industrie 4.0’ based production systems are likely to change the way future products are manufactured. As information technology gains influence on these systems there is a chance of higher flexibility due to decentralized logic and artificial intelligence. All this leads to a higher complexity and also indeterminism is feasible. Therefore simulation technologies will become a mandatory requirement, especially virtual commissioning will get necessary as the amount of software is rising.
                  A lot of manpower is required to establish and maintain a virtual commissioning system as it needs a large database of standard components. Therefore in most cases small- and mid-sized companies are forced to avoid such technologies. Using graph-based design languages to create virtual commissioning models can help to solve this problem. The basic principle is to shape an abstract model of a production system which will then be individually built within the domain specific tools. One of these should be a virtual commissioning tool to evaluate the functionality of the built model. If a change in the design is necessary, the new virtual commissioning model can be regenerated automatically. This approach is even more reasonable, if graph-based design languages are used throughout the whole product life cycle.",industry
10.1016/j.procs.2017.01.213,Conference Proceeding,Procedia Computer Science,scopus,2017-01-01,sciencedirect,Hybrid Agents Implementation for the Control of the Construction Company,https://api.elsevier.com/content/abstract/scopus_id/85016095509,"Planning the project duration together with separate works is an essential element of managing the construction. The final duration depends on multiple factors, including the funds, customer requests, and capabilities of the construction company. In order to avoid additional costs in penalties or additional expenses, the management needs to estimate the real construction duration in advance, before the contract is signed. Further on, these terms need to be monitored both in whole and for the specific jobs in order to be able to edit further stages with regard of the remaining time, resources and used resources ratio. The development of a decision support system for the construction company is a pressing problem due to the growing demand in decision making persons’ labor automation in planning and monitoring the construction processes. The paper presents the model and the application experience for such a system.",industry
10.1016/j.rser.2016.11.046,Journal,Renewable and Sustainable Energy Reviews,scopus,2017-01-01,sciencedirect,Comparison of different discharge strategies of grid-connected residential PV systems with energy storage in perspective of optimal battery energy storage system sizing,https://api.elsevier.com/content/abstract/scopus_id/85006456716,"The paper presents a yearly comparison of different residential self-consumption-reducing discharge strategies for grid connected residential PV systems with the Battery Energy Storage System (BESS). Altogether, three discharge strategies are taken into consideration; base case, adaptive algorithm and an energy-market-oriented remote-controlled strategy. All of the presented strategies are strictly limited to available residential load reduction based on the current BESS regulations. Furthermore, the simulations are run on real-world measurement data. The adaptive “grid-friendly” discharge algorithm utilizes global optima and a moving-average calculation to maximize the self-energy consumption in regards to the peak grid load periods. On the other hand a remote-controlled discharge scenario is taken into consideration to maximize self-consumption and to decrease the grid load when the intraday energy prices are the highest, if there is opportunity to do so. The system size optimization equations are based on a hidden layer feedforward neural network system. The main goal of the network is to predict the corresponding equations for the optimization software. Based on the neural network results, an “easy-to-use” BESS-size economic optimization web-based application has been developed to demonstrate the feasibility of the different discharge methodologies and to make easier different manufactures’ BESS system comparisons.",industry
10.1016/j.paerosci.2016.10.001,Journal,Progress in Aerospace Sciences,scopus,2017-01-01,sciencedirect,An evolutionary outlook of air traffic flow management techniques,https://api.elsevier.com/content/abstract/scopus_id/85006413338,"In recent years Air Traffic Flow Management (ATFM) has become pertinent even in regions without sustained overload conditions caused by dense traffic operations. Increasing traffic volumes in the face of constrained resources has created peak congestion at specific locations and times in many areas of the world. Increased environmental awareness and economic drivers have combined to create a resurgent interest in ATFM as evidenced by a spate of recent ATFM conferences and workshops mediated by official bodies such as ICAO, IATA, CANSO the FAA and Eurocontrol. Significant ATFM acquisitions in the last 5 years include South Africa, Australia and India. Singapore, Thailand and Korea are all expected to procure ATFM systems within a year while China is expected to develop a bespoke system. Asia-Pacific nations are particularly pro-active given the traffic growth projections for the region (by 2050 half of all air traffic will be to, from or within the Asia-Pacific region). National authorities now have access to recently published international standards to guide the development of national and regional operational concepts for ATFM, geared to Communications, Navigation, Surveillance/Air Traffic Management and Avionics (CNS+A) evolutions. This paper critically reviews the field to determine which ATFM research and development efforts hold the best promise for practical technological implementations, offering clear benefits both in terms of enhanced safety and efficiency in times of growing air traffic. An evolutionary approach is adopted starting from an ontology of current ATFM techniques and proceeding to identify the technological and regulatory evolutions required in the future CNS+A context, as the aviation industry moves forward with a clearer understanding of emerging operational needs, the geo-political realities of regional collaboration and the impending needs of global harmonisation.",industry
10.1016/j.ijpe.2016.10.021,Journal,International Journal of Production Economics,scopus,2017-01-01,sciencedirect,Single-hidden layer neural networks for forecasting intermittent demand,https://api.elsevier.com/content/abstract/scopus_id/84994731834,"Managing intermittent demand is a vital task in several industrial contexts, and good forecasting ability is a fundamental prerequisite for an efficient inventory control system in stochastic environments. In recent years, research has been conducted on single-hidden layer feedforward neural networks, with promising results. In particular, back-propagation has been adopted as a gradient descent-based algorithm for training networks. However, when managing a large number of items, it is not feasible to optimize networks at item level, due to the effort required for tuning the parameters during the training stage. A simpler and faster learning algorithm, called the extreme learning machine, has been therefore proposed in the literature to address this issue, but it has never been tried for forecasting intermittent demand. On the one hand, an extensive comparison of single-hidden layer networks trained by back-propagation is required to improve our understanding of them as predictors of intermittent demand. On the other hand, it is also worth testing extreme learning machines in this context, because of their lower computational complexity and good generalisation ability.
                  In this paper, neural networks trained by back-propagation and extreme learning machines are compared with benchmark neural networks, as well as standard forecasting methods for intermittent demand on real-time series, by combining different input patterns and architectures. A statistical analysis is then conducted to validate the best performance through different aggregation levels. Finally, some insights for practitioners are presented to improve the potential of neural networks for implementation in real environments.",industry
10.1016/j.energy.2016.09.096,Journal,Energy,scopus,2016-12-01,sciencedirect,Generation of realistic scenarios for multi-agent simulation of electricity markets,https://api.elsevier.com/content/abstract/scopus_id/84988734772,"Most market operators provide daily data on several market processes, including the results of all market transactions. The use of such data by electricity market simulators is essential for simulations quality, enabling the modelling of market behaviour in a much more realistic and efficient way. RealScen (Realistic Scenarios Generator) is a tool that creates realistic scenarios according to the purpose of the simulation: representing reality as it is, or on a smaller scale but still as representative as possible. This paper presents a novel methodology that enables RealScen to collect real electricity markets information and using it to represent market participants, as well as modelling their characteristics and behaviours. This is done using data analysis combined with artificial intelligence. This paper analyses the way players' characteristics are modelled, particularly in their representation in a smaller scale, simplifying the simulation while maintaining the quality of results. A study is also conducted, comparing real electricity market values with the market results achieved using the generated scenarios. The conducted study shows that the scenarios can fully represent the reality, or approximate it through a reduced number of representative software agents. As a result, the proposed methodology enables RealScen to represent markets behaviour, allowing the study and understanding of the interactions between market entities, and the study of new markets by assuring the realism of simulations.",industry
10.1016/j.epsr.2016.07.018,Journal,Electric Power Systems Research,scopus,2016-12-01,sciencedirect,Classification for consumption data in smart grid based on forecasting time series,https://api.elsevier.com/content/abstract/scopus_id/84981303127,"One of the most important tasks of present day smart grid implementations is to classify different types of consumers (households, office buildings and industrial plants) because they may be served by the power supplier with different parameters, rates, contracts.
                  In this paper, we propose a novel classification scheme for smart grid systems where the collected data are processed in order to increase the efficiency of electricity transportation as well as demand-supply management. The new scheme is based on forecasting the consumption time series obtained from a smart meter. Class assignment is determined using the forecast error. Different linear and nonlinear methods were tested based on the corresponding assumptions on the statistical behavior of the underlying consumption time series.
                  Performance tests were carried out with simulations in order to demonstrate the capabilities and to compare the achieved performance of the proposed scheme with existing solutions. The simulations have been executed using (i) artificially generated consumption data, which data came from a bottom-up semi-Markov model and (ii) real, measured power consumption data as well. The parameters of the model have been validated on real data. The numerical results have demonstrated that our method can better model and classify the consumption patterns of office-buildings than the existing methods. As a result, the proposed method may prove to be a promising classification tool.",industry
10.1016/j.eswa.2016.06.043,Journal,Expert Systems with Applications,scopus,2016-11-30,sciencedirect,Data quality assessment of maintenance reporting procedures,https://api.elsevier.com/content/abstract/scopus_id/84977274884,"Today’s largest and fastest growing companies’ assets are no longer physical, but rather digital (software, algorithms...). This is all the more true in the manufacturing, and particularly in the maintenance sector where quality of enterprise maintenance services are closely linked to the quality of maintenance data reporting procedures. If quality of the reported data is too low, it can results in wrong decision-making and loss of money. Furthermore, various maintenance experts are involved and directly concerned about the quality of enterprises’ daily maintenance data reporting (e.g., maintenance planners, plant managers...), each one having specific needs and responsibilities. To address this Multi-Criteria Decision Making (MCDM) problem, and since data quality is hardly considered in existing expert maintenance systems, this paper develops a maintenance reporting quality assessment (MRQA) dashboard that enables any company stakeholder to easily – and in real-time – assess/rank company branch offices in terms of maintenance reporting quality. From a theoretical standpoint, AHP is used to integrate various data quality dimensions as well as expert preferences. A use case describes how the proposed MRQA dashboard is being used by a Finnish multinational equipment manufacturer to assess and enhance reporting practices in a specific or a group of branch offices.",industry
10.1016/j.resconrec.2016.03.012,Journal,"Resources, Conservation and Recycling",scopus,2016-11-01,sciencedirect,Implementation of OPTIMASS to optimise municipal wastewater sludge processing chains: Proof of concept,https://api.elsevier.com/content/abstract/scopus_id/85028239611,"In sludge management, sludge is increasingly perceived as a marketable product rather than as a waste material. This awareness in combination with the variety of factors influencing the optimal management strategy and disposal route, introduces the need to optimise the sludge treatment throughout the whole chain instead of only minimising its production. In this paper, OPTIMASS, a mixed integer linear programming model to optimise strategic and tactical decisions in biomass-based supply chains, is proposed in order to meet this need. The applicability of OPTIMASS is illustrated through its implementation with a view to minimise the overall global warming impact of a real municipal wastewater sludge processing chain in “region X”. A first scenario addresses the optimisation of the allocation and treatment of municipal wastewater sludge within the current network. Second, OPTIMASS is used to identify the optimal location(s) for new drying facilities in this chain. Finally, the effect on the optimal chain of changes in municipal wastewater sludge production and of changes in global warming impact of the cement industry as a disposal route is evaluated.
                  The analysis reveals that municipal wastewater sludge processing chains can be considered to be instances of the generic biomass-based supply chain and that the OPTIMASS tool can be applied to support strategic and tactical decisions for optimising sludge management in case new technologies, new treatment facility locations, new disposal options, etc. are at stake. The validity of the OPTIMASS approach is confirmed by the close correspondence between its outcome and the results of a decision support system, specifically developed for the municipal wastewater sludge processing chain.",industry
10.1016/j.cie.2016.07.031,Journal,Computers and Industrial Engineering,scopus,2016-11-01,sciencedirect,TIMSPAT – Reachability graph search-based optimization tool for colored Petri net-based scheduling,https://api.elsevier.com/content/abstract/scopus_id/84991205081,"The combination of Petri net (PN) modeling with AI-based heuristic search (HS) algorithms (PNHS) has been successfully applied as an integrated approach to deal with scheduling problems that can be transformed into a search problem in the reachability graph. While several efficient HS algorithms have been proposed albeit using timed PN, the practical application of these algorithms requires an appropriate tool to facilitate its development and analysis. However, there is a lack of tool support for the optimization of timed colored PN (TCPN) models based on the PNHS approach for schedule generation. Because of its complex data structure, TCPN-based scheduling has often been limited to simulation-based performance analysis only. Also, it is quite difficult to evaluate the strength and tractability of algorithms for different scheduling scenarios due to the different computing platforms, programming languages and data structures employed. In this light, this paper presents a new tool called TIMSPAT, developed to overcome the shortcomings of existing tools. Some features that distinguish this tool are the collection of several HS algorithms, XML-based model integration, the event-driven exploration of the timed state space including its condensed variant, localized enabling of transitions, the introduction of static place, and the easy-to-use syntax statements. The tool is easily extensible and can be integrated as a component into existing PN simulators and software environments. A comparative study is performed on a real-world eyeglass production system to demonstrate the application of the tool for scheduling purposes.",industry
10.1016/j.csi.2016.03.003,Journal,Computer Standards and Interfaces,scopus,2016-11-01,sciencedirect,Intelligent software product line configurations: A literature review,https://api.elsevier.com/content/abstract/scopus_id/84964669931,"A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product.",industry
10.1016/j.knosys.2016.07.022,Journal,Knowledge-Based Systems,scopus,2016-10-15,sciencedirect,Software test quality rating: A paradigm shift in swarm computing for software certification,https://api.elsevier.com/content/abstract/scopus_id/84979704090,"Recently, software quality issues have emerged to be recognized as a fundamental point as we actualize an extensive growth of organizations involved in software industries. Still, these organizations cannot ensure the quality of their products; therefore abandoning customers in uncertainties. Software certification is the branch of quality by means that quality requires to be measured prior to certification admitting process. However, creating an official certification model is difficult due to the deficiency of data in the domain of software engineering. This research participates in solving the problem of assessing software quality by introducing a model that handles a fuzzy inference engine to mix both of the processes–driven and application-driven quality assurance procedures. The fundamental purpose of the suggested model is to enhance the compactness and the interpretability of the system's fuzzy rules via engaging an ant colony optimization algorithm (ACO), which attempts to discover a good rule description by a set of compound rules initially represented with traditional single rules. The proposed model is a fitting one that can be seen as practicing certification models that have already been created from software quality domain data and modifying them to a context-specific data. The model has been tested by a case study and the results have confirmed feasibility and practicality of the model in a real environment.",industry
10.1016/j.jclepro.2016.05.091,Journal,Journal of Cleaner Production,scopus,2016-10-01,sciencedirect,Developing an ant colony approach for green closed-loop supply chain network design: a case study in gold industry,https://api.elsevier.com/content/abstract/scopus_id/84988844589,"The forward/reverse logistics network design is an important and strategic issue due to its effects on efficiency and responsiveness of a supply chain. In practice, it is needed to formulate and solve real problems through efficient algorithms in a reasonable time. Hence, this paper tries to cover real case problem with a multi-objective model and an integrated forward/reverse logistics network design. Further, the model is customized and implemented for a case study in gold industry where the reverse logistics play crucial role. A new solution approach is applied for the proposed 7-layer network of the case study and the solutions are achieved in order solve the current difficulties of the investigated supply chain. This paper seeks to address how a multi objective logistics model in the gold industry can be created and solved through an efficient meta-heuristic algorithm. A green approach based on the CO2 emission is considered in the network design approach. The developed model includes four echelons in the forward direction and three echelons in the reverse. First, an integer linear programming model is developed to minimize costs and emissions. Then, in order to solve the model, an algorithm based on ant colony optimization is developed. The performance of the proposed algorithm has been compared with the optimum solutions of the LINGO software through various numerical examples based on the random data and real-world instances. The evaluation studies demonstrate that the proposed model is practical and applicable and the developed algorithm is reliable and efficient. The results prove the managerial implications of the model and the solution approach in terms of presenting appropriate modifications to the mangers of the selected supply chain. Further, a Taguchi-based parameter setting is undertaken to ensure using the appropriate parameters for the algorithm.",industry
10.1016/j.adhoc.2016.06.011,Journal,Ad Hoc Networks,scopus,2016-10-01,sciencedirect,Feature selection for performance characterization in multi-hop wireless sensor networks,https://api.elsevier.com/content/abstract/scopus_id/84977657790,"Current trends in Wireless Sensor Networks are faced with the challenge of shifting from testbeds in controlled environments to real-life deployments, characterized by unattended and long-term operation. The network performance in such settings depends on various factors, ranging from the operational space, the behavior of the protocol stack, the intra-network dynamics, and the status of each individual node. As such, characterizing the network’s high-level performance based exclusively on link-quality estimation, can yield episodic snapshots on the performance of specific, point-to-point links. The objective of this work is to provide an integrated framework for the unsupervised selection of the dominant features that have crucial impact on the performance of end-to-end links, established over a multi-hop topology. Our focus is on compressing the original feature vector of network parameters, by eliminating redundant network attributes with predictable behavior. The proposed approach is implemented alongside different cases of protocol stacks and evaluated on data collected from real-life deployments in rural and industrial environments. Discussions on the efficacy of the proposed scheme, and the dominant network characteristics per deployment are offered.",industry
10.1016/j.ijpe.2016.06.005,Journal,International Journal of Production Economics,scopus,2016-09-01,sciencedirect,Hybrid flow shop batching and scheduling with a bi-criteria objective,https://api.elsevier.com/content/abstract/scopus_id/84975859979,"This paper addresses the hybrid flow shop batching and scheduling problem where sequence-dependent family setup times are present and the objective is to simultaneously minimize the weighted sum of the total weighted completion time and total weighted tardiness. In particular, it disregards the group technology assumptions by allowing for the possibility of splitting pre-determined groups of jobs into inconsistent batches in order to improve the operational efficiency. A benchmark of small size problems is considered to show the benefits of batching on group scheduling. Since the problem is strongly NP-hard, several algorithms based upon tabu search are developed at three levels, which move back and forth between batching and scheduling phases. Two algorithms incorporate tabu search into the framework of path-relinking to exploit the information on good solutions. These tabu search/path-relinking algorithms comprise several distinguishing features including two relinking procedures to effectively construct paths and the stage-based improvement procedure to consider the move interdependency. The best tabu search algorithm as a local search algorithm is compared to a population-based algorithm, and the superiority of the former over the latter is shown using a statistical experiment. The initial solution finding mechanism is implemented to trigger the search into the solution space. The efficiency and effectiveness of the best algorithm is verified with the help of the results found by CPLEX. The results show that the best algorithm, based on tabu search/path relinking and the stage-based improvement procedure, could find solutions at least as good as CPLEX, but in drastically shorter computational time. In order to reflect the real industry requirements, dynamic machine availability times, dynamic job release times, machine eligibility and machine capability for processing jobs, desired lower bounds on batch sizes, and job skipping are considered.",industry
10.1016/j.jpowsour.2016.05.092,Journal,Journal of Power Sources,scopus,2016-08-30,sciencedirect,Prognostics of Proton Exchange Membrane Fuel Cells stack using an ensemble of constraints based connectionist networks,https://api.elsevier.com/content/abstract/scopus_id/84975104897,"Proton Exchange Membrane Fuel Cell (PEMFC) is considered the most versatile among available fuel cell technologies, which qualify for diverse applications. However, the large-scale industrial deployment of PEMFCs is limited due to their short life span and high exploitation costs. Therefore, ensuring fuel cell service for a long duration is of vital importance, which has led to Prognostics and Health Management of fuel cells. More precisely, prognostics of PEMFC is major area of focus nowadays, which aims at identifying degradation of PEMFC stack at early stages and estimating its Remaining Useful Life (RUL) for life cycle management. This paper presents a data-driven approach for prognostics of PEMFC stack using an ensemble of constraint based Summation Wavelet- Extreme Learning Machine (SW-ELM) models. This development aim at improving the robustness and applicability of prognostics of PEMFC for an online application, with limited learning data. The proposed approach is applied to real data from two different PEMFC stacks and compared with ensembles of well known connectionist algorithms. The results comparison on long-term prognostics of both PEMFC stacks validates our proposition.",industry
10.1016/j.cviu.2016.03.018,Journal,Computer Vision and Image Understanding,scopus,2016-07-01,sciencedirect,"Wize Mirror-a smart, multisensory cardio-metabolic risk monitoring system",https://api.elsevier.com/content/abstract/scopus_id/84963813154,"In the recent years personal health monitoring systems have been gaining popularity, both as a result of the pull from the general population, keen to improve well-being and early detection of possibly serious health conditions and the push from the industry eager to translate the current significant progress in computer vision and machine learning into commercial products. One of such systems is the Wize Mirror, built as a result of the FP7 funded SEMEOTICONS (SEMEiotic Oriented Technology for Individuals CardiOmetabolic risk self-assessmeNt and Self-monitoring) project. The project aims to translate the semeiotic code of the human face into computational descriptors and measures, automatically extracted from videos, multispectral images, and 3D scans of the face. The multisensory platform, being developed as the result of that project, in the form of a smart mirror, looks for signs related to cardio-metabolic risks. The goal is to enable users to self-monitor their well-being status over time and improve their life-style via tailored user guidance. This paper is focused on the description of the part of that system, utilising computer vision and machine learning techniques to perform 3D morphological analysis of the face and recognition of psycho-somatic status both linked with cardio-metabolic risks. The paper describes the concepts, methods and the developed implementations as well as reports on the results obtained on both real and synthetic datasets.",industry
10.1016/j.jobe.2016.04.010,Journal,Journal of Building Engineering,scopus,2016-06-01,sciencedirect,"Modeling and simulation controlling system of HVAC using fuzzy and predictive (radial basis function, RBF) controllers",https://api.elsevier.com/content/abstract/scopus_id/84965095816,"Heating, ventilating and air conditioning (HVAC) systems are used in buildings, industry and agriculture to provide thermal and humidity comfort. Modeling of HVAC system can help to design precise controlling systems. In this study, a HVAC system had been modeled using MATLAB simulation software that had been developed using a fuzzy controlling system and radial basis function (RBF) model of artificial neural network (ANN) as a predictive control system. Results of the modeled systems were extracted and compared with actual system. In order to compare results of the modeled and actual systems, comparing parameters, such as mean absolute error (MAE), root mean square error (RMSE), mean absolute percentage/relative error (MAPE) and coefficient of Pearson correlation (r) were applied. The results indicated that, the modeled systems was accurately controlling the system and the difference between real and modeled system was also close. In the results as a whole, the predictive controller (RBF network) has the best performance compared to fuzzy model.",industry
10.1016/j.epsr.2016.03.012,Journal,Electric Power Systems Research,scopus,2016-06-01,sciencedirect,Metalearning to support competitive electricity market players' strategic bidding,https://api.elsevier.com/content/abstract/scopus_id/84962522494,"Electricity markets are becoming more competitive, to some extent due to the increasing number of players that have moved from other sectors to the power industry. This is essentially resulting from incentives provided to distributed generation. Relevant changes in this domain are still occurring, such as the extension of national and regional markets to continental scales. Decision support tools have thereby become essential to help electricity market players in their negotiation process. This paper presents a metalearner to support electricity market players in bidding definition. The proposed metalearner uses a dynamic artificial neural network to create its own output, taking advantage on several learning algorithms already implemented in ALBidS (Adaptive Learning strategic Bidding System). The proposed metalearner considers different weights for each strategy, based on their individual performance. The metalearner's performance is analysed in scenarios based on real electricity markets data using MASCEM (Multi-Agent Simulator for Competitive Electricity Markets). Results show that the proposed metalearner is able to provide higher profits to market players when compared to other current methodologies and that results improve over time, as consequence of its learning process.",industry
10.1016/j.ins.2016.01.001,Journal,Information Sciences,scopus,2016-05-01,sciencedirect,Solving integrated process planning and scheduling problem with constructive meta-heuristics,https://api.elsevier.com/content/abstract/scopus_id/84957879888,"For product manufacturing, process planning is to select a series of manufacturing processes according to the product design specification, and scheduling is to allocate manufacturing resources such as machines and tools to these processes. It is a common problem that the process plan and the schedule are not able to cope with the changes in real time manufacturing. Integrated process planning and scheduling (IPPS) is to conduct the process planning and scheduling functions concurrently, with the aim to improve the dynamic responsiveness of the production schedule. This paper investigates the formulation and implementation of constructive meta-heuristics for solving IPPS problems. To begin with, a model representation is established to express IPPS problems with AND/OR graphs. With this model representation, a generic framework is proposed for implementing constructive meta-heuristics in the solution model. The generic framework provides a common procedure for the constructive meta-heuristics, which encapsulates the calculation of the search frontier and state transitions, and provides two interfaces for accommodating different constructive search algorithms. Ant colony optimization (ACO), a commonly-used algorithm which possesses all typical characteristics of constructive meta-heuristics, is adopted as a representative example for illustrating the implementation. Experiments and tests are conducted to validate the proposed system. The single objective minimizing the makespan is set for evaluating the performance of the proposed system. Experimental results of the benchmark problems have shown the effectiveness and high performance of the proposed approach based on the integration of the generic framework and ACO strategy.",industry
10.1016/B978-0-08-100571-2.00008-7,Book,Information Systems for the Fashion and Apparel Industry,scopus,2016-04-08,sciencedirect,Intelligent demand forecasting systems for fast fashion,https://api.elsevier.com/content/abstract/scopus_id/84979900385,"Sales forecasting in the fashion industry has been a very challenging issue for decades. Recently, the concept of fast fashion has emerged as a successful strategy for retailers. In terms of sales forecasting, this concept involves new approaches to deal with specific features such as the limited amount of historical data and shortened time for the computation. The literature review of existing methods in this domain shows that many models have been proposed. They are mainly based on artificial intelligence techniques. Among these techniques, we focus on the two models that arise as references for long-term and short-term forecasts to develop a two-stage sales forecasting system. Associated with a store replenishment model, which is inspired from a method implemented in a famous fast fashion brand, we propose to simulate our two-stage forecasting system on real data to evaluate the real benefits of advanced forecasting techniques for fast fashion retailing.",industry
10.1016/j.ymssp.2015.09.025,Journal,Mechanical Systems and Signal Processing,scopus,2016-03-01,sciencedirect,Classification of acoustic emission signals using wavelets and Random Forests: Application to localized corrosion,https://api.elsevier.com/content/abstract/scopus_id/84961163563,"This paper aims to propose a novel approach to classify acoustic emission (AE) signals deriving from corrosion experiments, even if embedded into a noisy environment. To validate this new methodology, synthetic data are first used throughout an in-depth analysis, comparing Random Forests (RF) to the k-Nearest Neighbor (k-NN) algorithm. Moreover, a new evaluation tool called the alter-class matrix (ACM) is introduced to simulate different degrees of uncertainty on labeled data for supervised classification. Then, tests on real cases involving noise and crevice corrosion are conducted, by preprocessing the waveforms including wavelet denoising and extracting a rich set of features as input of the RF algorithm. To this end, a software called RF-CAM has been developed. Results show that this approach is very efficient on ground truth data and is also very promising on real data, especially for its reliability, performance and speed, which are serious criteria for the chemical industry.",industry
10.1016/j.mechatronics.2015.06.015,Journal,Mechatronics,scopus,2016-03-01,sciencedirect,Symbolic discrete-time planning with continuous numeric action parameters for agent-controlled processes,https://api.elsevier.com/content/abstract/scopus_id/84939200231,"In industrial domains such as manufacturing control, a trend away from centralized planning and scheduling towards more flexible distributed agent-based approaches could be observed over recent years. To be of practical relevance, the local control mechanisms of the autonomous agents must be able to dependably adhere and dynamically adjust to complex numeric goal systems like business key performance indicators in an economically beneficial way. However, planning with numeric state variables and objectives still poses a challenging task within the field of artificial intelligence (AI).
                  In this article, a new general-purpose AI planning approach is presented that operates in two stages and extends existing domain-independent modeling formalisms like PDDL with continuous (i.e., infinite-domain) numeric action parameters, which are currently still unsupported by state-of-the-art AI planners. In doing so, it enables the solution of mathematical optimization problems at the action level of the planning tasks, which are inherent to many real-world control problems. To deal with certain difficulties concerning reliable and fast detection of action applicability that arise when planning with real-valued action parameters, the implemented planner allows resorting to an adjustable “satisficing” strategy by means of partial execution and subsequent repair of infeasible plans over the course of time. The functioning of the system is evaluated in a multi-agent simulation of a shop floor control scenario with focus on the effects the possible problem cases and different degrees of satisficing have on attained plan quality and total planning time. As the results demonstrate the basic practicability of the approach for the given setting, this contribution constitutes an important step towards the effective and dependable integration of complex numeric goal systems and non-linear multi-criteria optimization tasks into autonomous agent-controlled industrial processes in a reusable, domain-independent way.",industry
10.1016/j.seppur.2015.12.056,Journal,Separation and Purification Technology,scopus,2016-02-29,sciencedirect,Rapid cultivation of aerobic granule for the treatment of solvent recovery raffinate in a bench scale sequencing batch reactor,https://api.elsevier.com/content/abstract/scopus_id/84954169665,"Aerobic granular sludge (AGS) was cultivated in a bench scale sequencing batch reactor within 21days. Strategy of the rapid startup was inoculated with part of mature AGS during cultivation, while aerobic biological selector was implemented for the inhibiting outgrowth of filamentous bacteria and fast selection of zoogloea bacteria. Then, the cultivated AGS was employed for the treatment of solvent recovery raffinate. Stable AGS was successfully domesticated after 55days under strategy of gradually increase the proportion of real wastewater in influent. The domesticated AGS was orange, irregular shape, smooth and compact. SVI, SV30/SV5, MLVSS/MLSS, EPS, PN/PS, average particle size, granulation rate, (SOUR)H and (SOUR)N of AGS were 19.06mL/g, 0.97, 0.55, 30.05mg/g MLVSS, 1.10, 1.28mm, 98.87%, 32.47 and 7.97mg O2/hgVSS respectively. Finally, COD, TIN, NH4
                     +–N and TP of the effluent were lower than 25.9mg/L, 1.64mg/L, 1.13mg/L and 0.21mg/L, and their removal rate was more than 98.43%, 97.12%, 98.02% and 98.09% respectively. Thus, COD, TP removal, nitrification and denitrification were realized in a single bioreactor. The result indicated that the feasibility of AGS for high C/N ratio industrial wastewater treatment.",industry
10.1016/j.ifacol.2016.11.160,Conference Proceeding,IFAC-PapersOnLine,scopus,2016-01-01,sciencedirect,Neural networks as a diagnosing tool for industrial level measurement through non-contacting radar type and support to the decision for its better application,https://api.elsevier.com/content/abstract/scopus_id/85006454620,"The aim of this study was to develop an analysis tool based on artificial neural networks (ANN) to detect level measurement problems with free wave propagation radars. The trend of using this type of radar has been growing in the last ten years mainly because of its easy installation on the top of tanks and reservoirs, and for its low rate maintenance comparing to other level measurement technologies. For the experiments, a Rosemount radar was used and the training of the neural network was based on the data from the software Radar Master. Therefore, some network topologies in different scenarios were tested and it was possible to demonstrate the efficiency of the ANN with accuracy rate between 94.44 to 100% for the first experiment with networks using 10, 20 or 50 neurons in the hidden layer. This technique was applied in a real industrial application, a sugar and ethanol mill, and accuracy rate was about 87,0 to 96,1%. This methodology can be applied to asset management software for diagnosis report or troubleshooting which would increase the level measurement reliability and plant safety.",industry
10.1016/j.procir.2016.06.096,Conference Proceeding,Procedia CIRP,scopus,2016-01-01,sciencedirect,Transfer of Model of Innovative Smart Factory to Croatian Economy Using Lean Learning Factory,https://api.elsevier.com/content/abstract/scopus_id/84999791899,"Croatia's manufacturing industry faces many problems and obstacles that have a large impact on its competitiveness. Insufficiently educated and unskilled personnel, particularly in the production and management fields, are decreasing competitiveness that is necessary for survival in the global market. Objective of project Innovative Smart Enterprise is to establish a special learning environment in one Laboratory asLean Learning Factory, i.e. simulation of a real factory through specialized equipment. The Lean Learning Factory's mission is to integrate needed knowledge into the engineering curriculum. Therefore, Lean Learning Factory at University of Split is in continuous developing process to support practice-based engineering curriculum with possibility of learning necessary tools and methods, using didactic games or real life products and equipment. Solution proposal for best balance between toys and real products consider design and production line development for product Karet. It is a traditional and original product from Croatia, so it will raise enthusiasmin learning process in both students and industry employees. Two assembly lines will be developed, one traditionally equipped and one intelligent, networked, flexible, and fully improved by Lean tools. By deeper analysis of both assembly lines, hybrid assembly lines could be designed, to balance on one side assembly tact time according to customer demand and total cost of installation and running on the other side. Methods and tools adapted and implemented, in both design and analysis process for optimization of this hybrid assembly line would be scaled and adjusted for industry use as part as knowledge transfer from university to enterprises.",industry
10.1016/j.proeng.2016.07.416,Conference Proceeding,Procedia Engineering,scopus,2016-01-01,sciencedirect,Automated Detection of Faults in Wastewater Pipes from CCTV Footage by Using Random Forests,https://api.elsevier.com/content/abstract/scopus_id/84997822163,"Sewer systems require regular inspection in order to ensure their satisfactory condition. As most sewer networks consist of pipes too small for engineers to traverse, CCTV footage is used to record the interior of these pipes. This footage is manually analysed by qualified engineers, to determine the condition of the pipe and the presence of any faults. We propose a methodology, which automatically detects faults within the CCTV footage. This has the potential to dramatically reduce the time required to process the large volume of CCTV footage produced during a survey. The proposed methodology first characterises localised regions of each video frame using multiscale GIST features. Extremely randomised trees are then used to learn a classifier that distinguishes between frames showing a fault and normal frames. The technique is tested on 670 video segments from real sewer inspections of a variety of pipes, supplied by Wessex Water. Detection performance is assessed by plotting receiver operating characteristics and quantifying the area under the curve. Preliminary results indicate high detection accuracy of 88% and an area under the ROC curve of 96%. The machine learning used reduces the footage to a selection of frames containing faults, which can be quickly identified (whether by an engineer or another piece of software), showing promise for use in industrial wastewater network surveys.",industry
10.1016/j.ifacol.2016.07.167,Conference Proceeding,IFAC-PapersOnLine,scopus,2016-01-01,sciencedirect,A proposal for teaching SCADA systems using Virtual Industrial Plants in Engineering Education,https://api.elsevier.com/content/abstract/scopus_id/84994803392,"The main objectives of SCADA (Supervisory Control And Data Acquisition) systems are the supervisory analysis of the system, control algorithms validation, and data acquisition. These systems are normally implemented according to the international standards: UNE-EN ISO 9241, ISAIOI-Human-Machine Interfaces, ISA S5, and in the case treated in this paper The Spanish Royal Decree 488/1997. This paper presents a software architecture for the development of educational laboratories, through industrial virtual plants which models and logic are implemented in Matlab® and used within LabVIEW® through an appropriate protocol. Lab VIEW® from National Instruments, a specific purpose software for this kind of applications, was used, since it allows us to provide a friendly interface, to perform communications, data acquisition and the information management. In addition, to illustrate the use of the proposed architecture, different virtual industrial plants for students of different Bachelor and Master degrees in engineering at the University of Almeria have been developed. This paper shows the different virtual industrial plants that have been developed using SCADA systems to facilitate students’ learning of basic concepts and techniques for an Industrial Informatics course.",industry
10.1016/B978-0-444-63428-3.50407-0,Book Series,Computer Aided Chemical Engineering,scopus,2016-01-01,sciencedirect,Process Integration: Pinch Analysis and Mathematical Programming - Directions for Future Development,https://api.elsevier.com/content/abstract/scopus_id/84994259521,"Numerous studies have been performed process systems engineering field for improving the efficiency of supplying and using energy, water and other resources and consequently for reducing the emissions of greenhouse gases, volatile organic compounds and other pollutants, accumulating a significant body of methods, applications and results. It has become apparent that the resource inputs and effluents of industrial processes and the other units including the business centres, civic objects and even agricultural plants can and are often connected with each other. Most industrial plants and the other units throughout the world still use more energy and water than necessary, they are proven cases in the range 20 – 30 %, emitting too large volumes of Greenhouse Gases and other pollutants.
                  Water-saving measures and the reuse of water may reduce groundwater consumption by as much as 25 – 30 %. Usually reducing resource consumption is achieved by increasing internal recycling and the reuse of energy and material streams. Projects for improving process resource efficiencies can be very beneficial and also potentially improve the public perception of the companies.
                  Motivating, launching and carrying out such projects, however, involve appropriate optimisation, based on adequate process models, applied within the framework of appropriate resource minimisation strategies and procedures. Process Integration supporting process design, integration and optimisation has been around for nearly 45 years. It has been closely related to the development of process systems engineering, as well as utilising mathematical modelling and information technology.
                  In the broader sense Process Integration methods can be classified into those relying on process based insight and targeting on the one hand, mainly employing targeting, heuristics and artificial intelligence—AI. On the other hand are the methods employing detailed mathematical models usually implemented as algebraic models with embedded superstructures in the case of process network synthesis. The methods relying on thermodynamic insights have been first published in the early 1980-s (Linnhoff and Flower, 1978) as well as those using mathematical programming—MP (Papoulias and Grossmann, 1983). There can also be a combined approach (Klemeš and Kravanja, 2013).
                  On the one hand, the concept relying on thermodynamic and/or physical insights using the well-known Pinch Analysis has been the more widely accepted in both academia and industry. Process Integration has thus converged towards two schools of thought, the thermodynamic based (Pinch) and the mathematically based MP, each having its own advantages and drawbacks. The thermodynamic school has mostly preceded that of the MP in generating ideas based on engineering creativity. The MP school has enacted its ideas and described them as explicit mathematical models for solving advanced PI problems.
                  The collaboration between both approaches has been widening, taking from each other the more applicable parts. Its development has been accelerating as the combined methodology has been able to provide answers and support for important issues regarding economic development—energy, water and resources better utilisation and savings. This contribution is targeted towards a short overview of recent achievements and future challenges.",industry
10.1016/j.procir.2015.12.071,Conference Proceeding,Procedia CIRP,scopus,2016-01-01,sciencedirect,Enhancing Constraint Propagation in ACO-based Schedulers for Solving the Job Shop Scheduling Problem,https://api.elsevier.com/content/abstract/scopus_id/84968773482,"Increasing number of variants lead to growing complexity in planning processes in production. Not only is the initial planning a tremendous task if there is a huge variety of products but also reacting to changes becomes more frequent and more demanding. Many algorithms being able to solve the static problem need to perform a full recalculation if there is disturbance in production which makes them too time consuming for instant reactions to changes in production.
                  Ant Colony Optimization (ACO) has proven its potentials in solving the theoretical Job Shop Scheduling Problem offering the advantage of not needing an entire recalculation in the case of changes. But when using the algorithm for calculation in real time scenarios with returning data from production plants several restrictions have to be fulfilled. The reaction to those restriction is currently not sufficiently provided by implementations of the ACO which prevents the use in practical applications. These restrictions are modelled as constraints that can for example involve the reaction to disturbances like failures or manual changes. But also considering transportation times or providing the possibility to realize batch processes is discussed. There are different possibilities to realize the reactions to restrictions in ACO, but in this paper they are modelled as constraints affecting the ACO during optimization. The constraint propagation is implemented by restricting the selection of succeeding edges, an approach that only has little impact on computational performance.
                  In this paper the concept of constraining the Ant Colony Optimization in Job Shop Scheduling is being introduced and explained. Subsequently the demand for additional constraints is presented and enhancements to the existing approach are defined and commented theoretically.",industry
10.1016/j.eswa.2015.07.016,Journal,Expert Systems with Applications,scopus,2016-01-01,sciencedirect,Applying supplier selection methodologies in a multi-stakeholder environment: A case study and a critical assessment,https://api.elsevier.com/content/abstract/scopus_id/84944345232,"In the contemporary global market, supplier selection represents a crucial process for enhancing firms’ competitiveness. In firms operating in low-complexity sectors, supplier selection generally leverages on few significant variables (price, delivery time, quality) and it is often left to the buyers’ experience. On the other hand, in industries characterised by remarkable product complexity, supplier selection systems gain the characteristics of a multi-stakeholder and multi-criteria problem, which needs to be theoretically formalised and realistically adapted to specific contexts.
                  An increasing number of researches have been devoted to the development of different methodologies to cope with this problem. Nevertheless, while the number of applications is growing, there is little empirical evidence of the practical usefulness of such tools, that are mainly tested on numerical examples or computational experiments and focused on a dyadic version of the problem, overlooking the wider set of actors involved in the decision-making problem. The result is a clear dichotomy between academic theory and business practice.
                  Therefore, the paper contributes to understand the above dichotomy by evaluating the applicability to real-world multi-stakeholder problems of the two main approaches proposed in the literature to deal with supplier selection, the analytic hierarchic process (AHP) and the fuzzy set theory (FST). Based on an industrial case study, a thorough discussion is developed, dealing with the issues arising during the implementation and practical functioning of such decision support systems, also providing provide practical guidelines and managerial implications.",industry
10.1016/j.ultsonch.2015.07.022,Journal,Ultrasonics Sonochemistry,scopus,2016-01-01,sciencedirect,"Impact of ultrasound on solid-liquid extraction of phenolic compounds from maritime pine sawdust waste. Kinetics, optimization and large scale experiments",https://api.elsevier.com/content/abstract/scopus_id/84938390300,"Maritime pine sawdust, a by-product from industry of wood transformation, has been investigated as a potential source of polyphenols which were extracted by ultrasound-assisted maceration (UAM). UAM was optimized for enhancing extraction efficiency of polyphenols and reducing time-consuming. In a first time, a preliminary study was carried out to optimize the solid/liquid ratio (6g of dry material per mL) and the particle size (0.26cm2) by conventional maceration (CVM). Under these conditions, the optimum conditions for polyphenols extraction by UAM, obtained by response surface methodology, were 0.67W/cm2 for the ultrasonic intensity (UI), 40°C for the processing temperature (T) and 43min for the sonication time (t). UAM was compared with CVM, the results showed that the quantity of polyphenols was improved by 40% (342.4 and 233.5mg of catechin equivalent per 100g of dry basis, respectively for UAM and CVM). A multistage cross-current extraction procedure allowed evaluating the real impact of UAM on the solid–liquid extraction enhancement. The potential industrialization of this procedure was implemented through a transition from a lab sonicated reactor (3L) to a large scale one with 30L volume.",industry
10.1016/j.simpat.2015.05.011,Journal,Simulation Modelling Practice and Theory,scopus,2015-11-01,sciencedirect,A flexible framework for accurate simulation of cloud in-memory data stores,https://api.elsevier.com/content/abstract/scopus_id/84947019341,"In-memory (transactional) data stores, also referred to as data grids, are recognized as a first-class data management technology for cloud platforms, thanks to their ability to match the elasticity requirements imposed by the pay-as-you-go cost model. On the other hand, determining how performance and reliability/availability of these systems vary as a function of configuration parameters, such as the amount of cache servers to be deployed, and the degree of in-memory replication of slices of data, is far from being a trivial task. Yet, it is an essential aspect of the provisioning process of cloud platforms, given that it has an impact on the amount of cloud resources that are planned for usage. To cope with the issue of predicting/analysing the behavior of different configurations of cloud in-memory data stores, in this article we present a flexible simulation framework offering skeleton simulation models that can be easily specialized in order to capture the dynamics of diverse data grid systems, such as those related to the specific (distributed) protocol used to provide data consistency and/or transactional guarantees. Besides its flexibility, another peculiar aspect of the framework lies in that it integrates simulation and machine-learning (black-box) techniques, the latter being used to capture the dynamics of the data-exchange layer (e.g. the message passing layer) across the cache servers. This is a relevant aspect when considering that the actual data-transport/networking infrastructure on top of which the data grid is deployed might be unknown, hence being not feasible to be modeled via white-box (namely purely simulative) approaches. We also provide an extended experimental study aimed at validating instances of simulation models supported by our framework against execution dynamics of real data grid systems deployed on top of either private or public cloud infrastructures. Particularly, our validation test-bed has been based on an industrial-grade open-source data grid, namely Infinispan by JBoss/Red-Hat, and a de-facto standard benchmark for NoSQL platforms, namely YCSB by Yahoo. The validation study has been conducted by relying on both public and private cloud systems, scaling the underlying infrastructure up to 100 (resp. 140) Virtual Machines for the public (resp. private) cloud case. Further, we provide some experimental data related to a scenario where our framework is used for on-line capacity planning and reconfiguration of the data grid system.",industry
10.1016/j.compbiomed.2015.07.015,Journal,Computers in Biology and Medicine,scopus,2015-11-01,sciencedirect,"Implementation of a web based universal exchange and inference language for medicine: Sparse data, probabilities and inference in data mining of clinical data repositories",https://api.elsevier.com/content/abstract/scopus_id/84941884468,"We extend Q-UEL, our universal exchange language for interoperability and inference in healthcare and biomedicine, to the more traditional fields of public health surveys. These are the type associated with screening, epidemiological and cross-sectional studies, and cohort studies in some cases similar to clinical trials. There is the challenge that there is some degree of split between frequentist notions of probability as (a) classical measures based only on the idea of counting and proportion and on classical biostatistics as used in the above conservative disciplines, and (b) more subjectivist notions of uncertainty, belief, reliability, or confidence often used in automated inference and decision support systems. Samples in the above kind of public health survey are typically small compared with our earlier “Big Data” mining efforts. An issue addressed here is how much impact on decisions should sparse data have. We describe a new Q-UEL compatible toolkit including a data analytics application DiracMiner that also delivers more standard biostatistical results, DiracBuilder that uses its output to build Hyperbolic Dirac Nets (HDN) for decision support, and HDNcoherer that ensures that probabilities are mutually consistent. Use is exemplified by participating in a real word health-screening project, and also by deployment in a industrial platform called the BioIngine, a cognitive computing platform for health management.",industry
10.1016/j.eswa.2015.04.036,Journal,Expert Systems with Applications,scopus,2015-07-28,sciencedirect,Clustering and visualization of failure modes using an evolving tree,https://api.elsevier.com/content/abstract/scopus_id/84937967581,"Despite the popularity of Failure Mode and Effect Analysis (FMEA) in a wide range of industries, two well-known shortcomings are the complexity of the FMEA worksheet and its intricacy of use. To the best of our knowledge, the use of computation techniques for solving the aforementioned shortcomings is limited. As such, the idea of clustering and visualization pertaining to the failure modes in FMEA is proposed in this paper. A neural network visualization model with an incremental learning feature, i.e., the evolving tree (ETree), is adopted to allow the failure modes in FMEA to be clustered and visualized as a tree structure. In addition, the ideas of risk interval and risk ordering for different groups of failure modes are proposed to allow the failure modes to be ordered, analyzed, and evaluated in groups. The main advantages of the proposed method lie in its ability to transform failure modes in a complex FMEA worksheet to a tree structure for better visualization, while maintaining the risk evaluation and ordering features. It can be applied to the conventional FMEA methodology without requiring additional information or data. A real world case study in the edible bird nest industry in Sarawak (Borneo Island) is used to evaluate the usefulness of the proposed method. The experiments show that the failure modes in FMEA can be effectively visualized through the tree structure. A discussion with FMEA users engaged in the case study indicates that such visualization is helpful in comprehending and analyzing the respective failure modes, as compared with those in an FMEA table. The resulting tree structure, together with risk interval and risk ordering, provides a quick and easily understandable framework to elucidate important information from complex FMEA forms; therefore facilitating the decision-making tasks by FMEA users. The significance of this study is twofold, viz., the use of a computational visualization approach to tackling two well-known shortcomings of FMEA; and the use of ETree as an effective neural network learning paradigm to facilitate FMEA implementations. These findings aim to spearhead the potential adoption of FMEA as a useful and usable risk evaluation and management tool by the wider community.",industry
10.1016/j.measurement.2015.06.004,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2015-07-06,sciencedirect,A signal pre-processing algorithm designed for the needs of hardware implementation of neural classifiers used in condition monitoring,https://api.elsevier.com/content/abstract/scopus_id/84934767009,"Gearboxes have a significant influence on the durability and reliability of a power transmission system. Currently, extensive research studies are being carried out to increase the reliability of gearboxes working in the energy industry, especially with a focus on planetary gears in wind turbines and bucket wheel excavators. In this paper, a signal pre-processing algorithm designed for condition monitoring of planetary gears working in non-stationary operation is presented. The algorithm is dedicated for hardware implementation on Field Programmable Gate Arrays (FPGAs). The purpose of the algorithm is to estimate the features of a vibration signal that are related to failures, e.g. misalignment and unbalance. These features can serve as the components of an input vector for a neural classifier. The approach proposed here has several important benefits: it is resistant to small speed fluctuations up to 7%, it can be performed in real-time conditions and its implementation does not require many resources of FPGAs.",industry
10.1016/j.ijfoodmicro.2015.03.010,Journal,International Journal of Food Microbiology,scopus,2015-07-02,sciencedirect,A strategy to establish food safety model repositories,https://api.elsevier.com/content/abstract/scopus_id/84926308733,"Transferring the knowledge of predictive microbiology into real world food manufacturing applications is still a major challenge for the whole food safety modelling community. To facilitate this process, a strategy for creating open, community driven and web-based predictive microbial model repositories is proposed. These collaborative model resources could significantly improve the transfer of knowledge from research into commercial and governmental applications and also increase efficiency, transparency and usability of predictive models. To demonstrate the feasibility, predictive models of Salmonella in beef previously published in the scientific literature were re-implemented using an open source software tool called PMM-Lab. The models were made publicly available in a Food Safety Model Repository within the OpenML for Predictive Modelling in Food community project. Three different approaches were used to create new models in the model repositories: (1) all information relevant for model re-implementation is available in a scientific publication, (2) model parameters can be imported from tabular parameter collections and (3) models have to be generated from experimental data or primary model parameters. All three approaches were demonstrated in the paper. The sample Food Safety Model Repository is available via: http://sourceforge.net/projects/microbialmodelingexchange/files/models and the PMM-Lab software can be downloaded from http://sourceforge.net/projects/pmmlab/. This work also illustrates that a standardized information exchange format for predictive microbial models, as the key component of this strategy, could be established by adoption of resources from the Systems Biology domain.",industry
10.1016/j.ifacol.2015.08.045,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-07-01,sciencedirect,Downhole pressure estimation using committee machines and neural networks,https://api.elsevier.com/content/abstract/scopus_id/84992511260,"In gas-lifted oil wells the monitoring of downhole pressure plays an important role. However, the permanent downhole gauge (PDG) sensor often fails. Because maintenance or replacement of PDGs is usually unfeasible, soft-sensors are promising alternatives to monitor the downhole pressure in the case of sensor failure. In this paper, a data-driven soft-sensor is implemented to estimate the downhole pressure using committee machines composed by finite impulse response (FIR) neural networks. Experimental results in three real datasets of the same oil well indicate that the identified soft-sensor is able to predict the downhole pressure with satisfactory accuracy. The model input variables were selected by statistical tests which increased insight concerning such variables. Committee machines outperformed single-model soft-sensors on experimental data.",industry
10.1016/j.jal.2014.11.005,Journal,Journal of Applied Logic,scopus,2015-06-01,sciencedirect,Implementation and testing of a soft computing based model predictive control on an industrial controller,https://api.elsevier.com/content/abstract/scopus_id/84922903117,"This work presents a real time testing approach of an Intelligent Multiobjective Nonlinear-Model Predictive Control Strategy (iMO-NMPC). The goal is the testing and analysis of the feasibility and reliability of some Soft Computing (SC) techniques running on a real time industrial controller. In this predictive control strategy, a Multiobjective Genetic Algorithm is used together with a Recurrent Artificial Neural Network in order to obtain the control action at each sampling time. The entire development process, from the numeric simulation of the control scheme to its implementation and testing on a PC-based industrial controller, is also presented in this paper. The computational time requirements are discussed as well. The obtained results show that the SC techniques can be considered also to tackle highly nonlinear and coupled complex control problems in real time, thus optimising and enhancing the response of the control loop. Therefore this work is a contribution to spread the SC techniques in on-line control applications, where currently they are relegated mainly to be used off-line, as is the case of optimal tuning of control strategies.",industry
10.1016/j.bios.2014.07.084,Journal,Biosensors and Bioelectronics,scopus,2015-05-05,sciencedirect,"Lytic enzymes as selectivity means for label-free, microfluidic and impedimetric detection of whole-cell bacteria using ALD-Al<inf>2</inf>O<inf>3</inf> passivated microelectrodes",https://api.elsevier.com/content/abstract/scopus_id/84922271489,"Point-of-care (PoC) diagnostics for bacterial detection offer tremendous prospects for public health care improvement. However, such tools require the complex combination of the following performances: rapidity, selectivity, sensitivity, miniaturization and affordability. To meet these specifications, this paper presents a new selectivity method involving lysostaphin together with a CMOS-compatible impedance sensor for genus-specific bacterial detection. The method enables the sample matrix to be directly flown on the polydopamine-covered sensor surface without any pre-treatment, and considerably reduces the background noise. Experimental proof-of-concept, explored by simulations and confirmed through a setup combining simultaneous optical and electrical real-time monitoring, illustrates the selective and capacitive detection of Staphylococcus epidermidis in synthetic urine also containing Enterococcus faecium. While providing capabilities for miniaturization and system integration thanks to CMOS compatibility, the sensors show a detection limit of ca. 108 (CFU/mL).min in a 1.5μL microfluidic chamber with an additional setup time of 50min. The potentials, advantages and limitations of the method are also discussed.",industry
10.1016/j.compind.2015.05.001,Journal,Computers in Industry,scopus,2015-05-04,sciencedirect,Artificial cognitive control with self-x capabilities: A case study of a micro-manufacturing process,https://api.elsevier.com/content/abstract/scopus_id/84955410573,"Nowadays, even though cognitive control architectures form an important area of research, there are many constraints on the broad application of cognitive control at an industrial level and very few systematic approaches truly inspired by biological processes, from the perspective of control engineering. Thus, our main purpose here is the emulation of human socio-cognitive skills, so as to approach control engineering problems in an effective way at an industrial level. The artificial cognitive control architecture that we propose, based on the shared circuits model of socio-cognitive skills, seeks to overcome limitations from the perspectives of computer science, neuroscience and systems engineering. The design and implementation of artificial cognitive control architecture is focused on four key areas: (i) self-optimization and self-leaning capabilities by estimation of distribution and reinforcement-learning mechanisms; (ii) portability and scalability based on low-cost computing platforms; (iii) connectivity based on middleware; and (iv) model-driven approaches. The results of simulation and real-time application to force control of micro-manufacturing processes are presented as a proof of concept. The proof of concept of force control yields good transient responses, short settling times and acceptable steady-state error. The artificial cognitive control architecture built into a low-cost computing platform demonstrates the suitability of its implementation in an industrial setup.",industry
10.1016/j.ifacol.2015.06.036,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-05-01,sciencedirect,Dexrov: Dexterous undersea inspection and maintenance in presence of communication latencies,https://api.elsevier.com/content/abstract/scopus_id/84992521813,"Underwater inspection and maintenance (e.g. in the oil & gas industry) are demanding and costly activities for which ROV based setups are often deployed in addition or in substitution to deep divers - contributing to operations risks and costs cutting. However the operation of a ROV requires significant off-shore dedicated manpower to handle and operate the robotic platform. In order to reduce the burden of operations, DexROV proposes to work out more cost effective and time efficient ROV operations, where manned support is in a large extent delocalized onshore (i.e. from a ROV control center), possibly at a large distance from the actual operations, relying on satellite communications. The proposed scheme also makes provision for advanced dexterous manipulation capabilities, exploiting human expertise when deemed useful. The outcomes of the project will be integrated and evaluated in a series of tests and evaluation campaigns, culminating with a realistic deep sea (1,300 meters) trial.",industry
10.1016/j.ifacol.2015.06.159,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-05-01,sciencedirect,A benchmark dataset for depth sensor based activity recognition in a manufacturing process,https://api.elsevier.com/content/abstract/scopus_id/84953879013,Algorithms for automated recognition of human activities are crucial for supporting the next generation of process measures in manufacturing. While there is active research underway for many sensor systems and algorithms they will need to be tested in real-world conditions in order to mature and become robust or generalized enough for broad deployment in industry. In this paper we present a case study and dataset from a real-world setting along with three performance measures for six common classifiers. The intent is to provide a dataset and baseline performance level metrics so that others may compare their activity recognition algorithms to a common standard.,industry
10.1016/j.ifacol.2015.06.228,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-05-01,sciencedirect,Multicast dataset synchronization and agent negotiation in distributed manufacturing control systems,https://api.elsevier.com/content/abstract/scopus_id/84953870369,"Multi agent systems represent an elegant approach for the control architecture of manufacturing systems. Distributed control architectures have the potential to achieve greater flexibility by being capable of local decision making based on real time reasoning. One of the main challenges of these distributed architectures is represented by the capability to synchronize the production data across all execution points in a reliable and consistent fashion. In this context, this paper aims to resolve the problems associated with real time production data synchronization in distributed multi-agent control systems by proposing a common dataset synchronized across all agent entities using multicast network communication. On top of this common dataset approach, an agent negotiation mechanism is proposed that addresses the operation sequencing and resource allocation in decentralized operation model. The pilot implementation is using JADE multi agent platform and JGroups for real time data synchronization and NetLogo for abstract representation of the simulation system. Experimental results gathered from the pilot implementation are discussed.",industry
10.1016/j.isatra.2014.11.011,Journal,ISA Transactions,scopus,2015-05-01,sciencedirect,Online monitoring and control of particle size in the grinding process using least square support vector regression and resilient back propagation neural network,https://api.elsevier.com/content/abstract/scopus_id/84929271125,"Particle size soft sensing in cement mills will be largely helpful in maintaining desired cement fineness or Blaine. Despite the growing use of vertical roller mills (VRM) for clinker grinding, very few research work is available on VRM modeling. This article reports the design of three types of feed forward neural network models and least square support vector regression (LS-SVR) model of a VRM for online monitoring of cement fineness based on mill data collected from a cement plant. In the data pre-processing step, a comparative study of the various outlier detection algorithms has been performed. Subsequently, for model development, the advantage of algorithm based data splitting over random selection is presented. The training data set obtained by use of Kennard–Stone maximal intra distance criterion (CADEX algorithm) was used for development of LS-SVR, back propagation neural network, radial basis function neural network and generalized regression neural network models. Simulation results show that resilient back propagation model performs better than RBF network, regression network and LS-SVR model. Model implementation has been done in SIMULINK platform showing the online detection of abnormal data and real time estimation of cement Blaine from the knowledge of the input variables. Finally, closed loop study shows how the model can be effectively utilized for maintaining cement fineness at desired value.",industry
10.1016/j.asoc.2015.03.034,Journal,Applied Soft Computing Journal,scopus,2015-04-30,sciencedirect,A genetic algorithm based decision support system for the multi-objective node placement problem in next wireless generation network,https://api.elsevier.com/content/abstract/scopus_id/84929176273,"The node placement problem involves positioning and configuring infrastructure for wireless networks. Applied to next generation networks, it establishes a new wireless architecture able to integrate heterogeneous components that can collaborate and exchange data. Furthermore, the heterogeneity of wireless networks makes the problem more intractable. This paper presents a novel multi-objective node placement problem that optimizes concurrently four objectives: maximizing communication coverage, minimizing the active structures’ costs, maximizing of the total capacity bandwidth and minimizing the noise level in the network. Known to be 
                        NP
                     -hard, the problem can be approached by applying heuristics mainly for large problem instances. As the number of nodes to place is not determined beforehand; we propose to apply a multi-objective variable-length genetic algorithm (VLGA) that simultaneously searches for the optimal number, positions and nature of heterogeneous nodes and communication devices. The performance of the VLGA is highlighted through the implementation of a decision support system (DSS) applied to the surveillance maritime problem using real data instances. We compare the ability of the proposed algorithm with an existing multi-objective model from the literature in order to validate its effectiveness in dealing with heterogeneous components. The results show that the proposed model well fits the network architecture constraints with a better balance between the objectives applied to the surveillance problem.",industry
10.1016/j.mee.2015.01.018,Journal,Microelectronic Engineering,scopus,2015-04-20,sciencedirect,An FPGA based human detection system with embedded platform,https://api.elsevier.com/content/abstract/scopus_id/84922572817,"Focusing on the computing speed of the practical machine learning based human detection system at the testing (detecting) stage to reach the real-time requirement in an embedded platform, the idea of iterative computing HOG with FPGA circuit design is proposed. The completed HOG accelerator contains gradient calculation circuit module and histogram accumulation circuit module. The linear SVM classification algorithm producing a number of necessary weak classifiers is combined with Adaboost algorithm to establish a strong classifier. The human detection is successfully implemented on a portable embedded platform to reduce the system cost and size. Experimental result shows that the performance error of accuracy appears merely about 0.1–0.4% in comparison between the presented FPGA based HW/SW co-design and the PC based pure software. Meanwhile, the computing speed achieves the requirement of a real-time embedded system, 15fps.",industry
10.1016/B978-0-12-800341-1.00001-2,Book,Industrial Agents: Emerging Applications of Software Agents in Industry,scopus,2015-03-12,sciencedirect,Software Agent Systems,https://api.elsevier.com/content/abstract/scopus_id/84944408386,"Agents and multi-agent systems are one of the most fascinating topics in computer science. They attracted and unified not only researchers from nearly all computer science areas but also researchers from other core disciplines such as psychology, sociology, biology, or control engineering. In the meantime, agent-based systems successfully prove their usefulness in many different real-life application areas, especially industrial ones. This is a clear sign that this discipline has become mature. This chapter presents a comprehensive state-of-the-art introduction into advanced software agents and multi-agent systems. Properties and types of agents and multi-agent systems are discussed, which include precise definitions of both. A successful cooperation between agents is only possible if they can communicate in an efficient and semantically meaningful way. Thus, relevant communication strategies are discussed. Agent-based applications can be very powerful, complex systems. Their development can profit a lot from adequate support tools. Different development support options and environments are discussed in some detail. Due to their nature, multi-agent systems are excellent candidates for the realization of comprehensive simulations, especially if the individuality and uniqueness of components of the simulation environment play an important role. The second part of the chapter addresses supporting technologies and concepts. Ontologies, self-organization and emergence, and swarm intelligence and stigmergy are introduced and discussed in some detail.",industry
10.1016/j.isatra.2014.09.019,Journal,ISA Transactions,scopus,2015-03-01,sciencedirect,Soft sensor for real-time cement fineness estimation,https://api.elsevier.com/content/abstract/scopus_id/84926259783,"This paper describes the design and implementation of soft sensors to estimate cement fineness. Soft sensors are mathematical models that use available data to provide real-time information on process variables when the information, for whatever reason, is not available by direct measurement. In this application, soft sensors are used to provide information on process variable normally provided by off-line laboratory tests performed at large time intervals. Cement fineness is one of the crucial parameters that define the quality of produced cement. Providing real-time information on cement fineness using soft sensors can overcome limitations and problems that originate from a lack of information between two laboratory tests. The model inputs were selected from candidate process variables using an information theoretic approach. Models based on multi-layer perceptrons were developed, and their ability to estimate cement fineness of laboratory samples was analyzed. Models that had the best performance, and capacity to adopt changes in the cement grinding circuit were selected to implement soft sensors. Soft sensors were tested using data from a continuous cement production to demonstrate their use in real-time fineness estimation. Their performance was highly satisfactory, and the sensors proved to be capable of providing valuable information on cement grinding circuit performance. After successful off-line tests, soft sensors were implemented and installed in the control room of a cement factory. Results on the site confirm results obtained by tests conducted during soft sensor development.",industry
10.1016/j.compag.2014.12.010,Journal,Computers and Electronics in Agriculture,scopus,2015-02-01,sciencedirect,A Decision Support System to design modified atmosphere packaging for fresh produce based on a bipolar flexible querying approach,https://api.elsevier.com/content/abstract/scopus_id/84921031926,"To design new packaging for fresh food, stakeholders of the food chain express their needs and requirements, according to some goals and objectives. These requirements can be gathered into two groups: (i) fresh food related characteristics and (ii) packaging intrinsic characteristics. Modified Atmosphere Packaging (MAP) is an efficient way to delay senescence and spoilage and thus to extend the very short shelf life of respiring products such as fresh fruits and vegetables. Consequently, packaging O2/CO2 permeabilities must fit the requirements of fresh fruits and vegetable as predicted by virtual MAP simulating tools. Beyond gas permeabilities, the choice of a packaging material for fresh produce includes numerous other factors such as the cost, availability, potential contaminants of raw materials, process ability, and waste management constraints. For instance, the user may have the following multi-criteria query for his/her product asking for a packaging with optimal gas permeabilities that guarantee product quality and optionally a transparent packaging material made from renewable resources with a cost for raw material less than 3€/kg. To help stakeholders taking a rational decision based on the expressed needs, a new multi-criteria Decision Support System (DSS) for designing biodegradable packaging for fresh produce has been built. In this paper we present the functional specification, the software architecture and the implementation of the developed tool. This tool includes (i) a MAP simulation module combining mass transfer models and respiration of the food, (ii) a multi-criteria flexible querying module which handles imprecise, uncertain and missing data stored in the database. We detail its operational functioning through a real life case study to determine the most satisfactory materials for apricots packaging.",industry
10.1016/j.promfg.2015.07.372,Journal,Procedia Manufacturing,scopus,2015-01-01,sciencedirect,Case Study: Use of Online Tools in the Classroom and their Impact on Industrial Design Pedagogy,https://api.elsevier.com/content/abstract/scopus_id/85009959445,"Industrial Design education is going through a rapid evolution with more Design students making use of internet resources and tools such as crowdsourcing, 3D printing services, and other web-based tools to validate their ideas more quickly. The popularity of online 3D printing services such as Shapeways and Sculpteo accelerate the design process and learning. These services allow the designer to “print” virtually in any material such as plastics or metals. The impact of this new technology and other new web-based tools is significant not only in the industry but in the classroom as well. Current Industrial Design pedagogy is still partially based on technology, materials and processes that were developed a century ago. For example, pencils and paper are still the primary idea development tool. Books and magazines used to be the primary research tool but already have been surpassed by the Internet. Computer technology has improved significantly since the appearance of the first PCs, Macs and CNC machines. With all these advances in technology, one aspect of Industrial Design education that needs to be re-visited is the pedagogy of Design Drafting in this new age of online 3D printing services. The traditional Design pedagogy that was based on the development of different skills or competencies in separate courses or classes have not changed significantly in the last 40 years, 3D printing technology could potentially change this situation. Some new academic papers discuss this newer trend in Industrial Design schools but very few provide examples on how they implemented the new Internet-based 3D printing services in their curriculum. Industrial Design schools need to adapt quickly to the new reality, embracing Internet resources and online tools as core skills that every designer must have. This paper will discuss one case in particular where student projects were developed using online 3D printing services.",industry
10.1016/j.bica.2015.04.008,Journal,Biologically Inspired Cognitive Architectures,scopus,2015-01-01,sciencedirect,Automatic navigation of wall following mobile robot using Adaptive Resonance Theory of Type-1,https://api.elsevier.com/content/abstract/scopus_id/84960798237,"The automatic navigation of wall following robot is playing important role in various real world tasks such as underwater exploration, unmanned flight, and in automotive industries based on its computational complexity. In this work, a novel navigation approach based on biologically inspired neural network, known as “Adaptive Resonance Theory-1” which was proposed by Carpenter and Grossberg, has been implemented and investigated for navigation of wall following mobile robots. The proposed navigation algorithm is successfully tested with three sensor reading datasets obtained from clockwise navigation of SCITOS G5 mobile robot. Test decision accuracy (%), and simulation time were used as performance analysis parameters for the proposed algorithm and it has been found that the present work can achieve 99.59% of maximum decision accuracy.",industry
10.1016/j.procs.2015.07.575,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,A Decision Support System for Estimating Growth Parameters of Commercial Fish Stock in Fisheries Industries,https://api.elsevier.com/content/abstract/scopus_id/84948408653,"In this paper we present a Decision Support System (DSS) to estimate the values of the growth parameters from the yield effort data of a harvested population. The software is developed using Visual C# programming language in windows form application. Several softwares are used to check the validity of the multiple linear regression computational output in the DSS. The multiple linear regression computation is done using Ordinary Least Square (OLS) method and its computation comes from the discretization of two most known growth models, namely Logistic growth model and Gompertz growth model. We discretize the models in terms of yield effort variables and used the resulting equations as the bases for computing the intrinsic growth rate r and the carrying capacity K parameters which are the main ingredients in the MSY formula. Those models are also used as two selection tools placed in the main menu of the software. We use an existing published data as an example to estimate the growth parameters. We also compute the Maximum Sustainable Yield (MSY) for each model which represents the maximum amount of allowable biomass extracted from the fish population without harming the sustainability of the fisheries. Technically it suggests the value of the maximum sustainable yield as a decision to the harvester in managing the population and is often represented in a function of the growth parameters of the harvested population. Knowing the right growth parameters of the population is critical in determining the MSY, since the MSY is not stable. Hence, using an inaccurate values of growth parameters is likely detrimental in applying the MSY to the real fisheries. Our DSS has a contribution as a tool in reducing the error in calculating the growth parameters and the MSY. We found that the results are in agreement with the known literatures.",industry
10.1016/j.jmsy.2014.06.004,Journal,Journal of Manufacturing Systems,scopus,2015-01-01,sciencedirect,"A toolbox for the design, planning and operation of manufacturing networks in a mass customisation environment",https://api.elsevier.com/content/abstract/scopus_id/84942294486,"The task of design, planning and operation of manufacturing networks is becoming more and more challenging for companies, as globalisation, mass customisation and the turbulent economic landscape create demand volatility, uncertainties and high complexity. In this context, this paper investigates the performance of decentralised manufacturing networks through a set of methods developed into a software framework in a toolbox approach. The Tabu Search and Simulated Annealing metaheuristic methods are used together with an Artificial Intelligence method, called Intelligent Search Algorithm. A multi-criteria decision making procedure is carried out for the evaluation of the quality of alternative manufacturing network configurations using multiple conflicting criteria including dynamic complexity, reliability, cost, time, quality and environmental footprint. A comparison of the performance of each method based on the quality of the solutions that it provided is carried out. The statistical design of experiments robust engineering technique is used for the calibration of the adjustable parameters of the methods. Moreover, the impact of demand fluctuation to the operational performance of the alternative networks, expressed thorough a dynamic complexity indicator, is investigated through simulation. The developed framework is validated through a real life case, with data coming from the CNC machine building industry.",industry
10.1016/B978-0-444-63578-5.50097-9,Book Series,Computer Aided Chemical Engineering,scopus,2015-01-01,sciencedirect,Data Analysis and Modelling of a Fluid Catalytic Cracking Unit (FCCU) for an Implementation of Real Time Optimization,https://api.elsevier.com/content/abstract/scopus_id/84940474774,"In the Fluid Catalytic Cracking Units (FCCU) large hydrocarbon molecules are cracked into smaller molecules, generating high value products such as diesel, gasoline and useful petrochemical olefins. The control of these units is fundamental to maintain a satisfactory operation. Hence, the Real Time Optimization has proved an interesting strategy. A dynamic simulation of a FCCU was developed using a phenomenological industrial validated model. A Dynamic Neural Network (DNN) was trained with data from the FCCU model and gross and systematic errors were added to employ this system as a virtual plant. Data from this virtual plant were used to study strategies of online data processing, considering steady state identification (SSI) and gross error detection (GED), in order to eliminate measurement noise, as the initial steps into an RTO implementation.",industry
10.1016/j.future.2014.11.015,Journal,Future Generation Computer Systems,scopus,2015-01-01,sciencedirect,On-line failure prediction in safety-critical systems,https://api.elsevier.com/content/abstract/scopus_id/84917709364,"In safety-critical systems such as Air Traffic Control system, SCADA systems, Railways Control Systems, there has been a rapid transition from monolithic systems to highly modular ones, using off-the-shelf hardware and software applications possibly developed by different manufactures. This shift increased the probability that a fault occurring in an application propagates to others with the risk of a failure of the entire safety-critical system. This calls for new tools for the on-line detection of anomalous behaviors of the system, predicting thus a system failure before it happens, allowing the deployment of appropriate mitigation policies.
                  The paper proposes a novel architecture, namely CASPER, for online failure prediction that has the distinctive features to be (i) black-box: no knowledge of applications internals and logic of the system is required (ii) non-intrusive: no status information of the components is used such as CPU or memory usage; The architecture has been implemented to predict failures in a real Air Traffic Control System. CASPER exhibits high degree of accuracy in predicting failures with low false positive rate. The experimental validation shows how operators are provided with predictions issued a few hundred of seconds before the occurrence of the failure.",industry
10.1016/j.ijpe.2014.09.004,Journal,International Journal of Production Economics,scopus,2015-01-01,sciencedirect,An RFID-based intelligent decision support system architecture for production monitoring and scheduling in a distributed manufacturing environment,https://api.elsevier.com/content/abstract/scopus_id/84915733989,"Global manufacturing companies have some pressing needs to improve production visibility and decision-making performance by implementing effective production monitoring and scheduling. This paper proposes a radio frequency identification (RFID)-based intelligent decision support system architecture to handle production monitoring and scheduling in a distributed manufacturing environment. A pilot implementation of the architecture is reported in a distributed clothing manufacturing environment. RFID and cloud technologies were integrated for real-time and remote production capture and monitoring. Intelligent optimization techniques were also implemented to generate effective production scheduling solutions. A prototype system with remote monitoring and production scheduling functions was developed and implemented in a distributed manufacturing environment, which demonstrated the effectiveness of the architecture. The proposed architecture has good extensibility and scalability, which can easily be integrated with production decision-making as well as production and logistics operations in the supply chain. Lastly, this paper discusses the difficulties encountered and lessons learned during system implementation and the managerial implications of the proposed architecture.",industry
10.1016/j.asoc.2014.10.018,Journal,Applied Soft Computing Journal,scopus,2015-01-01,sciencedirect,Performance assessment of heat exchanger using intelligent decision making tools,https://api.elsevier.com/content/abstract/scopus_id/84912132496,"Process and manufacturing industries today are under pressure to deliver high quality outputs at lowest cost. The need for industry is therefore to implement cost savings measures immediately, in order to remain competitive. Organizations are making strenuous efforts to conserve energy and explore alternatives. This paper explores the development of an intelligent system to identify the degradation of heat exchanger system and to improve the energy performance through online monitoring system. The various stages adopted to achieve energy performance assessment are through experimentation, design of experiments and online monitoring system. Experiments are conducted as per full factorial design of experiments and the results are used to develop artificial neural network models. The predictive models are used to predict the overall heat transfer coefficient of clean/design heat exchanger. Fouled/real system value is computed with online measured data. Overall heat transfer coefficient of clean/design system is compared with the fouled/real system and reported. It is found that neural net work model trained with particle swarm optimization technique performs better comparable to other developed neural network models. The developed model is used to assess the performance of heat exchanger with the real/fouled system. The performance degradation is expressed using fouling factor, which is derived from the overall heat transfer coefficient of design system and real system. It supports the system to improve the performance by asset utilization, energy efficient and cost reduction in terms of production loss. This proposed online energy performance system is implemented into the real system and the adoptability is validated.",industry
10.1016/j.jss.2014.07.038,Journal,Journal of Systems and Software,scopus,2014-11-01,sciencedirect,A learning-based module extraction method for object-oriented systems,https://api.elsevier.com/content/abstract/scopus_id/84908163044,"Developers apply object-oriented (OO) design principles to produce modular, reusable software. Therefore, service-specific groups of related software classes called modules arise in OO systems. Extracting the modules is critical for better software comprehension, efficient architecture recovery, determination of service candidates to migrate legacy software to a service-oriented architecture, and transportation of such services to cloud-based distributed systems. In this study, we propose a novel approach to automatic module extraction to identify services in OO software systems. In our approach, first we create a weighted and directed graph of the software system in which vertices and edges represent the classes and their relations, respectively. Then, we apply a clustering algorithm over the graph to extract the modules. We calculate the weight of an edge by considering its probability of being within a module or between modules. To estimate these positional probabilities, we propose a machine-learning-based classification system that we train with data gathered from a real-world OO reference system. We have implemented an automatic module extraction tool and evaluated the proposed approach on several open-source and industrial projects. The experimental results show that the proposed approach generates highly accurate decompositions that are close to authoritative module structures and outperforms existing methods.",industry
10.1016/j.mejo.2013.12.006,Journal,Microelectronics Journal,scopus,2014-03-01,sciencedirect,Enhancing confidence in indirect analog/RF testing against the lack of correlation between regular parameters and indirect measurements,https://api.elsevier.com/content/abstract/scopus_id/84897670549,"The greedy specification testing remains mandatory for analog and radio frequency (RF) integrated circuits because of the accuracy of the sorting based on these measurements. Unfortunately, to be implemented, this kind of testing method often incurs very high costs (expensive instruments, long test time…). A common approach, in the literature, is the so-called indirect/alternate test strategy. This strategy consists in deriving targeted specifications from low-cost Indirect Measurements (IMs). During the industrial test phase, the estimation of regular specifications using IMs is based on a correlation model that has been built previously, during a training phase. Despite the substantial test cost reduction offered by this strategy, its deployment in industry is limited, mainly because of a lack of confidence in the accuracy of estimations made by the correlation model. A solution to increase the confidence in the estimation of specifications using the indirect approach is to implement redundancy in the prediction phase. In this paper, we demonstrate that the redundancy implementation brings more than identifying rare misjudged circuits from a high-correlated model. Indeed redundancy massively increases the accuracy despite of the lack of accurate models that have been assumed in previous implementations of redundant indirect testing. This approach is illustrated on a real case study for which we have experimental measurements on a set of 10,000 devices.",industry
10.1016/j.patcog.2013.09.007,Journal,Pattern Recognition,scopus,2014-03-01,sciencedirect,The cluster assessment of facial attractiveness using fuzzy neural network classifier based on 3D Moiré features,https://api.elsevier.com/content/abstract/scopus_id/84888386508,"Facial attractiveness has long been argued upon varied emphases by philosophers, artists, psychologists and biologists. A number of studies empirically investigated how facial attractiveness was influenced by 2D facial characteristics, such as symmetry, averageness and golden ratio. However, few implementations of facial beauty assessment were based on 3D facial features. The purpose of this paper is to propose a novel cluster assessment system for facial attractiveness that is characterized by the incorporation of 3D geometric Moiré features with an adjusted fuzzy neural network (FNN). We first extract 3D facial features from images acquired by a 3dMD scanner. Seven Moiré features are employed to represent a 3D facial image. The FNN classifier, taking the Moiré features as the parameters, is then trained and validated against independently conducted attractiveness ratings. A number of diverse referees were invited and offered their attractiveness ratings over a five-item Likert scale for 100 female facial images. The proposed assessment presents a high accuracy rate of 90%, and the area under curve (AUC) computed from the receiver operating characteristic (ROC) curve is 0.95. The results show that the perceptions of facial attractiveness are essentially consensus among raters, and can be mathematically modeled through supervised learning techniques. The high accuracy achieved proves that the proposed FNN classifier can serve as a general, automated and human-like judgment tool for objective classification of female facial attractiveness, and thus has potential applications to the entertainment industry, cosmetic industry, virtual media, and plastic surgery.",industry
10.1016/j.enbuild.2014.08.004,Journal,Energy and Buildings,scopus,2014-01-01,sciencedirect,Neural network model ensembles for building-level electricity load forecasts,https://api.elsevier.com/content/abstract/scopus_id/84907570987,"The future power grid is expected to provide unprecedented flexibility in how energy is generated, distributed, and managed, which increasingly necessitates an ability to perform accurate short-term small-scale electricity load and generation forecasting, e.g., at the level of individual buildings or sites. In this paper, we present a novel building-level neural network-based ensemble model for day-ahead electricity load forecasting and show that it outperforms the previously established best performing model, SARIMA, by up to 50%, in the context of load data from half a dozen operational commercial and industrial sites. In addition, we show a straightforward, automated way to select model parameters, making our model practical for use in real deployments.",industry
10.1016/j.anifeedsci.2014.04.017,Journal,Animal Feed Science and Technology,scopus,2014-01-01,sciencedirect,Keeping under control a liquid feed fermentation process for pigs: A reality scale pilot based study,https://api.elsevier.com/content/abstract/scopus_id/84903816625,"An original and fully automated liquid feeding pilot has been designed and implemented to monitor and optimize the fermentation process of liquid feed for pigs at a pre-industrial scale. The installation was designed and instrumented to continuously record the temperature, pH and redox potential (E
                     
                        h
                     ) during the fermentation course of wheat flour based feed mixed with water in a 1:2.5 (w:w) ratio. Single and multiple batches experiments were carried-out with feed inoculation achieved by leftover or with a selected culture of lactic acid producing bacteria (LAB). Physicochemical and microbiological characteristics of the fermentation process which include lactic and acetic acids and ethanol concentrations, enumerations of lactic acid producing bacteria, yeasts, total coliforms and Escherichia coli, were monitored and analyzed as a function of the main feed control factors: incubation time, operating temperature, feed time schedule and percentage of leftover. From batch experiments, it was observed that increasing the operating temperature from 15 to 30°C, accelerates the rate of fermentation by reducing about 5–6-folds the process latency and the duration to reach a pH value of 4.0 which is considered as optimal to achieve biosafety. Nevertheless, this does not prevent the blooming of coliforms as their counts increases from 4 to 6 log10
                     CFU/mL within 24h. In opposite, multiple batches are proved to be effective in both accelerating the fermentation rates and reducing the survival of Coliform bacteria in fermented liquid feed (FLF). Feed fermented at 25°C during 24-h cycles with a 22% leftover ensures the prominence of LAB strains over yeasts with a population level that stabilizes at around 9 log10
                     CFU/mL (vs. 7 log10
                     CFU/mL for single batches), a lactic acid production up to 35g/kg dry matter (DM) and a pH value between 5 and 3.5 throughout the period. Concomitantly, total Coliforms number decreases from 7.5 to 2.2 log10
                     CFU/mL within 72h whereas E. coli became undetectable beyond 48h. Addition of a starter culture (Pediococcus acidilactici, Bactocell®) at 9 log10
                     CFU/kg DM at the initial stage of FLF production reduces 25–35 times the total coliforms and E. coli counts. No significant differences in the amounts of organic compounds produced by the microflora as compared to the control FLF after 80h nor in the microbial levels are observed. It is concluded that sequences of fermentation cycles allows, in a given temperature range, establishing a positive, robust, microbial ecosystem.",industry
10.1016/j.powtec.2014.05.051,Journal,Powder Technology,scopus,2014-01-01,sciencedirect,"Soft sensing of particle size in a grinding process: Application of support vector regression, fuzzy inference and adaptive neuro fuzzy inference techniques for online monitoring of cement fineness",https://api.elsevier.com/content/abstract/scopus_id/84902965121,"Use of soft sensors for online particle size monitoring in a grinding process is a viable alternative since physical sensors for the same are not available for many such processes. Cement fineness is an important quality parameter in the cement grinding process. However, very few studies have been done for soft sensing of cement fineness in the grinding process. Moreover, most of the grinding process modeling approaches have been reported for ball mills and rarely any modeling of vertical roller mill is available. In this research, modeling of vertical roller mill used for clinker grinding has been done using support vector regression (SVR), fuzzy inference and adaptive neuro fuzzy inference(ANFIS) techniques since these techniques have not yet been largely explored for particle size soft sensing. The modeling has been done by collection of the real industrial data from a cement grinding process followed by data cleaning and a structured method of dividing the data into training and validation data sets using the Kennard–Stone subset selection algorithm. Optimum SVR hyper parameters were determined using a combined approach of analytical method and grid search plus cross validation. The models were developed using MATLAB from the training data and were tested with the validation data. Results reveal that the proposed ANFIS model of the clinker grinding process shows much superior performance compared with the other types of model. The ANFIS model was implemented in the SIMULINK environment for real-time monitoring of cement fineness from the knowledge of input variables and the model computation time was determined. It is observed that the model holds good promise to be implemented online for real-time estimation of cement fineness which will certainly help the plant operators in maintaining proper cement quality and in reducing losses.",industry
10.1016/j.eswa.2013.08.003,Journal,Expert Systems with Applications,scopus,2014-01-01,sciencedirect,Intelligent business processes composition based on multi-agent systems,https://api.elsevier.com/content/abstract/scopus_id/84888360250,"This paper proposes a novel model for automatic construction of business processes called IPCASCI (Intelligent business Processes Composition based on multi-Agent systems, Semantics and Cloud Integration). The software development industry requires agile construction of new products able to adapt to the emerging needs of a changing market. In this context, we present a method of software component reuse as a model (or methodology), which facilitates the semi-automatic reuse of web services on a cloud computing environment, leading to business process composition. The proposal is based on web service technology, including: (i) Automatic discovery of web services; (ii) Semantics description of web services; (iii) Automatic composition of existing web services to generate new ones; (iv) Automatic invocation of web services. As a result of this proposal, we have presented its implementation (as a tool) on a real case study. The evaluation of the case study and its results are proof of the reliability of IPCASCI.",industry
10.1016/j.asoc.2013.05.017,Journal,Applied Soft Computing Journal,scopus,2014-01-01,sciencedirect,A hybrid noise suppression filter for accuracy enhancement of commercial speech recognizers in varying noisy conditions,https://api.elsevier.com/content/abstract/scopus_id/84888294149,"Commercial speech recognizers have made possible many speech control applications such as wheelchair, tone-phone, multifunctional robotic arms and remote controls, for the disabled and paraplegic. However, they have a limitation in common in that recognition errors are likely to be produced when background noise surrounds the spoken command, thereby creating potential dangers for the disabled if recognition errors exist in the control systems. In this paper, a hybrid noise suppression filter is proposed to interface with the commercial speech recognizers in order to enhance the recognition accuracy under variant noisy conditions. It intends to decrease the recognition errors when the commercial speech recognizers are working under a noisy environment. It is based on a sigmoid function which can effectively enhance noisy speech using simple computational operations, while a robust estimator based on an adaptive-network-based fuzzy inference system is used to determine the appropriate operational parameters for the sigmoid function in order to produce effective speech enhancement under variant noisy conditions. The proposed hybrid noise suppression filter has the following advantages for commercial speech recognizers: (i) it is not possible to tune the inbuilt parameters on the commercial speech recognizers in order to obtain better accuracy; (ii) existing noise suppression filters are too complicated to be implemented for real-time speech recognition; and (iii) existing sigmoid function based filters can operate only in a single-noisy condition, but not under varying noisy conditions. The performance of the hybrid noise suppression filter was evaluated by interfacing it with a commercial speech recognizer, commonly used in electronic products. Experimental results show that improvement in terms of recognition accuracy and computational time can be achieved by the hybrid noise suppression filter when the commercial recognizer is working under various noisy environments in factories.",industry
10.1016/j.advengsoft.2013.09.003,Journal,Advances in Engineering Software,scopus,2014-01-01,sciencedirect,Software architecture knowledge for intelligent light maintenance,https://api.elsevier.com/content/abstract/scopus_id/84885359031,"The maintenance management plays an important role in the monitoring of business activities. It ensures a certain level of services in industrial systems by improving the ability to function in accordance with prescribed procedures. This has a decisive impact on the performance of these systems in terms of operational efficiency, reliability and associated intervention costs. To support the maintenance processes of a wide range of industrial services, a knowledge-based component is useful to perform the intelligent monitoring. In this context we propose a generic model for supporting and generating industrial lights maintenance processes. The modeled intelligent approach involves information structuring and knowledge sharing in the industrial setting and the implementation of specialized maintenance management software in the target information system. As a first step we defined computerized procedures from the conceptual structure of industrial data to ensure their interoperability and effective use of information and communication technologies in the software dedicated to the management of maintenance (E-candela). The second step is the implementation of this software architecture with specification of business rules, especially by organizing taxonomical information of the lighting systems, and applying intelligence-based operations and analysis to capitalize knowledge from maintenance experiences. Finally, the third step is the deployment of the software with contextual adaptation of the user interface to allow the management of operations, editions of the balance sheets and real-time location obtained through geolocation data. In practice, these computational intelligence-based modes of reasoning involve an engineering framework that facilitates the continuous improvement of a comprehensive maintenance regime.",industry
10.1016/j.ultras.2013.07.018,Journal,Ultrasonics,scopus,2014-01-01,sciencedirect,Ultrasonic sensor based defect detection and characterisation of ceramics,https://api.elsevier.com/content/abstract/scopus_id/84884211045,"Ceramic tiles, used in body armour systems, are currently inspected visually offline using an X-ray technique that is both time consuming and very expensive. The aim of this research is to develop a methodology to detect, locate and classify various manufacturing defects in Reaction Sintered Silicon Carbide (RSSC) ceramic tiles, using an ultrasonic sensing technique. Defects such as free silicon, un-sintered silicon carbide material and conventional porosity are often difficult to detect using conventional X-radiography. An alternative inspection system was developed to detect defects in ceramic components using an Artificial Neural Network (ANN) based signal processing technique. The inspection methodology proposed focuses on pre-processing of signals, de-noising, wavelet decomposition, feature extraction and post-processing of the signals for classification purposes. This research contributes to developing an on-line inspection system that would be far more cost effective than present methods and, moreover, assist manufacturers in checking the location of high density areas, defects and enable real time quality control, including the implementation of accept/reject criteria.",industry
10.1016/j.sna.2013.09.021,Journal,"Sensors and Actuators, A: Physical",scopus,2013-11-14,sciencedirect,Feasibility study of Hierarchical Temporal Memories applied to welding diagnostics,https://api.elsevier.com/content/abstract/scopus_id/84887294247,"Defect classification in on-line welding quality monitoring systems is an active area of research with a significant relevance to several industrial sectors where welding processes are extensively employed. Approaches based on some artificial intelligence implementations, like Artificial Neural Networks or Fuzzy Logic have been attempted, but their impact in real industrial scenarios is nowadays rather modest. In this paper a new approach based on Hierarchical Temporal Memories and the acquired plasma spectra is explored and analyzed by means of several arc-welding experimental tests. Results show the ability of the proposed solution to perform a suitable classification among several weld perturbations. The search for an optimal configuration of the algorithm and the usefulness of both spatial (spectral) and temporal identification of patterns will be also discussed, and the results will be compared with those provided by a solution based on feature selection and neural networks, exhibiting the better performance of the HTM model in terms of performance and handling of the input data.",industry
10.1016/j.fluid.2013.08.018,Journal,Fluid Phase Equilibria,scopus,2013-11-05,sciencedirect,Utilization of support vector machine to calculate gas compressibility factor,https://api.elsevier.com/content/abstract/scopus_id/84884180985,"The compressibility factor (Z-factor) is considered as a very important parameter in the petroleum industry because of its broad applications in PVT characteristics. In this study, a meta-learning algorithm called Least Square Support Vector Machine (LSSVM) was developed to predict the compressibility factor. In addition, the proposed technique was examined with previous models, exhibiting an R
                     2 and an MSE of 0.999 and 0.000014, respectively. A significant drawback in the conventional LSSVM is the determination of optimal parameters to attain desired output with a reasonable accuracy. To eliminate this problem, the current study introduced coupled simulated annealing (CSA) algorithm to develop a new model, known as CSA-LSSVM. The proposed algorithm included 4756 datasets to validate the effectiveness of the CSA-LSSVM model using statistical criteria. The new technique can be utilized in chemical and petroleum engineering software packages where the most accurate value of Z-factor is required to predict the behavior of real gas, significantly affecting design aspects of equipment involved in gas processing plants.",industry
10.1016/j.jprocont.2013.09.014,Journal,Journal of Process Control,scopus,2013-10-28,sciencedirect,A multilayer-perceptron based method for variable selection in soft sensor design,https://api.elsevier.com/content/abstract/scopus_id/84886080853,"The paper proposes a new method for variable selection for prediction settings and soft sensors applications. The new variable selection method is based on the multi-layer perceptron (MLP) neural network model, where the network is trained a single time, maintaining low computational cost. The proposed method was successfully applied, and compared with four state-of-the-art methods in one artificial dataset and three real-world datasets, two publicly available datasets (Box–Jenkins gas furnace and gas mileage), and a dataset of a problem where the objective is to estimate the fluoride concentration in the effluent of a real urban water treatment plant (WTP). The proposed method presents similar or better approximation performance when compared to the other four methods. In the experiments, among all the five methods, the proposed method selects the lowest number of variables and variables-delays pairs to achieve the best solution. In soft sensors applications having a lower number of variables is a positive factor for decreasing implementation costs, or even making the soft sensor feasible at all.",industry
10.1016/j.neucom.2013.02.039,Journal,Neurocomputing,scopus,2013-10-22,sciencedirect,Point and prediction interval estimation for electricity markets with machine learning techniques and wavelet transforms,https://api.elsevier.com/content/abstract/scopus_id/84881221196,"A growing number of countries all over the world are switching over to deregulated or the market structure of electricity sector with a view to enhance productivity, efficiency and to lower the prices. Barring a few cases, the deregulated structure is doing quite well in most of the countries. However a persistent issue that plagues the involved parties such as producers, traders, retailers etc., is the uncertainty that prevails in the system. Due to a number of known, unknown factors, the electricity prices exhibit fluctuating characteristics which is difficult to control as well as predict. Several forecasting techniques have been developed and successfully implemented for existing markets around the world with comparable performance. However, the uncertainty aspect of the point forecasts has not been analyzed significantly. In this work, an attempt is made to quantify such uncertainties existing in the market using statistical techniques like prediction intervals. Hybrid models using neural networks and Extreme Learning machines with wavelets as preprocessors are developed and applied for point as well as prediction interval forecasting for Ontario Electricity Market, PJM Day-Ahead and Real time markets.",industry
10.1016/j.engappai.2013.04.006,Journal,Engineering Applications of Artificial Intelligence,scopus,2013-09-01,sciencedirect,Post-design analysis for building and refining AI planning systems,https://api.elsevier.com/content/abstract/scopus_id/84880771590,"The growth of industrial applications of artificial intelligence has raised the need for design tools to aid in the conception and implementation of such complex systems. The design of automated planning systems faces several engineering challenges including the proper modeling of the domain knowledge: the creation of a model that represents the problem to be solved, the world that surrounds the system, and the ways the system can interact with and change the world in order to solve the problem. Knowledge modeling in AI planning is a hard task that involves acquiring the system requirements and making design decisions that can determine the behavior and performance of the resulting system. In this paper we investigate how knowledge acquired during a post-design phase of modeling can be used to improve the prospective model. A post-design framework is introduced which combines a knowledge engineering tool and a virtual prototyping environment for the analysis and simulation of plans. This framework demonstrates that post-design analysis supports the discovery of missing requirements and can guide the model refinement cycle. We present three case studies using benchmark domains and eight state-of-the-art planners. Our results demonstrate that significant improvements in plan quality and an increase in planning speed of up to three orders of magnitude can be achieved through a careful post-design process. We argue that such a process is critical for the deployment of AI planning technology in real-world engineering applications.",industry
10.1016/j.jsr.2013.04.005,Journal,Journal of Safety Research,scopus,2013-06-24,sciencedirect,Transferability and robustness of real-time freeway crash risk assessment,https://api.elsevier.com/content/abstract/scopus_id/84879088842,"Introduction
                  This study examines the data from single loop detectors on northbound (NB) US-101 in San Jose, California to estimate real-time crash risk assessment models.
               
                  Method
                  The classification tree and neural network based crash risk assessment models developed with data from NB US-101 are applied to data from the same freeway, as well as to the data from nearby segments of the SB US-101, NB I-880, and SB I-880 corridors. The performance of crash risk assessment models on these nearby segments is the focus of this research.
               
                  Results
                  The model applications show that it is in fact possible to use the same model for multiple freeways, as the underlying relationships between traffic data and crash risk remain similar.
               
                  Impact on Industry
                  The framework provided here may be helpful to authorities for freeway segments with newly installed traffic surveillance apparatuses, since the real-time crash risk assessment models from nearby freeways with existing infrastructure would be able to provide a reasonable estimate of crash risk. The robustness of the model output is also assessed by location, time of day, and day of week. The analysis shows that on some locations the models may require further learning due to higher than expected false positive (e.g., the I-680/I-280 interchange on US-101 NB) or false negative rates. The approach for post-processing the results from the model provides ideas to refine the model prior to or during the implementation.",industry
10.1016/j.neucom.2012.04.033,Journal,Neurocomputing,scopus,2013-06-03,sciencedirect,Applying soft computing techniques to optimise a dental milling process,https://api.elsevier.com/content/abstract/scopus_id/84875966713,"This study presents a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. This novel intelligent procedure is based on the following phases. Firstly, a neural model extracts the internal structure and the relevant features of the data set representing the system. Secondly, the dynamic system performance of different variables is specifically modelled using a supervised neural model and identification techniques. This constitutes the model for the fitness function of the production process, using relevant features of the data set. Finally, a genetic algorithm is used to optimise the machine parameters from a non parametric fitness function. The proposed novel approach was tested under real dental milling processes using a high-precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study.",industry
10.1016/j.engappai.2012.11.009,Journal,Engineering Applications of Artificial Intelligence,scopus,2013-05-01,sciencedirect,An intelligent system for wafer bin map defect diagnosis: An empirical study for semiconductor manufacturing,https://api.elsevier.com/content/abstract/scopus_id/84876945059,"Wafer bin maps (WBMs) that show specific spatial patterns can provide clue to identify process failures in the semiconductor manufacturing. In practice, most companies rely on experienced engineers to visually find the specific WBM patterns. However, as wafer size is enlarged and integrated circuit (IC) feature size is continuously shrinking, WBM patterns become complicated due to the differences of die size, wafer rotation, the density of failed dies and thus human judgments become inconsistent and unreliable. To fill the gaps, this study aims to develop a knowledge-based intelligent system for WBMs defect diagnosis for yield enhancement in wafer fabrication. The proposed system consisted of three parts: graphical user interface, the WBM clustering solution, and the knowledge database. In particular, the developed WBM clustering approach integrates spatial statistics test, cellular neural network (CNN), adaptive resonance theory (ART) neural network, and moment invariant (MI) to cluster different patterns effectively. In addition, an interactive converse interface is developed to present the possible root causes in the order of similarity matching and record the diagnosis know-how from the domain experts into the knowledge database. To validate the proposed WBM clustering solution, twelve different WBM patterns collected in real settings are used to demonstrate the performance of the proposed method in terms of purity, diversity, specificity, and efficiency. The results have shown the validity and practical viability of the proposed system. Indeed, the developed solution has been implemented in a leading semiconductor manufacturing company in Taiwan. The proposed WBM intelligent system can recognize specific failure patterns efficiently and also record the assignable root causes verified by the domain experts to enhance troubleshooting effectively.",industry
10.1016/j.robot.2012.12.005,Journal,Robotics and Autonomous Systems,scopus,2013-05-01,sciencedirect,A survey of bio-inspired robotics hands implementation: New directions in dexterous manipulation,https://api.elsevier.com/content/abstract/scopus_id/84875695547,"Recently, significant advances have been made in ROBOTICS, ARTIFICIAL INTELLIGENCE and other COGNITIVE related fields, allowing to make much sophisticated biomimetic robotics systems. In addition, enormous number of robots have been designed and assembled, explicitly realize biological oriented behaviors. Towards much skill behaviors and adequate grasping abilities (i.e. ARTICULATION and DEXTEROUS MANIPULATION), a new phase of dexterous hands have been developed recently with biomimetically oriented and bio-inspired functionalities. In this respect, this manuscript brings a detailed survey of biomimetic based dexterous robotics multi-fingered hands. The aim of this survey, is to find out the state of the art on dexterous robotics end-effectors, known in literature as (ROBOTIC HANDS) or (DEXTEROUS MULTI-FINGERED) robot hands. Hence, this review finds such biomimetic approaches using a framework that permits for a common description of biological and technical based hand manipulation behavior. In particular, the manuscript focuses on a number of developments that have been taking place over the past two decades, and some recent developments related to this biomimetic field of research. In conclusions, the study found that, there are rich research efforts in terms of KINEMATICS, DYNAMICS, MODELING and CONTROL methodologies. The survey is also indicating that, the topic of biomimetic inspired robotics systems make significant contributions to robotics hand design, in four main directions for future research. First, they provide a genuine world test of models of biologically inspired hand designs and dexterous manipulation behaviors. Second, they provide novel manipulation articulations and mechanisms available for industrial and domestic uses, most notably in the field of human like hand design and real world applications. Third, this survey has also indicated that, there are quite large number of attempts to acquire biologically inspired hands. These attempts were almost successful, where they exposed more novel ideas for further developments. Such inspirations were directed towards a number of topics related (HAND MECHANICS AND DESIGN), (HAND TACTILE SENSING), (HAND FORCE SENSING), (HAND SOFT ACTUATION) and (HAND CONFIGURATION AND TOPOLOGY). FOURTH, in terms of employing AI related sciences and cognitive thinking, it was also found that, rare and exceptional research attempts were directed towards the employment of biologically inspired thinking, i.e. (AI, BRAIN AND COGNITIVE SCIENCES) for hand upper control and towards much sophisticated dexterous movements. Throughout the study, it has been found there are number of efforts in terms of mechanics and hand designs, tactical sensing, however, for hand soft actuation, it seems this area of research is still far away from having a realistic muscular type fingers and hand movements.",industry
10.1016/j.compind.2012.11.005,Journal,Computers in Industry,scopus,2013-04-01,sciencedirect,IMAQCS: Design and implementation of an intelligent multi-agent system for monitoring and controlling quality of cement production processes,https://api.elsevier.com/content/abstract/scopus_id/84875245342,"In cement plant, since all processes are chemical and irreversible, monitoring and control is a critical factor. If the process is not controlled at any stage, the final product can be damaged or lost. Thus, in such environments, considering the quality of the product at each state is essential. Also, to control the process, communication among different parts of production line is essential. The wasted time in production line has a direct effect on process correction time and cement production performance. Here, a model of a new intelligent multi-agent quality control system (IMAQCS) for controlling the quality of cement production processes is suggested. This model, using of rule-based artificial intelligence technique, concentrates on relationship between departments in cement production line to monitor multi-attribute quality factors. With the presence of agents for controlling the quality of cement processes, real-time analyzing and decision making in a fault condition will be provided. In order to validate the proposed model, IMAQCS is deployed in real plants of a cement industries complex in Iran. The ability of the system in the process production environment is assessed. The effectiveness and efficiency of the system are demonstrated by reducing the process correction time and increasing the cement production performance. Finally, this system can effectively impact on factory resources and cost saving.",industry
10.1016/j.epsr.2013.01.011,Journal,Electric Power Systems Research,scopus,2013-02-21,sciencedirect,FPGA-based neural network harmonic estimation for continuous monitoring of the power line in industrial applications,https://api.elsevier.com/content/abstract/scopus_id/84873945333,"Manufacturing cells are present in almost all the industrial sector. Unfortunately, all machine tools into the manufacturing cell are connected to the same power line, implying that their operation adds nonlinear loads such as harmonics and interharmonics that affect the general machine-tool condition. A novel NN-based methodology for harmonic monitoring through time in transient and stationary signals that satisfies the IEC61000-4-7 standard is presented, as well as its implementation into a field programmable gate array (FPGA) for continuous and online monitoring. The proposed method and the developed instrument have been tested in a real manufacturing cell.",industry
10.1533/9780857093967.1.208,Book,Joining Textiles: Principles and Applications,scopus,2013-01-01,sciencedirect,Intelligent sewing systems for garment automation and robotics,https://api.elsevier.com/content/abstract/scopus_id/84903011824,"Sewing machine interactions at different speeds have been used to construct qualitative rules mapping fabric properties to optimum sewing machine settings for intelligent sewing machines. the inference procedures of fuzzy logic have been implemented in a neural network to allow for optimisation of output membership functions and, subsequently, self-learning. the technique is successfully applied to develop intelligent sewing machines and further implemented in textile and garment manufacturing. An intelligent manufacturing environment has been put forward in which fabric properties predict the sewability of any fabric, determine the minimum change of fabric properties required, and control in real time the stitching of a garment by using the feedback closed loop of the Neuro-Fuzzy model. the system has been successfully tried in an industrial setting. optimum settings were achieved under static and dynamic machine conditions, including for the properties of difficult fabrics and compensation for mishandling by the operator over the speed range of the sewing machine.",industry
10.1016/j.procir.2013.09.042,Conference Proceeding,Procedia CIRP,scopus,2013-01-01,sciencedirect,An enabling digital foundation towards smart machining,https://api.elsevier.com/content/abstract/scopus_id/84886789550,"Today's major challenges for manufacturing companies in the aerospace and automotive industries are clear: global cooperation with multiple supply chain partners, production optimization, management and tracking of information so as to meet new requirements in terms of traceability, security and sustainability. The need for a data exchange standard that allows disparate entities and their associated devices in a manufacturing system to share data seamlessly is clearly obvious. And the first expected impact is the ‘next generation’ smart controller that could really enable an intelligent machining process based on real-time monitoring and diagnosis, self-learning decision and adaptive optimization. The four-year project titled FoFdation envisions a ‘Digital and Smart Factory’ architecture and implementation. This has the potential to achieve significant benefits in earlier visibility of manufacturing issues, faster production ramp-up time, faster time to volume production and subsequently shorter time to market, reduced manufacturing costs and improved product quality, as well as sustainability objectives like low energy consumption and waste reduction. The present paper describes the on-going work with specific focus on the definition and implementation of the FoFdation Smart Machine Controller (SMC) in an adaptable architecture that satisfies both commercial and open source CNC controllers. It highlights the project's end use validation framework as well as sets a strong Manufacturing Information System foundation on which process optimization and control as well as sustainable practices can be based. It presents the general vision of the target solution for the SMC developed in the FoFdation project. It is based on efforts past and present both by academia and industry in various capacities and proposes tentative implementations based on the STEP-NC standard to define the machine controller of the future.",industry
10.3182/20130825-4-US-2038.00105,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2013-01-01,sciencedirect,An on-line training simulator built on dynamic simulations of crushing plants,https://api.elsevier.com/content/abstract/scopus_id/84885800887,"Crushing plants are widely used around the world as a pre-processing step in the mineral and mining industries or as standalone processing plants for final products in the aggregates industry. Despite automation and different types of advanced model predictive control, many the processes are still managed by operators. The skill of the operators influences the process performance and thus production yield. Therefore, it is important to train the operators so they know how to behave in different situations and to make them able to operate the process in the best possible way.
                  Different types of models for crushers and other production units have been developed during the years and the latest improvement is the addition of dynamic behavior which gives the crushing plants a time dependent behavior and performance. This can be used as a simulator for operators training. By connecting an Internet based Human Machine Interface (WebHMI) to a dynamic simulator with the models incorporated, an on-line training environment for operators can be achieved.
                  In this paper, a dynamic crushing plant simulator implemented in MATLAB/SIMULINK has been connected to a WebHMI. The WebHMI is accessible via the Internet, thus creating a realistic control room for operators’ training. In the created training environment, the operators can be trained under realistic conditions. Simple training scenarios and how they could be simulated are discussed. Apart from the increased level of knowledge and experience among the operators, the time aspect is an important factor. While a real crushing plant is still being built, the operators to be can already be trained, saving a lot of the commissioning and ramp up time.",industry
10.3182/20130828-3-UK-2039.00025,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2013-01-01,sciencedirect,A new extensive source for web-based control education - Contlab.eu,https://api.elsevier.com/content/abstract/scopus_id/84885206555,"Modern technologies allow to create a networked control system with off-the-shelf mobile devices. As such, there is the possibility of having the role of who offers and who uses a “remote” laboratory played by the same people. Extending recently published ideas, the paper presents a first nucleus of functionalities allowing one to create process simulators and controllers which run on a mobile application, and then share them with others. Some words are also spent on some of the possibilities opened by the proposal, sketching out some interesting didactic activities to propose to the students.",industry
10.1016/j.asoc.2012.05.031,Journal,Applied Soft Computing Journal,scopus,2013-01-01,sciencedirect,Optimal design of laser solid freeform fabrication system and real-time prediction of melt pool geometry using intelligent evolutionary algorithms,https://api.elsevier.com/content/abstract/scopus_id/84881665462,"With the rapid growth of laser applications and the introduction of high efficiency lasers (e.g. fiber lasers), laser material processing has gained increasing importance in a variety of industries. Among the applications of laser technology, laser cladding has received significant attention due to its high potential for material processing such as metallic coating, high value component repair, prototyping, and even low-volume manufacturing. In this paper, two optimization methods have been applied to obtain optimal operating parameters of Laser Solid Freeform Fabrication Process (LSFF) as a real world engineering problem. First, Particle Swarm Optimization (PSO) algorithm was implemented for real-time prediction of melt pool geometry. Then, a hybrid evolutionary algorithm called Self-organizing Pareto based Evolutionary Algorithm (SOPEA) was proposed to find the optimal process parameters. For further assurance on the performance of the proposed optimization technique, it was compared to some well-known vector optimization algorithms such as Non-dominated Sorting Genetic Algorithm (NSGA-II) and Strength Pareto Evolutionary Algorithm (SPEA 2). Thereafter, it was applied for simultaneous optimization of clad height and melt pool depth in LSFF process. Since there is no exact mathematical model for the clad height (deposited layer thickness) and the melt pool depth, the authors developed two Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to estimate these two process parameters. Optimization procedure being done, the archived non-dominated solutions were surveyed to find the appropriate ranges of process parameters with acceptable dilutions. Finally, the selected optimal ranges were used to find a case with the minimum rapid prototyping time. The results indicate the acceptable potential of evolutionary strategies for controlling and optimization of LSFF process as a complicated engineering problem.",industry
10.1016/j.riai.2013.05.002,Journal,RIAI - Revista Iberoamericana de Automatica e Informatica Industrial,scopus,2013-01-01,sciencedirect,Identification and wavenet control of AC motor,https://api.elsevier.com/content/abstract/scopus_id/84880211449,"En el presente artículo se muestra un esquema de identificación y control que sintoniza en línea las ganancias proporcional, integral y derivativa de un controlador PID discreto aplicado a un sistema dinámico SISO. Esto se logra empleando una red neuronal de base radial con funciones de activación wavelet hijas Morlet (wavenet) adicionalmente en cascada un filtro de respuesta infinita al impulso (IIR). Dicho esquema es aplicado en tiempo real para controlar la velocidad de un motor de inducción de CA trifásico del tipo jaula de ardilla (MIJA) alimentado con un variador de frecuencia trifásico, de esta forma se muestra cómo este esquema de identificación y control en línea, puede ser implementado en este tipo de plantas que son ampliamente utilizadas en la industria, sin la necesidad de obtener los parámetros del modelo matemático del conjunto variador de frecuencia-motor de inducción trifásico. Se presentan los resultados obtenidos en simulación numérica y experimentales, empleando para esto la plataforma de LabVIEW.
               
                  This paper presents a control scheme to tune online the proportional, integral and derivative gains of a discrete PID controller, through the identification and control of a SISO stable and minimum phase dynamic system. This is accomplished using a radial basis network neural with daughter Morlet wavelets activation functions in cascaded with an infinite impulse response (IIR) filter. This scheme is applied in real time to control the speed of an AC three-phase induction motor supplied with a three-phase inverter. So in this way we show how the identification and control scheme can be implemented in this type of plants that are widely used in industry, without the need of mathematical model parameters of the induction motor. We present numerical simulation and experimental results.",industry
10.1016/j.compchemeng.2012.06.021,Journal,Computers and Chemical Engineering,scopus,2012-12-20,sciencedirect,SmartGantt - An interactive system for generating and updating rescheduling knowledge using relational abstractions,https://api.elsevier.com/content/abstract/scopus_id/84869501412,"Generating and updating rescheduling knowledge that can be used in real time has become a key issue in reactive scheduling due to the dynamic and uncertain nature of industrial environments and the emergent trend towards cognitive systems in production planning and execution control. Disruptive events have a significant impact on the feasibility of plans and schedules. In this work, the automatic generation and update through learning of rescheduling knowledge using simulated transitions of abstract schedule states is proposed. An industrial example where a current schedule must be repaired in response to unplanned events such as the arrival of a rush order, raw material delay, or an equipment failure which gives rise to the need for rescheduling is discussed. A software prototype (SmartGantt) for interactive schedule repair in real-time is presented. Results demonstrate that responsiveness is dramatically improved by using relational reinforcement learning and relational abstractions to develop a repair policy.",industry
10.1016/j.dss.2012.08.006,Journal,Decision Support Systems,scopus,2012-12-01,sciencedirect,Sales forecasting for computer wholesalers: A comparison of multivariate adaptive regression splines and artificial neural networks,https://api.elsevier.com/content/abstract/scopus_id/84868667879,"Artificial neural networks (ANNs) have been found to be useful for sales/demand forecasting. However, one of the main shortcomings of ANNs is their inability to identify important forecasting variables. This study uses multivariate adaptive regression splines (MARS), a nonlinear and non-parametric regression methodology, to construct sales forecasting models for computer wholesalers. Through the outstanding variable screening ability of MARS, important sales forecasting variables for computer wholesalers can be obtained to enable them to make better sales management decisions. Two sets of real sales data collected from Taiwanese computer wholesalers are used to evaluate the performance of MARS. The experimental results show that the MARS model outperforms backpropagation neural networks, a support vector machine, a cerebellar model articulation controller neural network, an extreme learning machine, an ARIMA model, a multivariate linear regression model, and four two-stage forecasting schemes across various performance criteria. Moreover, the MARS forecasting results provide useful information about the relationships between the forecasting variables selected and sales amounts through the basis functions, important predictor variables, and the MARS prediction function obtained, and hence they have important implications for the implementation of appropriate sales decisions or strategies.",industry
10.1016/j.ymssp.2012.01.021,Journal,Mechanical Systems and Signal Processing,scopus,2012-07-01,sciencedirect,FPGA-based entropy neural processor for online detection of multiple combined faults on induction motors,https://api.elsevier.com/content/abstract/scopus_id/84860217701,"For industry, a faulty induction motor signifies production reduction and cost increase. Real-world induction motors can have one or more faults present at the same time that can mislead to a wrong decision about its operational condition. The detection of multiple combined faults is a demanding task, difficult to accomplish even with computing intensive techniques. This work introduces information entropy and artificial neural networks for detecting multiple combined faults by analyzing the 3-axis startup vibration signals of the rotating machine. A field programmable gate array implementation is developed for automatic online detection of single and combined faults in real time.",industry
10.1016/j.cirp.2012.03.065,Journal,CIRP Annals - Manufacturing Technology,scopus,2012-04-23,sciencedirect,Decision support systems for effective maintenance operations,https://api.elsevier.com/content/abstract/scopus_id/84861592241,"To compete successfully in the market place, leading manufacturing companies are pursuing effective maintenance operations. Existing computerized maintenance management systems (CMMS) can no longer meet the needs of dynamic maintenance operations. This paper describes newly developed decision support tools for effective maintenance operations: (1) data-driven short-term throughput bottleneck identification, (2) estimation of maintenance windows of opportunity, (3) prioritization of maintenance tasks, (4) joint production and maintenance scheduling systems, and (5) maintenance staff management. Mathematical algorithms and simulation tools are utilized to illustrate the concepts of these decision support systems. Results from real implementations in automotive manufacturing are presented to demonstrate the effectiveness of these tools.",industry
10.1016/j.jmsy.2011.09.002,Journal,Journal of Manufacturing Systems,scopus,2012-04-01,sciencedirect,Intelligent evaluation of supplier bids using a hybrid technique in distributed supply chains,https://api.elsevier.com/content/abstract/scopus_id/84858340427,"The main idea of this research is to devise the smart module to pick the best supplier bid(s) automatically. The hybrid model is composed of three useful tools: fuzzy logic, AHP, and QFD. The approach has been carefully implemented and verified via a real-world case study in a medium-to-large industry manufacturing vehicle tires and other rubber products. A collection of 12 assessment criteria classified into two categories have been considered. Eight factors are derived from customer suggestions and the other four are design specifications required to manufacture the product. The main outcomes are: a hybrid autonomous model to evaluate supplier bids without direct human intervention; devising a hybrid three-module method and overcoming complexity of computations in resulting algorithm by means of agents; outlining the best criteria to assess suppliers; evaluating the suppliers based on voice of customer during all stages of the process; and discussing analysis, design, and implementation issues of the evaluation agent. The paper includes implications for development of an integrated total system for supply chain coordination. The most important advantages of this work over earlier researches on supplier selection are: implementation of an autonomous assessment mechanism using intelligent agents for the first time, making the best out of three widely applied methodologies all at once, evaluation process mainly based on features of customer order, coordination of supply job based on a bidding system, and portal-mediated operation and control.",industry
10.1016/j.conengprac.2011.06.009,Journal,Control Engineering Practice,scopus,2012-04-01,sciencedirect,Data reconciliation and optimal management of hydrogen networks in a petrol refinery,https://api.elsevier.com/content/abstract/scopus_id/84857191932,"This paper describes the main problems associated to the management of hydrogen networks in petrol refineries and presents an approach to deal with them with the aim of operating the installation in the most profitable way. In particular, the problems of data reconciliation, economic optimization and interaction with the underlying basic control system are reviewed. The paper provides also a proposal for the implementation of the system and illustrates the approach with results obtained using real data from an industrial site.",industry
10.1016/j.eswa.2011.11.112,Journal,Expert Systems with Applications,scopus,2012-04-01,sciencedirect,A multi-agent-based decision support system for bankruptcy contagion effects,https://api.elsevier.com/content/abstract/scopus_id/84855900478,"With the increasing interdependence of marketing participants, distress experienced by a specific entity may cause other connecting firms to encounter financial difficulties, leading to a negative impact on their stock valuations. At the same time, individual investors have a great need to gain relevant information for portfolio risk management. The monitoring vision cannot be limited to investors’ portfolios but must take into account any potential candidates affected. Based on the ontological knowledge model of inter-firm relationships, the proposed multi-agent decision support system continuously observes real-time news reports and forecasts their potential impact on the corresponding stock price. After identifying relating companies for which significant market reactions can be expected, a wireless push-based message service promptly supplies information. A case study is used to illustrate the multi-agent-based decision support system (MAB-DSS) implementation and its use. The example shows that the MAB-DSS can automate the solution for intricate and dynamic valuation effects among interdependent firms and provide constructive advice for individual investors.",industry
10.1016/j.proeng.2012.07.193,Conference Proceeding,Procedia Engineering,scopus,2012-01-01,sciencedirect,Modelling and simulation for Industrial DC Motor using Intelligent control,https://api.elsevier.com/content/abstract/scopus_id/84877113112,"This paper presents an overview of Proportional Integral control (PI) and Artificial Intelligent control (AI) algorithms. AI and PI controller are analyzed using Matlab [Simulink] software. The DC motor is an attractive piece of equipment in many industrial applications requiring variable speed and load characteristics due to its ease of controllability. The main objective of this paper illustrates how the speed of the DC motor can be controlled using different controllers. The simulation results demonstrate that the responses of DC motor with an AI control which is Fuzzy Logic Control shows satisfactory well damped control performance. The results shows that Industrial DC Motor model develop using its physical parameters and controlled with an AI controller give better response, it means it can used as a controller to the real time DC Motor",industry
10.1016/j.compeleceng.2012.05.013,Journal,Computers and Electrical Engineering,scopus,2012-01-01,sciencedirect,Automatic network intrusion detection: Current techniques and open issues,https://api.elsevier.com/content/abstract/scopus_id/84866355973,"Automatic network intrusion detection has been an important research topic for the last 20years. In that time, approaches based on signatures describing intrusive behavior have become the de-facto industry standard. Alternatively, other novel techniques have been used for improving automation of the intrusion detection process. In this regard, statistical methods, machine learning and data mining techniques have been proposed arguing higher automation capabilities than signature-based approaches. However, the majority of these novel techniques have never been deployed on real-life scenarios. The fact is that signature-based still is the most widely used strategy for automatic intrusion detection. In the present article we survey the most relevant works in the field of automatic network intrusion detection. In contrast to previous surveys, our analysis considers several features required for truly deploying each one of the reviewed approaches. This wider perspective can help us to identify the possible causes behind the lack of acceptance of novel techniques by network security experts.",industry
10.3182/20120403-3-DE-3010.00066,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,A model-based approach for timing analysis of industrial automation systems,https://api.elsevier.com/content/abstract/scopus_id/84866095348,"This paper presents a temporal characterization for automation systems. The final goal is to achieve a whole model in which a schedulability analysis could be applied in order to assure that this kind of systems meet timing non-functional requirements of the application. This work is performed in the context of a Model Driven Development approach. The definition of three Domain Specific Models: control specification, and hardware and software architectures, is the base for the whole model. In particular, the software domain model uses the XML interface defined by PLCopen for expressing IEC 61131-3 automation projects. The information contained in the model of the application is processed to generate the temporal model of the automation system. A specific transformation of this model makes possible to carry out a schedulability analysis of the system. In particular, this is achieved by generating the specific input model of the well-known Modelling and Analysis Suite for Real-Time Applications, MAST.",industry
10.3182/20120403-3-DE-3010.00010,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,RodosVisor - An object-oriented and customizable hypervisor: The CPU virtualization,https://api.elsevier.com/content/abstract/scopus_id/84866091325,"RodosVisor is an object-oriented and bare-metal virtual machine monitor (VMM) or hypervisor designed for the aerospace industry, mainly to provide time and spatial separation to the NetworkCentric core avionics machine, Montenegro and Dittrich (2009). The NetworkCentric core avionics machine consists of several harmonized components working together to implement dependable computing in a simple way, with computing units managed by the local real-time operating system RODOS. To support partitioned software architectures such as AIR, Rufino et al. (2009), and MILS, DeLong, R. (2007), RodosVisor adapted the Popek and Goldberg's fidelity, efficiency and resource control virtualization requirements, Popek and Goldberg (1974), to the space application domain by extending them with extra ones, like timing determinism, reactivity and improved dependability. Another distinctive RodosVisor feature is the customized design based on generative programming techniques, such as aspect oriented programming and template meta-programming.",industry
10.3182/20120403-3-DE-3010.00082,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,Web monitoring system and gateway for serial communication PLC,https://api.elsevier.com/content/abstract/scopus_id/84866084543,"An industrial process requires interacting with the rest of the plant, being able to exchange data with other devices and monitoring systems in order to optimize production, reporting information and providing control capabilities to distant users. Internet, and, especially web browsers are an excellent tool to provide information for remote users, allowing not only monitoring but also controlling the industrial process as an SCADA software or HMI system. The proposed system does not need specific proprietary software and its associated license costs. In this work, a webserver system is implemented under a Freescale microcontroller, acting as a gateway for a simple PLC with single RS232 communication capabilities. The webserver is modular, providing independent access to single I/O PLC locations. Different webpage design can offer different monitoring capabilities by combining the required I/O modules according to the required application without any change in the microcontroller programming. This capability is close to SCADA software or industrial HMI systems where custom screens can be made. This proposal offers a low cost and flexible monitoring solution to old or basic industrial processes controlled by PLC with low communication capabilities. Using a web browser, the system can be monitored from any internet capable device: PC, tablet, smartphone, etc. An example for a pneumatic PID levitation control system is given.",industry
10.1016/j.autcon.2011.05.018,Journal,Automation in Construction,scopus,2012-01-01,sciencedirect,Simulation and analytical techniques for construction resource planning and scheduling,https://api.elsevier.com/content/abstract/scopus_id/81355138778,"To date, few construction methods or models in the literature have discussed about helping the project managers decide the near-optimum distributions of manpower, material, equipment and space according to their project objectives and project constraints. Thus, the traditional scheduling methods or models often result in a “seat-of-the-pants” style of management, rather than decision making based on an analysis of real data. This paper presents an intelligent scheduling system (ISS) that can help the project managers to find the near-optimum schedule plan according to their project objectives and project constraints. ISS uses simulation techniques to distribute resources and assign different levels of priorities to different activities in each simulation cycle to find the near-optimal solution. ISS considers and integrates most of the important construction factors (schedule, cost, space, manpower, equipment and material) simultaneously in a unified environment, which makes the resulting schedule that will be closer to optimal. Furthermore, ISS allows for what-if analyses of possible scenarios, and schedule adjustments based on unforeseen conditions (change orders, late material delivery, etc.). Finally, two sample applications and one real-world construction project are utilized to illustrate and compare the effectiveness of ISS with two widely used software packages, Primavera Project Planner and Microsoft Project.",industry
10.1016/j.wasman.2011.07.018,Journal,Waste Management,scopus,2011-12-01,sciencedirect,A web-based Decision Support System for the optimal management of construction and demolition waste,https://api.elsevier.com/content/abstract/scopus_id/80054853534,"Wastes from construction activities constitute nowadays the largest by quantity fraction of solid wastes in urban areas. In addition, it is widely accepted that the particular waste stream contains hazardous materials, such as insulating materials, plastic frames of doors, windows, etc. Their uncontrolled disposal result to long-term pollution costs, resource overuse and wasted energy. Within the framework of the DEWAM project, a web-based Decision Support System (DSS) application – namely DeconRCM – has been developed, aiming towards the identification of the optimal construction and demolition waste (CDW) management strategy that minimises end-of-life costs and maximises the recovery of salvaged building materials. This paper addresses both technical and functional structure of the developed web-based application. The web-based DSS provides an accurate estimation of the generated CDW quantities of twenty-one different waste streams (e.g. concrete, bricks, glass, etc.) for four different types of buildings (residential, office, commercial and industrial). With the use of mathematical programming, the DeconRCM provides also the user with the optimal end-of-life management alternative, taking into consideration both economic and environmental criteria. The DSS’s capabilities are illustrated through a real world case study of a typical five floor apartment building in Thessaloniki, Greece.",industry
10.1016/j.asoc.2011.05.011,Journal,Applied Soft Computing Journal,scopus,2011-12-01,sciencedirect,Credit risk evaluation using neural networks: Emotional versus conventional models,https://api.elsevier.com/content/abstract/scopus_id/80053571498,"Credit scoring and evaluation is one of the key analytical techniques in credit risk evaluation which has been an active research area in financial risk management. Artificial neural networks (NNs) have been considered to be accurate tools for credit analysis among others in the credit industry. Lately, emotional neural networks (EmNNs) have been suggested and applied successfully for pattern recognition. In this paper we investigate the efficiency of EmNNs and compare their performance to conventional NNs when applied to credit risk evaluation. In total 12 neural networks; based equally on emotional and conventional neural models; are arbitrated under three learning schemes to classify whether a credit application is approved or declined. The learning schemes differ in the ratio of training-to-validation data used during training and testing the neural networks. The emotional and conventional neural models are trained using real world credit application cases from the Australian credit approval datasets which has 690 cases; each case with 14 numerical attributes; based on which an application is accepted or rejected. The performance of the 12 neural networks will be evaluated using certain criteria. Experimental results suggest that both emotional and conventional neural models can be used effectively for credit risk evaluations, however the emotional models outperform their conventional counterparts in decision making speed and accuracy, thus, making them ideal for implementation in fast automatic processing of credit applications.",industry
10.1016/j.ijthermalsci.2011.06.020,Journal,International Journal of Thermal Sciences,scopus,2011-12-01,sciencedirect,A simplified method to evaluate the energy performance of CO<inf>2</inf> heat pump units,https://api.elsevier.com/content/abstract/scopus_id/80052736833,"The prediction of the performances of CO2 transcritical heat pumps demands accurate calculation methods, where a particular effort is devoted to the gas cooler modelling, as the correlation between high pressure and gas cooler outlet temperature strongly affects the cycle performance. The above-mentioned methods require a large amount of input data and calculation power. As a consequence they are often useless for the full characterisation of heat pumps which are sold on the market.
                  A simplified numerical method for the performance prediction of vapour compression heat pumps working in a transcritical cycle is presented, based only on performance data at the nominal rating conditions. The proposed procedure was validated against experimental data of two different tap water heat pumps. For the considered units, simulation results are in good agreement with the experimental ones. The deviations range from −6.4% to +1.7% and from −3.8% to +5.8% for the COPH
                      of the air/water heat pump and the water/water heat pump, respectively. The heating capacity deviations stayed within −5.5% and +1.7% range and within −5.0% and +7.9% range for the same units.
                  The proposed mathematical model appears to be a reliable tool to be used by the refrigeration industry or to be implemented into dynamic building-plant energy simulation codes. Finally, it represents a useful instrument for the definition of tailored approximated optimal high pressure curve considering the operating characteristics of the specific CO2 transcritical unit. It could also be implemented on board of a real unit control system where it could be used as model coupled to computational intelligence algorithms for pressure optimisation.",industry
10.1016/j.neucom.2011.06.027,Journal,Neurocomputing,scopus,2011-11-01,sciencedirect,Neural network based controller for Cr<sup>6+</sup>-Fe<sup>2+</sup> batch reduction process,https://api.elsevier.com/content/abstract/scopus_id/80053311549,An automated pilot plant has been designed and commissioned to carry out online/real-time data acquisition and control for the Cr6+–Fe2+ reduction process. Simulated data from the Cr6+–Fe2+ model derived are validated with online data and laboratory analysis using ICP-AES analysis method. The distinctive trend or patterns exhibited in the ORP profiles for the non-equilibrium model derived have been utilized to train neural network-based controllers for the process. The implementation of this process control is to ensure sufficient Fe2+ solution is dosed into the wastewater sample in order to reduce all Cr6+–Cr3+. The neural network controller has been utilized to compare the capability of set-point tracking with a PID controller in this process. For this process neural network-based controller dosed in less Fe2+ solution compared to the PID controller which hence reduces wastage of chemicals. Industrial Cr6+ wastewater samples obtained from an electro-plating factory has also been tested on the pilot plant using the neural network-based controller to determine its effectiveness to control the reduction process for a real plant. The results indicate the proposed controller is capable of fully reducing the Cr6+–Cr3+ in the batch treatment process with minimal dosage of Fe2+.,industry
10.1016/j.envsoft.2011.04.002,Journal,Environmental Modelling and Software,scopus,2011-10-01,sciencedirect,Application of the Analytic Hierarchy Process and the Analytic Network Process for the assessment of different wastewater treatment systems,https://api.elsevier.com/content/abstract/scopus_id/79957865817,"Multicriteria analyses (MCAs) are used to make comparative assessments of alternative projects or heterogeneous measures and allow several criteria to be taken into account simultaneously in a complex situation. The paper shows the application of different MCA techniques to a real decision problem concerning the choice of the most sustainable wastewater treatment (WWT) technology, namely Anaerobic digestion, Phytoremediation and Composting, for small cheese factories. Particularly, the Analytic Hierarchy Process (AHP) and its recent implementation, the Analytic Network Process (ANP), have been considered for prioritizing the different technologies. The models enable all the elements of the decision process to be considered, namely environmental aspects, technological factors and economic costs, and to compare them to find the best alternative. The AHP and ANP techniques are applied through specific software packages with user-friendly interfaces called Expertchoice and Superdecision, respectively. A comparison of the merits obtained from the different models shows that Phytoremediation results as the most sustainable WWT technology for small cheese factories and that the use of the ANP method, which allows more sophisticated analysis to be made, succeeds in offering better results.",industry
10.1016/j.eswa.2011.04.012,Journal,Expert Systems with Applications,scopus,2011-09-15,sciencedirect,Fast defect detection in homogeneous flat surface products,https://api.elsevier.com/content/abstract/scopus_id/79958015915,"This paper introduces a novel hybrid approach for both defect detection and localization in homogeneous flat surface products. Real time defect detection in industrial products is a challenging problem. Fast production speeds and the variable nature of production defects complicate the process of automating the defect detection task. Speeding up the detection process is achieved in this paper by implementing a hybrid approach that is based on the statistical decision theory, multi-scale and multi-directional analysis and a neural network implementation of the optimal Bayesian classifier. The coefficient of variation is first used as a homogeneity measure for approximate defect localization. Second, features are extracted from the log Gabor filter bank response to accurately localize and detect the defect while reducing the complexity of Gabor based inspection approaches. A probabilistic neural network (PNN) is used for fast defect classification based on the maximum posterior probability of the Log-Gabor based statistical features. Experimental results show a major performance enhancement over existing defect detection approaches.",industry
10.1016/j.ces.2011.03.041,Journal,Chemical Engineering Science,scopus,2011-08-01,sciencedirect,Successive approximate model based multi-objective optimization for an industrial straight grate iron ore induration process using evolutionary algorithm,https://api.elsevier.com/content/abstract/scopus_id/79958723405,"Multi-objective optimization of any complex industrial process using first principle computationally expensive models often demands a substantially higher computation time for evolutionary algorithms making it less amenable for real time implementation. A combination of the above-mentioned first principle model and approximate models based on artificial neural network (ANN) successively learnt in due course of optimization using the data obtained from first principle models can be intelligently used for function evaluation and thereby reduce the aforementioned computational burden to a large extent. In this work, a multi-objective optimization task (simultaneous maximization of throughput and Tumble index) of an industrial iron ore induration process has been studied to improve the operation of the process using the above-mentioned metamodeling approach. Different pressure and temperature values at different points of the furnace bed, grate speed and bed height have been used as decision variables whereas the bounds on cold compression strength, abrasion index, maximum pellet temperature and burn-through point temperature have been treated as constraints. A popular evolutionary multi-objective algorithm, NSGA II, amalgamated with the first principle model of the induration process and its successively improving approximation model based on ANN, has been adopted to carry out the task. The optimization results show that as compared to the PO solutions obtained using only the first principle model, (i) similar or better quality PO solutions can be achieved by this metamodeling procedure with a close to 50% savings in function evaluation and thereby computation time and (ii) by keeping the total number of function evaluations same, better quality PO solutions can be obtained.",industry
10.1016/j.eswa.2011.01.051,Journal,Expert Systems with Applications,scopus,2011-08-01,sciencedirect,Recommendation system for localized products in vending machines,https://api.elsevier.com/content/abstract/scopus_id/79953690190,"This paper proposes a framework of localized product recommendation system for automatic vending machines applications. The goal is to offer suitable recommendations of localized products to customers in distinct locations. We develop a hybrid technique that combines a meta-heuristic approach, clustering technique, classification, and statistical method. In the approach, an intelligent system is implemented to analyze product attributes and determine localized products based on the transaction data. To prove the feasibility and effectiveness of proposed approach, we implemented the system in several automatic vending machines owned by an information service company of Taiwan. Nine machines were selected and compared from two locations: living lab by Institute for Information Industry of Taiwan at Song-shan District and business office building at Nei-hu District in Taipei. The real life experiments showed that the profit of vending machine increases after applying our system.",industry
10.1016/j.eswa.2011.01.081,Journal,Expert Systems with Applications,scopus,2011-07-01,sciencedirect,Expert system for analysis of quality in production of electronics,https://api.elsevier.com/content/abstract/scopus_id/79952444562,"Quality issues have become increasingly important in the production of electronics, especially when dealing with electronic products not assimilated to the mainstream of consumer electronics, but rather to the group of industrial electronic devices and machinery designed to last for years or even decades. In this paper, an intelligent optimization and modeling system for electronics production is demonstrated. The system exploits real production data and can be used to diagnose and optimize the manufacturing processes. It contains three modules consisting of appropriate mathematical tools specifically tailored to each task: (1) preprocessing, (2) variable selection, and (3) optimization modules. Moreover, concrete examples are presented from the latter two modules, by using a wave soldering process as a case study. Currently, the system works on the Matlab platform, but can be programmed into standalone software and automated in the future. The results illustrate that the system can offer an efficient tool for diagnostics and process optimization in the electronics industry.",industry
10.1016/j.jchromb.2011.03.059,Journal,Journal of Chromatography B: Analytical Technologies in the Biomedical and Life Sciences,scopus,2011-06-01,sciencedirect,Influence of different spacer arms on Mimetic Ligand <sup>™</sup> A2P and B14 membranes for human IgG purification,https://api.elsevier.com/content/abstract/scopus_id/79955910197,"Microporous membranes are an attractive alternative to circumvent the typical drawbacks associated to bead-based chromatography. In particular, the present work intends to evaluate different affinity membranes for antibody capture, to be used as an alternative to Protein A resins. To this aim, two Mimetic Ligands™ A2P and B14, were coupled onto different epoxide and azide group activated membrane supports using different spacer arms and immobilization chemistries. The spacer chemistries investigated were 1,2-diaminoethane (2LP), 3,6-dioxa-1,8-octanedithiol (DES) and [1,2,3] triazole (TRZ). These new mimetic membrane materials were investigated by static and by dynamic binding capacity studies, using pure polyclonal human immunoglobulin G (IgG) solutions as well as a real cell culture supernatant containing monoclonal IgG1. The best results were obtained by combining the new B14 ligand with a TRZ-spacer and an improved Epoxy 2 membrane support material. The new B14-TRZ-Epoxy 2 membrane adsorbent provided binding capacities of approximately 3.1mg/mL, besides (i) a good selectivity towards IgG, (ii) high IgG recoveries of above 90%, (iii) a high Pluronic-F68 tolerance and (iv) no B14-ligand leakage under harsh cleaning-in-place conditions (0.6M sodium hydroxide). Furthermore, foreseeable improvements in binding capacity will promote the implementation of membrane adsorbers in antibody manufacturing.",industry
10.1016/j.matdes.2011.01.058,Journal,Materials and Design,scopus,2011-06-01,sciencedirect,A hybrid of back propagation neural network and genetic algorithm for optimization of injection molding process parameters,https://api.elsevier.com/content/abstract/scopus_id/79953161387,"This paper presents a hybrid optimization method for optimizing the process parameters during plastic injection molding (PIM). This proposed method combines a back propagation (BP) neural network method with an intelligence global optimization algorithm, i.e. genetic algorithm (GA). A multi-objective optimization model is established to optimize the process parameters during PIM on the basis of the finite element simulation software Moldflow, Orthogonal experiment method, BP neural network as well as Genetic algorithm. Optimization goals and design variables (process parameters during PIM) are specified by the requirement of manufacture. A BP artificial neural network model is developed to obtain the mathematical relationship between the optimization goals and process parameters. Genetic algorithm is applied to optimize the process parameters that would result in optimal solution of the optimization goals. A case study of a plastic article is presented. Warpage as well as clamp force during PIM are investigated as the optimization objectives. Mold temperature, melt temperature, packing pressure, packing time and cooling time are considered to be the design variables. The case study demonstrates that the proposed optimization method can adjust the process parameters accurately and effectively to satisfy the demand of real manufacture.",industry
10.1016/j.aca.2011.01.041,Journal,Analytica Chimica Acta,scopus,2011-03-18,sciencedirect,Biodiesel classification by base stock type (vegetable oil) using near infrared spectroscopy data,https://api.elsevier.com/content/abstract/scopus_id/79952487365,"The use of biofuels, such as bioethanol or biodiesel, has rapidly increased in the last few years. Near infrared (near-IR, NIR, or NIRS) spectroscopy (>4000cm−1) has previously been reported as a cheap and fast alternative for biodiesel quality control when compared with infrared, Raman, or nuclear magnetic resonance (NMR) methods; in addition, NIR can easily be done in real time (on-line). In this proof-of-principle paper, we attempt to find a correlation between the near infrared spectrum of a biodiesel sample and its base stock. This correlation is used to classify fuel samples into 10 groups according to their origin (vegetable oil): sunflower, coconut, palm, soy/soya, cottonseed, castor, Jatropha, etc. Principal component analysis (PCA) is used for outlier detection and dimensionality reduction of the NIR spectral data. Four different multivariate data analysis techniques are used to solve the classification problem, including regularized discriminant analysis (RDA), partial least squares method/projection on latent structures (PLS-DA), K-nearest neighbors (KNN) technique, and support vector machines (SVMs). Classifying biodiesel by feedstock (base stock) type can be successfully solved with modern machine learning techniques and NIR spectroscopy data. KNN and SVM methods were found to be highly effective for biodiesel classification by feedstock oil type. A classification error (E) of less than 5% can be reached using an SVM-based approach. If computational time is an important consideration, the KNN technique (E
                     =6.2%) can be recommended for practical (industrial) implementation. Comparison with gasoline and motor oil data shows the relative simplicity of this methodology for biodiesel classification.",industry
10.1016/j.asoc.2010.09.007,Journal,Applied Soft Computing Journal,scopus,2011-03-01,sciencedirect,Forecasting stock markets using wavelet transforms and recurrent neural networks: An integrated system based on artificial bee colony algorithm,https://api.elsevier.com/content/abstract/scopus_id/78751613501,"This study presents an integrated system where wavelet transforms and recurrent neural network (RNN) based on artificial bee colony (abc) algorithm (called ABC-RNN) are combined for stock price forecasting. The system comprises three stages. First, the wavelet transform using the Haar wavelet is applied to decompose the stock price time series and thus eliminate noise. Second, the RNN, which has a simple architecture and uses numerous fundamental and technical indicators, is applied to construct the input features chosen via Stepwise Regression-Correlation Selection (SRCS). Third, the Artificial Bee Colony algorithm (ABC) is utilized to optimize the RNN weights and biases under a parameter space design. For illustration and evaluation purposes, this study refers to the simulation results of several international stock markets, including the Dow Jones Industrial Average Index (DJIA), London FTSE-100 Index (FTSE), Tokyo Nikkei-225 Index (Nikkei), and Taiwan Stock Exchange Capitalization Weighted Stock Index (TAIEX). As these simulation results demonstrate, the proposed system is highly promising and can be implemented in a real-time trading system for forecasting stock prices and maximizing profits.",industry
10.1016/j.cma.2010.11.014,Journal,Computer Methods in Applied Mechanics and Engineering,scopus,2011-02-01,sciencedirect,Multi-objective optimization of turbomachinery using improved NSGA-II and approximation model,https://api.elsevier.com/content/abstract/scopus_id/78650612499,"Coupled optimization methods based on multi-objective genetic algorithms and approximation models are widely used in engineering optimizations. In the present paper, a similar framework is proposed for the aerodynamic optimization of turbomachinery by coupling the well known multi-objective genetic algorithm—NSGA-II and back propagation neural network. The verification results of mathematical problems show that the coupled method with the origin NSGA-II cannot get the real Pareto front due to the prediction error of BPNN. A modified crowding distance is proposed in cooperation with a coarse-to-fine approaching strategy based on the iterations between NSGA-II and BPNN. The results of mathematical model problems show the effect of these improving strategies. An industrial application case is implemented on a transonic axial compressor. The optimization objectives are to maximize efficiencies of two working points and to minimize the variation of the choked mass flow. CFD simulation is employed to provide the performance evaluation of initial training samples for BPNN. The optimized results are compared with optimization results of a single objective optimization based on weighting function. The comparison shows that the present framework can provide not only better solutions than the single objective optimization, but also various alternative solutions. The increase of computational costs is acceptable especially when approximation models are used.",industry
10.1016/j.procs.2011.08.020,Conference Proceeding,Procedia Computer Science,scopus,2011-01-01,sciencedirect,Model development of a virtual learning environment to enhance lean education,https://api.elsevier.com/content/abstract/scopus_id/84856459118,"Modern day industry is becoming leaner by the day. This demands engineers with an in-depth understanding of lean philosophies. Current methods for teaching lean include hands-on projects and simulation. However, simulation games available in the market lack simplicity, ability to store the results, and modeling power. The goal of this research is to develop a virtual simulation platform which would enable students to perform various experiments by applying lean concepts. The design addresses these deficiencies through the use of VE-Suite, a virtual engineering software. The design includes user-friendly dialogue boxes, graphical models of machines, performance display gauges, and an editable layout. The platform uses laws of operations management such as Little's law, economic order quantity (EOQ) models, and cycle time. These laws enable students to implement various lean concepts such as pull system, just-in-time (JIT), single piece flow, single minute exchange of dies (SMED), kaizen, kanban, U-layout, by modifying the process parameters such as process times, setup times, layout, number, and placement of machines. The simulation begins with a traditional push type mass production line and the students improve the line by implementing lean techniques. Thus, students experience the advantages of lean real time while facing the real life problems encountered in implementing it.",industry
10.1016/j.jngse.2011.08.002,Journal,Journal of Natural Gas Science and Engineering,scopus,2011-01-01,sciencedirect,An implementation of a distributed artificial intelligence architecture to the integrated production management,https://api.elsevier.com/content/abstract/scopus_id/84855498917,"The oil production process is highly complex and requires the combination of several disciplines and technological tools for its management. System integration and the automation of the workflows required to develop oil production operations are two main problems nowadays at the oil and gas production industry. This work approaches these problems through the implementation of distributed artificial intelligence architecture, designed for the automated production management. The architecture comprises a standardized schema to access information sources, a production ontological framework and an intelligent workflow mechanism based on multi-agent systems and electronic institution. Our architecture present several novelties: the incorporation of the semantic integration, the extension of the agents theory through the electronic institutions paradigm to solve the real-time decision problems typical in the industry, the Holon-Agent hybrid model used to make more feasible its implementation, among others. An oil production management case study is presented in order to demonstrate the applicability of the proposed architecture.",industry
10.1016/j.isatra.2011.06.006,Journal,ISA Transactions,scopus,2011-01-01,sciencedirect,A hybrid intelligent controller for a twin rotor MIMO system and its hardware implementation,https://api.elsevier.com/content/abstract/scopus_id/80052415887,"This paper presents a fuzzy PID control scheme with a real-valued genetic algorithm (RGA) to a setpoint control problem. The objective of this paper is to control a twin rotor MIMO system (TRMS) to move quickly and accurately to the desired attitudes, both the pitch angle and the azimuth angle in a cross-coupled condition. A fuzzy compensator is applied to the PID controller. The proposed control structure includes four PID controllers with independent inputs in 2-DOF. In order to reduce total error and control energy, all parameters of the controller are obtained by a RGA with the system performance index as a fitness function. The system performance index utilized the integral of time multiplied by the square error criterion (ITSE) to build a suitable fitness function in the RGA. A new method for RGA to solve more than 10 parameters in the control scheme is investigated. For real-time control, Xilinx Spartan II SP200 FPGA (Field Programmable Gate Array) is employed to construct a hardware-in-the-loop system through writing VHDL on this FPGA.",industry
10.1016/j.csda.2010.06.014,Journal,Computational Statistics and Data Analysis,scopus,2011-01-01,sciencedirect,Robust weighted kernel logistic regression in imbalanced and rare events data,https://api.elsevier.com/content/abstract/scopus_id/77956394683,"Recent developments in computing and technology, along with the availability of large amounts of raw data, have contributed to the creation of many effective techniques and algorithms in the fields of pattern recognition and machine learning. The main objectives for developing these algorithms include identifying patterns within the available data or making predictions, or both. Great success has been achieved with many classification techniques in real-life applications. With regard to binary data classification in particular, analysis of data containing rare events or disproportionate class distributions poses a great challenge to industry and to the machine learning community. This study examines rare events (REs) with binary dependent variables containing many more non-events (zeros) than events (ones). These variables are difficult to predict and to explain as has been evidenced in the literature. This research combines rare events corrections to Logistic Regression (LR) with truncated Newton methods and applies these techniques to Kernel Logistic Regression (KLR). The resulting model, Rare Event Weighted Kernel Logistic Regression (RE-WKLR), is a combination of weighting, regularization, approximate numerical methods, kernelization, bias correction, and efficient implementation, all of which are critical to enabling RE-WKLR to be an effective and powerful method for predicting rare events. Comparing RE-WKLR to SVM and TR-KLR, using non-linearly separable, small and large binary rare event datasets, we find that RE-WKLR is as fast as TR-KLR and much faster than SVM. In addition, according to the statistical significance test, RE-WKLR is more accurate than both SVM and TR-KLR.",industry
10.1016/j.robot.2010.08.004,Journal,Robotics and Autonomous Systems,scopus,2010-12-31,sciencedirect,Open-ended evolution as a means to self-organize heterogeneous multi-robot systems in real time,https://api.elsevier.com/content/abstract/scopus_id/78649913375,"This work deals with the application of multi-robot systems to real tasks and, in particular, their coordination through interaction based control systems. Within this field, the practical solutions that have been implemented in real robots mainly use strongly coordinated architectures and assignment strategies because of reliability and fault tolerance issues when addressing problems in reality. Emergent approaches have also been proposed with limited success, basically due to the unpredictability of the behaviors obtained. Here, an emergent approach, called r-ASiCo, is presented containing a procedure to produce predictable solutions and thus avoiding the typical problems associated with these techniques. The r-ASico algorithm is the real time version of the Asynchronous Situated Co-evolution algorithm (ASiCo), which exploits natural open-ended evolution to generate emergent complex collective behaviors and deals with systems made up of a huge number of elements and nonlinear interactions. The goal of r-ASiCo is to design the global behavior desired for the robot team as a collective entity and allow the emergence of behaviors through the interaction of the team members using social rules they learn to implement. To this end, r-ASiCo manages a series of features that are inherent to natural evolution based methods such as energy exchange and mating selection procedures, together with a technique to guide the evolution towards a design objective, the principled evaluation function selection procedure. Hence, this paper presents the components and operation of r-ASiCo and illustrates its application through a collective cleaning task example. It was implemented using 8 e-puck robots in two different real scenarios and its results complemented with those of a 30 e-puck case. The results show the capabilities of r-ASiCo to create a self-organized and adaptive multi-robot system configuration that is tolerant to environmental changes and to failures within the robot team.",industry
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry
10.1016/j.simpat.2010.04.010,Journal,Simulation Modelling Practice and Theory,scopus,2010-10-01,sciencedirect,Revisiting state space exploration of timed coloured petri net models to optimize manufacturing system's performance,https://api.elsevier.com/content/abstract/scopus_id/77955305732,"Due to constant fluctuations in market demands, nowadays scheduling of flexible manufacturing systems is taking great importance to improve competitiveness. Coloured Petri Nets (CPN) is a high level modelling formalism which have been widely used to model and verify systems, allowing representing not only the system’s dynamic behaviour but also the information flow. One approach that focuses in performance optimization of industrial systems is the one that uses the CPN formalism extended with time features (Timed Coloured Petri Nets) and explores all the possible states of the model (state space) looking for states of particular interest under industrial scope. Unfortunately, using the time extension, the state space becomes awkward for most industrial problems, reason why there is a recognized need of approaches that could tackle optimization problems such as the scheduling of manufacturing activities without simplifying any important aspect of the real system. In this paper a timed state space approach for properties verification and systems optimization is presented together with new algorithms in order to get better results when time is used as a cost function for optimizing the makespan of manufacturing systems. A benchmarking example of a job-shop is modelled in CPN formalism to illustrate the improvements that can be achieved with the proposed implementations.",industry
10.1016/j.jfoodeng.2010.02.027,Journal,Journal of Food Engineering,scopus,2010-07-01,sciencedirect,Nonlinear predictive control based on artificial neural network model for industrial crystallization,https://api.elsevier.com/content/abstract/scopus_id/77950043186,"This paper illustrates the benefits of a nonlinear model based predictive control (NMPC) strategy for setpoint tracking control of an industrial crystallization process. A neural networks model is used as internal model to predict process outputs. An optimization problem is solved to compute future control actions taking into account real-time control objectives. Furthermore, a more suitable output variable is used for process control: the mass of crystals in the solution is used instead of the traditional electrical conductivity. The performance of the NMPC implementation is assessed via simulation results based on industrial data.",industry
10.1016/j.engappai.2010.02.007,Journal,Engineering Applications of Artificial Intelligence,scopus,2010-06-01,sciencedirect,A knowledge-based architecture for distributed fault analysis in power networks,https://api.elsevier.com/content/abstract/scopus_id/77950510844,"Power industry around the world is facing several changes since deregulation with constant pressure put on improving security, reliability and quality of the power supply. Computational fault analysis and diagnosis of power networks have been active research topics with several theories and algorithms proposed. This paper proposes a distributed diagnostic algorithm for fault analysis in power networks. Distributed architecture for power network fault analysis (DAPFA) is an intelligent, model-based diagnostic algorithm that incorporates a hierarchical power network representation and model. The architecture is based on the industry’s substation automation implementation standards. The structural and functional model is a multi-level representation with each level depicting a more complex grouping of components than its predecessor in the hierarchy. The distributed functional representation contains the behavioral knowledge related to the components of that level in the structural model.
                  The diagnostic algorithm of DAPFA is designed to perform fault analysis in pre-diagnostic and diagnostic levels. Pre-diagnostic phase provides real-time analysis while the diagnostic phase provides the final diagnostic analysis. The diagnostic algorithm incorporates knowledge-based and model-based reasoning mechanisms with one of the model levels represented as a network of neural nets. The relevant algorithms and techniques are discussed. The resulting system has been implemented on a New Zealand sub-system and the results are analyzed.",industry
10.1016/j.matcom.2010.01.002,Journal,Mathematics and Computers in Simulation,scopus,2010-05-01,sciencedirect,Flow regimes identification and liquid-holdup prediction in horizontal multiphase flow based on neuro-fuzzy inference systems,https://api.elsevier.com/content/abstract/scopus_id/77953134320,"Numerous techniques have been used to identify flow regimes and liquid holdup in horizontal multiphase flow, but often neither perform well nor very accurate. Recently, neuro-fuzzy inference systems learning scheme have been gaining popularity in its capability for solving both prediction and classification problems. It is a hybrid intelligent systems scheme that is able to forecast an output in the uncertainty situations. This paper investigates the capabilities of neuro-fuzzy TypeI in identifying flow regimes and forecasting liquid holdup in horizontal multiphase flow. The performance of neuro-fuzzy modeling scheme is implemented using different real-world industry databases. Comparative studies were carried out to compare neuro-fuzzy systems performance with the most popular existing approaches in identifying flow regimes and predict liquid holdup in horizontal multiphase flow. Results show that neuro-fuzzy is flexible, reliable, outperforms the existing techniques and show bright future capabilities in solving different oil and gas industry problems, namely, rock mechanics properties, water saturation, faceis classification, and distinct bioinformatics applications.",industry
10.1016/S1684-1182(10)60014-X,Journal,"Journal of Microbiology, Immunology and Infection",scopus,2010-04-01,sciencedirect,Fast Diagnosis and Quantification for Porcine Circovirus Type 2 (PCV-2) Using Real-Time Polymerase Chain Reaction,https://api.elsevier.com/content/abstract/scopus_id/77951778976,"Background/Purpose
                  The postweaning multisystemic wasting syndrome, caused by the porcine circovirus type 2 (PCV-2), is a major disease that poses a significant threat to the global swine industry. The purpose of this study was to establish a real-time polymerase chain reaction (PCR) method for the quantification of PCV-2 and to enable the rapid differentiation of porcine circoviruses type 1 and 2 (PCV-1 and PCV-2). Such a method would significantly speed up the process of clinical diagnosis, and could also be used to study the pathogenic mechanisms of diseases associated with PCV-2.
               
                  Methods
                  Multiplex real-time PCR, together with LightCycler PCR data analysis software, was used for the quantification of PCV-2, and for the rapid differentiation of PCV-1 and PCV-2. A 263-bp DNA fragment was amplified from the 3′ end of the open reading frame-2 of PCV-2 by nested PCR, and its DNA sequence was verified as having 100% identity with a PCV-2 standard (NCBI accession number: AF055394). The 263-bp DNA fragment was cloned into the pGEM-T easy vector, and the recombinant plasmid was serially diluted and quantified using real-time PCR. A standard curve was then constructed for quantification of the PCV-2 levels in field samples. The differentiation of PCV-1 and PCV-2 was carried out by analyzing the melting temperatures of the genotype-specific PCR products.
               
                  Results
                  To quantify the PCV-2 levels in field samples, a standard curve (1 × 102 −1 × 109 copies/μL) was constructed. PCV-2 concentrations as low as 1 × 102 copies/mL could be detected in specimens taken from the lymph nodes or infected tissues in samples of PCV-2-infected pigs. The diagnosis of PCV-1 and PCV-2 infections and the quantification of the viral load in the field samples could be completed within 45 minutes after extracting the viral DNA using a commercial extraction kit.
               
                  Conclusion
                  This study demonstrate that real-time PCR is a clinically feasible method for the accurate quantification of PCV-2, and for the rapid differentiation of PCV-1 and PCV-2.",industry
10.1016/j.eswa.2009.09.057,Journal,Expert Systems with Applications,scopus,2010-04-01,sciencedirect,Rule based system for power quality disturbance classification incorporating S-transform features,https://api.elsevier.com/content/abstract/scopus_id/71349083572,"Detection and classification of power quality (PQ) disturbances in real-time is an important consideration to electric utilities and many industrial customers so that diagnosis and mitigation of such disturbances can be implemented quickly. This paper presents the design and development of a rule based system for intelligent classification of PQ disturbances using the S-transform features. A hardware system has been designed using advanced digital signal processor to provide fast data capture and processing of signals using the S-transform analysis. Distinct features of various disturbances are extracted from the S-transform analysis in which these features are used to formulate rules. A rule-based expert system is developed to automate the process of classifying the various types of disturbances. The disturbance classification results prove that the developed rule based system is more accurate than the neural network in classifying PQ disturbances such as voltage sag, swell, impulsive transient, notching and interruption.",industry
10.1016/j.cej.2010.01.018,Journal,Chemical Engineering Journal,scopus,2010-03-01,sciencedirect,"Industrial batch dryer data mining using intelligent pattern classifiers: Neural network, neuro-fuzzy and Takagi-Sugeno fuzzy models",https://api.elsevier.com/content/abstract/scopus_id/76449108616,"This contribution describes the pattern recognition based data analysis of an existing industrial batch dryer, and the comparison of three artificial intelligence techniques suited to perform classification tasks: neural networks trained using the Levenberg–Marquardt and the Levenberg–Marquardt method with Bayesian regularization, the neuro-fuzzy model based on clustering and grid partition, and the Takagi–Sugeno fuzzy models. The classifiers are used to quantify the dryer batch time and its variation during a certain production period, thus the motivation behind the work is genuine. The presented pattern recognition method implements a supervised learning approach and is based on pressure measurement profiles recorded by the plant data management software—the PI System from OSIsoft.
                  It is found that the neural networks trained with the Bayesian regularization have shown the most robust classification performance with respect to separation threshold selection. Furthermore, it is concluded that the application of artificial intelligence techniques in real chemical manufacturing facilities is feasible and provides useful information for process performance monitoring purposes. The pattern recognition findings presented in this paper are not case specific and can be directly used for the monitoring of a large variety of drying processes since the pressure profile features – vacuum check, pressure decrease, vacuum break – do not depend on the chemicals which are dried. Since the development of the artificial intelligent classifiers is presented in detail and step by step, this work may be interesting as a pattern recognition tutorial for chemical engineers.",industry
10.3182/20100831-4-fr-2021.00069,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2010-01-01,sciencedirect,Synthetic target systems in control education: Lessons teachers are learning from students,https://api.elsevier.com/content/abstract/scopus_id/84901939930,"In modern Programmable Logic Controllers (PLCs) programming education and training, software packages emulating industrial plants are replacing physical target systems. Whilst this approach is indubitably cost and safety effective, it is important to understand how educationally effective it is, and to what extent can it replace training on real plants. The paper analyses this problem from the feedback that authors got from HMS research and their students in a more than ten years build up experience of control education based on both real and synthetic systems. It concludes that synthetic plants are indeed effective in many training scenarios, but real target systems are still irreplaceable for a cluster of applications. Moreover, despite computer games technology is making synthetic target systems very appealing, both educators and simulation software developers recognize that there is still room for improvement. As such, seizing recent advances in computer technology, software developers are preparing a new generation of synthetic plants.",industry
10.1016/j.robot.2010.04.001,Journal,Robotics and Autonomous Systems,scopus,2010-01-01,sciencedirect,Visual servoing of redundant manipulator with Jacobian matrix estimation using self-organizing map,https://api.elsevier.com/content/abstract/scopus_id/80052724345,"Vision based redundant manipulator control with a neural network based learning strategy is discussed in this paper. The manipulator is visually controlled with stereo vision in an eye-to-hand configuration. A novel Kohonen’s self-organizing map (KSOM) based visual servoing scheme has been proposed for a redundant manipulator with 7 degrees of freedom (DOF). The inverse kinematic relationship of the manipulator is learned using a Kohonen’s self-organizing map. This learned map is shown to be an approximate estimate of the inverse Jacobian, which can then be used in conjunction with the proportional controller to achieve closed loop servoing in real-time. It is shown through Lyapunov stability analysis that the proposed learning based servoing scheme ensures global stability. A generalized weight update law is proposed for KSOM based inverse kinematic control, to resolve the redundancy during the learning phase. Unlike the existing visual servoing schemes, the proposed KSOM based scheme eliminates the computation of the pseudo-inverse of the Jacobian matrix in real-time. This makes the proposed algorithm computationally more efficient. The proposed scheme has been implemented on a 7 DOF PowerCube™ robot manipulator with visual feedback from two cameras.",industry
10.1016/j.eswa.2010.04.092,Journal,Expert Systems with Applications,scopus,2010-01-01,sciencedirect,A Bayesian petrophysical decision support system for estimation of reservoir compositions,https://api.elsevier.com/content/abstract/scopus_id/77957846821,"The exploration for oil and gas requires real-time petrophysical expertise to interpret measurement data acquired in boreholes and to recommend further steps. High time pressure and the far reaching nature of these decisions, as well as the limited opportunity to gain in depth petrophysical experience suggests that a decision support system that can aid the petrophysicist will be very useful.
                  In this paper we describe a Bayesian approach for obtaining compositional estimates that combines expert knowledge with information obtained from measurements. We define a prior model for the compositional volume fractions and observation models for each of the measurement tools. Both prior and observation models are based on domain expertise. These models are combined in a joint probability model. To deal with the nonlinearities in the model, Bayesian inference is implemented by using the hybrid Monte Carlo algorithm.
                  In the system, tool measurement values can entered and the posterior probability distribution of the compositional fractions can be obtained by applying Bayes’ rule. Bayesian inference is also used for optimal tool selection, using conditional entropy to select the most informative tool to obtain better estimates of the reservoir.
                  Reliability and consistency of the method is demonstrated by inference on synthetically generated data.",industry
10.1016/j.apenergy.2009.10.026,Journal,Applied Energy,scopus,2010-01-01,sciencedirect,A new spinning reserve requirement forecast method for deregulated electricity markets,https://api.elsevier.com/content/abstract/scopus_id/77950860487,"Ancillary services are necessary for maintaining the security and reliability of power systems and constitute an important part of trade in competitive electricity markets. Spinning Reserve (SR) is one of the most important ancillary services for saving power system stability and integrity in response to contingencies and disturbances that continuously occur in the power systems. Hence, an accurate day-ahead forecast of SR requirement helps the Independent System Operator (ISO) to conduct a reliable and economic operation of the power system. However, SR signal has complex, non-stationary and volatile behavior along the time domain and depends greatly on system load. In this paper, a new hybrid forecast engine is proposed for SR requirement prediction. The proposed forecast engine has an iterative training mechanism composed of Levenberg–Marquadt (LM) learning algorithm and Real Coded Genetic Algorithm (RCGA), implemented on the Multi-Layer Perceptron (MLP) neural network. The proposed forecast methodology is examined by means of real data of Pennsylvania–New Jersey–Maryland (PJM) electricity market and the California ISO (CAISO) controlled grid. The obtained forecast results are presented and compared with those of the other SR forecast methods.",industry
10.1016/j.ces.2009.11.003,Journal,Chemical Engineering Science,scopus,2010-01-01,sciencedirect,Neurocontrol of a multi-effect batch distillation pilot plant based on evolutionary reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/76049117435,"The time cost of first-principles dynamic modelling and the complexity of nonlinear control strategies may limit successful implementation of advanced process control. The maximum return on fixed capital within the processing industries is thus compromised. This study introduces a neurocontrol methodology that uses partial system identification and symbiotic memetic neuro-evolution (SMNE) for the development of neurocontrollers. Partial system identification is achieved using singular spectrum analysis (SSA) to extract state variables from time series data. The SMNE algorithm uses a symbiotic evolutionary algorithm and particle swarm optimisation to learn optimal neurocontroller weights from the partially identified system within a reinforcement learning framework. A multi-effect batch distillation (MEBAD) pilot plant was constructed to demonstrate the real world application of the neurocontrol methodology, motivated by the nonsteady state operation and nonlinear process interaction between multiple distillation columns. Multi-loop proportional integral (PI) control was implemented as a reduced model, reflecting an approach involving no modelling or significant controller tuning. Rapid multiple input multiple out nonlinear controller development was achieved using SSA and the SMNE algorithm, demonstrating comparable time and cost to implementation in relation to the reduced model. The optimal neurocontroller reduced the batch time and therefore the energy consumption by 45% compared to conventional multi-loop SISO PI control.",industry
10.1016/j.desal.2009.02.065,Journal,Desalination,scopus,2009-11-30,sciencedirect,Factorial experimental design for biosorption of iron and zinc using Typha domingensis phytomass,https://api.elsevier.com/content/abstract/scopus_id/71249156043,"Typha domingensis phytomass was used as a biosorbent for metal ions removal from wastewater. A full 23 factorial design of experiments was used to obtain the best conditions of biosorption of Fe3+ and Zn2+ from water solutions. The three factors considered were temperature, pH, and biosorbent dosage. Two levels for each factor were used; pH (2.5 and 6.0), temperature (25 and 45°C), and phytomass loading weight (0.5 and 1g/50ml). Batch experiments were carried out using 50ml solutions containing 10mg/l Fe3+ and 4mg/l Zn2+ simulating the concentration of those metals in a real wastewater effluent. The removal percentages of iron and zinc after 120min of contact time were then evaluated. The results were analyzed statistically using the Minitab 15 statistical software to determine the most important factors affecting the metals removal efficiency. The pH was found to be the most significant factor for the two studied metal ions.",industry
10.1016/j.ejor.2008.11.024,Journal,European Journal of Operational Research,scopus,2009-11-16,sciencedirect,"Tactical level planning in float glass manufacturing with co-production, random yields and substitutable products",https://api.elsevier.com/content/abstract/scopus_id/67349125429,"We investigate tactical level planning problems in float glass manufacturing. Float glass manufacturing is a process that has some unique properties such as uninterruptible production, random yields, partially controllable co-production compositions, complex relationships in sequencing of products, and substitutable products. Furthermore, changeover times and costs are very high, and production speed depends significantly on the product mix. These characteristics render measurement and management of the production capacity difficult. The motivation for this study is a real life problem faced at Trakya Cam in Turkey. Trakya Cam has multiple geographically separated production facilities. Since transportation of glass is expensive, logistics costs are high. In this paper, we consider multi-site aggregate planning, and color campaign duration and product mix planning. We develop a decision support system based on several mixed integer linear programming models in which production and transportation decisions are made simultaneously. The system has been fully implemented, and has been in use at Trakya Cam since 2005.",industry
10.1016/j.cie.2009.06.009,Journal,Computers and Industrial Engineering,scopus,2009-11-01,sciencedirect,Using Minimum Quantization Error chart for the monitoring of process states in multivariate manufacturing processes,https://api.elsevier.com/content/abstract/scopus_id/71849108091,"The need for multivariate statistical process control (MSPC) becomes more important as several variables should be monitored simultaneously. MSPC is implemented using a variety of techniques including neural networks (NNs). NNs have excellent noise tolerance in real time, requiring no hypothesis on statistical distribution of monitored processes. This feature makes NNs promising tools used for monitoring process changes. However, major NNs applied in SPC are based on supervised learning, which limits their wide applications. In the paper, a Self-Organizing Map (SOM)-based process monitoring approach is proposed for enhancing the monitoring of manufacturing processes. It is capable to provide a comprehensible and quantitative assessment value for current process state, which is achieved by the Minimum Quantization Error (MQE) calculation. Based on these MQE values over time series, an MQE chart is developed for monitoring process changes. The performance of MQE chart is analyzed in a bivariate process under the assumption that the predictable abnormal patterns are not available. The performance of MQE is further evaluated in a semiconductor batch manufacturing process. The experimental results indicate that MQE charts can become an effective monitoring and analysis tool for MSPC.",industry
10.1016/j.biotechadv.2009.05.003,Journal,Biotechnology Advances,scopus,2009-11-01,sciencedirect,Advances in on-line monitoring and control of mammalian cell cultures: Supporting the PAT initiative,https://api.elsevier.com/content/abstract/scopus_id/70349949073,"In recent years, much attention has been directed towards the development of global methods for on-line process monitoring, especially since the Food and Drug Administration (FDA) launched the Process Analytical Technology (PAT) guidance, stimulating biopharmaceutical companies to update their monitoring tools to ensure a pre-defined final product quality. The ideal technologies for biopharmaceutical processes should operate in situ, be non-invasive and generate on-line information about multiple key bioprocess and/or metabolic variables. A wide range of spectroscopic techniques based on in situ probes have already been tested in mammalian cell cultures, such as near infrared (NIR), mid infrared (MIR), 2D fluorescence and dielectric capacitance spectroscopy; similarly, the electronic nose technique based on chemical array sensors has been tested for in situ off-gas analysis of mammalian cell cultures. All these methods provide series of spectra, from which meaningful information must be extracted. In this sense, data mining techniques such as principal components regression (PCR), partial least squares (PLS) or artificial neural networks (ANN) have been applied to handle the dense flow of data generated from the real-time process analyzers. Furthermore, the implementation of feedback control methods would help to improve process performance and ultimately ensure reproducibility. This review discusses the suitability of several spectroscopic techniques coupled with chemometric methods for improved monitoring and control of mammalian cell processes.",industry
10.1016/j.engappai.2009.03.001,Journal,Engineering Applications of Artificial Intelligence,scopus,2009-06-01,sciencedirect,An implementing framework for holonic manufacturing control with multiple robot-vision stations,https://api.elsevier.com/content/abstract/scopus_id/67349215793,"The paper describes a holonic control architecture and implementing issues for agile job shop assembly with networked intelligent robots, based on the dynamic simulation of material processing and transportation. The holarchy was defined considering the PROSA reference architecture relative to which in-line vision-based quality control was added by help of feature-based descriptions of the material flow. Two solutions for production planning are proposed: a knowledge-based algorithm using production rules, and an OO resolved scheduling rate planner (RSRP) based on variable-timing simulation. Failure- and recovery-management are developed as generic scenarios embedding the CNP mechanism into production self-rescheduling. Aggregate Order Holon execution is realized by OPC-based PLC software integration and event-driven product transportation. The holonic control of multiple networked robot-vision stations also features tolerance to station computer- (IBM PC-type), station controller- (robot controller), quality control- (machine vision) and communication- (LAN) failure. Fault tolerance and high availability at shop-floor level are provided due to the multiple physical communication capabilities of the robot controllers, to their multiple-axis multitasking operating capability, and to hardware redundancy of single points of failure (SPOF). Implementing solutions and experiments are reported for a 6-station robot-vision assembly cell with twin-track closed-loop pallet transportation system and product-racking RD/WR devices. Future developments will consider manufacturing integration at enterprise level.",industry
10.1016/j.commatsci.2008.04.030,Journal,Computational Materials Science,scopus,2009-03-01,sciencedirect,Hybrid intelligent approach for modeling and optimization of semiconductor devices and nanostructures,https://api.elsevier.com/content/abstract/scopus_id/59749102668,"In this work, we present a hybrid intelligent approach for parameter extraction and design optimization of semiconductor nanoscale devices and nanostructures. Based on evolutionary algorithms, numerical methods, neural network scheme and parallel computing technique, the optimization methodology is developed and successfully implemented. In the hybrid approach, an evolutionary algorithm, such as genetic algorithm or particle swarm optimization, firstly searches the entire problem space to get a set of roughly estimated solutions. The numerical method, such as Levenberg–Marquardt method, then performs a local optima search and sets the local optima as the suggested values for the genetic algorithm to perform further optimizations. Meanwhile, the neural network is applied to investigate the influence of parameters on the optimized functions which thus guides the evolutionary direction of genetic algorithm. For solving real world problems, all functional blocks are performed under a PC-based Linux cluster system with message-passing interface libraries. This hybrid intelligent approach has experimentally been implemented and validated for different applications in semiconductor nanodevices and nanostructures. For semiconductor nanodevice parameter extraction, this approach shows its capability to automatically extract a set of global parameters among sixteen 90nm complementary metal oxide semiconductor (CMOS) devices. Compared with the measured current–voltage (I–V) curves of fabricated CMOS samples, the optimized I–V results are within 3% of accuracy. The computational examinations including sensitivity, convergence property, and parallelization are discussed. For parameter extraction of organic light emitting diode (OLED), the approach also achieves good accuracy for red, green, blue OLEDs. For the third and fourth applications, optimal structure design of silicon photonic taper waveguide and photonic crystal are further advanced by integrating a simulation-based technique in the developed system. All of these experiments demonstrate interesting results and validate the optimization methodology. The concept of hybrid intelligent approach may benefit modeling and optimization in diverse science and engineering problems.",industry
10.1016/j.engappai.2008.05.009,Journal,Engineering Applications of Artificial Intelligence,scopus,2009-02-01,sciencedirect,Identifying source(s) of out-of-control signals in multivariate manufacturing processes using selective neural network ensemble,https://api.elsevier.com/content/abstract/scopus_id/58249092619,"In multivariate statistical process control (MSPC), most multivariate quality control charts are shown to be effective in detecting out-of-control signals based upon an overall statistic. But these charts do not relieve the need for pinpointing source(s) of the out-of-control signals. Neural networks (NNs) have excellent noise tolerance and high pattern identification capability in real time, which have been applied successfully in MSPC. This study proposed a selective NN ensemble approach DPSOEN, where several selected NNs are jointly used to classify source(s) of out-of-control signals in multivariate processes. The immediate location of the abnormal source(s) can greatly narrow down the set of possible assignable causes, facilitating rapid analysis and corrective action by quality operators. The performance of DPSOEN is analyzed in multivariate processes. It shows improved generalization performance that outperforms those of single NNs and Ensemble All approach. The investigation proposed a heuristic approach for applying the DPSOEN-based model as an effective and useful tool to identify abnormal source(s) in bivariate statistical process control (SPC) with potential application for MSPC in general.",industry
10.1016/j.compchemeng.2008.05.019,Journal,Computers and Chemical Engineering,scopus,2009-01-13,sciencedirect,ANN-based soft-sensor for real-time process monitoring and control of an industrial polymerization process,https://api.elsevier.com/content/abstract/scopus_id/57049112694,"This paper presents the development and the industrial implementation of a virtual sensor (soft-sensor) in the polyethylene terephthalate (PET) production process. This soft-sensor, based on a feed-forward artificial neural network (ANN), was primarily used to provide on-line estimates of the PET viscosity, which is necessary for process control purposes. The ANN-based soft-sensor (ANN-SS) was also used for providing redundant measurements of the viscosity that could be compared to the results obtained from the process viscometer. It was shown that the proposed ANN-SS was able to adequately infer the polymer viscosity, in such a way so as this soft-sensor could be used in the real-time process control strategy. The proposed control system has successfully been applied in servo and regulatory problems, thus allowing an effective and feasible operation of the industrial plant.",industry
10.3182/20090603-3-RU-2001.0447,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2009-01-01,sciencedirect,Telematics application to optimize operation process of municipal heat and power plant,https://api.elsevier.com/content/abstract/scopus_id/79960930362,"Municipal heat and power plant is a complex and huge industrial system. The main elements of the system are the heating boilers. To perform the operation process of the boilers efficiently it is necessary to monitor a wide range of operation parameters in a real time. Great number of the parameters, short response time and big distance between the controlled objects are the main reasons for telematic systems implementation. But, the basic conditions of telematic system application are: measurement instruments and proper control algorithm. Nowadays, heating boilers are equipped with measurement systems by the producer. The heating boilers are very expensive devices whose operation phase is very long. Therefore, in many municipal heat and power plants, the production process is carried out using old type heating boilers. In such cases telematic systems should operate in spite of limited measurement vector. To do this, special control procedures ought to be implemented. In the paper, the heating boiler control algorithms are presented. Described algorithms are used in case of real industrial objects where a set of monitored parameters is not sufficient for executing full automatic control. An expert system dedicated to support the operation process of heating boiler is also presented. Because of limited information about the heating boiler operation, the process is controlled approximately. To deal with the indicated problem, the idea of fuzzy sets implementation is also described. The presented method of control process fuzzification can increase the quality of heating boiler operation. It will make the implementation of power industry telematics more efficient.",industry
10.3182/20090603-3-RU-2001.0280,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2009-01-01,sciencedirect,Multi-agent reinforcement learning for adaptive scheduling: Application to multi-site company,https://api.elsevier.com/content/abstract/scopus_id/79960899247,"In recent years, most companies have resorted to multi-site organization in an effort to improve their competitiveness and to adapt to current conditions. In this article, we propose a model for adaptive scheduling in multi-site companies. We adopt a multi-agent approach in which intelligent agents have reactive learning ability. This allows them to make accurate short-term decisions. Our model is implemented on a 3-tier architecture that ensures the security of the data exchanged between the various company sites. Experimentations on a real case study demonstrate the applicability and the effectiveness of our model concerning both optimality and reactivity.",industry
10.1016/j.mechatronics.2009.02.007,Journal,Mechatronics,scopus,2009-01-01,sciencedirect,Neural network based design of fault-tolerant controllers for automated sequential manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/67349155544,"This paper presents a novel application of recurrent neural network (RRN) to fault-tolerant control (FTC) of automated sequential manufacturing systems (ASMS) subject to sensor faults. Two RRNs are employed: the first one acts as an I/O relations recognizer and is able to detect faulty sensors and the latter is used as an inverse model of the AMSM to compute the desired control action in a faulty case according to nominal specifications. The learning process of these networks is carried out based on training data generated from the healthy manufacturing system controlled by a programmable logic controller (PLC). Design of the proposed fault-tolerant control system (FTCS) scheme is based on utilizing the two RNNs, a reconfigurable controller and a fault decision subsystem. The design procedure of the proposed FTCS is introduced. The proposed FTCS has been implemented and tested experimentally for a benchmark industrial ASMS subject to single or multiple faulty sensors. Experimental results show the effectiveness of the procedure for a real simple plant. In addition, the results prove these features of the proposed FTCS: (a) effectively improving the faulty control system behaviors, (b) accomplishing its proper functionality in handling single and multiple sensor faults, (c) identifying the sensor faults, and (d) being advantageous in reducing the complexity of the hardware redundancy.",industry
10.3168/jds.2008-1163,Journal,Journal of Dairy Science,scopus,2009-01-01,sciencedirect,"Prediction of coagulation properties, titratable acidity, and pH of bovine milk using mid-infrared spectroscopy",https://api.elsevier.com/content/abstract/scopus_id/58349117919,"This study investigated the potential application of mid-infrared spectroscopy (MIR 4,000–900cm−1) for the determination of milk coagulation properties (MCP), titratable acidity (TA), and pH in Brown Swiss milk samples (n=1,064). Because MCP directly influence the efficiency of the cheese-making process, there is strong industrial interest in developing a rapid method for their assessment. Currently, the determination of MCP involves time-consuming laboratory-based measurements, and it is not feasible to carry out these measurements on the large numbers of milk samples associated with milk recording programs. Mid-infrared spectroscopy is an objective and nondestructive technique providing rapid real-time analysis of food compositional and quality parameters. Analysis of milk rennet coagulation time (RCT, min), curd firmness (a30, mm), TA (SH°/50mL; SH°=Soxhlet-Henkel degree), and pH was carried out, and MIR data were recorded over the spectral range of 4,000 to 900cm−1. Models were developed by partial least squares regression using untreated and pretreated spectra. The MCP, TA, and pH prediction models were improved by using the combined spectral ranges of 1,600 to 900cm−1, 3,040 to 1,700cm−1, and 4,000 to 3,470cm−1. The root mean square errors of cross-validation for the developed models were 2.36min (RCT, range 24.9min), 6.86mm (a30, range 58mm), 0.25 SH°/50mL (TA, range 3.58 SH°/50mL), and 0.07 (pH, range 1.15). The most successfully predicted attributes were TA, RCT, and pH. The model for the prediction of TA provided approximate prediction (R2
                     =0.66), whereas the predictive models developed for RCT and pH could discriminate between high and low values (R2
                     =0.59 to 0.62). It was concluded that, although the models require further development to improve their accuracy before their application in industry, MIR spectroscopy has potential application for the assessment of RCT, TA, and pH during routine milk analysis in the dairy industry. The implementation of such models could be a means of improving MCP through phenotypic-based selection programs and to amend milk payment systems to incorporate MCP into their payment criteria.",industry
10.1016/j.jchromb.2008.11.006,Journal,Journal of Chromatography B: Analytical Technologies in the Biomedical and Life Sciences,scopus,2009-01-01,sciencedirect,Application of solid-phase microextraction and gas chromatography-mass spectrometry for measuring chemicals in saliva of synthetic leather workers,https://api.elsevier.com/content/abstract/scopus_id/57549092683,"Saliva is of interest as a diagnostic aid for oral and systemic diseases, to monitor therapeutic drugs, and detect illicit drug abuse. It is also attractive for biological monitoring of exposure to hazardous solvents. The major advantage of this indicator over other biological monitoring targets is that the saliva is noninvasive and less confidential in comparison with blood and urine. Salivary analysis is generally acceptable by study subjects and can be applied to investigation of a wide variety of compounds. However, very few studies have been conducted on the saliva matrix to monitor exposure to hazardous solvents. The aim of this study is to establish an analytical method, headspace solid-phase microextraction (HS-SPME) followed by gas chromatography–mass spectrometry (GC–MS), by which the saliva matrix can be monitored for multiple compounds with various polarities, such as methyl ethyl ketone (MEK), isopropyl alcohol (IPA), and N,N-dimethyl formamide (DMF) (common solvents used in synthetic leather manufacture), as well as acetone (ACE) and N-methyl formamide (NMF) (metabolites of IPA and DMF, respectively). We studied this technique as an alternative biological monitoring method for investigating exposure to hazardous solvents. A Carboxen/Polydimethylsiloxane (CAR/PDMS 75μm) fiber coating was employed for this study, and various extraction and desorption parameters were evaluated. The extraction efficiency and reproducibility of analyses was improved by pre-incubation. The limits of detection were 0.004, 0.003, 0.006, 0.05, and 0.10μg/mL for ACE, MEK, IPA, DMF, and NMF, respectively. Method validation was performed on standards spiked in blank saliva, and a correlation was made between HS-SPME and traditional solvent pretreatment methods. It was found that correlation coefficients (r) were greater than 0.996 for each analyte, with no significant differences (p
                     >0.05) between two methods. However, the SPME method achieved lower limits of detection, with good accuracy (recovery 95.3–109.2%) and precision (1.17–8.22% CV) for both intra- and inter-assay, when quality control samples were analyzed for all five compounds. The partition coefficient for each compound between the headspace of the saliva sample and the CAR/PDMS fiber coating was 90.9, 170.1, 36.4, 3.70 and 0.92 for ACE, MEK, IPA, DMF and NMF, respectively. Real sample analyses were performed on workers in a synthetic leather factory. In summary, the SPME method is a highly versatile and flexible technique for chemical measurement, and we demonstrate its application for monitoring biological exposure to hazardous solvents. Saliva monitoring using sensitive SPME approaches for determining workplace exposure should prove useful as an alternative exposure monitoring method.",industry
10.1016/j.eswa.2008.02.053,Journal,Expert Systems with Applications,scopus,2009-01-01,sciencedirect,Software reliability identification using functional networks: A comparative study,https://api.elsevier.com/content/abstract/scopus_id/56349084084,"Software engineering development has gradually become essential element in different aspects of the daily life and an important factor in numerous critical real-industry applications, such as, nuclear plants, medical monitoring control, real-time military, bioinformatics, oil and gas industry, and air traffic control. This paper proposes a functional network as a novel computational intelligence scheme for tracking and predicting the software reliability. Several applications are presented to illustrate this new intelligent system framework models. To demonstrate the usefulness of functional networks and the existing data mining schemes, we briefly describe the learning algorithm of functional networks associativity model in predicting the software reliability. Comparative studies will be carried out to compare the performance of functional networks with the most popular existing data mining techniques, such as, statistical regression multilayer feed forward neural networks, and support vector machines. The results show that the performance of functional networks is more reliable, stable, accurate, and outperforms other techniques.",industry
10.1016/j.eswa.2007.09.058,Journal,Expert Systems with Applications,scopus,2009-01-01,sciencedirect,Knowledge acquisition for decision support systems on an electronic assembly line,https://api.elsevier.com/content/abstract/scopus_id/53849097356,"Increasing global competition has made many manufacturing companies recognize that competitive manufacturing in terms of low cost and high quality is crucial for success. Real-time process control and production optimization are, however, extremely challenging areas because manufacturing processes are getting ever more complex and involve many different parameters. This is a major problem when building decision support systems especially in electronics manufacturing. Although problem-solving is a knowledge intensive activity undertaken by people on the production floor, it is quite common to have large databases and run blindly feature extraction and data mining methods. Performance of these methods could, however, be drastically increased when combined with knowledge or expertise of the process.
                  This paper describes how defect-related knowledge on an electronic assembly line can be integrated in the decision making process at an operational and organizational level. It focuses in particular on the efficient acquisition of shallow knowledge concerning everyday human interventions on the production lines, as well as on the factory-wide sharing of the resulting information for an improved defect management. Software with dedicated interfaces has been developed using a knowledge representation that supports portability and flexibility of the system. Semi-automatic knowledge acquisition from the production floor and generation of comprehensive reports for the quality department resulted in an improvement of the usability, usage, and usefulness of the decision support system.",industry
10.1016/j.petrol.2008.08.002,Journal,Journal of Petroleum Science and Engineering,scopus,2008-12-01,sciencedirect,Integration of seismic attributes and production data for infill drilling strategies - A virtual intelligence approach,https://api.elsevier.com/content/abstract/scopus_id/57049110413,"Field development strategies are at the forefront of common engineering practices in the oil and gas industry. Reservoir simulation is the most commonly applied methodology to generate an optimum field development plan. However, reservoir simulation can be an energy and cost intensive method that often relies on rather subjective assumption of input parameters, due to lack of accurate field data. In this paper, a new approach using Artificial Neural Network (ANN) technology is proposed to predict individual well performances and accordingly develop infill drilling strategies. Due to its predictive capabilities, ANN is used as a tool to construct a correlation for production prediction. Seismic attributes, which capture heterogeneity of the reservoir geology, and completion information are used as network inputs. In calculating the interference effects, the geometry of the flow system under consideration was used together with the geometric location and the starting production schedule of each well within the system. The method was successfully implemented on a case study of the 19N 94W Township of the Wamsutter field in Wyoming using actual seismic attributes, completion information, well configuration, and production data. Production predictions were generated by the network for all locations at which seismic attributes were available. More promising locations were then selected for infill drilling purposes based on predicted productions at these locations. The predicted initial rate and 10-yr cumulative production were considered in the selection of infill drilling locations with high productivity potential. Results from this work show that the ANN was able to map the relationship between production, completion information, interference effects, and reservoir characteristics captured in seismic attributes. The proposed methodology allowed the construction of spatial maps of gas production, revealing new sweet spots which could not be identified from the existing production history alone. The production maps derived from the ANN predictions contain important heterogeneous features associated with reservoir properties reflected in seismic data. Even though well interference was initially thought to have a limited effect on well performance for the case study presented, the incorporation of well interference parameters in the network design improved production predictions, suggesting that well interference has a more significant impact on well performance than originally anticipated.",industry
10.1016/j.ijmachtools.2008.06.003,Journal,International Journal of Machine Tools and Manufacture,scopus,2008-11-01,sciencedirect,A new approach to identifying the elastic behaviour of a manufacturing machine,https://api.elsevier.com/content/abstract/scopus_id/49949095540,"The dynamic solicitations imposed on modern production machines make the rigid body hypothesis obsolete in mechanical design. To increase productivity while ensuring effector trajectory quality, numerical control of the machine has to integrate the elastic deformations of machine parts. To reach this objective, the elastic behaviour of the machine has to be accurately predicted by a simple but efficient elastic model. This model should involve low-cost computation to be implemented in a real-time control loop, and also necessitates an accurate identification process to attain the required precision.
                  The paper proposes to model the elastic behaviour of the machine using an efficient mass–spring–damper model. The main contribution lies in a new approach to accurately identifying the elastic model parameters.
                  Instead of using modal analysis, the new approach is based on time domain identification using a specific metrological device. The measurement system developed and described in this paper is based on combined computer vision and inertial sensor data.
                  First, the displacements of points of interest on the machine part are tracked and measured by a standard digital camera (0.8Mega pixels and 30frames/s). A subpixelar detector allows us to reach a level of accuracy equal to 30μm (maximum error) for a displacement up to 300mm away from the sensor, but 30Hz image sampling does not provide enough information to get all details of the overall movement.
                  So secondly, accelerometers are added to get the required information to over-sample the vision data without additional error; by doing so the effective sampling frequency increases from 30Hz to 15kHz.
                  The proposed identification method is applied to a real industrial system; the results obtained in elastic deformation prediction are quite satisfactory and validate the new approach proposed in this paper.",industry
10.1016/j.eswa.2007.08.079,Journal,Expert Systems with Applications,scopus,2008-11-01,sciencedirect,Adaptive burn-in time decision system based on pattern recognition for intelligent reliability control,https://api.elsevier.com/content/abstract/scopus_id/48949098603,"Most semiconductor companies usually screen out early field failures of semiconductor device by conducting burn-in test for all manufactured devices. Burn-in is a production process whereby all manufactured devices operated under accelerated stresses for constant periods of time and accordingly crucial in productivity, on-time delivery and quality of semiconductor device. Many researches on the determination of optimal burn-in time and schedule of its operations have been conducted. Most of them, however, had some limitations to apply to real world because of their complexity and practical difficulties. Our study aims at providing an easy, efficient and more practical alternative approach. We present a multi-agent-based system, called adaptive burn-in time decision system (ABITDS) that predict the reliability level of a newly manufactured semiconductor lot on the basis of fail patterns of previously manufactured lots and then make an adaptive decision of burn-in time according to the predicted reliability level. The ABITDS uses SOM (Self-Organizing Map) neural network to firstly extract the patterns of defective chips within the wafer, wafer bin map patterns, of previously manufactured lots with good, normal and bad reliability level and predict the reliability levels of newly manufactured lots by measuring the similarity degrees between their wafer bin map patterns and the extracted ones. We implemented a web-based ABITDS prototype and validated the effectiveness of our approach through its application to real data of a semiconductor company.",industry
10.1016/j.micpro.2008.04.002,Journal,Microprocessors and Microsystems,scopus,2008-10-01,sciencedirect,A tunable high-performance architecture for enhancement of stream video captured under non-uniform lighting conditions,https://api.elsevier.com/content/abstract/scopus_id/54549122634,"A novel architecture for performing hue-saturation-value (HSV) domain enhancement of digital color images captured under non-uniform lighting conditions is proposed in this paper for video streaming applications. The approach promotes log-domain computation to eliminate all multiplications, divisions and exponentiations utilizing the compact high-speed logarithmic estimation modules. An optimized quadrant symmetric architecture is incorporated into the design of homomorphic filter for the enhancement of intensity value. Efficient modules are also presented for conversion between RGB and HSV color spaces with tunable H and S components in HSV for more flexible color rendering. The design is able to bring out details hidden in shadow regions of the image and preserve the bright parts with adjustable vividness and color shift for improvement of visual quality while maintaining its consistency. It is capable of producing 187.86 million outputs per second (MOPs) on Xilinx’s Virtex II XC2V2000-4ff896 field programmable gate array (FPGA) at a clock frequency of 187.86MHz. It can process over 179.1 (1024×1024) frames per second, which is very suitable for high definition videos, and consumes approximately 70.7% and 76.8% less hardware resource with 127% and 280% performance boost when compared to the designs with machine learning algorithm in [M.Z. Zhang, M.J. Seow, V.K. Asari, A high performance architecture for color image enhancement using a machine learning approach, International Journal of Computational Intelligence Research – Special Issue on Advances in Neural Networks 2(1) (2006) 40–47], and with separated dynamic and contrast enhancements in [H.T. Ngo, M.Z. Zhang, L. Tao, V.K. Asari, Design of a high performance architecture for real-time enhancement of video stream captured in extremely low lighting environment, International Journal of Embedded Systems: Special Issue on Media and Stream Processing, in press], respectively. This approach also provide 83.4 times performance gain with more consistent fidelity in the results compared to some DSP based implementations (256×256 frame size) [G.D. Hines, Z. Rahman, D.J. Jobson, G.A. Woodell, DSP implementation of the retinex image enhancement algorithm, visual information processing XIII, in: Proceedings of the SPIE, vol. 5438, 2004, pp. 13–24; G.D. Hines, Z. Rahman, D.J. Jobson, G.A. Woodell, Single-scale retinex using digital signal processors, in: Proceedings of the Global Signal Processing Conference, September 2004, pp. 1–6] under the reflectance-illuminance category of image enhancement models.",industry
10.1016/j.apmr.2007.11.030,Journal,Archives of Physical Medicine and Rehabilitation,scopus,2008-05-01,sciencedirect,Development of a Wheelchair Virtual Driving Environment: Trials With Subjects With Traumatic Brain Injury,https://api.elsevier.com/content/abstract/scopus_id/42649105111,"Spaeth DM, Mahajan H, Karmarkar A, Collins D, Cooper RA, Boninger ML. Development of a wheelchair virtual driving environment: trials with subjects with traumatic brain injury.
               
                  Objective
                  To develop and test a wheelchair virtual driving environment that can provide quantifiable measures of driving ability, offer driver training, and measure the performance of alternative controls.
               
                  Design
                  A virtual driving environment was developed. The wheelchair icon is displayed in a 2-dimensional, bird's eye view and has realistic steering and inertial properties. Eight subjects were recruited to test the virtual driving environment. They were clinically evaluated for range of motion, muscle strength, and visual field function. Driving capacity was assessed by a brief trial with an actual wheelchair. During virtual trials, subjects were seated in a stationary wheelchair; a standard motion sensing joystick (MSJ) was compared with an experimental isometric joystick by using a repeated-measures design.
               
                  Setting
                  Subjects made 2 laboratory visits. The first visit included clinical evaluation, tuning the isometric joystick, familiarization with virtual driving environment, and 4 driving tasks. The second visit included 40 trials with each joystick.
               
                  Participants
                  Subjects (n=8; 7 men, 1 woman) with a mean age of 22.65±2y and traumatic brain injury, both ambulatory and nonambulatory, were recruited.
               
                  Interventions
                  The MSJ used factory settings. A tuning program customized the isometric joystick transfer functions during visit 1. During the second visit, subjects performed 40 trials with each joystick.
               
                  Main Outcome Measure
                  The root mean square error (RMSE) was defined as the average deviation from track centerline (in meters) and speed (in m/s).
               
                  Results
                  Data analysis from the first 8 subjects showed no statistically significant differences between joysticks. RMSE averaged .12 to .21m; speed averaged .75m/s. For all tasks and joysticks, driving in reverse resulted in a higher RMSE and more virtual collisions than forward driving. RMSE rates were greater in left and right turns than straight and docking tasks.
               
                  Conclusions
                  Testing with instrumented real wheelchairs can validate the virtual driving environment and assess whether virtual driving skills transfer to actual driving.",industry
10.1016/j.snb.2007.12.032,Journal,"Sensors and Actuators, B: Chemical",scopus,2008-04-14,sciencedirect,Preemptive identification of optimum fermentation time for black tea using electronic nose,https://api.elsevier.com/content/abstract/scopus_id/41449097471,"During black tea manufacturing, tealeaves pass through the fermentation process, when the grassy smell is transformed into a floral smell. Optimum fermentation is extremely crucial in deciding the final quality of finished tea and it is very important to terminate the fermentation process at the right time. Present day industry practice for monitoring of fermentation is purely subjective and is carried out by experienced personnel. In this paper, a study has been made on real-time smell monitoring of black tea during the fermentation process using electronic nose as well as prediction of the correct fermentation time. The study has been implemented in two steps. First, for prediction of optimum fermentation time, five different time-delay neural networks (TDNNs), named as multiple-time-delay neural networks (m-TDNN), have been used. During the second study, we have investigated the possibility of existence of different smell stages during the fermentation runs of black tea processing using self-organizing map (SOM), and then used three TDNNs for different smell stages. The results show excellent promise for the instrument to be used by the industry.",industry
10.1016/j.jmatprotec.2007.05.044,Journal,Journal of Materials Processing Technology,scopus,2008-01-21,sciencedirect,A systematic solution methodology for inferential multivariate modelling of industrial grinding process,https://api.elsevier.com/content/abstract/scopus_id/36549063418,"The need for precision components and parts in manufacturing industries has bought an increase in the need for finishing operations that can satisfy this demand. In addition, there is a continuous demand for hard and tough materials that can withstand varying stress conditions to ensure prolonged service life of components and parts. The need to process these materials economically so as to meet stringent product quality requirements (generally expressed as composite of a family of properties, so-called multiple response characteristics) has become a real challenge for researchers and practitioners in manufacturing industries. Grinding has the potential to meet these critical needs for accurate and economic means of finishing parts, and generate the required surface topography. Despite this importance and popularity, grinding still remains one of the most difficult and least-understood processes due to lack of adequate inferential mechanistic and analytical multivariate models, for varied industrial situations. In this context, data-driven inferential linear or nonlinear multiple statistical regression, and artificial neural network modelling have become increasingly popular techniques for complex industrial grinding processes. Unfortunately, these techniques are either proposed and implemented in isolation or presented as a comparative evaluation grinding case study. A systematic solution methodology for inferential multivariate modelling, which addresses the different phases, starting from preliminary linear random x-case multivariate regression model, hypothesis testing of influence of addition of higher-order nonlinear terms to the adequate linear model (or presence of nonlinearity), and subsequent selection of a suitable nonlinear artificial neural network-based multivariate model, is lacking. In view of the above-mentioned conditional requirements, this paper attempts to provide a systematic methodology to develop a multivariate linear regression model, hypothesis testing for the influence of nonlinear terms to linear model, and accordingly selection of a suitable artificial neural network-based inferential model with improved prediction accuracy and control of grinding behaviour. The methodology suggests the use of various statistical techniques, such as Q–Q (quantile–quantile) plotting, data transformation, data standardization, outlier detection test, model adequacy test, model cross-validation and generalization. The suitability of the recommended methodology is illustrated with the help of an engine cylinder liner grinding (honing) case example, in a leading automotive manufacturing unit in India.",industry
10.1016/j.energy.2008.03.004,Journal,Energy,scopus,2008-01-01,sciencedirect,A new approach to ensure successful implementation of sustainable demand side management (DSM) in South African mines,https://api.elsevier.com/content/abstract/scopus_id/45849141587,"Demand side management (DSM) is seen as a short-term solution to the imminent problem of electricity supply shortages in South Africa. DSM aims to reduce peak loads with immediate results in a short time. The mining sector in South Africa is a large energy user with pumping one of the largest consuming systems. Therefore, DSM potential (load shift) should be investigated on these pumping systems.
                  For sustainable load shift, a system is required that simulates, optimises and controls the actual on-site situation. As no such equipment that performs all these processes could be found for deep South African mines, it was developed by HVAC International (Pty) Ltd. It is called the Real-time Energy Management System (REMS). With this system, maximum results can be obtained on a sustainable basis.
                  In this study, four similar DSM projects were investigated. These are described as case studies at gold mines in the Free State Province. For each of these studies a different new innovation was implemented. The innovations described include the adaptation of REMS to handle multi-level intricate pump systems, mines without any instrumentation and control infrastructure, as well as Three Chamber Piped Feeder Systems (3CPFSs).",industry
10.1016/j.asoc.2007.02.015,Journal,Applied Soft Computing Journal,scopus,2008-01-01,sciencedirect,Dynamic data mining technique for rules extraction in a process of battery charging,https://api.elsevier.com/content/abstract/scopus_id/40649097043,"Battery charging controllers design and application is a growing industry direction. Fast and efficient charging of battery packs is a problem which is difficult and often expensive to solve using conventional techniques. The majority of existing works on intelligent charging systems are based on expert knowledge and heuristics. Not all features of the desired charging behavior can be attained by the hard-wired logic implemented by expert generated rules. Because the battery charging is a highly dynamic process and the chemical technology a battery uses varies significantly for different battery types, data mining technique can be of real importance for extracting the charging rules from the large databases, especially when the charging logic is to be continuously changed during the life of the battery dependent on the type and characteristics of the battery and utilization conditions. In this paper we use soft computing-based data mining technique for extraction of control rules for effective and fast battery charging process. The obtained rules were used for NiCd battery charging. The comparative performance evaluation was done among the existing charging control methods and the proposed system, which demonstrated a significant increase of performance (minimum charging time and minimum overheating) using the soft computing-based approach.",industry
10.1016/j.epsr.2006.12.005,Journal,Electric Power Systems Research,scopus,2008-01-01,sciencedirect,ATC enhancement using TCSC via artificial intelligent techniques,https://api.elsevier.com/content/abstract/scopus_id/35148890919,Procurement of optimum available transfer capability (ATC) in the restructured electricity industry is a crucial challenge with regards to open access to transmission network. This paper presents an approach to determine the optimum location and optimum capacity of TCSC in order to improve ATC as well as voltage profile. Real genetic algorithm (RGA) associated with analytical hierarchy process (AHP) and fuzzy sets are implemented as a hybrid heuristic technique in this paper to optimize such a complicated problem. The effectiveness of the proposed methodology is examined through different case studies.,industry
10.1016/j.simpat.2007.04.011,Journal,Simulation Modelling Practice and Theory,scopus,2007-09-01,sciencedirect,Grey-box modeling of a motorcycle shock absorber for virtual prototyping applications,https://api.elsevier.com/content/abstract/scopus_id/34547773158,"There is an increasing use of virtual prototyping tools in the motorcycle industry, aimed at reducing the development time of new models and speeding up performance optimization, by providing the designer with an in-laboratory virtual test track. Virtual prototyping software are multibody simulation software, which require the availability of models of all the vehicle components. The choice of the model is then of paramount importance, since it heavily affects the accuracy and reliability of the simulation results. Conventional models (like linear models) are often inadequate to describe the behavior of complex nonlinear components, so that it is necessary to appeal to different modeling approaches. This is actually the case when dealing with motorcycle suspension systems, given that their most critical part, the shock absorber, exhibits nonlinear and time-variant behavior.
                  In this paper, a grey-box model of a racing motorcycle mono-tube shock absorber is proposed, which consists of a nonlinear parametric model and a black-box, neural-network-based model. The absorber model has been implemented in a numerical simulation environment, and validated against experimental test data. The results of the validation show that the model is able to reproduce the real behavior of the shock absorber with an accuracy that matches or even beats that of other models previously presented in the literature. The interfacing of the proposed model to the ADAMS virtual prototyping environment is also discussed.",industry
10.1016/j.cor.2005.05.019,Journal,Computers and Operations Research,scopus,2007-04-01,sciencedirect,Using mega-trend-diffusion and artificial samples in small data set learning for early flexible manufacturing system scheduling knowledge,https://api.elsevier.com/content/abstract/scopus_id/33748743279,"Neural networks are widely utilized to extract management knowledge from acquired data, but having enough real data is not always possible. In the early stages of dynamic flexible manufacturing system (FMS) environments, only a litter data is obtained, and this means that the scheduling knowledge is often unreliable. The purpose of this research is to utilize data expansion techniques for an obtained small data set to improve the accuracy of machine learning for FMS scheduling. This research proposes a mega-trend-diffusion technique to estimate the domain range of a small data set and produce artificial samples for training the modified backpropagation neural network (BPNN). The tool used is the Pythia software. The results of the FMS simulation model indicate that learning accuracy can be significantly improved when the proposed method is applied to a very small data set.",industry
10.1016/j.conengprac.2006.06.005,Journal,Control Engineering Practice,scopus,2007-02-01,sciencedirect,Prioritised A* search in real-time elevator dispatching,https://api.elsevier.com/content/abstract/scopus_id/33750177574,"Under the typical operating conditions of an elevator system, there is insufficient time to consider all dispatching alternatives and the major elevator companies normally adopt empirical techniques to reduce complexity and achieve acceptable performance. The current work has been able to demonstrate that in practical circumstances an optimal solution to the real-time elevator-dispatching problem can be obtained. The elevator dispatching problem is formulated as a heuristic search and is implemented using a novel extension of the popular A* search, termed prioritised A*, that retains the desirable admissibility and monotonicity of A*. PA* makes best use of the limited time available by ensuring the dispatcher considers the most important aspect of the problem first, namely to give each elevator its first assignment. In a manufacturing process, this is equivalent to ensuring that each machine is immediately given its first job, while the determination of the detailed order of the remaining jobs is refined later. This study has obtained access to extensive data records collected from installed elevator systems and their analysis has led to the identification of new passenger models able to deliver suitable high quality predictive data to improve the operations of the dispatcher.",industry
10.1016/B978-044452206-1/50034-3,Book,Parallel Computational Fluid Dynamics 2005,scopus,2006-12-01,sciencedirect,Numerical simulation of transonic flows by a double loop flexible evolution,https://api.elsevier.com/content/abstract/scopus_id/78651562872,"The chapter presents a methodology that uses evolutionary algorithms to obtain fluid velocities for potential flows inside a nozzle. A genetic algorithm with real encoding which, with the technique of partial sampling of the solution nodes, presents a number of advantages over other evolutionary algorithms is proposed in the chapter. These include the avoidance of a rigid connectivity to discretize the domain, thus making it a meshless method. The chapter also proposes a double-loop strategy with EAs which enables improvement of the methodology. The amount of computer storage is low and convergence behavior is good as the qualitative characteristics of the solution are taken into account in the algorithm. The artificial intelligence based on evolutionary algorithms can bring together lessons learned by developers and decision tools into advanced software developments, which can facilitate their use to solve complex challenging problems in industry.",industry
10.1016/j.compchemeng.2006.05.005,Journal,Computers and Chemical Engineering,scopus,2006-11-01,sciencedirect,An integrated framework for on-line supervised optimization,https://api.elsevier.com/content/abstract/scopus_id/33750367584,We present a finite element numerical study of heat transfer in lid driven channels with fully developed axial flow for non-Newtonian power law fluids. The effect of channel aspect ratio and material properties on temperature distribution and wall heat transfer are studied. The results show that in comparison with Newtonian fluids the shear thinning property of the fluids acts to reduce the local viscous dissipative heating and as a result the axial local fluid temperature is reduced. Applications of the results to scraped-surface heat exchanger design and operation are recommended.,industry
10.1016/j.ijmachtools.2005.10.002,Journal,International Journal of Machine Tools and Manufacture,scopus,2006-10-01,sciencedirect,Integrated machining error compensation method using OMM data and modified PNN algorithm,https://api.elsevier.com/content/abstract/scopus_id/33746476938,"This paper presents an integrated machining error compensation method based on polynomial neural network (PNN) approach and inspection database of on-machine-measurement (OMM) system. To improve the accuracy of the OMM system, geometric errors of the CNC machining center and probing errors are compensated. Machining error distributions of a specimen workpiece are measured to obtain error compensation parameters. To efficiently analyze the machining errors, two machining error parameters, Werr
                      and Derr
                     , are defined. Subsequently, these parameters can be modeled using the PNN approach, which is used to determine machining errors for the considered cutting conditions. Consequently, by using an iterative algorithm, tool path can be corrected to effectively reduce machining errors in the end-milling process. Required programs are developed using Ch language, and modified termination method are applied to reduce computation times. Experiments are carried out to validate the approaches proposed in this paper. The proposed integrated machining error compensation method can be effectively implemented in a real machining situation, producing much fewer errors.",industry
10.1016/j.compind.2006.02.011,Journal,Computers in Industry,scopus,2006-08-01,sciencedirect,SIMAP: Intelligent System for Predictive Maintenance. Application to the health condition monitoring of a windturbine gearbox,https://api.elsevier.com/content/abstract/scopus_id/33746237617,"SIMAP is the abbreviated name for the Intelligent System for Predictive Maintenance. It is a software application addressed to the diagnosis in real-time of industrial processes. It takes into account the information coming in real-time from different sensors and other information sources and tries to detect possible anomalies in the normal behaviour expected of the industrial components. The incipient detection of anomalies allows for an early diagnosis and the possibility to plan effective maintenance actions. Also, the continuous monitoring performed allows for an estimation in a qualitative form of the health condition of the components. SIMAP is a general tool oriented to the diagnosis and maintenance of industrial processes, however the first experience of its application has been at a windfarm. In this real case, SIMAP is able to optimize and to dynamically adapt a maintenance calendar for a monitored windturbine according to the real needs and operating life of it as well as other technical and economical criteria. In particular this paper presents the application of SIMAP to the health condition monitoring of a windturbine gearbox as an example of its capabilities and main features.",industry
10.1016/j.snb.2005.12.065,Journal,"Sensors and Actuators, B: Chemical",scopus,2006-07-28,sciencedirect,A novel chemical detector using cermet sensors and pattern recognition methods for toxic industrial chemicals,https://api.elsevier.com/content/abstract/scopus_id/33646858326,"The development and evaluation of a novel gas sensing system intended for use on US Navy vessels is presented. The purpose of this sensor is to provide real-time detection and quantification of a wide range of known toxic chemicals including toxic industrial chemicals (TICs) and combustible or corrosive gases. The sensor system incorporates an array of ceramic-metal (cermet) gas microsensors with multivariate pattern recognition techniques and represents a rugged, light-weight, and low-cost solution to analysis problems that would otherwise need to be addressed with multiple conventional electrochemical sensors. The smart microsensor arrays are being developed by combining cermet electrochemical sensors utilizing cyclic voltammetry, with intelligent firmware and software to drive the sensors and analyze the data. The chemical microsensor architecture may be modified for detection selectivity of a variety of chemical species. The microsensor arrays have potential application for monitoring hazardous chemicals in the part-per-million to part-per-billion range in a variety of internal and external environments. The arrays sense analytes using pattern recognition techniques to determine the presence of vapors of interest. A test demonstrator has been developed with a four-sensor array, readout electronics, and system control software. The four-sensor array was exposed to 15 test vapors. The 15-analyte sources, including two blood agents, 10 TICs and three simulants were generated at five different concentrations in humid air. The cermet sensor array provided unique responses for the various analytes tested. Similar analyte types produced similar results. The sensitivity is sufficient to detect all the analytes at their respective exposure limits. A variety of feature selection and pattern recognition methods are being investigated for robust detection.",industry
10.1016/j.simpat.2005.10.012,Journal,Simulation Modelling Practice and Theory,scopus,2006-07-01,sciencedirect,A virtual shop modeling system for industrial fabrication shops,https://api.elsevier.com/content/abstract/scopus_id/33744546468,"Industrial fabrication is both a production system with a high product mix and a project-based industry. The accuracy of short-term project-based planning, such as estimating and project scheduling, is extremely important for a project’s success. This paper proposes an integrated modeling system that explicitly models the product mix in order to improve the planning accuracy. Automated process planning and a processing time estimation method were implemented through the integration of simulation with such modeling methods as computer aided process planning and artificial neural networks. Two case studies are presented to demonstrate the capability of the proposed system.",industry
10.1016/j.jmatprotec.2005.04.041,Journal,Journal of Materials Processing Technology,scopus,2006-06-01,sciencedirect,An intelligent system for monitoring and optimization of ball-end milling process,https://api.elsevier.com/content/abstract/scopus_id/33646812230,"The paper presents an intelligent system for on-line monitoring and optimization of the cutting process on the model of the ball-end milling. An intelligent system for monitoring and optimization in ball-end milling is developed both in hardware and software. It is based on a PC, which is connected to the CNC main processor module through a serial-port so that control and communication can be realised. The monitoring system is based on LabVIEW software, the data acquisition system and the measuring devices (sensors) for the cutting force measuring. The system collects the variables of the cutting process by means of sensors. The measured values are delivered to the computer program through the data acquisition system for data processing and analysis. The optimization technique is based on genetic algorithms for the determination of the cutting conditions in machining operations. In metal cutting processes, cutting conditions have an influence on reducing the production cost and time and deciding the quality of a final product. Experimental results show that the proposed genetic algorithm-based procedure for solving the optimization problem is effective and efficient, and can be integrated into a real-time intelligent manufacturing system for solving complex machining optimization problems.",industry
10.1016/j.fm.2005.05.003,Journal,Food Microbiology,scopus,2006-05-01,sciencedirect,"Development of a real-time PCR assay targeting the sporulation gene, spo0A, for the enumeration of thermophilic bacilli in milk powder",https://api.elsevier.com/content/abstract/scopus_id/26044470853,"Thermophilic bacilli, such as Anoxybacillus, Geobacillus and Bacillus, are common contaminants growing within the processing lines of milk powder producing factories. These contaminants are used as indicator organisms for plant hygiene and specification limits based on their numbers have been implemented to ensure milk powder quality. In this study, we present a SYBR Green-based real-time PCR assay for the rapid detection and enumeration of these thermophilic bacilli in milk powder using the spo0A sporulation gene as quantification target. With this method the detection of thermophilic bacilli in milk powder can be accomplished within 1h. The detection limit for reconstituted and inoculated milk was 80vegetative cfuml−1 and 640sporesml−1, respectively.",industry
10.1016/j.engappai.2004.12.003,Journal,Engineering Applications of Artificial Intelligence,scopus,2005-08-01,sciencedirect,NEFCLASS-based neuro fuzzy controller for SRM drive,https://api.elsevier.com/content/abstract/scopus_id/18144374571,"Switched reluctance motor (SRM) is increasingly employed in industrial applications where variable speed is required because of their simple construction, ease of maintenance, low cost and high efficiency. However, the SRM performance often degrades for the machine parameter variations. The SRM converter is difficult to control due to its nonlinearities and parameter uncertainties. In this paper, to overcome this problem, a neuro fuzzy controller (NFC) is proposed. Heuristic rules are derived with the membership functions of the fuzzy variables tuned by a neural network (NN). The algorithm is implemented on a digital signal processor (TMS320F240) allowing great flexibility for various real time applications. Experimental results demonstrate the effectiveness of the NFC with various working conditions of the SRM.",industry
10.1016/j.ins.2004.09.011,Journal,Information Sciences,scopus,2005-05-13,sciencedirect,On the design of intelligent robotic agents for assembly,https://api.elsevier.com/content/abstract/scopus_id/16344396133,"Robotic agents can greatly be benefited from the integration of perceptual learning in order to monitor and adapt to changing environments. To be effective in complex unstructured environments, robots have to perceive the environment and adapt accordingly. In this paper it is discussed a biology inspired approach based on the adaptive resonance theory (ART) and implemented on an KUKA KR15 industrial robot during real-world operations (e.g. assembly operations). The approach intends to embed naturally the skill learning capability during manufacturing operations (i.e., within a flexible manufacturing system).
                  The integration of machine vision and force sensing has been useful to demonstrate the usefulness of the cognitive architecture to acquire knowledge and to effectively use it to improve its behaviour. Practical results are presented, showing that the robot is able to recognise a given component and to carry out the assembly. Adaptability is validated by using different component geometry during assemblies and also through skill learning which is shown by the robot’s dexterity.",industry
10.1016/j.asoc.2004.07.001,Journal,Applied Soft Computing Journal,scopus,2005-01-01,sciencedirect,Application of internal model control methods to industrial combustion,https://api.elsevier.com/content/abstract/scopus_id/10844285344,"Most practical systems are inherently non-linear to some extent in their behaviour and for their cost effective, smooth and safe operation, optimised control systems based on the non-linear models are required. To this end many useful techniques such as the stochastic modelling, sliding mode control and adaptive identification and control have been proposed in the literature. However, the high cost of implementation, the inability to capture imprecision with the required level of tolerance, and the in-flexibility against distortions in the operating variables, make them less attractive. To this end new artificial intelligence based techniques such as fuzzy logic, neural networks and probabilistic reasoning, are becoming more and more popular. Among these techniques neural networks have an edge over the others, mainly because of their ability to process large amount of available data, subsequent to the development of some interpretable models for solving engineering problems. Moreover, the ability to capture the non-linearities of a real system accurately and the versatility in being able to accommodate with ease, the various conventional and advanced strategies within their structures, make them much more attractive. The problem becomes more computationally worse and uncontrollable when inverse of the system does not exist. This problem is resolved when neural network based techniques such as internal model control (IMC) are applied to the real systems.
                  This paper outlines the application of neural networks based IMC methods for estimation/control of important input and output variables of a 0.5MW laboratory scale industrial furnace. The application involves inputs such as the airflow rate, swirl number and momentum ratio. The outputs include emission levels of oxides of nitrogen especially nitric oxide. The response to step and staircase inputs has been analysed. The results have been compared with standard linear quadratic controller. The control output of the IMC methods has resulted in almost similar steady state error performance to the linear quadratic regulator. Although the development process of the IMC method might take longer time because of the training and data arrangement but has the capability of readjustment after being developed.",industry
10.1016/j.compedu.2003.12.001,Journal,Computers and Education,scopus,2004-12-01,sciencedirect,Virtual reality simulations and animations in a web-based interactive manufacturing engineering module,https://api.elsevier.com/content/abstract/scopus_id/3042807121,"This paper presents a web-based interactive teaching package that provides a comprehensive and conducive yet dynamic and interactive environment for a module on automated machine tools in the Manufacturing Division at the National University of Singapore. The use of Internet technologies in this teaching tool makes it possible to conjure visualisations that cannot be achieved using traditional teaching materials such as transparencies. Virtual reality simulations and animations were developed and appropriately placed in the teaching materials to enhance the student understanding of complex concepts. This is especially useful in teaching automated machine tools, which deals primarily with the numerical control (NC) of the motions of automated machine tools. These virtual reality simulations and animations provide the capability of training students in NC programming and operations without the need to work on actual NC machines in the laboratory. The simulations are suitably placed in the package to engage the students and enhance their concentration, while at the same time generate interactions. Customised question types were also designed and implemented with a tutorial monitoring application.",industry
10.1016/j.envsoft.2003.10.003,Journal,Environmental Modelling and Software,scopus,2004-08-01,sciencedirect,Modelling SO<inf>2</inf> concentration at a point with statistical approaches,https://api.elsevier.com/content/abstract/scopus_id/3342982389,"In this paper, the results obtained by inter-comparing several statistical techniques for modelling SO2 concentration at a point such as neural networks, fuzzy logic, generalised additive techniques and other recently proposed statistical approaches are reported. The results of the inter-comparison are the fruits of collaboration between some of the partners of the APPETISE project funded under the Framework V Information Societies and Technologies (IST) programme. Two different cases for study were selected: the Siracusa industrial area, in Italy, where the pollution is dominated by industrial emissions and the Belfast urban area, in the UK, where domestic heating makes an important contribution. The different kinds of pollution (industrial/urban) and different locations of the areas considered make the results more general and interesting. In order to make the inter-comparison more objective, all the modellers considered the same datasets. Missing data in the original time series was filled by using appropriate techniques. The inter-comparison work was carried out on a rigorous basis according to the performance indices recommended by the European Topic Centre on Air and Climate Change (ETC/ACC). The targets for the implemented prediction models were defined according to the EC normative relating to limit values for sulphur dioxide. According to this normative, three different kinds of targets were considered namely daily mean values, daily maximum values and hourly mean values. The inter-compared models were tested on real cases of poor air quality. In the paper, the inter-compared techniques are ranked in terms of their capability to predict critical episodes. A ranking in terms of their predictability of the three different targets considered is also proposed. Several key issues are illustrated and discussed such as the role of input variable selection, the use of meteorological data, and the use of interpolated time series. Moreover, a novel approach referred to as the technique of balancing the training pattern set, which was successfully applied to improve the capability of ANN models to predict exceedences is introduced. The results show that there is no single modelling approach, which generates optimum results in terms of the full range of performance indices considered. In view of the implementation of a warning system for air quality control, approaches that are able to work better in the prediction of critical episodes must be preferred. Therefore, the artificial neural network prediction models can be recommended for this purpose. The best forecasts were achieved for daily averages of SO2 while daily maximum and hourly mean values are difficult to predict with acceptable accuracy.",industry
10.1016/j.engappai.2004.03.001,Journal,Engineering Applications of Artificial Intelligence,scopus,2004-04-01,sciencedirect,Artificial neural networks and neuro-fuzzy systems for modelling and controlling real systems: A comparative study,https://api.elsevier.com/content/abstract/scopus_id/2942564462,"This article presents a comparison of artificial neural networks and neuro-fuzzy systems applied for modelling and controlling a real system. The main objective is to model and control the temperature inside of a kiln for the ceramic industry. The details of all system components are described. The steps taken to arrive at the direct and inverse models using the two architectures: adaptive neuro fuzzy inference system and feedforward neural networks are described and compared. Finally, real-time control results using internal model control strategy are presented.
                  Using available Matlab software for both algorithms, the objective is to show the implementation steps for modelling and controlling a real system. Finally, the performances of the two solutions were compared through different parameters for a specific real didactic case.",industry
10.1016/j.engappai.2003.11.008,Journal,Engineering Applications of Artificial Intelligence,scopus,2004-02-01,sciencedirect,Incorporating fuzzy approaches for production planning in complex industrial environments: The roll shop case,https://api.elsevier.com/content/abstract/scopus_id/17544390830,"Operation of complex shops needs specific, sophisticated procedures in order to guarantee competitive plant performance. In this paper we present a hierarchy of models for roll shop departments in the steel industry, focusing on the calculation of the priority of the rolls to produce. A fuzzy-based model was developed and implemented in a real environment, allowing the simulation of expert behaviour, considering the characteristics of an environment with imprecise information. A description of the model and implementation experiences in a real shop are reported.",industry
10.1016/j.compind.2003.06.002,Journal,Computers in Industry,scopus,2004-02-01,sciencedirect,Web-based search system of pattern recognition for the pattern of industrial component by an innovative technology,https://api.elsevier.com/content/abstract/scopus_id/0347593788,"The real-time system uses a recurrent neural network (RNN) with associative memory for training and recognition. This study attempts to use associative memory to apply pattern recognition (PR) technology to the real-time pattern recognition of engineering components in a web-based recognition system with a client–server network structure. Remote engineers can draw the shape of the engineering components using the browser, and the recognition system then searches the component database via the Internet. Component patterns are stored in the database system considered here. Moreover, the data fields of each component pattern contain the properties and specifications of that pattern, except in the case of engineering components. The database system approach significantly improves recognition system capacity. The recognition system examined here employs parallel computing, which increases system recognition rate. The recognition system used in this work is an Internet based client–server network structure. The final phase of the system recognition applies database matching technology to processing recognition, and can solve the problem of spurious states. The system considered here is implemented on the Yang-Fen Automation Electrical Engineering Company as a case study. The experiment is continued for 4 months, and engineers are also used to operating the web-based pattern recognition system. Therefore, the cooperative plan described above is analyzed and discussed here. Finally, these papers propose the tradition methods compare with the innovative methods.",industry
10.1016/s1474-6670(17)31023-6,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2004-01-01,sciencedirect,Real-Time requirements in diagnostic systems,https://api.elsevier.com/content/abstract/scopus_id/85064459122,"While artificial intelligence methodologies are being applied towards increasingly realistic domains that require timely responses, real-time systems are coming to incorporate decision-making tools that require more intelligent capabilities. This paper’ describes a distributed multi agent architecture considering industrial demands for realtime diagnostic and decision support systems. The correctness of the behavior of such a system depends on the results of the computation as well as on the time at which the results can be provided. Especially in distributed systems, a proper handling of real-time requirements together with a deterministic behavior of all parts is extremely important to guarantee reliable and accurate data processing.",industry
10.1016/j.cie.2004.05.005,Journal,Computers and Industrial Engineering,scopus,2004-01-01,sciencedirect,Application of neural networks to heuristic scheduling algorithms,https://api.elsevier.com/content/abstract/scopus_id/3242706865,"This paper considers the use of artificial neural networks (ANNs) to model six different heuristic algorithms applied to the n job, m machine real flowshop scheduling problem with the objective of minimizing makespan. The objective is to obtain six ANN models to be used for the prediction of the completion times for each job processed on each machine and to introduce the fuzziness of scheduling information into flowshop scheduling. Fuzzy membership functions are generated for completion, job waiting and machine idle times. Different methods are proposed to obtain the fuzzy parameters. To model the functional relation between the input and output variables, multilayered feedforward networks (MFNs) trained with error backpropagation learning rule are used. The trained network is able to apply the learnt relationship to new problems. In this paper, an implementation alternative to the existing heuristic algorithms is provided. Once the network is trained adequately, it can provide an outcome (solution) faster than conventional iterative methods by its generalizing property. The results obtained from the study can be extended to solve the scheduling problems in the area of manufacturing.",industry
10.1016/j.cie.2004.05.002,Journal,Computers and Industrial Engineering,scopus,2004-01-01,sciencedirect,Functional evaluation of an event detection ensemble to detect anomalous system behavior,https://api.elsevier.com/content/abstract/scopus_id/3242702363,"Surveillance systems are established to manage the complexity of ensuring processes of interest behave as expected. They are formed in response to public demand that systems—natural and human-made—be predictable, managed, and under control. This paper examines the functionality of an event detection ensemble used to detect anomalous conditions a system of interest may exhibit. The event detection ensemble consists of an agent and its associated sensors, models, and detectors. In addition, the subsystem is presented in the broader context of a generalized surveillance design framework. A logical organization of components is presented as well as a demonstration implementation scheme. The event detection ensemble is evaluated for functionality using two hypothetical test cases representing real-world applications. Event signatures based upon prediction, extrapolation, and domain discrepancies are characterized and simulated. In addition, a radial basis neural network is employed to create a model capable of distinguishing these types of discrepancies. Finally, application and benefits of this design approach is discussed with respect to designing surveillance systems for human-made and natural systems. Discussion includes how this design framework can be applied to detect events in a wide array of industrial engineering applications.",industry
10.1016/S0895-7177(03)00120-1,Journal,Mathematical and Computer Modelling,scopus,2003-05-15,sciencedirect,The application of intelligent agent technology to simulation,https://api.elsevier.com/content/abstract/scopus_id/0038686924,"We have been successfully applying intelligent agent technology to several real world simulation problems such as semiconductor manufacturing processes, train operations, and electric motor assembly lines.
                  Agent technology is a methodology to realize an autonomous decentralized system with cooperative interactions among agents that model each element of the system. An intelligent agent has problem solving and learning skills, as well as the knowledge for that purpose. An agent is characterized as having autonomy, sociality, reactivity, and proactiveness.
                  In the development of a software system using agent technology, each element of the system is described in an independent and modular program code. This method makes addition, change, and deletion of an element much easier than the case of conventional programming. In almost any real world system, each element works mostly in an independent and parallel manner yet with interactions with each other, and matches well with the concept of an agent.
                  We have chosen a platform called the PIM, parallel inference machine, that describes and executes multiple agents in the independent and parallel manner. It makes the simulation application software development for a real world system much more straightforward than conventional computing platforms.
                  This paper describes the agent technology, its application to real world simulation and the platform “PIM”. It describes in some detail actual simulator application examples: semiconductor process system management, train operation management, and electric motor assembly line change.",industry
10.1016/S0305-0548(02)00044-8,Journal,Computers and Operations Research,scopus,2003-05-01,sciencedirect,Using MLP networks to design a production scheduling system,https://api.elsevier.com/content/abstract/scopus_id/0037405170,"This paper investigates the application of artificial neural networks to the problem of job shop scheduling with a scope of a deterministic time-varying demand pattern over a fixed planning horizon. The purpose of the research is to design and develop a job shop scheduling system (a scheduling software) that can generate effective job shop schedules using the multi-layered perceptron (MLP) networks. The contributions of this study include designing, developing, and implementing a production activity scheduling system using the MLP networks; developing a method for organizing sample data using a denotation bit to indicate processing sequence and processing time of a job simultaneously; using the back-propagation training process to control local minimal solutions; and developing a heuristics to improve and revise the initial production schedule. The proposed production activity schedule system is tested in a real production environment and illustrated in the paper with a sample case.",industry
10.1016/B978-075067495-9/50008-2,Book,Techniques for Adaptive Control,scopus,2003-01-01,sciencedirect,"Knowledgescape, an objectoriented real-time adaptive modeling and optimization expert control system for the process industries",https://api.elsevier.com/content/abstract/scopus_id/84904029450,"This chapter discusses KnowledgeScape, an object-oriented real-time expert control system for the process industries that have built-in adaptive modeling and optimization capabilities. The primary use of KnowledgeScape is the online continuous monitoring of plant performance and the calculation of new process setpoints, which maintain and optimize performance as feed and operating conditions vary. This model-based optimization ability adds true artificial intelligence to KnowledgeScape in that it creates the basis for learning about the process. It also discusses about the intelligent software objects, artificial intelligence tools, and process control and their use in KnowledgeScape. This chapter intents to present how neural networks are used within KnowledgeScape to provide learning and predictive capabilities; the implementation of genetic algorithms within Knowledgescape; and the Crisp Rules; Fuzzy Rules relating to KnowledgeScape. The sequence of steps needed to successfully configure and implement a KnowledgeScape system is summarized. This chapter concludes that a careful reflection of the descriptions of functions of KnowledgeScape suggests that its design can be used in a much broader sense. Specifically, KnowledgeScape can be used to embed intelligence in any process.",industry
10.1016/S1077-2014(03)00035-4,Journal,Real-Time Imaging,scopus,2003-01-01,sciencedirect,SVM approximation for real-time image segmentation by using an improved hyperrectangles-based method,https://api.elsevier.com/content/abstract/scopus_id/0142123423,"A real-time implementation of an approximation of the support vector machine (SVM) decision rule is proposed. This method is based on an improvement of a supervised classification method using hyperrectangles, which is useful for real-time image segmentation. The final decision combines the accuracy of the SVM learning algorithm and the speed of a hyperrectangles-based method. We review the principles of the classification methods and we evaluate the hardware implementation cost of each method. We present the combination algorithm, which consists of rejecting ambiguities in the learning set using SVM decision, before using the learning step of the hyperrectangles-based method. We present results obtained using Gaussian distribution and give an example of image segmentation from an industrial inspection problem. The results are evaluated regarding hardware cost as well as classification performances.",industry
10.1016/S0957-4158(03)00042-4,Journal,Mechatronics,scopus,2003-01-01,sciencedirect,Mechatronic design,https://api.elsevier.com/content/abstract/scopus_id/0041509193,"Mechatronic design is the integrated design of a mechanical system and its embedded control system. In order to make proper choices early in the design stage, tools are required that support modelling and simulation of physical systems––together with the controllers––with parameters that are directly related to the real-world system. Such software tools are becoming available now. Components in various physical domains (e.g. mechanical or electrical) can easily be selected from a library and combined into a ‘process’ that can be controlled by block-diagram-based (digital) controllers. A few examples will be discussed that show the use of such a tool in various stages of the design. The examples include a typical mechatronic system with a flexible transmission, a mobile robot, and an industrial linear motor with a neural-network-based learning feed-forward controller that compensates for cogging.",industry
10.1016/S0360-8352(03)00039-1,Journal,Computers and Industrial Engineering,scopus,2003-01-01,sciencedirect,A fuzzy neural network approach to machine condition monitoring,https://api.elsevier.com/content/abstract/scopus_id/0038789161,"This paper is focused on the implementation of a predictive neural network for use as an operator's aid in the diagnosis of faults with high prediction accuracy in an automated manufacturing environment In order to evaluate the performance of the model, the network has been tested using both simulated time series and real time machine vibration data collected in lab experiments.",industry
10.1016/s0141-9331(02)00069-8,Journal,Microprocessors and Microsystems,scopus,2002-12-20,sciencedirect,Real-time implementation of a dynamic fuzzy neural networks controller for a SCARA,https://api.elsevier.com/content/abstract/scopus_id/0037147579,"This paper presents the design, development and implementation of a Dynamic Fuzzy Neural Networks (D-FNNs) Controller suitable for real-time industrial applications. The unique feature of the D-FNNs controller is that it has dynamic self-organising structure, fast learning speed, good generalisation and flexibility in learning. The approach of rapid prototyping is employed to implement the D-FNNs controller with a view of controlling a Selectively Compliance Assembly Robot Arm (SCARA) in real time. Simulink, an iterative software for simulating dynamic systems, is used for modelling, simulation and analysis of the dynamic system. The D-FNNs controller was implemented through Real-Time Workshop (RTW). RTW generates C-codes from the Simulink block diagrams and in turn, the generated codes (object codes) are downloaded to the dSPACE DS1102 floating-point processor, together with the supporting files, for execution. The performance of the D-FNNs controller was found to be superior and it matches favourably with the simulation results.",industry
10.1016/S0924-0136(02)00201-7,Journal,Journal of Materials Processing Technology,scopus,2002-06-20,sciencedirect,AI-based condition monitoring of the drilling process,https://api.elsevier.com/content/abstract/scopus_id/0037142641,"With increasing competitive pressures, manufacturing systems in the automotive industry are being driven more and more aggressively. The pressures imposed on the processes and lack of system ‘slack’ have led to increased use of tool condition monitoring (TCM) systems. In parallel, there has been wide-ranging research in academia. However, a closer examination shows that there has been very little migration of this research into industrial practice. Furthermore, the success of industrially deployed monitoring systems has been poor. It has been suggested that a significant factor behind both these phenomenon has been the ‘difficult’ environment in which such systems must operate; an environment where they are subject to many stochastic influences, ranging from ambient conditions, to user input, to workpiece consistency.
                  Neural networks (NNs) have found increasing favour in manufacturing systems research because of their ability to perform robustly in noisy environments. Almost all the applications of this technology in TCM have been in the detection/prediction of tool wear. From an academic standpoint, it may be speculated that the lack of focus on breakage and missing tool detection has been due to the relatively trivial nature of detecting such anomalies in the laboratory environment. However, detection in the production environment is compromised by a wide range of factors, which can give rise to false alarms when such strategies are transported from laboratory conditions. In this paper, data from a real manufacturing process is used to demonstrate the potential application of NNs to the task of anomaly detection in the production environment.",industry
10.3182/20020721-6-es-1901.01502,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2002-01-01,sciencedirect,Neural network applications for model based fault detection with parity equations,https://api.elsevier.com/content/abstract/scopus_id/84866936261,"The rising complexity of modern automotive engines with an increasing number of actuators and sensors to minimise emissions and fuel consumption and to maximise engine driveability require a detailed supervision for fault detection and on-board diagnosis. The European Community Directive 98/69/EC requires on-board diagnosis for spark ignition engines and will require it for diesel engines as of January 2003, mainly to prevent excessive emissions. Beside this regulation it is also in the interest of the automobile manufactures to establish capable diagnosis systems for maintenance, repair and the benefit of their customers. This paper will describe applications of neural networks for modelling complex fluid- and thermodynamics with unknown physical model structure. Reference models, which describe the fault free process, are set up and identified with the special neural network LOLIMOT (Local-Linear-Model-Tree). Fault detection algorithms, which employ the method of parity equations, were successfully implemented and tested in real time with a 2 litre diesel engine and a Rapid Control Prototyping System. Measurements of online fault detection are shown for several built-in faults in the intake system of this diesel engine.",industry
10.1016/S0160-791X(02)00038-6,Journal,Technology in Society,scopus,2002-01-01,sciencedirect,Data mining techniques for customer relationship management,https://api.elsevier.com/content/abstract/scopus_id/0036863466,"Advancements in technology have made relationship marketing a reality in recent years. Technologies such as data warehousing, data mining, and campaign management software have made customer relationship management a new area where firms can gain a competitive advantage. Particularly through data mining—the extraction of hidden predictive information from large databases—organizations can identify valuable customers, predict future behaviors, and enable firms to make proactive, knowledge-driven decisions. The automated, future-oriented analyses made possible by data mining move beyond the analyses of past events typically provided by history-oriented tools such as decision support systems. Data mining tools answer business questions that in the past were too time-consuming to pursue. Yet, it is the answers to these questions make customer relationship management possible. Various techniques exist among data mining software, each with their own advantages and challenges for different types of applications. A particular dichotomy exists between neural networks and chi-square automated interaction detection (CHAID). While differing approaches abound in the realm of data mining, the use of some type of data mining is necessary to accomplish the goals of today’s customer relationship management philosophy.",industry
10.1016/S1359-835X(01)00013-6,Journal,Composites - Part A: Applied Science and Manufacturing,scopus,2001-12-01,sciencedirect,"Intelligent model-based control of preform permeation in liquid composite molding processes, with online optimization",https://api.elsevier.com/content/abstract/scopus_id/0035546371,"Manufacturing of quality products via liquid molding processes such as Resin Transfer Molding (RTM), calls for a precise control of resin progression through fibrous preforms during mold fill. Lack of an effective process control leads to formation of dry spots and voids that are detrimental to product quality. This study presents the use of physics-based process simulations in real-time, towards a generalized process control. The implementation of process simulations for on-line model-predictive control requires that the simulation time scales be less than the time scales of the process. An artificial neural network trained using data from numerical process models is used to provide rapid, real-time process simulations for the model-based control. A simulated annealing algorithm, working interactively with the neural network process model, is used to derive optimal control decisions rapidly and on-the-fly. The controller performance is systematically demonstrated for several processing scenarios.",industry
10.1016/S0950-7051(01)00151-4,Journal,Knowledge-Based Systems,scopus,2001-11-01,sciencedirect,Specifying fault tolerance in mission critical intelligent systems,https://api.elsevier.com/content/abstract/scopus_id/0035505448,"Real time intelligent systems are being increasingly used in mission critical applications in domains like military, aerospace, process control industry and medicine. Despite this vast potential, the major concern about deploying mission critical intelligent systems is their dependability. Dependability encompasses such notions as reliability, safety, security, maintainability and portability. A major concern about mission critical intelligent systems is their performance in the presence of failures. Intelligent systems are characterized by often non-existent, imprecise or rapidly changing specifications. This makes the task of characterizing an intelligent system's performance in the presence of failures much more difficult. In this paper, we characterize the failures that are likely in a mission critical intelligent system. We propose an extended I/O automata model to capture these failure specifications. We further demonstrate how these specifications can be realized in a real time expert system by structuring the knowledge base. This formalism can also be used to specify the fault tolerant properties of the underlying hardware and software over which the intelligent system resides. Thus we have an unified formalism to specify fault tolerance properties in hardware, system software and the intelligent system. This will enable us to reason about the performance of the entire system inclusive of all its components in an uniform manner.",industry
10.1016/S0952-1976(01)00033-1,Journal,Engineering Applications of Artificial Intelligence,scopus,2001-10-01,sciencedirect,Intelligent control of a rotary kiln fired with producer gas generated from biomass,https://api.elsevier.com/content/abstract/scopus_id/0035494049,"During the past decade, the academic world has been extremely active in developing new algorithms and theories in the field of artificial intelligence (AI) and intelligent systems. In most cases, however, emphasis has been placed more on theoretical frameworks and mathematical bases than on what the individual AI techniques could offer and on how different techniques could be applied to solve real industrial-scale problems. The reputation of intelligent systems has consequently suffered from an inability to transfer new and sophisticated techniques to industrial applications with identifiable benefits. As a result, although a wide range of intelligent control techniques has been available already for many years, most of the applications in the process industry are based on more conventional techniques. Recently, as awareness of intelligent systems has grown, industrial problems and implementations have fortunately received increasing attention. In this paper, an intelligent supervisory-level system implemented at one of the major Finnish pulp mills to control a lime kiln fired with producer gas generated from biomass is presented. First, the major results of a field study are summarised, with special attention paid to burnt lime quality aspects. Next, a novel linguistic equations approach, which provides flexible methods for both modelling and control, is briefly described. The overall structure and main functions of the developed control system are then described with the main emphasis on the control of temperature and lime quality. Finally, the results obtained during the extended testing period of the system are presented and discussed.",industry
10.1016/S0141-9331(01)00104-1,Journal,Microprocessors and Microsystems,scopus,2001-04-20,sciencedirect,QoS management in programmable networks through mobile agents,https://api.elsevier.com/content/abstract/scopus_id/0035917663,"In delivering multimedia services, quality of service represents a crucial commitment to be satisfied. Very often it has been considered only from a theoretical point of view, leaving any implementation details out of the discussion, mainly for the lack of concrete possibilities to execute its control and management effectively. Recent technological developments in the networking and distributed programming fields are now opening new challenging scenarios towards the negotiation and guarantee of QoS in the delivery of multimedia services through the network. Active or programmable networks are becoming a reality, and the migration of software components among network nodes seems to be the direction pursued by most of network manufactures. Mobile software agents represent a very attractive approach to the distributed control of computer networks and a valid alternative to the implementation of strategies for the management of QoS. In this paper, we present our approach to QoS management through mobile agents. The potentiality of this approach is shown through two application examples. The first one focuses on resource reservation through RSVP in an int-serv scenario, while the second one shows how to provide QoS to aggregated traffic flowing through a virtual network.",industry
10.1016/S0167-7799(00)01528-6,Journal,Trends in Biotechnology,scopus,2001-02-01,sciencedirect,Multivariate statistical monitoring of batch processes: An industrial case study of fermentation supervision,https://api.elsevier.com/content/abstract/scopus_id/0035253422,"This article describes the development of Multivariate Statistical Process Control (MSPC) procedures for monitoring batch processes and demonstrates its application with respect to industrial tylosin biosynthesis. Currently, the main fermentation phase is monitored using univariate statistical process control principles implemented within the G2 real-time expert system package. This development addresses integrating various process stages into a monitoring system and observing interactions among individual variables through the use of multivariate projection methods. The benefits of this approach will be discussed from an industrial perspective.",industry
10.1006/rtim.2001.0231,Journal,Real-Time Imaging,scopus,2001-01-01,sciencedirect,Real-time vision-based system for textile fabric inspection,https://api.elsevier.com/content/abstract/scopus_id/0035678020,"This paper presents an automatic vision-based system for quality control of web textile fabrics. The general hardware and software platform developed to solve this problem is presented and a powerful algorithm for defect inspection is proposed. Based on the improved binary, textural and neural network algorithms the proposed method gives good results in the detection of many types of fabric defects under real industrial conditions, where the presence of many types of noise is an inevitable phenomenon. A high detection rate with good localization accuracy, low rate of false alarms, compatibility with standard inspection tools and low price are the main advantages of the proposed system as well as the overall inspection approach.",industry
10.1016/S0360-8352(00)00078-4,Journal,Computers and Industrial Engineering,scopus,2001-01-01,sciencedirect,"Comparison of constraint logic programming and distributed problem solving: A case study for interactive, efficient and practicable job-shop scheduling",https://api.elsevier.com/content/abstract/scopus_id/0035247584,"The job-shop scheduling issue is more and more described not only in terms of efficiency (e.g. Makespan), but also in terms of interactivity and practicability. The aim of this paper is to evaluate the ability of two approaches inherited from artificial intelligence domain to contribute to the solving of this issue: Constraint Logic Programming (CLP) on the one hand, and, on the other hand, Distributed Problem Solving (DPS) inherited from Distributed Artificial Intelligence (DAI). This analysis is achieved through the use of two specific computerised tools: Constraint Handling In Prolog (CHIP) as a CLP software and the Distributed Production Scheduling System (DPSS) as a distributed problem solving system. These tools are then evaluated in terms of interactivity, efficiency and practicability. Interactivity is discussed according to qualitative points of view such as the ability to provide efficient decision support, a set of alternative solutions and the possibility to parameterise the algorithms. Efficiency is described in terms of optimality or sub-optimality by the analysis of the Makespan criterion vs. fixed computation time. Practicability is associated to the industrial viability of the methods: ability to cope with real industrial case study or ability to face real industrial contexts. Evaluation is then performed through a multiple criteria analysis. This analysis is achieved given an increasing number of operations to perform.
                  The results highlight the high complementary level of these two approaches, allowing us to provide a framework for a joint integration, which shall be optimised when taking into account the assets of each approach according to the three evaluation criteria.",industry
10.1016/S0019-0578(00)00047-1,Journal,ISA Transactions,scopus,2001-01-01,sciencedirect,Winner take-all experts network for sensor validation,https://api.elsevier.com/content/abstract/scopus_id/0035039968,"The validation of sensor measurements has become an integral part of the operation and control of modern industrial equipment. The sensor under harsh environment must be shown to consistently provide the correct measurements. Analysis of the validation hardware or software should trigger an alarm when the sensor signals deviate appreciably from the correct values. Neural network based models can be used to on-line estimate critical sensor values when neighboring sensor measurements are used as inputs. The underlying assumption is that the neighboring sensors share an analytical relationship. The discrepancy between the measured and predicted sensor values may then be used as an indicator for sensor health. The proposed Winner Take All Experts (WTAE) network based on a ‘divide and conquer’ strategy significantly reduces the computational time required to train the neural network. It employs a growing fuzzy clustering algorithm to divide a complicated problem into a series of simpler sub-problems and assigns an expert to each of them locally. After the sensor approximation, the outputs from the estimator and the real sensor readings are compared both in the time domain and the frequency domain. Three fault indicators are used to provide analytical redundancy to detect the sensor failure. In the decision stage, the intersection of three fuzzy sets accomplishes a decision level fusion, which indicates the confidence level of the sensor health. Two data sets, the Spectra Quest Machinery Fault Simulator data set and the Westland vibration data set, were used in simulations to demonstrate the performance of the proposed WTAE network. The simulation results show the proposed WTAE is competitive with or even superior to the existing approaches.",industry
10.1016/S0098-1354(00)00542-1,Journal,Computers and Chemical Engineering,scopus,2000-07-15,sciencedirect,On-line training method of ANN in DMS for side draw quality of refinery fractionator,https://api.elsevier.com/content/abstract/scopus_id/0034660705,"The dynamic on-line monitoring system (DMS) based on on-line training of artificial neural network (ANN) has been successfully implemented in a refinery for real-time, on-line and dynamic estimation of quality indexes of fractionator side draw. To improve the predicted accuracy of DMS and fit the new production cases, a novel ANN training method, on-line training method, has been proposed. It can save a lot of time and effort in contrast to the conventional off-line training for ANN. It also improves the reliability of DMS.",industry
10.1016/S0305-0548(99)00118-5,Journal,Computers and Operations Research,scopus,2000-06-01,sciencedirect,PREFDIS: A multicriteria decision support system for sorting decision problems,https://api.elsevier.com/content/abstract/scopus_id/0034009314,"This paper, following the methodological framework of multicriteria decision aid (MCDA), presents the PREFDIS (PREFerence DIScrimination) multicriteria decision support system to study sorting decision problems. The main characteristic and a major advantage of the system is the incorporation into its model base of four MCDA methods originating from the preference-disaggregation approach, namely the UTADIS method (UTilités Additives DIScriminantes) and three of its variants, referred to as UTADIS I, UTADIS II and UTADIS III. Using these methods, the decision maker (DM) can develop interactively powerful additive utility models to sort a set of alternatives into two or more predefined classes as accurately as possible, based on different sorting techniques. Furthermore, the system provides enriched preference modeling capabilities, including the modeling of non-monotone preferences. The friendly window-based user interface of the system enables the decision maker/user to take full advantage of the capabilities of the system in order to make effective real-time decisions.
               
                  Scope and purpose
                  The sorting problem refers to the assignment of a finite set of alternatives (actions, objects) to predefined ordered classes. Several real-world decision problems are addressed through the sorting approach, including financial decision-making problems, environmental decisions, marketing decisions, and even medical decisions (medical diagnosis). For several decades the sorting (discrimination) among two or more sets of objects has been studied from the multivariate statistical point of view. Recently, the possibilities of new approaches such as expert systems, neural networks, mathematical programming, multicriteria decision aid (MCDA), etc., have been explored, in order to study the sorting problem within a more flexible framework and to develop sorting models with higher discriminating and predicting ability. This paper presents the PREFDIS (PREFerence DISiscrimination) multicriteria decision support system for the study of sorting decision problems. The system incorporating four MCDA sorting methods enables the decision maker to develop interactively, in real time, additive utility models to sort a set of alternatives into two or more predefined classes.",industry
10.1016/S0952-1976(00)00052-X,Journal,Engineering Applications of Artificial Intelligence,scopus,2000-01-01,sciencedirect,Real-time integrated process supervision,https://api.elsevier.com/content/abstract/scopus_id/0034508997,"This paper presents the use of a micro-controller-based integrated process supervision (IPS) system as a real-time platform for investigative work in structuring expert control. Two different control approaches, based on classical and artificial intelligence techniques, were integrated within IPS and serve as practical examples of the structured approach to expert control. The IPS is a refinement of the expert control architecture. It allows the integration of several control techniques in a single generic framework. Specifically, the paper presents the extensive experimental results derived from a micro-controller-based implementation of IPS on the real-time control of a typical industrial heat-exchanger process. The classical approach, based on auto-tuning techniques, was implemented under the IPS framework. Three auto-tuning techniques, namely Ziegler–Nichols tuning, amplitude tuning and phase tuning were incorporated. In addition, neural-network-based control techniques using the modified cerebellar model articulation controller (MCMAC) were also seamlessly incorporated within the IPS scheme. The real-time experimental results using the IPS architecture significantly demonstrated the effectiveness of IPS in handling varying operating conditions. Furthermore, the inclusion of both AI and classical control techniques within a common supervisory framework adequately shows the generality of the architecture.",industry
10.1016/S0957-4158(99)00058-6,Journal,Mechatronics,scopus,2000-01-01,sciencedirect,Analysis and real-time implementation of a radial-basis-function neural-network compensator for high-performance robot manipulators,https://api.elsevier.com/content/abstract/scopus_id/0034135293,"System performance of robot manipulators with nonadaptive controllers might degrade significantly in the presence of structured or unstructured uncertainties. In order to improve the system performance, a novel radial-basis-function (RBF) neural-network (NN) compensator is proposed. With the RBF NN compensator introduced, the system errors and the NN weights with large dispersion in the initial NN weights are guaranteed to be bounded in the Lyapunov sense. The NN weights of the RBF NN compensator are adaptively tuned. Several software-based controllers, including the computed-torque control (CTC) and a few RBF NN schemes, are implemented in an industrial manipulator in real time. Experimental results are obtained to demonstrate the relative effectiveness of the proposed controllers in improving the tracking performance of the robot manipulators associated with structured or unstructured uncertainties.",industry
10.1205/026387600527554,Journal,Chemical Engineering Research and Design,scopus,2000-01-01,sciencedirect,MIMO soft sensors for estimating product quality with on-line correction,https://api.elsevier.com/content/abstract/scopus_id/0033947033,"The main difficulties of on-line quality control are the availability of on-line product quality measurements. Soft-sensing techniques supply attractive and efficient methods to deal with these difficulties. Soft sensors refer to the modelling approaches to estimating hard-to-measure process variables (e.g. quality variables) from other easy-to-measure variables (e.g. temperature, pressure and flowrate measurements). At present, much more research is concerned with multi-input single-output (MISO) systems than with MIMO systems in the field of soft-sensing modelling. In this paper, some MIMO soft-sensing techniques are studied for estimating multiple product quality variables simultaneously in a hydrocracking fractionator. RBF and fuzzy ARTMAP networks are used to build the models and the latter is shown to be more suitable for MIMO soft-sensing modelling. The issues of data pretreatment and on-line correction, which are very important for the industrial implementation of MIMO soft sensors, are discussed in detail. A useful method using a multivariable fuzzy PID (MFPID) on-line correction algorithm is proposed for the MIMO soft sensors enabling them to adapt with the fluctuation of process operating conditions and uncertain system disturbances. The real application results show that the proposed methods are effective for MIMO soft-sensing modelling and have great promise in industrial process applications.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
10.1016/S0950-5849(99)00012-9,Journal,Information and Software Technology,scopus,1999-05-15,sciencedirect,Integrating structured OO approaches with formal techniques for the development of real-time systems,https://api.elsevier.com/content/abstract/scopus_id/0032687181,"The use of formal methods in the development of time-critical applications is essential if we want to achieve a high level of assurance in them. However, these methods have not yet been widely accepted in industry as compared to the more established structured and informal techniques. A reliable linkage between these two techniques will provide the developer with a powerful tool for developing a provably correct system. In this article, we explore the issue of integrating a real-time formal technique, TAM (Temporal Agent Model), with an industry-strength structured methodology known as HRT-HOOD. TAM is a systematic formal approach for the development of real-time systems based on the refinement calculus. Within TAM, a formal specification can be written (in a logic-based formalism), analysed and then refined to concrete representation through successive applications of sound refinement laws. Both abstract specification and concrete implementation are allowed to freely intermix. HRT-HOOD is an extension to the Hierarchical Object-Oriented Design (HOOD) technique for the development of Hard Real-Time systems. It is a two-phase design technique dealing with the logical and physical architecture designs of the system which can handle both functional and non-functional requirement, respectively. The integrated technique is illustrated on a version of the mine control system.",industry
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",industry
10.1016/S0921-8890(98)00082-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Using mobile agents to improve the alignment between manufacturing and its IT support systems,https://api.elsevier.com/content/abstract/scopus_id/0033617064,"This paper proposes the notion that mobile agent technology can improve the alignment between IT systems and the real world processes they support. This can aid enterprise agility, particularly where distributed information is a feature, as in the virtual enterprise.
                  A model of the manufacturing sales/order process is proposed. The sales order is shown to be a naturally mobile element of the model. The subsequent decomposition of the agent types during detailed design and implementation reveals an abstract pattern for database query using agent technology. Finally, an overview of the security issues associated with mobile agents is discussed.",industry
10.1016/S0921-8890(98)00079-7,Journal,Robotics and Autonomous Systems,scopus,1999-04-30,sciencedirect,Agent-based design of holonic manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/0033617056,"This article presents a new approach to the design of the architecture of a computer-integrated manufacturing (CIM) system. It starts with presenting the basic ideas of novel approaches which are best characterised as fractal or holonic models for the design of flexible manufacturing systems (FMS). The article discusses hierarchical and decentralised concepts for the design of such systems and argues that software agents are the ideal means for their implementation. The agent architecture InteRRaP for agent design is presented and is used to describe a planning and control architecture for a CIM system, which is separated into the layer of the production planning and control system, the shop floor control systems, the flexible cell control layer, the autonomous system layer, and the machine control layer. Two application scenarios are described at the end of the article and results are reported which were obtained from experiments with the implementations for these application scenarios. While one of these scenarios — a model of an FMS — is more research-oriented, the second one — optimisation of a production line — is directly related to an industrial real-world setting.",industry
10.1016/s0957-4174(99)00034-2,Journal,Expert Systems with Applications,scopus,1999-01-01,sciencedirect,IntelliSPC: A hybrid intelligent tool for on-line economical statistical process control,https://api.elsevier.com/content/abstract/scopus_id/2142753458,"Statistical process control (SPC) has become one of the most commonly used tools, for maintaining an acceptable and stable level of quality characteristics in today's manufacturing. With the movement towards a computer integrated manufacturing (CIM) environment, computer based algorithms need to be developed to implement the various SPC tasks automatically.
                  This paper presents a hybrid intelligent tool (IntelliSPC) in which a neural network based control chart pattern recognition system, an expert system based control chart alarm interpretation system and a quality cost simulation system were integrated for on-line SPC. IntelliSPC was designed to provide the quality practitioners with the status of the process (in-control or out-of-control), the plausible causes for the out-of-control situation and cost-effective actions against the out-of-control situation. This tool was intended to be implemented in a scenario where sample data are being collected on-line by automated inspection devices and monitored by control charts.
                  An implementation example is provided to demonstrate how the proposed hybrid system could be usefully applied in a real-world automated production line. This work confirms the potential synergies of hybrid artificial intelligence (AI) techniques in a complex problem solving procedure, such as an automated SPC scheme.",industry
10.1016/S0969-806X(98)00275-8,Journal,Radiation Physics and Chemistry,scopus,1999-01-01,sciencedirect,NorTRACK(TM) product tracking system - Development and implementation,https://api.elsevier.com/content/abstract/scopus_id/0344178164,"This paper presents the experience gained by developers and users with implementation and operation of NorTRACKTM, a real-time computerized product tracking system. A Programmable Logic Controller (PLC) collects and transfers data in real time to NorTRACK’s OracleTM database on a Windows NTTM server network. After extensive development and Beta testing at MDS Nordion’s Canadian Irradiation Centre in Montreal, Canada, NorTRACK was installed in January 1997 with a new irradiation facility in Ethicon Endo-Surgery Inc.’s Albuquerque plant in the United States. NorTRACK communicates with the irradiator control and safety system, the plant's central manufacturing database, an innovative pallet staging and tote loading robot, and an automated dosimetry reading system. This integrated system allows the sterilization facility to monitor the irradiator operation and the flow of many products, through varied processing modes, continuously and reliably. As a result of operating with NorTRACK, both MDS Nordion’s CIC facility and the Endo-Surgery manufacturing site, are beginning to realize unique benefits in their respective operations. MDS Nordion is also initiating several future product enhancements and additional productivity modules. This paper describes the NorTRACK system, the various stages of the development project and Beta tests, and the experience of the users to date in their operations.",industry
10.1016/s0952-1976(98)00041-4,Journal,Engineering Applications of Artificial Intelligence,scopus,1999-01-01,sciencedirect,A framework for developing an agent-based collaborative service-support system in a manufacturing information network,https://api.elsevier.com/content/abstract/scopus_id/0033078886,"A framework for the development of a collaborative service-support system, which is an object-based tool designed to provide a service to the manufacturing firms within an enterprise information network, is presented here. The service is carried out by virtual agents (VAs), which are software programs designed to accomplish specific tasks, just like ‘real’ human agents with specialized skills. This proposed system is equipped with the ‘push delivery’ feature, which automatically directs updated information straight to the user without having to be requested every time. The new feature of this collaborative service-support system is the incorporation of the task-management system, which is characterized by the combined capabilities of a rule-based inference mechanism and the object-oriented technology, in order to achieve the decomposition of a job into individual tasks that are to be automatically undertaken by the relevant virtual agents. The virtual agents can act cooperatively and collaboratively, to achieve the given goal, under the control of a task-control subsystem. This proposed service-support system enables the easy accessing and use of accurate and updated manufacturing information on the network, and therefore enhances the organizational productivity of the companies involved. In this paper, the detailed architecture and the components included in the proposed system are described.",industry
10.1016/S0965-9978(98)00067-2,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,An intelligent approach to monitor and control the blanking process,https://api.elsevier.com/content/abstract/scopus_id/0033075089,"Sensory fusion can be used to infer and analyse complex phenomena and detect changes in a process from a set of measurements. This paper proposes to use Neural Network modelling to monitor product quality of blanking by assessing changes in tool geometry, material quality and tool configuration. The approach may also be used to detect internal and external fractures in the products of blanking. The neural network model is fed with representative parameters, extracted from acoustic signals emitted during fracture, the corresponding waveform and force/displacement data. The neural network model is used, after training to correlate the extracted features of various signals to changes in blanking process parameters such as tool geometry, material hardness and tools clearance. A computerised data-acquisition system, using specialised software and hardware is used to draw from several transducers to record data of the experiments. A total of 192 experiments were performed, using different blanking configurations and materials. This data is used to train the neural network model, which may be integrated with other elements of the control system, to provide a fully automated, real-time supervisory system for blanking. The system could be used to sort components, shutdown the press or alert operators in the event of manufacturing-related defects in the operating cycle.",industry
10.1016/S0965-9978(98)00074-X,Journal,Advances in Engineering Software,scopus,1999-01-01,sciencedirect,Application of an engineering algorithm with software code (C4.5) for specific ion detection in the chemical industries,https://api.elsevier.com/content/abstract/scopus_id/0032785975,"The development of an on-line computer-based classification system for the automatic detection of different ions, existing in different solutions, is addressed using a machine learning algorithm. Different parameters (current, mass and resistance) were collected simultaneously. Then these laboratory measurements are used by an algorithm software as a logged data file, resulting in to inducing a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of unknown solution used in the experiment. This is a new approach to data acquisition in chemical industries involving conducting polymers.",industry
