doi,publication_date,publication,publisher,title,abstract,database
10.1109/TIE.2016.2627020,2017-03-01,a,IEEE,deep model based domain adaptation for fault diagnosis," In recent years, machine learning techniques have been widely used to solve many problems for fault diagnosis. However, in many real-world fault diagnosis applications, the distribution of the source domain data (on which the model is trained) is different from the distribution of the target domain data (where the learned model is actually deployed), which leads to performance degradation. In this paper, we introduce domain adaptation, which can find the solution to this problem by adapting the classifier or the regression model trained in a source domain for use in a different but related target domain. In particular, we proposed a novel deep neural network model with domain adaptation for fault diagnosis. Two main contributions are concluded by comparing to the previous works: first, the proposed model can utilize domain adaptation meanwhile strengthening the representative information of the original data, so that a high classification accuracy in the target domain can be achieved, and second, we proposed several strategies to explore the optimal hyperparameters of the model. Experimental results, on several real-world datasets, demonstrate the effectiveness and the reliability of both the proposed model and the exploring strategies for the parameters.",project-academic
10.1016/J.CHAOS.2020.110059,2020-06-25,a,Pergamon,applications of machine learning and artificial intelligence for covid 19 sars cov 2 pandemic a review," Abstract None None Background and objective None During the recent global urgency, scientists, clinicians, and healthcare experts around the globe keep on searching for a new technology to support in tackling the Covid-19 pandemic. The evidence of Machine Learning (ML) and Artificial Intelligence (AI) application on the previous epidemic encourage researchers by giving a new angle to fight against the novel Coronavirus outbreak. This paper aims to comprehensively review the role of AI and ML as one significant method in the arena of screening, predicting, forecasting, contact tracing, and drug development for SARS-CoV-2 and its related epidemic. None None None Method None A selective assessment of information on the research article was executed on the databases related to the application of ML and AI technology on Covid-19. Rapid and critical analysis of the three crucial parameters, i.e., abstract, methodology, and the conclusion was done to relate to the model's possibilities for tackling the SARS-CoV-2 epidemic. None None None Result None This paper addresses on recent studies that apply ML and AI technology towards augmenting the researchers on multiple angles. It also addresses a few errors and challenges while using such algorithms in real-world problems. The paper also discusses suggestions conveying researchers on model design, medical experts, and policymakers in the current situation while tackling the Covid-19 pandemic and ahead. None None None Conclusion None The ongoing development in AI and ML has significantly improved treatment, medication, screening, prediction, forecasting, contact tracing, and drug/vaccine development process for the Covid-19 pandemic and reduce the human intervention in medical practice. However, most of the models are not deployed enough to show their real-world operation, but they are still up to the mark to tackle the SARS-CoV-2 epidemic.",project-academic
,2019-10-29,p,"Neural Information Processing Systems Foundation, Inc.",domain generalization via model agnostic learning of semantic features," Generalization capability to unseen domains is crucial for machine learning models when deploying to real-world conditions. We investigate the challenging problem of domain generalization, i.e., training a model on multi-domain source data such that it can directly generalize to target domains with unknown statistics. We adopt a model-agnostic learning paradigm with gradient-based meta-train and meta-test procedures to expose the optimization to domain shift. Further, we introduce two complementary losses which explicitly regularize the semantic structure of the feature space. Globally, we align a derived soft confusion matrix to preserve general knowledge of inter-class relationships. Locally, we promote domain-independent class-specific cohesion and separation of sample features with a metric-learning component. The effectiveness of our method is demonstrated with new state-of-the-art results on two common object recognition benchmarks. Our method also shows consistent improvement on a medical image segmentation task.",project-academic
,2019-11-06,a,,fooling lime and shap adversarial attacks on post hoc explanation methods," As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real-world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.",project-academic
10.1007/S10115-015-0830-Y,2016-02-01,a,Springer London,a survey on indexing techniques for big data taxonomy and performance evaluation," The explosive growth in volume, velocity, and diversity of data produced by mobile devices and cloud applications has contributed to the abundance of data or `big data.' Available solutions for efficient data storage and management cannot fulfill the needs of such heterogeneous data where the amount of data is continuously increasing. For efficient retrieval and management, existing indexing solutions become inefficient with the rapidly growing index size and seek time and an optimized index scheme is required for big data. Regarding real-world applications, the indexing issue with big data in cloud computing is widespread in healthcare, enterprises, scientific experiments, and social networks. To date, diverse soft computing, machine learning, and other techniques in terms of artificial intelligence have been utilized to satisfy the indexing requirements, yet in the literature, there is no reported state-of-the-art survey investigating the performance and consequences of techniques for solving indexing in big data issues as they enter cloud computing. The objective of this paper is to investigate and examine the existing indexing techniques for big data. Taxonomy of indexing techniques is developed to provide insight to enable researchers understand and select a technique as a basis to design an indexing mechanism with reduced time and space consumption for BD-MCC. In this study, 48 indexing techniques have been studied and compared based on 60 articles related to the topic. The indexing techniques' performance is analyzed based on their characteristics and big data indexing requirements. The main contribution of this study is taxonomy of categorized indexing techniques based on their method. The categories are non-artificial intelligence, artificial intelligence, and collaborative artificial intelligence indexing methods. In addition, the significance of different procedures and performance is analyzed, besides limitations of each technique. In conclusion, several key future research topics with potential to accelerate the progress and deployment of artificial intelligence-based cooperative indexing in BD-MCC are elaborated on.",project-academic
10.1145/3368555.3384468,2020-04-02,p,Proc ACM Conf Health Inference Learn (2020),hidden stratification causes clinically meaningful failures in machine learning for medical imaging," Machine learning models for medical image analysis often suffer from poor performance on important subsets of a population that are not identified during training or testing. For example, overall performance of a cancer detection model may be high, but the model may still consistently miss a rare but aggressive cancer subtype. We refer to this problem as hidden stratification, and observe that it results from incompletely describing the meaningful variation in a dataset. While hidden stratification can substantially reduce the clinical efficacy of machine learning models, its effects remain difficult to measure. In this work, we assess the utility of several possible techniques for measuring hidden stratification effects, and characterize these effects both via synthetic experiments on the CIFAR-100 benchmark dataset and on multiple real-world medical imaging datasets. Using these measurement techniques, we find evidence that hidden stratification can occur in unidentified imaging subsets with low prevalence, low label quality, subtle distinguishing features, or spurious correlates, and that it can result in relative performance differences of over 20% on clinically important subsets. Finally, we discuss the clinical implications of our findings, and suggest that evaluation of hidden stratification should be a critical component of any machine learning deployment in medical imaging.",project-academic
10.1038/S41591-019-0539-7,2019-08-12,a,Nat Med,an augmented reality microscope with real time artificial intelligence integration for cancer diagnosis," The microscopic assessment of tissue samples is instrumental for the diagnosis and staging of cancer, and thus guides therapy. However, these assessments demonstrate considerable variability and many regions of the world lack access to trained pathologists. Though artificial intelligence (AI) promises to improve the access and quality of healthcare, the costs of image digitization in pathology and difficulties in deploying AI solutions remain as barriers to real-world use. Here we propose a cost-effective solution: the augmented reality microscope (ARM). The ARM overlays AI-based information onto the current view of the sample in real time, enabling seamless integration of AI into routine workflows. We demonstrate the utility of ARM in the detection of metastatic breast cancer and the identification of prostate cancer, with latency compatible with real-time use. We anticipate that the ARM will remove barriers towards the use of AI designed to improve the accuracy and efficiency of cancer diagnosis.",project-academic
10.1038/S41746-020-00376-2,2021-01-08,a,Nature Publishing Group,deep learning enabled medical computer vision," A decade of unprecedented progress in artificial intelligence (AI) has demonstrated the potential for many fields-including medicine-to benefit from the insights that AI techniques can extract from data. Here we survey recent progress in the development of modern computer vision techniques-powered by deep learning-for medical applications, focusing on medical imaging, medical video, and clinical deployment. We start by briefly summarizing a decade of progress in convolutional neural networks, including the vision tasks they enable, in the context of healthcare. Next, we discuss several example medical imaging applications that stand to benefit-including cardiology, pathology, dermatology, ophthalmology-and propose new avenues for continued work. We then expand into general medical video, highlighting ways in which clinical workflows can integrate computer vision to enhance care. Finally, we discuss the challenges and hurdles required for real-world clinical deployment of these technologies.",project-academic
10.1038/S41598-020-62724-2,2019-08-15,a,,resolving challenges in deep learning based analyses of histopathological images using explanation methods," Deep learning has recently gained popularity in digital pathology due to its high prediction quality. However, the medical domain requires explanation and insight for a better understanding beyond standard quantitative performance evaluation. Recently, explanation methods have emerged, which are so far still rarely used in medicine. This work shows their application to generate heatmaps that allow to resolve common challenges encountered in deep learning-based digital histopathology analyses. These challenges comprise biases typically inherent to histopathology data. We study binary classification tasks of tumor tissue discrimination in publicly available haematoxylin and eosin slides of various tumor entities and investigate three types of biases: (1) biases which affect the entire dataset, (2) biases which are by chance correlated with class labels and (3) sampling biases. While standard analyses focus on patch-level evaluation, we advocate pixel-wise heatmaps, which offer a more precise and versatile diagnostic instrument and furthermore help to reveal biases in the data. This insight is shown to not only detect but also to be helpful to remove the effects of common hidden biases, which improves generalization within and across datasets. For example, we could see a trend of improved area under the receiver operating characteristic curve by 5% when reducing a labeling bias. Explanation techniques are thus demonstrated to be a helpful and highly relevant tool for the development and the deployment phases within the life cycle of real-world applications in digital pathology.",project-academic
,2020-11-06,a,,underspecification presents challenges for credibility in modern machine learning," ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.",project-academic
,2020-02-10,a,,interpretable off policy evaluation in reinforcement learning by highlighting influential transitions," Off-policy evaluation in reinforcement learning offers the chance of using observational data to improve future outcomes in domains such as healthcare and education, but safe deployment in high stakes settings requires ways of assessing its validity. Traditional measures such as confidence intervals may be insufficient due to noise, limited data and confounding. In this paper we develop a method that could serve as a hybrid human-AI system, to enable human experts to analyze the validity of policy evaluation estimates. This is accomplished by highlighting observations in the data whose removal will have a large effect on the OPE estimate, and formulating a set of rules for choosing which ones to present to domain experts for validation. We develop methods to compute exactly the influence functions for fitted Q-evaluation with two different function classes: kernel-based and linear least squares, as well as importance sampling methods. Experiments on medical simulations and real-world intensive care unit data demonstrate that our method can be used to identify limitations in the evaluation process and make evaluation more robust.",project-academic
10.1038/S41377-019-0209-Z,2019-10-21,a,Nature Publishing Group,intelligent metasurface imager and recognizer," There is an increasing need to remotely monitor people in daily life using radio-frequency probe signals. However, conventional systems can hardly be deployed in real-world settings since they typically require objects to either deliberately cooperate or carry a wireless active device or identification tag. To accomplish complicated successive tasks using a single device in real time, we propose the simultaneous use of a smart metasurface imager and recognizer, empowered by a network of artificial neural networks (ANNs) for adaptively controlling data flow. Here, three ANNs are employed in an integrated hierarchy, transforming measured microwave data into images of the whole human body, classifying specifically designated spots (hand and chest) within the whole image, and recognizing human hand signs instantly at a Wi-Fi frequency of 2.4 GHz. Instantaneous in situ full-scene imaging and adaptive recognition of hand signs and vital signs of multiple non-cooperative people were experimentally demonstrated. We also show that the proposed intelligent metasurface system works well even when it is passively excited by stray Wi-Fi signals that ubiquitously exist in our daily lives. The reported strategy could open up a new avenue for future smart cities, smart homes, human-device interaction interfaces, health monitoring, and safety screening free of visual privacy issues. Combining radio-frequency imaging with artificial intelligence could make it easier for computers to interact with individuals using non-verbal cues, such as sign language. Lianlin Li from Peking University in Beijing, China and Tie Jun Cui from Southeast University in Nanjing, China, and co-workers fabricated a meter-scale flat panel containing ‘meta-atoms’, tiny electronic devices that manipulate the phases of light waves, arranged in a grid-like pattern. By emitting microwave signals or manipulating stray Wi-Fi signals and detecting echoes bounced back, the metasurface can collect high-resolution imaging data on multiple non-cooperative subjects, even those behind solid walls. The teams fed the microwave data to a series of artificial intelligence algorithms that first identify human shapes, modify signal distributions to better focus on specific body parts, and recognize people's hand signs and vital signs . Experiments showed this setup could continuously monitor hand signals and breathing, even using stray Wi-Fi signals that ubiquitously exist in the daily lives.",project-academic
,2020-10-21,a,,incorporating interpretable output constraints in bayesian neural networks," Domains where supervised models are deployed often come with task-specific constraints, such as prior expert knowledge on the ground-truth function, or desiderata like safety and fairness. We introduce a novel probabilistic framework for reasoning with such constraints and formulate a prior that enables us to effectively incorporate them into Bayesian neural networks (BNNs), including a variant that can be amortized over tasks. The resulting Output-Constrained BNN (OC-BNN) is fully consistent with the Bayesian framework for uncertainty quantification and is amenable to black-box inference. Unlike typical BNN inference in uninterpretable parameter space, OC-BNNs widen the range of functional knowledge that can be incorporated, especially for model users without expertise in machine learning. We demonstrate the efficacy of OC-BNNs on real-world datasets, spanning multiple domains such as healthcare, criminal justice, and credit scoring.",project-academic
,2021-04-21,a,,uncertainty aware boosted ensembling in multi modal settings," Reliability of machine learning (ML) systems is crucial in safety-critical applications such as healthcare, and uncertainty estimation is a widely researched method to highlight the confidence of ML systems in deployment. Sequential and parallel ensemble techniques have shown improved performance of ML systems in multi-modal settings by leveraging the feature sets together. We propose an uncertainty-aware boosting technique for multi-modal ensembling in order to focus on the data points with higher associated uncertainty estimates, rather than the ones with higher loss values. We evaluate this method on healthcare tasks related to Dementia and Parkinson's disease which involve real-world multi-modal speech and text data, wherein our method shows an improved performance. Additional analysis suggests that introducing uncertainty-awareness into the boosted ensembles decreases the overall entropy of the system, making it more robust to heteroscedasticity in the data, as well as better calibrating each of the modalities along with high quality prediction intervals. We open-source our entire codebase at this https URL",project-academic
10.1109/IJCNN52387.2021.9534161,2021-07-18,p,IEEE,uncertainty aware boosted ensembling in multi modal settings," Reliability of machine learning (ML) systems is crucial in safety-critical applications such as healthcare, and uncertainty estimation is a widely researched method to highlight the confidence of ML systems in deployment. Sequential and parallel ensemble techniques have shown improved performance of ML systems in multi-modal settings by leveraging the feature sets together. We propose an uncertainty-aware boosting technique for multi-modal ensembling in order to focus on the data points with higher associated uncertainty estimates, rather than the ones with higher loss values. We evaluate this method on healthcare tasks related to Dementia and Parkinson's disease which involve real-world multi-modal speech and text data, wherein our method shows an improved performance. Additional analysis suggests that introducing uncertainty-awareness into the boosted ensembles decreases the overall entropy of the system, making it more robust to heteroscedasticity in the data, as well as better calibrating each of the modalities along with high quality prediction intervals. We open-source our entire codebase at https://github.com/usarawgi911//Uncertainty-aware-boosting.",project-academic
10.1038/S41591-021-01359-W,2021-06-01,a,Nature Publishing Group,clinical integration of machine learning for curative intent radiation treatment of patients with prostate cancer," Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in ‘simulated’ environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n = 50) and a prospective clinical deployment (n = 50) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47 h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake. An artificial intelligence system prospectively deployed to design radiation therapy plans for patients with prostate cancer illustrates the real-world impact of machine learning in clinical practice and identifies factors influencing human–algorithm interaction",project-academic
10.1038/S41591-019-0539-7,2018-11-21,a,,microscope 2 0 an augmented reality microscope with real time artificial intelligence integration," The brightfield microscope is instrumental in the visual examination of both biological and physical samples at sub-millimeter scales. One key clinical application has been in cancer histopathology, where the microscopic assessment of the tissue samples is used for the diagnosis and staging of cancer and thus guides clinical therapy. However, the interpretation of these samples is inherently subjective, resulting in significant diagnostic variability. Moreover, in many regions of the world, access to pathologists is severely limited due to lack of trained personnel. In this regard, Artificial Intelligence (AI) based tools promise to improve the access and quality of healthcare. However, despite significant advances in AI research, integration of these tools into real-world cancer diagnosis workflows remains challenging because of the costs of image digitization and difficulties in deploying AI solutions. Here we propose a cost-effective solution to the integration of AI: the Augmented Reality Microscope (ARM). The ARM overlays AI-based information onto the current view of the sample through the optical pathway in real-time, enabling seamless integration of AI into the regular microscopy workflow. We demonstrate the utility of ARM in the detection of lymph node metastases in breast cancer and the identification of prostate cancer with a latency that supports real-time workflows. We anticipate that ARM will remove barriers towards the use of AI in microscopic analysis and thus improve the accuracy and efficiency of cancer diagnosis. This approach is applicable to other microscopy tasks and AI algorithms in the life sciences and beyond.",project-academic
,2020-06-05,a,,deployment efficient reinforcement learning via model based offline optimization," Most reinforcement learning (RL) algorithms assume online access to the environment, in which one may readily interleave updates to the policy with experience collection using that policy. However, in many real-world applications such as health, education, dialogue agents, and robotics, the cost or potential risk of deploying a new data-collection policy is high, to the point that it can become prohibitive to update the data-collection policy more than a few times during learning. With this view, we propose a novel concept of deployment efficiency, measuring the number of distinct data-collection policies that are used during policy learning. We observe that naively applying existing model-free offline RL algorithms recursively does not lead to a practical deployment-efficient and sample-efficient algorithm. We propose a novel model-based algorithm, Behavior-Regularized Model-ENsemble (BREMEN) that can effectively optimize a policy offline using 10-20 times fewer data than prior works. Furthermore, the recursive application of BREMEN is able to achieve impressive deployment efficiency while maintaining the same or better sample efficiency, learning successful policies from scratch on simulated robotic environments with only 5-10 deployments, compared to typical values of hundreds to millions in standard RL baselines. Codes and pre-trained models are available at this https URL .",project-academic
,2021-07-18,p,PMLR,towards understanding and mitigating social biases in language models," As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",project-academic
10.1109/CHASE.2016.18,2016-06-27,p,IEEE,improving tuberculosis diagnostics using deep learning and mobile health technologies among resource poor and marginalized communities," Tuberculosis (TB) is a chronic infectious disease worldwide and remains a major cause of death globally. Of the estimated 9 million people who developed TB in 2013, over 80% were in South-East Asia, Western Pacific, and African. The majority of the infected populations was from resource-poor and marginalized communities with weak healthcare infrastructure. Reducing TB diagnosis delay is critical in mitigating disease transmission and minimizing the reproductive rate of the tuberculosis epidemic. The combination of machine learning and mobile computing techniques offers a unique opportunity to accelerate the TB diagnosis among these communities. The ultimate goal of our research is to reduce patient wait times for being diagnosed with this infectious disease by developing new machine learning and mobile health techniques to the TB diagnosis problem. In this paper, we first introduce major technique barriers and proposed system architecture. Then we report two major progresses we recently made. The first activity aims to develop large-scale, real-world and well-annotated X-ray image database dedicated for automated TB screening. The second research activity focus on developing effective and efficient computational models (in particularly, deep convolutional neural networks (CNN)-based models) to classify the image into different category of TB manifestations. Experimental results have demonstrated the effectiveness of our approach. Our future work includes: (1) to further improve the performance of the algorithms, and (2) to deploy our system in the city of Carabayllo in Peru, a densely occupied urban community and high-burden TB.",project-academic
10.1109/TMI.2020.3035424,2021-02-01,a,Institute of Electrical and Electronics Engineers (IEEE),mutual information based disentangled neural networks for classifying unseen categories in different domains application to fetal ultrasound imaging," Deep neural networks exhibit limited generalizability across images with different entangled domain features and categorical features. Learning generalizable features that can form universal categorical decision boundaries across domains is an interesting and difficult challenge. This problem occurs frequently in medical imaging applications when attempts are made to deploy and improve deep learning models across different image acquisition devices, across acquisition parameters or if some classes are unavailable in new training databases. To address this problem, we propose Mutual Information-based Disentangled Neural Networks (MIDNet), which extract generalizable categorical features to transfer knowledge to unseen categories in a target domain. The proposed MIDNet adopts a semi-supervised learning paradigm to alleviate the dependency on labeled data. This is important for real-world applications where data annotation is time-consuming, costly and requires training and expertise. We extensively evaluate the proposed method on fetal ultrasound datasets for two different image classification tasks where domain features are respectively defined by shadow artifacts and image acquisition devices. Experimental results show that the proposed method outperforms the state-of-the-art on the classification of unseen categories in a target domain with sparsely labeled training data.",project-academic
,2020-10-30,a,,mutual information based disentangled neural networks for classifying unseen categories in different domains application to fetal ultrasound imaging," Deep neural networks exhibit limited generalizability across images with different entangled domain features and categorical features. Learning generalizable features that can form universal categorical decision boundaries across domains is an interesting and difficult challenge. This problem occurs frequently in medical imaging applications when attempts are made to deploy and improve deep learning models across different image acquisition devices, across acquisition parameters or if some classes are unavailable in new training databases. To address this problem, we propose Mutual Information-based Disentangled Neural Networks (MIDNet), which extract generalizable categorical features to transfer knowledge to unseen categories in a target domain. The proposed MIDNet adopts a semi-supervised learning paradigm to alleviate the dependency on labeled data. This is important for real-world applications where data annotation is time-consuming, costly and requires training and expertise. We extensively evaluate the proposed method on fetal ultrasound datasets for two different image classification tasks where domain features are respectively defined by shadow artifacts and image acquisition devices. Experimental results show that the proposed method outperforms the state-of-the-art on the classification of unseen categories in a target domain with sparsely labeled training data.",project-academic
,2021-03-24,a,,addressing catastrophic forgetting for medical domain expansion," Model brittleness is a key concern when deploying deep learning models in real-world medical settings. A model that has high performance at one institution may suffer a significant decline in performance when tested at other institutions. While pooling datasets from multiple institutions and retraining may provide a straightforward solution, it is often infeasible and may compromise patient privacy. An alternative approach is to fine-tune the model on subsequent institutions after training on the original institution. Notably, this approach degrades model performance at the original institution, a phenomenon known as catastrophic forgetting. In this paper, we develop an approach to address catastrophic forget-ting based on elastic weight consolidation combined with modulation of batch normalization statistics under two scenarios: first, for expanding the domain from one imaging system's data to another imaging system's, and second, for expanding the domain from a large multi-institutional dataset to another single institution dataset. We show that our approach outperforms several other state-of-the-art approaches and provide theoretical justification for the efficacy of batch normalization modulation. The results of this study are generally applicable to the deployment of any clinical deep learning model which requires domain expansion.",project-academic
10.1016/J.IJROBP.2021.07.167,2021-11-01,a,Elsevier BV,clinical validation of deep learning algorithms for lung cancer radiotherapy targeting," PURPOSE/OBJECTIVE(S) Automated target segmentation for non-small cell lung cancer (NSCLC) patients has the potential to support radiation treatment planning. Artificial intelligence (AI) has demonstrated great promise in medical image segmentation tasks. However, most studies have been confined to in silico validation in small internal cohorts, lacking data on real-world clinical utility. In this study, we developed primary tumor and involved lymph node segmentation algorithms in computed tomography (CT) images. Validation is performed in multiple large multi-institutional cohorts to assess model generalizability. MATERIALS/METHODS Simulation CTs and ground truth annotations were collected from multiple public and private sources (total n = 2584). We employed the following benchmarks: Inter-observer (6 radiation oncologists, n = 20, median volumetric dice 0.83, 95% CI 0.82-0.84) and intra-observer (1 radiation oncologist, 3 reads, n = 21, median volumetric dice 0.88, 95% CI 0.84-0.9). We developed two segmentation algorithms: seed-point assisted and fully automated. Model training data (n = 787) comprised NSCLC-Radiomics (stages I-IIIB, n = 422) and LungRT-1 (stages IA-IV, n = 365). Validation was first performed in an internal dataset annotated by a single thoracic radiation oncologist (LungRT-1, n = 136). Additional validation included: (1) an internal dataset annotated by other radiation oncologists, including generalists, in our center (LungRT-2, n = 1075), (2) an external clinical trial dataset from 185 different institutions (RTOG-0617, n = 403), and (3) a dataset of early-stage surgical patients annotated for diagnostic purposes by radiologists (NSCLC-Radiogenomics, n = 142). Volumetric dice, using expert manual segmentations as ground truth, was used as an evaluation metric. RESULTS The model performance is comparable to the benchmarks when validated on internal data, with degrading performance in cohorts annotated by other radiation oncologists. CONCLUSION The results highlight the importance of assessing segmentation style among annotators and understanding model generalizability in external cohorts, all while cautioning against degrading performance in increasingly external data. Differences between radiologists and radiation oncologists performing the same segmentation task underscore the importance of clinical context in AI model deployment. Further validation includes studying the dosimetric impact of AI-generated segmentations, and conducting human subject experiments to assess AI output acceptance and time savings.",project-academic
10.2214/AJR.21.26717,2021-10-06,a,American Roentgen Ray Society,radiology implementation considerations for artificial intelligence ai applied to covid 19 from the ajr special series on ai applications," Hundreds of imaging-based artificial intelligence (AI) models have been developed in response to the COVID-19 pandemic. AI systems that incorporate imaging have shown promise in primary detection, severity grading, and prognostication of outcomes in COVID-19, and have enabled integration of imaging with a broad range of additional clinical and epidemiologic data. However, systematic reviews of AI models applied to COVID-19 medical imaging have highlighted problems in the field, including methodologic issues and problems in real-world deployment. Clinical use of such models should be informed by both the promise and potential pitfalls of implementation. How does a practicing radiologist make sense of this complex topic, and what factors should be considered in the implementation of AI tools for imaging of COVID-19? This critical review aims to help the radiologist understand the nuances that impact the clinical deployment of AI for imaging of COVID-19. We review imaging use cases for AI models in COVID-19 (e.g., diagnosis, severity assessment, and prognostication) and explore considerations for AI model development and testing, deployment infrastructure, clinical user interfaces, quality control, and institutional review board and regulatory approvals, with a practical focus on what a radiologist should consider when implementing an AI tool for COVID-19.",project-academic
,2020-08-19,a,,name that manufacturer relating image acquisition bias with task complexity when training deep learning models experiments on head ct," As interest in applying machine learning techniques for medical images continues to grow at a rapid pace, models are starting to be developed and deployed for clinical applications. In the clinical AI model development lifecycle (described by Lu et al. [1]), a crucial phase for machine learning scientists and clinicians is the proper design and collection of the data cohort. The ability to recognize various forms of biases and distribution shifts in the dataset is critical at this step. While it remains difficult to account for all potential sources of bias, techniques can be developed to identify specific types of bias in order to mitigate their impact. In this work we analyze how the distribution of scanner manufacturers in a dataset can contribute to the overall bias of deep learning models. We evaluate convolutional neural networks (CNN) for both classification and segmentation tasks, specifically two state-of-the-art models: ResNet [2] for classification and U-Net [3] for segmentation. We demonstrate that CNNs can learn to distinguish the imaging scanner manufacturer and that this bias can substantially impact model performance for both classification and segmentation tasks. By creating an original synthesis dataset of brain data mimicking the presence of more or less subtle lesions we also show that this bias is related to the difficulty of the task. Recognition of such bias is critical to develop robust, generalizable models that will be crucial for clinical applications in real-world data distributions.",project-academic
10.1109/ACCESS.2018.2789918,2018-01-05,a,IEEE,energy efficient architecture for wireless sensor networks in healthcare applications," The need to deploy wireless sensor networks (WSNs) for real-world applications, such as mobile multimedia for healthcare organizations, is increasing spectacularly. However, the energy problem remains one of the core barriers preventing an increase in investment in this technology. In this paper, we propose a new technique to resolve the problems due to limited energy sources. Using a quaternary transceiver (in the architecture on a sensor node), instead of a binary one, which will use the amplitude/phase, modulator/demodulator units to increase the number of bits transmitted per symbol. The system will reduce the consumption of energy in the transmission phase due to the increased bits transmitted per symbol. Moreover, neural network static random access memory (NN-SRAM) implementation in a clustering-based system for energy-constrained WSNs is proposed. The scheme reduces the total amount of energy consumption in storage and transmissions during the data dissemination process. Through simulation results based on MATLAB and Spice software tools, it is shown that the neural network static random access memory implementation in a clustering-based system reduces the energy consumption of the entire system by about 76.99%.",project-academic
10.1109/GLOBALSIP.2017.8308687,2017-11-01,p,IEEE,smart fog fog computing framework for unsupervised clustering analytics in wearable internet of things," The increasing use of wearables in smart telehealth system led to the generation of large medical big data. Cloud and fog services leverage these data for assisting clinical procedures. IoT Healthcare has been benefited from this large pool of generated data. This paper suggests the use of low-resource machine learning on Fog devices kept close to wearables for smart telehealth. For traditional telecare systems, the signal processing and machine learning modules are deployed in the cloud that processes physiological data. This paper presents a Fog architecture that relied on unsupervised machine learning big data analysis for discovering patterns in physiological data. We developed a prototype using Intel Edison and Raspberry Pi that was tested on real-world pathological speech data from telemonitoring of patients with Parkinson's disease (PD). Proposed architecture employed machine learning for analysis of pathological speech data obtained from smart watches worn by the patients with PD. Results show that proposed architecture is promising for low-resource machine learning. It could be useful for other applications within wearable IoT for smart telehealth scenarios by translating machine learning approaches from the cloud backend to edge computing devices such as Fog.",project-academic
,2017-12-25,a,,smart fog fog computing framework for unsupervised clustering analytics in wearable internet of things," The increasing use of wearables in smart telehealth generates heterogeneous medical big data. Cloud and fog services process these data for assisting clinical procedures. IoT based ehealthcare have greatly benefited from efficient data processing. This paper proposed and evaluated use of low resource machine learning on Fog devices kept close to the wearables for smart healthcare. In state of the art telecare systems, the signal processing and machine learning modules are deployed in the cloud for processing physiological data. We developed a prototype of Fog-based unsupervised machine learning big data analysis for discovering patterns in physiological data. We employed Intel Edison and Raspberry Pi as Fog computer in proposed architecture. We performed validation studies on real-world pathological speech data from in home monitoring of patients with Parkinson's disease (PD). Proposed architecture employed machine learning for analysis of pathological speech data obtained from smartwatches worn by the patients with PD. Results showed that proposed architecture is promising for low-resource clinical machine learning. It could be useful for other applications within wearable IoT for smart telehealth scenarios by translating machine learning approaches from the cloud backend to edge computing devices such as Fog.",project-academic
,2020-12-03,a,,ethical testing in the real world evaluating physical testing of adversarial machine learning," This paper critically assesses the adequacy and representativeness of physical domain testing for various adversarial machine learning (ML) attacks against computer vision systems involving human subjects. Many papers that deploy such attacks characterize themselves as ""real world."" Despite this framing, however, we found the physical or real-world testing conducted was minimal, provided few details about testing subjects and was often conducted as an afterthought or demonstration. Adversarial ML research without representative trials or testing is an ethical, scientific, and health/safety issue that can cause real harms. We introduce the problem and our methodology, and then critique the physical domain testing methodologies employed by papers in the field. We then explore various barriers to more inclusive physical testing in adversarial ML and offer recommendations to improve such testing notwithstanding these challenges.",project-academic
,2020-10-02,a,,multi domain clinical natural language processing with medcat the medical concept annotation toolkit," Electronic health records (EHR) contain large volumes of unstructured text, requiring the application of Information Extraction (IE) technologies to enable clinical analysis. We present the open-source Medical Concept Annotation Toolkit (MedCAT) that provides: a) a novel self-supervised machine learning algorithm for extracting concepts using any concept vocabulary including UMLS/SNOMED-CT; b) a feature-rich annotation interface for customising and training IE models; and c) integrations to the broader CogStack ecosystem for vendor-agnostic health system deployment. We show improved performance in extracting UMLS concepts from open datasets (F1:0.448-0.738 vs 0.429-0.650). Further real-world validation demonstrates SNOMED-CT extraction at 3 large London hospitals with self-supervised training over ~8.8B words from ~17M clinical records and further fine-tuning with ~6K clinician annotated examples. We show strong transferability (F1 > 0.94) between hospitals, datasets, and concept types indicating cross-domain EHR-agnostic utility for accelerated clinical and research use cases.",project-academic
10.1016/J.ARTMED.2021.102083,2021-05-01,a,Elsevier,multi domain clinical natural language processing with medcat the medical concept annotation toolkit," Abstract None None Electronic health records (EHR) contain large volumes of unstructured text, requiring the application of information extraction (IE) technologies to enable clinical analysis. We present the open source Medical Concept Annotation Toolkit (MedCAT) that provides: (a) a novel self-supervised machine learning algorithm for extracting concepts using any concept vocabulary including UMLS/SNOMED-CT; (b) a feature-rich annotation interface for customizing and training IE models; and (c) integrations to the broader CogStack ecosystem for vendor-agnostic health system deployment. We show improved performance in extracting UMLS concepts from open datasets (F1:0.448–0.738 vs 0.429–0.650). Further real-world validation demonstrates SNOMED-CT extraction at 3 large London hospitals with self-supervised training over None None None ∼ None None 8.8B words from None None None ∼ None None 17M clinical records and further fine-tuning with None None None ∼ None None 6K clinician annotated examples. We show strong transferability (F1 > 0.94) between hospitals, datasets and concept types indicating cross-domain EHR-agnostic utility for accelerated clinical and research use cases.",project-academic
,2019-08-13,a,,icebreaker element wise active information acquisition with bayesian deep latent gaussian model," In this paper we introduce the ice-start problem, i.e., the challenge of deploying machine learning models when only little or no training data is initially available, and acquiring each feature element of data is associated with costs. This setting is representative for the real-world machine learning applications. For instance, in the health-care domain, when training an AI system for predicting patient metrics from lab tests, obtaining every single measurement comes with a high cost. Active learning, where only the label is associated with a cost does not apply to such problem, because performing all possible lab tests to acquire a new training datum would be costly, as well as unnecessary due to redundancy. We propose Icebreaker, a principled framework to approach the ice-start problem. Icebreaker uses a full Bayesian Deep Latent Gaussian Model (BELGAM) with a novel inference method. Our proposed method combines recent advances in amortized inference and stochastic gradient MCMC to enable fast and accurate posterior inference. By utilizing BELGAM's ability to fully quantify model uncertainty, we also propose two information acquisition functions for imputation and active prediction problems. We demonstrate that BELGAM performs significantly better than the previous VAE (Variational autoencoder) based models, when the data set size is small, using both machine learning benchmarks and real-world recommender systems and health-care applications. Moreover, based on BELGAM, Icebreaker further improves the performance and demonstrate the ability to use minimum amount of the training data to obtain the highest test time performance.",project-academic
,2016-06-19,p,JMLR.org,forecasticu a prognostic decision support system for timely prediction of intensive care unit admission," We develop ForecastICU: a prognostic decision support system that monitors hospitalized patients and prompts alarms for intensive care unit (ICU) admissions. ForecastICU is first trained in an offline stage by constructing a Bayesian belief system that corresponds to its belief about how trajectories of physiological data streams of the patient map to a clinical status. After that, ForecastICU monitors a new patient in real-time by observing her physiological data stream, updating its belief about her status over time, and prompting an alarm whenever its belief process hits a predefined threshold (confidence). Using a real-world dataset obtained from UCLA Ronald Reagan Medical Center, we show that ForecastICU can predict ICU admissions 9 hours before a physician's decision (for a sensitivity of 40% and a precision of 50%). Also, ForecastICU performs consistently better than other state-of-the-art machine learning algorithms in terms of sensitivity, precision, and timeliness: it can predict ICU admissions 3 hours earlier, and offers a 7.8% gain in sensitivity and a 5.1% gain in precision compared to the best state-of-the-art algorithm. Moreover, ForecastICU offers an area under curve (AUC) gain of 22.3% compared to the Rothman index, which is the currently deployed technology in most hospital wards.",project-academic
10.2196/19297,2020-05-06,a,JMIR Publications Inc.,agile requirements engineering and software planning for a digital health platform to engage the effects of isolation caused by social distancing case study," Background: Social distancing and shielding measures have been put in place to reduce social interaction and slow the transmission of the coronavirus disease (COVID-19). For older people, self-isolation presents particular challenges for mental health and social relationships. As time progresses, continued social distancing could have a compounding impact on these concerns.
Objective: This project aims to provide a tool for older people and their families and peers to improve their well-being and health during and after regulated social distancing. First, we will evaluate the tool’s feasibility, acceptability, and usability to encourage positive nutrition, enhance physical activity, and enable virtual interaction while social distancing. Second, we will be implementing the app to provide an online community to assist families and peer groups in maintaining contact with older people using goal setting. Anonymized data from the app will be aggregated with other real-world data sources to develop a machine learning algorithm to improve the identification of patients with COVID-19 and track for real time use by health systems.
Methods: Development of this project is occurring at the time of publication, and therefore, a case study design was selected to provide a systematic means of capturing software engineering in progress. The app development framework for software design was based on agile methods. The evaluation of the app’s feasibility, acceptability and usability shall be conducted using Public Health England's guidance on evaluating digital health products, Bandura’s model of health promotion, the Reach Effectiveness Adoption Implementation Maintenance (RE-AIM) framework and the Nonadoption, Abandonment and Challenges to the Scale-up, Spread and Suitability (NASSS) framework.
Results: Making use of a pre-existing software framework for health behavior change, a proof of concept was developed, and a multistage app development and deployment for the solution was created. Grant submissions to fund the project and study execution have been sought at the time of publication, and prediscovery iteration of the solution has begun. Ethical approval for a feasibility study design is being sought.
Conclusions: This case study lays the foundations for future app development to combat mental and societal issues arising from social distancing measures. The app will be tested and evaluated in future studies to allow continuous improvement of the app. This novel contribution will provide an evidence-based exemplar for future app development in the space of social isolation and loneliness.",project-academic
,2021-07-21,a,,multi institution encrypted medical imaging ai validation without data sharing," Adoption of artificial intelligence medical imaging applications is often impeded by barriers between healthcare systems and algorithm developers given that access to both private patient data and commercial model IP is important to perform pre-deployment evaluation. This work investigates a framework for secure, privacy-preserving and AI-enabled medical imaging inference using CrypTFlow2, a state-of-the-art end-to-end compiler allowing cryptographically secure 2-party Computation (2PC) protocols between the machine learning model vendor and target patient data owner. A common DenseNet-121 chest x-ray diagnosis model was evaluated on multi-institutional chest radiographic imaging datasets both with and without CrypTFlow2 on two test sets spanning seven sites across the US and India, and comprising 1,149 chest x-ray images. We measure comparative AUROC performance between secure and insecure inference in multiple pathology classification tasks, and explore model output distributional shifts and resource constraints introduced by secure model inference. Secure inference with CrypTFlow2 demonstrated no significant difference in AUROC for all diagnoses, and model outputs from secure and insecure inference methods were distributionally equivalent. The use of CrypTFlow2 may allow off-the-shelf secure 2PC between healthcare systems and AI model vendors for medical imaging, without changes in performance, and can facilitate scalable pre-deployment infrastructure for real-world secure model evaluation without exposure to patient data or model IP.",project-academic
,2018-02-18,a,,radialgan leveraging multiple datasets to improve target specific predictive models using generative adversarial networks," Training complex machine learning models for prediction often requires a large amount of data that is not always readily available. Leveraging these external datasets from related but different sources is therefore an important task if good predictive models are to be built for deployment in settings where data can be rare. In this paper we propose a novel approach to the problem in which we use multiple GAN architectures to learn to translate from one dataset to another, thereby allowing us to effectively enlarge the target dataset, and therefore learn better predictive models than if we simply used the target dataset. We show the utility of such an approach, demonstrating that our method improves the prediction performance on the target domain over using just the target dataset and also show that our framework outperforms several other benchmarks on a collection of real-world medical datasets.",project-academic
10.1109/TETC.2021.3050733,2021-01-07,a,,sharks smart hacking approaches for risk scanning in internet of things and cyber physical systems based on machine learning," Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are increasingly being deployed across multiple functionalities, ranging from healthcare devices and wearables to critical infrastructures, e.g., nuclear power plants, autonomous vehicles, smart cities, and smart homes. These devices are inherently not secure across their comprehensive software, hardware, and network stacks, thus presenting a large attack surface that can be exploited by hackers. In this article, we present an innovative technique for detecting unknown system vulnerabilities, managing these vulnerabilities, and improving incident response when such vulnerabilities are exploited. The novelty of this approach lies in extracting intelligence from known real-world CPS/IoT attacks, representing them in the form of regular expressions, and employing machine learning (ML) techniques on this ensemble of regular expressions to generate new attack vectors and security vulnerabilities. Our results show that 10 new attack vectors and 122 new vulnerability exploits can be successfully generated that have the potential to exploit a CPS or an IoT ecosystem. The ML methodology achieves an accuracy of 97.4% and enables us to predict these attacks efficiently with an 87.2% reduction in the search space. We demonstrate the application of our method to the hacking of the in-vehicle network of a connected car. To defend against the known attacks and possible novel exploits, we discuss a defense-in-depth mechanism for various classes of attacks and the classification of data targeted by such attacks. This defense mechanism optimizes the cost of security measures based on the sensitivity of the protected resource, thus incentivizing its adoption in real-world CPS/IoT by cybersecurity practitioners.",project-academic
,2019-12-18,p,,icebreaker element wise efficient information acquisition with a bayesian deep latent gaussian model," In this paper, we address the ice-start problem, i.e., the challenge of deploying machine learning models when only a little or no training data is initially available, and acquiring each feature element of data is associated with costs. This setting is representative of the real-world machine learning applications. For instance, in the health care domain, obtaining every single measurement comes with a cost. We propose Icebreaker, a principled framework for elementwise training data acquisition. Icebreaker introduces a full Bayesian Deep Latent Gaussian Model (BELGAM) with a novel inference method, which combines recent advances in amortized inference and stochastic gradient MCMC to enable fast and accurate posterior inference. By utilizing BELGAM’s ability to fully quantify model uncertainty, we also propose two information acquisition functions for imputation and active prediction problems. We demonstrate that BELGAM performs significantly better than previous variational autoencoder (VAE) based models, when the data set size is small, using both machine learning benchmarks and real world recommender systems and health-care applications. Moreover, Icebreaker not only demonstrates improved performance compared to baselines, but it is also capable of achieving better test performance with less training data available.",project-academic
,2017-03-08,a,,dropout inference in bayesian neural networks with alpha divergences," To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed. Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty. Alpha-divergences are alternative divergences to VI's KL objective, which are able to avoid VI's uncertainty underestimation. But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners. We propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model. We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks. We study our model's epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model's uncertainty.",project-academic
10.1001/JAMAPSYCHIATRY.2018.2530,2018-12-01,a,American Medical Association,the science of prognosis in psychiatry a review," Importance None Prognosis is a venerable component of medical knowledge introduced by Hippocrates (460-377 BC). This educational review presents a contemporary evidence-based approach for how to incorporate clinical risk prediction models in modern psychiatry. The article is organized around key methodological themes most relevant for the science of prognosis in psychiatry. Within each theme, the article highlights key challenges and makes pragmatic recommendations to improve scientific understanding of prognosis in psychiatry. None Observations None The initial step to building clinical risk prediction models that can affect psychiatric care involves designing the model: preparation of the protocol and definition of the outcomes and of the statistical methods (theme 1). Further initial steps involve carefully selecting the predictors, preparing the data, and developing the model in these data. A subsequent step is the validation of the model to accurately test its generalizability (theme 2). The next consideration is that the accuracy of the clinical prediction model is affected by the incidence of the psychiatric condition under investigation (theme 3). Eventually, clinical prediction models need to be implemented in real-world clinical routine, and this is usually the most challenging step (theme 4). Advanced methods such as machine learning approaches can overcome some problems that undermine the previous steps (theme 5). The relevance of each of these themes to current clinical risk prediction modeling in psychiatry is discussed and recommendations are given. None Conclusions and Relevance None Together, these perspectives intend to contribute to an integrative, evidence-based science of prognosis in psychiatry. By focusing on the outcome of the individuals, rather than on the disease, clinical risk prediction modeling can become the cornerstone for a scientific and personalized psychiatry.",project-academic
10.1109/TNNLS.2018.2832648,2019-01-01,a,IEEE Trans Neural Netw Learn Syst,a cost sensitive deep belief network for imbalanced classification," Imbalanced data with a skewed class distribution are common in many real-world applications. Deep Belief Network (DBN) is a machine learning technique that is effective in classification tasks. However, conventional DBN does not work well for imbalanced data classification because it assumes equal costs for each class. To deal with this problem, cost-sensitive approaches assign different misclassification costs for different classes without disrupting the true data sample distributions. However, due to lack of prior knowledge, the misclassification costs are usually unknown and hard to choose in practice. Moreover, it has not been well studied as to how cost-sensitive learning could improve DBN performance on imbalanced data problems. This paper proposes an evolutionary cost-sensitive deep belief network (ECS-DBN) for imbalanced classification. ECS-DBN uses adaptive differential evolution to optimize the misclassification costs based on the training data that presents an effective approach to incorporating the evaluation measure (i.e., G-mean) into the objective function. We first optimize the misclassification costs, and then apply them to DBN. Adaptive differential evolution optimization is implemented as the optimization algorithm that automatically updates its corresponding parameters without the need of prior domain knowledge. The experiments have shown that the proposed approach consistently outperforms the state of the art on both benchmark data sets and real-world data set for fault diagnosis in tool condition monitoring.",project-academic
,2018-04-28,a,,a cost sensitive deep belief network for imbalanced classification," Imbalanced data with a skewed class distribution are common in many real-world applications. Deep Belief Network (DBN) is a machine learning technique that is effective in classification tasks. However, conventional DBN does not work well for imbalanced data classification because it assumes equal costs for each class. To deal with this problem, cost-sensitive approaches assign different misclassification costs for different classes without disrupting the true data sample distributions. However, due to lack of prior knowledge, the misclassification costs are usually unknown and hard to choose in practice. Moreover, it has not been well studied as to how cost-sensitive learning could improve DBN performance on imbalanced data problems. This paper proposes an evolutionary cost-sensitive deep belief network (ECS-DBN) for imbalanced classification. ECS-DBN uses adaptive differential evolution to optimize the misclassification costs based on training data, that presents an effective approach to incorporating the evaluation measure (i.e. G-mean) into the objective function. We first optimize the misclassification costs, then apply them to deep belief network. Adaptive differential evolution optimization is implemented as the optimization algorithm that automatically updates its corresponding parameters without the need of prior domain knowledge. The experiments have shown that the proposed approach consistently outperforms the state-of-the-art on both benchmark datasets and real-world dataset for fault diagnosis in tool condition monitoring.",project-academic
10.1109/ACCESS.2019.2891970,2019-01-01,a,IEEE Access,semi supervised learning with deep embedded clustering for image classification and segmentation," Deep neural networks usually require large labeled datasets to construct accurate models; however, in many real-world scenarios, such as medical image segmentation, labelling data is a time-consuming and costly human (expert) intelligent task. Semi-supervised methods leverage this issue by making use of a small labeled dataset and a larger set of unlabeled data. In this article, we present a flexible framework for semi-supervised learning that combines the power of supervised methods that learn feature representations using state-of-the-art deep convolutional neural networks with the deep embedded clustering algorithm that assigns data points to clusters based on their probability distributions and feature representations learned by the networks. Our proposed semi-supervised learning algorithm based on deep embedded clustering (SSLDEC) learns feature representations via iterations by alternatively using labeled and unlabeled data points and computing target distributions from predictions. During this iterative procedure the algorithm uses labeled samples to keep the model consistent and tuned with labeling, as it simultaneously learns to improve feature representation and predictions. SSLDEC requires few hyper-parameters and thus does not need large labeled validation sets, which addresses one of the main limitations of many semi-supervised learning algorithms. It is also flexible and can be used with many state-of-the-art deep neural network configurations for image classification and segmentation tasks. To this end, we implemented and tested our approach on benchmark image classification tasks as well as in a challenging medical image segmentation scenario. In benchmark classification tasks, SSLDEC outperformed several state-of-the-art semi-supervised learning methods, achieving 0.46% error on MNIST with 1000 labeled points, and 4.43% error on SVHN with 500 labeled points. In the iso-intense infant brain MRI tissue segmentation task, we implemented SSLDEC on a 3D densely connected fully convolutional neural network where we achieved significant improvement over supervised-only training as well as a semi-supervised method based on pseudo-labelling. Our results show that SSLDEC can be effectively used to reduce the need for costly expert annotations, enhancing applications such as automatic medical image segmentation.",project-academic
,2001-01-01,b,,computer vision and fuzzy neural systems," From the Publisher:

New computer vision techniques based on neural networks, fuzzy inference systems, and fuzzy-neural network models
Detailed tutorials, hands-on exercises, real-world examples, and proven algorithms 


CD-ROM: code libraries for the MATLAB neural network, fuzzy logic, and image processing toolboxes, test images from Kodak and Space Imaging, and more. 

The first complete guide to applying fuzzy-neural systems in computer vision. 

Recent advances in neural networks and fuzzy logic are transforming the field of computer vision, making it possible for computer vision applications to learn much as the brain does, and to handle imprecise visual data far more effectively. Now, Dr. Arun D. Kulkarni brings together the field's latest research and applications, presenting the field's first comprehensive tutorial and reference. 

Kulkarni starts by reviewing the fundamentals of computer vision, and the stages of a computer vision system. He shows how these stages have traditionally been implemented via statistical techniques; then introduces approaches that incorporate neural networks, fuzzy inference systems, and fuzzy-neural network models. Coverage includes: 

Preprocessing techniques such as radiometric or geometric corrections
Feature extraction, supervised and unsupervised classification, associative memories, and other techniques for improving accuracy and performance 
Key computer vision applications: remote sensing, medical imaging, compression, data mining, character recognition, stereovision, and more


Computer Vision and Fuzzy-Neural Systems illuminates the state-of-the-art throughhands-on exercises, real-world examples, and proven algorithms. It's an essential resource for every engineer, scientist, and programmer working in computer vision and a wide range of related fields. It can also be used as a textbook for undergraduate- or graduate-level courses in computer vision.
CD-ROM Included
Contains extensive library of MATLAB command files, executable files for some useful programs, and test images from Kodak and Space Imaging.

Author Biography: 
Dr. Arun D. Kulkarni is Professor of Computer Science at The University of Texas at Tyler, Tyler, Texas. His research interests include computer vision, fuzzy-neural systems, data mining, image processing, and artificial intelligence. He has authored a book and published more than 50 referred papers. His awards include the 1984 Fulbright Fellowship award and the 1997 NASA/ASSE Summer Faculty Fellowship. Dr. Kulkarni obtained his Ph.D. from the Indian Institute of Technology, Bombay, and was a post-doctoral fellow at Virginia Tech.",project-academic
10.1001/JAMAPSYCHIATRY.2017.3951,2017-12-03,a,JAMA Psychiatry,transdiagnostic symptom clusters and associations with brain behavior and daily function in mood anxiety and trauma disorders," Importance None The symptoms that define mood, anxiety, and trauma disorders are highly overlapping across disorders and heterogeneous within disorders. It is unknown whether coherent subtypes exist that span multiple diagnoses and are expressed functionally (in underlying cognition and brain function) and clinically (in daily function). The identification of cohesive subtypes would help disentangle the symptom overlap in our current diagnoses and serve as a tool for tailoring treatment choices. None Objective None To propose and demonstrate 1 approach for identifying subtypes within a transdiagnostic sample. None Design, Setting, and Participants None This cross-sectional study analyzed data from the Brain Research and Integrative Neuroscience Network Foundation Database that had been collected at the University of Sydney and University of Adelaide between 2006 and 2010 and replicated at Stanford University between 2013 and 2017. The study included 420 individuals with a primary diagnosis of major depressive disorder (n = 100), panic disorder (n = 53), posttraumatic stress disorder (n = 47), or no disorder (healthy control participants) (n = 220). Data were analyzed between October 2016 and October 2017. None Main Outcomes and Measures None We followed a data-driven approach to achieve the primary study outcome of identifying transdiagnostic subtypes. First, machine learning with a hierarchical clustering algorithm was implemented to classify participants based on self-reported negative mood, anxiety, and stress symptoms. Second, the robustness and generalizability of the subtypes were tested in an independent sample. Third, we assessed whether symptom subtypes were expressed at behavioral and physiological levels of functioning. Fourth, we evaluated the clinically meaningful differences in functional capacity of the subtypes. Findings were interpreted relative to a complementary diagnostic frame of reference. None Results None Four hundred twenty participants with a mean (SD) age of 39.8 (14.1) years were included in the final analysis; 256 (61.0%) were female. We identified 6 distinct subtypes characterized by tension (n=81; 19%), anxious arousal (n=55; 13%), general anxiety (n=38; 9%), anhedonia (n=29; 7%), melancholia (n=37; 9%), and normative mood (n=180; 43%), and these subtypes were replicated in an independent sample. Subtypes were expressed through differences in cognitive control (F5,383 = 5.13,P  None Conclusions and Relevance None These findings offer a data-driven framework for identifying robust subtypes that signify specific, coherent, meaningful associations between symptoms, behavior, brain function, and observable real-world function, and that cut acrossDSM-IV-defined diagnoses of major depressive disorder, panic disorder, and posttraumatic stress disorder.",project-academic
10.1016/S2589-7500(21)00086-8,2021-08-01,a,Elsevier,application of comprehensive artificial intelligence retinal expert care system a national real world evidence study," Summary None None Background None Medical artificial intelligence (AI) has entered the clinical implementation phase, although real-world performance of deep-learning systems (DLSs) for screening fundus disease remains unsatisfactory. Our study aimed to train a clinically applicable DLS for fundus diseases using data derived from the real world, and externally test the model using fundus photographs collected prospectively from the settings in which the model would most likely be adopted. None None None Methods None In this national real-world evidence study, we trained a DLS, the Comprehensive AI Retinal Expert (CARE) system, to identify the 14 most common retinal abnormalities using 207 228 colour fundus photographs derived from 16 clinical settings with different disease distributions. CARE was internally validated using 21 867 photographs and externally tested using 18 136 photographs prospectively collected from 35 real-world settings across China where CARE might be adopted, including eight tertiary hospitals, six community hospitals, and 21 physical examination centres. The performance of CARE was further compared with that of 16 ophthalmologists and tested using datasets with non-Chinese ethnicities and previously unused camera types. This study was registered with None ClinicalTrials.gov , None NCT04213430 , and is currently closed. None None None Findings None The area under the receiver operating characteristic curve (AUC) in the internal validation set was 0·955 (SD 0·046). AUC values in the external test set were 0·965 (0·035) in tertiary hospitals, 0·983 (0·031) in community hospitals, and 0·953 (0·042) in physical examination centres. The performance of CARE was similar to that of ophthalmologists. Large variations in sensitivity were observed among the ophthalmologists in different regions and with varying experience. The system retained strong identification performance when tested using the non-Chinese dataset (AUC 0·960, 95% CI 0·957–0·964 in referable diabetic retinopathy). None None None Interpretation None Our DLS (CARE) showed satisfactory performance for screening multiple retinal abnormalities in real-world settings using prospectively collected fundus photographs, and so could allow the system to be implemented and adopted for clinical care. None None None Funding None This study was funded by the National Key R&D Programme of China, the Science and Technology Planning Projects of Guangdong Province, the National Natural Science Foundation of China, the Natural Science Foundation of Guangdong Province, and the Fundamental Research Funds for the Central Universities. None None None Translation None For the Chinese translation of the abstract see Supplementary Materials section.",project-academic
,1996-01-01,b,,computational intelligence pc tools," Computational intelligence is an emerging field in computer science which combines fuzzy logic, neural networks, and genetic algorithms for a flexible yet powerful approach to scientific computing. Because computational intelligence combines three interrelated, mathematically-based tools, it has a wide variety of applications, from engineering and process control to experts systems. This book takes a hands-on, desktop-applications approach to the topic, featuring examples of specific real-world implementations and detailed case studies, with all pertinent code and software included on a floppy disk packaged with the book. Features: * Concise introduction to the concepts of fuzzy logic, neural networks, and genetic algorithms, and how they relate to one another within the context of computational intelligence. * Computational intellignece applications, including self-organizing feature maps, fuzzy calculator, evolutionary programming, and fuzzy neural networks. * Detailed case studies from engineering (F-16 flight system), systems control (mass transit scheduling), and medicine (appendicitis diagnosis). * Windows floppy disk with both source code and executable, self-contained programs for desktop implementation of all of the book's applications.",project-academic
,2012-03-14,b,,decision forests a unified framework for classification regression density estimation manifold learning and semi supervised learning," This review presents a unified, efficient model of random decision forests which can be applied to a number of machine learning, computer vision, and medical image analysis tasks.

Our model extends existing forest-based techniques as it unifies classification, regression, density estimation, manifold learning, semi-supervised learning, and active learning under the same decision forest framework. This gives us the opportunity to write and optimize the core implementation only once, with application to many diverse tasks.

The proposed model may be used both in a discriminative or generative way and may be applied to discrete or continuous, labeled or unlabeled data.

The main contributions of this review are: (1) Proposing a unified, probabilistic and efficient model for a variety of learning tasks; (2) Demonstrating margin-maximizing properties of classification forests; (3) Discussing probabilistic regression forests in comparison with other nonlinear regression algorithms; (4) Introducing density forests for estimating probability density functions; (5) Proposing an efficient algorithm for sampling from a density forest; (6) Introducing manifold forests for nonlinear dimensionality reduction; (7) Proposing new algorithms for transductive learning and active learning. Finally, we discuss how alternatives such as random ferns and extremely randomized trees stem from our more general forest model.

This document is directed at both students who wish to learn the basics of decision forests, as well as researchers interested in the new contributions. It presents both fundamental and novel concepts in a structured way, with many illustrative examples and real-world applications. Thorough comparisons with state-of-the-art algorithms such as support vector machines, boosting and Gaussian processes are presented and relative advantages and disadvantages discussed. The many synthetic examples and existing commercial applications demonstrate the validity of the proposed model and its flexibility.",project-academic
10.1038/S41591-018-0307-0,2019-01-07,a,Nat Med,the practical implementation of artificial intelligence technologies in medicine," The development of artificial intelligence (AI)-based technologies in medicine is advancing rapidly, but real-world clinical implementation has not yet become a reality. Here we review some of the key practical issues surrounding the implementation of AI into existing clinical workflows, including data sharing and privacy, transparency of algorithms, data standardization, and interoperability across multiple platforms, and concern for patient safety. We summarize the current regulatory environment in the United States and highlight comparisons with other regions in the world, notably Europe and China.",project-academic
,2011-10-28,a,Microsoft Technical Report,decision forests for classification regression density estimation manifold learning and semi supervised learning," This paper presents a unified, efficient model of random decision forests which can be applied to a number of machine learning, computer vision and medical image analysis tasks. Our model extends existing forest-based techniques as it unifies classification, regression, density estimation, manifold learning, semi-supervised learning and active learning under the same decision forest framework. This means that the core implementation needs be written and optimized only once, and can then be applied to many diverse tasks. The proposed model may be used both in a generative or discriminative way and may be applied to discrete or continuous, labelled or unlabelled data. The main contributions of this paper are: 1) proposing a single, probabilistic and efficient model for a variety of learning tasks; 2) demonstrating margin-maximizing properties of classification forests; 3) introducing density forests for learning accurate probability density functions; 4) proposing efficient algorithms for sampling from the forest generative model; 5) introducing manifold forests for non-linear embedding and dimensionality reduction; 6) proposing new and efficient forest-based algorithms for transductive and active learning. We discuss how alternatives such as random ferns and extremely randomized trees stem from our more general model. This paper is directed at both students who wish to learn the basics of decision forests, as well as researchers interested in our new contributions. It presents both fundamental and novel concepts in a structured way, with many illustrative examples and real-world applications. Thorough comparisons with state of the art algorithms such as support vector machines, boosting and Gaussian processes are presented and relative advantages and disadvantages discussed.The many synthetic examples and existing commercial applications demonstrate the validity of the proposed model and its flexibility. Powerpoint slides (with many examples and animations) are also available from http://research.microsoft.com/groups/vision/decisionforests.aspx",project-academic
,2015-01-01,b,,cognitive computing and big data analytics," A comprehensive guide to learning technologies that unlock the value in big data Cognitive Computing provides detailed guidance toward building a new class of systems that learn from experience and derive insights to unlock the value of big data. This book helps technologists understand cognitive computing's underlying technologies, from knowledge representation techniques and natural language processing algorithms to dynamic learning approaches based on accumulated evidence, rather than reprogramming. Detailed case examples from the financial, healthcare, and manufacturing walk readers step-by-step through the design and testing of cognitive systems, and expert perspectives from organizations such as Cleveland Clinic, Memorial Sloan-Kettering, as well as commercial vendors that are creating solutions. These organizations provide insight into the real-world implementation of cognitive computing systems. The IBM Watson cognitive computing platform is described in a detailed chapter because of its significance in helping to define this emerging market. In addition, the book includes implementations of emerging projects from Qualcomm, Hitachi, Google and Amazon. Today's cognitive computing solutions build on established concepts from artificial intelligence, natural language processing, ontologies, and leverage advances in big data management and analytics. They foreshadow an intelligent infrastructure that enables a new generation of customer and context-aware smart applications in all industries. Cognitive Computing is a comprehensive guide to the subject, providing both the theoretical and practical guidance technologists need. * Discover how cognitive computing evolved from promise to reality * Learn the elements that make up a cognitive computing system * Understand the groundbreaking hardware and software technologies behind cognitive computing * Learn to evaluate your own application portfolio to find the best candidates for pilot projects * Leverage cognitive computing capabilities to transform the organization Cognitive systems are rightly being hailed as the new era of computing. Learn how these technologies enable emerging firms to compete with entrenched giants, and forward-thinking established firms to disrupt their industries. Professionals who currently work with big data and analytics will see how cognitive computing builds on their foundation, and creates new opportunities. Cognitive Computing provides complete guidance to this new level of human-machine interaction.",project-academic
10.2337/DC20-1877,2021-01-05,a,American Diabetes Association,multicenter head to head real world validation study of seven automated artificial intelligence diabetic retinopathy screening systems," OBJECTIVE None With rising global prevalence of diabetic retinopathy (DR), automated DR screening is needed for primary care settings. Two automated artificial intelligence (AI)–based DR screening algorithms have U.S. Food and Drug Administration (FDA) approval. Several others are under consideration while in clinical use in other countries, but their real-world performance has not been evaluated systematically. We compared the performance of seven automated AI-based DR screening algorithms (including one FDA-approved algorithm) against human graders when analyzing real-world retinal imaging data. None RESEARCH DESIGN AND METHODS None This was a multicenter, noninterventional device validation study evaluating a total of 311,604 retinal images from 23,724 veterans who presented for teleretinal DR screening at the Veterans Affairs (VA) Puget Sound Health Care System (HCS) or Atlanta VA HCS from 2006 to 2018. Five companies provided seven algorithms, including one with FDA approval, that independently analyzed all scans, regardless of image quality. The sensitivity/specificity of each algorithm when classifying images as referable DR or not were compared with original VA teleretinal grades and a regraded arbitrated data set. Value per encounter was estimated. None RESULTS None Although high negative predictive values (82.72–93.69%) were observed, sensitivities varied widely (50.98–85.90%). Most algorithms performed no better than humans against the arbitrated data set, but two achieved higher sensitivities, and one yielded comparable sensitivity (80.47%, P = 0.441) and specificity (81.28%, P = 0.195). Notably, one had lower sensitivity (74.42%) for proliferative DR (P = 9.77 × 10−4) than the VA teleretinal graders. Value per encounter varied at $15.14–$18.06 for ophthalmologists and $7.74–$9.24 for optometrists. None CONCLUSIONS None The DR screening algorithms showed significant performance differences. These results argue for rigorous testing of all such algorithms on real-world data before clinical implementation.",project-academic
10.1038/S41746-018-0045-1,2018-12-05,a,,machine learned epidemiology real time detection of foodborne illness at scale," Machine learning has become an increasingly powerful tool for solving complex problems, and its application in public health has been underutilized. The objective of this study is to test the efficacy of a machine-learned model of foodborne illness detection in a real-world setting. To this end, we built FINDER, a machine-learned model for real-time detection of foodborne illness using anonymous and aggregated web search and location data. We computed the fraction of people who visited a particular restaurant and later searched for terms indicative of food poisoning to identify potentially unsafe restaurants. We used this information to focus restaurant inspections in two cities and demonstrated that FINDER improves the accuracy of health inspections; restaurants identified by FINDER are 3.1 times as likely to be deemed unsafe during the inspection as restaurants identified by existing methods. Additionally, FINDER enables us to ascertain previously intractable epidemiological information, for example, in 38% of cases the restaurant potentially causing food poisoning was not the last one visited, which may explain the lower precision of complaint-based inspections. We found that FINDER is able to reliably identify restaurants that have an active lapse in food safety, allowing for implementation of corrective actions that would prevent the potential spread of foodborne illness.",project-academic
,2021-05-31,p,,multibench multiscale benchmarks for multimodal representation learning," Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, human-computer interaction, and healthcare. Unfortunately, multimodal research has seen limited resources to study (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiBench, a systematic and unified large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench introduces impactful challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning. Simply applying methods proposed in different research areas can improve the state-of-the-art performance on 9/15 datasets. Therefore, MultiBench presents a milestone in unifying disjoint efforts in multimodal research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MultiBench, our standardized code, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community.",project-academic
10.1038/S42256-020-0198-X,2020-07-06,a,Nature Publishing Group,quantum approximate bayesian computation for nmr model inference," Recent technological advances may lead to the development of small-scale quantum computers that are capable of solving problems that cannot be tackled with classical computers. A limited number of algorithms have been proposed and their relevance to real-world problems is a subject of active investigation. Analysis of many-body quantum systems is particularly challenging for classical computers due to the exponential scaling of the Hilbert space dimension with the number of particles. Hence, solving the problems relevant to chemistry and condensed-matter physics is expected to be the first successful application of quantum computers. In this Article, we propose another class of problems from the quantum realm that can be solved efficiently on quantum computers: model inference for nuclear magnetic resonance (NMR) spectroscopy, which is important for biological and medical research. Our results are based on three interconnected studies. First, we use methods from classical machine learning to analyse a dataset of NMR spectra of small molecules. We perform stochastic neighbourhood embedding and identify clusters of spectra, and demonstrate that these clusters are correlated with the covalent structure of the molecules. Second, we propose a simple and efficient method, aided by a quantum simulator, to extract the NMR spectrum of any hypothetical molecule described by a parametric Heisenberg model. Third, we propose a simple variational Bayesian inference procedure for estimating the Hamiltonian parameters of experimentally relevant NMR spectra. Currently available quantum hardware is limited by noise, so practical implementations often involve a combination with classical approaches. Sels et al. identify a promising application for such a quantum–classic hybrid approach, namely inferring molecular structure from NMR spectra, by employing a range of machine learning tools in combination with a quantum simulator.",project-academic
10.1016/J.AHJ.2019.10.007,2020-01-01,a,Am Heart J,ecg ai guided screening for low ejection fraction eagle rationale and design of a pragmatic cluster randomized trial," Background None A deep learning algorithm to detect low ejection fraction (EF) using routine 12-lead electrocardiogram (ECG) has recently been developed and validated. The algorithm was incorporated into the electronic health record (EHR) to automatically screen for low EF, encouraging clinicians to obtain a confirmatory transthoracic echocardiogram (TTE) for previously undiagnosed patients, thereby facilitating early diagnosis and treatment. None None None Objectives None To prospectively evaluate a novel artificial intelligence (AI) screening tool for detecting low EF in primary care practices. None None None Design None The EAGLE trial is a pragmatic two-arm cluster randomized trial ( NCT04000087 ) that will randomize >100 clinical teams (i.e., clusters) to either intervention (access to the new AI screening tool) or control (usual care) at 48 primary care practices across Minnesota and Wisconsin. The trial is expected to involve approximately 400 clinicians and 20,000 patients. The primary endpoint is newly discovered EF ≤50%. Eligible patients will include adults who undergo ECG for any reason and have not been previously diagnosed with low EF. Data will be pulled from the EHR, and no contact will be made with patients. A positive deviance qualitative study and a post-implementation survey will be conducted among select clinicians to identify facilitators and barriers to using the new screening report. None None None Summary None This trial will examine the effectiveness of the AI-enabled ECG for detection of asymptomatic low EF in routine primary care practices and will be among the first to prospectively evaluate the value of AI in real-world practice. Its findings will inform future implementation strategies for the translation of other AI-enabled algorithms.",project-academic
10.1109/TVCG.2018.2864838,2019-01-01,a,IEEE,vis4ml an ontology for visual analytics assisted machine learning," While many VA workflows make use of machine-learned models to support analytical tasks, VA workflows have become increasingly important in understanding and improving Machine Learning (ML) processes. In this paper, we propose an ontology (VIS4ML) for a subarea of VA, namely “VA-assisted ML”. The purpose of VIS4ML is to describe and understand existing VA workflows used in ML as well as to detect gaps in ML processes and the potential of introducing advanced VA techniques to such processes. Ontologies have been widely used to map out the scope of a topic in biology, medicine, and many other disciplines. We adopt the scholarly methodologies for constructing VIS4ML, including the specification, conceptualization, formalization, implementation, and validation of ontologies. In particular, we reinterpret the traditional VA pipeline to encompass model-development workflows. We introduce necessary definitions, rules, syntaxes, and visual notations for formulating VIS4ML and make use of semantic web technologies for implementing it in the Web Ontology Language (OWL). VIS4ML captures the high-level knowledge about previous workflows where VA is used to assist in ML. It is consistent with the established VA concepts and will continue to evolve along with the future developments in VA and ML. While this ontology is an effort for building the theoretical foundation of VA, it can be used by practitioners in real-world applications to optimize model-development workflows by systematically examining the potential benefits that can be brought about by either machine or human capabilities. Meanwhile, VIS4ML is intended to be extensible and will continue to be updated to reflect future advancements in using VA for building high-quality data-analytical models or for building such models rapidly.",project-academic
10.1201/B11431,2011-12-20,b,CRC Press,manifold learning theory and applications," Trained to extract actionable information from large volumes of high-dimensional data, engineers and scientists often have trouble isolating meaningful low-dimensional structures hidden in their high-dimensional observations. Manifold learning, a groundbreaking technique designed to tackle these issues of dimensionality reduction, finds widespread application in machine learning, neural networks, pattern recognition, image processing, and computer vision. Filling a void in the literature, Manifold Learning Theory and Applications incorporates state-of-the-art techniques in manifold learning with a solid theoretical and practical treatment of the subject. Comprehensive in its coverage, this pioneering work explores this novel modality from algorithm creation to successful implementationoffering examples of applications in medical, biometrics, multimedia, and computer vision. Emphasizing implementation, it highlights the various permutations of manifold learning in industry including manifold optimization, large scale manifold learning, semidefinite programming for embedding, manifold models for signal acquisition, compression and processing, and multi scale manifold. Beginning with an introduction to manifold learning theories and applications, the book includes discussions on the relevance to nonlinear dimensionality reduction, clustering, graph-based subspace learning, spectral learning and embedding, extensions, and multi-manifold modeling. It synergizes cross-domain knowledge for interdisciplinary instructions, offers a rich set of specialized topics contributed by expert professionals and researchers from a variety of fields. Finally, the book discusses specific algorithms and methodologies using case studies to apply manifold learning for real-world problems.",project-academic
10.1111/TBJ.13718,2020-01-01,a,"John Wiley & Sons, Ltd",natural language processing to facilitate breast cancer research and management," The medical literature has been growing exponentially, and its size has become a barrier for physicians to locate and extract clinically useful information. As a promising solution, natural language processing (NLP), especially machine learning (ML)-based NLP is a technology that potentially provides a promising solution. ML-based NLP is based on training a computational algorithm with a large number of annotated examples to allow the computer to ""learn"" and ""predict"" the meaning of human language. Although NLP has been widely applied in industry and business, most physicians still are not aware of the huge potential of this technology in medicine, and the implementation of NLP in breast cancer research and management is fairly limited. With a real-world successful project of identifying penetrance papers for breast and other cancer susceptibility genes, this review illustrates how to train and evaluate an NLP-based medical abstract classifier, incorporate it into a semiautomatic meta-analysis procedure, and validate the effectiveness of this procedure. Other implementations of NLP technology in breast cancer research, such as parsing pathology reports and mining electronic healthcare records, are also discussed. We hope this review will help breast cancer physicians and researchers to recognize, understand, and apply this technology to meet their own clinical or research needs.",project-academic
,2019-08-23,a,,neural cognitive diagnosis for intelligent education systems," Cognitive diagnosis is a fundamental issue in intelligent education, which aims to discover the proficiency level of students on specific knowledge concepts. Existing approaches usually mine linear interactions of student exercising process by manual-designed function (e.g., logistic function), which is not sufficient for capturing complex relations between students and exercises. In this paper, we propose a general Neural Cognitive Diagnosis (NeuralCD) framework, which incorporates neural networks to learn the complex exercising interactions, for getting both accurate and interpretable diagnosis results. Specifically, we project students and exercises to factor vectors and leverage multi neural layers for modeling their interactions, where the monotonicity assumption is applied to ensure the interpretability of both factors. Furthermore, we propose two implementations of NeuralCD by specializing the required concepts of each exercise, i.e., the NeuralCDM with traditional Q-matrix and the improved NeuralCDM+ exploring the rich text content. Extensive experimental results on real-world datasets show the effectiveness of NeuralCD framework with both accuracy and interpretability.",project-academic
10.1609/AAAI.V34I04.6080,2020-04-03,p,Association for the Advancement of Artificial Intelligence (AAAI),neural cognitive diagnosis for intelligent education systems," Cognitive diagnosis is a fundamental issue in intelligent education, which aims to discover the proficiency level of students on specific knowledge concepts. Existing approaches usually mine linear interactions of student exercising process by manual-designed function (e.g., logistic function), which is not sufficient for capturing complex relations between students and exercises. In this paper, we propose a general Neural Cognitive Diagnosis (NeuralCD) framework, which incorporates neural networks to learn the complex exercising interactions, for getting both accurate and interpretable diagnosis results. Specifically, we project students and exercises to factor vectors and leverage multi neural layers for modeling their interactions, where the monotonicity assumption is applied to ensure the interpretability of both factors. Furthermore, we propose two implementations of NeuralCD by specializing the required concepts of each exercise, i.e., the NeuralCDM with traditional Q-matrix and the improved NeuralCDM+ exploring the rich text content. Extensive experimental results on real-world datasets show the effectiveness of NeuralCD framework with both accuracy and interpretability.",project-academic
10.1038/S41746-021-00416-5,2021-03-05,a,Springer Science and Business Media LLC,deep learning system to improve the quality and efficiency of volumetric heart segmentation for breast cancer," Although artificial intelligence algorithms are often developed and applied for narrow tasks, their implementation in other medical settings could help to improve patient care. Here we assess whether a deep-learning system for volumetric heart segmentation on computed tomography (CT) scans developed in cardiovascular radiology can optimize treatment planning in radiation oncology. The system was trained using multi-center data (n = 858) with manual heart segmentations provided by cardiovascular radiologists. Validation of the system was performed in an independent real-world dataset of 5677 breast cancer patients treated with radiation therapy at the Dana-Farber/Brigham and Women’s Cancer Center between 2008–2018. In a subset of 20 patients, the performance of the system was compared to eight radiation oncology experts by assessing segmentation time, agreement between experts, and accuracy with and without deep-learning assistance. To compare the performance to segmentations used in the clinic, concordance and failures (defined as Dice < 0.85) of the system were evaluated in the entire dataset. The system was successfully applied without retraining. With deep-learning assistance, segmentation time significantly decreased (4.0 min [IQR 3.1–5.0] vs. 2.0 min [IQR 1.3–3.5]; p < 0.001), and agreement increased (Dice 0.95 [IQR = 0.02]; vs. 0.97 [IQR = 0.02], p < 0.001). Expert accuracy was similar with and without deep-learning assistance (Dice 0.92 [IQR = 0.02] vs. 0.92 [IQR = 0.02]; p = 0.48), and not significantly different from deep-learning-only segmentations (Dice 0.92 [IQR = 0.02]; p ≥ 0.1). In comparison to real-world data, the system showed high concordance (Dice 0.89 [IQR = 0.06]) across 5677 patients and a significantly lower failure rate (p < 0.001). These results suggest that deep-learning algorithms can successfully be applied across medical specialties and improve clinical care beyond the original field of interest.",project-academic
10.1016/J.MEDIA.2020.101942,2021-04-01,a,Elsevier,automated interpretation of congenital heart disease from multi view echocardiograms," Congenital heart disease (CHD) is the most common birth defect and the leading cause of neonate death in China. Clinical diagnosis can be based on the selected 2D key-frames from five views. Limited by the availability of multi-view data, most methods have to rely on the insufficient single view analysis. This study proposes to automatically analyze the multi-view echocardiograms with a practical end-to-end framework. We collect the five-view echocardiograms video records of 1308 subjects (including normal controls, ventricular septal defect (VSD) patients and atrial septal defect (ASD) patients) with both disease labels and standard-view key-frame labels. Depthwise separable convolution-based multi-channel networks are adopted to largely reduce the network parameters. We also approach the imbalanced class problem by augmenting the positive training samples. Our 2D key-frame model can diagnose CHD or negative samples with an accuracy of 95.4%, and in negative, VSD or ASD classification with an accuracy of 92.3%. To further alleviate the work of key-frame selection in real-world implementation, we propose an adaptive soft attention scheme to directly explore the raw video data. Four kinds of neural aggregation methods are systematically investigated to fuse the information of an arbitrary number of frames in a video. Moreover, with a view detection module, the system can work without the view records. Our video-based model can diagnose with an accuracy of 93.9% (binary classification), and 92.1% (3-class classification) in a collected 2D video testing set, which does not need key-frame selection and view annotation in testing. The detailed ablation study and the interpretability analysis are provided. The presented model has high diagnostic rates for VSD and ASD that can be potentially applied to the clinical practice in the future. The short-term automated machine learning process can partially replace and promote the long-term professional training of primary doctors, improving the primary diagnosis rate of CHD in China, and laying the foundation for early diagnosis and timely treatment of children with CHD.",project-academic
10.1016/J.BREAST.2019.10.001,2020-02-01,a,Churchill Livingstone,the ethical legal and social implications of using artificial intelligence systems in breast cancer care," Abstract None None Breast cancer care is a leading area for development of artificial intelligence (AI), with applications including screening and diagnosis, risk calculation, prognostication and clinical decision-support, management planning, and precision medicine. We review the ethical, legal and social implications of these developments. We consider the values encoded in algorithms, the need to evaluate outcomes, and issues of bias and transferability, data ownership, confidentiality and consent, and legal, moral and professional responsibility. We consider potential effects for patients, including on trust in healthcare, and provide some social science explanations for the apparent rush to implement AI solutions. We conclude by anticipating future directions for AI in breast cancer care. Stakeholders in healthcare AI should acknowledge that their enterprise is an ethical, legal and social challenge, not just a technical challenge. Taking these challenges seriously will require broad engagement, imposition of conditions on implementation, and pre-emptive systems of oversight to ensure that development does not run ahead of evaluation and deliberation. Once artificial intelligence becomes institutionalised, it may be difficult to reverse: a proactive role for government, regulators and professional groups will help ensure introduction in robust research contexts, and the development of a sound evidence base regarding real-world effectiveness. Detailed public discussion is required to consider what kind of AI is acceptable rather than simply accepting what is offered, thus optimising outcomes for health systems, professionals, society and those receiving care.",project-academic
10.1038/S42256-021-00343-W,2021-05-31,a,Springer Science and Business Media LLC,improving performance of deep learning models with axiomatic attribution priors and expected gradients," Recent research has demonstrated that feature attribution methods for deep networks can themselves be incorporated into training; these attribution priors optimize for a model whose attributions have certain desirable properties—most frequently, that particular features are important or unimportant. These attribution priors are often based on attribution methods that are not guaranteed to satisfy desirable interpretability axioms, such as completeness and implementation invariance. Here we introduce attribution priors to optimize for higher-level properties of explanations, such as smoothness and sparsity, enabled by a fast new attribution method formulation called expected gradients that satisfies many important interpretability axioms. This improves model performance on many real-world tasks where previous attribution priors fail. Our experiments show that the gains from combining higher-level attribution priors with expected gradients attributions are consistent across image, gene expression and healthcare datasets. We believe that this work motivates and provides the necessary tools to support the widespread adoption of axiomatic attribution priors in many areas of applied machine learning. The implementations and our results have been made freely available to academic communities. Neural networks are becoming increasingly popular for applications in various domains, but in practice, further methods are necessary to make sure the models are learning patterns that agree with prior knowledge about the domain. A new approach introduces an explanation method, called ‘expected gradients’, that enables training with theoretically motivated feature attribution priors, to improve model performance on real-world tasks.",project-academic
,2019-06-25,a,,improving performance of deep learning models with axiomatic attribution priors and expected gradients," Recent research has demonstrated that feature attribution methods for deep networks can themselves be incorporated into training; these attribution priors optimize for a model whose attributions have certain desirable properties -- most frequently, that particular features are important or unimportant. These attribution priors are often based on attribution methods that are not guaranteed to satisfy desirable interpretability axioms, such as completeness and implementation invariance. Here, we introduce attribution priors to optimize for higher-level properties of explanations, such as smoothness and sparsity, enabled by a fast new attribution method formulation called expected gradients that satisfies many important interpretability axioms. This improves model performance on many real-world tasks where previous attribution priors fail. Our experiments show that the gains from combining higher-level attribution priors with expected gradients attributions are consistent across image, gene expression, and health care data sets. We believe this work motivates and provides the necessary tools to support the widespread adoption of axiomatic attribution priors in many areas of applied machine learning. The implementations and our results have been made freely available to academic communities.",project-academic
10.1093/JAMIAOPEN/OOAA033,2020-09-08,a,Oxford Academic,evaluating artificial intelligence in medicine phases of clinical research," Increased scrutiny of artificial intelligence (AI) applications in healthcare highlights the need for real-world evaluations for effectiveness and unintended consequences. The complexity of healthcare, compounded by the user- and context-dependent nature of AI applications, calls for a multifaceted approach beyond traditional in silico evaluation of AI. We propose an interdisciplinary, phased research framework for evaluation of AI implementations in healthcare. We draw analogies to and highlight differences from the clinical trial phases for drugs and medical devices, and we present study design and methodological guidance for each stage.",project-academic
10.1186/S12916-019-1382-X,2019-07-17,a,BioMed Central,beyond the hype of big data and artificial intelligence building foundations for knowledge and wisdom," Big data, coupled with the use of advanced analytical approaches, such as artificial intelligence (AI), have the potential to improve medical outcomes and population health. Data that are routinely generated from, for example, electronic medical records and smart devices have become progressively easier and cheaper to collect, process, and analyze. In recent decades, this has prompted a substantial increase in biomedical research efforts outside traditional clinical trial settings. Despite the apparent enthusiasm of researchers, funders, and the media, evidence is scarce for successful implementation of products, algorithms, and services arising that make a real difference to clinical care. This article collection provides concrete examples of how “big data” can be used to advance healthcare and discusses some of the limitations and challenges encountered with this type of research. It primarily focuses on real-world data, such as electronic medical records and genomic medicine, considers new developments in AI and digital health, and discusses ethical considerations and issues related to data sharing. Overall, we remain positive that big data studies and associated new technologies will continue to guide novel, exciting research that will ultimately improve healthcare and medicine—but we are also realistic that concerns remain about privacy, equity, security, and benefit to all.",project-academic
,2017-05-27,a,,targeted learning with daily ehr data," Electronic health records (EHR) data provide a cost and time-effective opportunity to conduct cohort studies of the effects of multiple time-point interventions in the diverse patient population found in real-world clinical settings. Because the computational cost of analyzing EHR data at daily (or more granular) scale can be quite high, a pragmatic approach has been to partition the follow-up into coarser intervals of pre-specified length. Current guidelines suggest employing a 'small' interval, but the feasibility and practical impact of this recommendation has not been evaluated and no formal methodology to inform this choice has been developed. We start filling these gaps by leveraging large-scale EHR data from a diabetes study to develop and illustrate a fast and scalable targeted learning approach that allows to follow the current recommendation and study its practical impact on inference. More specifically, we map daily EHR data into four analytic datasets using 90, 30, 15 and 5-day intervals. We apply a semi-parametric and doubly robust estimation approach, the longitudinal TMLE, to estimate the causal effects of four dynamic treatment rules with each dataset, and compare the resulting inferences. To overcome the computational challenges presented by the size of these data, we propose a novel TMLE implementation, the 'long-format TMLE', and rely on the latest advances in scalable data-adaptive machine-learning software, xgboost and h2o, for estimation of the TMLE nuisance parameters.",project-academic
10.1002/SIM.8164,2019-07-20,a,Stat Med,targeted learning with daily ehr data," Electronic health records (EHR) data provide a cost- and time-effective opportunity to conduct cohort studies of the effects of multiple time-point interventions in the diverse patient population found in real-world clinical settings. Because the computational cost of analyzing EHR data at daily (or more granular) scale can be quite high, a pragmatic approach has been to partition the follow-up into coarser intervals of pre-specified length (eg, quarterly or monthly intervals). The feasibility and practical impact of analyzing EHR data at a granular scale has not been previously evaluated. We start filling these gaps by leveraging large-scale EHR data from a diabetes study to develop a scalable targeted learning approach that allows analyses with small intervals. We then study the practical effects of selecting different coarsening intervals on inferences by reanalyzing data from the same large-scale pool of patients. Specifically, we map daily EHR data into four analytic datasets using 90-, 30-, 15-, and 5-day intervals. We apply a semiparametric and doubly robust estimation approach, the longitudinal Targeted Minimum Loss-Based Estimation (TMLE), to estimate the causal effects of four dynamic treatment rules with each dataset, and compare the resulting inferences. To overcome the computational challenges presented by the size of these data, we propose a novel TMLE implementation, the ""long-format TMLE,"" and rely on the latest advances in scalable data-adaptive machine-learning software, xgboost and h2o, for estimation of the TMLE nuisance parameters.",project-academic
10.1186/S12911-021-01634-3,2021-10-02,a,BioMed Central Ltd.,a framework for validating ai in precision medicine considerations from the european itfoc consortium," BACKGROUND Artificial intelligence (AI) has the potential to transform our healthcare systems significantly. New AI technologies based on machine learning approaches should play a key role in clinical decision-making in the future. However, their implementation in health care settings remains limited, mostly due to a lack of robust validation procedures. There is a need to develop reliable assessment frameworks for the clinical validation of AI. We present here an approach for assessing AI for predicting treatment response in triple-negative breast cancer (TNBC), using real-world data and molecular -omics data from clinical data warehouses and biobanks. METHODS The European ""ITFoC (Information Technology for the Future Of Cancer)"" consortium designed a framework for the clinical validation of AI technologies for predicting treatment response in oncology. RESULTS This framework is based on seven key steps specifying: (1) the intended use of AI, (2) the target population, (3) the timing of AI evaluation, (4) the datasets used for evaluation, (5) the procedures used for ensuring data safety (including data quality, privacy and security), (6) the metrics used for measuring performance, and (7) the procedures used to ensure that the AI is explainable. This framework forms the basis of a validation platform that we are building for the ""ITFoC Challenge"". This community-wide competition will make it possible to assess and compare AI algorithms for predicting the response to TNBC treatments with external real-world datasets. CONCLUSIONS The predictive performance and safety of AI technologies must be assessed in a robust, unbiased and transparent manner before their implementation in healthcare settings. We believe that the consideration of the ITFoC consortium will contribute to the safe transfer and implementation of AI in clinical settings, in the context of precision oncology and personalized care.",project-academic
10.1109/TCIAIG.2013.2285651,2014-03-01,a,IEEE,deepqa jeopardy gamification a machine learning perspective," DeepQA is a large-scale natural language processing (NLP) question-and-answer system that responds across a breadth of structured and unstructured data, from hundreds of analytics that are combined with over 50 models, trained through machine learning. After the 2011 historic milestone of defeating the two best human players in the Jeopardy! game show, the technology behind IBM Watson, DeepQA, is undergoing gamification into real-world business problems. Gamifying a business domain for Watson is a composite of functional, content, and training adaptation for nongame play. During domain gamification for medical, financial, government, or any other business, each system change affects the machine-learning process. As opposed to the original Watson Jeopardy!, whose class distribution of positive-to-negative labels is 1:100, in adaptation the computed training instances, question-and-answer pairs transformed into true-false labels, result in a very low positive-to-negative ratio of 1:100 000. Such initial extreme class imbalance during domain gamification poses a big challenge for the Watson machine-learning pipelines. The combination of ingested corpus sets, question-and-answer pairs, configuration settings, and NLP algorithms contribute toward the challenging data state. We propose several data engineering techniques, such as answer key vetting and expansion, source ingestion, oversampling classes, and question set modifications to increase the computed true labels. In addition, algorithm engineering, such as an implementation of the Newton-Raphson logistic regression with a regularization term, relaxes the constraints of class imbalance during training adaptation. We conclude by empirically demonstrating that data and algorithm engineering are complementary and indispensable to overcome the challenges in this first Watson gamification for real-world business problems.",project-academic
10.1136/BMJOPEN-2019-030482,2019-12-11,a,BMJ Open,current state of science in machine learning methods for automatic infant pain evaluation using facial expression information study protocol of a systematic review and meta analysis," Introduction None Infants can experience pain similar to adults, and improperly controlled pain stimuli could have a long-term adverse impact on their cognitive and neurological function development. The biggest challenge of achieving good infant pain control is obtaining objective pain assessment when direct communication is lacking. For years, computer scientists have developed many different facial expression-centred machine learning (ML) methods for automatic infant pain assessment. Many of these ML algorithms showed rather satisfactory performance and have demonstrated good potential to be further enhanced for implementation in real-world clinical settings. To date, there is no prior research that has systematically summarised and compared the performance of these ML algorithms. Our proposed meta-analysis will provide the first comprehensive evidence on this topic to guide further ML algorithm development and clinical implementation. None Methods and analysis None We will search four major public electronic medical and computer science databases including Web of Science, PubMed, Embase and IEEE Xplore Digital Library from January 2008 to present. All the articles will be imported into the Covidence platform for study eligibility screening and inclusion. Study-level extracted data will be stored in the Systematic Review Data Repository online platform. The primary outcome will be the prediction accuracy of the ML model. The secondary outcomes will be model utility measures including generalisability, interpretability and computational efficiency. All extracted outcome data will be imported into RevMan V.5.2.1 software and R V3.3.2 for analysis. Risk of bias will be summarised using the latest Prediction Model Study Risk of Bias Assessment Tool. None Ethics and dissemination None This systematic review and meta-analysis will only use study-level data from public databases, thus formal ethical approval is not required. The results will be disseminated in the form of an official publication in a peer-reviewed journal and/or presentation at relevant conferences. None PROSPERO registration number None CRD42019118784.",project-academic
10.1016/J.INS.2020.02.037,2020-06-01,a,Elsevier,a training integrity privacy preserving federated learning scheme with trusted execution environment," Abstract None None Machine learning models trained on sensitive real-world data promise improvements to everything from medical screening to disease outbreak discovery. In many application domains, learning participants would benefit from pooling their private datasets, training precise machine learning models on the aggregate data, and sharing the profits of using these models. Considering privacy and security concerns often prevent participants from contributing sensitive data for training, researchers proposed several techniques to achieve data privacy in federated learning systems. However, such techniques are susceptible to causative attacks, whereby malicious participants can inject false training results with the aim of corrupting the well-learned model. To end this, in this paper, we propose a new privacy-preserving federated learning scheme that guarantees the integrity of deep learning processes. Based on the Trusted Execution Environment (TEE), we design a training-integrity protocol for this scheme, in which causative attacks can be detected. Thus, each participant is compelled to execute the privacy-preserving learning algorithm of the scheme correctly. We evaluate the performance of our scheme by prototype implementations. The experimental result shows that the scheme is training-integrity and practical.",project-academic
10.1186/S12874-019-0681-4,2019-03-19,a,BioMed Central,machine learning in medicine a practical introduction," Following visible successes on a wide range of predictive tasks, machine learning techniques are attracting substantial interest from medical researchers and clinicians. We address the need for capacity development in this area by providing a conceptual introduction to machine learning alongside a practical guide to developing and evaluating predictive algorithms using freely-available open source software and public domain data. We demonstrate the use of machine learning techniques by developing three predictive models for cancer diagnosis using descriptions of nuclei sampled from breast masses. These algorithms include regularized General Linear Model regression (GLMs), Support Vector Machines (SVMs) with a radial basis function kernel, and single-layer Artificial Neural Networks. The publicly-available dataset describing the breast mass samples (N=683) was randomly split into evaluation (n=456) and validation (n=227) samples. We trained algorithms on data from the evaluation sample before they were used to predict the diagnostic outcome in the validation dataset. We compared the predictions made on the validation datasets with the real-world diagnostic decisions to calculate the accuracy, sensitivity, and specificity of the three models. We explored the use of averaging and voting ensembles to improve predictive performance. We provide a step-by-step guide to developing algorithms using the open-source R statistical programming environment. The trained algorithms were able to classify cell nuclei with high accuracy (.94 -.96), sensitivity (.97 -.99), and specificity (.85 -.94). Maximum accuracy (.96) and area under the curve (.97) was achieved using the SVM algorithm. Prediction performance increased marginally (accuracy =.97, sensitivity =.99, specificity =.95) when algorithms were arranged into a voting ensemble. We use a straightforward example to demonstrate the theory and practice of machine learning for clinicians and medical researchers. The principals which we demonstrate here can be readily applied to other complex tasks including natural language processing and image recognition.",project-academic
10.1109/JIOT.2015.2411227,2015-03-06,a,IEEE,a practical evaluation of information processing and abstraction techniques for the internet of things," The term Internet of Things (IoT) refers to the interaction and communication between billions of devices that produce and exchange data related to real-world objects (i.e. things). Extracting higher level information from the raw sensory data captured by the devices and representing this data as machine-interpretable or human-understandable information has several interesting applications. Deriving raw data into higher level information representations demands mechanisms to find, extract, and characterize meaningful abstractions from the raw data. This meaningful abstractions then have to be presented in a human and/or machine-understandable representation. However, the heterogeneity of the data originated from different sensor devices and application scenarios such as e-health, environmental monitoring, and smart home applications, and the dynamic nature of sensor data make it difficult to apply only one particular information processing technique to the underlying data. A considerable amount of methods from machine-learning, the semantic web, as well as pattern and data mining have been used to abstract from sensor observations to information representations. This paper provides a survey of the requirements and solutions and describes challenges in the area of information abstraction and presents an efficient workflow to extract meaningful information from raw sensor data based on the current state-of-the-art in this area. This paper also identifies research directions at the edge of information abstraction for sensor data. To ease the understanding of the abstraction workflow process, we introduce a software toolkit that implements the introduced techniques and motivates to apply them on various data sets.",project-academic
10.1109/TPDS.2018.2870403,2019-04-01,a,IEEE,online diagnosis of performance variation in hpc systems using machine learning," As the size and complexity of high performance computing (HPC) systems grow in line with advancements in hardware and software technology, HPC systems increasingly suffer from performance variations due to shared resource contention as well as software- and hardware-related problems. Such performance variations can lead to failures and inefficiencies, which impact the cost and resilience of HPC systems. To minimize the impact of performance variations, one must quickly and accurately detect and diagnose the anomalies that cause the variations and take mitigating actions. However, it is difficult to identify anomalies based on the voluminous, high-dimensional, and noisy data collected by system monitoring infrastructures. This paper presents a novel machine learning based framework to automatically diagnose performance anomalies at runtime. Our framework leverages historical resource usage data to extract signatures of previously-observed anomalies. We first convert collected time series data into easy-to-compute statistical features. We then identify the features that are required to detect anomalies, and extract the signatures of these anomalies. At runtime, we use these signatures to diagnose anomalies with negligible overhead. We evaluate our framework using experiments on a real-world HPC supercomputer and demonstrate that our approach successfully identifies 98 percent of injected anomalies and consistently outperforms existing anomaly diagnosis techniques.",project-academic
10.1007/S10549-016-3872-2,2016-06-23,a,Springer US,learning from social media utilizing advanced data extraction techniques to understand barriers to breast cancer treatment," Past examinations of breast cancer treatment barriers have typically included registry, claims-based, and smaller survey studies. We examined treatment barriers using a novel, comprehensive, social media analysis of online, candid discussions about breast cancer. Using an innovative toolset to search postings on social networks, message boards, patient communities, and topical sites, we performed a large-scale qualitative analysis. We examined the sentiments and barriers expressed about breast cancer treatments by Internet users during 1 year (2/1/14–1/31/15). We categorized posts based on thematic patterns and examined trends in discussions by race/ethnicity (white/black/Hispanic) when this information was available. We identified 1,024,041 unique posts related to breast cancer treatment. Overall, 57 % of posts expressed negative sentiments. Using machine learning software, we assigned treatment barriers for 387,238 posts (38 %). Barriers included emotional (23 % of posts), preferences and spiritual/religious beliefs (21 %), physical (18 %), resource (15 %), healthcare perceptions (9 %), treatment processes/duration (7 %), and relationships (7 %). Black and Hispanic (vs. white) users more frequently reported barriers related to healthcare perceptions, beliefs, and pre-diagnosis/diagnosis organizational challenges and fewer emotional barriers. Using a novel analysis of diverse social media users, we observed numerous breast cancer treatment barriers that differed by race/ethnicity. Social media is a powerful tool, allowing use of real-world data for qualitative research, capitalizing on the rich discussions occurring spontaneously online. Future research should focus on how to further employ and learn from this type of social intelligence research across all medical disciplines.",project-academic
,2020-08-29,a,,vr caps a virtual environment for capsule endoscopy," Current capsule endoscopes and next-generation robotic capsules for diagnosis and treatment of gastrointestinal diseases are complex cyber-physical platforms that must orchestrate complex software and hardware functions. The desired tasks for these systems include visual localization, depth estimation, 3D mapping, disease detection and segmentation, automated navigation, active control, path realization and optional therapeutic modules such as targeted drug delivery and biopsy sampling. Data-driven algorithms promise to enable many advanced functionalities for capsule endoscopes, but real-world data is challenging to obtain. Physically-realistic simulations providing synthetic data have emerged as a solution to the development of data-driven algorithms. In this work, we present a comprehensive simulation platform for capsule endoscopy operations and introduce VR-Caps, a virtual active capsule environment that simulates a range of normal and abnormal tissue conditions (e.g., inflated, dry, wet etc.) and varied organ types, capsule endoscope designs (e.g., mono, stereo, dual and 360°camera), and the type, number, strength, and placement of internal and external magnetic sources that enable active locomotion. VR-Caps makes it possible to both independently or jointly develop, optimize, and test medical imaging and analysis software for the current and next-generation endoscopic capsule systems. To validate this approach, we train state-of-the-art deep neural networks to accomplish various medical image analysis tasks using simulated data from VR-Caps and evaluate the performance of these models on real medical data. Results demonstrate the usefulness and effectiveness of the proposed virtual platform in developing algorithms that quantify fractional coverage, camera trajectory, 3D map reconstruction, and disease classification.",project-academic
10.1016/J.MEDIA.2021.101990,2021-02-06,a,Elsevier,vr caps a virtual environment for capsule endoscopy," Current capsule endoscopes and next-generation robotic capsules for diagnosis and treatment of gastrointestinal diseases are complex cyber-physical platforms that must orchestrate complex software and hardware functions. The desired tasks for these systems include visual localization, depth estimation, 3D mapping, disease detection and segmentation, automated navigation, active control, path realization and optional therapeutic modules such as targeted drug delivery and biopsy sampling. Data-driven algorithms promise to enable many advanced functionalities for capsule endoscopes, but real-world data is challenging to obtain. Physically-realistic simulations providing synthetic data have emerged as a solution to the development of data-driven algorithms. In this work, we present a comprehensive simulation platform for capsule endoscopy operations and introduce VR-Caps, a virtual active capsule environment that simulates a range of normal and abnormal tissue conditions (e.g., inflated, dry, wet etc.) and varied organ types, capsule endoscope designs (e.g., mono, stereo, dual and 360∘ camera), and the type, number, strength, and placement of internal and external magnetic sources that enable active locomotion. VR-Caps makes it possible to both independently or jointly develop, optimize, and test medical imaging and analysis software for the current and next-generation endoscopic capsule systems. To validate this approach, we train state-of-the-art deep neural networks to accomplish various medical image analysis tasks using simulated data from VR-Caps and evaluate the performance of these models on real medical data. Results demonstrate the usefulness and effectiveness of the proposed virtual platform in developing algorithms that quantify fractional coverage, camera trajectory, 3D map reconstruction, and disease classification. All of the code, pre-trained weights and created 3D organ models of the virtual environment with detailed instructions how to setup and use the environment are made publicly available at https://github.com/CapsuleEndoscope/VirtualCapsuleEndoscopy and a video demonstration can be seen in the supplementary videos (Video-I).",project-academic
10.1097/CM9.0000000000001002,2020-09-05,a,Ovid Technologies (Wolters Kluwer Health),diagnostic capacity of skin tumor artificial intelligence assisted decision making software in real world clinical settings," BACKGROUND: Youzhi artificial intelligence (AI) software is the AI-assisted decision-making system for diagnosing skin tumors. The high diagnostic accuracy of Youzhi AI software was previously validated in specific datasets. The objective of this study was to compare the performance of diagnostic capacity between Youzhi AI software and dermatologists in real-world clinical settings. METHODS: A total of 106 patients who underwent skin tumor resection in the Dermatology Department of China-Japan Friendship Hospital from July 2017 to June 2019 and were confirmed as skin tumors by pathological biopsy were selected. Dermoscopy and clinical images of 106 patients were diagnosed by Youzhi AI software and dermatologists at different dermoscopy diagnostic levels. The primary outcome was to compare the diagnostic accuracy of the Youzhi AI software with that of dermatologists and that measured in the laboratory using specific data sets. The secondary results included the sensitivity, specificity, positive predictive value, negative predictive value, F-measure, and Matthews correlation coefficient of Youzhi AI software in the real-world. RESULTS: The diagnostic accuracy of Youzhi AI software in real-world clinical settings was lower than that of the laboratory data (Pâ<â0.001). The output result of Youzhi AI software has good stability after several tests. Youzhi AI software diagnosed benign and malignant diseases by recognizing dermoscopic images and diagnosed disease types with higher diagnostic accuracy than by recognizing clinical images (Pâ=â0.008, Pâ=â0.016, respectively). Compared with dermatologists, Youzhi AI software was more accurate in the diagnosis of skin tumor types through the recognition of dermoscopic images (Pâ=â0.01). By evaluating the diagnostic performance of dermatologists under different modes, the diagnostic accuracy of dermatologists in diagnosing disease types by matching dermoscopic and clinical images was significantly higher than that by identifying dermoscopic and clinical images in random sequence (Pâ=â0.022). The diagnostic accuracy of dermatologists in the diagnosis of benign and malignant diseases by recognizing dermoscopic images was significantly higher than that by recognizing clinical images (Pâ=â0.010). CONCLUSION: The diagnostic accuracy of Youzhi AI software for skin tumors in real-world clinical settings was not as high as that of using special data sets in the laboratory. However, there was no significant difference between the diagnostic capacity of Youzhi AI software and the average diagnostic capacity of dermatologists. It can provide assistant diagnostic decisions for dermatologists in the current state.",project-academic
,2009-12-28,b,Springer-Verlag,intelligence and security informatics for international security information sharing and data mining," On September 11, 2001 the World became completely cognizant of the security challenges it faces on an international scale. With this awareness a commitment has come from the scientific, engineering, and health communities to help meet the world meet an array of security challenges. From these activities the science of ""Intelligence and Security Informatics,"" is emerging, which will influence a new generation of policy makers, practitioners, researchers, and students. INTELLIGENCE AND SECURITY INFORMATICS FOR INTERNATIONAL SECURITY: InformationSharing and Data Mining presents a systematic national security research framework, and within this context, the book discusses IT technical components, and directions. The book synthesizes the research in the field, focusing on information integration and data mining in particular. Integrated in the discussion are a number of real-world case studies illustrating how security technologies are developed and how they can be applied in critical law enforcement, emergency response, intelligence analysis, and terrorism contexts. The book reflects a decade of leading-edge research on intelligence and security informatics from the Artificial Intelligence Laboratory and the NSF COPLINK Center for Homeland Security Information Technology Research. Working in parallel with the research has been its application in real-world community situations by the centers director and the books author, Dr. Hsinchun Chen. The books audience is wide and includes the following: (1) college professors, research scientists, graduate students, and select undergraduate juniors and seniors in computer science, information management, information science, and other related public safety, intelligence analysis, and terrorism research disciplines; (2) researchers, analysts, and policy makers in federal departments, national laboratories, intelligence community, public safety and law enforcement agencies, and the emergency response community; and (3) consultants and practitioners in IT hardware, communication, and software companies, consulting firms, and defence contractors of varying sizes and countries.",project-academic
,2018-03-17,a,,the challenge of regulating clinical decision support software after 21st century cures," Clinical decision support (CDS) software broadly refers to software that assists healthcare providers in combining patient-specific data with general sources of medical knowledge to make better diagnostic and treatment decisions in the clinical setting. The 21st-Century Cures Act strips FDA of jurisdiction to regulate some (not all) CDS software. To qualify for this exclusion from FDA regulation, 21 U.S.C. § 360j(o)(1)(E)(iii) requires that the software must be intended to enable “the health care professional to independently review the basis” for its recommendations so that it is “not the intent that such health care professional rely primarily” on the software’s recommendations when making diagnostic and treatment decisions about individual patients. This article explores whether this is a workable standard as applied to advanced CDS software that uses machine learning to glean insights from real-world clinical experience and then applies these insights to improve the quality of patient care. We conclude that the standard Congress set out in 21st-Century Cures is potentially workable, but only if FDA takes additional steps to clarify the standards of transparency that CDS software must meet before it can escape FDA regulation. Transparency in this context includes algorithmic transparency, physician access to the underlying data that the software relies on in rendering decisions, and business transparency as reflected in the terms of contracts between CDS software vendors and users. 
FDA’s recent draft guidance on CDS software leaves a crucial question unresolved. This question cuts to the very heart of what is wrong with the US healthcare system and how to fix it: Is the problem simply that doctors are not heeding existing medical evidence — for example, by ignoring warnings in FDA-approved drug labeling or failing to keep up with findings in the peer-reviewed medical literature? Or is the problem that the existing evidence base is itself inadequate and flawed — for example, because FDA-approved labeling relies on contrived clinical trials that fail to reflect real patients, or because peer-reviewed literature is skewed by publication biases that favor studies in which the treatment worked, or because clinical practice guidelines can be captured by commercial interests? FDA’s draft guidance on CDS software — perhaps as an unintended consequence — would expedite market entry for simple CDS software that promotes physician conformity with the existing medical evidence base, while imposing higher regulatory hurdles that delay the clinical translation of machine-learning software that may be our best hope to overcome flaws in current medical evidence? Is this the right path forward?",project-academic
10.1177/0098858818789418,2018-08-14,a,Am J Law Med,the challenge of regulating clinical decision support software after 21st century cures," Clinical decision support (CDS) software broadly refers to software that assists healthcare providers in combining patient-specific data with general sources of medical knowledge to make better diagnostic and treatment decisions in the clinical setting. The 21st-Century Cures Act strips FDA of jurisdiction to regulate some (not all) CDS software. To qualify for this exclusion from FDA regulation, 21 U.S.C. § 360j(o)(1)(E)(iii) requires that the software must be intended to enable “the health care professional to independently review the basis” for its recommendations so that it is “not the intent that such health care professional rely primarily” on the software’s recommendations when making diagnostic and treatment decisions about individual patients. This article explores whether this is a workable standard as applied to advanced CDS software that uses machine learning to glean insights from real-world clinical experience and then applies these insights to improve the quality of patient care. We conclude that the standard Congress set out in 21st-Century Cures is potentially workable, but only if FDA takes additional steps to clarify the standards of transparency that CDS software must meet before it can escape FDA regulation. Transparency in this context includes algorithmic transparency, physician access to the underlying data that the software relies on in rendering decisions, and business transparency as reflected in the terms of contracts between CDS software vendors and users. 
FDA’s recent draft guidance on CDS software leaves a crucial question unresolved. This question cuts to the very heart of what is wrong with the US healthcare system and how to fix it: Is the problem simply that doctors are not heeding existing medical evidence — for example, by ignoring warnings in FDA-approved drug labeling or failing to keep up with findings in the peer-reviewed medical literature? Or is the problem that the existing evidence base is itself inadequate and flawed — for example, because FDA-approved labeling relies on contrived clinical trials that fail to reflect real patients, or because peer-reviewed literature is skewed by publication biases that favor studies in which the treatment worked, or because clinical practice guidelines can be captured by commercial interests? FDA’s draft guidance on CDS software — perhaps as an unintended consequence — would expedite market entry for simple CDS software that promotes physician conformity with the existing medical evidence base, while imposing higher regulatory hurdles that delay the clinical translation of machine-learning software that may be our best hope to overcome flaws in current medical evidence? Is this the right path forward?",project-academic
10.1109/ACCESS.2020.2970118,2020-01-28,a,IEEE,internet of things iot for next generation smart systems a review of current challenges future trends and prospects for emerging 5g iot scenarios," The Internet of Things (IoT)-centric concepts like augmented reality, high-resolution video streaming, self-driven cars, smart environment, e-health care, etc. have a ubiquitous presence now. These applications require higher data-rates, large bandwidth, increased capacity, low latency and high throughput. In light of these emerging concepts, IoT has revolutionized the world by providing seamless connectivity between heterogeneous networks (HetNets). The eventual aim of IoT is to introduce the plug and play technology providing the end-user, ease of operation, remotely access control and configurability. This paper presents the IoT technology from a bird’s eye view covering its statistical/architectural trends, use cases, challenges and future prospects. The paper also presents a detailed and extensive overview of the emerging 5G-IoT scenario. Fifth Generation (5G) cellular networks provide key enabling technologies for ubiquitous deployment of the IoT technology. These include carrier aggregation, multiple-input multiple-output (MIMO), massive-MIMO (M-MIMO), coordinated multipoint processing (CoMP), device-to-device (D2D) communications, centralized radio access network (CRAN), software-defined wireless sensor networking (SD-WSN), network function virtualization (NFV) and cognitive radios (CRs). This paper presents an exhaustive review for these key enabling technologies and also discusses the new emerging use cases of 5G-IoT driven by the advances in artificial intelligence, machine and deep learning, ongoing 5G initiatives, quality of service (QoS) requirements in 5G and its standardization issues. Finally, the paper discusses challenges in the implementation of 5G-IoT due to high data-rates requiring both cloud-based platforms and IoT devices based edge computing.",project-academic
10.1109/ACCESS.2019.2942390,2019-09-19,a,Institute of Electrical and Electronics Engineers (IEEE),machine learning for 5g b5g mobile and wireless communications potential limitations and future directions," Driven by the demand to accommodate today’s growing mobile traffic, 5G is designed to be a key enabler and a leading infrastructure provider in the information and communication technology industry by supporting a variety of forthcoming services with diverse requirements. Considering the ever-increasing complexity of the network, and the emergence of novel use cases such as autonomous cars, industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML) is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised, unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of ML in the context of mobile and wireless communication, organizing the literature in terms of the types of learning. We then discuss the promising approaches for how ML can contribute to supporting each target 5G network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G), providing future research directions for how ML can contribute to realizing B5G. This article is intended to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of autonomous 5G/B5G mobile and wireless communications.",project-academic
10.1007/S11606-014-3018-3,2015-02-01,a,Springer US,value added medical education engaging future doctors to transform health care delivery today," Medical student education typically aims to increase the supply of future physicians while ignoring current problems in health care delivery. This focus on the future fails to address the pressing challenges of today. These challenges include a shrinking ratio of adult primary care clinicians to population, a failure to address population health and the need for cost reductions, and medical training that prepares students to deal with patients, but not with teams and systems.1 Change is desperately needed to translate education into better health outcomes for all Americans today.

To achieve this change, significant reforms are needed in both practice redesign and medical education. One proposed solution to the capacity issue in primary care is to “share the care” with non-clinician health care team members and learners.1 Within this paradigm, an empowered team comprising of clinicians, non-clinicians (nurses, medical assistants, health educators), and learners share responsibilities so that all team members, operating at their maximum potential, contribute to the health of their patient panel.

We believe that sharing the care goes beyond addressing the capacity-demand problem and may serve as an ideal starting point for building a new vision for the future of medical education. What if every medical student starting in year one is embedded in a primary health care team and engaged in meaningful roles of providing care appropriate to their stage of training? What if, instead of just shadowing a physician in clinic, early medical students serve as health coaches, provide motivational interviewing to assist patients with behavior change, and participate in quality improvement projects? What if, even if they do not have the diagnostic or treatment skills to provide formal patient care yet, early medical students are trained to do population health management and can reach out to patients overdue for routine preventive (pap smears, colonoscopies, mammograms) and chronic care services (foot and eye exams for diabetics)? This model of medical training is not only aligned with many of the recommendations of the Carnegie Foundation report2—outpatient-focused, team-based, and patient-centered—but can add valuable capacity to our health care delivery system without changing the current infrastructure of academic health centers.

We call this “value-added medical education,” where powerful experiential learning experiences can also add value and capacity to our health care delivery system. This can be achieved by training and involving medical students in targeted patient care tasks. Students are eager to engage in care and take on responsibilities as part of the health care team in ways that do not generate duplicative work or consume additional energy from the clinical faculty. The underlying principles of value-added medical education are entirely compatible with the Institute of Medicine’s framework for a Learning Health System, and has been championed by leaders like Thomas Bodenheimer,1 Kevin Grumbach,3 and others who have already piloted a number of innovative programs that combine education and care in creative and synergistic ways.

At the University of California, San Francisco, where much of the pioneering work on value-added medical education has been done, students and faculty members are working together to reboot their curriculum by creating authentic workplace learning experiences that leverage the talents and commitments of every student to add value to the care of patients today. Other schools are also testing new methods of providing real world opportunities for trainees to participate in improving the quality of the health systems in which they work and study. The Health Professions Education Collaborative, sponsored by the Institute for Healthcare Improvement, helps drive curricular changes that promote team-based continuous quality improvement initiatives at 16 academic health centers.

Stanford University School of Medicine is also embedding some first year medical students in community health centers where they provide care through health coaching, motivational interviewing, and patient education delivered via longitudinal primary care community partnerships. Early medical students can participate in a patient navigator program to guide patients hospitalized with congestive heart failure through the discharge process, make follow-up telephone calls after they leave the hospital, and provide a supervised home visit to ensure patient safety and reduce preventable readmissions. In our free clinics, students are serving as referral coordinators and insurance counselors to educate uninsured patients about the Affordable Care Act and place them in medical homes as part of a bridge-to-care initiative. A scribe program designed to turn early trainees into clinical documentation experts has the potential to change our primary care clinics into high-performing practices, while equipping students with the technological skills needed to practice medicine in the electronic medical record era. All these programs are similar in that they balance the often conflicting missions of education and patient care by merging these two objectives into a single pursuit.

We are not the first to advocate for engaging medical students in systems improvement as an essential part of education reform. We echo the call of leaders like Catherine Lucey,4 who has written for all medical schools to “explicitly commit to implementing educational programs that measurably improve health care today while educating the physicians of tomorrow.” We add to her charge by arguing that value-added educational redesign is not only desirable to produce 21st century physicians able to achieve better health outcomes for the American people, but necessary as an immediate solution to address the growing imbalance between population demands for medical care and our capacity to provide care. We need solutions that can work right now, not decades into the future. There are more than 80,000 students in U.S. medical schools today.5 Harnessing their collective engagement and untapped capacity for patient care would be transformative.

Transforming this dream into reality will take leadership, redeployment of resources, and curricular redesign. Leadership is needed to articulate the vision, generate urgency for change, shift resources, provide faculty development, and build partnerships with new clinical training sites. This will require some redirection of existing resources currently devoted to early clinical training, including patient communication and physical exam courses. Value-added education programs have the potential to fuel clinical partners’ interest in having early medical students at their sites, since they are actively engaged in patient care tasks that contribute to the health of their patient populations. These partners may need to adjust workflow and space to accommodate their new medical student team members. At the same time, early clinical training courses will need to prepare their students with the knowledge and skills to take on meaningful responsibilities as part of the health care team, including motivational interviewing, health coaching, and population health management. Medical school course directors, clinical faculty, and community preceptors will need to communicate in order to enable integration between course knowledge and real world skills. Faculty and staff development will be needed in order to reimagine the clinical experience for students and provide the clinic staff with skills to actively involve students in their daily work. Curricular innovation and redesign will be essential to offer opportunity space for these new experiences and skills. However, none of these factors is insurmountable, especially in light of the social good derived from student learning and engagement in improving health care delivery.

A vision for a new kind of medical education is taking shape. We believe that the principles of value-added medical education should and will play an important role in this revolution. These principles include: (1) early integrated workplace learning for all medical students, (2) an interprofessional team-based quality improvement culture to promote understanding and respect of non-clinician providers, (3) collaborative and data driven population health management, (4) optimization of professional roles that are learner-centered and continuously adjusted to changing stages of development, and (5) the fusion of robust experiential learning experiences with the delivery of high-performing, patient-centered primary care. The challenge now is to make value-added training a standard part of the curriculum in every medical school. We can do this by taking lessons learned from successful pilots and implementing them widely with the support of professional organizations and accrediting bodies. It is time to share the care with our future colleagues.",project-academic
,2016-02-12,p,AAAI Press,deploying nemesis preventing foodborne illness by data mining social media," Foodborne illness afflicts 48 million people annually in the U.S. alone. Over 128,000 are hospitalized and 3,000 die from the infection. While preventable with proper food safety practices, the traditional restaurant inspection process has limited impact given the predictability and low frequency of inspections, and the dynamic nature of the kitchen environment. Despite this reality, the inspection process has remained largely unchanged for decades. We apply machine learning to Twitter data and develop a system that automatically detects venues likely to pose a public health hazard. Health professionals subsequently inspect individual flagged venues in a double blind experiment spanning the entire Las Vegas metropolitan area over three months. By contrast, previous research in this domain has been limited to indirect correlative validation using only aggregate statistics. We show that adaptive inspection process is 63% more effective at identifying problematic venues than the current state of the art. The live deployment shows that if every inspection in Las Vegas became adaptive, we can prevent over 9,000 cases of foodborne illness and 557 hospitalizations annually. Additionally, adaptive inspections result in unexpected benefits, including the identification of venues lacking permits, contagious kitchen staff, and fewer customer complaints filed with the Las Vegas health department.",project-academic
10.1609/AIMAG.V38I1.2711,2017-03-31,a,,deploying nemesis preventing foodborne illness by data mining social media," Foodborne illness afflicts 48 million people annually in the U.S. alone. Over 128,000 are hospitalized and 3,000 die from the infection. While preventable with proper food safety practices, the traditional restaurant inspection process has limited impact given the predictability and low frequency of inspections, and the dynamic nature of the kitchen environment. Despite this reality, the inspection process has remained largely unchanged for decades. CDC has even identified food safety as one of seven ”winnable battles”; however, progress to date has been limited. In this work, we demonstrate significant improvements in food safety by marrying AI and the standard inspection process. We apply machine learning to Twitter data, develop a system that automatically detects venues likely to pose a public health hazard, and demonstrate its efficacy in the Las Vegas metropolitan area in a double-blind experiment conducted over three months in collaboration with Nevada’s health department. By contrast, previous research in this domain has been limited to indirect correlative validation using only aggregate statistics. We show that adaptive inspection process is 64 percent more effective at identifying problematic venues than the current state of the art. If fully deployed, our approach could prevent over 9,000 cases of foodborne illness and 557 hospitalizations annually in Las Vegas alone. Additionally, adaptive inspections result in unexpected benefits, including the identification of venues lacking permits, contagious kitchen staff, and fewer customer complaints filed with the Las Vegas health department.",project-academic
10.2196/19867,2020-07-28,a,"JMIR Publications Inc., Toronto, Canada",pediatric mental and behavioral health in the period of quarantine and social distancing with covid 19," The coronavirus disease (COVID-19) pandemic has spread rapidly throughout the world and has had a long-term impact. The pandemic has caused great harm to society and caused serious psychological trauma to many people. Children are a vulnerable group in this global public health emergency, as their nervous systems, endocrine systems, and hypothalamic-pituitary-adrenal axes are not well developed. Psychological crises often cause children to produce feelings of abandonment, despair, incapacity, and exhaustion, and even raise the risk of suicide. Children with mental illnesses are especially vulnerable during the quarantine and social distancing period. The inclusion of psychosocial support for children and their families are part of the health responses to disaster and disaster recovery. Based on the biopsychosocial model, some children may have catastrophic thoughts and be prone to experience despair, numbness, flashbacks, and other serious emotional and behavioral reactions. In severe cases, there may be symptoms of psychosis or posttraumatic stress disorder. Timely and appropriate protections are needed to prevent the occurrence of psychological and behavioral problems. The emerging digital applications and health services such as telehealth, social media, mobile health, and remote interactive online education are able to bridge the social distance and support mental and behavioral health for children. Based on the psychological development characteristics of children, this study also illustrates interventions on the psychological impact from the COVID-19 pandemic. Even though the world has been struggling to curb the influences of the pandemic, the quarantine and social distancing policies will have long-term impacts on children. Innovative digital solutions and informatics tools are needed more than ever to mitigate the negative consequences on children. Health care delivery and services should envision and implement innovative paradigms to meet broad well-being needs and child health as the quarantine and social distancing over a longer term becomes a new reality. Future research on children's mental and behavioral health should pay more attention to novel solutions that incorporate cutting edge interactive technologies and digital approaches, leveraging considerable advances in pervasive and ubiquitous computing, human-computer interaction, and health informatics among many others. Digital approaches, health technologies, and informatics are supposed to be designed and implemented to support public health surveillance and critical responses to children's growth and development. For instance, human-computer interactions, augmented reality, and virtual reality could be incorporated to remote psychological supporting service for children's health; mobile technologies could be used to monitor children's mental and behavioral health while protecting their individual privacy; big data and artificial intelligence could be used to support decision making on whether children should go out for physical activities and whether schools should be reopened. Implications to clinical practices, psychological therapeutic practices, and future research directions to address current effort gaps are highlighted in this study.",project-academic
10.1186/S12942-018-0144-X,2018-07-05,a,BioMed Central,geospatial blockchain promises challenges and scenarios in health and healthcare," A PubMed query run in June 2018 using the keyword ‘blockchain’ retrieved 40 indexed papers, a reflection of the growing interest in blockchain among the medical and healthcare research and practice communities. Blockchain’s foundations of decentralisation, cryptographic security and immutability make it a strong contender in reshaping the healthcare landscape worldwide. Blockchain solutions are currently being explored for: (1) securing patient and provider identities; (2) managing pharmaceutical and medical device supply chains; (3) clinical research and data monetisation; (4) medical fraud detection; (5) public health surveillance; (6) enabling truly public and open geo-tagged data; (7) powering many Internet of Things-connected autonomous devices, wearables, drones and vehicles, via the distributed peer-to-peer apps they run, to deliver the full vision of smart healthy cities and regions; and (8) blockchain-enabled augmented reality in crisis mapping and recovery scenarios, including mechanisms for validating, crediting and rewarding crowdsourced geo-tagged data, among other emerging use cases. Geospatially-enabled blockchain solutions exist today that use a crypto-spatial coordinate system to add an immutable spatial context that regular blockchains lack. These geospatial blockchains do not just record an entry’s specific time, but also require and validate its associated proof of location, allowing accurate spatiotemporal mapping of physical world events. Blockchain and distributed ledger technology face similar challenges as any other technology threatening to disintermediate legacy processes and commercial interests, namely the challenges of blockchain interoperability, security and privacy, as well as the need to find suitable and sustainable business models of implementation. Nevertheless, we expect blockchain technologies to get increasingly powerful and robust, as they become coupled with artificial intelligence (AI) in various real-word healthcare solutions involving AI-mediated data exchange on blockchains.",project-academic
10.1093/OXFORDHB/9780199942237.001.0001,2014-12-02,b,Oxford University Press,the oxford handbook of affective computing," Affective Computing is a growing multidisciplinary field encompassing computer science, engineering, psychology, education, neuroscience, and many other disciplines. It explores how affective factors influence interactions between humans and technology, how affect sensing and affect generation techniques can inform our understanding of human affect, and on the design, implementation, and evaluation of systems that intricately involve affect at their core. The Oxford Handbook of Affective Computing will help both new and experienced researchers identify trends, concepts, methodologies, and applications in this burgeouning field. The volume features 41 chapters divided into five main sections: history and theory, detection, generation, methodologies, and applications. Section One begins with a look at the makings of AC and a historical review of the science of emotion. Chapters discuss the theoretical underpinnings of AC from an interdisciplinary perspective involving the affective, cognitive, social, media, and brain sciences. Section Two focuses on affect detection or affect recognition, which is one of the most commonly investigated areas in AC. Section Three examines aspects of affect generation including the synthesis of emotion and its expression via facial features, speech, postures and gestures. Cultural issues in affect generation are also discussed. Section Four features chapters on methodological issues in AC research, including data collection techniques, multimodal affect databases, emotion representation formats, crowdsourcing techniques, machine learning approaches, affect elicitation techniques, useful AC tools, and ethical issues in AC. Finally, Section Five highlights existing and future applications of AC in domains such as formal and informal learning, games, robotics, virtual reality, autism research, healthcare, cyberpsychology, music, deception, reflective writing, and cyberpsychology.With chapters authored by world leaders in each area, The Oxford Handbook of Affective Computing is suitable for use as a textbook in undergraduate or graduate courses in AC, and will serve as a valuable resource for students, researchers, and practitioners across the globe.",project-academic
10.1109/PHM-PARIS.2019.00054,2019-05-02,p,IEEE,domain adaptive transfer learning for fault diagnosis," Thanks to digitization of industrial assets in fleets, the ambitious goal of transferring fault diagnosis models from one machine to the other has raised great interest. Solving these domain adaptive transfer learning tasks has the potential to save large efforts on manually labeling data and modifying models for new machines in the same fleet. Although datadriven methods have shown great potential in fault diagnosis applications, their ability to generalize on new machines and new working conditions are limited because of their tendency to overfit to the training set in reality. One promising solution to this problem is to use domain adaptation techniques. It aims to improve model performance on the target new machine. Inspired by its successful implementation in computer vision, we introduced Domain-Adversarial Neural Networks (DANN) to our context, along with two other popular methods existing in previous fault diagnosis research. We then carefully justify the applicability of these methods in realistic fault diagnosis settings, and offer a unified experimental protocol for a fair comparison between domain adaptation methods for fault diagnosis problems.",project-academic
,2019-05-15,a,,domain adaptive transfer learning for fault diagnosis," Thanks to digitization of industrial assets in fleets, the ambitious goal of transferring fault diagnosis models fromone machine to the other has raised great interest. Solving these domain adaptive transfer learning tasks has the potential to save large efforts on manually labeling data and modifying models for new machines in the same fleet. Although data-driven methods have shown great potential in fault diagnosis applications, their ability to generalize on new machines and new working conditions are limited because of their tendency to overfit to the training set in reality. One promising solution to this problem is to use domain adaptation techniques. It aims to improve model performance on the target new machine. Inspired by its successful implementation in computer vision, we introduced Domain-Adversarial Neural Networks (DANN) to our context, along with two other popular methods existing in previous fault diagnosis research. We then carefully justify the applicability of these methods in realistic fault diagnosis settings, and offer a unified experimental protocol for a fair comparison between domain adaptation methods for fault diagnosis problems.",project-academic
10.1259/BJR.20190855,2020-01-22,a,Br J Radiol,artificial intelligence reshaping the practice of radiological sciences in the 21st century," Advances in computing hardware and software platforms have led to the recent resurgence in artificial intelligence (AI) touching almost every aspect of our daily lives by its capability for automating complex tasks or providing superior predictive analytics. AI applications are currently spanning many diverse fields from economics to entertainment, to manufacturing, as well as medicine. Since modern AI's inception decades ago, practitioners in radiological sciences have been pioneering its development and implementation in medicine, particularly in areas related to diagnostic imaging and therapy. In this anniversary article, we embark on a journey to reflect on the learned lessons from past AI's chequered history. We further summarize the current status of AI in radiological sciences, highlighting, with examples, its impressive achievements and effect on re-shaping the practice of medical imaging and radiotherapy in the areas of computer-aided detection, diagnosis, prognosis, and decision support. Moving beyond the commercial hype of AI into reality, we discuss the current challenges to overcome, for AI to achieve its promised hope of providing better precision healthcare for each patient while reducing cost burden on their families and the society at large.",project-academic
10.1007/S11904-020-00490-6,2020-06-01,a,Springer US,artificial intelligence and machine learning for hiv prevention emerging approaches to ending the epidemic," We review applications of artificial intelligence (AI), including machine learning (ML), in the field of HIV prevention. ML approaches have been used to identify potential candidates for preexposure prophylaxis (PrEP) in healthcare settings in the USA and Denmark and in a population-based research setting in Eastern Africa. Although still in the proof-of-concept stage, other applications include ML with smartphone-collected and social media data to promote real-time HIV risk reduction, virtual reality tools to facilitate HIV serodisclosure, and chatbots for HIV education. ML has also been used for causal inference in HIV prevention studies. ML has strong potential to improve delivery of PrEP, with this approach moving from development to implementation. Development and evaluation of AI and ML strategies for HIV prevention may benefit from an implementation science approach, including qualitative assessments with end users, and should be developed and evaluated with attention to equity.",project-academic
10.1016/J.JACR.2019.05.047,2019-09-01,a,J Am Coll Radiol,strengths weaknesses opportunities and threats analysis of artificial intelligence and machine learning applications in radiology," Currently, the use of artificial intelligence (AI) in radiology, particularly machine learning (ML), has become a reality in clinical practice. Since the end of the last century, several ML algorithms have been introduced for a wide range of common imaging tasks, not only for diagnostic purposes but also for image acquisition and postprocessing. AI is now recognized to be a driving initiative in every aspect of radiology. There is growing evidence of the advantages of AI in radiology creating seamless imaging workflows for radiologists or even replacing radiologists. Most of the current AI methods have some internal and external disadvantages that are impeding their ultimate implementation in the clinical arena. As such, AI can be considered a portion of a business trying to be introduced in the health care market. For this reason, this review analyzes the current status of AI, and specifically ML, applied to radiology from the scope of strengths, weaknesses, opportunities, and threats (SWOT) analysis.",project-academic
10.4301/S1807-1775201916001,2019-01-17,a,TECSI Laboratório de Tecnologia e Sistemas de Informação - FEA/USP,the future digital work force robotic process automation rpa," The Robotic Process Automation (RPA) is a new wave of future technologies. Robotic Process Automation is one of the most advanced technologies in the area of computers science, electronic and communications, mechanical engineering, and information technology. It is a combination of both hardware and software, networking and automation for doing things very simple. In this light, the research manuscript investigated the secondary data - which is available on google, academic and research databases. The investigation went for totally 6 months, i.e., 1-1-2018 to 30-6-2018. A very few empirical articles, white papers, blogs and were found RPA and came across to compose this research manuscript. This study is exploratory in nature because of the contemporary phenomenon. The keywords used in searching of the database were Robotic Process Automation, RPA, Robots, Artificial Intelligence, Blue Prism. The study finally discovered that Robots and Robotic Process Automation technologies are becoming compulsory as a part to do business operations in organizations across the globe. Robotic Process Automation can bring immediate value to the core business processes including employee payroll, employee status changes, new hire recruitment, and onboarding, accounts receivable and payable, invoice processing, inventory management, report creation, software installations, data migration, and vendor onboarding etc. to name a few applications. Besides, the Robotic Process Automation has abundant applications including healthcare and pharmaceuticals, financial services, outsourcing, retail, telecom, energy and utilities, real estate and FMCG and many more sectors. To put in the right place of RPA in business operations, their many allied technologies are working at the background level, artificial intelligence, machine learning, deep learning, data analytics, HR analytics, virtual reality (second life), home automation, blockchain technologies, 4D printing etc. Moreover, it covers the content of different start-ups companies and existing companies - their RPA applications used across the world. This manuscript will be a good guideline for the academicians, researchers, students, and practitioners to get an overall idea.",project-academic
10.3233/978-1-60750-050-6-93,2009-01-01,a,Stud Health Technol Inform,using reality mining to improve public health and medicine," We live our lives in digital networks. We wake up in the morning, check our e-mail, make a quick phone call, commute to work, buy lunch. Many of these transactions leave digital breadcrumbs--tiny records of our daily experiences. Reality mining, which pulls together these crumbs using statistical analysis and machine learning methods, offers an increasingly comprehensive picture of our lives, both individually and collectively, with the potential of transforming our understanding of ourselves, our organizations, and our society in a fashion that was barely conceivable just a few years ago. It is for this reason that reality mining was recently identified by Technology Review as one of ""10 emerging technologies that could change the world"". Many everyday devices provide the raw database upon which reality mining builds; sensors in mobile phones, cars, security cameras, RFID ('smart card') readers, and others, all allow for the measurement of human physical and social activity. Computational models based on such data have the potential to dramatically transform the arenas of both individual and community health. Reality mining can provide new opportunities with respect to diagnosis, patient and treatment monitoring, health services planning, surveillance of disease and risk factors, and public health investigation and disease control. Currently, the single most important source of reality mining data is the ubiquitous mobile phone. Every time a person uses a mobile phone, a few bits of information are left behind. The phone pings the nearest mobile-phone towers, revealing its location. The mobile phone service provider records the duration of the call and the number dialed. In the near future, mobile phones and other technologies will collect even more information about their users, recording everything from their physical activity to their conversational cadences. While such data pose a potential threat to individual privacy, they also offer great potential value both to individuals and communities. With the aid of data-mining algorithms, these data could shed light on individual patterns of behavior and even on the well-being of communities, creating new ways to improve public health and medicine. To illustrate, consider two examples of how reality mining may benefit individual health care. By taking advantage of special sensors in mobile phones, such as the microphone or the accelerometers built into newer devices such as Apple's iPhone, important diagnostic data can be captured. Clinical pilot data demonstrate that it may be possible to diagnose depression from the way a person talks--a depressed person tends to speak more slowly, a change that speech analysis software on a phone might recognize more readily than friends or family do. Similarly, monitoring a phone's motion sensors can also reveal small changes in gait, which could be an early indicator of ailments such as Parkinson's disease. Within the next few years reality mining will become more common, thanks in part to the proliferation and increasing sophistication of mobile phones. Many handheld devices now have the processing power of low-end desktop computers, and they can also collect more varied data, due to components such as GPS chips that track location. The Chief Technology Officer of EMC, a large digital storage company, estimates that this sort of personal sensor data will balloon from 10% of all stored information to 90% within the next decade. While the promise of reality mining is great, the idea of collecting so much personal information naturally raises many questions about privacy. It is crucial that behavior-logging technology not be forced on anyone. But legal statutes are lagging behind data collection capabilities, making it particularly important to begin discussing how the technology will and should be used. Therefore, an additional focus of this chapter will be the development of a legal and ethical framework concerning the data used by reality mining techniques.",project-academic
10.1101/2021.06.24.21259307,2021-06-27,a,Cold Spring Harbor Laboratory Press,technology in palliative care tip the identification of digital priorities for palliative care research using a modified delphi method," Background
Developments in digital health (describing technologies which use computing platforms, connectivity, software, and sensors for health care and related purposes) has the potential to transform the delivery of health and social care to help citizens manage their own health. Currently, we lack consensus about digital health research priorities in palliative care and lack theories about how these technologies might improve care outcomes. Global palliative care need is expected to increase due to the consequences of an ageing population; therefore, it is important for healthcare leaders to identify innovations to ensure that an increasingly frail population have appropriate access to palliative care services. Consequently, it is important to articulate research priorities as the first step to determine how we should allocate finite research resources to a field saturated with rapidly developing innovations. 

Aims
To identify research priority areas for digital health in palliative care.

Methods
We selected the digital health trends, most relevant to palliative care, from a list of emerging trends reported by the Future Today Institute. We conducted a modified Delphi process and consensus meeting with palliative care experts to identify research priorities. We used the views of public representatives to gain their perspectives of the agreed priorities.

Results
One hundred and three experts (representing 11 countries) participated in the 1st Delphi round. Fifty-five participated in the 2nd round (53% of 1st round). Eleven experts attended the final consensus meeting. We identified 16 priorities areas, which were summarised into eight themes. These themes were: big data, mobile devices, telehealth and telemedicine, virtual reality, artificial intelligence, the smart home, biotechnology and digital legacy.

Conclusions
The identified priorities in this paper represent a wide range of important emerging areas in field of digital health, personalised medicine, and data science. Human-centred design and robust governance systems should be considered in future research. It is important that the barriers and risks of using these technologies in palliative care are properly addressed to ensure that these tools are used meaningfully, wisely and safely and that do not cause unintentional harm.",project-academic
10.1016/J.GAITPOST.2018.11.029,2019-02-01,a,Elsevier BV,three dimensional cameras and skeleton pose tracking for physical function assessment a review of uses validity current developments and kinect alternatives," Abstract None None Background None Three-dimensional camera systems that integrate depth assessment with traditional two-dimensional images, such as the Microsoft Kinect, Intel Realsense, StereoLabs Zed and Orbecc, hold great promise as physical function assessment tools. When combined with point cloud and skeleton pose tracking software they can be used to assess many different aspects of physical function and anatomy. These assessments have received great interest over the past decade, and will likely receive further study as the integration of depth sensing and augmented reality smartphone cameras occurs more in everyday life. None None None Research Question None The aim of this review is to discuss how these devices work, what options are available, the best methods for performing assessments and how they can be used in the future. None None None Methods None Firstly, a review of the Microsoft Kinect devices and associated artificial intelligence, automated skeleton tracking algorithms is provided. This includes a narrative critique of the validity and clinical utility of these devices for assessing different aspects of physical function including spatiotemporal, kinematic and inverse dynamics data derived from gait and balance trials, and anatomical assessments performed using the depth sensor information. Methods for improving the accuracy of data are examined, including multiple-camera systems and sensor fusion with inertial monitoring units, model fitting, and marker tracking. Secondly, alternative hardware, including other structured light and time of flight methods, stereoscopic cameras and augmented reality leveraging smartphone and tablet cameras to perform measurements in three-dimensional space are summarised. Software options related to depth sensing cameras are then discussed, focussing on recent advances such as OpenPose and web-based methods such as PoseNet. None None None Results and Significance None The clinical and non-laboratory utility of these devices holds great promise for physical function assessment, and recent developments could strengthen their ability to provide important and impactful health-related data.",project-academic
10.1136/SVN-2017-000101,2017-12-01,a,BMJ Publishing Group,artificial intelligence in healthcare past present and future," Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.",project-academic
10.1016/J.ZEMEDI.2018.11.002,2019-05-01,a,Elsevier BV,an overview of deep learning in medical imaging focusing on mri," What has happened in machine learning lately, and what does it mean for the future of medical image analysis? Machine learning has witnessed a tremendous amount of attention over the last few years. The current boom started around 2009 when so-called deep artificial neural networks began outperforming other established models on a number of important benchmarks. Deep neural networks are now the state-of-the-art machine learning models across a variety of areas, from image analysis to natural language processing, and widely deployed in academia and industry. These developments have a huge potential for medical imaging technology, medical data analysis, medical diagnostics and healthcare in general, slowly being realized. We provide a short overview of recent advances and some associated challenges in machine learning applied to medical image processing and image analysis. As this has become a very broad and fast expanding field we will not survey the entire landscape of applications, but put particular focus on deep learning in MRI. Our aim is threefold: (i) give a brief introduction to deep learning with pointers to core references; (ii) indicate how deep learning has been applied to the entire MRI processing chain, from acquisition to image retrieval, from segmentation to disease prediction; (iii) provide a starting point for people interested in experimenting and perhaps contributing to the field of deep learning for medical imaging by pointing out good educational resources, state-of-the-art open-source code, and interesting sources of data and problems related medical imaging.",project-academic
10.1109/TMC.2018.2866249,2019-08-01,a,Institute of Electrical and Electronics Engineers (IEEE),classifying iot devices in smart environments using network traffic characteristics," The Internet of Things (IoT) is being hailed as the next wave revolutionizing our society, and smart homes, enterprises, and cities are increasingly being equipped with a plethora of IoT devices. Yet, operators of such smart environments may not even be fully aware of their IoT assets, let alone whether each IoT device is functioning properly safe from cyber-attacks. In this paper, we address this challenge by developing a robust framework for IoT device classification using traffic characteristics obtained at the network level. Our contributions are fourfold. First, we instrument a smart environment with 28 different IoT devices spanning cameras, lights, plugs, motion sensors, appliances, and health-monitors. We collect and synthesize traffic traces from this infrastructure for a period of six months, a subset of which we release as open data for the community to use. Second, we present insights into the underlying network traffic characteristics using statistical attributes such as activity cycles, port numbers, signalling patterns, and cipher suites. Third, we develop a multi-stage machine learning based classification algorithm and demonstrate its ability to identify specific IoT devices with over 99 percent accuracy based on their network activity. Finally, we discuss the trade-offs between cost, speed, and performance involved in deploying the classification framework in real-time. Our study paves the way for operators of smart environments to monitor their IoT assets for presence, functionality, and cyber-security without requiring any specialized devices or protocols.",project-academic
,2018-04-01,a,,manipulating machine learning poisoning attacks and countermeasures for regression learning," As machine learning becomes widely used for automated decisions, attackers have strong incentives to manipulate the results and models generated by machine learning algorithms. In this paper, we perform the first systematic study of poisoning attacks and their countermeasures for linear regression models. In poisoning attacks, attackers deliberately influence the training data to manipulate the results of a predictive model. We propose a theoretically-grounded optimization framework specifically designed for linear regression and demonstrate its effectiveness on a range of datasets and models. We also introduce a fast statistical attack that requires limited knowledge of the training process. Finally, we design a new principled defense method that is highly resilient against all poisoning attacks. We provide formal guarantees about its convergence and an upper bound on the effect of poisoning attacks when the defense is deployed. We evaluate extensively our attacks and defenses on three realistic datasets from health care, loan assessment, and real estate domains.",project-academic
10.1109/SP.2018.00057,2018-05-20,p,IEEE,manipulating machine learning poisoning attacks and countermeasures for regression learning," As machine learning becomes widely used for automated decisions, attackers have strong incentives to manipulate the results and models generated by machine learning algorithms. In this paper, we perform the first systematic study of poisoning attacks and their countermeasures for linear regression models. In poisoning attacks, attackers deliberately influence the training data to manipulate the results of a predictive model. We propose a theoretically-grounded optimization framework specifically designed for linear regression and demonstrate its effectiveness on a range of datasets and models. We also introduce a fast statistical attack that requires limited knowledge of the training process. Finally, we design a new principled defense method that is highly resilient against all poisoning attacks. We provide formal guarantees about its convergence and an upper bound on the effect of poisoning attacks when the defense is deployed. We evaluate extensively our attacks and defenses on three realistic datasets from health care, loan assessment, and real estate domains.",project-academic
10.1186/S12916-019-1426-2,2019-10-29,a,BioMed Central,key challenges for delivering clinical impact with artificial intelligence," Artificial intelligence (AI) research in healthcare is accelerating rapidly, with potential applications being demonstrated across various domains of medicine. However, there are currently limited examples of such techniques being successfully deployed into clinical practice. This article explores the main challenges and limitations of AI in healthcare, and considers the steps required to translate these potentially transformative technologies from research to clinical practice. Key challenges for the translation of AI systems in healthcare include those intrinsic to the science of machine learning, logistical difficulties in implementation, and consideration of the barriers to adoption as well as of the necessary sociocultural or pathway changes. Robust peer-reviewed clinical evaluation as part of randomised controlled trials should be viewed as the gold standard for evidence generation, but conducting these in practice may not always be appropriate or feasible. Performance metrics should aim to capture real clinical applicability and be understandable to intended users. Regulation that balances the pace of innovation with the potential for harm, alongside thoughtful post-market surveillance, is required to ensure that patients are not exposed to dangerous interventions nor deprived of access to beneficial innovations. Mechanisms to enable direct comparisons of AI systems must be developed, including the use of independent, local and representative test sets. Developers of AI algorithms must be vigilant to potential dangers, including dataset shift, accidental fitting of confounders, unintended discriminatory bias, the challenges of generalisation to new populations, and the unintended negative consequences of new algorithms on health outcomes. The safe and timely translation of AI research into clinically validated and appropriately regulated systems that can benefit everyone is challenging. Robust clinical evaluation, using metrics that are intuitive to clinicians and ideally go beyond measures of technical accuracy to include quality of care and patient outcomes, is essential. Further work is required (1) to identify themes of algorithmic bias and unfairness while developing mitigations to address these, (2) to reduce brittleness and improve generalisability, and (3) to develop methods for improved interpretability of machine learning predictions. If these goals can be achieved, the benefits for patients are likely to be transformational.",project-academic
10.1145/3132747.3132772,2017-10-14,p,ACM,resource central understanding and predicting workloads for improved resource management in large cloud platforms," Cloud research to date has lacked data on the characteristics of the production virtual machine (VM) workloads of large cloud providers. A thorough understanding of these characteristics can inform the providers' resource management systems, e.g. VM scheduler, power manager, server health manager. In this paper, we first introduce an extensive characterization of Microsoft Azure's VM workload, including distributions of the VMs' lifetime, deployment size, and resource consumption. We then show that certain VM behaviors are fairly consistent over multiple lifetimes, i.e. history is an accurate predictor of future behavior. Based on this observation, we next introduce Resource Central (RC), a system that collects VM telemetry, learns these behaviors offline, and provides predictions online to various resource managers via a general client-side library. As an example of RC's online use, we modify Azure's VM scheduler to leverage predictions in oversubscribing servers (with oversubscribable VM types), while retaining high VM performance. Using real VM traces, we then show that the prediction-informed schedules increase utilization and prevent physical resource exhaustion. We conclude that providers can exploit their workloads' characteristics and machine learning to improve resource management substantially.",project-academic
10.1145/3375627.3375830,2020-02-07,p,Association for Computing Machinery (ACM),fooling lime and shap adversarial attacks on post hoc explanation methods," As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.",project-academic
10.1016/J.FUTURE.2019.10.043,2020-03-01,a,North-Holland,healthfog an ensemble deep learning based smart healthcare system for automatic diagnosis of heart diseases in integrated iot and fog computing environments," Abstract None None Cloud computing provides resources over the Internet and allows a plethora of applications to be deployed to provide services for different industries. The major bottleneck being faced currently in these cloud frameworks is their limited scalability and hence inability to cater to the requirements of centralized Internet of Things (IoT) based compute environments. The main reason for this is that latency-sensitive applications like health monitoring and surveillance systems now require computation over large amounts of data (Big Data) transferred to centralized database and from database to cloud data centers which leads to drop in performance of such systems. The new paradigms of fog and edge computing provide innovative solutions by bringing resources closer to the user and provide low latency and energy efficient solutions for data processing compared to cloud domains. Still, the current fog models have many limitations and focus from a limited perspective on either accuracy of results or reduced response time but not both. We proposed a novel framework called HealthFog for integrating ensemble deep learning in Edge computing devices and deployed it for a real-life application of automatic Heart Disease analysis. HealthFog delivers healthcare as a fog service using IoT devices and efficiently manages the data of heart patients, which comes as user requests. Fog-enabled cloud framework, FogBus is used to deploy and test the performance of the proposed model in terms of power consumption, network bandwidth, latency, jitter, accuracy and execution time. HealthFog is configurable to various operation modes which provide the best Quality of Service or prediction accuracy, as required, in diverse fog computation scenarios and for different user requirements.",project-academic
10.1109/TKDE.2007.1042,2008-08-01,a,IEEE Computer Society,sensor based abnormal human activity detection," With the availability of affordable sensors and sensor networks, sensor-based human activity recognition has attracted much attention in artificial intelligence and ubiquitous computing. In this paper, we present a novel two-phase approach for detecting abnormal activities based on wireless sensors attached to a human body. Detecting abnormal activities is a particular important task in security monitoring and healthcare applications of sensor networks, among many others. Traditional approaches to this problem suffer from a high false positive rate, particularly when the collected sensor data are biased towards normal data while the abnormal events are rare. Therefore, there is a lack of training data for many traditional data mining methods to be applied. To solve this problem, our approach first employs a one-class support vector machine (SVM) that is trained on commonly available normal activities, which filters out the activities that have a very high probability of being normal. We then derive abnormal activity models from a general normal model via a kernel nonlinear regression (KNLR) to reduce false positive rate in an unsupervised manner. We show that our approach provides a good tradeoff between abnormality detection rate and false alarm rate, and allows abnormal activity models to be automatically derived without the need to explicitly label the abnormal training data, which are scarce. We demonstrate the effectiveness of our approach using real data collected from a sensor network that is deployed in a realistic setting.",project-academic
10.1136/BMJGH-2018-000798,2018-08-01,a,BMJ Specialist Journals,artificial intelligence ai and global health how can ai contribute to health in resource poor settings," The field of artificial intelligence (AI) has evolved considerably in the last 60 years. While there are now many AI applications that have been deployed in high-income country contexts, use in resource-poor settings remains relatively nascent. With a few notable exceptions, there are limited examples of AI being used in such settings. However, there are signs that this is changing. Several high-profile meetings have been convened in recent years to discuss the development and deployment of AI applications to reduce poverty and deliver a broad range of critical public services. We provide a general overview of AI and how it can be used to improve health outcomes in resource-poor settings. We also describe some of the current ethical debates around patient safety and privacy. Despite current challenges, AI holds tremendous promise for transforming the provision of healthcare services in resource-poor settings. Many health system hurdles in such settings could be overcome with the use of AI and other complementary emerging technologies. Further research and investments in the development of AI tools tailored to resource-poor settings will accelerate realising of the full potential of AI for improving global health.",project-academic
10.1109/TMI.2020.2973595,2020-02-12,a,IEEE,generalizing deep learning for medical image segmentation to unseen domains via deep stacked transformation," Recent advances in deep learning for medical image segmentation demonstrate expert-level accuracy. However, application of these models in clinically realistic environments can result in poor generalization and decreased accuracy, mainly due to the domain shift across different hospitals, scanner vendors, imaging protocols, and patient populations etc. Common transfer learning and domain adaptation techniques are proposed to address this bottleneck. However, these solutions require data (and annotations) from the target domain to retrain the model, and is therefore restrictive in practice for widespread model deployment. Ideally, we wish to have a trained (locked) model that can work uniformly well across unseen domains without further training. In this paper, we propose a deep stacked transformation approach for domain generalization. Specifically, a series of None None None ${n}$ None None None stacked transformations are applied to each image during network training. The underlying assumption is that the “expected” domain shift for a specific medical imaging modality could be simulated by applying extensive data augmentation on a single source domain, and consequently, a deep model trained on the augmented “big” data (BigAug) could generalize well on unseen domains. We exploit four surprisingly effective, but previously understudied, image-based characteristics for data augmentation to overcome the domain generalization problem. We train and evaluate the BigAug model (with None None None ${n}={9}$ None None None transformations) on three different 3D segmentation tasks (prostate gland, left atrial, left ventricle) covering two medical imaging modalities (MRI and ultrasound) involving eight publicly available challenge datasets. The results show that when training on relatively small dataset (n = 10~32 volumes, depending on the size of the available datasets) from a single source domain: (i) BigAug models degrade an average of 11%(Dice score change) from source to unseen domain, substantially better than conventional augmentation (degrading 39%) and CycleGAN-based domain adaptation method (degrading 25%), (ii) BigAug is better than “shallower” stacked transforms (i.e. those with fewer transforms) on unseen domains and demonstrates modest improvement to conventional augmentation on the source domain, (iii) after training with BigAug on one source domain, performance on an unseen domain is similar to training a model from scratch on that domain when using the same number of training samples. When training on large datasets (n = 465 volumes) with BigAug, (iv) application to unseen domains reaches the performance of state-of-the-art fully supervised models that are trained and tested on their source domains. These findings establish a strong benchmark for the study of domain generalization in medical imaging, and can be generalized to the design of highly robust deep segmentation models for clinical deployment.",project-academic
,2019-11-19,a,,the human body is a black box supporting clinical decision making with deep learning," Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to accuracy, fairness, accountability, and transparency that come from actual, situated use. Serious questions remain under examined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing on model interpretability to ensure a fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.",project-academic
10.1145/3351095.3372827,2020-01-27,p,ACM,the human body is a black box supporting clinical decision making with deep learning," Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.",project-academic
,2018-04-15,a,,adversarial attacks against medical deep learning systems," The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we demonstrate that adversarial examples are capable of manipulating deep learning systems across three clinical domains. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. Our models are representative of the current state of the art in medical computer vision and, in some cases, directly reflect architectures already seeing deployment in real world clinical settings. In addition to the technical contribution of our paper, we synthesize a large body of knowledge about the healthcare system to argue that medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.",project-academic
10.4108/ICST.PERVASIVEHEALTH2010.8851,2010-03-22,p,IEEE,improving activity classification for health applications on mobile devices using active and semi supervised learning," Mobile phones' increasing ubiquity has created many opportunities for personal context sensing. Personal activity is an important part of a user's context, and automatically recognizing it is vital for health and fitness monitoring applications. Recording a stream of activity data enables monitoring patients with chronic conditions affecting ambulation and motion, as well as those undergoing rehabilitation treatments. Modern mobile phones are powerful enough to perform activity classification in real time, but they typically use a static classifier that is trained in advance or require the user to manually add training data after the application is on his/her device. This paper investigates ways of automatically augmenting activity classifiers after they are deployed in an application. It compares active learning and three different semi-supervised learning methods, self-learning, En-Co-Training, and democratic co-learning, to determine which show promise for this purpose. The results show that active learning, En-Co-Training, and democratic co-learning perform well when the initial classifier's accuracy is low (75–80%). When the initial accuracy is already high (90%), these methods are no longer effective, but they do not hurt the accuracy either. Overall, active learning gave the highest improvement, but democratic co-learning was almost as good and does not require user interaction. Thus, democratic co-learning would be the best choice for most applications, since it would significantly increase the accuracy for initial classifiers that performed poorly.",project-academic
10.1016/S2213-2600(18)30300-X,2018-12-01,a,Elsevier,machine learning for real time prediction of complications in critical care a retrospective study," Summary None Background None The large amount of clinical signals in intensive care units can easily overwhelm health-care personnel and can lead to treatment delays, suboptimal care, or clinical errors. The aim of this study was to apply deep machine learning methods to predict severe complications during critical care in real time after cardiothoracic surgery. None Methods None We used deep learning methods (recurrent neural networks) to predict several severe complications (mortality, renal failure with a need for renal replacement therapy, and postoperative bleeding leading to operative revision) in post cardiosurgical care in real time. Adult patients who underwent major open heart surgery from Jan 1, 2000, to Dec 31, 2016, in a German tertiary care centre for cardiovascular diseases formed the main derivation dataset. We measured the accuracy and timeliness of the deep learning model's forecasts and compared predictive quality to that of established standard-of-care clinical reference tools (clinical rule for postoperative bleeding, Simplified Acute Physiology Score II for mortality, and the Kidney Disease: Improving Global Outcomes staging criteria for acute renal failure) using positive predictive value (PPV), negative predictive value, sensitivity, specificity, area under the curve (AUC), and the F1 measure (which computes a harmonic mean of sensitivity and PPV). Results were externally retrospectively validated with 5898 cases from the published MIMIC-III dataset. None Findings None Of 47 559 intensive care admissions (corresponding to 42 007 patients), we included 11 492 (corresponding to 9269 patients). The deep learning models yielded accurate predictions with the following PPV and sensitivity scores: PPV 0·90 and sensitivity 0·85 for mortality, 0·87 and 0·94 for renal failure, and 0·84 and 0·74 for bleeding. The predictions significantly outperformed the standard clinical reference tools, improving the absolute complication prediction AUC by 0·29 (95% CI 0·23–0·35) for bleeding, by 0·24 (0·19–0·29) for mortality, and by 0·24 (0·13–0·35) for renal failure (p None Interpretation None The observed improvements in prediction for all three investigated clinical outcomes have the potential to improve critical care. These findings are noteworthy in that they use routinely collected clinical data exclusively, without the need for any manual processing. The deep machine learning method showed AUC scores that significantly surpass those of clinical reference tools, especially soon after admission. Taken together, these properties are encouraging for prospective deployment in critical care settings to direct the staff's attention towards patients who are most at risk. None Funding None No specific funding.",project-academic
10.2196/26627,2021-04-05,a,JMIR Publications Inc.,artificial intelligence enabled analysis of public attitudes on facebook and twitter toward covid 19 vaccines in the united kingdom and the united states observational study," Background: Global efforts toward the development and deployment of a vaccine for COVID-19 are rapidly advancing. To achieve herd immunity, widespread administration of vaccines is required, which necessitates significant cooperation from the general public. As such, it is crucial that governments and public health agencies understand public sentiments toward vaccines, which can help guide educational campaigns and other targeted policy interventions.
Objective: The aim of this study was to develop and apply an artificial intelligence–based approach to analyze public sentiments on social media in the United Kingdom and the United States toward COVID-19 vaccines to better understand the public attitude and concerns regarding COVID-19 vaccines.
Methods: Over 300,000 social media posts related to COVID-19 vaccines were extracted, including 23,571 Facebook posts from the United Kingdom and 144,864 from the United States, along with 40,268 tweets from the United Kingdom and 98,385 from the United States from March 1 to November 22, 2020. We used natural language processing and deep learning–based techniques to predict average sentiments, sentiment trends, and topics of discussion. These factors were analyzed longitudinally and geospatially, and manual reading of randomly selected posts on points of interest helped identify underlying themes and validated insights from the analysis.
Results: Overall averaged positive, negative, and neutral sentiments were at 58%, 22%, and 17% in the United Kingdom, compared to 56%, 24%, and 18% in the United States, respectively. Public optimism over vaccine development, effectiveness, and trials as well as concerns over their safety, economic viability, and corporation control were identified. We compared our findings to those of nationwide surveys in both countries and found them to correlate broadly.
Conclusions: Artificial intelligence–enabled social media analysis should be considered for adoption by institutions and governments alongside surveys and other conventional methods of assessing public attitude. Such analyses could enable real-time assessment, at scale, of public confidence and trust in COVID-19 vaccines, help address the concerns of vaccine sceptics, and help develop more effective policies and communication strategies to maximize uptake.",project-academic
,2018-10-25,a,,wearable affective robot," With the development of the artificial intelligence (AI), the AI applications have influenced and changed people's daily life greatly. Here, a wearable affective robot that integrates the affective robot, social robot, brain wearable, and wearable 2.0 is proposed for the first time. The proposed wearable affective robot is intended for a wide population, and we believe that it can improve the human health on the spirit level, meeting the fashion requirements at the same time. In this paper, the architecture and design of an innovative wearable affective robot, which is dubbed as Fitbot, are introduced in terms of hardware and algorithm's perspectives. In addition, the important functional component of the robot-brain wearable device is introduced from the aspect of the hardware design, EEG data acquisition and analysis, user behavior perception, and algorithm deployment, etc. Then, the EEG based cognition of user's behavior is realized. Through the continuous acquisition of the in-depth, in-breadth data, the Fitbot we present can gradually enrich user's life modeling and enable the wearable robot to recognize user's intention and further understand the behavioral motivation behind the user's emotion. The learning algorithm for the life modeling embedded in Fitbot can achieve better user's experience of affective social interaction. Finally, the application service scenarios and some challenging issues of a wearable affective robot are discussed.",project-academic
10.1109/ACCESS.2018.2877919,2018-10-24,a,IEEE,wearable affective robot," With the development of the artificial intelligence (AI), the AI applications have influenced and changed people’s daily life greatly. Here, a wearable affective robot that integrates the affective robot, social robot, brain wearable, and Wearable 2.0 is proposed for the first time. The proposed wearable affective robot is intended for a wide population, and we believe that it can improve the human health on the spirit level, meeting the fashion requirements at the same time. In this paper, the architecture and design of an innovative wearable affective robot, which is dubbed as Fitbot, are introduced in terms of hardware and algorithm’s perspectives. In addition, the important functional component of the robot-brain wearable device is introduced from the aspect of the hardware design, EEG data acquisition and analysis, user behavior perception, and algorithm deployment. Then, the EEG-based cognition of user’s behavior is realized. Through the continuous acquisition of the in-depth, in-breadth data, the Fitbot we present can gradually enrich user’s life modeling and enable the wearable robot to recognize user’s intention and further understand the behavioral motivation behind the user’s emotion. The learning algorithm for the life modeling embedded in Fitbot can achieve better user’s experience of affective social interaction. Finally, the application service scenarios and some challenging issues of a wearable affective robot are discussed.",project-academic
,2020-09-18,a,PMLR,fast structured clinical documentation via contextual autocomplete," We present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. We dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. By constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. Furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. To our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.",project-academic
,2019-05-30,a,,a review of deep learning with special emphasis on architectures applications and recent trends," Deep learning has solved a problem that as little as five years ago was thought by many to be intractable - the automatic recognition of patterns in data; and it can do so with accuracy that often surpasses human beings. It has solved problems beyond the realm of traditional, hand-crafted machine learning algorithms and captured the imagination of practitioners trying to make sense out of the flood of data that now inundates our society. As public awareness of the efficacy of DL increases so does the desire to make use of it. But even for highly trained professionals it can be daunting to approach the rapidly increasing body of knowledge produced by experts in the field. Where does one start? How does one determine if a particular model is applicable to their problem? How does one train and deploy such a network? A primer on the subject can be a good place to start. With that in mind, we present an overview of some of the key multilayer ANNs that comprise DL. We also discuss some new automatic architecture optimization protocols that use multi-agent approaches. Further, since guaranteeing system uptime is becoming critical to many computer applications, we include a section on using neural networks for fault detection and subsequent mitigation. This is followed by an exploratory survey of several application areas where DL has emerged as a game-changing technology: anomalous behavior detection in financial applications or in financial time-series forecasting, predictive and prescriptive analytics, medical image processing and analysis and power systems research. The thrust of this review is to outline emerging areas of application-oriented research within the DL community as well as to provide a reference to researchers seeking to use it in their work for what it does best: statistical pattern recognition with unparalleled learning capacity with the ability to scale with information.",project-academic
10.1016/J.KNOSYS.2020.105596,2020-04-22,a,Elsevier,a review of deep learning with special emphasis on architectures applications and recent trends," Abstract None None Deep learning (DL) has solved a problem that a few years ago was thought to be intractable — the automatic recognition of patterns in spatial and temporal data with an accuracy superior to that of humans. It has solved problems beyond the realm of traditional, hand-crafted machine learning algorithms and captured the imagination of practitioners who are inundated with all types of data. As public awareness of the efficacy of DL increases so does the desire to make use of it. But even for highly trained professionals it can be daunting to approach the rapidly increasing body of knowledge in the field. Where does one start? How does one determine if a particular DL model is applicable to their problem? How does one train and deploy them? With these questions in mind, we present an overview of some of the key DL architectures. We also discuss some new automatic architecture optimization protocols that use multi-agent approaches. Further, since guaranteeing system uptime is critical to many applications, a section dwells on using DL for fault detection and mitigation. This is followed by an exploratory survey of several areas where DL emerged as a game-changer: fraud detection in financial applications, financial time-series forecasting, predictive and prescriptive analytics, medical image processing, power systems research and recommender systems. The thrust of this review is to outline emerging applications of DL and provide a reference to researchers seeking to use DL in their work for pattern recognition with unparalleled learning capacity and the ability to scale with data.",project-academic
,2017-09-01,,,self selection restaurant automatic charging method," The invention provides a self-selection restaurant automatic charging method based on computer vision technology. The system comprises a dinner plate sensing module, a dish division module and a dish identification module functionally. The dinner plate sensing module adopts an infrared sensing and static detection cascaded method, thereby ensuring high definition of pictures while realizing fast acquisition; the dish division module adopts an area generation network, thereby ensuring interference resistance for sheltering situations while realizing accurate positioning and segmentation; and the dish identification module adopts a depth twin measurement network, thereby overcoming dependence of a conventional deep learning method on the number of training samples. The system can directly identify dishes based on the images, so that when the system is applied to the restaurant, bowls and dishes do not need to be replaced, and the system is convenient to deploy and low in cost. The method can collect detailed and specific pictures and consumption information and the like, can provide sale analysis for the restaurant and diet health analysis for clients, and is an important link for realizing smart restaurant based on big data.",project-academic
10.2196/JMIR.9268,2018-01-30,a,JMIR Publications Inc.,prediction of incident hypertension within the next year prospective study using statewide electronic health records and machine learning," Background: As a high-prevalence health condition, hypertension is clinically costly, difficult to manage, and often leads to severe and life-threatening diseases such as cardiovascular disease (CVD) and stroke. Objective: The aim of this study was to develop and validate prospectively a risk prediction model of incident essential hypertension within the following year. Methods: Data from individual patient electronic health records (EHRs) were extracted from the Maine Health Information Exchange network. Retrospective (N=823,627, calendar year 2013) and prospective (N=680,810, calendar year 2014) cohorts were formed. A machine learning algorithm, XGBoost, was adopted in the process of feature selection and model building. It generated an ensemble of classification trees and assigned a final predictive risk score to each individual. Results: The 1-year incident hypertension risk model attained areas under the curve (AUCs) of 0.917 and 0.870 in the retrospective and prospective cohorts, respectively. Risk scores were calculated and stratified into five risk categories, with 4526 out of 381,544 patients (1.19%) in the lowest risk category (score 0-0.05) and 21,050 out of 41,329 patients (50.93%) in the highest risk category (score 0.4-1) receiving a diagnosis of incident hypertension in the following 1 year. Type 2 diabetes, lipid disorders, CVDs, mental illness, clinical utilization indicators, and socioeconomic determinants were recognized as driving or associated features of incident essential hypertension. The very high risk population mainly comprised elderly (age>50 years) individuals with multiple chronic conditions, especially those receiving medications for mental disorders. Disparities were also found in social determinants, including some community-level factors associated with higher risk and others that were protective against hypertension. Conclusions: With statewide EHR datasets, our study prospectively validated an accurate 1-year risk prediction model for incident essential hypertension. Our real-time predictive analytic model has been deployed in the state of Maine, providing implications in interventions for hypertension and related diseases and hopefully enhancing hypertension care. None [J Med Internet Res 2018;20(1):e22]",project-academic
10.1109/HPCA.2018.00018,2018-02-01,p,IEEE Computer Society,in situ ai towards autonomous and incremental deep learning for iot systems," Recent years have seen an exploration of data volumes from a myriad of IoT devices, such as various sensors and ubiquitous cameras. The deluge of IoT data creates enormous opportunities for us to explore the physical world, especially with the help of deep learning techniques. Traditionally, the Cloud is the option for deploying deep learning based applications. However, the challenges of Cloud-centric IoT systems are increasing due to significant data movement overhead, escalating energy needs, and privacy issues. Rather than constantly moving a tremendous amount of raw data to the Cloud, it would be beneficial to leverage the emerging powerful IoT devices to perform the inference task. Nevertheless, the statically trained model could not efficiently handle the dynamic data in the real in-situ environments, which leads to low accuracy. Moreover, the big raw IoT data challenges the traditional supervised training method in the Cloud. To tackle the above challenges, we propose In-situ AI, the first Autonomous and Incremental computing framework and architecture for deep learning based IoT applications. We equip deep learning based IoT system with autonomous IoT data diagnosis (minimize data movement), and incremental and unsupervised training method (tackle the big raw IoT data generated in ever-changing in-situ environments). To provide efficient architectural support for this new computing paradigm, we first characterize the two In-situ AI tasks (i.e. inference and diagnosis tasks) on two popular IoT devices (i.e. mobile GPU and FPGA) and explore the design space and tradeoffs. Based on the characterization results, we propose two working modes for the In-situ AI tasks, including Single-running and Co-running modes. Moreover, we craft analytical models for these two modes to guide the best configuration selection. We also develop a novel two-level weight shared In-situ AI architecture to efficiently deploy In-situ tasks to IoT node. Compared with traditional IoT systems, our In-situ AI can reduce data movement by 28-71%, which further yields 1.4X-3.3X speedup on model update and contributes to 30-70% energy saving.",project-academic
10.1109/INFCOM.2013.6567115,2013-04-14,p,IEEE,pipac patient infusion pattern based access control scheme for wireless insulin pump system," Wireless insulin pumps have been widely deployed in hospitals and home healthcare systems. Most of these insulin pump systems have limited security mechanisms embedded to protect them from malicious attacks. In this paper, two attacks against insulin pump systems via wireless links are investigated: a single acute overdose with a significant amount of medication, and chronic overdose with an insignificant amount of extra medication over a long time period, e.g., several months. These attacks can be launched unobtrusively and may jeopardize patients' lives. It is very important and urgent to protect patients from these attacks. To address this issue, we propose a novel patient infusion pattern based access control scheme (PIPAC) for wireless insulin pumps. This scheme employs a supervised learning approach to learn normal patient infusions pattern with the dosage amount, rate, and time of infusion, which are automatically recorded in insulin pump logs. The generated regression models are used to dynamically configure a safety infusion range for abnormal infusion identification. The proposed algorithm is evaluated with real insulin pump logs used by several patients for up to 6 months. The evaluation results demonstrate that our scheme can reliably detect the single overdose attack with a success rate up to 98% and defend against the chronic overdose attack with a very high success rate.",project-academic
10.1038/S41746-019-0189-7,2019-11-18,a,Nature Publishing Group,human machine partnership with artificial intelligence for chest radiograph diagnosis," Human-in-the-loop (HITL) AI may enable an ideal symbiosis of human experts and AI models, harnessing the advantages of both while at the same time overcoming their respective limitations. The purpose of this study was to investigate a novel collective intelligence technology designed to amplify the diagnostic accuracy of networked human groups by forming real-time systems modeled on biological swarms. Using small groups of radiologists, the swarm-based technology was applied to the diagnosis of pneumonia on chest radiographs and compared against human experts alone, as well as two state-of-the-art deep learning AI models. Our work demonstrates that both the swarm-based technology and deep-learning technology achieved superior diagnostic accuracy than the human experts alone. Our work further demonstrates that when used in combination, the swarm-based technology and deep-learning technology outperformed either method alone. The superior diagnostic accuracy of the combined HITL AI solution compared to radiologists and AI alone has broad implications for the surging clinical AI deployment and implementation strategies in future practice.",project-academic
10.1109/TCBB.2018.2827029,2018-11-01,a,IEEE,predicting hospital readmission via cost sensitive deep learning," With increased use of electronic medical records (EMRs), data mining on medical data has great potential to improve the quality of hospital treatment and increase the survival rate of patients. Early readmission prediction enables early intervention, which is essential to preventing serious or life-threatening events, and act as a substantial contributor to reduce healthcare costs. Existing works on predicting readmission often focus on certain vital signs and diseases by extracting statistical features. They also fail to consider skewness of class labels in medical data and different costs of misclassification errors. In this paper, we recur to the merits of convolutional neural networks (CNN) to automatically learn features from time series of vital sign, and categorical feature embedding to effectively encode feature vectors with heterogeneous clinical features, such as demographics, hospitalization history, vital signs, and laboratory tests. Then, both learnt features via CNN and statistical features via feature embedding are fed into a multilayer perceptron (MLP) for prediction. We use a cost-sensitive formulation to train MLP during prediction to tackle the imbalance and skewness challenge. We validate the proposed approach on two real medical datasets from Barnes-Jewish Hospital, and all data is taken from historical EMR databases and reflects the kinds of data that would realistically be available at the clinical prediction system in hospitals. We find that early prediction of readmission is possible and when compared with state-of-the-art existing methods used by hospitals, our methods perform significantly better. For example, using the general hospital wards data for 30-day readmission prediction, the area under the curve (AUC) for the proposed model was 0.70, significantly higher than all the baseline methods. Based on these results, a system is being deployed in hospital settings with the proposed forecasting algorithms to support treatment.",project-academic
10.1002/APS3.11390,2020-09-01,a,"John Wiley & Sons, Ltd",the plant pathology challenge 2020 data set to classify foliar disease of apples," Premise None Apple orchards in the United States are under constant threat from a large number of pathogens and insects. Appropriate and timely deployment of disease management depends on early disease detection. Incorrect and delayed diagnosis can result in either excessive or inadequate use of chemicals, with increased production costs and increased environmental and health impacts. None Methods and results None We have manually captured 3651 high-quality, real-life symptom images of multiple apple foliar diseases, with variable illumination, angles, surfaces, and noise. A subset of images, expert-annotated to create a pilot data set for apple scab, cedar apple rust, and healthy leaves, was made available to the Kaggle community for the Plant Pathology Challenge as part of the Fine-Grained Visual Categorization (FGVC) workshop at the 2020 Computer Vision and Pattern Recognition conference (CVPR 2020). Participants were asked to use the image data set to train a machine learning model to classify disease categories and develop an algorithm for disease severity quantification. The top three area under the ROC curve (AUC) values submitted to the private leaderboard were 0.98445, 0.98182, and 0.98089. We also trained an off-the-shelf convolutional neural network on this data for disease classification and achieved 97% accuracy on a held-out test set. None Discussion None This data set will contribute toward development and deployment of machine learning-based automated plant disease classification algorithms to ultimately realize fast and accurate disease detection. We will continue to add images to the pilot data set for a larger, more comprehensive expert-annotated data set for future Kaggle competitions and to explore more advanced methods for disease classification and quantification.",project-academic
10.1145/3219819.3220063,2018-07-19,p,ACM,exact and consistent interpretation for piecewise linear neural networks a closed form solution," Strong intelligent machines powered by deep neural networks are increasingly deployed as black boxes to make decisions in risk-sensitive domains, such as finance and medical. To reduce potential risk and build trust with users, it is critical to interpret how such machines make their decisions. Existing works interpret a pre-trained neural network by analyzing hidden neurons, mimicking pre-trained models or approximating local predictions. However, these methods do not provide a guarantee on the exactness and consistency of their interpretations. In this paper, we propose an elegant closed form solution named $OpenBox$ to compute exact and consistent interpretations for the family of Piecewise Linear Neural Networks (PLNN). The major idea is to first transform a PLNN into a mathematically equivalent set of linear classifiers, then interpret each linear classifier by the features that dominate its prediction. We further apply $OpenBox$ to demonstrate the effectiveness of non-negative and sparse constraints on improving the interpretability of PLNNs. The extensive experiments on both synthetic and real world data sets clearly demonstrate the exactness and consistency of our interpretation.",project-academic
,2018-02-17,a,,exact and consistent interpretation for piecewise linear neural networks a closed form solution," Strong intelligent machines powered by deep neural networks are increasingly deployed as black boxes to make decisions in risk-sensitive domains, such as finance and medical. To reduce potential risk and build trust with users, it is critical to interpret how such machines make their decisions. Existing works interpret a pre-trained neural network by analyzing hidden neurons, mimicking pre-trained models or approximating local predictions. However, these methods do not provide a guarantee on the exactness and consistency of their interpretation. In this paper, we propose an elegant closed form solution named $OpenBox$ to compute exact and consistent interpretations for the family of Piecewise Linear Neural Networks (PLNN). The major idea is to first transform a PLNN into a mathematically equivalent set of linear classifiers, then interpret each linear classifier by the features that dominate its prediction. We further apply $OpenBox$ to demonstrate the effectiveness of non-negative and sparse constraints on improving the interpretability of PLNNs. The extensive experiments on both synthetic and real world data sets clearly demonstrate the exactness and consistency of our interpretation.",project-academic
10.1145/1943513.1943524,2011-02-21,p,ACM,detection of anomalous insiders in collaborative environments via relational analysis of access logs," Collaborative information systems (CIS) are deployed within a diverse array of environments, ranging from the Internet to intelligence agencies to healthcare. It is increasingly the case that such systems are applied to manage sensitive information, making them targets for malicious insiders. While sophisticated security mechanisms have been developed to detect insider threats in various file systems, they are neither designed to model nor to monitor collaborative environments in which users function in dynamic teams with complex behavior. In this paper, we introduce a community-based anomaly detection system (CADS), an unsupervised learning framework to detect insider threats based on information recorded in the access logs of collaborative environments. CADS is based on the observation that typical users tend to form community structures, such that users with low affinity to such communities are indicative of anomalous and potentially illicit behavior. The model consists of two primary components: relational pattern extraction and anomaly detection. For relational pattern extraction, CADS infers community structures from CIS access logs, and subsequently derives communities, which serve as the CADS pattern core. CADS then uses a formal statistical model to measure the deviation of users from the inferred communities to predict which users are anomalies. To empirically evaluate the threat detection model, we perform an analysis with six months of access logs from a real electronic health record system in a large medical center, as well as a publicly available dataset for replication purposes. The results illustrate that CADS can distinguish simulated anomalous users in the context of real user behavior with a high degree of certainty and with significant performance gains in comparison to several competing anomaly detection models.",project-academic
10.3390/S20092653,2020-08-06,a,,an intelligent non invasive real time human activity recognition system for next generation healthcare," Human motion detection is getting considerable attention in the field of Artificial Intelligence (AI) driven healthcare systems. Human motion can be used to provide remote healthcare solutions for vulnerable people by identifying particular movements such as falls, gait and breathing disorders. This can allow people to live more independent lifestyles and still have the safety of being monitored if more direct care is needed. At present wearable devices can provide real time monitoring by deploying equipment on a person's body. However, putting devices on a person's body all the time make it uncomfortable and the elderly tends to forget it to wear as well in addition to the insecurity of being tracked all the time. This paper demonstrates how human motions can be detected in quasi-real-time scenario using a non-invasive method. Patterns in the wireless signals presents particular human body motions as each movement induces a unique change in the wireless medium. These changes can be used to identify particular body motions. This work produces a dataset that contains patterns of radio wave signals obtained using software defined radios (SDRs) to establish if a subject is standing up or sitting down as a test case. The dataset was used to create a machine learning model, which was used in a developed application to provide a quasi-real-time classification of standing or sitting state. The machine learning model was able to achieve 96.70 % accuracy using the Random Forest algorithm using 10 fold cross validation. A benchmark dataset of wearable devices was compared to the proposed dataset and results showed the proposed dataset to have similar accuracy of nearly 90 %. The machine learning models developed in this paper are tested for two activities but the developed system is designed and applicable for detecting and differentiating x number of activities.",project-academic
10.1109/TIE.2017.2752151,2018-04-01,a,IEEE,scalable and unsupervised feature engineering using vibration imaging and deep learning for rotor system diagnosis," This paper proposes a scalable and unsupervised feature engineering method that uses vibration imaging and deep learning. For scalability, a vibration imaging approach is devised that incorporates data from systems with various scales, such as small testbeds and real field-deployed systems. Moreover, a deep learning approach is proposed for unsupervised feature engineering. The overall procedure includes three key steps: 1) vibration image generation; 2) unsupervised feature extraction; and 3) fault classifier design. To demonstrate the validity of the proposed approach, three case studies are conducted using an RK4 rotor kit and a power plant journal bearing system. By incorporating smaller-system data as well as real-system data, the proposed approach can substantially increase the applicability of the fault diagnosis method while maintaining good accuracy. Moreover, the time and effort needed to develop a diagnostic approach for other rotor systems can be reduced considerably.",project-academic
10.1016/J.COMPELECENG.2017.03.009,2017-03-16,a,Pergamon,applying spark based machine learning model on streaming big data for health status prediction," Abstract None None Machine learning is one of the driving forces of science and commerce, but the proliferation of Big Data demands paradigm shifts from traditional methods in the application of machine learning techniques on this voluminous data having varying velocity. With the availability of large health care datasets and progressions in machine learning techniques, computers are now well equipped in diagnosing many health issues. This work aims at developing a real time remote health status prediction system built around open source Big Data processing engine, the Apache Spark, deployed in the cloud which focus on applying machine learning model on streaming Big Data. In this scalable system, the user tweets his health attributes and the application receives the same in real time, extracts the attributes and applies machine learning model to predict user's health status which is then directly messaged to him/her instantly for taking appropriate action.",project-academic
10.5555/2615731.2617417,2014-05-05,p,International Foundation for Autonomous Agents and Multiagent Systems,offline policy evaluation across representations with applications to educational games," Consider an autonomous teacher agent trying to adaptively sequence material to best keep a student engaged, or a medical agent trying to help suggest treatments to maximize patient outcomes. To solve these complex reinforcement learning problems, we must first decide on a policy representation. But determining the best representation can be challenging, since the environment includes many poorly-understood processes (such as student engagement) and is therefore difficult to accurately simulate. These domains are also high stakes, making it infeasible to evaluate candidate representations by running them online. Instead, one must leverage existing data to learn and evaluate new policies for future use. In this paper, we present a data-driven methodology for comparing and validating policies offline. Our method is unbiased, agnostic to representation, and focuses on the ability of each policy to generalize to new data. We apply this methodology to a partially-observable, high-dimensional concept sequencing problem in an educational game. Guided by our evaluation methodology, we propose a novel feature compaction method that substantially improves policy performance on this problem. We deploy the best-performing policies to 2,000 real students and show that the learned adaptive policy shows statistically significant improvement over random and expert baselines, improving our achievement-based reward measure by 32%.",project-academic
,2020-04-24,a,,the plant pathology 2020 challenge dataset to classify foliar disease of apples," Apple orchards in the U.S. are under constant threat from a large number of pathogens and insects. Appropriate and timely deployment of disease management depends on early disease detection. Incorrect and delayed diagnosis can result in either excessive or inadequate use of chemicals, with increased production costs, environmental, and health impacts. We have manually captured 3,651 high-quality, real-life symptom images of multiple apple foliar diseases, with variable illumination, angles, surfaces, and noise. A subset, expert-annotated to create a pilot dataset for apple scab, cedar apple rust, and healthy leaves, was made available to the Kaggle community for 'Plant Pathology Challenge'; part of the Fine-Grained Visual Categorization (FGVC) workshop at CVPR 2020 (Computer Vision and Pattern Recognition). We also trained an off-the-shelf convolutional neural network (CNN) on this data for disease classification and achieved 97% accuracy on a held-out test set. This dataset will contribute towards development and deployment of machine learning-based automated plant disease classification algorithms to ultimately realize fast and accurate disease detection. We will continue to add images to the pilot dataset for a larger, more comprehensive expert-annotated dataset for future Kaggle competitions and to explore more advanced methods for disease classification and quantification.",project-academic
10.1093/JLB/LSAA043,2020-07-25,a,Oxford Academic,increasing access to care telehealth during covid 19," The coronavirus disease 2019 (COVID-19) public health emergency has amplified both the potential value and the challenges with healthcare providers deploying telehealth solutions. As people across the country find ways to stay at home, telehealth preserves an opportunity to obtain necessary healthcare services. Further, telehealth can help individuals avoid COVID-19 infection, free up hospital beds and other resources for those patients most in need, and prevent infected individuals from spreading that infection. Federal and state regulators have recognized this potential of telehealth and have quickly changed a variety of laws and regulations to enable healthcare providers to deploy solutions quickly. These changes can provide lasting benefits for the use of telehealth well after the current crisis. However, to best realize telehealth's benefits, further legal and regulatory actions are necessary. Specifically, lawmakers and regulators should focus on six areas: reimbursement, privacy/cybersecurity, liability, licensure, technology access, and artificial intelligence.",project-academic
10.1371/JOURNAL.PONE.0205392,2018-10-11,a,Public Library of Science,accurate real time localization tracking in a clinical environment using bluetooth low energy and deep learning," Deep learning has started to revolutionize several different industries, and the applications of these methods in medicine are now becoming more commonplace. This study focuses on investigating the feasibility of tracking patients and clinical staff wearing Bluetooth Low Energy (BLE) tags in a radiation oncology clinic using artificial neural networks (ANNs) and convolutional neural networks (CNNs). The performance of these networks was compared to relative received signal strength indicator (RSSI) thresholding and triangulation. By utilizing temporal information, a combined CNN+ANN network was capable of correctly identifying the location of the BLE tag with an accuracy of 99.9%. It outperformed a CNN model (accuracy = 94%), a thresholding model employing majority voting (accuracy = 95%), and a triangulation classifier utilizing majority voting (accuracy = 95%). Future studies will seek to deploy this affordable real time location system in hospitals to improve clinical workflow, efficiency, and patient safety.",project-academic
10.1145/3448104,2021-03-29,a,Association for Computing Machinery (ACM),splitsr an end to end approach to super resolution on mobile devices," Super-resolution (SR) is a coveted image processing technique for mobile apps ranging from the basic camera apps to mobile health. Existing SR algorithms rely on deep learning models with significant memory requirements, so they have yet to be deployed on mobile devices and instead operate in the cloud to achieve feasible inference time. This shortcoming prevents existing SR methods from being used in applications that require near real-time latency. In this work, we demonstrate state-of-the-art latency and accuracy for on-device super-resolution using a novel hybrid architecture called SplitSR and a novel lightweight residual block called SplitSRBlock. The SplitSRBlock supports channel-splitting, allowing the residual blocks to retain spatial information while reducing the computation in the channel dimension. SplitSR has a hybrid design consisting of standard convolutional blocks and lightweight residual blocks, allowing people to tune SplitSR for their computational budget. We evaluate our system on a low-end ARM CPU, demonstrating both higher accuracy and up to 5× faster inference than previous approaches. We then deploy our model onto a smartphone in an app called ZoomSR to demonstrate the first-ever instance of on-device, deep learning-based SR. We conducted a user study with 15 participants to have them assess the perceived quality of images that were post-processed by SplitSR. Relative to bilinear interpolation --- the existing standard for on-device SR --- participants showed a statistically significant preference when looking at both images (Z=-9.270, p",project-academic
,2021-01-20,a,,splitsr an end to end approach to super resolution on mobile devices," Super-resolution (SR) is a coveted image processing technique for mobile apps ranging from the basic camera apps to mobile health. Existing SR algorithms rely on deep learning models with significant memory requirements, so they have yet to be deployed on mobile devices and instead operate in the cloud to achieve feasible inference time. This shortcoming prevents existing SR methods from being used in applications that require near real-time latency. In this work, we demonstrate state-of-the-art latency and accuracy for on-device super-resolution using a novel hybrid architecture called SplitSR and a novel lightweight residual block called SplitSRBlock. The SplitSRBlock supports channel-splitting, allowing the residual blocks to retain spatial information while reducing the computation in the channel dimension. SplitSR has a hybrid design consisting of standard convolutional blocks and lightweight residual blocks, allowing people to tune SplitSR for their computational budget. We evaluate our system on a low-end ARM CPU, demonstrating both higher accuracy and up to 5 times faster inference than previous approaches. We then deploy our model onto a smartphone in an app called ZoomSR to demonstrate the first-ever instance of on-device, deep learning-based SR. We conducted a user study with 15 participants to have them assess the perceived quality of images that were post-processed by SplitSR. Relative to bilinear interpolation -- the existing standard for on-device SR -- participants showed a statistically significant preference when looking at both images (Z=-9.270, p<0.01) and text (Z=-6.486, p<0.01).",project-academic
10.1109/CVPR42600.2020.00852,2020-06-01,p,IEEE,the knowledge within methods for data free model compression," Background: Recently, an extensive amount of research has been focused on compressing and accelerating Deep Neural Networks (DNN). So far, high compression rate algorithms require part of the training dataset for a low precision calibration, or a fine-tuning process. However, this requirement is unacceptable when the data is unavailable or contains sensitive information, as in medical and biometric use-cases. Contributions: We present three methods for generating synthetic samples from trained models. Then, we demonstrate how these samples can be used to calibrate and fine-tune quantized models without using any real data in the process. Our best performing method has a negligible accuracy degradation compared to the original training set. This method, which leverages intrinsic batch normalization layers' statistics of the trained model, can be used to evaluate data similarity. Our approach opens a path towards genuine data-free model compression, alleviating the need for training data during model deployment.",project-academic
,2019-12-03,a,,the knowledge within methods for data free model compression," Recently, an extensive amount of research has been focused on compressing and accelerating Deep Neural Networks (DNN). So far, high compression rate algorithms require part of the training dataset for a low precision calibration, or a fine-tuning process. However, this requirement is unacceptable when the data is unavailable or contains sensitive information, as in medical and biometric use-cases. We present three methods for generating synthetic samples from trained models. Then, we demonstrate how these samples can be used to calibrate and fine-tune quantized models without using any real data in the process. Our best performing method has a negligible accuracy degradation compared to the original training set. This method, which leverages intrinsic batch normalization layers' statistics of the trained model, can be used to evaluate data similarity. Our approach opens a path towards genuine data-free model compression, alleviating the need for training data during model deployment.",project-academic
10.1145/3447548.3467274,2021-08-14,p,ACM,s lime stabilized lime for model explanation," An increasing number of machine learning models have been deployed in domains with high stakes such as finance and healthcare. Despite their superior performances, many models are black boxes in nature which are hard to explain. There are growing efforts for researchers to develop methods to interpret these black-box models. Post hoc explanations based on perturbations, such as LIME [39], are widely used approaches to interpret a machine learning model after it has been built. This class of methods has been shown to exhibit large instability, posing serious challenges to the effectiveness of the method itself and harming user trust. In this paper, we propose S-LIME, which utilizes a hypothesis testing framework based on central limit theorem for determining the number of perturbation points needed to guarantee stability of the resulting explanation. Experiments on both simulated and real world data sets are provided to demonstrate the effectiveness of our method.",project-academic
,2020-07-16,a,,enhanced detection of fetal pose in 3d mri by deep reinforcement learning with physical structure priors on anatomy," Fetal MRI is heavily constrained by unpredictable and substantial fetal motion that causes image artifacts and limits the set of viable diagnostic image contrasts. Current mitigation of motion artifacts is predominantly performed by fast, single-shot MRI and retrospective motion correction. Estimation of fetal pose in real time during MRI stands to benefit prospective methods to detect and mitigate fetal motion artifacts where inferred fetal motion is combined with online slice prescription with low-latency decision making. Current developments of deep reinforcement learning (DRL), offer a novel approach for fetal landmarks detection. In this task 15 agents are deployed to detect 15 landmarks simultaneously by DRL. The optimization is challenging, and here we propose an improved DRL that incorporates priors on physical structure of the fetal body. First, we use graph communication layers to improve the communication among agents based on a graph where each node represents a fetal-body landmark. Further, additional reward based on the distance between agents and physical structures such as the fetal limbs is used to fully exploit physical structure. Evaluation of this method on a repository of 3-mm resolution in vivo data demonstrates a mean accuracy of landmark estimation within 10 mm of ground truth as 87.3%, and a mean error of 6.9 mm. The proposed DRL for fetal pose landmark search demonstrates a potential clinical utility for online detection of fetal motion that guides real-time mitigation of motion artifacts as well as health diagnosis during MRI of the pregnant mother.",project-academic
10.1007/978-3-030-59725-2_38,2020-10-04,p,"Springer, Cham",enhanced detection of fetal pose in 3d mri by deep reinforcement learning with physical structure priors on anatomy," Fetal MRI is heavily constrained by unpredictable and substantial fetal motion that causes image artifacts and limits the set of viable diagnostic image contrasts. Current mitigation of motion artifacts is predominantly performed by fast, single-shot MRI and retrospective motion correction. Estimation of fetal pose in real time during MRI stands to benefit prospective methods to detect and mitigate fetal motion artifacts where inferred fetal motion is combined with online slice prescription with low-latency decision making. Current developments of deep reinforcement learning (DRL), offer a novel approach for fetal landmarks detection. In this task 15 agents are deployed to detect 15 landmarks simultaneously by DRL. The optimization is challenging, and here we propose an improved DRL that incorporates priors on physical structure of the fetal body. First, we use graph communication layers to improve the communication among agents based on a graph where each node represents a fetal-body landmark. Further, additional reward based on the distance between agents and physical structures such as the fetal limbs is used to fully exploit physical structure. Evaluation of this method on a repository of 3-mm resolution in vivo data demonstrates a mean accuracy of landmark estimation 10 mm of ground truth as 87.3%, and a mean error of 6.9 mm. The proposed DRL for fetal pose landmark search demonstrates a potential clinical utility for online detection of fetal motion that guides real-time mitigation of motion artifacts as well as health diagnosis during MRI of the pregnant mother.",project-academic
10.1016/J.MICPRO.2020.103301,2021-02-01,a,Elsevier,iot enabled cancer prediction system to enhance the authentication and security using cloud computing," Abstract None None In recent days, Internet of Things, Cloud Computing, Deep learning, Machine learning and Artificial Intelligence are considered to be an emerging technologies to solve variety of real world problems. These techniques are importantly applied in various fields such as healthcare systems, transportation systems, agriculture and smart cities to produce fruitful results for number of issues in today's environment. This research work focuses on one such application in the field of IoT together with cloud computing. More number of sensors that are deployed in human body is used to collect patient related data such as deviation in body temperature and others which leads to variation in blood cells that turned to be cancerous cells. Main intention of this work is design a cancer prediction system using Internet of Things upon extracting the details of blood results to test whether it is normal or abnormal. In addition to this, encryption is done on the blood results of cancer affected patient and store it in cloud for quick reference through Internet for the doctor or healthcare nurse to handle the patient data secretly. This research work concentrates on enhancing the health care computations and processing. It provides a framework to enhance the performance of the existing health care industry across the globe. As the entire medical data has to be saved in cloud, the traditional medical treatment limitations can be overcome. Encryption and decryption is done using AES algorithm in order to provide authentication and security in handling cancer patients. The main focus is to handle healthcare data effectively for the patient when they are away from the home town since the needed cancer treatment details are stored in cloud. The task completion time is greatly reduce from 400 to 160  by using VMs. CloudSim gives an adaptable simulation structure that empowers displaying and reproduced results.",project-academic
10.1145/3278576.3278597,2018-09-26,p,IEEE Institute of Electrical and Electronic Engineers,empowering healthcare iot systems with hierarchical edge based deep learning," Remote health monitoring is a powerful tool to provide preventive care and early intervention for populations-at-risk. Such monitoring systems are becoming available nowadays due to recent advancements in Internet-of-Things (IoT) paradigms, enabling ubiquitous monitoring. These systems require a high level of quality in attributes such as availability and accuracy due to patients critical conditions in the monitoring. Deep learning methods are very promising in such health applications to obtain a satisfactory performance, where a considerable amount of data is available. These methods are perfectly positioned in the cloud servers in a centralized cloud-based IoT system. However, the response time and availability of these systems highly depend on the quality of Internet connection. On the other hand, smart gateway devices are unable to implement deep learning methods (such as training models) due to their limited computational capacities. In our previous work, we proposed a hierarchical computing architecture (HiCH), where both edge and cloud computing resources were efficiently exploited, allocating heavy tasks of a conventional machine learning method to the cloud servers and outsourcing the hypothesis function to the edge. Due to this local decision making, the availability of the system was highly improved. In this paper, we investigate the feasibility of deploying the Convolutional Neural Network (CNN) based classification model as an example of deep learning methods in this architecture. Therefore, the system benefits from the features of the HiCH and the CNN, ensuring a high-level availability and accuracy. We demonstrate a real-time health monitoring for a case study on ECG classifications and evaluate the performance of the system in terms of response time and accuracy.",project-academic
10.1038/S41598-019-56927-5,2020-01-13,a,Nature Publishing Group,precision medicine and artificial intelligence a pilot study on deep learning for hypoglycemic events detection based on ecg," Tracking the fluctuations in blood glucose levels is important for healthy subjects and crucial diabetic patients. Tight glucose monitoring reduces the risk of hypoglycemia, which can result in a series of complications, especially in diabetic patients, such as confusion, irritability, seizure and can even be fatal in specific conditions. Hypoglycemia affects the electrophysiology of the heart. However, due to strong inter-subject heterogeneity, previous studies based on a cohort of subjects failed to deploy electrocardiogram (ECG)-based hypoglycemic detection systems reliably. The current study used personalised medicine approach and Artificial Intelligence (AI) to automatically detect nocturnal hypoglycemia using a few heartbeats of raw ECG signal recorded with non-invasive, wearable devices, in healthy individuals, monitored 24 hours for 14 consecutive days. Additionally, we present a visualisation method enabling clinicians to visualise which part of the ECG signal (e.g., T-wave, ST-interval) is significantly associated with the hypoglycemic event in each subject, overcoming the intelligibility problem of deep-learning methods. These results advance the feasibility of a real-time, non-invasive hypoglycemia alarming system using short excerpts of ECG signal.",project-academic
10.1109/TDSC.2012.11,2012-05-01,a,IEEE,detecting anomalous insiders in collaborative information systems," Collaborative information systems (CISs) are deployed within a diverse array of environments that manage sensitive information. Current security mechanisms detect insider threats, but they are ill-suited to monitor systems in which users function in dynamic teams. In this paper, we introduce the community anomaly detection system (CADS), an unsupervised learning framework to detect insider threats based on the access logs of collaborative environments. The framework is based on the observation that typical CIS users tend to form community structures based on the subjects accessed (e.g., patients' records viewed by healthcare providers). CADS consists of two components: 1) relational pattern extraction, which derives community structures and 2) anomaly prediction, which leverages a statistical model to determine when users have sufficiently deviated from communities. We further extend CADS into MetaCADS to account for the semantics of subjects (e.g., patients' diagnoses). To empirically evaluate the framework, we perform an assessment with three months of access logs from a real electronic health record (EHR) system in a large medical center. The results illustrate our models exhibit significant performance gains over state-of-the-art competitors. When the number of illicit users is low, MetaCADS is the best model, but as the number grows, commonly accessed semantics lead to hiding in a crowd, such that CADS is more prudent.",project-academic
10.1016/J.JPOWSOUR.2016.05.092,2016-08-30,a,Elsevier,prognostics of proton exchange membrane fuel cells stack using an ensemble of constraints based connectionist networks," Proton Exchange Membrane Fuel Cell (PEMFC) is considered the most versatile among available fuel cell technologies, which qualify for diverse applications. However, the large-scale industrial deployment of PEMFCs is limited due to their short life span and high exploitation costs. Therefore, ensuring fuel cell service for a long duration is of vital importance, which has led to Prognostics and Health Management of fuel cells. More precisely, prognostics of PEMFC is major area of focus nowadays, which aims at identifying degradation of PEMFC stack at early stages and estimating its Remaining Useful Life (RUL) for life cycle management. This paper presents a data-driven approach for prognostics of PEMFC stack using an ensemble of constraint based Summation Wavelet- Extreme Learning Machine (SW-ELM) models. This development aim at improving the robustness and applicability of prognostics of PEMFC for an online application, with limited learning data. The proposed approach is applied to real data from two different PEMFC stacks and compared with ensembles of well known connectionist algorithms. The results comparison on long-term prognostics of both PEMFC stacks validates our proposition.",project-academic
10.1109/ICIT.2015.7125235,2015-03-17,p,IEEE,improving accuracy of long term prognostics of pemfc stack to estimate remaining useful life," Proton Exchange Membrane Fuel cells (PEMFC) are energy systems that facilitate electrochemical reactions to create electrical energy from chemical energy of hydrogen. PEMFC are promising source of renewable energy that can operate on low temperature and have the advantages of high power density and low pollutant emissions. However, PEMFC technology is still in the developing phase, and its large-scale industrial deployment requires increasing the life span of fuel cells and decreasing their exploitation costs. In this context, Prognostics and Health Management of fuel cells is an emerging field, which aims at identifying degradation at early stages and estimating the Remaining Useful Life (RUL) for life cycle management. Indeed, due to prognostics capability, the accurate estimates of RUL enables safe operation of the equipment and timely decisions to prolong its life span. This paper contributes data-driven prognostics of PEMFC by an ensemble of constraint based Summation Wavelet-Extreme Learning Machine (SW-ELM) algorithm to improve accuracy and robustness of long-term prognostics. The SW-ELM is used for ensemble modeling due to its enhanced applicability for real applications as compared to conventional data-driven algorithms. The proposed prognostics model is validated on run-to-failure data of PEMFC stack, which had the life span of 1750 hours. The results confirm capability of the prognostics model to achieve accurate RUL estimates.",project-academic
10.1109/ACCESS.2019.2919736,2019-06-03,a,IEEE,federated learning based computation offloading optimization in edge computing supported internet of things," Recently, smart cities, smart homes, and smart medical systems have challenged the functionality and connectivity of the large-scale Internet of Things (IoT) devices. Thus, with the idea of offloading intensive computing tasks from them to edge nodes (ENs), edge computing emerged to supplement these limited devices. Benefit from this advantage, IoT devices can save more energy and still maintain the quality of the services they should provide. However, computational offload decisions involve federation and complex resource management and should be determined in the real-time face to dynamic workloads and radio environments. Therefore, in this work, we use multiple deep reinforcement learning (DRL) agents deployed on multiple edge nodes to indicate the decisions of the IoT devices. On the other hand, with the aim of making DRL-based decisions feasible and further reducing the transmission costs between the IoT devices and edge nodes, federated learning (FL) is used to train DRL agents in a distributed fashion. The experimental results demonstrate the effectiveness of the decision scheme and federated learning in the dynamic IoT system.",project-academic
10.1016/J.DSS.2016.02.005,2016-04-01,p,North-Holland,a case based reasoning system for aiding detection and classification of nosocomial infections," Nowadays, it is recognized worldwide that healthcare-associated infections are responsible for an increase in patient morbidity, mortality, and higher costs related to prolonged hospital stays. As electronic health data are increasingly available today, there is a unique opportunity to implement real-time decision support systems for automating the surveillance of healthcare-associated infections. As a consequence, different electronic surveillance systems have been implemented to date with varying degrees of success. However, there have been few instances in which clinical data and physician narratives with the potential to significantly improve electronic surveillance alternatives have been adopted. In this context, the present work introduces a case-based reasoning system for the automatic surveillance and diagnosis of healthcare-associated infections. The developed system makes use of different machine learning techniques in order to (i) automatically extract evidence from different types of data including clinical unstructured documents, (ii) incorporate static a priori knowledge handled by infection preventionists, and (iii) dynamically generate new knowledge as well as understandable explanations about the system's decisions. Results obtained from a real deployment in a public hospital belonging to the Spanish National Health System trained with 2569 samples belonging to 1800 patients during more than 10 consecutive months recognize the usefulness of the system. Display Omitted Automatic surveillance of healthcare-associated infections.Diagnostic decision support system aiding monitoring and control.Case-based reasoning system for classifying nosocomial infections.Static rule-based knowledge representation and dynamic induction process.Natural language processing for physician narratives and nurses' comments.",project-academic
,2019-09-23,a,,model based and data driven strategies in medical image computing," Model-based approaches for image reconstruction, analysis and interpretation have made significant progress over the last decades. Many of these approaches are based on either mathematical, physical or biological models. A challenge for these approaches is the modelling of the underlying processes (e.g. the physics of image acquisition or the patho-physiology of a disease) with appropriate levels of detail and realism. With the availability of large amounts of imaging data and machine learning (in particular deep learning) techniques, data-driven approaches have become more widespread for use in different tasks in reconstruction, analysis and interpretation. These approaches learn statistical models directly from labelled or unlabeled image data and have been shown to be very powerful for extracting clinically useful information from medical imaging. While these data-driven approaches often outperform traditional model-based approaches, their clinical deployment often poses challenges in terms of robustness, generalization ability and interpretability. In this article, we discuss what developments have motivated the shift from model-based approaches towards data-driven strategies and what potential problems are associated with the move towards purely data-driven approaches, in particular deep learning. We also discuss some of the open challenges for data-driven approaches, e.g. generalization to new unseen data (e.g. transfer learning), robustness to adversarial attacks and interpretability. Finally, we conclude with a discussion on how these approaches may lead to the development of more closely coupled imaging pipelines that are optimized in an end-to-end fashion.",project-academic
10.1109/JPROC.2019.2943836,2020-01-01,a,IEEE,model based and data driven strategies in medical image computing," Model-based approaches for image reconstruction, analysis, and interpretation have made significant progress over the past decades. Many of these approaches are based on either mathematical, physical, or biological models. A challenge for these approaches is the modeling of the underlying processes (e.g., the physics of image acquisition or the patho-physiology of a disease) with appropriate levels of detail and realism. With the availability of large amounts of imaging data and machine learning (in particular deep learning) techniques, data-driven approaches have become more widespread for use in different tasks in reconstruction, analysis, and interpretation. These approaches learn statistical models directly from labeled or unlabeled image data and have been shown to be very powerful for extracting clinically useful information from medical imaging. While these data-driven approaches often outperform traditional model-based approaches, their clinical deployment often poses challenges in terms of robustness, generalization ability, and interpretability. In this article, we discuss what developments have motivated the shift from model-based approaches toward data-driven strategies and what potential problems are associated with the move toward purely data-driven approaches, in particular deep learning. We also discuss some of the open challenges for data-driven approaches, e.g., generalization to new unseen data (e.g., transfer learning), robustness to adversarial attacks, and interpretability. Finally, we conclude with a discussion on how these approaches may lead to the development of more closely coupled imaging pipelines that are optimized in an end-to-end fashion.",project-academic
10.1109/MTS.2021.3056295,2021-03-15,a,Institute of Electrical and Electronics Engineers (IEEE),toward a more equal world the human rights approach to extending the benefits of artificial intelligence," We are all None aware of the huge potential for artificial intelligence (AI) to bring massive benefits to under-served populations, advancing equal access to public services such as health, education, social assistance, or public transportation, for example. We are equally aware that AI can drive inequality, concentrating wealth, resources, and decision-making power in the hands of a few countries, companies, or citizens. Artificial intelligence for equity (AI4Eq) None [1] None as presented in this magazine, calls upon academics, AI developers, civil society, and government policy-makers to work collaboratively toward a technological transformation that increases the benefits to society, reduces inequality, and aims to leave no one behind. A call for equity rests on the human rights principle of equality and nondiscrimination. AI design, development, and deployment (AI-DDD) can and should be harnessed to reduce inequality and increase the share of the world’s population that is able to live in dignity and fully realize their human potential. This commentary argues, first, that far preferable to an ethics framework, adopting a human rights framework for AI-DDD offers the potential for a robust and enforceable set of guidelines for the pursuit of AI4Eq. Second, the commentary introduces the work of IEEE in proposing practical recommendations for AI4Eq, so that people living in high-income countries (HICs), low- and middle-income countries (LMICs), alike, share AI applications’ widespread benefit to humanity.",project-academic
10.1145/3391403.3399458,2020-07-13,p,ACM,matching algorithms for blood donation," Managing perishable inventory, such as blood stock awaiting use by patients in need, has been a topic of research for decades. This has been investigated across several disciplines: medical and social scientists have investigated who donates blood, how frequently, and why; management science researchers have long studied the blood supply chain from a logistical perspective. Yet global demand for blood still far exceeds supply, and unmet need is greatest in low- and middle-income countries. Both academics and policy experts suggest that large-scale coordination is necessary to alleviate demand for donor blood. Using the recently-deployed Facebook Blood Donation tool, we conduct the first large-scale algorithmic matching of blood donors with donation opportunities. In both simulations and real experiments we match potential donors with opportunities, guided by a machine learning model trained on prior observations of donor behavior. While measuring actual donation rates remains a challenge, we measure donor action (i.e., calling a blood bank or making an appointment) as a proxy for actual donation. Simulations suggest that even a simple matching strategy can increase donor action rate by 10-15%; a pilot experiment with real donors finds a slightly smaller increase of roughly 5%. While overall action rates remain low, even this modest increase among donors in a global network corresponds to many thousands of more potential donors taking action toward donation. Further, observing donor action on a social network can shed light onto donor behavior and response to incentives. Our initial findings align with several observations made in the medical and social science literature regarding donor behavior.",project-academic
10.1109/PRDC47002.2019.00056,2019-01-01,p,Institute of Electrical and Electronics Engineers,deep learning based intrusion detection for iot networks," Internet of Things (IoT) has an immense potential for a plethora of applications ranging from healthcare automation to defence networks and the power grid. The security of an IoT network is essentially paramount to the security of the underlying computing and communication infrastructure. However, due to constrained resources and limited computational capabilities, IoT networks are prone to various attacks. Thus, safeguarding the IoT network from adversarial attacks is of vital importance and can be realised through planning and deployment of effective security controls; one such control being an intrusion detection system. In this paper, we present a novel intrusion detection scheme for IoT networks that classifies traffic flow through the application of deep learning concepts. We adopt a newly published IoT dataset and generate generic features from the field information in packet level. We develop a feed-forward neural networks model for binary and multi-class classification including denial of service, distributed denial of service, reconnaissance and information theft attacks against IoT devices. Results obtained through the evaluation of the proposed scheme via the processed dataset illustrate a high classification accuracy.",project-academic
10.1007/978-3-030-59719-1_9,2020-06-23,a,,deep generative model based quality control for cardiac mri segmentation," In recent years, convolutional neural networks have demonstrated promising performance in a variety of medical image segmentation tasks. However, when a trained segmentation model is deployed into the real clinical world, the model may not perform optimally. A major challenge is the potential poor-quality segmentations generated due to degraded image quality or domain shift issues. There is a timely need to develop an automated quality control method that can detect poor segmentations and feedback to clinicians. Here we propose a novel deep generative model-based framework for quality control of cardiac MRI segmentation. It first learns a manifold of good-quality image-segmentation pairs using a generative model. The quality of a given test segmentation is then assessed by evaluating the difference from its projection onto the good-quality manifold. In particular, the projection is refined through iterative search in the latent space. The proposed method achieves high prediction accuracy on two publicly available cardiac MRI datasets. Moreover, it shows better generalisation ability than traditional regression-based methods. Our approach provides a real-time and model-agnostic quality control for cardiac MRI segmentation, which has the potential to be integrated into clinical image analysis workflows.",project-academic
,2021-02-01,a,,machine learning pipeline for battery state of health estimation," Lithium-ion batteries are ubiquitous in modern day applications ranging from portable electronics to electric vehicles. Irrespective of the application, reliable real-time estimation of battery state of health (SOH) by on-board computers is crucial to the safe operation of the battery, ultimately safeguarding asset integrity. In this paper, we design and evaluate a machine learning pipeline for estimation of battery capacity fade - a metric of battery health - on 179 cells cycled under various conditions. The pipeline estimates battery SOH with an associated confidence interval by using two parametric and two non-parametric algorithms. Using segments of charge voltage and current curves, the pipeline engineers 30 features, performs automatic feature selection and calibrates the algorithms. When deployed on cells operated under the fast-charging protocol, the best model achieves a root mean squared percent error of 0.45\%. This work provides insights into the design of scalable data-driven models for battery SOH estimation, emphasising the value of confidence bounds around the prediction. The pipeline methodology combines experimental data with machine learning modelling and can be generalized to other critical components that require real-time estimation of SOH.",project-academic
10.1038/S42256-021-00312-3,2021-02-01,a,Nature Publishing Group,machine learning pipeline for battery state of health estimation," Lithium-ion batteries are ubiquitous in applications ranging from portable electronics to electric vehicles. Irrespective of the application, reliable real-time estimation of battery state of health (SOH) by on-board computers is crucial to the safe operation of the battery, ultimately safeguarding asset integrity. In this Article, we design and evaluate a machine learning pipeline for estimation of battery capacity fade—a metric of battery health—on 179 cells cycled under various conditions. The pipeline estimates battery SOH with an associated confidence interval by using two parametric and two non-parametric algorithms. Using segments of charge voltage and current curves, the pipeline engineers 30 features, performs automatic feature selection and calibrates the algorithms. When deployed on cells operated under the fast-charging protocol, the best model achieves a root-mean-squared error of 0.45%. This work provides insights into the design of scalable data-driven models for battery SOH estimation, emphasizing the value of confidence bounds around the prediction. The pipeline methodology combines experimental data with machine learning modelling and could be applied to other critical components that require real-time estimation of SOH. Rechargeable lithium-ion batteries play a crucial role in many modern-day applications, including portable electronics and electric vehicles, but they degrade over time. To ensure safe operation, a battery’s ‘state of health’ should be monitored in real time, and this machine learning pipeline, tested on a variety of charging conditions, can provide such an online estimation of battery state of health.",project-academic
10.1016/J.INFFUS.2017.05.004,2017-05-29,a,Elsevier,real time activity monitoring with a wristband and a smartphone," Abstract None None Activity monitoring is a very important task in lifestyle and health domains where physical activity of a person plays an important role in further reasoning or for providing personalized recommendations. To make such services available to a broader population, one should use devices that most users already have, such as smartphones. Since trends show an increasing popularity of wrist-worn wearables we also consider a sensor-rich wristband as an optional device in this research. We present a real-time activity monitoring algorithm which utilizes data from the smartphone sensors, wristband sensors or their fusion for activity recognition and estimation of energy expenditure of the user. The algorithm detects which devices are present and uses an interval of walking for gravity detection and normalization of the orientation of the devices. The normalized data is afterwards used for the detection of the location of the smartphone on the body, which serves as a context for the selection of location-specific classification model for activity recognition. The recognized activity is finally used for the selection of one or multiple regression models for the estimation of the user’s energy expenditure. To develop the machine-learning models, which can be deployed on the smartphone, we optimized the number and type of extracted features via automatic feature selection. We evaluated each step of the algorithm and each device configuration, and compared the human energy expenditure estimation results against the Bodymedia armband and Microsoft Band 2. We also evaluated the benefit of decision fusion where appropriate. The results show that we achieve a 87% ± 5% average accuracy for activity recognition and that we outperformed both competing devices in the estimation of human energy expenditure by achieving the mean absolute error of 0.6 ± 0.1 MET on average.",project-academic
10.1109/TBME.2020.3006158,2021-01-01,a,IEEE,multidimensional ground reaction forces and moments from wearable sensor accelerations via deep learning," Objective: None Monitoring athlete internal workload exposure, including prevention of catastrophic non-contact knee injuries, relies on the existence of a custom early-warning detection system. This system must be able to estimate accurate, reliable, and valid musculoskeletal joint loads, for sporting maneuvers in near real-time and during match play. However, current methods are constrained to laboratory instrumentation, are labor and cost intensive, and require highly trained specialist knowledge, thereby limiting their ecological validity and wider deployment. An informative next step towards this goal would be a new method to obtain ground kinetics in the field. None Methods: None Here we show that kinematic data obtained from wearable sensor accelerometers, in lieu of embedded force platforms, can leverage recent supervised learning techniques to predict near real-time multidimensional ground reaction forces and moments (GRF/M). Competing convolutional neural network (CNN) deep learning models were trained using laboratory-derived stance phase GRF/M data and simulated sensor accelerations for running and sidestepping maneuvers derived from nearly half a million legacy motion trials. Then, predictions were made from each model driven by five sensor accelerations recorded during independent inter-laboratory data capture sessions. None Results: None The proposed deep learning workbench achieved correlations to ground truth, by maximum discrete GRF component, of vertical None None $F_z$ None None 0.97, anterior None None $F_y$ None None 0.96 (both running), and lateral None None $F_x$ None None 0.87 (sidestepping), with the strongest mean recorded across GRF components 0.89, and for GRM 0.65 (both sidestepping). None Conclusion: None These best-case correlations indicate the plausibility of the approach although the range of results was disappointing. The goal to accurately estimate near real-time on-field GRF/M will be improved by the lessons learned in this study. None Significance: None Coaching, medical, and allied health staff could ultimately use this technology to monitor a range of joint loading indicators during game play, with the aim to minimize the occurrence of non-contact injuries in elite and community-level sports.",project-academic
10.1109/ISBI.2019.8759290,2019-04-08,p,IEEE,weakly supervised learning in deformable em image registration using slice interpolation," Alignment of large-scale serial-section electron microscopy (ssEM) images is crucial for successful analysis in nano-scale connectomics. Despite various image registration algorithms proposed in the past, large-scale ssEM alignment remains challenging due to the size and complex nature of the data. Recently, the application of unsupervised machine learning in medical image registration has shown promise in efforts to replace an expensive numerical computation process with a once-deployed feed-forward neural network. However, the anisotropy in most ssEM data makes it difficult to directly adopt such learning-based methods for the registration of these images. Here, we propose a novel deformable image registration approach based on weakly supervised learning that can be applied to registering ssEM images at scale. The proposed method leverages slice interpolation to improve registration between images with sudden and large structural changes. In addition, the proposed method only requires roughly aligned data for training the interpolation network while the deformation network can be trained in an unsupervised fashion. We demonstrate the efficacy of the method on real ssEM datasets.",project-academic
10.1016/J.IPM.2018.04.011,2019-05-01,a,Pergamon,real time processing of social media with sentinel a syndromic surveillance system incorporating deep learning for health classification," Abstract None None Interest in real-time syndromic surveillance based on social media data has greatly increased in recent years. The ability to detect disease outbreaks earlier than traditional methods would be highly useful for public health officials. This paper describes a software system which is built upon recent developments in machine learning and data processing to achieve this goal. The system is built from reusable modules integrated into data processing pipelines that are easily deployable and configurable. It applies deep learning to the problem of classifying health-related tweets and is able to do so with high accuracy. It has the capability to detect illness outbreaks from Twitter data and then to build up and display information about these outbreaks, including relevant news articles, to provide situational awareness. It also provides nowcasting functionality of current disease levels from previous clinical data combined with Twitter data. None The preliminary results are promising, with the system being able to detect outbreaks of influenza-like illness symptoms which could then be confirmed by existing official sources. The Nowcasting module shows that using social media data can improve prediction for multiple diseases over simply using traditional data sources.",project-academic
10.1109/TAFFC.2020.3015018,2020-08-07,a,Institute of Electrical and Electronics Engineers Inc.,beyond mobile apps a survey of technologies for mental well being," Mental health problems are on the rise globally and strain national health systems worldwide. Mental disorders are closely associated with fear of stigma, structural barriers such as financial burden, and lack of available services and resources which often prohibit the delivery of frequent clinical advice and monitoring. Technologies for mental well-being exhibit a range of attractive properties, which facilitate the delivery of state-of-the-art clinical monitoring. This review article provides an overview of traditional techniques followed by their technological alternatives, sensing devices, behaviour changing tools, and feedback interfaces. The challenges presented by these technologies are then discussed with data collection, privacy, and battery life being some of the key issues which need to be carefully considered for the successful deployment of mental health toolkits. Finally, the opportunities this growing research area presents are discussed including the use of portable tangible interfaces combining sensing and feedback technologies. Capitalising on the data these ubiquitous devices can record, state of the art machine learning algorithms can lead to the development of robust clinical decision support tools towards diagnosis and improvement of mental well-being delivery in real-time.",project-academic
10.1101/2020.10.21.20210948,2020-10-25,a,Cold Spring Harbor Laboratory Press,predicting dengue incidence leveraging internet based data sources a case study in 20 cities in brazil," Abstract None The dengue virus affects millions of people every year worldwide, causing large epidemic outbreaks that disrupt people’s lives and severely strain healthcare systems. In the absence of a reliable vaccine against it or an effective treatment to manage the illness in humans, most efforts to combat dengue infections have focused on preventing its vectors, mainly the Aedes aegypti mosquito, from flourishing across the world. These mosquito-control strategies need reliable disease activity surveillance systems to be deployed. Despite significant efforts to estimate dengue incidence using a variety of data sources and methods, little work has been done to understand the relative contribution of the different data sources to improved prediction. Additionally, scholarship on the topic had initially focused on prediction systems at the national- and state-levels, and much remains to be done at the finer spatial resolutions at which health policy interventions often occur. We develop a methodological framework to assess and compare dengue incidence estimates at the city level, and evaluate the performance of a collection of models on 20 different cities in Brazil. The data sources we use towards this end are weekly incidence counts from prior years (seasonal autoregressive terms), weekly-aggregated weather variables, and real-time internet search data. We find that both random forest-based models and LASSO regression-based models effectively leverage these multiple data sources to produce accurate predictions, and that while the performance between them is comparable on average, the former method produces fewer extreme outliers, and can thus be considered more robust. For real-time predictions that assume long delays (6-8 weeks) in the availability of epidemiological data, we find that real-time internet search data are the strongest predictors of dengue incidence, whereas for predictions that assume short delays (1-3 weeks), in which the error rate is halved (as measured by relative RMSE), short-term and seasonal autocorrelation are the dominant predictors. Despite the difficulties inherent to city-level prediction, our framework achieves meaningful and actionable estimates across cities with different demographic, geographic and epidemic characteristics. None Author Summary None As the incidence of infectious diseases like dengue continues to increase throughout the world, tracking their spread in real time poses a significant challenge to local and national health authorities. Accurate incidence data are often difficult to obtain as outbreaks emerge and unfold, both due the partial reach of serological surveillance (especially in rural areas), and due to delays in reporting, which result in post-hoc adjustments to what should have been real-time data. Thus, a range of ‘nowcasting’ tools have been developed to estimate disease trends, using different mathematical and statistical methodologies to fill the temporal data gap. Over the past several years, researchers have investigated how to best incorporate internet search data into predictive models, since these can be obtained in real-time. Still, most such models have been regression-based, and have tended to underperform in cases when epidemiological data are only available after long reporting delays. Moreover, in tropical countries, attention has increasingly turned from testing and applying models at the national level to models at higher spatial resolutions, such as states and cities. Here, we develop machine learning models based on both LASSO regression and on random forest ensembles, and proceed to apply and compare them across 20 cities in Brazil. We find that our methodology produces meaningful and actionable disease estimates at the city level with both underlying model classes, and that the two perform comparably across most metrics, although the ensemble method produces fewer outliers. We also compare model performance and the relative contribution of different data sources across diverse geographic, demographic and epidemic conditions.",project-academic
10.1038/S41746-020-0264-0,2020-04-20,a,Springer Science and Business Media LLC,weak supervision as an efficient approach for automated seizure detection in electroencephalography," Automated seizure detection from electroencephalography (EEG) would improve the quality of patient care while reducing medical costs, but achieving reliably high performance across patients has proven difficult. Convolutional Neural Networks (CNNs) show promise in addressing this problem, but they are limited by a lack of large labeled training datasets. We propose using imperfect but plentiful archived annotations to train CNNs for automated, real-time EEG seizure detection across patients. While these weak annotations indicate possible seizures with precision scores as low as 0.37, they are commonly produced in large volumes within existing clinical workflows by a mixed group of technicians, fellows, students, and board-certified epileptologists. We find that CNNs trained using such weak annotations achieve Area Under the Receiver Operating Characteristic curve (AUROC) values of 0.93 and 0.94 for pediatric and adult seizure onset detection, respectively. Compared to currently deployed clinical software, our model provides a 31% increase (18 points) in F1-score for pediatric patients and a 17% increase (11 points) for adult patients. These results demonstrate that weak annotations, which are sustainably collected via existing clinical workflows, can be leveraged to produce clinically useful seizure detection models.",project-academic
10.1016/J.ESWA.2020.113405,2020-10-01,a,Pergamon,a data analytic framework for physical fatigue management using wearable sensors," Abstract None None The use of expert systems in optimizing and transforming human performance has been limited in practice due to the lack of understanding of how an individual’s performance deteriorates with fatigue accumulation, which can vary based on both the worker and the workplace conditions. As a first step toward realizing the human-centered approach to artificial intelligence and expert systems, this paper lays the foundation for a data analytic approach to managing fatigue in physically-demanding workplaces. The proposed framework capitalizes on continuously collected human performance data from wearable sensor technologies, and is centered around four distinct phases of fatigue: (a) detection, where machine learning methodologies are deployed to detect the occurrence of fatigue; (b) identification, where key features relating to the fatigue occurrence is to be identified; (c) diagnosis, where the fatigue mode is identified based on the knowledge generated in the previous two phases; and (d) recovery, where a suitable intervention is applied to return the worker to mitigate the detrimental effects of fatigue on the worker. Moreover, the framework establishes criteria for feature and machine learning algorithm selection for fatigue management. Two specific application cases of the framework, for two types of manufacturing-related tasks, are presented. Based on the proposed framework and a large number of test sets used in the two case studies, we have shown that: (i) only one wearable sensor is needed for fatigue detection with an average accuracy of  ≥ 0.850 and a random forest model comprised of",project-academic
10.1039/C8AN00065D,2018-04-30,a,The Royal Society of Chemistry,post hoc support vector machine learning for impedimetric biosensors based on weak protein ligand interactions," Impedimetric biosensors for measuring small molecules based on weak/transient interactions between bioreceptors and target analytes are a challenge for detection electronics, particularly in field studies or in the analysis of complex matrices. Protein–ligand binding sensors have enormous potential for biosensing, but achieving accuracy in complex solutions is a major challenge. There is a need for simple post hoc analytical tools that are not computationally expensive, yet provide near real time feedback on data derived from impedance spectra. Here, we show the use of a simple, open source support vector machine learning algorithm for analyzing impedimetric data in lieu of using equivalent circuit analysis. We demonstrate two different protein-based biosensors to show that the tool can be used for various applications. We conclude with a mobile phone-based demonstration focused on the measurement of acetone, an important biomarker related to the onset of diabetic ketoacidosis. In all conditions tested, the open source classifier was capable of performing as well as, or better, than the equivalent circuit analysis for characterizing weak/transient interactions between a model ligand (acetone) and a small chemosensory protein derived from the tsetse fly. In addition, the tool has a low computational requirement, facilitating use for mobile acquisition systems such as mobile phones. The protocol is deployed through Jupyter notebook (an open source computing environment available for mobile phone, tablet or computer use) and the code was written in Python. For each of the applications, we provide step-by-step instructions in English, Spanish, Mandarin and Portuguese to facilitate widespread use. All codes were based on scikit-learn, an open source software machine learning library in the Python language, and were processed in Jupyter notebook, an open-source web application for Python. The tool can easily be integrated with the mobile biosensor equipment for rapid detection, facilitating use by a broad range of impedimetric biosensor users. This post hoc analysis tool can serve as a launchpad for the convergence of nanobiosensors in planetary health monitoring applications based on mobile phone hardware.",project-academic
10.26434/CHEMRXIV.12609899.V1,2020-07-06,a,,the photoswitch dataset a molecular machine learning benchmark for the advancement of synthetic chemistry," The space of synthesizable molecules is greater than $10^{60}$, meaning only a vanishingly small fraction of these molecules have ever been realized in the lab. In order to prioritize which regions of this space to explore next, synthetic chemists need access to accurate molecular property predictions. While great advances in molecular machine learning have been made, there is a dearth of benchmarks featuring properties that are useful for the synthetic chemist. Focussing directly on the needs of the synthetic chemist, we introduce the Photoswitch Dataset, a new benchmark for molecular machine learning where improvements in model performance can be immediately observed in the throughput of promising molecules synthesized in the lab. Photoswitches are a versatile class of molecule for medical and renewable energy applications where a molecule's efficacy is governed by its electronic transition wavelengths. We demonstrate superior performance in predicting these wavelengths compared to both time-dependent density functional theory (TD-DFT), the incumbent first principles quantum mechanical approach, as well as a panel of human experts. Our baseline models are currently being deployed in the lab as part of the decision process for candidate synthesis. It is our hope that this benchmark can drive real discoveries in photoswitch chemistry and that future benchmarks can be introduced to pivot learning algorithm development to benefit more expansive areas of synthetic chemistry.",project-academic
,2020-06-28,a,,the photoswitch dataset a molecular machine learning benchmark for the advancement of synthetic chemistry," The space of synthesizable molecules is greater than $10^{60}$, meaning only a vanishingly small fraction of these molecules have ever been realized in the lab. In order to prioritize which regions of this space to explore next, synthetic chemists need access to accurate molecular property predictions. While great advances in molecular machine learning have been made, there is a dearth of benchmarks featuring properties that are useful for the synthetic chemist. Focussing directly on the needs of the synthetic chemist, we introduce the Photoswitch Dataset, a new benchmark for molecular machine learning where improvements in model performance can be immediately observed in the throughput of promising molecules synthesized in the lab. Photoswitches are a versatile class of molecule for medical and renewable energy applications where a molecule's efficacy is governed by its electronic transition wavelengths. We demonstrate superior performance in predicting these wavelengths compared to both time-dependent density functional theory (TD-DFT), the incumbent first principles quantum mechanical approach, as well as a panel of human experts. Our baseline models are currently being deployed in the lab as part of the decision process for candidate synthesis. It is our hope that this benchmark can drive real discoveries in photoswitch chemistry and that future benchmarks can be introduced to pivot learning algorithm development to benefit more expansive areas of synthetic chemistry.",project-academic
10.1016/J.COMPBIOMED.2020.103792,2020-06-01,a,Pergamon,automated detection of covid 19 cases using deep neural networks with x ray images," The novel coronavirus 2019 (COVID-2019), which first appeared in Wuhan city of China in December 2019, spread rapidly around the world and became a pandemic. It has caused a devastating effect on both daily lives, public health, and the global economy. It is critical to detect the positive cases as early as possible so as to prevent the further spread of this epidemic and to quickly treat affected patients. The need for auxiliary diagnostic tools has increased as there are no accurate automated toolkits available. Recent findings obtained using radiology imaging techniques suggest that such images contain salient information about the COVID-19 virus. Application of advanced artificial intelligence (AI) techniques coupled with radiological imaging can be helpful for the accurate detection of this disease, and can also be assistive to overcome the problem of a lack of specialized physicians in remote villages. In this study, a new model for automatic COVID-19 detection using raw chest X-ray images is presented. The proposed model is developed to provide accurate diagnostics for binary classification (COVID vs. No-Findings) and multi-class classification (COVID vs. No-Findings vs. Pneumonia). Our model produced a classification accuracy of 98.08% for binary classes and 87.02% for multi-class cases. The DarkNet model was used in our study as a classifier for the you only look once (YOLO) real time object detection system. We implemented 17 convolutional layers and introduced different filtering on each layer. Our model (available at (https://github.com/muhammedtalo/COVID-19)) can be employed to assist radiologists in validating their initial screening, and can also be employed via cloud to immediately screen patients.",project-academic
10.1016/J.COMPBIOMED.2018.09.009,2018-11-01,a,Comput Biol Med,arrhythmia detection using deep convolutional neural network with long duration ecg signals," Abstract None None This article presents a new deep learning approach for cardiac arrhythmia (17 classes) detection based on long-duration electrocardiography (ECG) signal analysis. Cardiovascular disease prevention is one of the most important tasks of any health care system as about 50 million people are at risk of heart disease in the world. Although automatic analysis of ECG signal is very popular, current methods are not satisfactory. The goal of our research was to design a new method based on deep learning to efficiently and quickly classify cardiac arrhythmias. Described research are based on 1000 ECG signal fragments from the MIT - BIH Arrhythmia database for one lead (MLII) from 45 persons. Approach based on the analysis of 10-s ECG signal fragments (not a single QRS complex) is applied (on average, 13 times less classifications/analysis). A complete end-to-end structure was designed instead of the hand-crafted feature extraction and selection used in traditional methods. Our main contribution is to design a new 1D-Convolutional Neural Network model (1D-CNN). The proposed method is 1) efficient, 2) fast (real-time classification) 3) non-complex and 4) simple to use (combined feature extraction and selection, and classification in one stage). Deep 1D-CNN achieved a recognition overall accuracy of 17 cardiac arrhythmia disorders (classes) at a level of 91.33% and classification time per single sample of 0.015 s. Compared to the current research, our results are one of the best results to date, and our solution can be implemented in mobile devices and cloud computing.",project-academic
10.1038/NPHOTON.2014.249,2014-12-01,a,Nature Publishing Group,network of time multiplexed optical parametric oscillators as a coherent ising machine," A network of four degenerate optical parametric oscillators (OPOs) is employed to find the ground state of the Ising Hamiltonian. The good performance of the network reveals the potential of OPOs for many similar problems. Finding the ground states of the Ising Hamiltonian1 maps to various combinatorial optimization problems in biology, medicine, wireless communications, artificial intelligence and social network. So far, no efficient classical and quantum algorithm is known for these problems and intensive research is focused on creating physical systems—Ising machines—capable of finding the absolute or approximate ground states of the Ising Hamiltonian2,3,4,5,6. Here, we report an Ising machine using a network of degenerate optical parametric oscillators (OPOs). Spins are represented with above-threshold binary phases of the OPOs and the Ising couplings are realized by mutual injections7. The network is implemented in a single OPO ring cavity with multiple trains of femtosecond pulses and configurable mutual couplings, and operates at room temperature. We programmed a small non-deterministic polynomial time-hard problem on a 4-OPO Ising machine and in 1,000 runs no computational error was detected.",project-academic
10.1038/S41591-019-0715-9,2020-01-06,a,Nature Publishing Group,near real time intraoperative brain tumor diagnosis using stimulated raman histology and deep neural networks," Intraoperative diagnosis is essential for providing safe and effective care during cancer surgery1. The existing workflow for intraoperative diagnosis based on hematoxylin and eosin staining of processed tissue is time, resource and labor intensive2,3. Moreover, interpretation of intraoperative histologic images is dependent on a contracting, unevenly distributed, pathology workforce4. In the present study, we report a parallel workflow that combines stimulated Raman histology (SRH)5-7, a label-free optical imaging method and deep convolutional neural networks (CNNs) to predict diagnosis at the bedside in near real-time in an automated fashion. Specifically, our CNNs, trained on over 2.5 million SRH images, predict brain tumor diagnosis in the operating room in under 150 s, an order of magnitude faster than conventional techniques (for example, 20-30 min)2. In a multicenter, prospective clinical trial (n = 278), we demonstrated that CNN-based diagnosis of SRH images was noninferior to pathologist-based interpretation of conventional histologic images (overall accuracy, 94.6% versus 93.9%). Our CNNs learned a hierarchy of recognizable histologic feature representations to classify the major histopathologic classes of brain tumors. In addition, we implemented a semantic segmentation method to identify tumor-infiltrated diagnostic regions within SRH images. These results demonstrate how intraoperative cancer diagnosis can be streamlined, creating a complementary pathway for tissue diagnosis that is independent of a traditional pathology laboratory.",project-academic
,2015-12-07,,,methods and machine learning systems for predicting the liklihood or risk of having cancer," Embodiments of the present invention relate generally to non-invasive methods and diagnostic tests that measure biomarkers (e.g., tumor antigens), and computer-implemented machine learning methods, apparatuses, systems, and computer-readable media for assessing a likelihood that a patient has a disease, relative to a patient population or a cohort population. In one embodiment, techniques are provided for the use of artificial intelligence / machine learning systems that can incorporate and analyze medical data to perform a risk analysis to determine a likelihood for having cancer. By utilizing algorithms generated from the biomarker levels (e.g., tumor antigens) from large volumes of longitudinal or prospectively collected blood samples (e.g., real world data from one or more regions where blood based tumor biomarker cancer screening is commonplace) together with one or more clinical parameters (e.g. age, smoking history, disease signs or symptoms) a risk level of that patient having a cancer type is provided.",project-academic
10.1016/J.ESWA.2007.06.004,2008-07-01,a,"Pergamon Press, Inc.",design of a hybrid system for the diabetes and heart diseases," Data can be classified according to their properties. Classification is implemented by developing a model with existing records by using sample data. One of the aims of classification is to increase the reliability of the results obtained from the data. Fuzzy and crisp values are used together in medical data. Regarding to this, a new method is presented for classification of data of a medical database in this study. Also a hybrid neural network that includes artificial neural network (ANN) and fuzzy neural network (FNN) was developed. Two real-time problem data were investigated for determining the applicability of the proposed method. The data were obtained from the University of California at Irvine (UCI) machine learning repository. The datasets are Pima Indians diabetes and Cleveland heart disease. In order to evaluate the performance of the proposed method accuracy, sensitivity and specificity performance measures that are used commonly in medical classification studies were used. The classification accuracies of these datasets were obtained by k-fold cross-validation. The proposed method achieved accuracy values 84.24% and 86.8% for Pima Indians diabetes dataset and Cleveland heart disease dataset, respectively. It has been observed that these results are one of the best results compared with results obtained from related previous studies and reported in the UCI web sites.",project-academic
10.1109/JSSC.2013.2253226,2013-04-03,a,IEEE,a low power processor with configurable embedded machine learning accelerators for high order and adaptive analysis of medical sensor signals," Low-power sensing technologies have emerged for acquiring physiologically indicative patient signals. However, to enable devices with high clinical value, a critical requirement is the ability to analyze the signals to extract specific medical information. Yet given the complexities of the underlying processes, signal analysis poses numerous challenges. Data-driven methods based on machine learning offer distinct solutions, but unfortunately the computations are not well supported by traditional DSP. This paper presents a custom processor that integrates a CPU with configurable accelerators for discriminative machine-learning functions. A support-vector-machine accelerator realizes various classification algorithms as well as various kernel functions and kernel formulations, enabling range of points within an accuracy-versus-energy and -memory trade space. An accelerator for embedded active learning enables prospective adaptation of the signal models by utilizing sensed data for patient-specific customization, while minimizing the effort from human experts. The prototype is implemented in 130-nm CMOS and operates from 1.2 V-0.55 V (0.7 V for SRAMs). Medical applications for EEG-based seizure detection and ECG-based cardiac-arrhythmia detection are demonstrated using clinical data, while consuming 273 μJ and 124 μJ per detection, respectively; this represents 62.4t and 144.7t energy reduction compared to an implementation based on the CPU. A patient-adaptive cardiac-arrhythmia detector is also demonstrated, reducing the analysis-effort required for model customization by 20 t.",project-academic
10.1038/S41746-017-0015-Z,2018-04-04,a,Nature Publishing Group,advanced machine learning in action identification of intracranial hemorrhage on computed tomography scans of the head with clinical workflow integration," Intracranial hemorrhage (ICH) requires prompt diagnosis to optimize patient outcomes. We hypothesized that machine learning algorithms could automatically analyze computed tomography (CT) of the head, prioritize radiology worklists and reduce time to diagnosis of ICH. 46,583 head CTs (~2 million images) acquired from 2007–2017 were collected from several facilities across Geisinger. A deep convolutional neural network was trained on 37,074 studies and subsequently evaluated on 9499 unseen studies. The predictive model was implemented prospectively for 3 months to re-prioritize “routine” head CT studies as “stat” on realtime radiology worklists if an ICH was detected. Time to diagnosis was compared between the re-prioritized “stat” and “routine” studies. A neuroradiologist blinded to the study reviewed false positive studies to determine whether the dictating radiologist overlooked ICH. The model achieved an area under the ROC curve of 0.846 (0.837–0.856). During implementation, 94 of 347 “routine” studies were re-prioritized to “stat”, and 60/94 had ICH identified by the radiologist. Five new cases of ICH were identified, and median time to diagnosis was significantly reduced (p < 0.0001) from 512 to 19 min. In particular, one outpatient with vague symptoms on anti-coagulation was found to have an ICH which was treated promptly with reversal of anticoagulation, resulting in a good clinical outcome. Of the 34 false positives, the blinded over-reader identified four probable ICH cases overlooked in original interpretation. In conclusion, an artificial intelligence algorithm can prioritize radiology worklists to reduce time to diagnosis of new outpatient ICH by 96% and may also identify subtle ICH overlooked by radiologists. This demonstrates the positive impact of advanced machine learning in radiology workflow optimization.",project-academic
10.1186/S13336-015-0019-3,2015-03-26,a,BioMed Central,clinical decision support systems for improving diagnostic accuracy and achieving precision medicine," As research laboratories and clinics collaborate to achieve precision medicine, both communities are required to understand mandated electronic health/medical record (EHR/EMR) initiatives that will be fully implemented in all clinics in the United States by 2015. Stakeholders will need to evaluate current record keeping practices and optimize and standardize methodologies to capture nearly all information in digital format. Collaborative efforts from academic and industry sectors are crucial to achieving higher efficacy in patient care while minimizing costs. Currently existing digitized data and information are present in multiple formats and are largely unstructured. In the absence of a universally accepted management system, departments and institutions continue to generate silos of information. As a result, invaluable and newly discovered knowledge is difficult to access. To accelerate biomedical research and reduce healthcare costs, clinical and bioinformatics systems must employ common data elements to create structured annotation forms enabling laboratories and clinics to capture sharable data in real time. Conversion of these datasets to knowable information should be a routine institutionalized process. New scientific knowledge and clinical discoveries can be shared via integrated knowledge environments defined by flexible data models and extensive use of standards, ontologies, vocabularies, and thesauri. In the clinical setting, aggregated knowledge must be displayed in user-friendly formats so that physicians, non-technical laboratory personnel, nurses, data/research coordinators, and end-users can enter data, access information, and understand the output. The effort to connect astronomical numbers of data points, including ‘-omics’-based molecular data, individual genome sequences, experimental data, patient clinical phenotypes, and follow-up data is a monumental task. Roadblocks to this vision of integration and interoperability include ethical, legal, and logistical concerns. Ensuring data security and protection of patient rights while simultaneously facilitating standardization is paramount to maintaining public support. The capabilities of supercomputing need to be applied strategically. A standardized, methodological implementation must be applied to developed artificial intelligence systems with the ability to integrate data and information into clinically relevant knowledge. Ultimately, the integration of bioinformatics and clinical data in a clinical decision support system promises precision medicine and cost effective and personalized patient care.",project-academic
10.1016/J.JMSY.2017.02.011,2017-04-01,a,Elsevier,a fog computing based framework for process monitoring and prognosis in cyber manufacturing," Abstract None None Small- and medium-sized manufacturers, as well as large original equipment manufacturers (OEMs), have faced an increasing need for the development of intelligent manufacturing machines with affordable sensing technologies and data-driven intelligence. Existing monitoring systems and prognostics approaches are not capable of collecting the large volumes of real-time data or building large-scale predictive models that are essential to achieving significant advances in cyber-manufacturing. The objective of this paper is to introduce a new computational framework that enables remote real-time sensing, monitoring, and scalable high performance computing for diagnosis and prognosis. This framework utilizes wireless sensor networks, cloud computing, and machine learning. A proof-of-concept prototype is developed to demonstrate how the framework can enable manufacturers to monitor machine health conditions and generate predictive analytics. Experimental results are provided to demonstrate capabilities and utility of the framework such as how vibrations and energy consumption of pumps in a power plant and CNC machines in a factory floor can be monitored using a wireless sensor network. In addition, a machine learning algorithm, implemented on a public cloud, is used to predict tool wear in milling operations.",project-academic
10.1186/S40537-019-0212-5,2019-12-01,a,SpringerOpen,intelligent video surveillance a review through deep learning techniques for crowd analysis," Big data applications are consuming most of the space in industry and research area. Among the widespread examples of big data, the role of video streams from CCTV cameras is equally important as other sources like social media data, sensor data, agriculture data, medical data and data evolved from space research. Surveillance videos have a major contribution in unstructured big data. CCTV cameras are implemented in all places where security having much importance. Manual surveillance seems tedious and time consuming. Security can be defined in different terms in different contexts like theft identification, violence detection, chances of explosion etc. In crowded public places the term security covers almost all type of abnormal events. Among them violence detection is difficult to handle since it involves group activity. The anomalous or abnormal activity analysis in a crowd video scene is very difficult due to several real world constraints. The paper includes a deep rooted survey which starts from object recognition, action recognition, crowd analysis and finally violence detection in a crowd environment. Majority of the papers reviewed in this survey are based on deep learning technique. Various deep learning methods are compared in terms of their algorithms and models. The main focus of this survey is application of deep learning techniques in detecting the exact count, involved persons and the happened activity in a large crowd at all climate conditions. Paper discusses the underlying deep learning implementation technology involved in various crowd video analysis methods. Real time processing, an important issue which is yet to be explored more in this field is also considered. Not many methods are there in handling all these issues simultaneously. The issues recognized in existing methods are identified and summarized. Also future direction is given to reduce the obstacles identified. The survey provides a bibliographic summary of papers from ScienceDirect, IEEE Xplore and ACM digital library.",project-academic
10.1093/BIOINFORMATICS/BTZ470,2020-01-01,a,Bioinformatics,scaling tree based automated machine learning to biomedical big data with a feature set selector," Motivation None Automated machine learning (AutoML) systems are helpful data science assistants designed to scan data for novel features, select appropriate supervised learning models and optimize their parameters. For this purpose, Tree-based Pipeline Optimization Tool (TPOT) was developed using strongly typed genetic programing (GP) to recommend an optimized analysis pipeline for the data scientist's prediction problem. However, like other AutoML systems, TPOT may reach computational resource limits when working on big data such as whole-genome expression data. None Results None We introduce two new features implemented in TPOT that helps increase the system's scalability: Feature Set Selector (FSS) and Template. FSS provides the option to specify subsets of the features as separate datasets, assuming the signals come from one or more of these specific data subsets. FSS increases TPOT's efficiency in application on big data by slicing the entire dataset into smaller sets of features and allowing GP to select the best subset in the final pipeline. Template enforces type constraints with strongly typed GP and enables the incorporation of FSS at the beginning of each pipeline. Consequently, FSS and Template help reduce TPOT computation time and may provide more interpretable results. Our simulations show TPOT-FSS significantly outperforms a tuned XGBoost model and standard TPOT implementation. We apply TPOT-FSS to real RNA-Seq data from a study of major depressive disorder. Independent of the previous study that identified significant association with depression severity of two modules, TPOT-FSS corroborates that one of the modules is largely predictive of the clinical diagnosis of each individual. None Availability and implementation None Detailed simulation and analysis code needed to reproduce the results in this study is available at https://github.com/lelaboratoire/tpot-fss. Implementation of the new TPOT operators is available at https://github.com/EpistasisLab/tpot. None Supplementary information None Supplementary data are available at Bioinformatics online.",project-academic
10.1016/J.CONENGPRAC.2004.03.002,2005-02-01,a,Pergamon,fault detection for modern diesel engines using signal and process model based methods," Abstract None None Modern Diesel engines with direct fuel injection and turbo charging have shown a significant progress in fuel consumption, emissions and driveability. Together with exhaust gas recirculation and variable geometry turbochargers they became complicated and complex processes. Therefore, fault detection and diagnosis is not easily done and need to be improved. This contribution shows a systematic development of fault detection and diagnosis methods for two system components of Diesel engines, the intake system and the injection system together with the combustion process. By applying semiphysical dynamic process models, identification with special neural networks, signal models and parity equations residuals are generated. Detectable deflections of these residuals lead to symptoms which are the basis for the detection of several faults. Experiments with a 2.0 l Diesel engine on a dynamic test bench as well as in the vehicle have demonstrated the detection and diagnosis of several implemented faults in real time with reasonable calculation effort.",project-academic
10.1016/J.PROCS.2018.04.060,2018-05-01,a,Elsevier BV,real time driver drowsiness detection for android application using deep neural networks techniques," Road crashes and related forms of accidents are a common cause of injury and death among the human population. According to 2015 data from the World Health Organization, road traffic injuries resulted in approximately 1.25 million deaths worldwide, i.e. approximately every 25 seconds an individual will experience a fatal crash. While the cost of traffic accidents in Europe is estimated at around 160 billion Euros, driver drowsiness accounts for approximately 100,000 accidents per year in the United States alone as reported by The American National Highway Traffic Safety Administration (NHTSA). In this paper, a novel approach towards real-time drowsiness detection is proposed. This approach is based on a deep learning method that can be implemented on Android applications with high accuracy. The main contribution of this work is the compression of heavy baseline model to a lightweight model. Moreover, minimal network structure is designed based on facial landmark key point detection to recognize whether the driver is drowsy. The proposed model is able to achieve an accuracy of more than 80%.",project-academic
,2018-11-05,a,,real time driver drowsiness detection for android application using deep neural networks techniques," Road crashes and related forms of accidents are a common cause of injury and death among the human population. According to 2015 data from the World Health Organization, road traffic injuries resulted in approximately 1.25 million deaths worldwide, i.e. approximately every 25 seconds an individual will experience a fatal crash. While the cost of traffic accidents in Europe is estimated at around 160 billion Euros, driver drowsiness accounts for approximately 100,000 accidents per year in the United States alone as reported by The American National Highway Traffic Safety Administration (NHTSA). In this paper, a novel approach towards real-time drowsiness detection is proposed. This approach is based on a deep learning method that can be implemented on Android applications with high accuracy. The main contribution of this work is the compression of heavy baseline model to a lightweight model. Moreover, minimal network structure is designed based on facial landmark key point detection to recognize whether the driver is drowsy. The proposed model is able to achieve an accuracy of more than 80%. Keywords: Driver Monitoring System; Drowsiness Detection; Deep Learning; Real-time Deep Neural Network; Android.",project-academic
,2018-07-09,a,,convolutional recurrent neural networks for glucose prediction," Control of blood glucose is essential for diabetes management. Current digital therapeutic approaches for subjects with Type 1 diabetes mellitus (T1DM) such as the artificial pancreas and insulin bolus calculators leverage machine learning techniques for predicting subcutaneous glucose for improved control. Deep learning has recently been applied in healthcare and medical research to achieve state-of-the-art results in a range of tasks including disease diagnosis, and patient state prediction among others. In this work, we present a deep learning model that is capable of forecasting glucose levels with leading accuracy for simulated patient cases (RMSE = 9.38$\pm$0.71 [mg/dL] over a 30-minute horizon, RMSE = 18.87$\pm$2.25 [mg/dL] over a 60-minute horizon) and real patient cases (RMSE = 21.07$\pm$2.35 [mg/dL] for 30-minute, RMSE = 33.27$\pm$4.79\% for 60-minute). In addition, the model provides competitive performance in providing effective prediction horizon ($PH_{eff}$) with minimal time lag both in a simulated patient dataset ($PH_{eff}$ = 29.0$\pm$0.7 for 30-min and $PH_{eff}$ = 49.8$\pm$2.9 for 60-min) and in a real patient dataset ($PH_{eff}$ = 19.3$\pm$3.1 for 30-min and $PH_{eff}$ = 29.3$\pm$9.4 for 60-min). This approach is evaluated on a dataset of 10 simulated cases generated from the UVa/Padova simulator and a clinical dataset of 10 real cases each containing glucose readings, insulin bolus, and meal (carbohydrate) data. Performance of the recurrent convolutional neural network is benchmarked against four algorithms. The proposed algorithm is implemented on an Android mobile phone, with an execution time of $6$ms on a phone compared to an execution time of $780$ms on a laptop.",project-academic
10.1109/JBHI.2019.2908488,2020-02-01,a,IEEE,convolutional recurrent neural networks for glucose prediction," Control of blood glucose is essential for diabetes management. Current digital therapeutic approaches for subjects with type 1 diabetes mellitus such as the artificial pancreas and insulin bolus calculators leverage machine learning techniques for predicting subcutaneous glucose for improved control. Deep learning has recently been applied in healthcare and medical research to achieve state-of-the-art results in a range of tasks including disease diagnosis, and patient state prediction among others. In this paper, we present a deep learning model that is capable of forecasting glucose levels with leading accuracy for simulated patient cases (root-mean-square error (RMSE) = 9.38  None $\pm$ None None 0.71 [mg/dL] over a 30-min horizon, RMSE = 18.87  None $\pm$ None None 2.25 [mg/dL] over a 60-min horizon) and real patient cases (RMSE = 21.07  None $\pm$ None None 2.35 [mg/dL] for 30 min, RMSE = 33.27  None $\pm$ None None 4.79% for 60 min). In addition, the model provides competitive performance in providing effective prediction horizon ( None $\text{PH}_{\text{eff}}$ None ) with minimal time lag both in a simulated patient dataset ( None $\text{PH}_{\text{eff}}$ None None = 29.0 None None $\pm$ None None 0.7 for 30 min and None None $\text{PH}_{\text{eff}}$ None None = 49.8  None $\pm$ None None 2.9 for 60 min) and in a real patient dataset ( None $\text{PH}_{\text{eff}}$ None None = 19.3  None $\pm$ None None 3.1 for 30 min and None None $\text{PH}_{\text{eff}}$ None None = 29.3  None $\pm$ None None 9.4 for 60 min). This approach is evaluated on a dataset of ten simulated cases generated from the UVA/Padova simulator and a clinical dataset of ten real cases each containing glucose readings, insulin bolus, and meal (carbohydrate) data. Performance of the recurrent convolutional neural network is benchmarked against four algorithms. The proposed algorithm is implemented on an Android mobile phone, with an execution time of 6 ms on a phone compared to an execution time of 780 ms on a laptop.",project-academic
10.1016/J.ENG.2018.11.027,2019-05-08,a,Elsevier,the state of the art of data science and engineering in structural health monitoring," Abstract None None Structural health monitoring (SHM) is a multi-discipline field that involves the automatic sensing of structural loads and response by means of a large number of sensors and instruments, followed by a diagnosis of the structural health based on the collected data. Because an SHM system implemented into a structure automatically senses, evaluates, and warns about structural conditions in real time, massive data are a significant feature of SHM. The techniques related to massive data are referred to as data science and engineering, and include acquisition techniques, transition techniques, management techniques, and processing and mining algorithms for massive data. This paper provides a brief review of the state of the art of data science and engineering in SHM as investigated by these authors, and covers the compressive sampling-based data-acquisition algorithm, the anomaly data diagnosis approach using a deep learning algorithm, crack identification approaches using computer vision techniques, and condition assessment approaches for bridges using machine learning algorithms. Future trends are discussed in the conclusion.",project-academic
10.1016/J.ISATRA.2018.12.025,2019-12-01,a,ISA Trans,deep residual learning based fault diagnosis method for rotating machinery," Effective fault diagnosis of rotating machinery has always been an important issue in real industries. In the recent years, data-driven fault diagnosis methods such as neural networks have been receiving increasing attention due to their great merits of high diagnosis accuracy and easy implementation. However, it is mostly difficult to fully train a deep neural network since gradients in optimization may vanish or explode during back-propagation, which results in deterioration and noticeable variance in model performance. In fault diagnosis researches, larger data sequence of machinery vibration signal containing sufficient information is usually preferred and consequently, deep models with large capacity are generally adopted. In order to improve network training, a residual learning algorithm is proposed in this paper. The proposed architecture significantly improves the information flow throughout the network, which is well suited for processing machinery vibration signal with variable sequential length. Little prior expertise on fault diagnosis and signal processing is required, that facilitates industrial applications of the proposed method. Experiments on a popular rolling bearing dataset are implemented to validate the proposed method. The results of this study suggest that the proposed intelligent fault diagnosis method for rotating machinery offers a new and promising approach.",project-academic
10.2196/12214,2019-04-04,a,JMIR Public Health Surveill,improved real time influenza surveillance using internet search data in eight latin american countries," Background: Novel influenza surveillance systems that leverage Internet-based real-time data sources including Internet search frequencies, social-network information, and crowd-sourced flu surveillance tools have shown improved accuracy over the past few years in data-rich countries like the United States. These systems not only track flu activity accurately, but they also report flu estimates a week or more ahead of the publication of reports produced by healthcare-based systems, such as those implemented and managed by the Centers for Disease Control and Prevention. Previous work has shown that the predictive capabilities of novel flu surveillance systems, like Google Flu Trends (GFT), in developing countries in Latin America have not yet delivered acceptable flu estimates. Objective: The aim of this study was to show that recent methodological improvements on the use of Internet search engine information to track diseases can lead to improved retrospective flu estimates in multiple countries in Latin America. Methods: A machine learning-based methodology that uses flu-related Internet search activity and historical information to monitor flu activity, named ARGO (AutoRegression with Google search), was extended to generate flu predictions for 8 Latin American countries (Argentina, Bolivia, Brazil, Chile, Mexico, Paraguay, Peru, and Uruguay) for the time period: January 2012 to December of 2016. These retrospective (out-of-sample) Influenza activity predictions were compared with historically observed flu suspected cases in each country, as reported by Flunet, an influenza surveillance database maintained by the World Health Organization. For a baseline comparison, retrospective (out-of-sample) flu estimates were produced for the same time period using autoregressive models that only leverage historical flu activity information. Results: Our results show that ARGO-like models’ predictive power outperform autoregressive models in 6 out of 8 countries in the 2012-2016 time period. Moreover, ARGO significantly improves on historical flu estimates produced by the now discontinued GFT for the time period of 2012-2015, where GFT information is publicly available. Conclusions: We demonstrate here that a self-correcting machine learning method, leveraging Internet-based disease-related search activity and historical flu trends, has the potential to produce reliable and timely flu estimates in multiple Latin American countries. This methodology may prove helpful to local public health officials who design and implement interventions aimed at mitigating the effects of influenza outbreaks. Our methodology generally outperforms both the now-discontinued tool GFT, and autoregressive methodologies that exploit only historical flu activity to produce future disease estimates.",project-academic
10.1016/J.COMPMEDIMAG.2016.05.004,2017-01-01,a,Comput Med Imaging Graph,retinal vessel segmentation in colour fundus images using extreme learning machine," Attributes of the retinal vessel play important role in systemic conditions and ophthalmic diagnosis. In this paper, a supervised method based on Extreme Learning Machine (ELM) is proposed to segment retinal vessel. Firstly, a set of 39-D discriminative feature vectors, consisting of local features, morphological features, phase congruency, Hessian and divergence of vector fields, is extracted for each pixel of the fundus image. Then a matrix is constructed for pixel of the training set based on the feature vector and the manual labels, and acts as the input of the ELM classifier. The output of classifier is the binary retinal vascular segmentation. Finally, an optimization processing is implemented to remove the region less than 30 pixels which is isolated from the retinal vascilar. The experimental results testing on the public Digital Retinal Images for Vessel Extraction (DRIVE) database demonstrate that the proposed method is much faster than the other methods in segmenting the retinal vessels. Meanwhile the average accuracy, sensitivity, and specificity are 0.9607, 0.7140 and 0.9868, respectively. Moreover the proposed method exhibits high speed and robustness on a new Retinal Images for Screening (RIS) database. Therefore it has potential applications for real-time computer-aided diagnosis and disease screening.",project-academic
10.1609/AAAI.V33I01.33011126,2019-07-17,p,Association for the Advancement of Artificial Intelligence (AAAI),gamenet graph augmented memory networks for recommending medication combination," Recent progress in deep learning is revolutionizing the healthcare domain including providing solutions to medication recommendations, especially recommending medication combination for patients with complex health conditions. Existing approaches either do not customize based on patient health history, or ignore existing knowledge on drug-drug interactions (DDI) that might lead to adverse outcomes. To fill this gap, we propose the Graph Augmented Memory Networks (GAMENet), which integrates the drug-drug interactions knowledge graph by a memory module implemented as a graph convolutional networks, and models longitudinal patient records as the query. It is trained end-to-end to provide safe and personalized recommendation of medication combination. We demonstrate the effectiveness and safety of GAMENet by comparing with several state-of-the-art methods on real EHR data. GAMENet outperformed all baselines in all effectiveness measures, and also achieved 3.60% DDI rate reduction from existing EHR data.",project-academic
,2018-09-06,a,,gamenet graph augmented memory networks for recommending medication combination," Recent progress in deep learning is revolutionizing the healthcare domain including providing solutions to medication recommendations, especially recommending medication combination for patients with complex health conditions. Existing approaches either do not customize based on patient health history, or ignore existing knowledge on drug-drug interactions (DDI) that might lead to adverse outcomes. To fill this gap, we propose the Graph Augmented Memory Networks (GAMENet), which integrates the drug-drug interactions knowledge graph by a memory module implemented as a graph convolutional networks, and models longitudinal patient records as the query. It is trained end-to-end to provide safe and personalized recommendation of medication combination. We demonstrate the effectiveness and safety of GAMENet by comparing with several state-of-the-art methods on real EHR data. GAMENet outperformed all baselines in all effectiveness measures, and also achieved 3.60% DDI rate reduction from existing EHR data.",project-academic
10.22489/CINC.2017.066-138,2017-09-14,p,Computing in Cardiology,robust ecg signal classification for detection of atrial fibrillation using a novel neural network," Electrocardiograms (ECG) provide a non-invasive approach for clinical diagnosis in patients with cardiac problems, particularly atrial fibrillation (AF). Robust, automatic AF detection in clinics remains challenging. Deep learning has emerged as an effective tool for handling complex data analysis with minimal pre- and post-processing. A 16-layer 1D Convolutional Neural Network (CNN) was designed to classify the ECGs including AF. One of the key advances of the proposed CNN was that skip connections were employed to enhance the rate of information transfer throughout the network by connecting layers earlier in the network with layers later in the network. Skip connections led to a significant increase in the feature learning capabilities of the CNN as well as speeding up the training time. For comparisons, we also have implemented recurrent neural networks (RNN) and spectrogram learning. The CNN was trained on 8,528 ECGs and tested on 3,685 ECGs ranging from 9 to 60 seconds in length. The proposed 16-layer CNN outperformed RNNs and spectrogram learning. The training of the CNN took 2 hours on a Titan XPascal GPU (NVidia) with 3840 cores. The testing accuracy for the CNN was 82% and the runtime was ∼0.01 seconds for each signal classification. Particularly, the proposed CNN identified normal rhythm, AF and other rhythms with an accuracy of 90%, 82% and 75% respectively. We have demonstrated a novel CNN with skip connections to perform efficient, automatic ECG signal classification that could potentially aid robust patient diagnosis in real time.",project-academic
10.1109/TII.2019.2915846,2020-01-01,a,IEEE,a global manufacturing big data ecosystem for fault detection in predictive maintenance," Artificial intelligence, big data, machine learning, cloud computing, and Internet of Things (IoT) are terms which have driven the fourth industrial revolution. The digital revolution has transformed the manufacturing industry into smart manufacturing through the development of intelligent systems. In this paper, a big data ecosystem is presented for the implementation of fault detection and diagnosis in predictive maintenance with real industrial big data gathered directly from large-scale global manufacturing plants, aiming to provide a complete architecture which could be used in industrial IoT-based smart manufacturing in an industrial 4.0 system. The proposed architecture overcomes multiple challenges including big data ingestion, integration, transformation, storage, analytics, and visualization in a real-time environment using various technologies such as the data lake, NoSQL database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and other techniques. Transformation protocols, authentication, and data encryption methods are also utilized to address data and network security issues. A MapReduce-based distributed PCA model is designed for fault detection and diagnosis. In a large-scale manufacturing system, not all kinds of failure data are accessible, and the absence of labels precludes all the supervised methods in the predictive phase. Furthermore, the proposed framework takes advantage of some of the characteristics of PCA such as its ease of implementation on Spark, its simple algorithmic structure, and its real-time processing ability. All these elements are essential for smart manufacturing in the evolution to Industry 4.0. The proposed detection system has been implemented into the real-time industrial production system in a cooperated company, running for several years, and the results successfully provide an alarm warning several days before the fault happens. A test case involving several outages in 2014 is reported and analyzed in detail during the experiment section.",project-academic
10.3390/S19092167,2019-05-10,a,Multidisciplinary Digital Publishing Institute,image thresholding improves 3 dimensional convolutional neural network diagnosis of different acute brain hemorrhages on computed tomography scans," Intracranial hemorrhage is a medical emergency that requires urgent diagnosis and immediate treatment to improve patient outcome. Machine learning algorithms can be used to perform medical image classification and assist clinicians in diagnosing radiological scans. In this paper, we apply 3-dimensional convolutional neural networks (3D CNN) to classify computed tomography (CT) brain scans into normal scans (N) and abnormal scans containing subarachnoid hemorrhage (SAH), intraparenchymal hemorrhage (IPH), acute subdural hemorrhage (ASDH) and brain polytrauma hemorrhage (BPH). The dataset used consists of 399 volumetric CT brain images representing approximately 12,000 images from the National Neuroscience Institute, Singapore. We used a 3D CNN to perform both 2-class (normal versus a specific abnormal class) and 4-class classification (between normal, SAH, IPH, ASDH). We apply image thresholding at the image pre-processing step, that improves 3D CNN classification accuracy and performance by accentuating the pixel intensities that contribute most to feature discrimination. For 2-class classification, the F1 scores for various pairs of medical diagnoses ranged from 0.706 to 0.902 without thresholding. With thresholding implemented, the F1 scores improved and ranged from 0.919 to 0.952. Our results are comparable to, and in some cases, exceed the results published in other work applying 3D CNN to CT or magnetic resonance imaging (MRI) brain scan classification. This work represents a direct application of a 3D CNN to a real hospital scenario involving a medically emergent CT brain diagnosis.",project-academic
10.1016/J.ENVSOFT.2003.03.007,2004-09-01,a,Elsevier,designing and building real environmental decision support systems," The complexity of environmental problems makes necessary the development and application of new tools capable of processing not only numerical aspects, but also experience from experts and wide public participation, which are all needed in decision-making processes. Environmental decision support systems (EDSSs) are among the most promising approaches to confront this complexity. The fact that different tools (artificial intelligence techniques, statistical/numerical methods, geographical information systems, and environmental ontologies) can be integrated under different architectures confers EDSSs the ability to confront complex problems, and the capability to support learning and decision-making processes. In this paper, we present our experience, obtained over the last 10 years, in designing and building two real EDSSs, one for wastewater plant supervision, and one for the selection of wastewater treatment systems for communities with less than 2000 inhabitants. The flow diagram followed to build the EDSS is presented for each of the systems, together with a discussion of the tasks involved in each step (problem analysis, data collection and knowledge acquisition, model selection, model implementation, and EDSS validation). In addition, the architecture used is presented, showing how the five levels on which it is based (data gathering, diagnosis, decision support, plans, and actions) have been implemented. Finally, we present our opinion on the research issues that need to be addressed in order to improve the ability of EDSSs to cope with complexity in environmental problems (integration of data and knowledge, improvement of knowledge acquisition methods, new protocols to share and reuse knowledge, development of benchmarks, involvement of end-users), thus increasing our understanding of the environment and contributing to the sustainable development of society.  2003 Elsevier Ltd. All rights reserved.",project-academic
10.7717/PEERJ.6257,2019-01-25,a,PeerJ,a scalable discrete time survival model for neural networks," There is currently great interest in applying neural networks to prediction tasks in medicine. It is important for predictive models to be able to use survival data, where each patient has a known follow-up time and event/censoring indicator. This avoids information loss when training the model and enables generation of predicted survival curves. In this paper, we describe a discrete-time survival model that is designed to be used with neural networks, which we refer to as Nnet-survival. The model is trained with the maximum likelihood method using mini-batch stochastic gradient descent (SGD). The use of SGD enables rapid convergence and application to large datasets that do not fit in memory. The model is flexible, so that the baseline hazard rate and the effect of the input data on hazard probability can vary with follow-up time. It has been implemented in the Keras deep learning framework, and source code for the model and several examples is available online. We demonstrate the performance of the model on both simulated and real data and compare it to existing models Cox-nnet and Deepsurv.",project-academic
10.1007/S11606-019-05035-1,2019-05-14,a,J Gen Intern Med,ten ways artificial intelligence will transform primary care," Artificial intelligence (AI) is poised as a transformational force in healthcare. This paper presents a current environmental scan, through the eyes of primary care physicians, of the top ten ways AI will impact primary care and its key stakeholders. We discuss ten distinct problem spaces and the most promising AI innovations in each, estimating potential market sizes and the Quadruple Aims that are most likely to be affected. Primary care is where the power, opportunity, and future of AI are most likely to be realized in the broadest and most ambitious scale. We propose how these AI-powered innovations must augment, not subvert, the patient-physician relationship for physicians and patients to accept them. AI implemented poorly risks pushing humanity to the margins; done wisely, AI can free up physicians' cognitive and emotional space for patients, and shift the focus away from transactional tasks to personalized care. The challenge will be for humans to have the wisdom and willingness to discern AI's optimal role in twenty-first century healthcare, and to determine when it strengthens and when it undermines human healing. Ongoing research will determine the impact of AI technologies in achieving better care, better health, lower costs, and improved well-being of the workforce.",project-academic
10.1109/JTEHM.2018.2869398,2018-09-10,a,Institute of Electrical and Electronics Engineers Inc.,integrating brain implants with local and distributed computing devices a next generation epilepsy management system," Brain stimulation has emerged as an effective treatment for a wide range of neurological and psychiatric diseases. Parkinson’s disease, epilepsy, and essential tremor have FDA indications for electrical brain stimulation using intracranially implanted electrodes. Interfacing implantable brain devices with local and cloud computing resources have the potential to improve electrical stimulation efficacy, disease tracking, and management. Epilepsy, in particular, is a neurological disease that might benefit from the integration of brain implants with off-the-body computing for tracking disease and therapy. Recent clinical trials have demonstrated seizure forecasting, seizure detection, and therapeutic electrical stimulation in patients with drug-resistant focal epilepsy. In this paper, we describe a next-generation epilepsy management system that integrates local handheld and cloud-computing resources wirelessly coupled to an implanted device with embedded payloads (sensors, intracranial EEG telemetry, electrical stimulation, classifiers, and control policy implementation). The handheld device and cloud computing resources can provide a seamless interface between patients and physicians, and realtime intracranial EEG can be used to classify brain state (wake/sleep, preseizure, and seizure), implement control policies for electrical stimulation, and track patient health. This system creates a flexible platform in which low demand analytics requiring fast response times are embedded in the implanted device and more complex algorithms are implemented in offthebody local and distributed cloud computing environments. The system enables tracking and management of epileptic neural networks operating over time scales ranging from milliseconds to months.",project-academic
,2018-04-27,b,,sufficient dimension reduction methods and applications with r," Sufficient dimension reduction is a rapidly developing research field that has wide applications in regression diagnostics, data visualization, machine learning, genomics, image processing, pattern recognition, and medicine, because they are fields that produce large datasets with a large number of variables. Sufficient Dimension Reduction: Methods and Applications with R introduces the basic theories and the main methodologies, provides practical and easy-to-use algorithms and computer codes to implement these methodologies, and surveys the recent advances at the frontiers of this field.

Features


Provides comprehensive coverage of this emerging research field.


Synthesizes a wide variety of dimension reduction methods under a few unifying principles such as projection in Hilbert spaces, kernel mapping, and von Mises expansion.


Reflects most recent advances such as nonlinear sufficient dimension reduction, dimension folding for tensorial data, as well as sufficient dimension reduction for functional data.


Includes a set of computer codes written in R that are easily implemented by the readers.


Uses real data sets available online to illustrate the usage and power of the described methods.


Sufficient dimension reduction has undergone momentous development in recent years, partly due to the increased demands for techniques to process high-dimensional data, a hallmark of our age of Big Data. This book will serve as the perfect entry into the field for the beginning researchers or a handy reference for the advanced ones. 

The author

Bing Li obtained his Ph.D. from the University of Chicago. He is currently a Professor of Statistics at the Pennsylvania State University. His research interests cover sufficient dimension reduction, statistical graphical models, functional data analysis, machine learning, estimating equations and quasilikelihood, and robust statistics. He is a fellow of the Institute of Mathematical Statistics and the American Statistical Association. He is an Associate Editor for The Annals of Statistics and the Journal of the American Statistical Association.",project-academic
10.1145/2623330.2630805,2014-08-24,p,,computational epidemiology," As recent pandemics such as SARS and the Swine Flu outbreak have shown, diseases spread very fast in today's interconnected world, making public health an important research area. Some of the basic questions are: How can an outbreak be contained before it becomes an epidemic, and what disease surveillance strategies should be implemented? These problems have been studied traditionally using differential equation methods, which are amenable to analysis and closed form solutions. However, these models are based on complete mixing assumptions, which do not hold for realistic populations, thereby limiting their utility.In this tutorial, we focus on an approach based on diffusion processes on complex networks. This captures more realistic populations, but leads to novel mathematical and computational challenges. The structure of the underlying networks has a significant impact on the dynamical properties, motivating the need for improved network models, and efficient algorithms for computing network and dynamical properties that scale to large networks. We provide an overview of the state of the art in computational epidemiology, which is a multi-disciplinary research area, that overlaps different areas in computer science, including data mining, machine learning, high performance computing and theoretical computer science, as well as mathematics, economics and statistics. Specifically, we will discuss mathematical and computational models, problems of inference, forecasting and state assessment, and epidemic containment.",project-academic
10.1109/MCOM.2011.5783996,2011-06-07,a,IEEE,an adaptive user interface based on spatiotemporal structure learning," We developed a user interface prototype for the Android smartphone, which recommends a number of applications to best match the user's context. To consider the user's context of use, we utilized 5 prototypical variables; time, location, weather, emotion, and activities. The developed system derives the best three recommended applications based on the results of supervised machine learning from such data sets. To consider the history of past context information, in addition to the current one, we developed a novel and effective probabilistic learning and inference algorithm named ""Spatiotemporal Structure Learning."" By extending Naive Bayesian Classifier, the spatiotemporal structure learning can create a probability model which represents relationship between time-series contextual variables. We implemented a prototype system which shows the current context and the inferred recommendation of applications. For the prototype system, we developed an Android widget application for the user interface and a Java-based server application which learns structure from training data and provides inference results in real time. To gather training data and evaluate the proposed system, we conducted a pilot study which showed 69 percent accuracy in predicting the user's application usage. The prototype demonstrated the feasibility of an adaptive user interface applied to a state of the art smartphone. We also believe that the suggested spatiotemporal structure learning can be applied to number of application areas including healthcare or energy problems.",project-academic
10.1109/CCNC.2011.5766642,2011-05-12,p,IEEE,an adaptive user interface based on spatiotemporal structure learning," We developed a user interface prototype for the Android smartphone which recommends a number of applications to best match the user's context. To consider the user's context of use, we utilized 5 prototypical variables; time, location, weather, emotion, and activities. The developed system derives the best three recommended applications based on the results of supervised learning from such data sets. To consider the history of past context information, in addition to the current one, we developed a novel and effective probabilistic learning and inference algorithm named “Spatiotemporal Structure Learning”. By extending Naive Bayesian Classifier, the spatiotemporal structure learning can create a probability model which represents relationship between time-series contextual variables. We implemented a prototype system which shows the current context and the inferred recommendation of applications. For the prototype system, we developed an Android widget application for the user interface and a Java-based server application which learns structure from training data and provides inference results in real-time. To gather training data and evaluate proposed system, we conducted a pilot study which showed 69% of accuracy in predicting the user's application usage. The prototype demonstrated the feasibility of an adaptive user interface applied to a state of the art smartphone. We also believe that the suggested spatiotemporal structure learning can be applied to number of application areas including healthcare or energy problems.",project-academic
10.1016/J.ESWA.2017.05.073,2017-11-15,a,Elsevier,medical image analysis using wavelet transform and deep belief networks," Abstract None None This paper introduces a three-step framework for classifying multiclass radiography images. The first step utilizes a de-noising technique based on wavelet transform (WT) and the statistical Kolmogorov Smirnov (KS) test to remove noise and insignificant features of the images. An unsupervised deep belief network (DBN) is designed for learning the unlabelled features in the second step. Although small-scale DBNs have demonstrated significant potential, the computational cost of training the restricted Boltzmann machine is a major issue when scaling to large networks. Moreover, noise in radiography images can cause a significant corruption of information that hinders the performance of DBNs. The combination of WT and KS test in the first step helps improve performance of DBNs. Discriminative feature subsets obtained in the first two steps serve as inputs into classifiers in the third step for evaluations. Five frequently used classifiers including naive Bayes, radial basis function network, random forest, sequential minimal optimization, and support vector machine and four different case studies are implemented for experiments using the Image Retrieval in Medical Application data set. The experimental results show that the three-step framework has significantly reduced computational cost and yielded a great performance for multiclass radiography image classification. Along with effective applications in image processing in other fields published in the literature, deep learning network in this paper has again demonstrated its robustness in handling a complex set of medical images. This implies that the proposed approach can be implemented in real practice for analysing noisy radiography images, which have many useful medical applications such as diagnosis of diseases related to lung, breast, musculoskeletal or pediatric studies.",project-academic
10.1007/S10916-018-0934-5,2018-05-01,a,Springer US,behavioral modeling for mental health using machine learning algorithms," Mental health is an indicator of emotional, psychological and social well-being of an individual. It determines how an individual thinks, feels and handle situations. Positive mental health helps one to work productively and realize their full potential. Mental health is important at every stage of life, from childhood and adolescence through adulthood. Many factors contribute to mental health problems which lead to mental illness like stress, social anxiety, depression, obsessive compulsive disorder, drug addiction, and personality disorders. It is becoming increasingly important to determine the onset of the mental illness to maintain proper life balance. The nature of machine learning algorithms and Artificial Intelligence (AI) can be fully harnessed for predicting the onset of mental illness. Such applications when implemented in real time will benefit the society by serving as a monitoring tool for individuals with deviant behavior. This research work proposes to apply various machine learning algorithms such as support vector machines, decision trees, na?ve bayes classifier, K-nearest neighbor classifier and logistic regression to identify state of mental health in a target group. The responses obtained from the target group for the designed questionnaire were first subject to unsupervised learning techniques. The labels obtained as a result of clustering were validated by computing the Mean Opinion Score. These cluster labels were then used to build classifiers to predict the mental health of an individual. Population from various groups like high school students, college students and working professionals were considered as target groups. The research presents an analysis of applying the aforementioned machine learning algorithms on the target groups and also suggests directions for future work.",project-academic
10.1109/ICASSP.2016.7472881,2016-03-20,p,IEEE,heart trend an affordable heart condition monitoring system exploiting morphological pattern," In this paper we leverage the power of smartphone to enable proactive in-house heart condition monitoring. We introduce Heart-Trend, a nonparametric model to analyze and detect heart abnormality conditions like arrhythmia from photoplethysmogram (PPG) signal. It does on-demand heart status monitoring using smartphones (can also be implemented in PC/ICU monitors) and facilitates timely detection of heart condition deterioration to permit early diagnosis and prevention of fatal heart diseases. Proposed robust anomaly analytics engine accurately detects the morphological trend to find abnormal heart condition in real time through machine learning based trend prediction. PPG signal is frequently corrupted by ambient noise, and motion artifacts, which lead to high amount of false alarms. We introduce precise denoising technique that identifies and eliminates the corrupted segments of clinical signal to minimize its impact on the decision process and analytics. We demonstrate that Heart-Trend ensures high detection capability with lower false alarm rates.",project-academic
10.1016/J.SCITOTENV.2018.08.221,2019-01-15,a,Elsevier,design and implementation of a hybrid model based on two layer decomposition method coupled with extreme learning machines to support real time environmental monitoring of water quality parameters," Accurate prediction of water quality parameters plays a crucial and decisive role in environmental monitoring, ecological systems sustainability, human health, aquaculture and improved agricultural practices. In this study a new hybrid two-layer decomposition model based on the complete ensemble empirical mode decomposition algorithm with adaptive noise (CEEMDAN) and the variational mode decomposition (VMD) algorithm coupled with extreme learning machines (ELM) and also least square support vector machine (LSSVM) was designed to support real-time environmental monitoring of water quality parameters, i.e. chlorophyll-a (Chl-a) and dissolved oxygen (DO) in a Lake reservoir. Daily measurements of Chl-a and DO for June 2012–May 2013 were employed where the partial autocorrelation function was applied to screen the relevant inputs for the model construction. The variables were then split into training, validation and testing subsets where the first stage of the model testing captured the superiority of the ELM over the LSSVM algorithm. To improve these standalone predictive models, a second stage implemented a two-layer decomposition with the model inputs decomposed in the form of high and low frequency oscillations, represented by the intrinsic mode function (IMF) through the CEEMDAN algorithm. The highest frequency component, IMF1 was further decomposed with the VMD algorithm to segregate key model input features, leading to a two-layer hybrid VMD-CEEMDAN model. The VMD-CEEMDAN-ELM model was able to reduce the root mean square and the mean absolute error by about 14.04% and 7.12% for the Chl-a estimation and about 5.33% and 4.30% for the DO estimation, respectively, compared with the standalone counterparts. Overall, the developed methodology demonstrates the robustness of the two-phase VMD-CEEMDAN-ELM model in identifying and analyzing critical water quality parameters with a limited set of model construction data over daily horizons, and thus, to actively support environmental monitoring tasks, especially in case of high-frequency, and relatively complex, real-time datasets.",project-academic
,2018-06-20,a,,generative adversarial networks for image to image translation on multi contrast mr images a comparison of cyclegan and unit," In medical imaging, a general problem is that it is costly and time consuming to collect high quality data from healthy and diseased subjects. Generative adversarial networks (GANs) is a deep learning method that has been developed for synthesizing data. GANs can thereby be used to generate more realistic training data, to improve classification performance of machine learning algorithms. Another application of GANs is image-to-image translations, e.g. generating magnetic resonance (MR) images from computed tomography (CT) images, which can be used to obtain multimodal datasets from a single modality. Here, we evaluate two unsupervised GAN models (CycleGAN and UNIT) for image-to-image translation of T1- and T2-weighted MR images, by comparing generated synthetic MR images to ground truth images. We also evaluate two supervised models; a modification of CycleGAN and a pure generator model. A small perceptual study was also performed to evaluate how visually realistic the synthesized images are. It is shown that the implemented GAN models can synthesize visually realistic MR images (incorrectly labeled as real by a human). It is also shown that models producing more visually realistic synthetic images not necessarily have better quantitative error measurements, when compared to ground truth data. Code is available at this https URL",project-academic
10.1117/1.JMI.8.4.041204,2021-01-28,a,SPIE,deepamo a multi slice multi view anthropomorphic model observer for visual detection tasks performed on volume images," Purpose: We propose a deep learning-based anthropomorphic model observer (DeepAMO) for image quality evaluation of multi-orientation, multi-slice image sets with respect to a clinically realistic 3D defect detection task.
Approach: The DeepAMO is developed based on a hypothetical model of the decision process of a human reader performing a detection task using a 3D volume. The DeepAMO is comprised of three sequential stages: defect segmentation, defect confirmation (DC), and rating value inference. The input to the DeepAMO is a composite image, typical of that used to view 3D volumes in clinical practice. The output is a rating value designed to reproduce a human observer’s defect detection performance. In stages 2 and 3, we propose: (1) a projection-based DC block that confirms defect presence in two 2D orthogonal orientations and (2) a calibration method that “learns” the mapping from the features of stage 2 to the distribution of observer ratings from the human observer rating data (thus modeling inter- or intraobserver variability) using a mixture density network. We implemented and evaluated the DeepAMO in the context of  Tc99m-DMSA SPECT imaging. A human observer study was conducted, with two medical imaging physics graduate students serving as observers. A 5  ×  2-fold cross-validation experiment was conducted to test the statistical equivalence in defect detection performance between the DeepAMO and the human observer. We also compared the performance of the DeepAMO to an unoptimized implementation of a scanning linear discriminant observer (SLDO).
Results: The results show that the DeepAMO’s and human observer’s performances on unseen images were statistically equivalent with a margin of difference (ΔAUC) of 0.0426 at p  <  0.05, using 288 training images. A limited implementation of an SLDO had a substantially higher AUC (0.99) compared to the DeepAMO and human observer.
Conclusion: The results show that the DeepAMO has the potential to reproduce the absolute performance, and not just the relative ranking of human observers on a clinically realistic defect detection task, and that building conceptual components of the human reading process into deep learning-based models can allow training of these models in settings where limited training images are available.",project-academic
10.1158/1557-3265.ADI21-IA-21,2021-03-01,p,American Association for Cancer Research,abstract ia 21 ai in an imaging center challenges and opportunities," There is significant and well-founded enthusiasm for the promise of AI to improve healthcare outcomes across diverse clinical applications in oncology. However, careful analyses of the impact of AI tools in actual “real world” clinical settings are sparse in this relatively early phase of scientific discovery. The rigor applied to deep learning model development (training, testing, external validation) must be matched with equal rigor in assessing patient outcomes after implementation of deep learning tools in actual clinical settings. None We can learn from our past endeavors in computer-assisted image interpretation. Tools for computer aided detection were approved by the FDA in 1998 to help radiologists find breast cancers on mammograms. However, after widespread clinical implementation (based on promising results in reader studies and reimbursement approvals), clinical trials to assess the impact of CAD on diagnostic accuracy of mammography revealed the tools did not improve human performance in findings cancers, and in many cases increased false positive readings by humans using CAD assistance. None Due to severely limited healthcare resources during pandemics, and to protect patients and healthcare workers, state governments and the COVID-19 Pandemic Breast Cancer Consortium urged providers to focus cancer screening efforts on those patients at higher risk. These mandates were and continue to be necessary responses to support fair allocation of scarce resources to maximize benefits for all patients across the full spectrum of healthcare needs and acknowledge the shift in risk-benefit ratios for screening in average risk women. None After screening mammography programs were closed due to the COVID-19 pandemic, we implemented an AI image-only risk model strategy at MGH to invite women at higher risk to return to screening for breast cancer. The AI model identified the top 60% of patients at risk, who were diagnosed with 90% of cancers, while the top 60% of at-risk patients based on traditional models harbored less than 50% of cancers. Our AI image-only risk model performed best in identifying women with current breast cancer, followed by strategies which used five-year traditional risk models and included all women with a personal history of breast cancer in invitations to screen. Lifetime risk models performed extremely poorly. Operating at the traditional “high risk” threshold of >20% lifetime risk, these models identified fewer than 10% of patients with cancer in the total population screened. None AI-based risk models may support accurate and equitable risk-based screening. It may be possible with early intervention to reduce the widening disparities and later stage breast cancer diagnoses we are experiencing with COVID and mitigate the risk of delayed diagnoses and increased mortality from breast cancer during and after pandemics. Strategies for careful and rigorous assessment of clinical implementation are essential for AI to have the impact in health desired by all. None Citation Format: Constance Lehman. AI in an imaging center: Challenges and opportunities [abstract]. In: Proceedings of the AACR Virtual Special Conference on Artificial Intelligence, Diagnosis, and Imaging; 2021 Jan 13-14. Philadelphia (PA): AACR; Clin Cancer Res 2021;27(5_Suppl):Abstract nr IA-21.",project-academic
10.1109/IHTC.2015.7238043,2015-09-03,p,IEEE,using the extreme learning machine elm technique for heart disease diagnosis," One of the most important applications of machine learning systems is the diagnosis of heart disease which affect the lives of millions of people. Patients suffering from heart disease have lot of independent factors such as age, sex, serum cholesterol, blood sugar, etc. in common which can be used very effectively for diagnosis. In this paper an Extreme Learning Machine (ELM) algorithm is used to model these factors. The proposed system can replace a costly medical checkups with a warning system for patients of the probable presence of heart disease. The system is implemented on real data collected by the Cleveland Clinic Foundation where around 300 patients information has been collected. Simulation results show this architecture has about 80% accuracy in determining heart disease.",project-academic
10.1109/TBCAS.2020.3004544,2020-06-24,a,Institute of Electrical and Electronics Engineers (IEEE),resot resource efficient oblique trees for neural signal classification," Classifiers that can be implemented on chip with minimal computational and memory resources are essential for edge computing in emerging applications such as medical and IoT devices. This paper introduces a machine learning model based on oblique decision trees to enable resource-efficient classification on a neural implant. By integrating model compression with probabilistic routing and implementing cost-aware learning, our proposed model could significantly reduce the memory and hardware cost compared to state-of-the-art models, while maintaining the classification accuracy. We trained the resource-efficient oblique tree with power-efficient regularization (ResOT-PE) on three neural classification tasks to evaluate the performance, memory, and hardware requirements. On seizure detection task, we were able to reduce the model size by 3.4× and the feature extraction cost by 14.6× compared to the ensemble of boosted trees, using the intracranial EEG from 10 epilepsy patients. In a second experiment, we tested the ResOT-PE model on tremor detection for Parkinson's disease, using the local field potentials from 12 patients implanted with a deep-brain stimulation (DBS) device. We achieved a comparable classification performance as the state-of-the-art boosted tree ensemble, while reducing the model size and feature extraction cost by 10.6× and 6.8×, respectively. We also tested on a 6-class finger movement detection task using ECoG recordings from 9 subjects, reducing the model size by 17.6× and feature computation cost by 5.1×. The proposed model can enable a low-power and memory-efficient implementation of classifiers for real-time neurological disease detection and motor decoding.",project-academic
,2020-06-14,a,,resot resource efficient oblique trees for neural signal classification," Classifiers that can be implemented on chip with minimal computational and memory resources are essential for edge computing in emerging applications such as medical and IoT devices. This paper introduces a machine learning model based on oblique decision trees to enable resource-efficient classification on a neural implant. By integrating model compression with probabilistic routing and implementing cost-aware learning, our proposed model could significantly reduce the memory and hardware cost compared to state-of-the-art models, while maintaining the classification accuracy. We trained the resource-efficient oblique tree with power-efficient regularization (ResOT-PE) on three neural classification tasks to evaluate the performance, memory, and hardware requirements. On seizure detection task, we were able to reduce the model size by 3.4X and the feature extraction cost by 14.6X compared to the ensemble of boosted trees, using the intracranial EEG from 10 epilepsy patients. In a second experiment, we tested the ResOT-PE model on tremor detection for Parkinson's disease, using the local field potentials from 12 patients implanted with a deep-brain stimulation (DBS) device. We achieved a comparable classification performance as the state-of-the-art boosted tree ensemble, while reducing the model size and feature extraction cost by 10.6X and 6.8X, respectively. We also tested on a 6-class finger movement detection task using ECoG recordings from 9 subjects, reducing the model size by 17.6X and feature computation cost by 5.1X. The proposed model can enable a low-power and memory-efficient implementation of classifiers for real-time neurological disease detection and motor decoding.",project-academic
10.1093/JAMIA/OCZ119,2019-11-01,a,J Am Med Inform Assoc,real world evidence in cardiovascular medicine ensuring data validity in electronic health record based studies," Objective None With growing availability of digital health data and technology, health-related studies are increasingly augmented or implemented using real world data (RWD). Recent federal initiatives promote the use of RWD to make clinical assertions that influence regulatory decision-making. Our objective was to determine whether traditional real world evidence (RWE) techniques in cardiovascular medicine achieve accuracy sufficient for credible clinical assertions, also known as ""regulatory-grade"" RWE. None Design None Retrospective observational study using electronic health records (EHR), 2010-2016. None Methods None A predefined set of clinical concepts was extracted from EHR structured (EHR-S) and unstructured (EHR-U) data using traditional query techniques and artificial intelligence (AI) technologies, respectively. Performance was evaluated against manually annotated cohorts using standard metrics. Accuracy was compared to pre-defined criteria for regulatory-grade. Differences in accuracy were compared using Chi-square test. None Results None The dataset included 10 840 clinical notes. Individual concept occurrence ranged from 194 for coronary artery bypass graft to 4502 for diabetes mellitus. In EHR-S, average recall and precision were 51.7% and 98.3%, respectively and 95.5% and 95.3% in EHR-U, respectively. For each clinical concept, EHR-S accuracy was below regulatory-grade, while EHR-U met or exceeded criteria, with the exception of medications. None Conclusions None Identifying an appropriate RWE approach is dependent on cohorts studied and accuracy required. In this study, recall varied greatly between EHR-S and EHR-U. Overall, EHR-S did not meet regulatory grade criteria, while EHR-U did. These results suggest that recall should be routinely measured in EHR-based studes intended for regulatory use. Furthermore, advanced data and technologies may be required to achieve regulatory grade results.",project-academic
10.1109/JSSC.2019.2912304,2019-05-13,a,IEEE,a 1 06 mu w smart ecg processor in 65 nm cmos for real time biometric authentication and personal cardiac monitoring," Many wearable devices employ the sensors for physiological signals (e.g., electrocardiogram or ECG) to continuously monitor personal health (e.g., cardiac monitoring). Considering private medical data storage, secure access to such wearable devices becomes a crucial necessity. Exploiting the ECG sensors present on wearable devices, we investigate the possibility of using ECG as the individually unique source for device authentication. In particular, we propose to use ECG features toward both cardiac monitoring and neural-network-based biometric authentication. For such complex functionalities to be seamlessly integrated in wearable devices, an accurate algorithm must be implemented with ultralow power and a small form factor. In this paper, a smart ECG processor is presented for ECG-based authentication as well as cardiac monitoring. Data-driven Lasso regression and low-precision techniques are developed to compress neural networks for feature extraction by 24.4 None None $\times $ None None . The 65-nm testchip consumes 1.06 None None None $\mu \text{W}$ None None None at 0.55 V for real-time ECG authentication. For authentication, equal error rates of 1.70%/2.18%/2.48% (best/average/worst) are achieved on the in-house 645-subject database. For cardiac monitoring, 93.13% arrhythmia detection sensitivity and 89.78% specificity are achieved for 42 subjects in the MIT-BIH arrhythmia database.",project-academic
10.1186/S13673-019-0190-9,2019-12-01,a,SpringerOpen,ciot net a scalable cognitive iot based smart city network architecture," In the recent era, artificial intelligence (AI) is being used to support numerous solutions for human beings, such as healthcare, autonomous transportation, and so on. Cognitive computing is represented as a next-generation application AI-based solutions which provide human–machine interaction with personalized interactions and services that imitate human behavior. On the other hand, a large volume of data is generated from smart city applications such as healthcare, smart transportation, retail industry, and firefighting. There is always a concern on how to efficiently manage the large volume of generated data. Recently many existing researches discussed the analysis of the large quantity of data using cognitive computing; however, these researches are failed to handle the certain problems, namely scalability, and flexibility of data gathered in a smart city environment. Data captured from millions of sensors can be cross implemented across various cognitive computing applications to ensure real-time responses. In this paper, we study the cognitive internet of things (CIoT) and propose a CIoT-based smart city network (CIoT-Net) architecture which describes how data gathered from smart city applications can be analyzed using cognitive computing and handle the scalability and flexibility problems. We discuss various technologies such as AI and big data analysis to implement the proposed architecture. Finally, we describe the possible research challenges and opportunities while implementing the proposed architecture.",project-academic
10.4187/RESPCARE.07561,2020-09-01,a,Respiratory Care,adding continuous vital sign information to static clinical data improves the prediction of length of stay after intubation a data driven machine learning approach," BACKGROUND: None Bedside monitors in the ICU routinely measure and collect patients9 physiologic data in real time to continuously assess the health status of patients who are critically ill. With the advent of increased computational power and the ability to store and rapidly process big data sets in recent years, these physiologic data show promise in identifying specific outcomes and/or events during patients9 ICU hospitalization. None METHODS: None We introduced a methodology designed to automatically extract information from continuous-in-time vital sign data collected from bedside monitors to predict if a patient will experience a prolonged stay (length of stay) on mechanical ventilation, defined as >4 d, in a pediatric ICU. None RESULTS: None Continuous-in-time vital signs information and clinical history data were retrospectively collected for 284 ICU subjects from their first 24 h on mechanical ventilation from a medical-surgical pediatric ICU at Boston Children9s Hospital. Multiple machine learning models were trained on multiple subsets of these subjects to predict the likelihood that each of these subjects would experience a long stay. We evaluated the predictive power of our models strictly on unseen hold-out validation sets of subjects. Our methodology achieved model performance of >83% (area under the curve) by using only vital sign information as input, and performances of 90% (area under the curve) by combining vital sign information with subjects9 static clinical data readily available in electronic health records. We implemented this approach on 300 independently trained experiments with different choices of training and hold-out validation sets to ensure the consistency and robustness of our results in our study sample. The predictive power of our approach outperformed recent efforts that used deep learning to predict a similar task. None CONCLUSIONS: None Our proposed workflow may prove useful in the design of scalable approaches for real-time predictive systems in ICU environments, exploiting real-time vital sign information from bedside monitors. (ClinicalTrials.gov registration NCT02184208.)",project-academic
10.1016/J.ARTMED.2015.08.003,2015-11-01,a,Elsevier,a fuzzy ontology oriented case based reasoning framework for semantic diabetes diagnosis," Propose a fuzzy ontology based semantic-CBR framework.Propose a novel OWL2 fuzzy case-base ontology.Propose a fuzzy semantic case retrieval algorithm using an SNOMED CT fragment.Implement the fuzzy KI-CBR system using diabetes diagnosis as a case study.Combine fuzzy logic and ontology semantics in CBR enhances the CBR accuracy. ObjectiveCase-based reasoning (CBR) is a problem-solving paradigm that uses past knowledge to interpret or solve new problems. It is suitable for experience-based and theory-less problems. Building a semantically intelligent CBR that mimic the expert thinking can solve many problems especially medical ones. MethodsKnowledge-intensive CBR using formal ontologies is an evolvement of this paradigm. Ontologies can be used for case representation and storage, and it can be used as a background knowledge. Using standard medical ontologies, such as SNOMED CT, enhances the interoperability and integration with the health care systems. Moreover, utilizing vague or imprecise knowledge further improves the CBR semantic effectiveness. This paper proposes a fuzzy ontology-based CBR framework. It proposes a fuzzy case-base OWL2 ontology, and a fuzzy semantic retrieval algorithm that handles many feature types. MaterialThis framework is implemented and tested on the diabetes diagnosis problem. The fuzzy ontology is populated with 60 real diabetic cases. The effectiveness of the proposed approach is illustrated with a set of experiments and case studies. ResultsThe resulting system can answer complex medical queries related to semantic understanding of medical concepts and handling of vague terms. The resulting fuzzy case-base ontology has 63 concepts, 54 (fuzzy) object properties, 138 (fuzzy) datatype properties, 105 fuzzy datatypes, and 2640 instances. The system achieves an accuracy of 97.67%. We compare our framework with existing CBR systems and a set of five machine-learning classifiers; our system outperforms all of these systems. ConclusionBuilding an integrated CBR system can improve its performance. Representing CBR knowledge using the fuzzy ontology and building a case retrieval algorithm that treats different features differently improves the accuracy of the resulting systems.",project-academic
10.1109/ACCESS.2019.2909267,2019-04-04,a,IEEE,da dcgan an effective methodology for dc series arc fault diagnosis in photovoltaic systems," DC arc faults, especially series arcing, can occur in photovoltaic (PV) systems and pose a challenging detection and protection problem. Machine learning-based methods are increasingly being used for fault diagnosis applications. However, the performance of such detection algorithms will degrade because of variations between the source domain data used during the development and the target domain data encountered in operation of the field. Furthermore, the fault’s data in the target domain for model training are usually not available. In this paper, domain adaptation combined with deep convolutional generative adversarial network (DA-DCGAN)-based methodology is proposed, where DA-DCGAN first learns an intelligent normal-to-arcing transformation from the source-domain data. Then by generating dummy arcing data with the learned transformation using the normal data from the target domain and employing domain adaptation, a robust and reliable fault diagnosis scheme can be achieved for the target domain. The PV loop current is framed and arranged into a 2D matrix as input for cross-domain DC series arc fault diagnosis. The system is validated offline using pre-recorded PV loop current data from a real 1.5-kW grid-connected rooftop PV system. Also, the proposed method is implemented in an embedded system and tested in real-time according to UL-1699B standard. The experimental results clearly demonstrate benefits of DA-DCGAN and confirm the effectiveness of the proposed methodology for practical PV applications.",project-academic
10.1039/C6TB00152A,2016-05-11,a,The Royal Society of Chemistry,graphene scaffolds in progressive nanotechnology stem cell based tissue engineering of the nervous system," Although graphene/stem cell-based tissue engineering has recently emerged and has promisingly and progressively been utilized for developing one of the most effective regenerative nanomedicines, it suffers from low differentiation efficiency, low hybridization after transplantation and lack of appropriate scaffolds required in implantations without any degrading in functionality of the cells. In fact, recent studies have demonstrated that the unique properties of graphene can successfully resolve all of these challenges. Among various stem cells, neural stem cells (NSCs) and their neural differentiation on graphene have attracted a lot of interest, because graphene-based neuronal tissue engineering can promisingly realize the regenerative therapy of various incurable neurological diseases/disorders and the fabrication of neuronal networks. Hence, in this review, we further focused on the potential bioapplications of graphene-based nanomaterials for the proliferation and differentiation of NSCs. Then, various stimulation techniques (including electrical, pulsed laser, flash photo, near infrared (NIR), chemical and morphological stimuli) which have recently been implemented in graphene-based stem cell differentiations were reviewed. The possibility of degradation of graphene scaffolds (NIR-assisted photodegradation of three-dimensional graphene nanomesh scaffolds) was also discussed based on the latest achievements. The biocompatibility of graphene scaffolds and their probable toxicities (especially after the disintegration of graphene scaffolds and distribution of its platelets in the body), which is still an important challenge, were reviewed and discussed. Finally, the initial recent efforts for fabrication of neuronal networks on graphene materials were presented. Since there has been no in vivo application of graphene in neuronal regenerative medicine, we hope that this review can excite further and concentrated investigations on in vivo (and even in vitro) neural proliferation, stimulation and differentiation of stem cells on biocompatible graphene scaffolds having the potential of degradability for the generation of implantable neuronal networks.",project-academic
10.2967/JNUMED.119.239327,2020-09-01,a,Society of Nuclear Medicine,projection space implementation of deep learning guided low dose brain pet imaging improves performance over implementation in image space," Our purpose was to assess the performance of full-dose (FD) PET image synthesis in both image and sinogram space from low-dose (LD) PET images and sinograms without sacrificing diagnostic quality using deep learning techniques. Methods: Clinical brain PET/CT studies of 140 patients were retrospectively used for LD-to-FD PET conversion. Five percent of the events were randomly selected from the FD list-mode PET data to simulate a realistic LD acquisition. A modified 3-dimensional U-Net model was implemented to predict FD sinograms in the projection space (PSS) and FD images in image space (PIS) from their corresponding LD sinograms and images, respectively. The quality of the predicted PET images was assessed by 2 nuclear medicine specialists using a 5-point grading scheme. Quantitative analysis using established metrics including the peak signal-to-noise ratio (PSNR), structural similarity index metric (SSIM), regionwise SUV bias, and first-, second- and high-order texture radiomic features in 83 brain regions for the test and evaluation datasets was also performed. Results: All PSS images were scored 4 or higher (good to excellent) by the nuclear medicine specialists. PSNR and SSIM values of 0.96 ± 0.03 and 0.97 ± 0.02, respectively, were obtained for PIS, and values of 31.70 ± 0.75 and 37.30 ± 0.71, respectively, were obtained for PSS. The average SUV bias calculated over all brain regions was 0.24% ± 0.96% and 1.05% ± 1.44% for PSS and PIS, respectively. The Bland-Altman plots reported the lowest SUV bias (0.02) and variance (95% confidence interval, -0.92 to +0.84) for PSS, compared with the reference FD images. The relative error of the homogeneity radiomic feature belonging to the gray-level cooccurrence matrix category was -1.07 ± 1.77 and 0.28 ± 1.4 for PIS and PSS, respectively. Conclusion: The qualitative assessment and quantitative analysis demonstrated that the FD PET PSS led to superior performance, resulting in higher image quality and lower SUV bias and variance than for FD PET PIS.",project-academic
,2020-01-01,p,AMIA Annu Symp Proc,using natural language processing and machine learning to identify hospitalized patients with opioid use disorder," Opioid use disorder (OUD) represents a global public health crisis that challenges classic clinical decision making. As existing hospital screening methods are resource-intensive, patients with OUD are significantly under-detected. An automated and accurate approach is needed to improve OUD identification so that appropriate care can be provided to these patients in a timely fashion. In this study, we used a large-scale clinical database from Mass General Brigham (MGB; formerly Partners HealthCare) to develop an OUD patient identification algorithm, using multiple machine learning methods. Working closely with an addiction psychiatrist, we developed a set of hand-crafted rules for identifying information suggestive of OUD from free-text clinical notes. We implemented a natural language processing (NLP)-based classification algorithm within the Medical Text Extraction, Reasoning and Mapping System (MTERMS) tool suite to automatically label patients as positive or negative for OUD based on these rules. We further used the NLP output as features to build multiple machine learning and a neural classifier. Our methods yielded robust performance for classifying hospitalized patients as positive or negative for OUD, with the best performing feature set and model combination achieving an F1 score of 0.97. These results show promise for the future development of a real-time tool for quickly and accurately identifying patients with OUD in the hospital setting.",project-academic
10.1371/JOURNAL.PONE.0195798,2018-04-13,a,Public Library of Science,automatic lesion detection and segmentation of 18f fet pet in gliomas a full 3d u net convolutional neural network study," Introduction 
Amino-acids positron emission tomography (PET) is increasingly used in the diagnostic workup of patients with gliomas, including differential diagnosis, evaluation of tumor extension, treatment planning and follow-up. Recently, progresses of computer vision and machine learning have been translated for medical imaging. Aim was to demonstrate the feasibility of an automated 18F-fluoro-ethyl-tyrosine (18F-FET) PET lesion detection and segmentation relying on a full 3D U-Net Convolutional Neural Network (CNN).


Methods 
All dynamic 18F-FET PET brain image volumes were temporally realigned to the first dynamic acquisition, coregistered and spatially normalized onto the Montreal Neurological Institute template. Ground truth segmentations were obtained using manual delineation and thresholding (1.3 x background). The volumetric CNN was implemented based on a modified Keras implementation of a U-Net library with 3 layers for the encoding and decoding paths. Dice similarity coefficient (DSC) was used as an accuracy measure of segmentation.


Results 
Thirty-seven patients were included (26 [70%] in the training set and 11 [30%] in the validation set). All 11 lesions were accurately detected with no false positive, resulting in a sensitivity and a specificity for the detection at the tumor level of 100%. After 150 epochs, DSC reached 0.7924 in the training set and 0.7911 in the validation set. After morphological dilatation and fixed thresholding of the predicted U-Net mask a substantial improvement of the DSC to 0.8231 (+ 4.1%) was noted. At the voxel level, this segmentation led to a 0.88 sensitivity [95% CI, 87.1 to, 88.2%] a 0.99 specificity [99.9 to 99.9%], a 0.78 positive predictive value: [76.9 to 78.3%], and a 0.99 negative predictive value [99.9 to 99.9%].


Conclusions 
With relatively high performance, it was proposed the first full 3D automated procedure for segmentation of 18F-FET PET brain images of patients with different gliomas using a U-Net CNN architecture.",project-academic
,2018-02-09,,,emr data drive based gdm forecasting method," The invention discloses an EMR (Electronic Medical Record) data drive based GDM (Gestational Diabetes Mellitus) forecasting method playing an increasing important role in smart medical service. The invention proposes a machine learning based GMD forecasting frame and constructs three forecasting frames including a full-domain data forecasting model, a staging data forecasting model and a weekly data forecasting model according different time window division methods for collected data. After a forecasting item is identified, high-dimensional EMR data digging is implemented through seven steps including input and ETL data cleaning, correlation of a medical record code and feature data, EMR data pre-treatment, secondary data treatment, feature engineering, machine learning and forecasting application. A mark data set related to definite diagnosis is constructed by using clinic data and is divided into two sub sets used for model training and testing. The method performs forecasting through supporting a support vector machine, a Bayesian network, a decision making tree and an integration based hybrid model and GDM mode classification is realized.",project-academic
10.12688/GATESOPENRES.13072.1,2019-10-31,a,F1000 Research Limited,leveraging google search data to track influenza outbreaks in africa," Background: Traditionally, public health agencies track seasonal influenza activity by collecting information from clinics, hospitals, and laboratories. The inherent slowness of the processes used to collect influenza activity data limits the ability of public health agencies to adapt to unexpected changes in influenza activity in near real-time. In recent years, new influenza surveillance methods that use nontraditional data sources, such as Google searches, have been proposed to successfully estimate influenza activity in near real-time. However, most of these methods have been designed for and implemented in high-income countries even though influenza disease burden remains high in low- to middle-income countries. Here, we seek to predict influenza activity in near real-time in Africa using machine learning models that combine Google searches with traditional epidemiological data. None Methods: We extend the AutoRegression with Google search data (ARGO) model to track influenza activity in near-real-time in Africa. The ARGO model, which was originally designed to predict influenza activity in the United States, combines influenza-related Google searches with historical laboratory-confirmed influenza trends. We evaluate the predictive performance of the ARGO model and compare it with several benchmark models in Algeria, Ghana, Morocco, and South Africa. We also explore the advantages and limitations of using Google search data to monitor influenza activity. None Results: In South Africa, Algeria, and Morocco, the ARGO model outperforms all benchmark models, suggesting that incorporating influenza-related Google search information in predictive models in these countries leads to improved predictions. In Ghana, however, the ARGO model and the autoregressive model of historical influenza activity have comparable performances. None Conclusions: These results demonstrate that the quality of the ARGO predictions is higher in regions where influenza activity is seasonal, historical influenza activity is recorded consistently, and the volume of influenza-related Google search queries is enough to appear as non-zero in the Google Trends tool.",project-academic
10.1109/TSP.2019.8768883,2019-07-01,p,IEEE,edge ai in lora based health monitoring fall detection system with fog computing and lstm recurrent neural networks," Remote healthcare monitoring has exponentially grown over the past decade together with the increasing penetration of Internet of Things (IoT) platforms. IoT-based health systems help to improve the quality of healthcare services through real-time data acquisition and processing. However, traditional IoT architectures have some limitations. For instance, they cannot properly function in areas with poor or unstable Internet. Low power wide area network (LPWAN) technologies, including long-range communication protocols such as LoRa, are a potential candidate to overcome the lacking network infrastructure. Nevertheless, LPWANs have limited transmission bandwidth not suitable for high data rate applications such as fall detection systems or electrocardiography monitoring. Therefore, data processing and compression are required at the edge of the network. We propose a system architecture with integrated artificial intelligence that combines Edge and Fog computing, LPWAN technology, IoT and deep learning algorithms to perform health monitoring tasks. In particular, we demonstrate the feasibility and effectiveness of this architecture via a use case of fall detection using recurrent neural networks. We have implemented a fall detection system from the sensor node and Edge gateway to cloud services and end-user applications. The system uses inertial data as input and achieves an average precision of over 90% and an average recall over 95% in fall detection.",project-academic
10.1016/J.JBI.2018.05.004,2018-05-16,a,J Biomed Inform,simulation of patient flow in multiple healthcare units using process and data mining techniques for model identification," Abstract None None Introduction None An approach to building a hybrid simulation of patient flow is introduced with a combination of data-driven methods for automation of model identification. The approach is described with a conceptual framework and basic methods for combination of different techniques. The implementation of the proposed approach for simulation of the acute coronary syndrome (ACS) was developed and used in an experimental study. None None None Methods None A combination of data, text, process mining techniques, and machine learning approaches for the analysis of electronic health records (EHRs) with discrete-event simulation (DES) and queueing theory for the simulation of patient flow was proposed. The performed analysis of EHRs for ACS patients enabled identification of several classes of clinical pathways (CPs) which were used to implement a more realistic simulation of the patient flow. The developed solution was implemented using Python libraries (SimPy, SciPy, and others). None None None Results None The proposed approach enables more a realistic and detailed simulation of the patient flow within a group of related departments. An experimental study shows an improved simulation of patient length of stay for ACS patient flow obtained from EHRs in Almazov National Medical Research Centre in Saint Petersburg, Russia. None None None Conclusion None The proposed approach, methods, and solutions provide a conceptual, methodological, and programming framework for the implementation of a simulation of complex and diverse scenarios within a flow of patients for different purposes: decision making, training, management optimization, and others.",project-academic
10.1109/TFUZZ.2015.2453153,2016-04-01,a,IEEE,modified ahp for gene selection and cancer classification using type 2 fuzzy logic," This paper proposes a modification to the analytic hierarchy process (AHP) to select the most informative genes that serve as inputs to an interval type-2 fuzzy logic system (IT2FLS) for cancer classification. Unlike the conventional AHP, the modified AHP allows us to process quantitative factors that are ranking outcomes of individual gene selection methods including t-test, entropy, receiver operating characteristic curve, Wilcoxon test, and signal-to-noise ratio. The IT2FLS is introduced for the classification task due to its great ability for handling nonlinear, noisy, and outlier data, which are common problems in cancer microarray gene expression profiles. An unsupervised learning strategy using the fuzzy c-means clustering is employed to initialize parameters of the IT2FLS. Other classifiers such as multilayer perceptron network, support vector machine, and fuzzy ARTMAP are also implemented for comparisons. Experiments are carried out on three well-known microarray datasets: diffuse large B-cell lymphoma, leukemia cancer, and prostate. Rather than the traditional cross validation, leave-one-out cross-validation strategy is applied for the experiments. Results demonstrate the performance dominance of the IT2FLS against the competing classifiers. More noticeably, the modified AHP improves the classification performance not only of the IT2FLS but of all other classifiers as well. Accordingly, the proposed combination between the modified AHP and IT2FLS is a powerful tool for cancer classification and can be implemented as a real clinical decision support system that is useful for medical practitioners.",project-academic
10.1002/MP.14545,2020-12-01,a,"John Wiley & Sons, Ltd",generating anthropomorphic phantoms using fully unsupervised deformable image registration with convolutional neural networks," Purpose None Computerized phantoms have been widely used in nuclear medicine imaging for imaging system optimization and validation. Although the existing computerized phantoms can model anatomical variations through organ and phantom scaling, they do not provide a way to fully reproduce the anatomical variations and details seen in humans. In this work, we present a novel registration-based method for creating highly anatomically detailed computerized phantoms. We experimentally show substantially improved image similarity of the generated phantom to a patient image. None Methods None We propose a deep-learning-based unsupervised registration method to generate a highly anatomically detailed computerized phantom by warping an XCAT phantom to a patient computed tomography (CT) scan. We implemented and evaluated the proposed method using the NURBS-based XCAT phantom and a publicly available low-dose CT dataset from TCIA. A rigorous tradeoff analysis between image similarity and deformation regularization was conducted to select the loss function and regularization term for the proposed method. A novel SSIM-based unsupervised objective function was proposed. Finally, ablation studies were conducted to evaluate the performance of the proposed method (using the optimal regularization and loss function) and the current state-of-the-art unsupervised registration methods. None Results None The proposed method outperformed the state-of-the-art registration methods, such as SyN and VoxelMorph, by more than 8%, measured by the SSIM and less than 30%, by the MSE. The phantom generated by the proposed method was highly detailed and was almost identical in appearance to a patient image. None Conclusions None A deep-learning-based unsupervised registration method was developed to create anthropomorphic phantoms with anatomies labels that can be used as the basis for modeling organ properties. Experimental results demonstrate the effectiveness of the proposed method. The resulting anthropomorphic phantom is highly realistic. Combined with realistic simulations of the image formation process, the generated phantoms could serve in many applications of medical imaging research.",project-academic
10.1109/ACCESS.2020.3004790,2020-06-25,a,Institute of Electrical and Electronics Engineers (IEEE),a novel smart healthcare design simulation and implementation using healthcare 4 0 processes," Blockchain technology is found to have its applicability in almost every domain because of its advantages such as crypto-security, transparency, immutability, decentralized data network. In present times, a smart healthcare system with a blockchain data network and healthcare 4.0 processes provides transparency, easy and faster accessibility, security, efficiency, etc. Healthcare 4.0 trends include industry 4.0 processes such as the internet of things (IoT), industrial IoT (IIoT), cognitive computing, artificial intelligence, cloud computing, fog computing, edge computing, etc. The goal of this work is to design a smart healthcare system and it is found to be possible through integration and interoperability of Blockchain 3.0 and Healthcare 4.0 in consideration with healthcare ground-realities. Here, healthcare 4.0 processes used for data accessibility are targeted to be validated through statistical simulation-optimization methods and algorithms. The blockchain is implemented in the Ethereum network, and with associated programming languages, tools, and techniques such as solidity, web3.js, Athena, etc. Further, this work prepares a comparative and comprehensive survey of state-of-the-art blockchain-based smart healthcare systems. The comprehensive survey includes methodology, applications, requirements, outcomes, future directions, etc. A list of groups, organizations, and enterprises are prepared that are working in electronic health records (EHR), electronic medical records (EMR) or electronic personal records (EPR) mainly, and a comparative analysis is drawn concerning adopting the blockchain technology in their processes. This work has explored optimization algorithms applicable to Healthcare 4.0 trends and improves the performance of blockchain-based decentralized applications for the smart healthcare system. Further, smart contracts and their designs are prepared for the proposed system to expedite the trust-building and payment systems. This work has considered simulation and implementation to validate the proposed approach. Simulation results show that the Gas value required (indicating block size and expenditure) lies within current Etherum network Gas limits. The proposed system is active because block utilization lies above 80%. Automated smart contract execution is below 20 seconds. A good number (average 3 per simulation time) is generated in the network that indicates a health competition. Although there is error observed in simulation and implementation that lies between 0.55% and 4.24%, these errors are not affecting overall system performance because simulated and actual (taken in state-of-the-art) data variations are negligible.",project-academic
10.1371/JOURNAL.PONE.0161401,2016-08-17,a,Public Library of Science,prediction of clinical deterioration in hospitalized adult patients with hematologic malignancies using a neural network model," Introduction 
Clinical deterioration (ICU transfer and cardiac arrest) occurs during approximately 5–10% of hospital admissions. Existing prediction models have a high false positive rate, leading to multiple false alarms and alarm fatigue. We used routine vital signs and laboratory values obtained from the electronic medical record (EMR) along with a machine learning algorithm called a neural network to develop a prediction model that would increase the predictive accuracy and decrease false alarm rates.


Design 
Retrospective cohort study.


Setting 
The hematologic malignancy unit in an academic medical center in the United States.


Patient Population 
Adult patients admitted to the hematologic malignancy unit from 2009 to 2010.


Intervention 
None.


Measurements and Main Results 
Vital signs and laboratory values were obtained from the electronic medical record system and then used as predictors (features). A neural network was used to build a model to predict clinical deterioration events (ICU transfer and cardiac arrest). The performance of the neural network model was compared to the VitalPac Early Warning Score (ViEWS). Five hundred sixty five consecutive total admissions were available with 43 admissions resulting in clinical deterioration. Using simulation, the neural network outperformed the ViEWS model with a positive predictive value of 82% compared to 24%, respectively.


Conclusion 
We developed and tested a neural network-based prediction model for clinical deterioration in patients hospitalized in the hematologic malignancy unit. Our neural network model outperformed an existing model, substantially increasing the positive predictive value, allowing the clinician to be confident in the alarm raised. This system can be readily implemented in a real-time fashion in existing EMR systems.",project-academic
10.1007/S11831-018-9257-4,2019-07-01,a,Springer Netherlands,image segmentation using computational intelligence techniques review," Image segmentation methodology is a part of nearly all computer schemes as a pre-processing phase to excerpt more meaningful and useful information for analysing the objects within an image. Segmentation of an image is one of the most conjoint scientific matter, essential technology and critical constraint for image investigation and dispensation. There has been a lot of research work conceded in several emerging algorithms and approaches for segmentation, but even at present, no solitary standard technique has been proposed. The methodologies present are broadly classified among two classes i.e. traditional approaches and Soft computing approaches or Computational Intelligence (CI) approaches. In this article, our emphasis is to focus on Soft Computing (SC) techniques which has been adopted for segmenting an image. Nowadays, it is quite often seen that SC or CI is cast-off frequently in Information Technology and Computer Technology. However, Soft Computing approaches working synergistically provides in anyway, malleable information processing competence to manipulate real-life enigmatic circumstances. The impetus of these methodologies is to feat the lenience for ambiguity, roughness, imprecise acumen and partial veracity for the sake to attain compliance, sturdiness and economical results. Neural Networks (NNs), Fuzzy Logic (FL), and Genetic Algorithm (GA) are the fundamental approaches of SC regulation. SC approaches has been broadly implemented and studied in the number of applications including scientific analysis, medical, engineering, management, humanities etc. The paper focuses on introducing the various SC methodologies and presenting numerous applications in image segmentation. The acumen is to corroborate the probabilities of smearing computational intelligence to segmentation of an image. The available articles about usage of SC in segmentation are investigated, especially focusing on the core approaches like FL, NN and GA and efforts has been also made for collaborating new techniques like Fuzzy C-Means from FL family and Deep Neural Network or Convolutional Neural Network from NN family. The impression behind this work is to simulate core Soft Computing methodologies, along with encapsulating various terminologies like evaluation parameters, tools, databases, noises etc. which can be advantageous for researchers. This study also identifies approaches of SC being used, often collectively to resolve the distinctive dilemma of image segmentation, concluding with a general discussion about methodologies, applications followed by proposed work.",project-academic
10.1007/978-3-319-73676-1_14,2018-01-01,a,"Springer, Cham",fog assisted cloud computing in era of big data and internet of things systems architectures and applications," This book chapter discusses the concept of edge-assisted cloud computing and its relation to the emerging domain of “Fog-of-things (FoT)”. Such systems employ low-power embedded computers to provide local computation close to clients or cloud. The discussed architectures cover applications in medical, healthcare, wellness and fitness monitoring, geo-information processing, mineral resource management, etc. Cloud computing can get assistance by transferring some of the processing and decision making to the edge either close to client layer or cloud backend. Fog of Things refers to an amalgamation of multiple fog nodes that could communicate with each other with the Internet of Things. The clouds act as the final destination for heavy-weight processing, long-term storage and analysis. We propose application-specific architectures GeoFog and Fog2Fog that are flexible and user-orientated. The fog devices act as intermediate intelligent nodes in such systems where these could decide if further processing is required or not. The preliminary data analysis, signal filtering, data cleaning, feature extraction could be implemented on edge computer leading to a reduction of computational load in the cloud. In several practical cases, such as tele healthcare of patients with Parkinson’s disease, edge computing may decide not to proceed for data transmission to cloud (Barik et al., in 5th IEEE Global Conference on Signal and Information Processing 2017, IEEE, 2017) [4]. Towards the end of this research paper, we cover the idea of translating machine learning such as clustering, decoding deep neural network models etc. on fog devices that could lead to scalable inferences. Fog2Fog communication is discussed with respect to analytical models for power savings. The book chapter concludes by interesting case studies on real world situations and practical data. Future pointers to research directions, challenges and strategies to manage these are discussed as well. We summarize case studies employing proposed architectures in various application areas. The use of edge devices for processing offloads the cloud leading to an enhanced efficiency and performance.",project-academic
10.3390/S16081264,2016-08-10,a,Multidisciplinary Digital Publishing Institute (MDPI),human behavior analysis by means of multimodal context mining," There is sufficient evidence proving the impact that negative lifestyle choices have on people’s health and wellness. Changing unhealthy behaviours requires raising people’s self-awareness and also providing healthcare experts with a thorough and continuous description of the user’s conduct. Several monitoring techniques have been proposed in the past to track users’ behaviour; however, these approaches are either subjective and prone to misreporting, such as questionnaires, or only focus on a specific component of context, such as activity counters. This work presents an innovative multimodal context mining framework to inspect and infer human behaviour in a more holistic fashion. The proposed approach extends beyond the state-of-the-art, since it not only explores a sole type of context, but also combines diverse levels of context in an integral manner. Namely, low-level contexts, including activities, emotions and locations, are identified from heterogeneous sensory data through machine learning techniques. Low-level contexts are combined using ontological mechanisms to derive a more abstract representation of the user’s context, here referred to as high-level context. An initial implementation of the proposed framework supporting real-time context identification is also presented. The developed system is evaluated for various realistic scenarios making use of a novel multimodal context open dataset and data on-the-go, demonstrating prominent context-aware capabilities at both low and high levels.",project-academic
10.1038/S41567-019-0648-8,2019-08-26,a,Springer Science and Business Media LLC,quantum convolutional neural networks," Neural network-based machine learning has recently proven successful for many complex applications ranging from image recognition to precision medicine. However, its direct application to problems in quantum physics is challenging due to the exponential complexity of many-body systems. Motivated by recent advances in realizing quantum information processors, we introduce and analyse a quantum circuit-based algorithm inspired by convolutional neural networks, a highly effective model in machine learning. Our quantum convolutional neural network (QCNN) uses only O(log(N)) variational parameters for input sizes of N qubits, allowing for its efficient training and implementation on realistic, near-term quantum devices. To explicitly illustrate its capabilities, we show that QCNNs can accurately recognize quantum states associated with a one-dimensional symmetry-protected topological phase, with performance surpassing existing approaches. We further demonstrate that QCNNs can be used to devise a quantum error correction scheme optimized for a given, unknown error model that substantially outperforms known quantum codes of comparable complexity. The potential experimental realizations and generalizations of QCNNs are also discussed. A quantum circuit-based algorithm inspired by convolutional neural networks is shown to successfully perform quantum phase recognition and devise quantum error correcting codes when applied to arbitrary input quantum states.",project-academic
10.1088/0964-1726/25/5/053001,2016-03-30,a,IOP Publishing,guided wave based structural health monitoring a review," The paper provides a state of the art review of guided wave based structural health monitoring (SHM). First, the fundamental concepts of guided wave propagation and its implementation for SHM is explained. Following sections present the different modeling schemes adopted, developments in the area of transducers for generation, and sensing of wave, signal processing and imaging technique, statistical and machine learning schemes for feature extraction. Next, a section is presented on the recent advancements in nonlinear guided wave for SHM. This is followed by section on Rayleigh and SH waves. Next is a section on real-life implementation of guided wave for industrial problems. The paper, though briefly talks about the early development for completeness,. is primarily focussed on the recent progress made in the last decade. The paper ends by discussing and highlighting the future directions and open areas of research in guided wave based SHM.",project-academic
10.1002/WIDM.1312,2019-07-01,a,"John Wiley & Sons, Ltd",causability and explainability of artificial intelligence in medicine," Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system This article is categorized under: Fundamental Concepts of Data and Knowledge > Human Centricity and User Interaction.",project-academic
10.1109/JBHI.2016.2633287,2017-01-01,p,Institute of Electrical and Electronics Engineers (IEEE),a deep learning approach to on node sensor data analytics for mobile or wearable devices," The increasing popularity of wearable devices in recent years means that a diverse range of physiological and functional data can now be captured continuously for applications in sports, wellbeing, and healthcare. This wealth of information requires efficient methods of classification and analysis where deep learning is a promising technique for large-scale data analytics. While deep learning has been successful in implementations that utilize high-performance computing platforms, its use on low-power wearable devices is limited by resource constraints. In this paper, we propose a deep learning methodology, which combines features learned from inertial sensor data together with complementary information from a set of shallow features to enable accurate and real-time activity classification. The design of this combined method aims to overcome some of the limitations present in a typical deep learning framework where on-node computation is required. To optimize the proposed method for real-time on-node computation, spectral domain preprocessing is used before the data are passed onto the deep learning framework. The classification accuracy of our proposed deep learning approach is evaluated against state-of-the-art methods using both laboratory and real world activity datasets. Our results show the validity of the approach on different human activity datasets, outperforming other methods, including the two methods used within our combined pipeline. We also demonstrate that the computation times for the proposed method are consistent with the constraints of real-time on-node processing on smartphones and a wearable sensor platform.",project-academic
,2019-05-09,a,,1d convolutional neural networks and applications a survey," During the last decade, Convolutional Neural Networks (CNNs) have become the de facto standard for various Computer Vision and Machine Learning operations. CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and millions of parameters have the ability to learn complex objects and patterns providing that they can be trained on a massive size visual database with ground-truth labels. With a proper training, this unique ability makes them the primary tool for various engineering applications for 2D signals such as images and video frames. Yet, this may not be a viable option in numerous applications over 1D signals especially when the training data is scarce or application-specific. To address this issue, 1D CNNs have recently been proposed and immediately achieved the state-of-the-art performance levels in several applications such as personalized biomedical data classification and early diagnosis, structural health monitoring, anomaly detection and identification in power electronics and motor-fault detection. Another major advantage is that a real-time and low-cost hardware implementation is feasible due to the simple and compact configuration of 1D CNNs that perform only 1D convolutions (scalar multiplications and additions). This paper presents a comprehensive review of the general architecture and principals of 1D CNNs along with their major engineering applications, especially focused on the recent progress in this field. Their state-of-the-art performance is highlighted concluding with their unique properties. The benchmark datasets and the principal 1D CNN software used in those applications are also publically shared in a dedicated website.",project-academic
10.1016/J.YMSSP.2020.107398,2021-04-01,a,Academic Press,1d convolutional neural networks and applications a survey," Abstract None None During the last decade, Convolutional Neural Networks (CNNs) have become the de facto standard for various Computer Vision and Machine Learning operations. CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and millions of parameters have the ability to learn complex objects and patterns providing that they can be trained on a massive size visual database with ground-truth labels. With a proper training, this unique ability makes them the primary tool for various engineering applications for 2D signals such as images and video frames. Yet, this may not be a viable option in numerous applications over 1D signals especially when the training data is scarce or application specific. To address this issue, 1D CNNs have recently been proposed and immediately achieved the state-of-the-art performance levels in several applications such as personalized biomedical data classification and early diagnosis, structural health monitoring, anomaly detection and identification in power electronics and electrical motor fault detection. Another major advantage is that a real-time and low-cost hardware implementation is feasible due to the simple and compact configuration of 1D CNNs that perform only 1D convolutions (scalar multiplications and additions). This paper presents a comprehensive review of the general architecture and principals of 1D CNNs along with their major engineering applications, especially focused on the recent progress in this field. Their state-of-the-art performance is highlighted concluding with their unique properties. The benchmark datasets and the principal 1D CNN software used in those applications are also publicly shared in a dedicated website. While there has not been a paper on the review of 1D CNNs and its applications in the literature, this paper fulfills this gap.",project-academic
10.1109/JIOT.2017.2670022,2017-02-16,a,IEEE,real time signal quality aware ecg telemetry system for iot based health care monitoring," In this paper, we propose a novel signal quality-aware Internet of Things (IoT)-enabled electrocardiogram (ECG) telemetry system for continuous cardiac health monitoring applications. The proposed quality-aware ECG monitoring system consists of three modules: 1) ECG signal sensing module; 2) automated signal quality assessment (SQA) module; and 3) signal-quality aware (SQAw) ECG analysis and transmission module. The main objectives of this paper are: design and development of a light-weight ECG SQA method for automatically classifying the acquired ECG signal into acceptable or unacceptable class and real-time implementation of proposed IoT-enabled ECG monitoring framework using ECG sensors, Arduino, Android phone, Bluetooth, and cloud server. The proposed framework is tested and validated using the ECG signals taken from the MIT-BIH arrhythmia and Physionet challenge databases and the real-time recorded ECG signals under different physical activities. Experimental results show that the proposed SQA method achieves promising results in identifying the unacceptable quality of ECG signals and outperforms existing methods based on the morphological and RR interval features and machine learning approaches. This paper further shows that the transmission of acceptable quality of ECG signals can significantly improve the battery lifetime of IoT-enabled devices. The proposed quality-aware IoT paradigm has great potential for assessing clinical acceptability of ECG signals in improvement of accuracy and reliability of unsupervised diagnosis system.",project-academic
10.1016/J.INFFUS.2018.06.002,2019-03-01,a,Elsevier,data fusion and multiple classifier systems for human activity detection and health monitoring review and open research directions," Abstract None None Activity detection and classification using different sensor modalities have emerged as revolutionary technology for real-time and autonomous monitoring in behaviour analysis, ambient assisted living, activity of daily living (ADL), elderly care, rehabilitations, entertainments and surveillance in smart home environments. Wearable devices, smart-phones and ambient environments devices are equipped with variety of sensors such as accelerometers, gyroscopes, magnetometer, heart rate, pressure and wearable camera for activity detection and monitoring. These sensors are pre-processed and different feature sets such as time domain, frequency domain, wavelet transform are extracted and transform using machine learning algorithm for human activity classification and monitoring. Recently, deep learning algorithms for automatic feature representation have also been proposed to lessen the burden of reliance on handcrafted features and to increase performance accuracy. Initially, one set of sensor data, features or classifiers were used for activity recognition applications. However, there are new trends on the implementation of fusion strategies to combine sensors data, features and classifiers to provide diversity, offer higher generalization, and tackle challenging issues. For instances, combination of inertial sensors provide mechanism to differentiate activity of similar patterns and accurate posture identification while other multimodal sensor data are used for energy expenditure estimations, object localizations in smart homes and health status monitoring. Hence, the focus of this review is to provide in-depth and comprehensive analysis of data fusion and multiple classifier systems techniques for human activity recognition with emphasis on mobile and wearable devices. First, data fusion methods and modalities were presented and also feature fusion, including deep learning fusion for human activity recognition were critically analysed, and their applications, strengths and issues were identified. Furthermore, the review presents different multiple classifier system design and fusion methods that were recently proposed in literature. Finally, open research problems that require further research and improvements are identified and discussed.",project-academic
10.1007/S11886-013-0441-8,2014-01-01,a,Springer US,artificial intelligence in medicine and cardiac imaging harnessing big data and advanced computing to provide personalized medical diagnosis and treatment," Although advances in information technology in the past decade have come in quantum leaps in nearly every aspect of our lives, they seem to be coming at a slower pace in the field of medicine. However, the implementation of electronic health records (EHR) in hospitals is increasing rapidly, accelerated by the meaningful use initiatives associated with the Center for Medicare & Medicaid Services EHR Incentive Programs. The transition to electronic medical records and availability of patient data has been associated with increases in the volume and complexity of patient information, as well as an increase in medical alerts, with resulting “alert fatigue” and increased expectations for rapid and accurate diagnosis and treatment. Unfortunately, these increased demands on health care providers create greater risk for diagnostic and therapeutic errors. In the near future, artificial intelligence (AI)/machine learning will likely assist physicians with differential diagnosis of disease, treatment options suggestions, and recommendations, and, in the case of medical imaging, with cues in image interpretation. Mining and advanced analysis of “big data” in health care provide the potential not only to perform “in silico” research but also to provide “real time” diagnostic and (potentially) therapeutic recommendations based on empirical data. “On demand” access to high-performance computing and large health care databases will support and sustain our ability to achieve personalized medicine. The IBM Jeopardy! Challenge, which pitted the best all-time human players against the Watson computer, captured the imagination of millions of people across the world and demonstrated the potential to apply AI approaches to a wide variety of subject matter, including medicine. The combination of AI, big data, and massively parallel computing offers the potential to create a revolutionary way of practicing evidence-based, personalized medicine.",project-academic
10.1093/BIB/BBV118,2017-01-01,a,Oxford University Press,translational bioinformatics in the era of real time biomedical health care and wellness data streams," Monitoring and modeling biomedical, health care and wellness data from individuals and converging data on a population scale have tremendous potential to improve understanding of the transition to the healthy state of human physiology to disease setting. Wellness monitoring devices and companion software applications capable of generating alerts and sharing data with health care providers or social networks are now available. The accessibility and clinical utility of such data for disease or wellness research are currently limited. Designing methods for streaming data capture, real-time data aggregation, machine learning, predictive analytics and visualization solutions to integrate wellness or health monitoring data elements with the electronic medical records (EMRs) maintained by health care providers permits better utilization. Integration of population-scale biomedical, health care and wellness data would help to stratify patients for active health management and to understand clinically asymptomatic patients and underlying illness trajectories. In this article, we discuss various health-monitoring devices, their ability to capture the unique state of health represented in a patient and their application in individualized diagnostics, prognosis, clinical or wellness intervention. We also discuss examples of translational bioinformatics approaches to integrating patient-generated data with existing EMRs, personal health records, patient portals and clinical data repositories. Briefly, translational bioinformatics methods, tools and resources are at the center of these advances in implementing real-time biomedical and health care analytics in the clinical setting. Furthermore, these advances are poised to play a significant role in clinical decision-making and implementation of data-driven medicine and wellness care.",project-academic
10.1016/J.GIE.2019.03.019,2019-07-01,a,Elsevier,quality assurance of computer aided detection and diagnosis in colonoscopy," Recent breakthroughs in artificial intelligence (AI), specifically via its emerging sub-field ""deep learning,"" have direct implications for computer-aided detection and diagnosis (CADe and/or CADx) for colonoscopy. AI is expected to have at least 2 major roles in colonoscopy practice—polyp detection (CADe) and polyp characterization (CADx). CADe has the potential to decrease the polyp miss rate, contributing to improving adenoma detection, whereas CADx can improve the accuracy of colorectal polyp optical diagnosis, leading to reduction of unnecessary polypectomy of non-neoplastic lesions, potential implementation of a resect-and-discard paradigm, and proper application of advanced resection techniques. A growing number of medical-engineering researchers are developing both CADe and CADx systems, some of which allow real-time recognition of polyps or in vivo identification of adenomas, with over 90% accuracy. However, the quality of the developed AI systems as well as that of the study designs vary significantly, hence raising some concerns regarding the generalization of the proposed AI systems. Initial studies were conducted in an exploratory or retrospective fashion by using stored images and likely overestimating the results. These drawbacks potentially hinder smooth implementation of this novel technology into colonoscopy practice. The aim of this article is to review both contributions and limitations in recent machine-learning-based CADe and/or CADx colonoscopy studies and propose some principles that should underlie system development and clinical testing.",project-academic
,2019-01-01,a,Elsevier,quality assurance of computer aided detection and diagnosis in colonoscopy," Recent breakthroughs in artificial intelligence (AI), specifically via its emerging sub-field ""deep learning,"" have direct implications for computer-aided detection and diagnosis (CADe and/or CADx) for colonoscopy. AI is expected to have at least 2 major roles in colonoscopy practice—polyp detection (CADe) and polyp characterization (CADx). CADe has the potential to decrease the polyp miss rate, contributing to improving adenoma detection, whereas CADx can improve the accuracy of colorectal polyp optical diagnosis, leading to reduction of unnecessary polypectomy of non-neoplastic lesions, potential implementation of a resect-and-discard paradigm, and proper application of advanced resection techniques. A growing number of medical-engineering researchers are developing both CADe and CADx systems, some of which allow real-time recognition of polyps or in vivo identification of adenomas, with over 90% accuracy. However, the quality of the developed AI systems as well as that of the study designs vary significantly, hence raising some concerns regarding the generalization of the proposed AI systems. Initial studies were conducted in an exploratory or retrospective fashion by using stored images and likely overestimating the results. These drawbacks potentially hinder smooth implementation of this novel technology into colonoscopy practice. The aim of this article is to review both contributions and limitations in recent machine-learning-based CADe and/or CADx colonoscopy studies and propose some principles that should underlie system development and clinical testing.",project-academic
,2020-03-24,a,,covid 19 and computer audition an overview on what speech sound analysis could contribute in the sars cov 2 corona crisis," At the time of writing, the world population is suffering from more than 10,000 registered COVID-19 disease epidemic induced deaths since the outbreak of the Corona virus more than three months ago now officially known as SARS-CoV-2. Since, tremendous efforts have been made worldwide to counter-steer and control the epidemic by now labelled as pandemic. In this contribution, we provide an overview on the potential for computer audition (CA), i.e., the usage of speech and sound analysis by artificial intelligence to help in this scenario. We first survey which types of related or contextually significant phenomena can be automatically assessed from speech or sound. These include the automatic recognition and monitoring of breathing, dry and wet coughing or sneezing sounds, speech under cold, eating behaviour, sleepiness, or pain to name but a few. Then, we consider potential use-cases for exploitation. These include risk assessment and diagnosis based on symptom histograms and their development over time, as well as monitoring of spread, social distancing and its effects, treatment and recovery, and patient wellbeing. We quickly guide further through challenges that need to be faced for real-life usage. We come to the conclusion that CA appears ready for implementation of (pre-)diagnosis and monitoring tools, and more generally provides rich and significant, yet so far untapped potential in the fight against COVID-19 spread.",project-academic
10.3389/FDGTH.2021.564906,2021-03-29,a,Front Digit Health,covid 19 and computer audition an overview on what speech sound analysis could contribute in the sars cov 2 corona crisis," At the time of writing this article, the world population is suffering from more than 2 million registered COVID-19 disease epidemic-induced deaths since the outbreak of the corona virus, which is now officially known as SARS-CoV-2. However, tremendous efforts have been made worldwide to counter-steer and control the epidemic by now labelled as pandemic. In this contribution, we provide an overview on the potential for computer audition (CA), i.e., the usage of speech and sound analysis by artificial intelligence to help in this scenario. We first survey which types of related or contextually significant phenomena can be automatically assessed from speech or sound. These include the automatic recognition and monitoring of COVID-19 directly or its symptoms such as breathing, dry, and wet coughing or sneezing sounds, speech under cold, eating behaviour, sleepiness, or pain to name but a few. Then, we consider potential use-cases for exploitation. These include risk assessment and diagnosis based on symptom histograms and their development over time, as well as monitoring of spread, social distancing and its effects, treatment and recovery, and patient well-being. We quickly guide further through challenges that need to be faced for real-life usage and limitations also in comparison with non-audio solutions. We come to the conclusion that CA appears ready for implementation of (pre-)diagnosis and monitoring tools, and more generally provides rich and significant, yet so far untapped potential in the fight against COVID-19 spread.",project-academic
10.1016/J.COMCOM.2020.02.069,2020-03-15,a,Elsevier,applications of artificial intelligence and machine learning in smart cities," Abstract None None Smart cities are aimed to efficiently manage growing urbanization, energy consumption, maintain a green environment, improve the economic and living standards of their citizens, and raise the people’s capabilities to efficiently use and adopt the modern information and communication technology (ICT). In the smart cities concept, ICT is playing a vital role in policy design, decision, implementation, and ultimate productive services. The primary objective of this review is to explore the role of artificial intelligence (AI), machine learning (ML), and deep reinforcement learning (DRL) in the evolution of smart cities. The preceding techniques are efficiently used to design optimal policy regarding various smart city-oriented complex problems. In this survey, we present in-depth details of the applications of the prior techniques in intelligent transportation systems (ITSs), cyber-security, energy-efficient utilization of smart grids (SGs), effective use of unmanned aerial vehicles (UAVs) to assure the best services of 5G and beyond 5G (B5G) communications, and smart health care system in a smart city. Finally, we present various research challenges and future research directions where the aforementioned techniques can play an outstanding role to realize the concept of a smart city.",project-academic
10.3390/E17042367,2015-04-20,a,Multidisciplinary Digital Publishing Institute,an entropy based network anomaly detection method," Data mining is an interdisciplinary subfield of computer science involving methods at the intersection of artificial intelligence, machine learning and statistics. One of the data mining tasks is anomaly detection which is the analysis of large quantities of data to identify items, events or observations which do not conform to an expected pattern. Anomaly detection is applicable in a variety of domains, e.g., fraud detection, fault detection, system health monitoring but this article focuses on application of anomaly detection in the field of network intrusion detection.The main goal of the article is to prove that an entropy-based approach is suitable to detect modern botnet-like malware based on anomalous patterns in network. This aim is achieved by realization of the following points: (i) preparation of a concept of original entropy-based network anomaly detection method, (ii) implementation of the method, (iii) preparation of original dataset, (iv) evaluation of the method.",project-academic
10.1016/J.JPOWSOUR.2017.01.105,2017-03-15,a,Elsevier,adaptive prognosis of lithium ion batteries based on the combination of particle filters and radial basis function neural networks," Abstract None None Lithium-Ion rechargeable batteries are widespread power sources with applications to consumer electronics, electrical vehicles, unmanned aerial and spatial vehicles, etc. The failure to supply the required power levels may lead to severe safety and economical consequences. Thus, in view of the implementation of adequate maintenance strategies, the development of diagnostic and prognostic tools for monitoring the state of health of the batteries and predicting their remaining useful life is becoming a crucial task. Here, we propose a method for predicting the end of discharge of Li-Ion batteries, which stems from the combination of particle filters with radial basis function neural networks. The major innovation lies in the fact that the radial basis function model is adaptively trained on-line, i.e., its parameters are identified in real time by the particle filter as new observations of the battery terminal voltage become available. By doing so, the prognostic algorithm achieves the flexibility needed to provide sound end-of-discharge time predictions as the charge-discharge cycles progress, even in presence of anomalous behaviors due to failures or unforeseen operating conditions. The method is demonstrated with reference to actual Li-Ion battery discharge data contained in the prognostics data repository of the NASA Ames Research Center database.",project-academic
10.1186/1472-6947-13-71,2013-07-11,a,BioMed Central,development and evaluation of a de identification procedure for a case register sourced from mental health electronic records," Electronic health records (EHRs) provide enormous potential for health research but also present data governance challenges. Ensuring de-identification is a pre-requisite for use of EHR data without prior consent. The South London and Maudsley NHS Trust (SLaM), one of the largest secondary mental healthcare providers in Europe, has developed, from its EHRs, a de-identified psychiatric case register, the Clinical Record Interactive Search (CRIS), for secondary research. We describe development, implementation and evaluation of a bespoke de-identification algorithm used to create the register. It is designed to create dictionaries using patient identifiers (PIs) entered into dedicated source fields and then identify, match and mask them (with ZZZZZ) when they appear in medical texts. We deemed this approach would be effective, given high coverage of PI in the dedicated fields and the effectiveness of the masking combined with elements of a security model. We conducted two separate performance tests i) to test performance of the algorithm in masking individual true PIs entered in dedicated fields and then found in text (using 500 patient notes) and ii) to compare the performance of the CRIS pattern matching algorithm with a machine learning algorithm, called the MITRE Identification Scrubber Toolkit – MIST (using 70 patient notes – 50 notes to train, 20 notes to test on). We also report any incidences of potential breaches, defined by occurrences of 3 or more true or apparent PIs in the same patient’s notes (and in an additional set of longitudinal notes for 50 patients); and we consider the possibility of inferring information despite de-identification. True PIs were masked with 98.8% precision and 97.6% recall. As anticipated, potential PIs did appear, owing to misspellings entered within the EHRs. We found one potential breach. In a separate performance test, with a different set of notes, CRIS yielded 100% precision and 88.5% recall, while MIST yielded a 95.1% and 78.1%, respectively. We discuss how we overcome the realistic possibility – albeit of low probability – of potential breaches through implementation of the security model. CRIS is a de-identified psychiatric database sourced from EHRs, which protects patient anonymity and maximises data available for research. CRIS demonstrates the advantage of combining an effective de-identification algorithm with a carefully designed security model. The paper advances much needed discussion of EHR de-identification – particularly in relation to criteria to assess de-identification, and considering the contexts of de-identified research databases when assessing the risk of breaches of confidential patient information.",project-academic
10.7150/THNO.28447,2019-01-01,a,Ivyspring International Publisher,artificial intelligence based decision making for age related macular degeneration," Artificial intelligence (AI) based on convolutional neural networks (CNNs) has a great potential to enhance medical workflow and improve health care quality. Of particular interest is practical implementation of such AI-based software as a cloud-based tool aimed for telemedicine, the practice of providing medical care from a distance using electronic interfaces. Methods: In this study, we used a dataset of labeled 35,900 optical coherence tomography (OCT) images obtained from age-related macular degeneration (AMD) patients and used them to train three types of CNNs to perform AMD diagnosis. Results: Here, we present an AI- and cloud-based telemedicine interaction tool for diagnosis and proposed treatment of AMD. Through deep learning process based on the analysis of preprocessed optical coherence tomography (OCT) imaging data, our AI-based system achieved the same image discrimination rate as that of retinal specialists in our hospital. The AI platform's detection accuracy was generally higher than 90% and was significantly superior (p < 0.001) to that of medical students (69.4% and 68.9%) and equal (p = 0.99) to that of retinal specialists (92.73% and 91.90%). Furthermore, it provided appropriate treatment recommendations comparable to those of retinal specialists. Conclusions: We therefore developed a website for realistic cloud computing based on this AI platform, available at https://www.ym.edu.tw/~AI-OCT/. Patients can upload their OCT images to the website to verify whether they have AMD and require treatment. Using an AI-based cloud service represents a real solution for medical imaging diagnostics and telemedicine.",project-academic
10.1109/ICASSP.2019.8682194,2019-05-12,p,IEEE,1 d convolutional neural networks for signal processing applications," 1D Convolutional Neural Networks (CNNs) have recently become the state-of-the-art technique for crucial signal processing applications such as patient-specific ECG classification, structural health monitoring, anomaly detection in power electronics circuitry and motor-fault detection. This is an expected outcome as there are numerous advantages of using an adaptive and compact 1D CNN instead of a conventional (2D) deep counterparts. First of all, compact 1D CNNs can be efficiently trained with a limited dataset of 1D signals while the 2D deep CNNs, besides requiring 1D to 2D data transformation, usually need datasets with massive size, e.g., in the ""Big Data"" scale in order to prevent the well-known ""overfitting"" problem. 1D CNNs can directly be applied to the raw signal (e.g., current, voltage, vibration, etc.) without requiring any pre- or post-processing such as feature extraction, selection, dimension reduction, denoising, etc. Furthermore, due to the simple and compact configuration of such adaptive 1D CNNs that perform only linear 1D convolutions (scalar multiplications and additions), a real-time and low-cost hardware implementation is feasible. This paper reviews the major signal processing applications of compact 1D CNNs with a brief theoretical background. We will present their state-of-the-art performances and conclude with focusing on some major properties. Keywords – 1-D CNNs, Biomedical Signal Processing, SHM",project-academic
,2000-01-01,b,,safe and sound artificial intelligence in hazardous applications," Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",project-academic
10.1121/1.1480419,2000-12-01,b,CRC Press,handbook of neural network signal processing," From the Publisher:
The use of neural networks is permeating every area of signal processing. They can provide powerful means for solving many problems, especially in nonlinear, real-time, adaptive, and blind signal processing. The Handbook of Neural Network Signal Processing brings together applications that were previously scattered among various publications to provide an up-to-date, detailed treatment of the subject from an engineering point of view.The authors cover basic principles, modeling, algorithms, architectures, implementation procedures, and well-designed simulation examples of audio, video, speech, communication, geophysical, sonar, radar, medical, and many other signals. The subject of neural networks and their application to signal processing is constantly improving. You need a handy reference that will inform you of current applications in this new area. The Handbook of Neural Network Signal Processing provides this much needed service for all engineers and scientists in the field.",project-academic
10.1016/J.CRAD.2019.02.006,2019-05-01,a,Clin Radiol,artificial intelligence in breast imaging," This article reviews current limitations and future opportunities for the application of computer-aided detection (CAD) systems and artificial intelligence in breast imaging. Traditional CAD systems in mammography screening have followed a rules-based approach, incorporating domain knowledge into hand-crafted features before using classical machine learning techniques as a classifier. The first commercial CAD system, ImageChecker M1000, relies on computer vision techniques for pattern recognition. Unfortunately, CAD systems have been shown to adversely affect some radiologists' performance and increase recall rates. The Digital Mammography DREAM Challenge was a multidisciplinary collaboration that provided 640,000 mammography images for teams to help decrease false-positive rates in breast cancer screening. Winning solutions leveraged deep learning's (DL) automatic hierarchical feature learning capabilities and used convolutional neural networks. Start-ups Therapixel and Kheiron Medical Technologies are using DL for breast cancer screening. With increasing use of digital breast tomosynthesis, specific artificial intelligence (AI)-CAD systems are emerging to include iCAD's PowerLook Tomo Detection and ScreenPoint Medical's Transpara. Other AI-CAD systems are focusing on breast diagnostic techniques such as ultrasound and magnetic resonance imaging (MRI). There is a gap in the market for contrast-enhanced spectral mammography AI-CAD tools. Clinical implementation of AI-CAD tools requires testing in scenarios mimicking real life to prove its usefulness in the clinical environment. This requires a large and representative dataset for testing and assessment of the reader's interaction with the tools. A cost-effectiveness assessment should be undertaken, with a large feasibility study carried out to ensure there are no unintended consequences. AI-CAD systems should incorporate explainable AI in accordance with the European Union General Data Protection Regulation (GDPR).",project-academic
10.1109/ACCESS.2018.2875677,2018-10-12,a,IEEE,patient2vec a personalized interpretable deep representation of the longitudinal electronic health record," The wide implementation of electronic health record (EHR) systems facilitates the collection of large-scale health data from real clinical settings. Despite the significant increase in adoption of EHR systems, these data remain largely unexplored, but present a rich data source for knowledge discovery from patient health histories in tasks, such as understanding disease correlations and predicting health outcomes. However, the heterogeneity, sparsity, noise, and bias in these data present many complex challenges. This complexity makes it difficult to translate potentially relevant information into machine learning algorithms. In this paper, we propose a computational framework, None Patient2Vec , to learn an interpretable deep representation of longitudinal EHR data, which is personalized for each patient. To evaluate this approach, we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive performance with baseline methods. None Patient2Vec None produces a vector space with meaningful structure, and it achieves an area under curve around 0.799, outperforming baseline methods. In the end, the learned feature importance can be visualized and interpreted at both the individual and population levels to bring clinical insights.",project-academic
10.1109/MSP.2019.2903715,2019-06-26,a,IEEE,radar signal processing for sensing in assisted living the challenges associated with real time implementation of emerging algorithms," This article covers radar signal processing for sensing in the context of assisted living (AL). This is presented through three example applications: human activity recognition (HAR) for activities of daily living (ADL), respiratory disorders, and sleep stages (SSs) classification. The common challenge of classification is discussed within a framework of measurements/preprocessing, feature extraction, and classification algorithms for supervised learning. Then, the specific challenges of the three applications from a signal processing standpoint are detailed in their specific data processing and ad hoc classification strategies. Here, the focus is on recent trends in the field of activity recognition (multidomain, multimodal, and fusion), health-care applications based on vital signs (superresolution techniques), and comments related to outstanding challenges. Finally, this article explores challenges associated with the real-time implementation of signal processing/classification algorithms.",project-academic
,2017-09-08,,,personalized recommendation system based on deep learning under social network," The invention discloses a personalized recommendation system based on deep learning under a social network. The system mainly comprises an offline learning module and an online recommendation module. The offline learning module firstly generates a training sample seat to construct a deep convolutional neural network learning module with an attention mechanism and carries out iterative optimization on parameters in the learning module; and the online recommendation module carries out real-time item recommendation on a newly-registered user based on the learning model obtained through training. Compared with the prior art, the system has the advantages of high accuracy, fast speed and simplicity and easiness in implementation and can be effectively applied to the fields, such as electronic commerce, public opinion monitoring, intelligent transportation and medical treatment and health.",project-academic
10.1109/ACCESS.2018.2846609,2018-06-12,a,IEEE,ubehealth a personalized ubiquitous cloud and edge enabled networked healthcare system for smart cities," Smart city advancements are driving massive transformations of healthcare, the largest global industry. The drivers include increasing demands for ubiquitous, preventive, and personalized healthcare, to be provided to the public at reduced risks and costs. Mobile cloud computing could potentially meet the future healthcare demands by enabling anytime, anywhere capture and analyses of patients’ data. However, network latency, bandwidth, and reliability are among the many challenges hindering the realization of next-generation healthcare. This paper proposes a ubiquitous healthcare framework, UbeHealth, that leverages edge computing, deep learning, big data, high-performance computing (HPC), and the Internet of Things (IoT) to address the aforementioned challenges. The framework enables an enhanced network quality of service using its three main components and four layers. Deep learning, big data, and HPC are used to predict network traffic, which in turn are used by the Cloudlet and network layers to optimize data rates, data caching, and routing decisions. Application protocols of the traffic flows are classified, enabling the network layer to meet applications’ communication requirements better and to detect malicious traffic and anomalous data. Clustering is used to identify the different kinds of data originating from the same application protocols. A proof of concept UbeHealth system has been developed based on the framework. A detailed literature review is used to capture the design requirements for the proposed system. The system is described in detail including the algorithmic implementation of the three components and four layers. Three widely used data sets are used to evaluate the UbeHealth system.",project-academic
10.3414/ME00-01-0052,2011-09-14,a,NIH Public Access,probability machines consistent probability estimation using nonparametric learning machines," Summary Background—Most machine learning approaches only provide a classification for binary responses. However, probabilities are required for risk estimation using individual patient characteristics. It has been shown recently that every statistical learning machine known to be consistent for a nonparametric regression problem is a probability machine that is provably consistent for this estimation problem. Objectives—The aim of this paper is to show how random forests and nearest neighbors can be used for consistent estimation of individual probabilities. Methods—Two random forest algorithms and two nearest neighbor algorithms are described in detail for estimation of individual probabilities. We discuss the consistency of random forests, nearest neighbors and other learning machines in detail. We conduct a simulation study to illustrate the validity of the methods. We exemplify the algorithms by analyzing two well-known data sets on the diagnosis of appendicitis and the diagnosis of diabetes in Pima Indians. Results—Simulations demonstrate the validity of the method. With the real data application, we show the accuracy and practicality of this approach. We provide sample code from R packages in which the probability estimation is already available. This means that all calculations can be performed using existing software. Conclusions—Random forest algorithms as well as nearest neighbor approaches are valid machine learning methods for estimating individual probabilities for binary responses. Freely available implementations are available in R and may be used for applications.",project-academic
10.1111/DEN.13317,2019-02-14,a,Dig Endosc,artificial intelligence and upper gastrointestinal endoscopy current status and future perspective," With recent breakthroughs in artificial intelligence, computer-aided diagnosis (CAD) for upper gastrointestinal endoscopy is gaining increasing attention. Main research focuses in this field include automated identification of dysplasia in Barrett's esophagus and detection of early gastric cancers. By helping endoscopists avoid missing and mischaracterizing neoplastic change in both the esophagus and the stomach, these technologies potentially contribute to solving current limitations of gastroscopy. Currently, optical diagnosis of early-stage dysplasia related to Barrett's esophagus can be precisely achieved only by endoscopists proficient in advanced endoscopic imaging, and the false-negative rate for detecting gastric cancer is approximately 10%. Ideally, these novel technologies should work during real-time gastroscopy to provide on-site decision support for endoscopists regardless of their skill; however, previous studies of these topics remain ex vivo and experimental in design. Therefore, the feasibility, effectiveness, and safety of CAD for upper gastrointestinal endoscopy in clinical practice remain unknown, although a considerable number of pilot studies have been conducted by both engineers and medical doctors with excellent results. This review summarizes current publications relating to CAD for upper gastrointestinal endoscopy from the perspective of endoscopists and aims to indicate what is required for future research and implementation in clinical practice.",project-academic
10.1097/ACO.0B013E328337339C,2010-04-01,a,Curr Opin Anaesthesiol,anesthesia 2 0 internet based information resources and web 2 0 applications in anesthesia education," PURPOSE OF REVIEW Informatics is a broad field encompassing artificial intelligence, cognitive science, computer science, information science, and social science. The goal of this review is to illustrate how Web 2.0 information technologies could be used to improve anesthesia education. RECENT FINDINGS Educators in all specialties of medicine are increasingly studying Web 2.0 technologies to maximize postgraduate medical education of housestaff. These technologies include microblogging, blogs, really simple syndication (RSS) feeds, podcasts, wikis, and social bookmarking and networking. 'Anesthesia 2.0' reflects our expectation that these technologies will foster innovation and interactivity in anesthesia-related web resources which embraces the principles of openness, sharing, and interconnectedness that represent the Web 2.0 movement. Although several recent studies have shown benefits of implementing these systems into medical education, much more investigation is needed. SUMMARY Although direct practice and observation in the operating room are essential, Web 2.0 technologies hold great promise to innovate anesthesia education and clinical practice such that the resident learner need not be in a classroom for a didactic talk, or even in the operating room to see how an arterial line is properly placed. Thoughtful research to maximize implementation of these technologies should be a priority for development by academic anesthesiology departments. Web 2.0 and advanced informatics resources will be part of physician lifelong learning and clinical practice.",project-academic
10.1109/TIM.2011.2169182,2012-02-01,a,IEEE,prediction of machine health condition using neuro fuzzy and bayesian algorithms," This paper proposes a novel approach for machine health condition prognosis based on neuro-fuzzy systems (NFSs) and Bayesian algorithms. The NFS, after training with machine condition data, is employed as a prognostic model to forecast the evolution of the machine fault state with time. An online model update scheme is developed on the basis of the probability density function (PDF) of the NFS residuals between the actual and predicted condition data. Bayesian estimation algorithms adopt the model's predicted data as prior information in combination with online measurements to update the degree of belief in the forecasting estimations. In order to simplify the implementation of the proposed approach, a recursive Bayesian algorithm called particle filtering is utilized to calculate in real time a posterior PDF by a set of random samples (or particles) with associated weights. When new data become available, the weights of all particles are updated, and then, predictions are carried out, which form the PDF of the predicted estimations. The developed method is evaluated via two experimental cases-a cracked carrier plate and a faulty bearing. The prediction performance is compared with three prevalent machine condition predictors-recurrent neural networks, NFSs, and recurrent NFSs. The results demonstrate that the proposed approach can predict machine conditions more accurately.",project-academic
10.23736/S0393-2249.19.03613-0,2020-02-01,a,Edizioni Minerva Medica,artificial intelligence and neural networks in urology current clinical applications," INTRODUCTION As we enter the era of ""big data,"" an increasing amount of complex health-care data will become available. These data are often redundant, ""noisy,"" and characterized by wide variability. In order to offer a precise and transversal view of a clinical scenario the artificial intelligence (AI) with machine learning (ML) algorithms and Artificial neuron networks (ANNs) process were adopted, with a promising wide diffusion in the near future. The present work aims to provide a comprehensive and critical overview of the current and potential applications of AI and ANNs in urology. EVIDENCE ACQUISITION A non-systematic review of the literature was performed by screening Medline, PubMed, the Cochrane Database, and Embase to detect pertinent studies regarding the application of AI and ANN in Urology. EVIDENCE SYNTHESIS The main application of AI in urology is the field of genitourinary cancers. Focusing on prostate cancer, AI was applied for the prediction of prostate biopsy results. For bladder cancer, the prediction of recurrence-free probability and diagnostic evaluation were analysed with ML algorithms. For kidney and testis cancer, anecdotal experiences were reported for staging and prediction of diseases recurrence. More recently, AI has been applied in non-oncological diseases like stones and functional urology. CONCLUSIONS AI technologies are growing their role in health care; but, up to now, their ""real-life"" implementation remains limited. However, in the near future, the potential of AI-driven era could change the clinical practice in Urology, improving overall patient outcomes.",project-academic
10.1093/JAS/SKY014,2018-04-14,a,American Society of Animal Science,big data analytics and precision animal agriculture symposium machine learning and data mining advance predictive big data analysis in precision animal agriculture," Precision animal agriculture is poised to rise to prominence in the livestock enterprise in the domains of management, production, welfare, sustainability, health surveillance, and environmental footprint. Considerable progress has been made in the use of tools to routinely monitor and collect information from animals and farms in a less laborious manner than before. These efforts have enabled the animal sciences to embark on information technology-driven discoveries to improve animal agriculture. However, the growing amount and complexity of data generated by fully automated, high-throughput data recording or phenotyping platforms, including digital images, sensor and sound data, unmanned systems, and information obtained from real-time noninvasive computer vision, pose challenges to the successful implementation of precision animal agriculture. The emerging fields of machine learning and data mining are expected to be instrumental in helping meet the daunting challenges facing global agriculture. Yet, their impact and potential in ""big data"" analysis have not been adequately appreciated in the animal science community, where this recognition has remained only fragmentary. To address such knowledge gaps, this article outlines a framework for machine learning and data mining and offers a glimpse into how they can be applied to solve pressing problems in animal sciences.",project-academic
10.1371/JOURNAL.PGEN.1004754,2014-11-13,a,Public Library of Science,regularized machine learning in the genetic prediction of complex traits," Supervised machine learning aims at constructing a genotype–phenotype model by learning such genetic patterns from a labeled set of training examples that will also provide accurate phenotypic predictions in new cases with similar genetic background. Such predictive models are increasingly being applied to the mining of panels of genetic variants, environmental, or other nongenetic factors in the prediction of various complex traits and disease phenotypes [1]–[8]. These studies are providing increasing evidence in support of the idea that machine learning provides a complementary view into the analysis of high-dimensional genetic datasets as compared to standard statistical association testing approaches. In contrast to identifying variants explaining most of the phenotypic variation at the population level, supervised machine learning models aim to maximize the predictive (or generalization) power at the level of individuals, hence providing exciting opportunities for e.g., individualized risk prediction based on personal genetic profiles [9]–[11]. Machine learning models can also deal with genetic interactions, which are known to play an important role in the development and treatment of many complex diseases [12]–[16], but are often missed by single-locus association tests [17]. Even in the absence of significant single-loci marginal effects, multilocus panels from distinct molecular pathways may provide synergistic contribution to the prediction power, thereby revealing part of such hidden heritability component that has remained missing because of too small marginal effects to pass the stringent genome-wide significance filters [18]. Multivariate modeling approaches have already been shown to provide improved insights into genetic mechanisms and the interaction networks behind many complex traits, including atherosclerosis, coronary heart disease, and lipid levels, which would have gone undetected using the standard univariate modeling [2], [19]–[22]. However, machine learning models also come with inherent pitfalls, such as increased computational complexity and the risk for model overfitting, which must be understood in order to avoid reporting unrealistic prediction models or over-optimistic prediction results.

We argue here that many medical applications of machine learning models in genetic disease risk prediction rely essentially on two factors: effective model regularization and rigorous model validation. We demonstrate the effects of these factors using representative examples from the literature as well as illustrative case examples. This review is not meant to be a comprehensive survey of all predictive modeling approaches, but we focus on regularized machine learning models, which enforces constraints on the complexity of the learned models so that they would ignore irrelevant patterns in the training examples. Simple risk allele counting or other multilocus risk models that do not incorporate any model parameters to be learned are outside the scope of this review; in fact, such simplistic models that assume independent variants may lead to suboptimal prediction performance in the presence of either direct or indirect interactions through epistasis effects or linkage disequilibrium, respectively [23], [24]. Perhaps the simplest models considered here as learning approaches are those based on weighted risk allele summaries [23], [25]. However, even with such basic risk models intended for predictive purposes, it is important to learn the model parameters (e.g., select the variants and determine their weights) based on training data only; otherwise there is a severe risk of model overfitting, i.e., models not being capable of generalizing to new samples [5]. Representative examples of how model learning and regularization approaches address the overfitting problem are briefly summarized in Box 1, while those readers interested in their implementation details are referred to the accompanying Text S1. We specifically promote here the use of such regularized machine learning models that are scalable to the entire genome-wide scale, often based on linear models, which are easy to interpret and also enable straightforward variable selection. Genome-scale approaches avoid the need of relying on two-stage approaches [26], which apply standard statistical procedures to reduce the number of variants, since such prefiltering may miss predictive interactions across loci and therefore lead to reduced predictive performance [8], [24], [25], [27], [28].


Box 1. Synthesis of Learning Models for Genetic Risk Prediction
The aim of risk models is to capture in a mathematical form the patterns in the genetic and non-genetic data most important for the prediction of disease susceptibility. The first step in model building involves choosing the functional form of the model (e.g., linear or nonlinear), and then making use of a given training data to determine the adjustable parameters of the model (e.g., a subset of variants, their weights, and other model parameters). While it is often sufficient for a statistical model to enable high enough explanatory power in the discovery material, without being overly complicated, a predictive model is also required to generalize to unseen cases.

One consideration in the model construction is how to encode the genotypic measurements using genotype models, such as the dominant, recessive, multiplicative, or additive model, each implying different assumptions about the genetic effects in the data [79]. Categorical variables 0, 1, and 2 are typically used for treating genetic predictor variables (e.g., minor allele dosage), while numeric values are required for continuous risk factors (e.g., blood pressure). Expected posterior probabilities of the genotypes can also be used, especially for imputed genotypes. Transforming the genotype categories into three binary features is an alternative way to deal with missing values without imputation (used in the T1D example; see Text S1 for details).


Statistical or machine learning models identify statistical or predictive interactions, respectively, rather than biological interactions between or within variants [12], [80]. While nonlinear models may better capture complex genetic interactions [7], [81], linear models are easier to interpret and provide a scalable option for performing supervised selection of multilocus variant panels at the genome-wide scale [3]. In linear models, genetic interactions are modeled implicitly by selecting such variant combinations that together are predictive of the phenotype, rather than considering pairwise gene–gene relationships explicitly. Formally, trait yi to be predicted for an individual i is modeled as a linear combination of the individual's predictor variables xij:




(1)


Here, the weights wj are assumed constant across the n individuals, w 0 is the bias offset term and p indicates the number of predictors discovered in the training data. In its basic form, Eq. 1 can be used for modeling continuous traits y (linear regression). For case-control classification, the binary dependent variable y is often transformed using a logistic loss function, which models the probability of a case class given a genotype profile and other risk factor covariates x (logistic regression). It has been shown that the logistic regression and naive Bayes risk models are mathematically very closely related in the context of genetic risk prediction [81].",project-academic
10.1167/TVST.9.2.5,2020-02-10,a,The Association for Research in Vision and Ophthalmology,artificial intelligence in retinopathy of prematurity diagnosis," Retinopathy of prematurity (ROP) is a leading cause of childhood blindness worldwide. The diagnosis of ROP is subclassified by zone, stage, and plus disease, with each area demonstrating significant intra- and interexpert subjectivity and disagreement. In addition to improved efficiencies for ROP screening, artificial intelligence may lead to automated, quantifiable, and objective diagnosis in ROP. This review focuses on the development of artificial intelligence for automated diagnosis of plus disease in ROP and highlights the clinical and technical challenges of both the development and implementation of artificial intelligence in the real world.",project-academic
10.1093/JAMIA/OCZ152,2020-02-01,a,J Am Med Inform Assoc,consumer health information and question answering helping consumers find answers to their health related information needs," Objective None Consumers increasingly turn to the internet in search of health-related information; and they want their questions answered with short and precise passages, rather than needing to analyze lists of relevant documents returned by search engines and reading each document to find an answer. We aim to answer consumer health questions with information from reliable sources. None Materials and methods None We combine knowledge-based, traditional machine and deep learning approaches to understand consumers' questions and select the best answers from consumer-oriented sources. We evaluate the end-to-end system and its components on simple questions generated in a pilot development of MedlinePlus Alexa skill, as well as the short and long real-life questions submitted to the National Library of Medicine by consumers. None Results None Our system achieves 78.7% mean average precision and 87.9% mean reciprocal rank on simple Alexa questions, and 44.5% mean average precision and 51.6% mean reciprocal rank on real-life questions submitted by National Library of Medicine consumers. None Discussion None The ensemble of deep learning, domain knowledge, and traditional approaches recognizes question type and focus well in the simple questions, but it leaves room for improvement on the real-life consumers' questions. Information retrieval approaches alone are sufficient for finding answers to simple Alexa questions. Answering real-life questions, however, benefits from a combination of information retrieval and inference approaches. None Conclusion None A pilot practical implementation of research needed to help consumers find reliable answers to their health-related questions demonstrates that for most questions the reliable answers exist and can be found automatically with acceptable accuracy.",project-academic
10.1016/J.JAIP.2021.03.039,2021-04-05,a,Elsevier,predicting severe asthma exacerbations in children blueprint for today and tomorrow," Severe asthma exacerbations are the primary cause of morbidity and mortality in children with asthma. Accurate prediction of children at risk for severe exacerbations, defined as those requiring systemic corticosteroids, emergency department visit, and/or hospitalization, would considerably reduce health care utilization and improve symptoms and quality of life. Substantial progress has been made in identifying high-risk exacerbation-prone children. Known risk factors for exacerbations include demographic characteristics (ie, low income, minority race/ethnicity), poor asthma control, environmental exposures (ie, aeroallergen exposure/sensitization, concomitant viral infection), inflammatory biomarkers, genetic polymorphisms, and markers from other ""omic"" technologies. The strongest risk factor for a future severe exacerbation remains having had one in the previous year. Combining risk factors into composite scores and use of advanced predictive analytic techniques such as machine learning are recent methods used to achieve stronger prediction of severe exacerbations. However, these methods are limited in prediction efficiency and are currently unable to predict children at risk for impending (within days) severe exacerbations. Thus, we provide a commentary on strategies that have potential to allow for accurate and reliable prediction of children at risk for impending exacerbations. These approaches include implementation of passive, real-time monitoring of impending exacerbation predictors, use of population health strategies, prediction of severe exacerbation responders versus nonresponders to conventional exacerbation management, and considerations for preschool-age children who can be especially high risk. Rigorous prediction and prevention of severe asthma exacerbations is needed to advance asthma management and improve the associated morbidity and mortality.",project-academic
10.1145/3191752,2018-03-26,a,ACM,third eye a mobilephone enabled crowdsensing system for air quality monitoring," Air pollution has raised people's public health concerns in major cities, especially for Particulate Matter under 2.5μm (PM2.5) due to its significant impact on human respiratory and circulation systems. In this paper, we present the design, implementation, and evaluation of a mobile application, Third-Eye, that can turn mobile phones into high-quality PM2.5 monitors, thereby enabling a crowdsensing way for fine-grained PM2.5 monitoring in the city. We explore two ways, crowdsensing and web crawling, to efficiently build large-scale datasets of the outdoor images taken by mobile phone, weather data, and air-pollution data. Then, we leverage two deep learning models, Convolutional Neural Network (CNN) for images and Long Short Term Memory (LSTM) network for weather and air-pollution data, to build an end-to-end framework for training PM2.5 inference models. Our App has been downloaded more than 2,000 times and runs more than 1 year. The real user data based evaluation shows that Third-Eye achieves 17.38 μg/m3 average error and 81.55% classification accuracy, which outperforms 5 state-of-the-art methods, including three scattered interpolations and two image based estimation methods. The results also demonstrate how Third-Eye offers substantial enhancements over typical portable PM2.5 monitors by simultaneously improving accessibility, portability, and accuracy.",project-academic
,2019-01-08,,,tumor malignant risk stratification auxiliary diagnosis system of artificial intelligence medical image," The invention discloses a tumor malignant risk stratification auxiliary diagnosis system of an artificial intelligence medical image. The system comprises a data acquisition module, a data preprocessing module, a model establishment module, a model verification and optimization module, a stratification diagnosis module and a database platform. The tumor malignant risk stratification auxiliary diagnosis system of the invention is based on artificial intelligence technology, successive stratification of the malignant risk of the tumor can be achieved, the clinical diagnosis thinking is simulated, based on the high-precision ability of detecting benign lesions and malignant tumors of artificial intelligence model, and the space-occupying lesions with definite imaging features are diagnosed automatically. As a result, the system can substantially assist the clinical management decision of space-occupying lesions, improve the existing work flow of clinical diagnosis, increase the confidenceof doctors in diagnosis, reduce the work pressure, reduce the anxiety of patients with low-risk malignant lesions, greatly improve the diagnostic rate of benign lesions and malignant tumors, and hopefully realize the landing implementation of artificial intelligence clinical auxiliary diagnosis.",project-academic
10.1063/1.5122249,2020-01-16,a,American Institute of Physics,electrolyte gated transistors for synaptic electronics neuromorphic computing and adaptable biointerfacing," Functional emulation of biological synapses using electronic devices is regarded as the first step toward neuromorphic engineering and artificial neural networks (ANNs). Electrolyte-gated transistors (EGTs) are mixed ionic–electronic conductivity devices capable of efficient gate-channel capacitance coupling, biocompatibility, and flexible architectures. Electrolyte gating offers significant advantages for the realization of neuromorphic devices/architectures, including ultralow-voltage operation and the ability to form parallel-interconnected networks with minimal hardwired connectivity. In this review, the most recent developments in EGT-based electronics are introduced with their synaptic behaviors and detailed mechanisms, including short-/long-term plasticity, global regulation phenomena, lateral coupling between device terminals, and spatiotemporal correlated functions. Analog memory phenomena allow for the implementation of perceptron-based ANNs. Due to their mixed-conductivity phenomena, neuromorphic circuits based on EGTs allow for facile interfacing with biological environments. We also discuss the future challenges in implementing low power, high speed, and reliable neuromorphic computing for large-scale ANNs with these neuromorphic devices. The advancement of neuromorphic devices that rely on EGTs highlights the importance of this field for neuromorphic computing and for novel healthcare technologies in the form of adaptable or trainable biointerfacing.",project-academic
10.1002/PLD3.252,2020-08-01,a,"John Wiley & Sons, Ltd",plant science decadal vision 2020 2030 reimagining the potential of plants for a healthy and sustainable future," Plants, and the biological systems around them, are key to the future health of the planet and its inhabitants. The Plant Science Decadal Vision 2020-2030 frames our ability to perform vital and far-reaching research in plant systems sciences, essential to how we value participants and apply emerging technologies. We outline a comprehensive vision for addressing some of our most pressing global problems through discovery, practical applications, and education. The Decadal Vision was developed by the participants at the Plant Summit 2019, a community event organized by the Plant Science Research Network. The Decadal Vision describes a holistic vision for the next decade of plant science that blends recommendations for research, people, and technology. Going beyond discoveries and applications, we, the plant science community, must implement bold, innovative changes to research cultures and training paradigms in this era of automation, virtualization, and the looming shadow of climate change. Our vision and hopes for the next decade are encapsulated in the phrase reimagining the potential of plants for a healthy and sustainable future. The Decadal Vision recognizes the vital intersection of human and scientific elements and demands an integrated implementation of strategies for research (Goals 1-4), people (Goals 5 and 6), and technology (Goals 7 and 8). This report is intended to help inspire and guide the research community, scientific societies, federal funding agencies, private philanthropies, corporations, educators, entrepreneurs, and early career researchers over the next 10 years. The research encompass experimental and computational approaches to understanding and predicting ecosystem behavior; novel production systems for food, feed, and fiber with greater crop diversity, efficiency, productivity, and resilience that improve ecosystem health; approaches to realize the potential for advances in nutrition, discovery and engineering of plant-based medicines, and ""green infrastructure."" Launching the Transparent Plant will use experimental and computational approaches to break down the phytobiome into a ""parts store"" that supports tinkering and supports query, prediction, and rapid-response problem solving. Equity, diversity, and inclusion are indispensable cornerstones of realizing our vision. We make recommendations around funding and systems that support customized professional development. Plant systems are frequently taken for granted therefore we make recommendations to improve plant awareness and community science programs to increase understanding of scientific research. We prioritize emerging technologies, focusing on non-invasive imaging, sensors, and plug-and-play portable lab technologies, coupled with enabling computational advances. Plant systems science will benefit from data management and future advances in automation, machine learning, natural language processing, and artificial intelligence-assisted data integration, pattern identification, and decision making. Implementation of this vision will transform plant systems science and ripple outwards through society and across the globe. Beyond deepening our biological understanding, we envision entirely new applications. We further anticipate a wave of diversification of plant systems practitioners while stimulating community engagement, underpinning increasing entrepreneurship. This surge of engagement and knowledge will help satisfy and stoke people's natural curiosity about the future, and their desire to prepare for it, as they seek fuller information about food, health, climate and ecological systems.",project-academic
10.1007/S10877-018-0219-Z,2019-10-01,a,J Clin Monit Comput,applying machine learning to continuously monitored physiological data," The use of machine learning (ML) in healthcare has enormous potential for improving disease detection, clinical decision support, and workflow efficiencies. In this commentary, we review published and potential applications for the use of ML for monitoring within the hospital environment. We present use cases as well as several questions regarding the application of ML to the analysis of the vast amount of complex data that clinicians must interpret in the realm of continuous physiological monitoring. ML, especially employed in bidirectional conjunction with electronic health record data, has the potential to extract much more useful information out of this currently under-analyzed data source from a population level. As a data driven entity, ML is dependent on copious, high quality input data so that error can be introduced by low quality data sources. At present, while ML is being studied in hybrid formulations along with static expert systems for monitoring applications, it is not yet actively incorporated in the formal artificial learning sense of an algorithm constantly learning and updating its rules without external intervention. Finally, innovations in monitoring, including those supported by ML, will pose regulatory and medico-legal challenges, as well as questions regarding precisely how to incorporate these features into clinical care and medical education. Rigorous evaluation of ML techniques compared to traditional methods or other AI methods will be required to validate the algorithms developed with consideration of database limitations and potential learning errors. Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development: Future research is needed to evaluate all AI based programs before clinical implementation in non-research settings.",project-academic
10.1136/BMJOPEN-2021-052287,2021-07-28,a,British Medical Journal Publishing Group,diverse experts perspectives on ethical issues of using machine learning to predict hiv aids risk in sub saharan africa a modified delphi study," Objective None To better understand diverse experts’ views about the ethical implications of ongoing research funded by the National Institutes of Health that uses machine learning to predict HIV/AIDS risk in sub-Saharan Africa (SSA) based on publicly available Demographic and Health Surveys data. None Design None Three rounds of semi-structured surveys in an online expert panel using a modified Delphi approach. None Participants None Experts in informatics, African public health and HIV/AIDS and bioethics were invited to participate. None Measures None Perceived importance of or agreement about relevance of ethical issues on 5-point unipolar Likert scales. Qualitative data analysis identified emergent themes related to ethical issues and development of an ethical framework and recommendations for open-ended questions. None Results None Of the 35 invited experts, 22 participated in the online expert panel (63%). Emergent themes were the inclusion of African researchers in all aspects of study design, analysis and dissemination to identify and address local contextual issues, as well as engagement of communities. Experts focused on engagement with health and science professionals to address risks, benefits and communication of findings. Respondents prioritised the mitigation of stigma to research participants but recognised trade-offs between privacy and the need to disseminate findings to realise public health benefits. Strategies for responsible communication of results were suggested, including careful word choice in presentation of results and limited dissemination to need-to-know stakeholders such as public health planners. None Conclusion None Experts identified ethical issues specific to the African context and to research on sensitive, publicly available data and strategies for addressing these issues. These findings can be used to inform an ethical implementation framework with research stage-specific recommendations on how to use publicly available data for machine learning-based predictive analytics to predict HIV/AIDS risk in SSA.",project-academic
,2021-01-23,a,,a raspberry pi based traumatic brain injury detection system for single channel electroencephalogram," Traumatic Brain Injury (TBI) is a common cause of death and disability. However, existing tools for TBI diagnosis are either subjective or require extensive clinical setup and expertise. The increasing affordability and reduction in size of relatively high-performance computing systems combined with promising results from TBI related machine learning research make it possible to create compact and portable systems for early detection of TBI. This work describes a Raspberry Pi based portable, real-time data acquisition, and automated processing system that uses machine learning to efficiently identify TBI and automatically score sleep stages from a single-channel Electroen-cephalogram (EEG) signal. We discuss the design, implementation, and verification of the system that can digitize EEG signal using an Analog to Digital Converter (ADC) and perform real-time signal classification to detect the presence of mild TBI (mTBI). We utilize Convolutional Neural Networks (CNN) and XGBoost based predictive models to evaluate the performance and demonstrate the versatility of the system to operate with multiple types of predictive models. We achieve a peak classification accuracy of more than 90% with a classification time of less than 1 s across 16 s - 64 s epochs for TBI vs control conditions. This work can enable development of systems suitable for field use without requiring specialized medical equipment for early TBI detection applications and TBI research. Further, this work opens avenues to implement connected, real-time TBI related health and wellness monitoring systems.",project-academic
10.3390/S21082779,2021-04-15,a,Multidisciplinary Digital Publishing Institute,a raspberry pi based traumatic brain injury detection system for single channel electroencephalogram," Traumatic Brain Injury (TBI) is a common cause of death and disability. However, existing tools for TBI diagnosis are either subjective or require extensive clinical setup and expertise. The increasing affordability and reduction in the size of relatively high-performance computing systems combined with promising results from TBI related machine learning research make it possible to create compact and portable systems for early detection of TBI. This work describes a Raspberry Pi based portable, real-time data acquisition, and automated processing system that uses machine learning to efficiently identify TBI and automatically score sleep stages from a single-channel Electroencephalogram (EEG) signal. We discuss the design, implementation, and verification of the system that can digitize the EEG signal using an Analog to Digital Converter (ADC) and perform real-time signal classification to detect the presence of mild TBI (mTBI). We utilize Convolutional Neural Networks (CNN) and XGBoost based predictive models to evaluate the performance and demonstrate the versatility of the system to operate with multiple types of predictive models. We achieve a peak classification accuracy of more than 90% with a classification time of less than 1 s across 16-64 s epochs for TBI vs. control conditions. This work can enable the development of systems suitable for field use without requiring specialized medical equipment for early TBI detection applications and TBI research. Further, this work opens avenues to implement connected, real-time TBI related health and wellness monitoring systems.",project-academic
10.1038/S41746-020-00318-Y,2020-08-21,a,Nature Publishing Group,developing a delivery science for artificial intelligence in healthcare," Artificial Intelligence (AI) has generated a large amount of excitement in healthcare, mostly driven by the emergence of increasingly accurate machine learning models. However, the promise of AI delivering scalable and sustained value for patient care in the real world setting has yet to be realized. In order to safely and effectively bring AI into use in healthcare, there needs to be a concerted effort around not just the creation, but also the delivery of AI. This AI ""delivery science"" will require a broader set of tools, such as design thinking, process improvement, and implementation science, as well as a broader definition of what AI will look like in practice, which includes not just machine learning models and their predictions, but also the new systems for care delivery that they enable. The careful design, implementation, and evaluation of these AI enabled systems will be important in the effort to understand how AI can improve healthcare.",project-academic
10.1016/J.INFFUS.2021.07.016,2022-01-01,a,Elsevier,unbox the black box for the medical explainable ai via multi modal and multi centre data fusion a mini review two showcases and beyond," Abstract None None Explainable Artificial Intelligence (XAI) is an emerging research topic of machine learning aimed at unboxing how AI systems’ black-box choices are made. This research field inspects the measures and models involved in decision-making and seeks solutions to explain them explicitly. Many of the machine learning algorithms cannot manifest how and why a decision has been cast. This is particularly true of the most popular deep neural network approaches currently in use. Consequently, our confidence in AI systems can be hindered by the lack of explainability in these black-box models. The XAI becomes more and more crucial for deep learning powered applications, especially for medical and healthcare studies, although in general these deep neural networks can return an arresting dividend in performance. The insufficient explainability and transparency in most existing AI systems can be one of the major reasons that successful implementation and integration of AI tools into routine clinical practice are uncommon. In this study, we first surveyed the current progress of XAI and in particular its advances in healthcare applications. We then introduced our solutions for XAI leveraging multi-modal and multi-centre data fusion, and subsequently validated in two showcases following real clinical scenarios. Comprehensive quantitative and qualitative analyses can prove the efficacy of our proposed XAI solutions, from which we can envisage successful applications in a broader range of clinical questions.",project-academic
,2021-02-03,a,,unbox the black box for the medical explainable ai via multi modal and multi centre data fusion a mini review two showcases and beyond," Explainable Artificial Intelligence (XAI) is an emerging research topic of machine learning aimed at unboxing how AI systems' black-box choices are made. This research field inspects the measures and models involved in decision-making and seeks solutions to explain them explicitly. Many of the machine learning algorithms can not manifest how and why a decision has been cast. This is particularly true of the most popular deep neural network approaches currently in use. Consequently, our confidence in AI systems can be hindered by the lack of explainability in these black-box models. The XAI becomes more and more crucial for deep learning powered applications, especially for medical and healthcare studies, although in general these deep neural networks can return an arresting dividend in performance. The insufficient explainability and transparency in most existing AI systems can be one of the major reasons that successful implementation and integration of AI tools into routine clinical practice are uncommon. In this study, we first surveyed the current progress of XAI and in particular its advances in healthcare applications. We then introduced our solutions for XAI leveraging multi-modal and multi-centre data fusion, and subsequently validated in two showcases following real clinical scenarios. Comprehensive quantitative and qualitative analyses can prove the efficacy of our proposed XAI solutions, from which we can envisage successful applications in a broader range of clinical questions.",project-academic
10.1109/ICODSE.2016.7936110,2016-10-01,p,,focused crawler for the acquisition of health articles," The health intervention by using technology can be the alternative to the doctor, especially for common health problem. To support the technology, we need health knowledge base as the foundation. The artificial intelligence and hardware development nowadays support this requirement. The big picture of our research is building the application that can utilize the health knowledge base to provide health intervention. As the first step, we collect the articles related to health. To realize it, we build the focused crawler that implements multithreaded programming, Larger-Sites-First algorithm and also Naive Bayes classifier. We find that the articles acquisition is going to saturate along with the increment of threads. Furthermore, the implementation of Larger-Sites-First algorithm do increase the number of crawled articles, but it is not significant. In addition, Naive Bayes recognizes ≥ 90 percent articles in perfect condition for both health and non-health category. However, the performance goes down when recognizing the non-health articles which contain health keywords.",project-academic
10.1109/ACCESS.2020.2970178,2020-01-29,a,IEEE,a novel software engineering approach toward using machine learning for improving the efficiency of health systems," Recently, machine learning has become a hot research topic. Therefore, this study investigates the interaction between software engineering and machine learning within the context of health systems. We proposed a novel framework for health informatics: the framework and methodology of software engineering for machine learning in health informatics (SEMLHI). The SEMLHI framework includes four modules (software, machine learning, machine learning algorithms, and health informatics data) that organize the tasks in the framework using a SEMLHI methodology, thereby enabling researchers and developers to analyze health informatics software from an engineering perspective and providing developers with a new road map for designing health applications with system functions and software implementations. Our novel approach sheds light on its features and allows users to study and analyze the user requirements and determine both the function of objects related to the system and the machine learning algorithms that must be applied to the dataset. Our dataset used in this research consists of real data and was originally collected from a hospital run by the Palestine government covering the last three years. The SEMLHI methodology includes seven phases: designing, implementing, maintaining and defining workflows; structuring information; ensuring security and privacy; performance testing and evaluation; and releasing the software applications.",project-academic
,2018-01-01,a,,privacy preserving logistic regression training," Logistic regression is a popular technique used in machine learning to construct classification models. Since the construction of such models is based on computing with large datasets, it is an appealing idea to outsource this computation to a cloud service. The privacy-sensitive nature of the input data requires appropriate privacy preserving measures before outsourcing it. Homomorphic encryption enables one to compute on encrypted data directly, without decryption and can be used to mitigate the privacy concerns raised by using a cloud service. In this paper, we propose an algorithm (and its implementation) to train a logistic regression model on a homomorphically encrypted dataset. The core of our algorithm consists of a new iterative method that can be seen as a simplified form of the fixed Hessian method, but with a much lower multiplicative complexity. We test the new method on two interesting real life applications: the first application is in medicine and constructs a model to predict the probability for a patient to have cancer, given genomic data as input; the second application is in finance and the model predicts the probability of a credit card transaction to be fraudulent. The method produces accurate results for both applications, comparable to running standard algorithms on plaintext data. This article introduces a new simple iterative algorithm to train a logistic regression model that is tailored to be applied on a homomorphically encrypted dataset. This algorithm can be used as a privacy-preserving technique to build a binary classification model and can be applied in a wide range of problems that can be modelled with logistic regression. Our implementation results show that our method can handle the large datasets used in logistic regression training.",project-academic
10.1186/S12920-018-0398-Y,2018-10-11,a,BioMed Central,privacy preserving logistic regression training," Logistic regression is a popular technique used in machine learning to construct classification models. Since the construction of such models is based on computing with large datasets, it is an appealing idea to outsource this computation to a cloud service. The privacy-sensitive nature of the input data requires appropriate privacy preserving measures before outsourcing it. Homomorphic encryption enables one to compute on encrypted data directly, without decryption and can be used to mitigate the privacy concerns raised by using a cloud service. In this paper, we propose an algorithm (and its implementation) to train a logistic regression model on a homomorphically encrypted dataset. The core of our algorithm consists of a new iterative method that can be seen as a simplified form of the fixed Hessian method, but with a much lower multiplicative complexity. We test the new method on two interesting real life applications: the first application is in medicine and constructs a model to predict the probability for a patient to have cancer, given genomic data as input; the second application is in finance and the model predicts the probability of a credit card transaction to be fraudulent. The method produces accurate results for both applications, comparable to running standard algorithms on plaintext data. This article introduces a new simple iterative algorithm to train a logistic regression model that is tailored to be applied on a homomorphically encrypted dataset. This algorithm can be used as a privacy-preserving technique to build a binary classification model and can be applied in a wide range of problems that can be modelled with logistic regression. Our implementation results show that our method can handle the large datasets used in logistic regression training.",project-academic
10.1007/S10489-015-0725-3,2016-04-01,a,Springer US,hierarchical feature selection based on relative dependency for gear fault diagnosis," Feature selection is an important aspect under study in machine learning based diagnosis, that aims to remove irrelevant features for reaching good performance in the diagnostic systems. The behaviour of diagnostic models could be sensitive with regard to the amount of features, and significant features can represent the problem better than the entire set. Consequently, algorithms to identify these features are valuable contributions. This work deals with the feature selection problem through attribute clustering. The proposed algorithm is inspired by existing approaches, where the relative dependency between attributes is used to calculate dissimilarity values. The centroids of the created clusters are selected as representative attributes. The selection algorithm uses a random process for proposing centroid candidates, in this way, the inherent exploration in random search is included. A hierarchical procedure is proposed for implementing this algorithm. In each level of the hierarchy, the entire set of available attributes is split in disjoint sets and the selection process is applied on each subset. Once the significant attributes are proposed for each subset, a new set of available attributes is created and the selection process runs again in the next level. The hierarchical implementation aims to refine the search space in each level on a reduced set of selected attributes, while the computational time-consumption is improved also. The approach is tested with real data collected from a test bed, results show that the diagnosis precision by using a Random Forest based classifier is over 98 % with only 12 % of the attributes from the available set.",project-academic
10.1002/MP.14140,2020-05-01,a,"John Wiley & Sons, Ltd",introduction to machine and deep learning for medical physicists," Recent years have witnessed tremendous growth in the application of machine learning (ML) and deep learning (DL) techniques in medical physics. Embracing the current big data era, medical physicists equipped with these state-of-the-art tools should be able to solve pressing problems in modern radiation oncology. Here, a review of the basic aspects involved in ML/DL model building, including data processing, model training, and validation for medical physics applications is presented and discussed. Machine learning can be categorized based on the underlying task into supervised learning, unsupervised learning, or reinforcement learning; each of these categories has its own input/output dataset characteristics and aims to solve different classes of problems in medical physics ranging from automation of processes to predictive analytics. It is recognized that data size requirements may vary depending on the specific medical physics application and the nature of the algorithms applied. Data processing, which is a crucial step for model stability and precision, should be performed before training the model. Deep learning as a subset of ML is able to learn multilevel representations from raw input data, eliminating the necessity for hand crafted features in classical ML. It can be thought of as an extension of the classical linear models but with multilayer (deep) structures and nonlinear activation functions. The logic of going ""deeper"" is related to learning complex data structures and its realization has been aided by recent advancements in parallel computing architectures and the development of more robust optimization methods for efficient training of these algorithms. Model validation is an essential part of ML/DL model building. Without it, the model being developed cannot be easily trusted to generalize to unseen data. Whenever applying ML/DL, one should keep in mind, according to Amara's law, that humans may tend to overestimate the ability of a technology in the short term and underestimate its capability in the long term. To establish ML/DL role into standard clinical workflow, models considering balance between accuracy and interpretability should be developed. Machine learning/DL algorithms have potential in numerous radiation oncology applications, including automatizing mundane procedures, improving efficiency and safety of auto-contouring, treatment planning, quality assurance, motion management, and outcome predictions. Medical physicists have been at the frontiers of technology translation into medicine and they ought to be prepared to embrace the inevitable role of ML/DL in the practice of radiation oncology and lead its clinical implementation.",project-academic
10.1109/IEEM.2018.8607399,2018-12-01,p,IEEE,a chatbot supported smart wireless interactive healthcare system for weight control and health promotion," People who are overweight and obese have a greater risk of developing serious diseases and health conditions. A steadily increasing trend of obesity is not only limited to developed countries, but to developing nations as well. As smartphones have rapidly gained mainstream popularity, mobile applications (apps) are used in public health as intervention to keep track of diets, activity as well as weight, which is deemed more accurate than relying on user’s self-report measure, for the sake of weight management. A solution called “Smart Wireless Interactive Healthcare System” (SWITCHes) is developed to facilitate objective data reception and transmission in a real-time manner. Based on the user data acquired from SWITCHes app and the auxiliary data from medical instruments, not only SWITCHes app can engage user with tailored feedback in an interactive way, in terms of artificial intelligence-powered health chatbot, but the healthcare professional can provide the more accurate medical advice to user also. This paper presents an overview of development and implementation of SWITCHes.",project-academic
10.1016/J.FUTURE.2016.03.019,2017-01-01,a,North-Holland,an intelligent cloud based data processing broker for mobile e health multimedia applications," Abstract None None Mobile e-health applications provide users and healthcare practitioners with an insightful way to check users/patients’ status and monitor their daily calorie intake. Mobile e-health applications provide users and healthcare practitioners with an insightful way to check users/patients’ status and monitor their daily activities. This paper proposes a cloud-based mobile e-health calorie system that can classify food objects in the plate and further compute the overall calorie of each food object with high accuracy. The novelty in our system is that we are not only offloading heavy computational functions of the system to the cloud, but also employing an intelligent cloud-broker mechanism to strategically and efficiently utilize cloud instances to provide accurate and improved time response results. The broker system uses a dynamic cloud allocation mechanism that takes decisions on allocating and de-allocating cloud instances in real-time for ensuring the average response time stays within a predefined threshold. In this paper, we further demonstrate various scenarios to explain the workflow of the cloud components including: segmentation, deep learning, indexing food images, decision making algorithms, calorie computation, scheduling management as part of the proposed cloud broker model. The implementation results of our system showed that the proposed cloud broker results in a 45% gain in the overall time taken to process the images in the cloud. With the use of dynamic cloud allocation mechanism, we were able to reduce the average time consumption by 77.21% when 60 images were processed in parallel.",project-academic
,2021-05-25,a,,optical coherent dot product chip for sophisticated deep learning regression," Optical implementations of neural networks (ONNs) herald the next-generation high-speed and energy-efficient deep learning computing by harnessing the technical advantages of large bandwidth and high parallelism of optics. However, because of limited hardware scale and incomplete numerical domain, the majority of existing ONNs are merely studied and benchmarked with basic classification tasks. Given that regression is a fundamental form of deep learning and accounts for a large part of current artificial intelligence applications, we demonstrate a silicon-based optical coherent dot-product chip (OCDC) capable of completing deep learning regression tasks. The OCDC adopts optical fields rather than intensities to represent values in the complete real-value domain. It conducts matrix multiplications and convolutions in neural networks of any complexity via reconfiguration and reusing. Hardware deviations are compensated via in-situ backpropagation control owing to the simplicity of chip architecture, thus enhancing the numerical accuracy of analog computing. Therefore, the OCDC meets the fundamental requirement for regression tasks and we successfully demonstrate a representative neural network, the AUTOMAP (a cutting-edge neural network model for image reconstruction). The quality of reconstructed images by the OCDC and a 32-bit digital computer is comparable. To best of our knowledge, there is no precedent of performing such state-of-the-art regression tasks on ONN chip. It is anticipated that the OCDC can promote novel accomplishment of ONNs in modern AI applications including autonomous driving, natural language processing, medical diagnosis, and scientific study. Moreover, the OCDC and auxiliary electronics have the potential to be monolithically fabricated with CMOS-compatible silicon photonic integration technologies.",project-academic
10.1002/LARY.27850,2020-01-01,a,"John Wiley & Sons, Ltd",a contemporary review of machine learning in otolaryngology head and neck surgery," One of the key challenges with big data is leveraging the complex network of information to yield useful clinical insights. The confluence of massive amounts of health data and a desire to make inferences and insights on these data has produced a substantial amount of interest in machine-learning analytic methods. There has been a drastic increase in the otolaryngology literature volume describing novel applications of machine learning within the past 5 years. In this timely contemporary review, we provide an overview of popular machine-learning techniques, and review recent machine-learning applications in otolaryngology-head and neck surgery including neurotology, head and neck oncology, laryngology, and rhinology. Investigators have realized significant success in validated models with model sensitivities and specificities approaching 100%. Challenges remain in the implementation of machine-learning algorithms. This may be in part the unfamiliarity of these techniques to clinician leaders on the front lines of patient care. Spreading awareness and confidence in machine learning will follow with further validation and proof-of-value analyses that demonstrate model performance superiority over established methods. We are poised to see a greater influx of machine-learning applications to clinical problems in otolaryngology-head and neck surgery, and it is prudent for providers to understand the potential benefits and limitations of these technologies. Laryngoscope, 130:45-51, 2020.",project-academic
10.1161/JAHA.119.012788,2019-09-03,a,J Am Heart Assoc,artificial intelligence practical primer for clinical research in cardiovascular disease," Artificial intelligence (AI) has begun to permeate and reform the field of medicine and cardiovascular medicine. Impacting about 100 million patients in the United States, the burden of cardiovascular disease is felt in a diverse array of demographics.1, 2 Meanwhile, routine mediums such as multimodality images, electronic health records (EHR), and mobile health devices store troves of underutilized data for each patient. AI has the potential to improve and influence the status quo, with capacity to learn from these massive data and apply knowledge from them to distinct circumstances.3, 4 With considerable information in each heart beat, cardiovascular medicine will definitely be one of the fields that embrace AI to move toward personalized and precise care.5

AI has already been woven into the fabric of everyday life. From an internet search engine, email spam and malware filtering, to uncovering fraudulent credit card purchases, AI addresses an individual's needs in the realms of business, entertainment, and technology. Unfortunately, medicine, including cardiology, has not fully embraced this revolution, with only a limited number of AI‐based clinical applications being available. Nevertheless, there is promise towards routine implementation; machine learning and deep learning have seen an exponential surge of cardiovascular publications in the past decade.6, 7 These methods have proven beneficial in a variety of complex areas including echocardiogram interpretation and diastolic dysfunction grade stratification.8, 9 The US Food and Drug Administration has already approved several devices that utilize AI features.10 Imagine coming to work finding that your system has analyzed all your patients while you were sleeping: their laboratory data, imaging results, symptoms, and mobile device data to calculate their risk of cardiovascular events, death, hospitalization, whether medications should be adjusted/added/removed, or whether they should be referred for an examination. The system presents you with the reasons for its recommendations, and you are confident that they are as good as those given by the most experienced physicians. This may allow you to spend time in shared decision making with your patients, both objectively and compassionately. Although there are currently several barriers/challenges to adoption of AI in clinical practice, undoubtedly, AI will drive current healthcare practice towards a more individualized and precision‐based approach over the next several years. Therefore, general understanding of AI techniques by clinicians and researchers in cardiovascular medicine is paramount. In this review article, we describe the fundamentals of AI that clinicians and researchers should understand, its definition and principles, how to interpret and apply AI in cardiovascular research, limitations, and future perspectives.",project-academic
10.1136/BMJ.B220,2009-01-21,a,British Medical Journal Publishing Group,surgical safety checklists," Improve collaborative teamwork, minimise surprises, and reduce harm to patients

Surgical deaths and complications are a global public health problem. The World Health Organization estimates that each year half a million deaths related to surgery could be prevented.1 2 In England and Wales, the National Patient Safety Agency’s national reporting and learning system recorded 129 419 surgery related events in 2007.3 In the United States, the state of Minnesota (with less than 2% of the US population) reported 21 surgeries in the wrong site during one year (October 2007 to October 2008) . 4 The real situation is probably even worse though, because most safety incidents are not reported.5

In June 2008, WHO launched the Safe Surgery Saves Lives campaign.2 This included a “surgical safety checklist” (www.who.int/patientsafety/safesurgery/en/) to ensure that the entire operating theatre team has a common understanding of the patient and the surgical procedure, and that evidence based interventions such as antibiotic prophylaxis or deep vein thrombosis prophylaxis are reliably given.2 The 19 item checklist is completed in three stages—before induction of anaesthesia (sign in), just before skin incision (time out), and before the patient leaves the operating theatre (sign out). Items on the checklist must be verbally confirmed with the patient and other team members. The WHO Safe Surgery Saves Lives Study Group has published a study of 3733 patients before implementation and …",project-academic
10.1038/S41598-018-23534-9,2018-03-26,a,Sci Rep,extracting biological age from biomedical data via deep learning too much of a good thing," Age-related physiological changes in humans are linearly associated with age. Naturally, linear combinations of physiological measures trained to estimate chronological age have recently emerged as a practical way to quantify aging in the form of biological age. In this work, we used one-week long physical activity records from a 2003-2006 National Health and Nutrition Examination Survey (NHANES) to compare three increasingly accurate biological age models: the unsupervised Principal Components Analysis (PCA) score, a multivariate linear regression, and a state-of-the-art deep convolutional neural network (CNN). We found that the supervised approaches produce better chronological age estimations at the expense of a loss of the association between the aging acceleration and all-cause mortality. Consequently, we turned to the NHANES death register directly and introduced a novel way to train parametric proportional hazards models suitable for out-of-the-box implementation with any modern machine learning software. As a demonstration, we produced a separate deep CNN for mortality risks prediction that outperformed any of the biological age or a simple linear proportional hazards model. Altogether, our findings demonstrate the emerging potential of combined wearable sensors and deep learning technologies for applications involving continuous health risk monitoring and real-time feedback to patients and care providers.",project-academic
10.1101/219162,2017-11-16,a,Cold Spring Harbor Laboratory,extracting biological age from biomedical data via deep learning too much of a good thing," Aging-related physiological changes are systemic and, at least in humans, are linearly associated with age. Therefore, linear combinations of physiological measures trained to estimate chronological age have recently emerged as a practical way to quantify aging in the form of biological age. Aging acceleration, defined as the difference between the predicted and chronological age was found to be elevated in patients with major diseases and is predictive of mortality. In this work, we compare three increasingly accurate biological age models: metrics derived from unsupervised Principal Components Analysis (PCA), alongside two supervised biological age models; a multivariate linear regression and a state-of-the-art deep convolution neural network (CNN). All predictions were made using one-week long locomotor activity records from a 2003-2006 National Health and Nutrition Examination Survey (NHANES) dataset. We found that application of the supervised approaches improves the accuracy of the chronological age estimation at the expense of a loss of the association between the None aging acceleration predicted by the model and all-cause mortality. Instead, we turned to the NHANES death register and introduced a novel way to train parametric proportional hazards models in a form suitable for out-of-the-box implementation with any modern machine learning software. Finally, we characterized a proof-of-concept example, a separate deep CNN trained to predict mortality risks that outperformed any of the biological age or simple linear proportional hazards models. Our findings demonstrate the emerging potential of combined wearable sensors and deep learning technologies for applications involving continuous health risk monitoring and real-time feedback to patients and care providers.",project-academic
10.1093/BIOINFORMATICS/BTAB311,2021-07-12,a,Oxford Academic,on the feasibility of deep learning applications using raw mass spectrometry data," Summary None In recent years, SWATH-MS has become the proteomic method of choice for data-independent-acquisition, as it enables high proteome coverage, accuracy and reproducibility. However, data analysis is convoluted and requires prior information and expert curation. Furthermore, as quantification is limited to a small set of peptides, potentially important biological information may be discarded. Here we demonstrate that deep learning can be used to learn discriminative features directly from raw MS data, eliminating hence the need of elaborate data processing pipelines. Using transfer learning to overcome sample sparsity, we exploit a collection of publicly available deep learning models already trained for the task of natural image classification. These models are used to produce feature vectors from each mass spectrometry (MS) raw image, which are later used as input for a classifier trained to distinguish tumor from normal prostate biopsies. Although the deep learning models were originally trained for a completely different classification task and no additional fine-tuning is performed on them, we achieve a highly remarkable classification performance of 0.876 AUC. We investigate different types of image preprocessing and encoding. We also investigate whether the inclusion of the secondary MS2 spectra improves the classification performance. Throughout all tested models, we use standard protein expression vectors as gold standards. Even with our naive implementation, our results suggest that the application of deep learning and transfer learning techniques might pave the way to the broader usage of raw mass spectrometry data in real-time diagnosis. None Availability and implementation None The open source code used to generate the results from MS images is available on GitHub: https://ibm.biz/mstransc. The raw MS data underlying this article cannot be shared publicly for the privacy of individuals that participated in the study. Processed data including the MS images, their encodings, classification labels and results can be accessed at the following link: https://ibm.box.com/v/mstc-supplementary. None Supplementary information None Supplementary data are available at Bioinformatics online.",project-academic
10.1016/J.IRBM.2021.06.010,2021-07-12,a,Elsevier Masson,backpropagation neural network for processing of missing data in breast cancer detection," Abstract None None Background None A complete dataset is essential for biomedical implementation. Due to the limitation of objective or subjective factors, missing data often occurs, which exerts uncertainty in the subsequent data processing. Commonly used methods of interpolation are interpolating substitute values that keep minimum error. Some applications of statistics are usually used for handling this problem. None None None Methods None We are trying to find a higher performance interpolation method compared with the usual statistic methods, by using artificial intelligence which is in full swing today. The prediction and classification of backpropagation neural network are used in this paper, describes a missing data interpolation method to propose the interpolation model that mines association rules in the data. In the experiment, depending on a multi-layer network structure, the model is trained and tested by sample data, constantly revises network weights and thresholds. The error function decreases along the negative gradient direction and approaches the expected real output. The model is validated on the breast cancer dataset, and we select real samples from the data set for validation, moreover, add four traditional methods as a control group. None None None Results None The proposed method has great performance improvement in the interpolation of missing data. Experimental results show that the interpolation accuracy of our proposed method (84%) is higher than four traditional methods (1.33%, 74.67%, 73.33%, 77.33%) as mentioned in this paper, BPNN stays low in MSE evaluation. Finally, we analyze the performance of various methods in processing missing data. None None None Conclusions None The study in this paper has estimated missing data with high accuracy as much as possible to reduce the negative impact in the diagnosis of real life. At the same time, it can also assist in missing data processing in the biomedical field.",project-academic
10.1101/2020.07.04.20146456,2020-07-06,a,Cold Spring Harbor Laboratory Press,wearable respiration monitoring interpretable inference with context and sensor biomarkers," Abstract None Breathing rate (BR), minute ventilation (VE), and other respiratory parameters are essential for real-time patient monitoring in many acute health conditions, such as asthma. The clinical standard for measuring respiration, namely Spirometry, is hardly suitable for continuous use. Wearables can track many physiological signals, like ECG and motion, yet not respiration. Deriving respiration from other modalities has become an area of active research. In this work, we infer respiratory parameters from wearable ECG and wrist motion signals. We propose a modular and generalizable classification-regression pipeline to utilize available context information, such as physical activity, in learning context-conditioned inference models. Morphological and power domain novel features from the wearable ECG are extracted to use with these models. Exploratory feature selection methods are incorporated in this pipeline to discover application-specific interpretable biomarkers. Using data from 15 subjects, we evaluate two implementations of the proposed pipeline: for inferring BR and VE. Each implementation compares generalized linear model, random forest, support vector machine, Gaussian process regression, and neighborhood component analysis as contextual regression models. Permutation, regularization, and relevance determination methods are used to rank the ECG features to identify robust ECG biomarkers across models and activities. This work demonstrates the potential of wearable sensors not only in continuous monitoring, but also in designing biomarker-driven preventive measures.",project-academic
10.1109/JBHI.2020.3035776,2020-07-02,a,,wearable respiration monitoring interpretable inference with context and sensor biomarkers," Breathing rate (BR), minute ventilation (VE), and other respiratory parameters are essential for real-time patient monitoring in many acute health conditions, such as asthma. The clinical standard for measuring respiration, namely Spirometry, is hardly suitable for continuous use. Wearables can track many physiological signals, like ECG and motion, yet not respiration. Deriving respiration from other modalities has become an area of active research. In this work, we infer respiratory parameters from wearable ECG and wrist motion signals. We propose a modular and generalizable classification-regression pipeline to utilize available context information, such as physical activity, in learning context-conditioned inference models. Morphological and power domain novel features from the wearable ECG are extracted to use with these models. Exploratory feature selection methods are incorporated in this pipeline to discover application-specific interpretable biomarkers. Using data from 15 subjects, we evaluate two implementations of the proposed pipeline: for inferring BR and VE. Each implementation compares generalized linear model, random forest, support vector machine, Gaussian process regression, and neighborhood component analysis as contextual regression models. Permutation, regularization, and relevance determination methods are used to rank the ECG features to identify robust ECG biomarkers across models and activities. This work demonstrates the potential of wearable sensors not only in continuous monitoring, but also in designing biomarker-driven preventive measures.",project-academic
10.1038/NMETH.3901,2016-09-01,a,Nature Research,the perseus computational platform for comprehensive analysis of prote omics data," A main bottleneck in proteomics is the downstream biological analysis of highly multivariate quantitative protein abundance data generated using mass-spectrometry-based analysis. We developed the Perseus software platform (http://www.perseus-framework.org) to support biological and biomedical researchers in interpreting protein quantification, interaction and post-translational modification data. Perseus contains a comprehensive portfolio of statistical tools for high-dimensional omics data analysis covering normalization, pattern recognition, time-series analysis, cross-omics comparisons and multiple-hypothesis testing. A machine learning module supports the classification and validation of patient groups for diagnosis and prognosis, and it also detects predictive protein signatures. Central to Perseus is a user-friendly, interactive workflow environment that provides complete documentation of computational methods used in a publication. All activities in Perseus are realized as plugins, and users can extend the software by programming their own, which can be shared through a plugin store. We anticipate that Perseus's arsenal of algorithms and its intuitive usability will empower interdisciplinary analysis of complex large data sets.",project-academic
,2017-05-31,,,automatic detection system for pulmonary nodule in chest ct computed tomography image," The invention relates to an automatic detection system for a pulmonary nodule in a chest CT (Computed Tomography) image. The system provides improvements for the problems of large calculated amount of computer aided software, inaccurate prediction and few prediction varieties. The improvement of the invention comprises the steps of acquiring a CT image; segmenting a pulmonary tissue; detecting a suspected nodular lesion area in the pulmonary tissue; classifying nidi based on a nidus classification model of deep learning; and outputting an image mark and a diagnosis report. The system has high nodule detection rate and relatively low false positive rate, acquires an accurate locating, quantitative and qualitative result of a nodular lesion and a prediction probability thereof. The end-to-end (from a CT machine end to a doctor end) nodular lesion screening is truly realized, the accuracy and operability demands of doctors are satisfied and the system has wide mar5ket application prospects.",project-academic
10.1093/BIOINFORMATICS/BTI033,2005-03-01,a,Oxford University Press,a comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis," Motivation: Cancer diagnosis is one of the most important emerging clinical applications of gene expression microarray technology. We are seeking to develop a computer system for powerful and reliable cancer diagnostic model creation based on microarray data. To keep a realistic perspective on clinical applications we focus on multicategory diagnosis. To equip the system with the optimum combination of classifier, gene selection and cross-validation methods, we performed a systematic and comprehensive evaluation of several major algorithms for multicategory classification, several gene selection methods, multiple ensemble classifier methods and two cross-validation designs using 11 datasets spanning 74 diagnostic categories and 41 cancer types and 12 normal tissue types.

Results: Multicategory support vector machines (MC-SVMs) are the most effective classifiers in performing accurate cancer diagnosis from gene expression data. The MC-SVM techniques by Crammer and Singer, Weston and Watkins and one-versus-rest were found to be the best methods in this domain. MC-SVMs outperform other popular machine learning algorithms, such as k-nearest neighbors, backpropagation and probabilistic neural networks, often to a remarkable degree. Gene selection techniques can significantly improve the classification performance of both MC-SVMs and other non-SVM learning algorithms. Ensemble classifiers do not generally improve performance of the best non-ensemble models. These results guided the construction of a software system GEMS (Gene Expression Model Selector) that automates high-quality model construction and enforces sound optimization and performance estimation procedures. This is the first such system to be informed by a rigorous comparative analysis of the available algorithms and datasets.

Availability: The software system GEMS is available for download from http://www.gems-system.org for non-commercial use.

Contact: alexander.statnikov@vanderbilt.edu",project-academic
10.1109/TSMCC.2009.2033566,2010-03-01,a,IEEE,a survey on the application of genetic programming to classification," Classification is one of the most researched questions in machine learning and data mining. A wide range of real problems have been stated as classification problems, for example credit scoring, bankruptcy prediction, medical diagnosis, pattern recognition, text categorization, software quality assessment, and many more. The use of evolutionary algorithms for training classifiers has been studied in the past few decades. Genetic programming (GP) is a flexible and powerful evolutionary technique with some features that can be very valuable and suitable for the evolution of classifiers. This paper surveys existing literature about the application of genetic programming to classification, to show the different ways in which this evolutionary algorithm can help in the construction of accurate and reliable classifiers.",project-academic
,2012-04-25,p,USENIX Association,structured comparative analysis of systems logs to diagnose performance problems," Diagnosis and correction of performance issues in modern, large-scale distributed systems can be a daunting task, since a single developer is unlikely to be familiar with the entire system and it is hard to characterize the behavior of a software system without completely understanding its internal components. This paper describes DISTALYZER, an automated tool to support developer investigation of performance issues in distributed systems. We aim to leverage the vast log data available from large scale systems, while reducing the level of knowledge required for a developer to use our tool. Specifically, given two sets of logs, one with good and one with bad performance, DISTALYZER uses machine learning techniques to compare system behaviors extracted from the logs and automatically infer the strongest associations between system components and performance. The tool outputs a set of inter-related event occurrences and variable values that exhibit the largest divergence across the logs sets and most directly affect the overall performance of the system. These patterns are presented to the developer for inspection, to help them understand which system component(s) likely contain the root cause of the observed performance issue, thus alleviating the need for many human hours of manual inspection. We demonstrate the generality and effectiveness of DISTALYZER on three real distributed systems by showing how it discovers and highlights the root cause of six performance issues across the systems. DISTALYZER has broad applicability to other systems since it is dependent only on the logs for input, and not on the source code.",project-academic
10.1109/ACCESS.2018.2872430,2018-09-27,a,Institute of Electrical and Electronics Engineers (IEEE),datanet deep learning based encrypted network traffic classification in sdn home gateway," A smart home network will support various smart devices and applications, e.g., home automation devices, E-health devices, regular computing devices, and so on. Most devices in a smart home access the Internet through a home gateway (HGW). In this paper, we propose a software-defined-network (SDN)-HGW framework to better manage distributed smart home networks and support the SDN controller of the core network. The SDN controller enables efficient network quality-of-service management based on real-time traffic monitoring and resource allocation of the core network. However, it cannot provide network management in distributed smart homes. Our proposed SDN-HGW extends the control to the access network, i.e., a smart home network, for better end-to-end network management. Specifically, the proposed SDN-HGW can achieve distributed application awareness by classifying data traffic in a smart home network. Most existing traffic classification solutions, e.g., deep packet inspection, cannot provide real-time application awareness for encrypted data traffic. To tackle those issues, we develop encrypted data classifiers (denoted as DataNets) based on three deep learning schemes, i.e., multilayer perceptron, stacked autoencoder, and convolutional neural networks, using an open data set that has over 200 000 encrypted data samples from 15 applications. A data preprocessing scheme is proposed to process raw data packets and the tested data set so that DataNet can be created. The experimental results show that the developed DataNets can be applied to enable distributed application-aware SDN-HGW in future smart home networks.",project-academic
10.1007/S10664-013-9258-8,2014-12-01,a,Springer US,bug characteristics in open source software," To design effective tools for detecting and recovering from software failures requires a deep understanding of software bug characteristics. We study software bug characteristics by sampling 2,060 real world bugs in three large, representative open-source projects--the Linux kernel, Mozilla, and Apache. We manually study these bugs in three dimensions--root causes, impacts, and components. We further study the correlation between categories in different dimensions, and the trend of different types of bugs. The findings include: (1) semantic bugs are the dominant root cause. As software evolves, semantic bugs increase, while memory-related bugs decrease, calling for more research effort to address semantic bugs; (2) the Linux kernel operating system (OS) has more concurrency bugs than its non-OS counterparts, suggesting more effort into detecting concurrency bugs in operating system code; and (3) reported security bugs are increasing, and the majority of them are caused by semantic bugs, suggesting more support to help developers diagnose and fix security bugs, especially semantic security bugs. In addition, to reduce the manual effort in building bug benchmarks for evaluating bug detection and diagnosis tools, we use machine learning techniques to classify 109,014 bugs automatically.",project-academic
,2017-11-10,a,,neural symbolic learning and reasoning a survey and interpretation," The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.",project-academic
10.1016/J.ARTMED.2007.11.006,2008-02-01,a,Elsevier Science Publishers Ltd.,medic medical embedded device for individualized care," Objective: Presented work highlights the development and initial validation of a medical embedded device for individualized care (MEDIC), which is based on a novel software architecture, enabling sensor management and disease prediction capabilities, and commercially available microelectronic components, sensors and conventional personal digital assistant (PDA) (or a cell phone). Methods and materials: In this paper, we present a general architecture for a wearable sensor system that can be customized to an individual patient's needs. This architecture is based on embedded artificial intelligence that permits autonomous operation, sensor management and inference, and may be applied to a general purpose wearable medical diagnostics. Results: A prototype of the system has been developed based on a standard PDA and wireless sensor nodes equipped with commercially available Bluetooth radio components, permitting real-time streaming of high-bandwidth data from various physiological and contextual sensors. We also present the results of abnormal gait diagnosis using the complete system from our evaluation, and illustrate how the wearable system and its operation can be remotely configured and managed by either enterprise systems or medical personnel at centralized locations. Conclusion: By using commercially available hardware components and software architecture presented in this paper, the MEDIC system can be rapidly configured, providing medical researchers with broadband sensor data from remote patients and platform access to best adapt operation for diagnostic operation objectives.",project-academic
10.1109/JBHI.2018.2868656,2019-07-01,a,IEEE,robust methods for real time diabetic foot ulcer detection and localization on mobile devices," Current practice for diabetic foot ulcers (DFU) screening involves detection and localization by podiatrists. Existing automated solutions either focus on segmentation or classification. In this work, we design deep learning methods for real-time DFU localization. To produce a robust deep learning model, we collected an extensive database of 1775 images of DFU. Two medical experts produced the ground truths of this data set by outlining the region of interest of DFU with an annotator software. Using five-fold cross-validation, overall, faster R-CNN with InceptionV2 model using two-tier transfer learning achieved a mean average precision of 91.8%, the speed of 48 ms for inferencing a single image and with a model size of 57.2 MB. To demonstrate the robustness and practicality of our solution to real-time prediction, we evaluated the performance of the models on a NVIDIA Jetson TX2 and a smartphone app. This work demonstrates the capability of deep learning in real-time localization of DFU, which can be further improved with a more extensive data set.",project-academic
,2004-01-01,p,,inductive system health monitoring, The Inductive Monitoring System (IMS) software was developed to provide a technique to automatically produce health monitoring knowledge bases for systems that are either difficult to model (simulate) with a computer or which require computer models that are too complex to use for real time monitoring. IMS uses nominal data sets collected either directly from the system or from simulations to build a knowledge base that can be used to detect anomalous behavior in the system. Machine learning and data mining techniques are used to characterize typical system behavior by extracting general classes of nominal data from archived data sets. IMS is able to monitor the system by comparing real time operational data with these classes. We present a description of learning and monitoring method used by IMS and summarize some recent IMS results.,project-academic
10.3389/FNINF.2018.00083,2018-11-14,a,Frontiers Media SA,the temple university hospital seizure detection corpus," The electroencephalogram (EEG), which has been in clinical use for over 70 years, is still an essential tool for diagnosis of neural functioning (Kennett, 2012). Well-known applications of EEGs include identification of epilepsy and epileptic seizures, anoxic and hypoxic damage to the brain, and identification of neural disorders such as hemorrhagic stroke, ischemia and toxic metabolic encephalopathy (Drury, 1988). More recently there has been interest in diagnosing Alzheimer's (Tsolaki et al., 2014), head trauma (Rapp et al., 2015), and sleep disorders (Younes, 2017). Many of these clinical applications now involve the collection of large amounts of data (e.g., 72-h continuous EEG recordings), which makes manual interpretation challenging. Similarly, the increased use of EEGs in critical care has created a significant demand for high-performance automatic interpretation software (e.g., real-time seizure detection).

A critical obstacle in the development of machine learning (ML) technology for these applications is the lack of big data resources to support training of complex deep learning systems. One of the most popular transcribed seizure databases available to the research community, the CHB-MIT Corpus (Goldberger et al., 2000), only consists of 23 subjects. Though high performance has been achieved on this corpus (Shoeb and Guttag, 2010), these results have not been representative of clinical performance (Golmohammadi et al., 2018). Therefore, we introduce the TUH EEG Seizure Corpus (TUSZ), which is the largest open source corpus of its type and represents an accurate characterization of clinical conditions.

Since seizures occur only a small fraction of the time in this type of data, and manual annotation of such low-yield data would be prohibitively expensive and unproductive, we developed a triage process for locating seizure recordings. We automatically selected data from the much larger TUH EEG Corpus (Obeid and Picone, 2016) that met certain selection criteria. Three approaches were used to identify files with a high probability that a seizure event occurred: (1) keyword search of EEG reports for sessions that were likely to contain seizures (e.g., reports containing phrases such as “seizure begins with” and “evolution”), (2) automatic detection of seizure events using commercially available software (Persyst Development Corporation., 2017), and (3) automatic detection using an experimental deep learning system (Golmohammadi et al., 2018). Data for which approaches (2) and (3) were in agreement were given highest priority.

Accurate annotation of an EEG requires extensive training. For this reason, manual annotation of EEGs is usually done by board-certified neurologists with many years of post-medical school training. Consequently, it is difficult to transcribe large amounts of data because such expertise is in short supply and is most often focused on clinical practice. Previous attempts to employ panels of experts or use crowdsourcing strategies were not productive (Obeid et al., 2017). However, we have demonstrated that a viable alternative is to use a team of highly trained undergraduates at the Neural Engineering Data Consortium (NEDC) at Temple University. These students have been trained to transcribe data for seizure events (e.g., start/stop times; seizure type) at accuracy levels that rival expert neurologists at a fraction of the cost (Obeid et al., 2017; Shah et al. in review). In order to validate the team's work, a portion of their annotations were compared to those of expert neurologists and shown to have a high inter-rater agreement.

In this paper, we describe the techniques used to develop TUSZ, evaluate their effectiveness, and present some descriptive statistics on the resulting corpus.",project-academic
10.1007/S10664-020-09881-0,2020-11-01,a,Springer US,testing machine learning based systems a systematic mapping," A Machine Learning based System (MLS) is a software system including one or more components that learn how to perform a task from a given data set. The increasing adoption of MLSs in safety critical domains such as autonomous driving, healthcare, and finance has fostered much attention towards the quality assurance of such systems. Despite the advances in software testing, MLSs bring novel and unprecedented challenges, since their behaviour is defined jointly by the code that implements them and the data used for training them. To identify the existing solutions for functional testing of MLSs, and classify them from three different perspectives: (1) the context of the problem they address, (2) their features, and (3) their empirical evaluation. To report demographic information about the ongoing research. To identify open challenges for future research. We conducted a systematic mapping study about testing techniques for MLSs driven by 33 research questions. We followed existing guidelines when defining our research protocol so as to increase the repeatability and reliability of our results. We identified 70 relevant primary studies, mostly published in the last years. We identified 11 problems addressed in the literature. We investigated multiple aspects of the testing approaches, such as the used/proposed adequacy criteria, the algorithms for test input generation, and the test oracles. The most active research areas in MLS testing address automated scenario/input generation and test oracle creation. MLS testing is a rapidly growing and developing research area, with many open challenges, such as the generation of realistic inputs and the definition of reliable evaluation metrics and benchmarks.",project-academic
10.3389/FPUBH.2019.00081,2019-04-12,a,Frontiers Media SA,crowdbreaks tracking health trends using public social media data and crowdsourcing," In the past decade, tracking health trends using social media data has shown great promise, due to a powerful combination of massive adoption of social media around the world, and increasingly potent hardware and software that enables us to work with these new big data streams. At the same time, many challenging problems have been identified. First, there is often a mismatch between how rapidly online data can change, and how rapidly algorithms are updated, which means that there is limited reusability for algorithms trained on past data as their performance decreases over time. Second, much of the work is focusing on specific issues during a specific past period in time, even though public health institutions would need flexible tools to assess multiple evolving situations in real time. Third, most tools providing such capabilities are proprietary systems with little algorithmic or data transparency, and thus little buy-in from the global public health and research community. Here, we introduce Crowdbreaks, an open platform which allows tracking of health trends by making use of continuous crowdsourced labelling of public social media content. The system is built in a way which automatizes the typical workflow from data collection, filtering, labelling and training of machine learning classifiers and therefore can greatly accelerate the research process in the public health domain. This work describes the technical aspects of the platform, thereby covering the functionalities at its current state and exploring its future use cases and extensions.",project-academic
10.1016/J.GIE.2008.04.031,2008-12-01,a,Elsevier,neural network analysis of dynamic sequences of eus elastography used for the differential diagnosis of chronic pancreatitis and pancreatic cancer," Background None EUS elastography is a newly developed imaging procedure that characterizes the differences of hardness and strain between diseased and normal tissue. None Objective None To assess the accuracy of real-time EUS elastography in pancreatic lesions. None Design None Cross-sectional feasibility study. None Patients None The study group included, in total, 68 patients with normal pancreas (N = 22), chronic pancreatitis (N = 11), pancreatic adenocarcinoma (N = 32), and pancreatic neuroendocrine tumors (N = 3). A subgroup analysis of 43 cases with focal pancreatic masses was also performed. None Interventions None A postprocessing software analysis was used to examine the EUS elastography movies by calculating hue histograms of each individual image, data that were further subjected to an extended neural network analysis to differentiate benign from malignant patterns. None Main Outcome Measurements None To differentiate normal pancreas, chronic pancreatitis, pancreatic cancer, and neuroendocrine tumors. None Results None Based on a cutoff of 175 for the mean hue histogram values recorded on the region of interest, the sensitivity, specificity, and accuracy of differentiation of benign and malignant masses were 91.4%, 87.9%, and 89.7%, respectively. The positive and negative predictive values were 88.9% and 90.6%, respectively. Multilayer perceptron neural networks with both one and two hidden layers of neurons (3-layer perceptron and 4-layer perceptron) were trained to learn how to classify cases as benign or malignant, and yielded an excellent testing performance of 95% on average, together with a high training performance that equaled 97% on average. None Limitation None A lack of the surgical standard in all cases. None Conclusions None EUS elastography is a promising method that allows characterization and differentiation of normal pancreas, chronic pancreatitis, and pancreatic cancer. The currently developed methodology, based on artificial neural network processing of EUS elastography digitalized movies, enabled an optimal prediction of the types of pancreatic lesions. Future multicentric, randomized studies with adequate power will have to establish the clinical impact of this procedure for the differential diagnosis of focal pancreatic masses.",project-academic
10.2196/10410,2018-07-04,a,JMIR Publications Inc.,health care robotics qualitative exploration of key challenges and future directions," Background: The emergence of robotics is transforming industries around the world. Robot technologies are evolving exponentially, particularly as they converge with other functionalities such as artificial intelligence to learn from their environment, from each other, and from humans. Objective: The goal of the research was to understand the emerging role of robotics in health care and identify existing and likely future challenges to maximize the benefits associated with robotics and related convergent technologies. Methods: We conducted qualitative semistructured one-to-one interviews exploring the role of robotic applications in health care contexts. Using purposive sampling, we identified a diverse range of stakeholders involved in conceiving, procuring, developing, and using robotics in a range of national and international health care settings. Interviews were digitally recorded, transcribed verbatim, and analyzed thematically, supported by NVivo 10 (QSR International) software. Theoretically, this work was informed by the sociotechnical perspective, where social and technical systems are understood as being interdependent. Results: We conducted 21 interviews and these accounts suggested that there are significant opportunities for improving the safety, quality, and efficiency of health care through robotics, but our analysis identified 4 major barriers that need to be effectively negotiated to realize these: (1) no clear pull from professionals and patients, (2) appearance of robots and associated expectations and concerns, (3) disruption of the way work is organized and distributed, and (4) new ethical and legal challenges requiring flexible liability and ethical frameworks. Conclusions: Sociotechnical challenges associated with the effective integration of robotic applications in health care settings are likely to be significant, particularly for patient-facing functions. These need to be identified and addressed for effective innovation and adoption.",project-academic
10.1016/J.ESWA.2011.06.046,2012-01-01,a,Pergamon,diagnosing diabetes using neural networks on small mobile devices, Pervasive computing is often mentioned in the context of improving healthcare. This paper presents a novel approach for diagnosing diabetes using neural networks and pervasive healthcare computing technologies. The recent developments in small mobile devices and wireless communications provide a strong motivation to develop new software techniques and mobile services for pervasive healthcare computing. A distributed end-to-end pervasive healthcare system utilizing neural network computations for diagnosing illnesses was developed. This work presents the initial results for a simple client (patient's PDA) and server (powerful desktop PC) two-tier pervasive healthcare architecture. The computations of neural network operations on both client and server sides and wireless network communications between them are optimized for real time use of pervasive healthcare services.,project-academic
10.1001/JAMACARDIO.2021.0185,2021-06-01,a,American Medical Association,utility of a deep learning algorithm to guide novices to acquire echocardiograms for limited diagnostic use," Importance None Artificial intelligence (AI) has been applied to analysis of medical imaging in recent years, but AI to guide the acquisition of ultrasonography images is a novel area of investigation. A novel deep-learning (DL) algorithm, trained on more than 5 million examples of the outcome of ultrasonographic probe movement on image quality, can provide real-time prescriptive guidance for novice operators to obtain limited diagnostic transthoracic echocardiographic images. None Objective None To test whether novice users could obtain 10-view transthoracic echocardiographic studies of diagnostic quality using this DL-based software. None Design, Setting, and Participants None This prospective, multicenter diagnostic study was conducted in 2 academic hospitals. A cohort of 8 nurses who had not previously conducted echocardiograms was recruited and trained with AI. Each nurse scanned 30 patients aged at least 18 years who were scheduled to undergo a clinically indicated echocardiogram at Northwestern Memorial Hospital or Minneapolis Heart Institute between March and May 2019. These scans were compared with those of sonographers using the same echocardiographic hardware but without AI guidance. None Interventions None Each patient underwent paired limited echocardiograms: one from a nurse without prior echocardiography experience using the DL algorithm and the other from a sonographer without the DL algorithm. Five level 3–trained echocardiographers independently and blindly evaluated each acquisition. None Main Outcomes and Measures None Four primary end points were sequentially assessed: qualitative judgement about left ventricular size and function, right ventricular size, and the presence of a pericardial effusion. Secondary end points included 6 other clinical parameters and comparison of scans by nurses vs sonographers. None Results None A total of 240 patients (mean [SD] age, 61 [16] years old; 139 men [57.9%]; 79 [32.9%] with body mass indexes >30) completed the study. Eight nurses each scanned 30 patients using the DL algorithm, producing studies judged to be of diagnostic quality for left ventricular size, function, and pericardial effusion in 237 of 240 cases (98.8%) and right ventricular size in 222 of 240 cases (92.5%). For the secondary end points, nurse and sonographer scans were not significantly different for most parameters. None Conclusions and Relevance None This DL algorithm allows novices without experience in ultrasonography to obtain diagnostic transthoracic echocardiographic studies for evaluation of left ventricular size and function, right ventricular size, and presence of a nontrivial pericardial effusion, expanding the reach of echocardiography to clinical settings in which immediate interrogation of anatomy and cardiac function is needed and settings with limited resources.",project-academic
,2009-02-11,,,device for real time monitoring mine roof rock formation or concrete structure stability," The invention relates to a device for monitoring the stability of roof strata of a mine or concrete engineering in real time. The device mainly comprises a microseism signal conduction rod, a microseism signal sensing element and a microseism signal collection and analysis host computer. The microseism signal collection and analysis host computer is provided with a filter circuit for filtering the affection of ambient noise signal, a rapid data sampling circuit and a professional diagnosis and analysis software for analyzing the stability of the roof strata of the mine or the concrete structure in real time by utilizing artificial intelligence technology. The device is suitable for coal mines or other mines, can be used for the stability monitoring of underground construction which adopting concrete bar and concrete as the materials, such as metro stations or tunnels, and can also be used for the stability monitoring of dams, bridges, tall buildings and mountain landslides. Through a set of technology of digital filtering recognition and collection for the microseism signal, the recognition rate for acoustic signal released during rock breaking can be improved above 80 percent, and the purpose of monitoring the stability of the roof strata of the mine or the concrete structure is well completed.",project-academic
10.1016/J.CGH.2011.09.014,2012-01-01,a,Elsevier,efficacy of an artificial neural network based approach to endoscopic ultrasound elastography in diagnosis of focal pancreatic masses," Background & Aims None By using strain assessment, real-time endoscopic ultrasound (EUS) elastography provides additional information about a lesion's characteristics in the pancreas. We assessed the accuracy of real-time EUS elastography in focal pancreatic lesions using computer-aided diagnosis by artificial neural network analysis. None Methods None We performed a prospective, blinded, multicentric study at of 258 patients (774 recordings from EUS elastography) who were diagnosed with chronic pancreatitis (n = 47) or pancreatic adenocarcinoma (n = 211) from 13 tertiary academic medical centers in Europe (the European EUS Elastography Multicentric Study Group). We used postprocessing software analysis to compute individual frames of elastography movies recorded by retrieving hue histogram data from a dynamic sequence of EUS elastography into a numeric matrix. The data then were analyzed in an extended neural network analysis, to automatically differentiate benign from malignant patterns. None Results None The neural computing approach had 91.14% training accuracy (95% confidence interval [CI], 89.87%–92.42%) and 84.27% testing accuracy (95% CI, 83.09%–85.44%). These results were obtained using the 10-fold cross-validation technique. The statistical analysis of the classification process showed a sensitivity of 87.59%, a specificity of 82.94%, a positive predictive value of 96.25%, and a negative predictive value of 57.22%. Moreover, the corresponding area under the receiver operating characteristic curve was 0.94 (95% CI, 0.91%–0.97%), which was significantly higher than the values obtained by simple mean hue histogram analysis, for which the area under the receiver operating characteristic was 0.85. None Conclusions None Use of the artificial intelligence methodology via artificial neural networks supports the medical decision process, providing fast and accurate diagnoses.",project-academic
10.1371/JOURNAL.PONE.0191664,2018-06-18,a,PUBLIC LIBRARY SCIENCE,low cost three dimensional printed phantom for neuraxial anesthesia training development and comparison to a commercial model," Neuraxial anesthesia (spinal and epidural anesthesia) procedures have significant learning curves and have been traditionally taught at the bed side, exposing patients to the increased risk associated with procedures done by novices. Simulation based medical education allows trainees to repeatedly practice and hone their skills prior to patient interaction. Wide-spread adoption of simulation-based medical education for procedural teaching has been slow due to the expense and limited variety of commercially available phantoms. Free/Libre/open-source (FLOS) software and desktop 3D printing technologies has enabled the fabrication of low-cost, patient-specific medical phantoms. However, few studies have evaluated the performance of these devices compared to commercially available phantoms. This paper describes the fabrication of a low-cost 3D printed neuraxial phantom based on computed tomorography (CT) scan data, and expert validation data comparing this phantom to a commercially available model. None Methods Anonymized CT DICOM data was segmented to create a 3D model of the lumbar spine. The 3D model was modified, placed inside a digitally designed housing unit and fabricated on a desktop 3D printer using polylactic acid (PLA) filament. The model was filled with an echogenic solution of gelatin with psyllium fiber. Twenty-two staff anesthesiologists performed a spinal and epidural on the 3D printed simulator and a commercially available Simulab phantom. Participants evaluated the tactile and ultrasound imaging fidelity of both phantoms via Likert-scale questionnaire. None None Results The 3D printed neuraxial phantom cost $13 to print and required 25 hours of non-supervised printing and 2 hours of assembly time. The 3D printed phantom was found to be less realistic to surface palpation than the Simulab phantom due to fragility of the silicone but had significantly better fidelity for loss of resistance, dural puncture and ultrasound imaging than the Simulab phantom. None None Conclusion Low-cost neuraxial phantoms with fidelity comparable to commercial models can be produced using CT data and low-cost infrastructure consisting of FLOS software and desktop 3D printers.",project-academic
,2013-01-01,a,,comparative analysis of classification function techniques for heart disease prediction," 2 ABSTRACT: The data mining can be referred as discovery of relationships in large databases automatically and in some cases it is used for predicting relationships based on the results discovered. Data mining plays an important role in various applications such as business organizations, e-commerce, health care industry, scientific and engineering. In the health care industry, the data mining is mainly used for Disease Prediction. Various data mining techniques are available for predicting diseases namely clustering, classification, association rules, regression and etc. This paper analyses the performance of various classification function techniques in data mining for predicting the heart disease from the heart disease data set. The classification function algorithms used and tested in this work are Logistics, Multi Layer Perception and Sequential Minimal Optimization algorithms. Comparative analysis is done by using Waikato Environment for Knowledge Analysis or in short, WEKA. It is open source software which consists of a collection of machine learning algorithms for data mining tasks. The performance factors used for analysing the efficiency of algorithms are clustering accuracy and error rate. The result shows that logistics classification function efficiency is better than multi layer perception and sequential minimal optimization. Data mining can be defined as the extraction of useful knowledge from large data repositories. Compared with other data mining application fields, medical data mining plays a vital role and it has some unique characteristics. Data mining techniques are the result of a long process of research and product development. This evolution began when business data was first stored on computers, continued with improvements in data access, and more recently, generated technologies that allow users to navigate through their data in real time. Data mining takes this evolutionary process beyond retrospective data access and navigation to prospective and proactive information delivery. Data mining is ready for application in the business community because it is supported by three technologies that are now sufficiently mature: Massive data collection, Powerful multiprocessor computers and Data mining algorithms The medical data mining has the high potential in medical domain for extracting the hidden patterns in the datasets (3). These patterns are used for clinical diagnosis and prognosis. The medical data are widely distributed, heterogeneous, voluminous in nature. The data should be integrated and collected to provide a user oriented approach to novel and hidden patterns of the data. A major problem in medical science or bioinformatics analysis is in attaining the correct diagnosis of certain important information. For an ultimate diagnosis, normally, many tests generally involve the classification or clustering of large scale data. The test procedures are said to be necessary in order to reach the ultimate diagnosis. However, on the other hand, too many tests could complicate the main diagnosis process and lead to the difficulty in obtaining the end results, particularly in the case of finding disease many tests are should be performed. This kind of difficulty could be resolved with the aid of machine learning which could be used directly to obtain the end result with the aid of several artificial intelligent algorithms which perform the role as classifiers. Classification is one of the most important techniques in data mining. If a categorization process is to be done, the data is to be classified, and/or codified, and then it can be placed into chunks that are manageable by a human (12). This paper describes classification function algorithms and it also analyzes the performance of these algorithms. The performance factors used for analysis are accuracy and error measures. The accuracy measures are True Positive (TP) rate, F Measure, ROC area and Kappa Statistics. The error measures are Mean Absolute Error (M.A.E), Root Mean Squared Error (R.M.S.E), Relative Absolute Error (R.A.E) and Relative Root Squared Error (R.R.S.E).",project-academic
10.1002/JRSM.1311,2018-09-25,a,John Wiley & Sons Ltd,prioritising references for systematic reviews with robotanalyst a user study," Screening references is a time-consuming step necessary for systematic reviews and guideline development. Previous studies have shown that human effort can be reduced by using machine learning software to prioritise large reference collections such that most of the relevant references are identified before screening is completed. We describe and evaluate RobotAnalyst, a Web-based software system that combines text-mining and machine learning algorithms for organising references by their content and actively prioritising them based on a relevancy classification model trained and updated throughout the process. We report an evaluation over 22 reference collections (most are related to public health topics) screened using RobotAnalyst with a total of 43 610 abstract-level decisions. The number of references that needed to be screened to identify 95% of the abstract-level inclusions for the evidence review was reduced on 19 of the 22 collections. Significant gains over random sampling were achieved for all reviews conducted with active prioritisation, as compared with only two of five when prioritisation was not used. RobotAnalyst's descriptive clustering and topic modelling functionalities were also evaluated by public health analysts. Descriptive clustering provided more coherent organisation than topic modelling, and the content of the clusters was apparent to the users across a varying number of clusters. This is the first large-scale study using technology-assisted screening to perform new reviews, and the positive results provide empirical evidence that RobotAnalyst can accelerate the identification of relevant studies. The results also highlight the issue of user complacency and the need for a stopping criterion to realise the work savings.",project-academic
10.1080/10253890.2019.1584180,2019-04-04,a,Informa UK Limited,stress measurement using speech recent advancements validation issues and ethical and privacy considerations," Life stress is a well-established risk factor for a variety of mental and physical health problems, including anxiety disorders, depression, chronic pain, heart disease, asthma, autoimmune diseases, and neurodegenerative disorders. The purpose of this article is to describe emerging approaches for assessing stress using speech, which we do by reviewing the methodological advantages of these digital health tools, and the validation, ethical, and privacy issues raised by these technologies. As we describe, it is now possible to assess stress via the speech signal using smartphones and smart speakers that employ software programs and artificial intelligence to analyze several features of speech and speech acoustics, including pitch, jitter, energy, rate, and length and number of pauses. Because these digital devices are ubiquitous, we can now assess individuals' stress levels in real time in almost any natural environment in which people speak. These technologies thus have great potential for advancing digital health initiatives that involve continuously monitoring changes in psychosocial functioning and disease risk over time. However, speech-based indices of stress have yet to be well-validated against stress biomarkers (e.g., cortisol, cytokines) that predict disease risk. In addition, acquiring speech samples raises the possibility that conversations intended to be private could one day be made public; moreover, obtaining real-time psychosocial risk information prompts ethical questions regarding how these data should be used for medical, commercial, and personal purposes. Although assessing stress using speech thus has enormous potential, there are critical validation, privacy, and ethical issues that must be addressed.",project-academic
10.1038/S42256-021-00337-8,2021-06-01,a,Nature Publishing Group,end to end privacy preserving deep learning on multi institutional medical imaging," Using large, multi-national datasets for high-performance medical imaging AI systems requires innovation in privacy-preserving machine learning so models can train on sensitive data without requiring data transfer. Here we present PriMIA (Privacy-preserving Medical Image Analysis), a free, open-source software framework for differentially private, securely aggregated federated learning and encrypted inference on medical imaging data. We test PriMIA using a real-life case study in which an expert-level deep convolutional neural network classifies paediatric chest X-rays; the resulting model’s classification performance is on par with locally, non-securely trained models. We theoretically and empirically evaluate our framework’s performance and privacy guarantees, and demonstrate that the protections provided prevent the reconstruction of usable data by a gradient-based model inversion attack. Finally, we successfully employ the trained model in an end-to-end encrypted remote inference scenario using secure multi-party computation to prevent the disclosure of the data and the model. Gaining access to medical data to train AI applications can present problems due to patient privacy or proprietary interests. A way forward can be privacy-preserving federated learning schemes. Kaissis, Ziller and colleagues demonstrate here their open source framework for privacy-preserving medical image analysis in a remote inference scenario.",project-academic
10.1016/J.IJROBP.2018.08.032,2018-11-15,a,Elsevier Science,a deep look into the future of quantitative imaging in oncology a statement of working principles and proposal for change," The adoption of enterprise digital imaging, along with the development of quantitative imaging methods and the re-emergence of statistical learning, has opened the opportunity for more personalized cancer treatments through transformative data science research. In the last 5 years, accumulating evidence has indicated that noninvasive advanced imaging analytics (i.e., radiomics) can reveal key components of tumor phenotype for multiple lesions at multiple time points over the course of treatment. Many groups using homegrown software have extracted engineered and deep quantitative features on 3-dimensional medical images for better spatial and longitudinal understanding of tumor biology and for the prediction of diverse outcomes. These developments could augment patient stratification and prognostication, buttressing emerging targeted therapeutic approaches. Unfortunately, the rapid growth in popularity of this immature scientific discipline has resulted in many early publications that miss key information or use underpowered patient data sets, without production of generalizable results. Quantitative imaging research is complex, and key principles should be followed to realize its full potential. The fields of quantitative imaging and radiomics in particular require a renewed focus on optimal study design and reporting practices, standardization, interpretability, data sharing, and clinical trials. Standardization of image acquisition, feature calculation, and statistical analysis (i.e., machine learning) are required for the field to move forward. A new data-sharing paradigm enacted among open and diverse participants (medical institutions, vendors and associations) should be embraced for faster development and comprehensive clinical validation of imaging biomarkers. In this review and critique of the field, we propose working principles and fundamental changes to the current scientific approach, with the goal of high-impact research and development of actionable prediction models that will yield more meaningful applications of precision cancer medicine.",project-academic
10.3233/THC-2009-0546,2009-08-01,a,IOS Press,annotating smart environment sensor data for activity learning," The pervasive sensing technologies found in smart homes offer unprecedented opportunities for providing health monitoring and assistance to individuals experiencing difficulties living independently at home. In order to monitor the functional health of smart home residents, we need to design technologies that recognize and track the activities that people perform at home. Machine learning techniques can perform this task, but the software algorithms rely upon large amounts of sample data that is correctly labeled with the corresponding activity. Labeling, or annotating, sensor data with the corresponding activity can be time consuming, may require input from the smart home resident, and is often inaccurate. Therefore, in this paper we investigate four alternative mechanisms for annotating sensor data with a corresponding activity label. We evaluate the alternative methods along the dimensions of annotation time, resident burden, and accuracy using sensor data collected in a real smart apartment.",project-academic
10.1109/JBHI.2018.2867619,2019-07-01,a,Institute of Electrical and Electronics Engineers (IEEE),multimodal ambulatory sleep detection using lstm recurrent neural networks," Unobtrusive and accurate ambulatory methods are needed to monitor long-term sleep patterns for improving health. Previously developed ambulatory sleep detection methods rely either in whole or in part on self-reported diary data as ground truth, which is a problem, since people often do not fill them out accurately. This paper presents an algorithm that uses multimodal data from smartphones and wearable technologies to detect sleep/wake state and sleep onset/offset using a type of recurrent neural network with long-short-term memory (LSTM) cells for synthesizing temporal information. We collected 5580 days of multimodal data from 186 participants and compared the new method for sleep/wake classification and sleep onset/offset detection to, first, nontemporal machine learning methods and, second, a state-of-the-art actigraphy software. The new LSTM method achieved a sleep/wake classification accuracy of 96.5%, and sleep onset/offset detection None None $F_1$ None None scores of 0.86 and 0.84, respectively, with mean absolute errors of 5.0 and 5.5 min, res-pectively, when compared with sleep/wake state and sleep onset/offset assessed using actigraphy and sleep diaries. The LSTM results were statistically superior to those from nontemporal machine learning algorithms and the actigraphy software. We show good generalization of the new algorithm by comparing participant-dependent and participant-independent models, and we show how to make the model nearly realtime with slightly reduced performance.",project-academic
10.1109/JBHI.2015.2427511,2015-04-28,a,IEEE J Biomed Health Inform,a natural walking monitor for pulmonary patients using mobile phones," Mobile devices have the potential to continuously monitor health by collecting movement data including walking speed during natural walking. Natural walking is walking without artificial speed constraints present in both treadmill and nurse-assisted walking. Fitness trackers have become popular which record steps taken and distance, typically using a fixed stride length. While useful for everyday purposes, medical monitoring requires precise accuracy and testing on real patients with a scientifically valid measure. Walking speed is closely linked to morbidity in patients and widely used for medical assessment via measured walking. The 6-min walk test (6MWT) is a standard assessment for chronic obstructive pulmonary disease and congestive heart failure. Current generation smartphone hardware contains similar sensor chips as in medical devices and popular fitness devices. We developed a middleware software, MoveSense, which runs on standalone smartphones while providing comparable readings to medical accelerometers. We evaluate six machine learning methods to obtain gait speed during natural walking training models to predict natural walking speed and distance during a 6MWT with 28 pulmonary patients and ten subjects without pulmonary condition. We also compare our model's accuracy to popular fitness devices. Our universally trained support vector machine models produce 6MWT distance with 3.23% error during a controlled 6MWT and 11.2% during natural free walking. Furthermore, our model attains 7.9% error when tested on five subjects for distance estimation compared to the 50–400% error seen in fitness devices during natural walking.",project-academic
,2015-11-04,,,hospital intelligence assessment triage system based on artificial neural network," A hospital intelligence assessment triage system based on an artificial neural network is formed by 1, a portable assessment instruction terminal, 2, a data analysis center and 3, a scalability triage command platform, wherein the portable assessment instruction terminal comprises a man-machine interface, communication, CPU control and a general data interface; the data analysis center detects patient information uploaded by the portable assessment instruction terminal, feeds back a triage result in real time and transmits to the scalability triage command platform; an intelligent artificial neural network module learns a correction algorithm through a case so that optimization is achieved; the scalability triage command platform comprises a scalability software interface, a medical resource database and a triage calling server module; according to a demand, a medical resource database can be expanded and edited and a triage instruction is issued. In the system, through an expansion mode, a general intelligent command platform which can adapt to various kinds of hospital triage services is realized so that medical resources can be managed in a hierarchical mode, are associated with each other and become a unified scheduling integral body. Through intelligent analysis, a patient enters into an optimum medical process. Time is saved, resource configuration is optimized and efficiency is increased.",project-academic
