id,updated,published,title,summary,database
http://arxiv.org/abs/2012.01872v2,2021-05-06T02:52:49Z,2020-12-03T12:31:07Z,Towards Repairing Neural Networks Correctly,"Neural networks are increasingly applied to support decision making in
safety-critical applications (like autonomous cars, unmanned aerial vehicles
and face recognition based authentication). While many impressive static
verification techniques have been proposed to tackle the correctness problem of
neural networks, it is possible that static verification may never be
sufficiently scalable to handle real-world neural networks. In this work, we
propose a runtime verification method to ensure the correctness of neural
networks. Given a neural network and a desirable safety property, we adopt
state-of-the-art static verification techniques to identify strategically
locations to introduce additional gates which ""correct"" neural network
behaviors at runtime. Experiment results show that our approach effectively
generates neural networks which are guaranteed to satisfy the properties,
whilst being consistent with the original neural network most of the time.",arxiv
http://arxiv.org/abs/1703.10049v4,2021-08-10T07:28:52Z,2017-03-29T14:12:42Z,"Autonomous Recharging and Flight Mission Planning for Battery-operated
  Autonomous Drones","Unmanned aerial vehicles (UAVs), commonly known as drones, are being
increasingly deployed throughout the globe as a means to streamline logistic
and monitoring routines. When dispatched on autonomous missions, drones require
an intelligent decision-making system for trajectory planning and tour
optimization. Given the limited capacity of their onboard batteries, a key
design challenge is ensuring the underlying algorithms can efficiently optimize
the mission objectives along with recharging operations during long-haul
flights. Against this backdrop, the present work undertakes a comprehensive
study on automated management systems for battery-constrained drones: (1) We
construct a machine learning model to estimate the energy expenditure of
drones, considering diverse real-world factors and flight scenarios. (2)
Leveraging this model, the joint problem of flight mission planning and
recharging optimization is formulated as a multi-criteria combinatorial program
aimed at completing a tour mission for a set of target sites in the shortest
time while minimizing recharging duration. (3) We devise an efficient
approximation algorithm, with provable near-optimal performance guarantees, and
implement it in a drone management system, which supports real-time flight path
tracking and re-computation in dynamic environments. (4) We validate the
effectiveness and practicality of the proposed approach through extensive
numerical simulations as well as real-world experiments.",arxiv
http://arxiv.org/abs/2004.06154v1,2020-04-13T18:53:12Z,2020-04-13T18:53:12Z,"An Efficient UAV-based Artificial Intelligence Framework for Real-Time
  Visual Tasks","Modern Unmanned Aerial Vehicles equipped with state of the art artificial
intelligence (AI) technologies are opening to a wide plethora of novel and
interesting applications. While this field received a strong impact from the
recent AI breakthroughs, most of the provided solutions either entirely rely on
commercial software or provide a weak integration interface which denies the
development of additional techniques. This leads us to propose a novel and
efficient framework for the UAV-AI joint technology. Intelligent UAV systems
encounter complex challenges to be tackled without human control. One of these
complex challenges is to be able to carry out computer vision tasks in
real-time use cases. In this paper we focus on this challenge and introduce a
multi-layer AI (MLAI) framework to allow easy integration of ad-hoc
visual-based AI applications. To show its features and its advantages, we
implemented and evaluated different modern visual-based deep learning models
for object detection, target tracking and target handover.",arxiv
http://arxiv.org/abs/1712.04248v2,2018-02-16T14:40:42Z,2017-12-12T11:36:26Z,"Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box
  Machine Learning Models","Many machine learning algorithms are vulnerable to almost imperceptible
perturbations of their inputs. So far it was unclear how much risk adversarial
perturbations carry for the safety of real-world machine learning applications
because most methods used to generate such perturbations rely either on
detailed model information (gradient-based attacks) or on confidence scores
such as class probabilities (score-based attacks), neither of which are
available in most real-world scenarios. In many such cases one currently needs
to retreat to transfer-based attacks which rely on cumbersome substitute
models, need access to the training data and can be defended against. Here we
emphasise the importance of attacks which solely rely on the final model
decision. Such decision-based attacks are (1) applicable to real-world
black-box models such as autonomous cars, (2) need less knowledge and are
easier to apply than transfer-based attacks and (3) are more robust to simple
defences than gradient- or score-based attacks. Previous attacks in this
category were limited to simple models or simple datasets. Here we introduce
the Boundary Attack, a decision-based attack that starts from a large
adversarial perturbation and then seeks to reduce the perturbation while
staying adversarial. The attack is conceptually simple, requires close to no
hyperparameter tuning, does not rely on substitute models and is competitive
with the best gradient-based attacks in standard computer vision tasks like
ImageNet. We apply the attack on two black-box algorithms from Clarifai.com.
The Boundary Attack in particular and the class of decision-based attacks in
general open new avenues to study the robustness of machine learning models and
raise new questions regarding the safety of deployed machine learning systems.
An implementation of the attack is available as part of Foolbox at
https://github.com/bethgelab/foolbox .",arxiv
http://arxiv.org/abs/1411.6326v1,2014-11-24T02:09:59Z,2014-11-24T02:09:59Z,Vision and Learning for Deliberative Monocular Cluttered Flight,"Cameras provide a rich source of information while being passive, cheap and
lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work
we present the first implementation of receding horizon control, which is
widely used in ground vehicles, with monocular vision as the only sensing mode
for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a
number of contributions: novel coupling of perception and control via relevant
and diverse, multiple interpretations of the scene around the robot, leveraging
recent advances in machine learning to showcase anytime budgeted cost-sensitive
feature selection, and fast non-linear regression for monocular depth
prediction. We empirically demonstrate the efficacy of our novel pipeline via
real world experiments of more than 2 kms through dense trees with a quadrotor
built from off-the-shelf parts. Moreover our pipeline is designed to combine
information from other modalities like stereo and lidar as well if available.",arxiv
http://arxiv.org/abs/2009.11722v1,2020-09-23T09:23:29Z,2020-09-23T09:23:29Z,"Cloud2Edge Elastic AI Framework for Prototyping and Deployment of AI
  Inference Engines in Autonomous Vehicles","Self-driving cars and autonomous vehicles are revolutionizing the automotive
sector, shaping the future of mobility altogether. Although the integration of
novel technologies such as Artificial Intelligence (AI) and Cloud/Edge
computing provides golden opportunities to improve autonomous driving
applications, there is the need to modernize accordingly the whole prototyping
and deployment cycle of AI components. This paper proposes a novel framework
for developing so-called AI Inference Engines for autonomous driving
applications based on deep learning modules, where training tasks are deployed
elastically over both Cloud and Edge resources, with the purpose of reducing
the required network bandwidth, as well as mitigating privacy issues. Based on
our proposed data driven V-Model, we introduce a simple yet elegant solution
for the AI components development cycle, where prototyping takes place in the
cloud according to the Software-in-the-Loop (SiL) paradigm, while deployment
and evaluation on the target ECUs (Electronic Control Units) is performed as
Hardware-in-the-Loop (HiL) testing. The effectiveness of the proposed framework
is demonstrated using two real-world use-cases of AI inference engines for
autonomous vehicles, that is environment perception and most probable path
prediction.",arxiv
http://arxiv.org/abs/2010.02663v1,2020-10-06T12:23:05Z,2020-10-06T12:23:05Z,"Heterogeneous Multi-Agent Reinforcement Learning for Unknown Environment
  Mapping","Reinforcement learning in heterogeneous multi-agent scenarios is important
for real-world applications but presents challenges beyond those seen in
homogeneous settings and simple benchmarks. In this work, we present an
actor-critic algorithm that allows a team of heterogeneous agents to learn
decentralized control policies for covering an unknown environment. This task
is of interest to national security and emergency response organizations that
would like to enhance situational awareness in hazardous areas by deploying
teams of unmanned aerial vehicles. To solve this multi-agent coverage path
planning problem in unknown environments, we augment a multi-agent actor-critic
architecture with a new state encoding structure and triplet learning loss to
support heterogeneous agent learning. We developed a simulation environment
that includes real-world environmental factors such as turbulence, delayed
communication, and agent loss, to train teams of agents as well as probe their
robustness and flexibility to such disturbances.",arxiv
http://arxiv.org/abs/2105.14190v1,2021-05-20T01:38:45Z,2021-05-20T01:38:45Z,RaspberryPI for mosquito neutralization by power laser,"In this article for the first time, comprehensive studies of mosquito
neutralization using machine vision and a 1 W power laser are considered.
Developed laser installation with Raspberry Pi that changing the direction of
the laser with a galvanometer. We developed a program for mosquito tracking in
real. The possibility of using deep neural networks, Haar cascades, machine
learning for mosquito recognition was considered. We considered in detail the
classification problems of mosquitoes in images. A recommendation is given for
the implementation of this device based on a microcontroller for subsequent use
as part of an unmanned aerial vehicle. Any harmful insects in the fields can be
used as objects for control.",arxiv
http://arxiv.org/abs/1708.08559v2,2018-03-20T06:10:24Z,2017-08-28T23:26:14Z,"DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous
  Cars","Recent advances in Deep Neural Networks (DNNs) have led to the development of
DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can
drive without any human intervention. Most major manufacturers including Tesla,
GM, Ford, BMW, and Waymo/Google are working on building and testing different
types of autonomous vehicles. The lawmakers of several US states including
California, Texas, and New York have passed new legislation to fast-track the
process of testing and deployment of autonomous vehicles on their roads.
  However, despite their spectacular progress, DNNs, just like traditional
software, often demonstrate incorrect or unexpected corner case behaviors that
can lead to potentially fatal collisions. Several such real-world accidents
involving autonomous cars have already happened including one which resulted in
a fatality. Most existing testing techniques for DNN-driven vehicles are
heavily dependent on the manual collection of test data under different driving
conditions which become prohibitively expensive as the number of test
conditions increases.
  In this paper, we design, implement and evaluate DeepTest, a systematic
testing tool for automatically detecting erroneous behaviors of DNN-driven
vehicles that can potentially lead to fatal crashes. First, our tool is
designed to automatically generated test cases leveraging real-world changes in
driving conditions like rain, fog, lighting conditions, etc. DeepTest
systematically explores different parts of the DNN logic by generating test
inputs that maximize the numbers of activated neurons. DeepTest found thousands
of erroneous behaviors under different realistic driving conditions (e.g.,
blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in
three top performing DNNs in the Udacity self-driving car challenge.",arxiv
http://arxiv.org/abs/2012.05410v1,2020-12-10T02:08:47Z,2020-12-10T02:08:47Z,Artificial Intelligence at the Edge,"The Internet of Things (IoT) and edge computing applications aim to support a
variety of societal needs, including the global pandemic situation that the
entire world is currently experiencing and responses to natural disasters.
  The need for real-time interactive applications such as immersive video
conferencing, augmented/virtual reality, and autonomous vehicles, in education,
healthcare, disaster recovery and other domains, has never been higher. At the
same time, there have been recent technological breakthroughs in highly
relevant fields such as artificial intelligence (AI)/machine learning (ML),
advanced communication systems (5G and beyond), privacy-preserving
computations, and hardware accelerators. 5G mobile communication networks
increase communication capacity, reduce transmission latency and error, and
save energy -- capabilities that are essential for new applications. The
envisioned future 6G technology will integrate many more technologies,
including for example visible light communication, to support groundbreaking
applications, such as holographic communications and high precision
manufacturing. Many of these applications require computations and analytics
close to application end-points: that is, at the edge of the network, rather
than in a centralized cloud. AI techniques applied at the edge have tremendous
potential both to power new applications and to need more efficient operation
of edge infrastructure. However, it is critical to understand where to deploy
AI systems within complex ecosystems consisting of advanced applications and
the specific real-time requirements towards AI systems.",arxiv
http://arxiv.org/abs/1811.06187v1,2018-11-15T05:26:38Z,2018-11-15T05:26:38Z,"Intervention Aided Reinforcement Learning for Safe and Practical Policy
  Optimization in Navigation","Combining deep neural networks with reinforcement learning has shown great
potential in the next-generation intelligent control. However, there are
challenges in terms of safety and cost in practical applications. In this
paper, we propose the Intervention Aided Reinforcement Learning (IARL)
framework, which utilizes human intervened robot-environment interaction to
improve the policy. We used the Unmanned Aerial Vehicle (UAV) as the test
platform. We built neural networks as our policy to map sensor readings to
control signals on the UAV. Our experiment scenarios cover both simulation and
reality. We show that our approach substantially reduces the human intervention
and improves the performance in autonomous navigation, at the same time it
ensures safety and keeps training cost acceptable.",arxiv
http://arxiv.org/abs/2008.06189v1,2020-08-14T04:35:10Z,2020-08-14T04:35:10Z,"An Improved Deep Convolutional Neural Network-Based Autonomous Road
  Inspection Scheme Using Unmanned Aerial Vehicles","Advancements in artificial intelligence (AI) gives a great opportunity to
develop an autonomous devices. The contribution of this work is an improved
convolutional neural network (CNN) model and its implementation for the
detection of road cracks, potholes, and yellow lane in the road. The purpose of
yellow lane detection and tracking is to realize autonomous navigation of
unmanned aerial vehicle (UAV) by following yellow lane while detecting and
reporting the road cracks and potholes to the server through WIFI or 5G medium.
The fabrication of own data set is a hectic and time-consuming task. The data
set is created, labeled and trained using default and an improved model. The
performance of both these models is benchmarked with respect to accuracy, mean
average precision (mAP) and detection time. In the testing phase, it was
observed that the performance of the improved model is better in respect of
accuracy and mAP. The improved model is implemented in UAV using the robot
operating system for the autonomous detection of potholes and cracks in roads
via UAV front camera vision in real-time.",arxiv
http://arxiv.org/abs/1708.05884v4,2018-11-22T15:12:02Z,2017-08-19T18:13:23Z,"Teaching UAVs to Race: End-to-End Regression of Agile Controls in
  Simulation","Automating the navigation of unmanned aerial vehicles (UAVs) in diverse
scenarios has gained much attention in recent years. However, teaching UAVs to
fly in challenging environments remains an unsolved problem, mainly due to the
lack of training data. In this paper, we train a deep neural network to predict
UAV controls from raw image data for the task of autonomous UAV racing in a
photo-realistic simulation. Training is done through imitation learning with
data augmentation to allow for the correction of navigation mistakes. Extensive
experiments demonstrate that our trained network (when sufficient data
augmentation is used) outperforms state-of-the-art methods and flies more
consistently than many human pilots. Additionally, we show that our optimized
network architecture can run in real-time on embedded hardware, allowing for
efficient on-board processing critical for real-world deployment. From a
broader perspective, our results underline the importance of extensive data
augmentation techniques to improve robustness in end-to-end learning setups.",arxiv
http://arxiv.org/abs/2012.10706v4,2021-07-30T13:36:36Z,2020-12-19T14:53:56Z,Siamese Anchor Proposal Network for High-Speed Aerial Tracking,"In the domain of visual tracking, most deep learning-based trackers highlight
the accuracy but casting aside efficiency. Therefore, their real-world
deployment on mobile platforms like the unmanned aerial vehicle (UAV) is
impeded. In this work, a novel two-stage Siamese network-based method is
proposed for aerial tracking, \textit{i.e.}, stage-1 for high-quality anchor
proposal generation, stage-2 for refining the anchor proposal. Different from
anchor-based methods with numerous pre-defined fixed-sized anchors, our
no-prior method can 1) increase the robustness and generalization to different
objects with various sizes, especially to small, occluded, and fast-moving
objects, under complex scenarios in light of the adaptive anchor generation, 2)
make calculation feasible due to the substantial decrease of anchor numbers. In
addition, compared to anchor-free methods, our framework has better performance
owing to refinement at stage-2. Comprehensive experiments on three benchmarks
have proven the superior performance of our approach, with a speed of around
200 frames/s.",arxiv
http://arxiv.org/abs/1801.05086v1,2018-01-16T01:14:12Z,2018-01-16T01:14:12Z,Autonomous UAV Navigation Using Reinforcement Learning,"Unmanned aerial vehicles (UAV) are commonly used for missions in unknown
environments, where an exact mathematical model of the environment may not be
available. This paper provides a framework for using reinforcement learning to
allow the UAV to navigate successfully in such environments. We conducted our
simulation and real implementation to show how the UAVs can successfully learn
to navigate through an unknown environment. Technical aspects regarding to
applying reinforcement learning algorithm to a UAV system and UAV flight
control were also addressed. This will enable continuing research using a UAV
with learning capabilities in more important applications, such as wildfire
monitoring, or search and rescue missions.",arxiv
http://arxiv.org/abs/1807.11785v1,2018-07-31T12:17:41Z,2018-07-31T12:17:41Z,Transfer Learning-Based Crack Detection by Autonomous UAVs,"Unmanned Aerial Vehicles (UAVs) have recently shown great performance
collecting visual data through autonomous exploration and mapping in building
inspection. Yet, the number of studies is limited considering the post
processing of the data and its integration with autonomous UAVs. These will
enable huge steps onward into full automation of building inspection. In this
regard, this work presents a decision making tool for revisiting tasks in
visual building inspection by autonomous UAVs. The tool is an implementation of
fine-tuning a pretrained Convolutional Neural Network (CNN) for surface crack
detection. It offers an optional mechanism for task planning of revisiting
pinpoint locations during inspection. It is integrated to a quadrotor UAV
system that can autonomously navigate in GPS-denied environments. The UAV is
equipped with onboard sensors and computers for autonomous localization,
mapping and motion planning. The integrated system is tested through
simulations and real-world experiments. The results show that the system
achieves crack detection and autonomous navigation in GPS-denied environments
for building inspection.",arxiv
http://arxiv.org/abs/2107.14389v1,2021-07-30T01:37:24Z,2021-07-30T01:37:24Z,DarkLighter: Light Up the Darkness for UAV Tracking,"Recent years have witnessed the fast evolution and promising performance of
the convolutional neural network (CNN)-based trackers, which aim at imitating
biological visual systems. However, current CNN-based trackers can hardly
generalize well to low-light scenes that are commonly lacked in the existing
training set. In indistinguishable night scenarios frequently encountered in
unmanned aerial vehicle (UAV) tracking-based applications, the robustness of
the state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial
tracking in the dark through a general fashion, this work proposes a low-light
image enhancer namely DarkLighter, which dedicates to alleviate the impact of
poor illumination and noise iteratively. A lightweight map estimation network,
i.e., ME-Net, is trained to efficiently estimate illumination maps and noise
maps jointly. Experiments are conducted with several SOTA trackers on numerous
UAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability
and universality of DarkLighter, with high efficiency. Moreover, DarkLighter
has further been implemented on a typical UAV system. Real-world tests at night
scenes have verified its practicability and dependability.",arxiv
http://arxiv.org/abs/2110.11573v1,2021-10-22T03:52:45Z,2021-10-22T03:52:45Z,"ModEL: A Modularized End-to-end Reinforcement Learning Framework for
  Autonomous Driving","Heated debates continue over the best autonomous driving framework. The
classic modular pipeline is widely adopted in the industry owing to its great
interpretability and stability, whereas the end-to-end paradigm has
demonstrated considerable simplicity and learnability along with the rise of
deep learning. We introduce a new modularized end-to-end reinforcement learning
framework (ModEL) for autonomous driving, which combines the merits of both
previous approaches. The autonomous driving stack of ModEL is decomposed into
perception, planning, and control module, leveraging scene understanding,
end-to-end reinforcement learning, and PID control respectively. Furthermore,
we build a fully functional autonomous vehicle to deploy this framework.
Through extensive simulation and real-world experiments, our framework has
shown great generalizability to various complicated scenarios and outperforms
the competing baselines.",arxiv
http://arxiv.org/abs/2108.05457v1,2021-08-11T21:39:51Z,2021-08-11T21:39:51Z,"Low-level Pose Control of Tilting Multirotor for Wall Perching Tasks
  Using Reinforcement Learning","Recently, needs for unmanned aerial vehicles (UAVs) that are attachable to
the wall have been highlighted. As one of the ways to address the need,
researches on various tilting multirotors that can increase maneuverability has
been employed. Unfortunately, existing studies on the tilting multirotors
require considerable amounts of prior information on the complex dynamic model.
Meanwhile, reinforcement learning on quadrotors has been studied to mitigate
this issue. Yet, these are only been applied to standard quadrotors, whose
systems are less complex than those of tilting multirotors. In this paper, a
novel reinforcement learning-based method is proposed to control a tilting
multirotor on real-world applications, which is the first attempt to apply
reinforcement learning to a tilting multirotor. To do so, we propose a novel
reward function for a neural network model that takes power efficiency into
account. The model is initially trained over a simulated environment and then
fine-tuned using real-world data in order to overcome the sim-to-real gap
issue. Furthermore, a novel, efficient state representation with respect to the
goal frame that helps the network learn optimal policy better is proposed. As
verified on real-world experiments, our proposed method shows robust
controllability by overcoming the complex dynamics of tilting multirotors.",arxiv
http://arxiv.org/abs/1804.10829v3,2018-07-01T17:33:53Z,2018-04-28T16:37:01Z,Formal Security Analysis of Neural Networks using Symbolic Intervals,"Due to the increasing deployment of Deep Neural Networks (DNNs) in real-world
security-critical domains including autonomous vehicles and collision avoidance
systems, formally checking security properties of DNNs, especially under
different attacker capabilities, is becoming crucial. Most existing security
testing techniques for DNNs try to find adversarial examples without providing
any formal security guarantees about the non-existence of such adversarial
examples. Recently, several projects have used different types of
Satisfiability Modulo Theory (SMT) solvers to formally check security
properties of DNNs. However, all of these approaches are limited by the high
overhead caused by the solver.
  In this paper, we present a new direction for formally checking security
properties of DNNs without using SMT solvers. Instead, we leverage interval
arithmetic to compute rigorous bounds on the DNN outputs. Our approach, unlike
existing solver-based approaches, is easily parallelizable. We further present
symbolic interval analysis along with several other optimizations to minimize
overestimations of output bounds.
  We design, implement, and evaluate our approach as part of ReluVal, a system
for formally checking security properties of Relu-based DNNs. Our extensive
empirical results show that ReluVal outperforms Reluplex, a state-of-the-art
solver-based system, by 200 times on average. On a single 8-core machine
without GPUs, within 4 hours, ReluVal is able to verify a security property
that Reluplex deemed inconclusive due to timeout after running for more than 5
days. Our experiments demonstrate that symbolic interval analysis is a
promising new direction towards rigorously analyzing different security
properties of DNNs.",arxiv
http://arxiv.org/abs/2109.01800v1,2021-09-04T06:27:13Z,2021-09-04T06:27:13Z,"A Comprehensive Approach for UAV Small Object Detection with
  Simulation-based Transfer Learning and Adaptive Fusion","Precisely detection of Unmanned Aerial Vehicles(UAVs) plays a critical role
in UAV defense systems. Deep learning is widely adopted for UAV object
detection whereas researches on this topic are limited by the amount of dataset
and small scale of UAV. To tackle these problems, a novel comprehensive
approach that combines transfer learning based on simulation data and adaptive
fusion is proposed. Firstly, the open-source plugin AirSim proposed by
Microsoft is used to generate mass realistic simulation data. Secondly,
transfer learning is applied to obtain a pre-trained YOLOv5 model on the
simulated dataset and fine-tuned model on the real-world dataset. Finally, an
adaptive fusion mechanism is proposed to further improve small object detection
performance. Experiment results demonstrate the effectiveness of
simulation-based transfer learning which leads to a 2.7% performance increase
on UAV object detection. Furthermore, with transfer learning and adaptive
fusion mechanism, 7.1% improvement is achieved compared to the original YOLO v5
model.",arxiv
http://arxiv.org/abs/1709.03339v3,2018-02-27T10:14:24Z,2017-09-11T11:39:47Z,Autonomous Quadrotor Landing using Deep Reinforcement Learning,"Landing an unmanned aerial vehicle (UAV) on a ground marker is an open
problem despite the effort of the research community. Previous attempts mostly
focused on the analysis of hand-crafted geometric features and the use of
external sensors in order to allow the vehicle to approach the land-pad. In
this article, we propose a method based on deep reinforcement learning that
only requires low-resolution images taken from a down-looking camera in order
to identify the position of the marker and land the UAV on it. The proposed
approach is based on a hierarchy of Deep Q-Networks (DQNs) used as high-level
control policy for the navigation toward the marker. We implemented different
technical solutions, such as the combination of vanilla and double DQNs, and a
partitioned buffer replay. Using domain randomization we trained the vehicle on
uniform textures and we tested it on a large variety of simulated and
real-world environments. The overall performance is comparable with a
state-of-the-art algorithm and human pilots.",arxiv
http://arxiv.org/abs/2104.14006v1,2021-04-28T20:24:10Z,2021-04-28T20:24:10Z,"EmergencyNet: Efficient Aerial Image Classification for Drone-Based
  Emergency Monitoring Using Atrous Convolutional Feature Fusion","Deep learning-based algorithms can provide state-of-the-art accuracy for
remote sensing technologies such as unmanned aerial vehicles (UAVs)/drones,
potentially enhancing their remote sensing capabilities for many emergency
response and disaster management applications. In particular, UAVs equipped
with camera sensors can operating in remote and difficult to access
disaster-stricken areas, analyze the image and alert in the presence of various
calamities such as collapsed buildings, flood, or fire in order to faster
mitigate their effects on the environment and on human population. However, the
integration of deep learning introduces heavy computational requirements,
preventing the deployment of such deep neural networks in many scenarios that
impose low-latency constraints on inference, in order to make mission-critical
decisions in real time. To this end, this article focuses on the efficient
aerial image classification from on-board a UAV for emergency
response/monitoring applications. Specifically, a dedicated Aerial Image
Database for Emergency Response applications is introduced and a comparative
analysis of existing approaches is performed. Through this analysis a
lightweight convolutional neural network architecture is proposed, referred to
as EmergencyNet, based on atrous convolutions to process multiresolution
features and capable of running efficiently on low-power embedded platforms
achieving upto 20x higher performance compared to existing models with minimal
memory requirements with less than 1% accuracy drop compared to
state-of-the-art models.",arxiv
http://arxiv.org/abs/2106.15045v1,2021-06-29T01:16:01Z,2021-06-29T01:16:01Z,"EVPropNet: Detecting Drones By Finding Propellers For Mid-Air Landing
  And Following","The rapid rise of accessibility of unmanned aerial vehicles or drones pose a
threat to general security and confidentiality. Most of the commercially
available or custom-built drones are multi-rotors and are comprised of multiple
propellers. Since these propellers rotate at a high-speed, they are generally
the fastest moving parts of an image and cannot be directly ""seen"" by a
classical camera without severe motion blur. We utilize a class of sensors that
are particularly suitable for such scenarios called event cameras, which have a
high temporal resolution, low-latency, and high dynamic range.
  In this paper, we model the geometry of a propeller and use it to generate
simulated events which are used to train a deep neural network called EVPropNet
to detect propellers from the data of an event camera. EVPropNet directly
transfers to the real world without any fine-tuning or retraining. We present
two applications of our network: (a) tracking and following an unmarked drone
and (b) landing on a near-hover drone. We successfully evaluate and demonstrate
the proposed approach in many real-world experiments with different propeller
shapes and sizes. Our network can detect propellers at a rate of 85.1% even
when 60% of the propeller is occluded and can run at upto 35Hz on a 2W power
budget. To our knowledge, this is the first deep learning-based solution for
detecting propellers (to detect drones). Finally, our applications also show an
impressive success rate of 92% and 90% for the tracking and landing tasks
respectively.",arxiv
http://arxiv.org/abs/2002.00831v1,2020-02-03T15:39:56Z,2020-02-03T15:39:56Z,An Actor-Critic-Based UAV-BSs Deployment Method for Dynamic Environments,"In this paper, the real-time deployment of unmanned aerial vehicles (UAVs) as
flying base stations (BSs) for optimizing the throughput of mobile users is
investigated for UAV networks. This problem is formulated as a time-varying
mixed-integer non-convex programming (MINP) problem, which is challenging to
find an optimal solution in a short time with conventional optimization
techniques. Hence, we propose an actor-critic-based (AC-based) deep
reinforcement learning (DRL) method to find near-optimal UAV positions at every
moment. In the proposed method, the process searching for the solution
iteratively at a particular moment is modeled as a Markov decision process
(MDP). To handle infinite state and action spaces and improve the robustness of
the decision process, two powerful neural networks (NNs) are configured to
evaluate the UAV position adjustments and make decisions, respectively.
Compared with the heuristic algorithm, sequential least-squares programming and
fixed UAVs methods, simulation results have shown that the proposed method
outperforms these three benchmarks in terms of the throughput at every moment
in UAV networks.",arxiv
http://arxiv.org/abs/2107.13869v2,2021-07-30T13:55:57Z,2021-07-29T10:11:36Z,"Autonomous UAV Base Stations for Next Generation Wireless Networks: A
  Deep Learning Approach","To address the ever-growing connectivity demands of wireless communications,
the adoption of ingenious solutions, such as Unmanned Aerial Vehicles (UAVs) as
mobile Base Stations (BSs), is imperative. In general, the location of a UAV
Base Station (UAV-BS) is determined by optimization algorithms, which have high
computationally complexities and place heavy demands on UAV resources. In this
paper, we show that a Convolutional Neural Network (CNN) model can be trained
to infer the location of a UAV-BS in real time. In so doing, we create a
framework to determine the UAV locations that considers the deployment of
Mobile Users (MUs) to generate labels by using the data obtained from an
optimization algorithm. Performance evaluations reveal that once the CNN model
is trained with the given labels and locations of MUs, the proposed approach is
capable of approximating the results given by the adopted optimization
algorithm with high fidelity, outperforming Reinforcement Learning (RL)-based
approaches. We also explore future research challenges and highlight key
issues.",arxiv
http://arxiv.org/abs/1909.02562v1,2019-09-05T13:21:22Z,2019-09-05T13:21:22Z,"TFCheck : A TensorFlow Library for Detecting Training Issues in Neural
  Network Programs","The increasing inclusion of Machine Learning (ML) models in safety critical
systems like autonomous cars have led to the development of multiple
model-based ML testing techniques. One common denominator of these testing
techniques is their assumption that training programs are adequate and
bug-free. These techniques only focus on assessing the performance of the
constructed model using manually labeled data or automatically generated data.
However, their assumptions about the training program are not always true as
training programs can contain inconsistencies and bugs. In this paper, we
examine training issues in ML programs and propose a catalog of verification
routines that can be used to detect the identified issues, automatically. We
implemented the routines in a Tensorflow-based library named TFCheck. Using
TFCheck, practitioners can detect the aforementioned issues automatically. To
assess the effectiveness of TFCheck, we conducted a case study with real-world,
mutants, and synthetic training programs. Results show that TFCheck can
successfully detect training issues in ML code implementations.",arxiv
http://arxiv.org/abs/1905.04166v1,2019-05-10T13:34:18Z,2019-05-10T13:34:18Z,"An Open Source and Open Hardware Deep Learning-powered Visual Navigation
  Engine for Autonomous Nano-UAVs","Nano-size unmanned aerial vehicles (UAVs), with few centimeters of diameter
and sub-10 Watts of total power budget, have so far been considered incapable
of running sophisticated visual-based autonomous navigation software without
external aid from base-stations, ad-hoc local positioning infrastructure, and
powerful external computation servers. In this work, we present what is, to the
best of our knowledge, the first 27g nano-UAV system able to run aboard an
end-to-end, closed-loop visual pipeline for autonomous navigation based on a
state-of-the-art deep-learning algorithm, built upon the open-source CrazyFlie
2.0 nano-quadrotor. Our visual navigation engine is enabled by the combination
of an ultra-low power computing device (the GAP8 system-on-chip) with a novel
methodology for the deployment of deep convolutional neural networks (CNNs). We
enable onboard real-time execution of a state-of-the-art deep CNN at up to
18Hz. Field experiments demonstrate that the system's high responsiveness
prevents collisions with unexpected dynamic obstacles up to a flight speed of
1.5m/s. In addition, we also demonstrate the capability of our visual
navigation engine of fully autonomous indoor navigation on a 113m previously
unseen path. To share our key findings with the embedded and robotics
communities and foster further developments in autonomous nano-UAVs, we
publicly release all our code, datasets, and trained networks.",arxiv
http://arxiv.org/abs/1912.00752v3,2020-07-15T14:08:12Z,2019-11-28T03:03:24Z,"Deep Learning for Optimal Deployment of UAVs with Visible Light
  Communications","In this paper, the problem of dynamical deployment of unmanned aerial
vehicles (UAVs) equipped with visible light communication (VLC) capabilities
for optimizing the energy efficiency of UAV-enabled networks is studied. In the
studied model, the UAVs can simultaneously provide communications and
illumination to service ground users. Since ambient illumination increases the
interference over VLC links while reducing the illumination threshold of the
UAVs, it is necessary to consider the illumination distribution of the target
area for UAV deployment optimization. This problem is formulated as an
optimization problem which jointly optimizes UAV deployment, user association,
and power efficiency while meeting the illumination and communication
requirements of users. To solve this problem, an algorithm that combines the
machine learning framework of gated recurrent units (GRUs) with convolutional
neural networks (CNNs) is proposed. Using GRUs and CNNs, the UAVs can model the
long-term historical illumination distribution and predict the future
illumination distribution. Given the prediction of illumination distribution,
the original nonconvex optimization problem can be divided into two
sub-problems and is then solved using a low-complexity, iterative algorithm.
Then, the proposed algorithm enables UAVs to determine the their deployment and
user association to minimize the total transmit power. Simulation results using
real data from the Earth observations group (EOG) at NOAA/NCEI show that the
proposed approach can achieve up to 68.9% reduction in total transmit power
compared to a conventional optimal UAV deployment that does not consider the
illumination distribution and user association.",arxiv
http://arxiv.org/abs/1907.12650v2,2020-01-18T19:33:53Z,2019-07-30T15:41:01Z,"Beyond Safety Drivers: Staffing a Teleoperations System for Autonomous
  Vehicles","Driverless vehicles promise a host of societal benefits including
dramatically improved safety, increased accessibility, greater productivity,
and higher quality of life. As this new technology approaches widespread
deployment, both industry and government are making provisions for
teleoperations systems, in which remote human agents provide assistance to
driverless vehicles. This assistance can involve real-time remote operation and
even ahead-of-time input via human-in-the-loop artificial intelligence systems.
In this paper, we address the problem of staffing such a remote support center.
Our analysis focuses on the tradeoffs between the total number of remote
agents, the reliability of the remote support system, and the resulting safety
of the driverless vehicles. By establishing a novel connection between queues
with large batch arrivals and storage processes, we determine the probability
of the system exceeding its service capacity. This connection drives our
staffing methodology. We also develop a numerical method to compute the exact
staffing level needed to achieve various performance measures. This moment
generating function based technique may be of independent interest, and our
overall staffing analysis may be of use in other applications that combine
human expertise and automated systems.",arxiv
http://arxiv.org/abs/2103.10873v1,2021-03-19T15:56:58Z,2021-03-19T15:56:58Z,"Fully Onboard AI-powered Human-Drone Pose Estimation on Ultra-low Power
  Autonomous Flying Nano-UAVs","Artificial intelligence-powered pocket-sized air robots have the potential to
revolutionize the Internet-of-Things ecosystem, acting as autonomous,
unobtrusive, and ubiquitous smart sensors. With a few cm$^{2}$ form-factor,
nano-sized unmanned aerial vehicles (UAVs) are the natural befit for indoor
human-drone interaction missions, as the pose estimation task we address in
this work. However, this scenario is challenged by the nano-UAVs' limited
payload and computational power that severely relegates the onboard brain to
the sub-100 mW microcontroller unit-class. Our work stands at the intersection
of the novel parallel ultra-low-power (PULP) architectural paradigm and our
general development methodology for deep neural network (DNN) visual pipelines,
i.e., covering from perception to control. Addressing the DNN model design,
from training and dataset augmentation to 8-bit quantization and deployment, we
demonstrate how a PULP-based processor, aboard a nano-UAV, is sufficient for
the real-time execution (up to 135 frame/s) of our novel DNN, called
PULP-Frontnet. We showcase how, scaling our model's memory and computational
requirement, we can significantly improve the onboard inference (top energy
efficiency of 0.43 mJ/frame) with no compromise in the quality-of-result vs. a
resource-unconstrained baseline (i.e., full-precision DNN). Field experiments
demonstrate a closed-loop top-notch autonomous navigation capability, with a
heavily resource-constrained 27-gram Crazyflie 2.1 nano-quadrotor. Compared
against the control performance achieved using an ideal sensing setup, onboard
relative pose inference yields excellent drone behavior in terms of median
absolute errors, such as positional (onboard: 41 cm, ideal: 26 cm) and angular
(onboard: 3.7$^{\circ}$, ideal: 4.1$^{\circ}$).",arxiv
http://arxiv.org/abs/2005.00336v2,2020-05-06T18:55:28Z,2020-04-03T22:46:34Z,"On-board Deep-learning-based Unmanned Aerial Vehicle Fault Cause
  Detection and Identification","With the increase in use of Unmanned Aerial Vehicles (UAVs)/drones, it is
important to detect and identify causes of failure in real time for proper
recovery from a potential crash-like scenario or post incident forensics
analysis. The cause of crash could be either a fault in the sensor/actuator
system, a physical damage/attack, or a cyber attack on the drone's software. In
this paper, we propose novel architectures based on deep Convolutional and Long
Short-Term Memory Neural Networks (CNNs and LSTMs) to detect (via Autoencoder)
and classify drone mis-operations based on sensor data. The proposed
architectures are able to learn high-level features automatically from the raw
sensor data and learn the spatial and temporal dynamics in the sensor data. We
validate the proposed deep-learning architectures via simulations and
experiments on a real drone. Empirical results show that our solution is able
to detect with over 90% accuracy and classify various types of drone
mis-operations (with about 99% accuracy (simulation data) and upto 88% accuracy
(experimental data)).",arxiv
http://arxiv.org/abs/2104.07246v1,2021-04-15T05:33:03Z,2021-04-15T05:33:03Z,"Human-in-the-Loop Deep Reinforcement Learning with Application to
  Autonomous Driving","Due to the limited smartness and abilities of machine intelligence, currently
autonomous vehicles are still unable to handle all kinds of situations and
completely replace drivers. Because humans exhibit strong robustness and
adaptability in complex driving scenarios, it is of great importance to
introduce humans into the training loop of artificial intelligence, leveraging
human intelligence to further advance machine learning algorithms. In this
study, a real-time human-guidance-based deep reinforcement learning (Hug-DRL)
method is developed for policy training of autonomous driving. Leveraging a
newly designed control transfer mechanism between human and automation, human
is able to intervene and correct the agent's unreasonable actions in real time
when necessary during the model training process. Based on this
human-in-the-loop guidance mechanism, an improved actor-critic architecture
with modified policy and value networks is developed. The fast convergence of
the proposed Hug-DRL allows real-time human guidance actions to be fused into
the agent's training loop, further improving the efficiency and performance of
deep reinforcement learning. The developed method is validated by
human-in-the-loop experiments with 40 subjects and compared with other
state-of-the-art learning approaches. The results suggest that the proposed
method can effectively enhance the training efficiency and performance of the
deep reinforcement learning algorithm under human guidance, without imposing
specific requirements on participant expertise and experience.",arxiv
http://arxiv.org/abs/2003.04816v1,2020-02-21T07:29:15Z,2020-02-21T07:29:15Z,"Data Freshness and Energy-Efficient UAV Navigation Optimization: A Deep
  Reinforcement Learning Approach","In this paper, we design a navigation policy for multiple unmanned aerial
vehicles (UAVs) where mobile base stations (BSs) are deployed to improve the
data freshness and connectivity to the Internet of Things (IoT) devices. First,
we formulate an energy-efficient trajectory optimization problem in which the
objective is to maximize the energy efficiency by optimizing the UAV-BS
trajectory policy. We also incorporate different contextual information such as
energy and age of information (AoI) constraints to ensure the data freshness at
the ground BS. Second, we propose an agile deep reinforcement learning with
experience replay model to solve the formulated problem concerning the
contextual constraints for the UAV-BS navigation. Moreover, the proposed
approach is well-suited for solving the problem, since the state space of the
problem is extremely large and finding the best trajectory policy with useful
contextual features is too complex for the UAV-BSs. By applying the proposed
trained model, an effective real-time trajectory policy for the UAV-BSs
captures the observable network states over time. Finally, the simulation
results illustrate the proposed approach is 3.6% and 3.13% more energy
efficient than those of the greedy and baseline deep Q Network (DQN)
approaches.",arxiv
http://arxiv.org/abs/2012.06992v1,2020-12-13T07:28:18Z,2020-12-13T07:28:18Z,"Edge Intelligence for Autonomous Driving in 6G Wireless System: Design
  Challenges and Solutions","In a level-5 autonomous driving system, the autonomous driving vehicles (AVs)
are expected to sense the surroundings via analyzing a large amount of data
captured by a variety of onboard sensors in near-real-time. As a result,
enormous computing costs will be introduced to the AVs for processing the tasks
with the deployed machine learning (ML) model, while the inference accuracy may
not be guaranteed. In this context, the advent of edge intelligence (EI) and
sixth-generation (6G) wireless networking are expected to pave the way to more
reliable and safer autonomous driving by providing multi-access edge computing
(MEC) together with ML to AVs in close proximity. To realize this goal, we
propose a two-tier EI-empowered autonomous driving framework. In the
autonomous-vehicles tier, the autonomous vehicles are deployed with the shallow
layers by splitting the trained deep neural network model. In the
edge-intelligence tier, an edge server is implemented with the remaining layers
(also deep layers) and an appropriately trained multi-task learning (MTL)
model. In particular, obtaining the optimal offloading strategy (including the
binary offloading decision and the computational resources allocation) can be
formulated as a mixed-integer nonlinear programming (MINLP) problem, which is
solved via MTL in near-real-time with high accuracy. On another note, an
edge-vehicle joint inference is proposed through neural network segmentation to
achieve efficient online inference with data privacy-preserving and less
communication delay. Experiments demonstrate the effectiveness of the proposed
framework, and open research topics are finally listed.",arxiv
http://arxiv.org/abs/2001.11610v1,2020-01-30T23:49:15Z,2020-01-30T23:49:15Z,"UAV Autonomous Localization using Macro-Features Matching with a CAD
  Model","Research in the field of autonomous Unmanned Aerial Vehicles (UAVs) has
significantly advanced in recent years, mainly due to their relevance in a
large variety of commercial, industrial, and military applications. However,
UAV navigation in GPS-denied environments continues to be a challenging problem
that has been tackled in recent research through sensor-based approaches. This
paper presents a novel offline, portable, real-time in-door UAV localization
technique that relies on macro-feature detection and matching. The proposed
system leverages the support of machine learning, traditional computer vision
techniques, and pre-existing knowledge of the environment. The main
contribution of this work is the real-time creation of a macro-feature
description vector from the UAV captured images which are simultaneously
matched with an offline pre-existing vector from a Computer-Aided Design (CAD)
model. This results in a quick UAV localization within the CAD model. The
effectiveness and accuracy of the proposed system were evaluated through
simulations and experimental prototype implementation. Final results reveal the
algorithm's low computational burden as well as its ease of deployment in
GPS-denied environments.",arxiv
http://arxiv.org/abs/2012.15717v2,2021-11-07T05:23:47Z,2020-12-31T17:15:09Z,"FGraDA: A Dataset and Benchmark for Fine-Grained Domain Adaptation in
  Machine Translation","Previous research for adapting a general neural machine translation (NMT)
model into a specific domain usually neglects the diversity in translation
within the same domain, which is a core problem for domain adaptation in
real-world scenarios. One representative of such challenging scenarios is to
deploy a translation system for a conference with a specific topic, e.g.,
global warming or coronavirus, where there are usually extremely less resources
due to the limited schedule. To motivate wider investigation in such a
scenario, we present a real-world fine-grained domain adaptation task in
machine translation (FGraDA). The FGraDA dataset consists of Chinese-English
translation task for four sub-domains of information technology: autonomous
vehicles, AI education, real-time networks, and smart phone. Each sub-domain is
equipped with a development set and test set for evaluation purposes. To be
closer to reality, FGraDA does not employ any in-domain bilingual training data
but provides bilingual dictionaries and wiki knowledge base, which can be
easier obtained within a short time. We benchmark the fine-grained domain
adaptation task and present in-depth analyses showing that there are still
challenging problems to further improve the performance with heterogeneous
resources.",arxiv
http://arxiv.org/abs/1810.09729v1,2018-10-23T08:51:54Z,2018-10-23T08:51:54Z,"Design Challenges of Multi-UAV Systems in Cyber-Physical Applications: A
  Comprehensive Survey, and Future Directions","Unmanned Aerial Vehicles (UAVs) have recently rapidly grown to facilitate a
wide range of innovative applications that can fundamentally change the way
cyber-physical systems (CPSs) are designed. CPSs are a modern generation of
systems with synergic cooperation between computational and physical potentials
that can interact with humans through several new mechanisms. The main
advantages of using UAVs in CPS application is their exceptional features,
including their mobility, dynamism, effortless deployment, adaptive altitude,
agility, adjustability, and effective appraisal of real-world functions anytime
and anywhere. Furthermore, from the technology perspective, UAVs are predicted
to be a vital element of the development of advanced CPSs. Therefore, in this
survey, we aim to pinpoint the most fundamental and important design challenges
of multi-UAV systems for CPS applications. We highlight key and versatile
aspects that span the coverage and tracking of targets and infrastructure
objects, energy-efficient navigation, and image analysis using machine learning
for fine-grained CPS applications. Key prototypes and testbeds are also
investigated to show how these practical technologies can facilitate CPS
applications. We present and propose state-of-the-art algorithms to address
design challenges with both quantitative and qualitative methods and map these
challenges with important CPS applications to draw insightful conclusions on
the challenges of each application. Finally, we summarize potential new
directions and ideas that could shape future research in these areas.",arxiv
http://arxiv.org/abs/1909.10914v1,2019-09-23T09:20:44Z,2019-09-23T09:20:44Z,Sensor-Augmented Neural Adaptive Bitrate Video Streaming on UAVs,"Recent advances in unmanned aerial vehicle (UAV) technology have
revolutionized a broad class of civil and military applications. However, the
designs of wireless technologies that enable real-time streaming of
high-definition video between UAVs and ground clients present a conundrum. Most
existing adaptive bitrate (ABR) algorithms are not optimized for the
air-to-ground links, which usually fluctuate dramatically due to the dynamic
flight states of the UAV. In this paper, we present SA-ABR, a new
sensor-augmented system that generates ABR video streaming algorithms with the
assistance of various kinds of inherent sensor data that are used to pilot
UAVs. By incorporating the inherent sensor data with network observations,
SA-ABR trains a deep reinforcement learning (DRL) model to extract salient
features from the flight state information and automatically learn an ABR
algorithm to adapt to the varying UAV channel capacity through the training
process. SA-ABR does not rely on any assumptions or models about UAV's flight
states or the environment, but instead, it makes decisions by exploiting
temporal properties of past throughput through the long short-term memory
(LSTM) to adapt itself to a wide range of highly dynamic environments. We have
implemented SA-ABR in a commercial UAV and evaluated it in the wild. We compare
SA-ABR with a variety of existing state-of-the-art ABR algorithms, and the
results show that our system outperforms the best known existing ABR algorithm
by 21.4% in terms of the average quality of experience (QoE) reward.",arxiv
http://arxiv.org/abs/2102.13253v1,2021-02-26T01:31:28Z,2021-02-26T01:31:28Z,"On the Visual-based Safe Landing of UAVs in Populated Areas: a Crucial
  Aspect for Urban Deployment","Autonomous landing of Unmanned Aerial Vehicles (UAVs) in crowded scenarios is
crucial for successful deployment of UAVs in populated areas, particularly in
emergency landing situations where the highest priority is to avoid hurting
people. In this work, a new visual-based algorithm for identifying Safe Landing
Zones (SLZ) in crowded scenarios is proposed, considering a camera mounted on
an UAV, where the people in the scene move with unknown dynamics. To do so, a
density map is generated for each image frame using a Deep Neural Network, from
where a binary occupancy map is obtained aiming to overestimate the people's
location for security reasons. Then, the occupancy map is projected to the
head's plane, and the SLZ candidates are obtained as circular regions in the
head's plane with a minimum security radius. Finally, to keep track of the SLZ
candidates, a multiple instance tracking algorithm is implemented using Kalman
Filters along with the Hungarian algorithm for data association. Several
scenarios were studied to prove the validity of the proposed strategy,
including public datasets and real uncontrolled scenarios with people moving in
public squares, taken from an UAV in flight. The study showed promising results
in the search of preventing the UAV from hurting people during emergency
landing.",arxiv
http://arxiv.org/abs/1809.04471v2,2018-10-19T16:09:37Z,2018-09-12T14:10:35Z,Learning structure-from-motion from motion,"This work is based on a questioning of the quality metrics used by deep
neural networks performing depth prediction from a single image, and then of
the usability of recently published works on unsupervised learning of depth
from videos. To overcome their limitations, we propose to learn in the same
unsupervised manner a depth map inference system from monocular videos that
takes a pair of images as input. This algorithm actually learns
structure-from-motion from motion, and not only structure from context
appearance. The scale factor issue is explicitly treated, and the absolute
depth map can be estimated from camera displacement magnitude, which can be
easily measured from cheap external sensors. Our solution is also much more
robust with respect to domain variation and adaptation via fine tuning, because
it does not rely entirely in depth from context. Two use cases are considered,
unstabilized moving camera videos, and stabilized ones. This choice is
motivated by the UAV (for Unmanned Aerial Vehicle) use case that generally
provides reliable orientation measurement. We provide a set of experiments
showing that, used in real conditions where only speed can be known, our
network outperforms competitors for most depth quality measures. Results are
given on the well known KITTI dataset, which provides robust stabilization for
our second use case, but also contains moving scenes which are very typical of
the in-car road context. We then present results on a synthetic dataset that we
believe to be more representative of typical UAV scenes. Lastly, we present two
domain adaptation use cases showing superior robustness of our method compared
to single view depth algorithms, which indicates that it is better suited for
highly variable visual contexts.",arxiv
http://arxiv.org/abs/2104.02108v1,2021-04-05T18:35:11Z,2021-04-05T18:35:11Z,Control of a Tail-Sitter VTOL UAV Based on Recurrent Neural Networks,"Tail-sitter vertical takeoff and landing (VTOL) unmanned aerial vehicles
(UAVs) have the capability of hovering and performing efficient level flight
with compact mechanical structures. We present a unified controller design for
such UAVs, based on recurrent neural networks. An advantage of this design
method is that the various flight modes (i.e., hovering, transition and level
flight) of a VTOL UAV are controlled in a unified manner, as opposed to
treating them separately and in the runtime switching one from another. The
proposed controller consists of an outer-loop position controller and an
inner-loop attitude controller. The inner-loop controller is composed of a
proportional attitude controller and a loop-shaping linear angular rate
controller. For the outer-loop controller, we propose a nonlinear solver to
compute the desired attitude and thrust, based on the UAV dynamics and an
aerodynamic model, in addition to a cascaded PID controller for the position
and velocity tracking. We employ a recurrent neural network (RNN) to
approximate the behavior of the nonlinear solver, which suffers from high
computational complexity. The proposed RNN has negligible approximation errors,
and can be implemented in real-time (e.g., 50 Hz). Moreover, the RNN generates
much smoother outputs than the nonlinear solver. We provide an analysis of the
stability and robustness of the overall closed-loop system. Simulation and
experiments are also presented to demonstrate the effectiveness of the proposed
method.",arxiv
http://arxiv.org/abs/2108.01884v1,2021-08-04T07:30:04Z,2021-08-04T07:30:04Z,"Adaptive Path Planning for UAV-based Multi-Resolution Semantic
  Segmentation","In this paper, we address the problem of adaptive path planning for accurate
semantic segmentation of terrain using unmanned aerial vehicles (UAVs). The
usage of UAVs for terrain monitoring and remote sensing is rapidly gaining
momentum due to their high mobility, low cost, and flexible deployment.
However, a key challenge is planning missions to maximize the value of acquired
data in large environments given flight time limitations. To address this, we
propose an online planning algorithm which adapts the UAV paths to obtain
high-resolution semantic segmentations necessary in areas on the terrain with
fine details as they are detected in incoming images. This enables us to
perform close inspections at low altitudes only where required, without wasting
energy on exhaustive mapping at maximum resolution. A key feature of our
approach is a new accuracy model for deep learning-based architectures that
captures the relationship between UAV altitude and semantic segmentation
accuracy. We evaluate our approach on the application of crop/weed segmentation
in precision agriculture using real-world field data.",arxiv
http://arxiv.org/abs/2006.15175v1,2020-06-26T19:06:32Z,2020-06-26T19:06:32Z,Application of Neuroevolution in Autonomous Cars,"With the onset of Electric vehicles, and them becoming more and more popular,
autonomous cars are the future in the travel/driving experience. The barrier to
reaching level 5 autonomy is the difficulty in the collection of data that
incorporates good driving habits and the lack thereof. The problem with current
implementations of self-driving cars is the need for massively large datasets
and the need to evaluate the driving in the dataset. We propose a system that
requires no data for its training. An evolutionary model would have the
capability to optimize itself towards the fitness function. We have implemented
Neuroevolution, a form of genetic algorithm, to train/evolve self-driving cars
in a simulated virtual environment with the help of Unreal Engine 4, which
utilizes Nvidia's PhysX Physics Engine to portray real-world vehicle dynamics
accurately. We were able to observe the serendipitous nature of evolution and
have exploited it to reach our optimal solution. We also demonstrate the ease
in generalizing attributes brought about by genetic algorithms and how they may
be used as a boilerplate upon which other machine learning techniques may be
used to improve the overall driving experience.",arxiv
http://arxiv.org/abs/2106.07299v1,2021-06-14T11:05:53Z,2021-06-14T11:05:53Z,"Dynamic Based Estimator for UAVs with Real-time Identification Using DNN
  and the Modified Relay Feedback Test","Control performance of Unmanned Aerial Vehicles (UAVs) is directly affected
by their ability to estimate their states accurately. With the increasing
popularity of autonomous UAV solutions in real world applications, it is
imperative to develop robust adaptive estimators that can ameliorate sensor
noises in low-cost UAVs. Utilizing the knowledge of UAV dynamics in estimation
can provide significant advantages, but remains challenging due to the complex
and expensive pre-flight experiments required to obtain UAV dynamic parameters.
In this paper, we propose two decoupled dynamic model based Extended Kalman
Filters for UAVs, that provide high rate estimates for position, and velocity
of rotational and translational states, as well as filtered inertial
acceleration. The dynamic model parameters are estimated online using the Deep
Neural Network and Modified Relay Feedback Test (DNN-MRFT) framework, without
requiring any prior knowledge of the UAV physical parameters. The designed
filters with real-time identified process model parameters are tested
experimentally and showed two advantages. Firstly, smooth and lag-free
estimates of the UAV rotational speed and inertial acceleration are obtained,
and used to improve the closed loop system performance, reducing the controller
action by over 6 %. Secondly, the proposed approach enabled the UAV to track
aggressive trajectories with low rate position measurements, a task usually
infeasible under those conditions. The experimental data shows that we achieved
estimation performance matching other methods that requires full knowledge of
the UAV parameters.",arxiv
http://arxiv.org/abs/2011.01840v1,2020-11-03T16:50:37Z,2020-11-03T16:50:37Z,"Distributional Reinforcement Learning for mmWave Communications with
  Intelligent Reflectors on a UAV","In this paper, a novel communication framework that uses an unmanned aerial
vehicle (UAV)-carried intelligent reflector (IR) is proposed to enhance
multi-user downlink transmissions over millimeter wave (mmWave) frequencies. In
order to maximize the downlink sum-rate, the optimal precoding matrix (at the
base station) and reflection coefficient (at the IR) are jointly derived. Next,
to address the uncertainty of mmWave channels and maintain line-of-sight links
in a real-time manner, a distributional reinforcement learning approach, based
on quantile regression optimization, is proposed to learn the propagation
environment of mmWave communications, and, then, optimize the location of the
UAV-IR so as to maximize the long-term downlink communication capacity.
Simulation results show that the proposed learning-based deployment of the
UAV-IR yields a significant advantage, compared to a non-learning UAV-IR, a
static IR, and a direct transmission schemes, in terms of the average data rate
and the achievable line-of-sight probability of downlink mmWave communications.",arxiv
http://arxiv.org/abs/1808.00259v1,2018-08-01T10:50:47Z,2018-08-01T10:50:47Z,Drone Detection Using Depth Maps,"Obstacle avoidance is a key feature for safe Unmanned Aerial Vehicle (UAV)
navigation. While solutions have been proposed for static obstacle avoidance,
systems enabling avoidance of dynamic objects, such as drones, are hard to
implement due to the detection range and field-of-view (FOV) requirements, as
well as the constraints for integrating such systems on-board small UAVs. In
this work, a dataset of 6k synthetic depth maps of drones has been generated
and used to train a state-of-the-art deep learning-based drone detection model.
While many sensing technologies can only provide relative altitude and azimuth
of an obstacle, our depth map-based approach enables full 3D localization of
the obstacle. This is extremely useful for collision avoidance, as 3D
localization of detected drones is key to perform efficient collision-free path
planning. The proposed detection technique has been validated in several real
depth map sequences, with multiple types of drones flying at up to 2 m/s,
achieving an average precision of 98.7%, an average recall of 74.7% and a
record detection range of 9.5 meters.",arxiv
http://arxiv.org/abs/1911.03887v2,2021-02-13T15:42:03Z,2019-11-10T10:24:04Z,"Deep Reinforcement Learning Based Dynamic Trajectory Control for
  UAV-assisted Mobile Edge Computing","In this paper, we consider a platform of flying mobile edge computing
(F-MEC), where unmanned aerial vehicles (UAVs) serve as equipment providing
computation resource, and they enable task offloading from user equipment (UE).
We aim to minimize energy consumption of all the UEs via optimizing the user
association, resource allocation and the trajectory of UAVs. To this end, we
first propose a Convex optimizAtion based Trajectory control algorithm (CAT),
which solves the problem in an iterative way by using block coordinate descent
(BCD) method. Then, to make the real-time decision while taking into account
the dynamics of the environment (i.e., UAV may take off from different
locations), we propose a deep Reinforcement leArning based Trajectory control
algorithm (RAT). In RAT, we apply the Prioritized Experience Replay (PER) to
improve the convergence of the training procedure. Different from the convex
optimization based algorithm which may be susceptible to the initial points and
requires iterations, RAT can be adapted to any taking off points of the UAVs
and can obtain the solution more rapidly than CAT once training process has
been completed. Simulation results show that the proposed CAT and RAT achieve
the similar performance and both outperform traditional algorithms.",arxiv
http://arxiv.org/abs/1905.11299v1,2019-05-27T15:32:59Z,2019-05-27T15:32:59Z,"ImgSensingNet: UAV Vision Guided Aerial-Ground Air Quality Sensing
  System","Given the increasingly serious air pollution problem, the monitoring of air
quality index (AQI) in urban areas has drawn considerable attention. This paper
presents ImgSensingNet, a vision guided aerial-ground sensing system, for
fine-grained air quality monitoring and forecasting using the fusion of haze
images taken by the unmanned-aerial-vehicle (UAV) and the AQI data collected by
an on-ground three-dimensional (3D) wireless sensor network (WSN).
Specifically, ImgSensingNet first leverages the computer vision technique to
tell the AQI scale in different regions from the taken haze images, where
haze-relevant features and a deep convolutional neural network (CNN) are
designed for direct learning between haze images and corresponding AQI scale.
Based on the learnt AQI scale, ImgSensingNet determines whether to wake up
on-ground wireless sensors for small-scale AQI monitoring and inference, which
can greatly reduce the energy consumption of the system. An entropy-based model
is employed for accurate real-time AQI inference at unmeasured locations and
future air quality distribution forecasting. We implement and evaluate
ImgSensingNet on two university campuses since Feb. 2018, and has collected
17,630 photos and 2.6 millions of AQI data samples. Experimental results
confirm that ImgSensingNet can achieve higher inference accuracy while greatly
reduce the energy consumption, compared to state-of-the-art AQI monitoring
approaches.",arxiv
http://arxiv.org/abs/2111.06123v1,2021-11-11T10:01:01Z,2021-11-11T10:01:01Z,"Spatio-Temporal Scene-Graph Embedding for Autonomous Vehicle Collision
  Prediction","In autonomous vehicles (AVs), early warning systems rely on collision
prediction to ensure occupant safety. However, state-of-the-art methods using
deep convolutional networks either fail at modeling collisions or are too
expensive/slow, making them less suitable for deployment on AV edge hardware.
To address these limitations, we propose sg2vec, a spatio-temporal scene-graph
embedding methodology that uses Graph Neural Network (GNN) and Long Short-Term
Memory (LSTM) layers to predict future collisions via visual scene perception.
We demonstrate that sg2vec predicts collisions 8.11% more accurately and 39.07%
earlier than the state-of-the-art method on synthesized datasets, and 29.47%
more accurately on a challenging real-world collision dataset. We also show
that sg2vec is better than the state-of-the-art at transferring knowledge from
synthetic datasets to real-world driving datasets. Finally, we demonstrate that
sg2vec performs inference 9.3x faster with an 88.0% smaller model, 32.4% less
power, and 92.8% less energy than the state-of-the-art method on the
industry-standard Nvidia DRIVE PX 2 platform, making it more suitable for
implementation on the edge.",arxiv
http://arxiv.org/abs/1803.00680v2,2019-03-17T01:34:35Z,2018-03-02T01:34:06Z,"A Tutorial on UAVs for Wireless Networks: Applications, Challenges, and
  Open Problems","The use of flying platforms such as unmanned aerial vehicles (UAVs),
popularly known as drones, is rapidly growing. In particular, with their
inherent attributes such as mobility, flexibility, and adaptive altitude, UAVs
admit several key potential applications in wireless systems. On the one hand,
UAVs can be used as aerial base stations to enhance coverage, capacity,
reliability, and energy efficiency of wireless networks. On the other hand,
UAVs can operate as flying mobile terminals within a cellular network. Such
cellular-connected UAVs can enable several applications ranging from real-time
video streaming to item delivery. In this paper, a comprehensive tutorial on
the potential benefits and applications of UAVs in wireless communications is
presented. Moreover, the important challenges and the fundamental tradeoffs in
UAV-enabled wireless networks are thoroughly investigated. In particular, the
key UAV challenges such as three-dimensional deployment, performance analysis,
channel modeling, and energy efficiency are explored along with representative
results. Then, open problems and potential research directions pertaining to
UAV communications are introduced. Finally, various analytical frameworks and
mathematical tools such as optimization theory, machine learning, stochastic
geometry, transport theory, and game theory are described. The use of such
tools for addressing unique UAV problems is also presented. In a nutshell, this
tutorial provides key guidelines on how to analyze, optimize, and design
UAV-based wireless communication systems.",arxiv
http://arxiv.org/abs/1812.01803v3,2019-04-06T04:46:52Z,2018-12-05T03:31:02Z,"ECC: Platform-Independent Energy-Constrained Deep Neural Network
  Compression via a Bilinear Regression Model","Many DNN-enabled vision applications constantly operate under severe energy
constraints such as unmanned aerial vehicles, Augmented Reality headsets, and
smartphones. Designing DNNs that can meet a stringent energy budget is becoming
increasingly important. This paper proposes ECC, a framework that compresses
DNNs to meet a given energy constraint while minimizing accuracy loss. The key
idea of ECC is to model the DNN energy consumption via a novel bilinear
regression function. The energy estimate model allows us to formulate DNN
compression as a constrained optimization that minimizes the DNN loss function
over the energy constraint. The optimization problem, however, has nontrivial
constraints. Therefore, existing deep learning solvers do not apply directly.
We propose an optimization algorithm that combines the essence of the
Alternating Direction Method of Multipliers (ADMM) framework with
gradient-based learning algorithms. The algorithm decomposes the original
constrained optimization into several subproblems that are solved iteratively
and efficiently. ECC is also portable across different hardware platforms
without requiring hardware knowledge. Experiments show that ECC achieves higher
accuracy under the same or lower energy budget compared to state-of-the-art
resource-constrained DNN compression techniques.",arxiv
http://arxiv.org/abs/2107.08325v1,2021-07-18T00:00:48Z,2021-07-18T00:00:48Z,"Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement
  Learning","Autonomous car racing is a challenging task in the robotic control area.
Traditional modular methods require accurate mapping, localization and
planning, which makes them computationally inefficient and sensitive to
environmental changes. Recently, deep-learning-based end-to-end systems have
shown promising results for autonomous driving/racing. However, they are
commonly implemented by supervised imitation learning (IL), which suffers from
the distribution mismatch problem, or by reinforcement learning (RL), which
requires a huge amount of risky interaction data. In this work, we present a
general deep imitative reinforcement learning approach (DIRL), which
successfully achieves agile autonomous racing using visual inputs. The driving
knowledge is acquired from both IL and model-based RL, where the agent can
learn from human teachers as well as perform self-improvement by safely
interacting with an offline world model. We validate our algorithm both in a
high-fidelity driving simulation and on a real-world 1/20-scale RC-car with
limited onboard computation. The evaluation results demonstrate that our method
outperforms previous IL and RL methods in terms of sample efficiency and task
performance. Demonstration videos are available at
https://caipeide.github.io/autorace-dirl/",arxiv
http://arxiv.org/abs/2106.05082v2,2021-06-11T04:14:53Z,2021-06-09T14:01:34Z,Agile wide-field imaging with selective high resolution,"Wide-field and high-resolution (HR) imaging is essential for various
applications such as aviation reconnaissance, topographic mapping and safety
monitoring. The existing techniques require a large-scale detector array to
capture HR images of the whole field, resulting in high complexity and heavy
cost. In this work, we report an agile wide-field imaging framework with
selective high resolution that requires only two detectors. It builds on the
statistical sparsity prior of natural scenes that the important targets locate
only at small regions of interests (ROI), instead of the whole field. Under
this assumption, we use a short-focal camera to image wide field with a certain
low resolution, and use a long-focal camera to acquire the HR images of ROI. To
automatically locate ROI in the wide field in real time, we propose an
efficient deep-learning based multiscale registration method that is robust and
blind to the large setting differences (focal, white balance, etc) between the
two cameras. Using the registered location, the long-focal camera mounted on a
gimbal enables real-time tracking of the ROI for continuous HR imaging. We
demonstrated the novel imaging framework by building a proof-of-concept setup
with only 1181 gram weight, and assembled it on an unmanned aerial vehicle for
air-to-ground monitoring. Experiments show that the setup maintains
120$^{\circ}$ wide field-of-view (FOV) with selective 0.45$mrad$ instantaneous
FOV.",arxiv
http://arxiv.org/abs/2105.07209v1,2021-05-15T12:01:16Z,2021-05-15T12:01:16Z,Aerial-PASS: Panoramic Annular Scene Segmentation in Drone Videos,"Aerial pixel-wise scene perception of the surrounding environment is an
important task for UAVs (Unmanned Aerial Vehicles). Previous research works
mainly adopt conventional pinhole cameras or fisheye cameras as the imaging
device. However, these imaging systems cannot achieve large Field of View
(FoV), small size, and lightweight at the same time. To this end, we design a
UAV system with a Panoramic Annular Lens (PAL), which has the characteristics
of small size, low weight, and a 360-degree annular FoV. A lightweight
panoramic annular semantic segmentation neural network model is designed to
achieve high-accuracy and real-time scene parsing. In addition, we present the
first drone-perspective panoramic scene segmentation dataset Aerial-PASS, with
annotated labels of track, field, and others. A comprehensive variety of
experiments shows that the designed system performs satisfactorily in aerial
panoramic scene parsing. In particular, our proposed model strikes an excellent
trade-off between segmentation performance and inference speed suitable,
validated on both public street-scene and our established aerial-scene
datasets.",arxiv
http://arxiv.org/abs/2106.03459v2,2021-10-11T11:00:04Z,2021-06-07T09:40:23Z,"Systematic Online Tuning of Multirotor UAVs for Accurate Trajectory
  Tracking Under Wind Disturbances and In-Flight Dynamics Changes","The demand for accurate and fast trajectory tracking for multirotor Unmanned
Aerial Vehicles (UAVs) have grown recently due to advances in UAV avionics
technology and application domains. In many applications, the multirotor UAV is
required to accurately perform aggressive maneuvers in challenging scenarios
like the presence of external wind disturbances or in-flight payload changes.
In this paper, we propose a systematic controller tuning approach based on
identification results obtained by a recently developed Deep Neural Networks
with the Modified Relay Feedback Test (DNN-MRFT) algorithm. We formulate a
linear equivalent representation suitable for DNN-MRFT using feedback
linearization. This representation enables the analytical investigation of
different controller structures and tuning settings, and captures the
non-linearity trends of the system. With this approach, the trade-off between
performance and robustness in design was made possible which is convenient for
the design of controllers of UAVs operating in uncertain environments. We
demonstrate that our approach is adaptive and robust through a set of
experiments, where accurate trajectory tracking is maintained despite
significant changes to the UAV aerodynamic characteristics and the application
of wind disturbance. Due to the model-based system design, it was possible to
obtain low discrepancy between simulation and experimental results which is
beneficial for potential use of the proposed approach for real-time model-based
planning and fault detection tasks. We obtained RMSE of $3.59 \; cm$ when
tracking aggressive trajectories in the presence of strong wind, which is on
par with state-of-the-art.",arxiv
http://arxiv.org/abs/2107.01581v1,2021-07-04T09:37:45Z,2021-07-04T09:37:45Z,"Unified Identification and Tuning Approach Using Deep Neural Networks
  For Visual Servoing Applications","Vision based control of Unmanned Aerial Vehicles (UAVs) has been adopted by a
wide range of applications due to the availability of low-cost on-board sensors
and computers. Tuning such systems to work properly requires extensive domain
specific experience which limits the growth of emerging applications. Moreover,
obtaining performance limits of UAV based visual servoing with the current
state-of-the-art is not possible due to the complexity of the models used. In
this paper, we present a systematic approach for real-time identification and
tuning of visual servoing systems based on a novel robustified version of the
recent deep neural networks with the modified relay feedback test (DNN-MRFT)
approach. The proposed robust DNN-MRFT algorithm can be used with a multitude
of vision sensors and estimation algorithms despite the high levels of sensor's
noise. Sensitivity of MRFT to perturbations is investigated and its effect on
identification and tuning performance is analyzed. DNN-MRFT was able to detect
performance changes due to the use of slower vision sensors, or due to the
integration of accelerometer measurements. Experimental identification results
were closely matching simulation results, which can be used to explain system
behaviour and anticipate the closed loop performance limits given a certain
hardware and software setup. Finally, we demonstrate the capability of the
DNN-MRFT tuned visual servoing systems to reject external disturbances. Some
advantages of the suggested robust identification approach compared to existing
visual servoing design approaches are presented.",arxiv
http://arxiv.org/abs/2001.07769v3,2020-02-16T22:19:32Z,2020-01-21T20:41:27Z,"Massif: Interactive Interpretation of Adversarial Attacks on Deep
  Learning","Deep neural networks (DNNs) are increasingly powering high-stakes
applications such as autonomous cars and healthcare; however, DNNs are often
treated as ""black boxes"" in such applications. Recent research has also
revealed that DNNs are highly vulnerable to adversarial attacks, raising
serious concerns over deploying DNNs in the real world. To overcome these
deficiencies, we are developing Massif, an interactive tool for deciphering
adversarial attacks. Massif identifies and interactively visualizes neurons and
their connections inside a DNN that are strongly activated or suppressed by an
adversarial attack. Massif provides both a high-level, interpretable overview
of the effect of an attack on a DNN, and a low-level, detailed description of
the affected neurons. These tightly coupled views in Massif help people better
understand which input features are most vulnerable or important for correct
predictions.",arxiv
http://arxiv.org/abs/1610.01585v1,2016-10-05T19:41:12Z,2016-10-05T19:41:12Z,"Caching in the Sky: Proactive Deployment of Cache-Enabled Unmanned
  Aerial Vehicles for Optimized Quality-of-Experience","In this paper, the problem of proactive deployment of cache-enabled unmanned
aerial vehicles (UAVs) for optimizing the quality-of-experience (QoE) of
wireless devices in a cloud radio access network (CRAN) is studied. In the
considered model, the network can leverage human-centric information such as
users' visited locations, requested contents, gender, job, and device type to
predict the content request distribution and mobility pattern of each user.
Then, given these behavior predictions, the proposed approach seeks to find the
user-UAV associations, the optimal UAVs' locations, and the contents to cache
at UAVs. This problem is formulated as an optimization problem whose goal is to
maximize the users' QoE while minimizing the transmit power used by the UAVs.
To solve this problem, a novel algorithm based on the machine learning
framework of conceptor-based echo state networks (ESNs) is proposed. Using
ESNs, the network can effectively predict each user's content request
distribution and its mobility pattern when limited information on the states of
users and the network is available. Based on the predictions of the users'
content request distribution and their mobility patterns, we derive the optimal
user-UAV association, optimal locations of the UAVs as well as the content to
cache at UAVs. Simulation results using real pedestrian mobility patterns from
BUPT and actual content transmission data from Youku show that the proposed
algorithm can yield 40% and 61% gains, respectively, in terms of the average
transmit power and the percentage of the users with satisfied QoE compared to a
benchmark algorithm without caching and a benchmark solution without UAVs.",arxiv
http://arxiv.org/abs/2007.05828v1,2020-07-11T18:41:47Z,2020-07-11T18:41:47Z,Understanding Object Detection Through An Adversarial Lens,"Deep neural networks based object detection models have revolutionized
computer vision and fueled the development of a wide range of visual
recognition applications. However, recent studies have revealed that deep
object detectors can be compromised under adversarial attacks, causing a victim
detector to detect no object, fake objects, or mislabeled objects. With object
detection being used pervasively in many security-critical applications, such
as autonomous vehicles and smart cities, we argue that a holistic approach for
an in-depth understanding of adversarial attacks and vulnerabilities of deep
object detection systems is of utmost importance for the research community to
develop robust defense mechanisms. This paper presents a framework for
analyzing and evaluating vulnerabilities of the state-of-the-art object
detectors under an adversarial lens, aiming to analyze and demystify the attack
strategies, adverse effects, and costs, as well as the cross-model and
cross-resolution transferability of attacks. Using a set of quantitative
metrics, extensive experiments are performed on six representative deep object
detectors from three popular families (YOLOv3, SSD, and Faster R-CNN) with two
benchmark datasets (PASCAL VOC and MS COCO). We demonstrate that the proposed
framework can serve as a methodical benchmark for analyzing adversarial
behaviors and risks in real-time object detection systems. We conjecture that
this framework can also serve as a tool to assess the security risks and the
adversarial robustness of deep object detectors to be deployed in real-world
applications.",arxiv
http://arxiv.org/abs/2110.07742v1,2021-10-14T21:53:03Z,2021-10-14T21:53:03Z,"Beyond Classification: Directly Training Spiking Neural Networks for
  Semantic Segmentation","Spiking Neural Networks (SNNs) have recently emerged as the low-power
alternative to Artificial Neural Networks (ANNs) because of their sparse,
asynchronous, and binary event-driven processing. Due to their energy
efficiency, SNNs have a high possibility of being deployed for real-world,
resource-constrained systems such as autonomous vehicles and drones. However,
owing to their non-differentiable and complex neuronal dynamics, most previous
SNN optimization methods have been limited to image recognition. In this paper,
we explore the SNN applications beyond classification and present semantic
segmentation networks configured with spiking neurons. Specifically, we first
investigate two representative SNN optimization techniques for recognition
tasks (i.e., ANN-SNN conversion and surrogate gradient learning) on semantic
segmentation datasets. We observe that, when converted from ANNs, SNNs suffer
from high latency and low performance due to the spatial variance of features.
Therefore, we directly train networks with surrogate gradient learning,
resulting in lower latency and higher performance than ANN-SNN conversion.
Moreover, we redesign two fundamental ANN segmentation architectures (i.e.,
Fully Convolutional Networks and DeepLab) for the SNN domain. We conduct
experiments on two public semantic segmentation benchmarks including the PASCAL
VOC2012 dataset and the DDD17 event-based dataset. In addition to showing the
feasibility of SNNs for semantic segmentation, we show that SNNs can be more
robust and energy-efficient compared to their ANN counterparts in this domain.",arxiv
http://arxiv.org/abs/1909.07554v1,2019-09-17T02:22:09Z,2019-09-17T02:22:09Z,"Gated Recurrent Units Learning for Optimal Deployment of Visible Light
  Communications Enabled UAVs","In this paper, the problem of optimizing the deployment of unmanned aerial
vehicles (UAVs) equipped with visible light communication (VLC) capabilities is
studied. In the studied model, the UAVs can simultaneously provide
communications and illumination to service ground users. Ambient illumination
increases the interference over VLC links while reducing the illumination
threshold of the UAVs. Therefore, it is necessary to consider the illumination
distribution of the target area for UAV deployment optimization. This problem
is formulated as an optimization problem whose goal is to minimize the total
transmit power while meeting the illumination and communication requirements of
users. To solve this problem, an algorithm based on the machine learning
framework of gated recurrent units (GRUs) is proposed. Using GRUs, the UAVs can
model the long-term historical illumination distribution and predict the future
illumination distribution. In order to reduce the complexity of the prediction
algorithm while accurately predicting the illumination distribution, a Gaussian
mixture model (GMM) is used to fit the illumination distribution of the target
area at each time slot. Based on the predicted illumination distribution, the
optimization problem is proved to be a convex optimization problem that can be
solved by using duality. Simulations using real data from the Earth
observations group (EOG) at NOAA/NCEI show that the proposed approach can
achieve up to 22.1% reduction in transmit power compared to a conventional
optimal UAV deployment that does not consider the illumination distribution.
The results also show that UAVs must hover at areas having strong illumination,
thus providing useful guidelines on the deployment of VLC-enabled UAVs.",arxiv
http://arxiv.org/abs/2003.00946v1,2020-03-02T14:48:29Z,2020-03-02T14:48:29Z,"A Self-Supervised Learning Approach to Rapid Path Planning for Car-Like
  Vehicles Maneuvering in Urban Environment","An efficient path planner for autonomous car-like vehicles should handle the
strong kinematic constraints, particularly in confined spaces commonly
encountered while maneuvering in city traffic, and should enable rapid
planning, as the city traffic scenarios are highly dynamic. State-of-the-art
planning algorithms handle such difficult cases at high computational cost,
often yielding non-deterministic results. However, feasible local paths can be
quickly generated leveraging the past planning experience gained in the same or
similar environment. While learning through supervised training is problematic
for real traffic scenarios, we introduce in this paper a novel neural
network-based method for path planning, which employs a gradient-based
self-supervised learning algorithm to predict feasible paths. This approach
strongly exploits the experience gained in the past and rapidly yields feasible
maneuver plans for car-like vehicles with limited steering-angle. The
effectiveness of such an approach has been confirmed by computational
experiments.",arxiv
http://arxiv.org/abs/2105.14052v1,2021-05-28T18:37:12Z,2021-05-28T18:37:12Z,"Targeted Deep Learning: Framework, Methods, and Applications","Deep learning systems are typically designed to perform for a wide range of
test inputs. For example, deep learning systems in autonomous cars are supposed
to deal with traffic situations for which they were not specifically trained.
In general, the ability to cope with a broad spectrum of unseen test inputs is
called generalization. Generalization is definitely important in applications
where the possible test inputs are known but plentiful or simply unknown, but
there are also cases where the possible inputs are few and unlabeled but known
beforehand. For example, medicine is currently interested in targeting
treatments to individual patients; the number of patients at any given time is
usually small (typically one), their diagnoses/responses/... are still unknown,
but their general characteristics (such as genome information, protein levels
in the blood, and so forth) are known before the treatment. We propose to call
deep learning in such applications targeted deep learning. In this paper, we
introduce a framework for targeted deep learning, and we devise and test an
approach for adapting standard pipelines to the requirements of targeted deep
learning. The approach is very general yet easy to use: it can be implemented
as a simple data-preprocessing step. We demonstrate on a variety of real-world
data that our approach can indeed render standard deep learning faster and more
accurate when the test inputs are known beforehand.",arxiv
http://arxiv.org/abs/2107.01784v1,2021-07-05T04:34:51Z,2021-07-05T04:34:51Z,"Learning a Model for Inferring a Spatial Road Lane Network Graph using
  Self-Supervision","Interconnected road lanes are a central concept for navigating urban roads.
Currently, most autonomous vehicles rely on preconstructed lane maps as
designing an algorithmic model is difficult. However, the generation and
maintenance of such maps is costly and hinders large-scale adoption of
autonomous vehicle technology. This paper presents the first self-supervised
learning method to train a model to infer a spatially grounded lane-level road
network graph based on a dense segmented representation of the road scene
generated from onboard sensors. A formal road lane network model is presented
and proves that any structured road scene can be represented by a directed
acyclic graph of at most depth three while retaining the notion of intersection
regions, and that this is the most compressed representation. The formal model
is implemented by a hybrid neural and search-based model, utilizing a novel
barrier function loss formulation for robust learning from partial labels.
Experiments are conducted for all common road intersection layouts. Results
show that the model can generalize to new road layouts, unlike previous
approaches, demonstrating its potential for real-world application as a
practical learning-based lane-level map generator.",arxiv
http://arxiv.org/abs/2005.07424v1,2020-05-15T09:05:17Z,2020-05-15T09:05:17Z,"Exploring the Capabilities and Limits of 3D Monocular Object Detection
  -- A Study on Simulation and Real World Data","3D object detection based on monocular camera data is a key enabler for
autonomous driving. The task however, is ill-posed due to lack of depth
information in 2D images. Recent deep learning methods show promising results
to recover depth information from single images by learning priors about the
environment. Several competing strategies tackle this problem. In addition to
the network design, the major difference of these competing approaches lies in
using a supervised or self-supervised optimization loss function, which require
different data and ground truth information. In this paper, we evaluate the
performance of a 3D object detection pipeline which is parameterizable with
different depth estimation configurations. We implement a simple distance
calculation approach based on camera intrinsics and 2D bounding box size, a
self-supervised, and a supervised learning approach for depth estimation.
  Ground truth depth information cannot be recorded reliable in real world
scenarios. This shifts our training focus to simulation data. In simulation,
labeling and ground truth generation can be automatized. We evaluate the
detection pipeline on simulator data and a real world sequence from an
autonomous vehicle on a race track. The benefit of simulation training to real
world application is investigated. Advantages and drawbacks of the different
depth estimation strategies are discussed.",arxiv
http://arxiv.org/abs/2107.14573v1,2021-07-30T12:11:31Z,2021-07-30T12:11:31Z,Neural Network Based Model Predictive Control for an Autonomous Vehicle,"We study learning based controllers as a replacement for model predictive
controllers (MPC) for the control of autonomous vehicles. We concentrate for
the experiments on the simple yet representative bicycle model. We compare
training by supervised learning and by reinforcement learning. We also discuss
the neural net architectures so as to obtain small nets with the best
performances. This work aims at producing controllers that can both be embedded
on real-time platforms and amenable to verification by formal methods
techniques.",arxiv
http://arxiv.org/abs/1907.08985v2,2020-02-08T22:59:03Z,2019-07-21T15:16:12Z,"Achieving Super-Linear Speedup across Multi-FPGA for Real-Time DNN
  Inference","Real-time Deep Neural Network (DNN) inference with low-latency requirement
has become increasingly important for numerous applications in both cloud
computing (e.g., Apple's Siri) and edge computing (e.g., Google/Waymo's
driverless car). FPGA-based DNN accelerators have demonstrated both superior
flexibility and performance; in addition, for real-time inference with low
batch size, FPGA is expected to achieve further performance improvement.
However, the performance gain from the single-FPGA design is obstructed by the
limited on-chip resource. In this paper, we employ multiple FPGAs to
cooperatively run DNNs with the objective of achieving super-linear speed-up
against single-FPGA design. In implementing such systems, we found two barriers
that hinder us from achieving the design goal: (1) the lack of a clear
partition scheme for each DNN layer to fully exploit parallelism, and (2) the
insufficient bandwidth between the off-chip memory and the accelerator due to
the growing size of DNNs. To tackle these issues, we propose a general
framework, ""Super-LIP"", which can support different kinds of DNNs. In this
paper, we take Convolutional Neural Network (CNN) as a vehicle to illustrate
Super-LIP. We first formulate an accurate system-level model to support the
exploration of best partition schemes. Then, we develop a novel design
methodology to effectively alleviate the heavy loads on memory bandwidth by
moving traffic from memory bus to inter-FPGA links. We implement Super-LIP
based on ZCU102 FPGA boards. Results demonstrate that Super-LIP with 2 FPGAs
can achieve 3.48x speedup, compared to the state-of-the-art single-FPGA design.
What is more, as the number of FPGAs scales up, the system latency can be
further reduced while maintaining high energy efficiency.",arxiv
http://arxiv.org/abs/2101.02780v1,2021-01-07T22:01:30Z,2021-01-07T22:01:30Z,"SHARKS: Smart Hacking Approaches for RisK Scanning in Internet-of-Things
  and Cyber-Physical Systems based on Machine Learning","Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are
increasingly being deployed across multiple functionalities, ranging from
healthcare devices and wearables to critical infrastructures, e.g., nuclear
power plants, autonomous vehicles, smart cities, and smart homes. These devices
are inherently not secure across their comprehensive software, hardware, and
network stacks, thus presenting a large attack surface that can be exploited by
hackers. In this article, we present an innovative technique for detecting
unknown system vulnerabilities, managing these vulnerabilities, and improving
incident response when such vulnerabilities are exploited. The novelty of this
approach lies in extracting intelligence from known real-world CPS/IoT attacks,
representing them in the form of regular expressions, and employing machine
learning (ML) techniques on this ensemble of regular expressions to generate
new attack vectors and security vulnerabilities. Our results show that 10 new
attack vectors and 122 new vulnerability exploits can be successfully generated
that have the potential to exploit a CPS or an IoT ecosystem. The ML
methodology achieves an accuracy of 97.4% and enables us to predict these
attacks efficiently with an 87.2% reduction in the search space. We demonstrate
the application of our method to the hacking of the in-vehicle network of a
connected car. To defend against the known attacks and possible novel exploits,
we discuss a defense-in-depth mechanism for various classes of attacks and the
classification of data targeted by such attacks. This defense mechanism
optimizes the cost of security measures based on the sensitivity of the
protected resource, thus incentivizing its adoption in real-world CPS/IoT by
cybersecurity practitioners.",arxiv
http://arxiv.org/abs/2104.13617v2,2021-05-01T07:17:36Z,2021-04-28T07:54:40Z,"End-to-End Intersection Handling using Multi-Agent Deep Reinforcement
  Learning","Navigating through intersections is one of the main challenging tasks for an
autonomous vehicle. However, for the majority of intersections regulated by
traffic lights, the problem could be solved by a simple rule-based method in
which the autonomous vehicle behavior is closely related to the traffic light
states. In this work, we focus on the implementation of a system able to
navigate through intersections where only traffic signs are provided. We
propose a multi-agent system using a continuous, model-free Deep Reinforcement
Learning algorithm used to train a neural network for predicting both the
acceleration and the steering angle at each time step. We demonstrate that
agents learn both the basic rules needed to handle intersections by
understanding the priorities of other learners inside the environment, and to
drive safely along their paths. Moreover, a comparison between our system and a
rule-based method proves that our model achieves better results especially with
dense traffic conditions. Finally, we test our system on real world scenarios
using real recorded traffic data, proving that our module is able to generalize
both to unseen environments and to different traffic conditions.",arxiv
http://arxiv.org/abs/2006.01250v6,2021-06-21T18:21:58Z,2020-05-09T09:41:46Z,RUHSNet: 3D Object Detection Using Lidar Data in Real Time,"In this work, we address the problem of 3D object detection from point cloud
data in real time. For autonomous vehicles to work, it is very important for
the perception component to detect the real world objects with both high
accuracy and fast inference. We propose a novel neural network architecture
along with the training and optimization details for detecting 3D objects in
point cloud data. We compare the results with different backbone architectures
including the standard ones like VGG, ResNet, Inception with our backbone. Also
we present the optimization and ablation studies including designing an
efficient anchor. We use the Kitti 3D Birds Eye View dataset for benchmarking
and validating our results. Our work surpasses the state of the art in this
domain both in terms of average precision and speed running at > 30 FPS. This
makes it a feasible option to be deployed in real time applications including
self driving cars.",arxiv
http://arxiv.org/abs/1705.05065v2,2017-07-18T05:30:28Z,2017-05-15T04:06:22Z,"AirSim: High-Fidelity Visual and Physical Simulation for Autonomous
  Vehicles","Developing and testing algorithms for autonomous vehicles in real world is an
expensive and time consuming process. Also, in order to utilize recent advances
in machine intelligence and deep learning we need to collect a large amount of
annotated training data in a variety of conditions and environments. We present
a new simulator built on Unreal Engine that offers physically and visually
realistic simulations for both of these goals. Our simulator includes a physics
engine that can operate at a high frequency for real-time hardware-in-the-loop
(HITL) simulations with support for popular protocols (e.g. MavLink). The
simulator is designed from the ground up to be extensible to accommodate new
types of vehicles, hardware platforms and software protocols. In addition, the
modular design enables various components to be easily usable independently in
other projects. We demonstrate the simulator by first implementing a quadrotor
as an autonomous vehicle and then experimentally comparing the software
components with real-world flights.",arxiv
http://arxiv.org/abs/1710.06270v2,2017-10-18T06:46:05Z,2017-10-17T13:38:16Z,"Procedural Modeling and Physically Based Rendering for Synthetic Data
  Generation in Automotive Applications","We present an overview and evaluation of a new, systematic approach for
generation of highly realistic, annotated synthetic data for training of deep
neural networks in computer vision tasks. The main contribution is a procedural
world modeling approach enabling high variability coupled with physically
accurate image synthesis, and is a departure from the hand-modeled virtual
worlds and approximate image synthesis methods used in real-time applications.
The benefits of our approach include flexible, physically accurate and scalable
image synthesis, implicit wide coverage of classes and features, and complete
data introspection for annotations, which all contribute to quality and cost
efficiency. To evaluate our approach and the efficacy of the resulting data, we
use semantic segmentation for autonomous vehicles and robotic navigation as the
main application, and we train multiple deep learning architectures using
synthetic data with and without fine tuning on organic (i.e. real-world) data.
The evaluation shows that our approach improves the neural network's
performance and that even modest implementation efforts produce
state-of-the-art results.",arxiv
http://arxiv.org/abs/2109.01896v3,2021-10-28T17:14:06Z,2021-09-04T16:26:31Z,"GamePlan: Game-Theoretic Multi-Agent Planning with Human Drivers at
  Intersections, Roundabouts, and Merging","We present a new method for multi-agent planning involving human drivers and
autonomous vehicles (AVs) in unsignaled intersections, roundabouts, and during
merging. In multi-agent planning, the main challenge is to predict the actions
of other agents, especially human drivers, as their intentions are hidden from
other agents. Our algorithm uses game theory to develop a new auction, called
GamePlan, that directly determines the optimal action for each agent based on
their driving style (which is observable via commonly available sensors).
GamePlan assigns a higher priority to more aggressive or impatient drivers and
a lower priority to more conservative or patient drivers; we theoretically
prove that such an approach is game-theoretically optimal prevents collisions
and deadlocks. We compare our approach with prior state-of-the-art auction
techniques including economic auctions, time-based auctions (first-in
first-out), and random bidding and show that each of these methods result in
collisions among agents when taking into account driver behavior. We
additionally compare with methods based on deep reinforcement learning, deep
learning, and game theory and present our benefits over these approaches.
Finally, we show that our approach can be implemented in the real-world with
human drivers.",arxiv
http://arxiv.org/abs/1808.06940v1,2018-08-20T09:25:30Z,2018-08-20T09:25:30Z,End to End Vehicle Lateral Control Using a Single Fisheye Camera,"Convolutional neural networks are commonly used to control the steering angle
for autonomous cars. Most of the time, multiple long range cameras are used to
generate lateral failure cases. In this paper we present a novel model to
generate this data and label augmentation using only one short range fisheye
camera. We present our simulator and how it can be used as a consistent metric
for lateral end-to-end control evaluation. Experiments are conducted on a
custom dataset corresponding to more than 10000 km and 200 hours of open road
driving. Finally we evaluate this model on real world driving scenarios, open
road and a custom test track with challenging obstacle avoidance and sharp
turns. In our simulator based on real-world videos, the final model was capable
of more than 99% autonomy on urban road",arxiv
http://arxiv.org/abs/1811.00145v3,2019-01-12T19:27:45Z,2018-10-31T22:47:22Z,Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation,"While recent developments in autonomous vehicle (AV) technology highlight
substantial progress, we lack tools for rigorous and scalable testing.
Real-world testing, the $\textit{de facto}$ evaluation environment, places the
public in danger, and, due to the rare nature of accidents, will require
billions of miles in order to statistically validate performance claims. We
implement a simulation framework that can test an entire modern autonomous
driving system, including, in particular, systems that employ deep-learning
perception and control algorithms. Using adaptive importance-sampling methods
to accelerate rare-event probability evaluation, we estimate the probability of
an accident under a base distribution governing standard traffic behavior. We
demonstrate our framework on a highway scenario, accelerating system evaluation
by $2$-$20$ times over naive Monte Carlo sampling methods and $10$-$300
\mathsf{P}$ times (where $\mathsf{P}$ is the number of processors) over
real-world testing.",arxiv
http://arxiv.org/abs/2005.13976v1,2020-05-22T19:00:38Z,2020-05-22T19:00:38Z,"Towards Automated Safety Coverage and Testing for Autonomous Vehicles
  with Reinforcement Learning","The kind of closed-loop verification likely to be required for autonomous
vehicle (AV) safety testing is beyond the reach of traditional test
methodologies and discrete verification. Validation puts the autonomous vehicle
system to the test in scenarios or situations that the system would likely
encounter in everyday driving after its release. These scenarios can either be
controlled directly in a physical (closed-course proving ground) or virtual
(simulation of predefined scenarios) environment, or they can arise
spontaneously during operation in the real world (open-road testing or
simulation of randomly generated scenarios).
  In AV testing, simulation serves primarily two purposes: to assist the
development of a robust autonomous vehicle and to test and validate the AV
before release. A challenge arises from the sheer number of scenario variations
that can be constructed from each of the above sources due to the high number
of variables involved (most of which are continuous). Even with continuous
variables discretized, the possible number of combinations becomes practically
infeasible to test. To overcome this challenge we propose using reinforcement
learning (RL) to generate failure examples and unexpected traffic situations
for the AV software implementation. Although reinforcement learning algorithms
have achieved notable results in games and some robotic manipulations, this
technique has not been widely scaled up to the more challenging real world
applications like autonomous driving.",arxiv
http://arxiv.org/abs/2005.02979v3,2021-10-14T16:40:00Z,2020-05-06T17:31:51Z,"A Survey of Algorithms for Black-Box Safety Validation of Cyber-Physical
  Systems","Autonomous cyber-physical systems (CPS) can improve safety and efficiency for
safety-critical applications, but require rigorous testing before deployment.
The complexity of these systems often precludes the use of formal verification
and real-world testing can be too dangerous during development. Therefore,
simulation-based techniques have been developed that treat the system under
test as a black box operating in a simulated environment. Safety validation
tasks include finding disturbances in the environment that cause the system to
fail (falsification), finding the most-likely failure, and estimating the
probability that the system fails. Motivated by the prevalence of
safety-critical artificial intelligence, this work provides a survey of
state-of-the-art safety validation techniques for CPS with a focus on applied
algorithms and their modifications for the safety validation problem. We
present and discuss algorithms in the domains of optimization, path planning,
reinforcement learning, and importance sampling. Problem decomposition
techniques are presented to help scale algorithms to large state spaces, which
are common for CPS. A brief overview of safety-critical applications is given,
including autonomous vehicles and aircraft collision avoidance systems.
Finally, we present a survey of existing academic and commercially available
safety validation tools.",arxiv
http://arxiv.org/abs/2010.06626v2,2020-12-29T01:09:57Z,2020-10-13T18:37:38Z,"On Deep Learning Techniques to Boost Monocular Depth Estimation for
  Autonomous Navigation","Inferring the depth of images is a fundamental inverse problem within the
field of Computer Vision since depth information is obtained through 2D images,
which can be generated from infinite possibilities of observed real scenes.
Benefiting from the progress of Convolutional Neural Networks (CNNs) to explore
structural features and spatial image information, Single Image Depth
Estimation (SIDE) is often highlighted in scopes of scientific and
technological innovation, as this concept provides advantages related to its
low implementation cost and robustness to environmental conditions. In the
context of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by
producing high-quality depth maps, which are essential during the autonomous
navigation process in different locations. However, such networks are usually
supervised by sparse and noisy depth data, from Light Detection and Ranging
(LiDAR) laser scans, and are carried out at high computational cost, requiring
high-performance Graphic Processing Units (GPUs). Therefore, we propose a new
lightweight and fast supervised CNN architecture combined with novel feature
extraction models which are designed for real-world autonomous navigation. We
also introduce an efficient surface normals module, jointly with a simple
geometric 2.5D loss function, to solve SIDE problems. We also innovate by
incorporating multiple Deep Learning techniques, such as the use of
densification algorithms and additional semantic, surface normals and depth
information to train our framework. The method introduced in this work focuses
on robotic applications in indoor and outdoor environments and its results are
evaluated on the competitive and publicly available NYU Depth V2 and KITTI
Depth datasets.",arxiv
http://arxiv.org/abs/1806.07987v2,2018-09-13T17:20:44Z,2018-06-20T21:12:43Z,"A Hierarchical Deep Architecture and Mini-Batch Selection Method For
  Joint Traffic Sign and Light Detection","Traffic light and sign detectors on autonomous cars are integral for road
scene perception. The literature is abundant with deep learning networks that
detect either lights or signs, not both, which makes them unsuitable for
real-life deployment due to the limited graphics processing unit (GPU) memory
and power available on embedded systems. The root cause of this issue is that
no public dataset contains both traffic light and sign labels, which leads to
difficulties in developing a joint detection framework. We present a deep
hierarchical architecture in conjunction with a mini-batch proposal selection
mechanism that allows a network to detect both traffic lights and signs from
training on separate traffic light and sign datasets. Our method solves the
overlapping issue where instances from one dataset are not labelled in the
other dataset. We are the first to present a network that performs joint
detection on traffic lights and signs. We measure our network on the
Tsinghua-Tencent 100K benchmark for traffic sign detection and the Bosch Small
Traffic Lights benchmark for traffic light detection and show it outperforms
the existing Bosch Small Traffic light state-of-the-art method. We focus on
autonomous car deployment and show our network is more suitable than others
because of its low memory footprint and real-time image processing time.
Qualitative results can be viewed at https://youtu.be/_YmogPzBXOw",arxiv
http://arxiv.org/abs/1904.07537v1,2019-04-16T08:49:06Z,2019-04-16T08:49:06Z,"Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic
  Point Clouds","Accurate detection of 3D objects is a fundamental problem in computer vision
and has an enormous impact on autonomous cars, augmented/virtual reality and
many applications in robotics. In this work we present a novel fusion of neural
network based state-of-the-art 3D detector and visual semantic segmentation in
the context of autonomous driving. Additionally, we introduce
Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable
evaluation metric for comparison of object detections, which speeds up our
inference time up to 20\% and halves training time. On top, we apply
state-of-the-art online multi target feature tracking on the object
measurements to further increase accuracy and robustness utilizing temporal
information. Our experiments on KITTI show that we achieve same results as
state-of-the-art in all related categories, while maintaining the performance
and accuracy trade-off and still run in real-time. Furthermore, our model is
the first one that fuses visual semantic with 3D object detection.",arxiv
http://arxiv.org/abs/1909.05314v1,2019-09-11T19:10:07Z,2019-09-11T19:10:07Z,"ScieNet: Deep Learning with Spike-assisted Contextual Information
  Extraction","Deep neural networks (DNNs) provide high image classification accuracy, but
experience significant performance degradation when perturbation from various
sources are present in the input. The lack of resilience to input perturbations
makes DNN less reliable for systems interacting with physical world such as
autonomous vehicles, robotics, to name a few, where imperfect input is the
normal condition. We present a hybrid deep network architecture with
spike-assisted contextual information extraction (ScieNet). ScieNet integrates
unsupervised learning using spiking neural network (SNN) for unsupervised
contextual informationextraction with a back-end DNN trained for
classification. The integrated network demonstrates high resilience to input
perturbations without relying on prior training on perturbed inputs. We
demonstrate ScieNet with different back-end DNNs for image classification using
CIFAR dataset considering stochastic (noise) and structured (rain) input
perturbations. Experimental results demonstrate significant improvement in
accuracy on noisy and rainy images without prior training, while maintaining
state-of-the-art accuracy on clean images.",arxiv
http://arxiv.org/abs/2110.00808v1,2021-10-02T13:55:50Z,2021-10-02T13:55:50Z,Cycle-Consistent World Models for Domain Independent Latent Imagination,"End-to-end autonomous driving seeks to solve the perception, decision, and
control problems in an integrated way, which can be easier to generalize at
scale and be more adapting to new scenarios. However, high costs and risks make
it very hard to train autonomous cars in the real world. Simulations can
therefore be a powerful tool to enable training. Due to slightly different
observations, agents trained and evaluated solely in simulation often perform
well there but have difficulties in real-world environments. To tackle this
problem, we propose a novel model-based reinforcement learning approach called
Cycleconsistent World Models. Contrary to related approaches, our model can
embed two modalities in a shared latent space and thereby learn from samples in
one modality (e.g., simulated data) and be used for inference in different
domain (e.g., real-world data). Our experiments using different modalities in
the CARLA simulator showed that this enables CCWM to outperform
state-of-the-art domain adaptation approaches. Furthermore, we show that CCWM
can decode a given latent representation into semantically coherent
observations in both modalities.",arxiv
http://arxiv.org/abs/2102.10398v3,2021-02-28T00:47:42Z,2021-02-20T17:35:23Z,All-Chalcogenide Programmable All-Optical Deep Neural Networks,"Deeplearning algorithms are revolutionising many aspects of modern life.
Typically, they are implemented in CMOS-based hardware with severely limited
memory access times and inefficient data-routing. All-optical neural networks
without any electro-optic conversions could alleviate these shortcomings.
However, an all-optical nonlinear activation function, which is a vital
building block for optical neural networks, needs to be developed efficiently
on-chip. Here, we introduce and demonstrate both optical synapse weighting and
all-optical nonlinear thresholding using two different effects in a
chalcogenide material photonic platform. We show how the structural phase
transitions in a wide-bandgap phase-change material enables storing the neural
network weights via non-volatile photonic memory, whilst resonant bond
destabilisation is used as a nonlinear activation threshold without changing
the material. These two different transitions within chalcogenides enable
programmable neural networks with near-zero static power consumption once
trained, in addition to picosecond delays performing inference tasks not
limited by wire charging that limit electrical circuits; for instance, we show
that nanosecond-order weight programming and near-instantaneous weight updates
enable accurate inference tasks within 20 picoseconds in a 3-layer all-optical
neural network. Optical neural networks that bypass electro-optic conversion
altogether hold promise for network-edge machine learning applications where
decision-making in real-time are critical, such as for autonomous vehicles or
navigation systems such as signal pre-processing of LIDAR systems.",arxiv
http://arxiv.org/abs/2010.01931v1,2020-10-05T11:41:11Z,2020-10-05T11:41:11Z,Offline Learning for Planning: A Summary,"The training of autonomous agents often requires expensive and unsafe
trial-and-error interactions with the environment. Nowadays several data sets
containing recorded experiences of intelligent agents performing various tasks,
spanning from the control of unmanned vehicles to human-robot interaction and
medical applications are accessible on the internet. With the intention of
limiting the costs of the learning procedure it is convenient to exploit the
information that is already available rather than collecting new data.
Nevertheless, the incapability to augment the batch can lead the autonomous
agents to develop far from optimal behaviours when the sampled experiences do
not allow for a good estimate of the true distribution of the environment.
Offline learning is the area of machine learning concerned with efficiently
obtaining an optimal policy with a batch of previously collected experiences
without further interaction with the environment. In this paper we adumbrate
the ideas motivating the development of the state-of-the-art offline learning
baselines. The listed methods consist in the introduction of epistemic
uncertainty dependent constraints during the classical resolution of a Markov
Decision Process, with and without function approximators, that aims to
alleviate the bad effects of the distributional mismatch between the available
samples and real world. We provide comments on the practical utility of the
theoretical bounds that justify the application of these algorithms and suggest
the utilization of Generative Adversarial Networks to estimate the
distributional shift that affects all of the proposed model-free and
model-based approaches.",arxiv
http://arxiv.org/abs/2107.12137v2,2021-08-11T06:54:54Z,2021-07-26T12:18:23Z,AA3DNet: Attention Augmented Real Time 3D Object Detection,"In this work, we address the problem of 3D object detection from point cloud
data in real time. For autonomous vehicles to work, it is very important for
the perception component to detect the real world objects with both high
accuracy and fast inference. We propose a novel neural network architecture
along with the training and optimization details for detecting 3D objects using
point cloud data. We present anchor design along with custom loss functions
used in this work. A combination of spatial and channel wise attention module
is used in this work. We use the Kitti 3D Birds Eye View dataset for
benchmarking and validating our results. Our method surpasses previous state of
the art in this domain both in terms of average precision and speed running at
> 30 FPS. Finally, we present the ablation study to demonstrate that the
performance of our network is generalizable. This makes it a feasible option to
be deployed in real time applications like self driving cars.",arxiv
http://arxiv.org/abs/2101.10463v2,2021-01-27T02:22:33Z,2021-01-25T22:34:06Z,"RTGPU: Real-Time GPU Scheduling of Hard Deadline Parallel Tasks with
  Fine-Grain Utilization","Many emerging cyber-physical systems, such as autonomous vehicles and robots,
rely heavily on artificial intelligence and machine learning algorithms to
perform important system operations. Since these highly parallel applications
are computationally intensive, they need to be accelerated by graphics
processing units (GPUs) to meet stringent timing constraints. However, despite
the wide adoption of GPUs, efficiently scheduling multiple GPU applications
while providing rigorous real-time guarantees remains a challenge. In this
paper, we propose RTGPU, which can schedule the execution of multiple GPU
applications in real-time to meet hard deadlines. Each GPU application can have
multiple CPU execution and memory copy segments, as well as GPU kernels. We
start with a model to explicitly account for the CPU and memory copy segments
of these applications. We then consider the GPU architecture in the development
of a precise timing model for the GPU kernels and leverage a technique known as
persistent threads to implement fine-grained kernel scheduling with improved
performance through interleaved execution. Next, we propose a general method
for scheduling parallel GPU applications in real time. Finally, to schedule
multiple parallel GPU applications, we propose a practical real-time scheduling
algorithm based on federated scheduling and grid search (for GPU kernel
segments) with uniprocessor fixed priority scheduling (for multiple CPU and
memory copy segments). Our approach provides superior schedulability compared
with previous work, and gives real-time guarantees to meet hard deadlines for
multiple GPU applications according to comprehensive validation and evaluation
on a real NVIDIA GTX1080Ti GPU system.",arxiv
http://arxiv.org/abs/1909.03854v1,2019-09-09T13:40:12Z,2019-09-09T13:40:12Z,A Convolutional Neural Network Approach Towards Self-Driving Cars,"A convolutional neural network (CNN) approach is used to implement a level 2
autonomous vehicle by mapping pixels from the camera input to the steering
commands. The network automatically learns the maximum variable features from
the camera input, hence requires minimal human intervention. Given realistic
frames as input, the driving policy trained on the dataset by NVIDIA and
Udacity can adapt to real-world driving in a controlled environment. The CNN is
tested on the CARLA open-source driving simulator. Details of a beta-testing
platform are also presented, which consists of an ultrasonic sensor for
obstacle detection and an RGBD camera for real-time position monitoring at
10Hz. Arduino Mega and Raspberry Pi are used for motor control and processing
respectively to output the steering angle, which is converted to angular
velocity for steering.",arxiv
http://arxiv.org/abs/1904.06025v2,2020-02-21T18:00:23Z,2019-04-12T04:01:18Z,"Interaction-aware Decision Making with Adaptive Strategies under Merging
  Scenarios","In order to drive safely and efficiently under merging scenarios, autonomous
vehicles should be aware of their surroundings and make decisions by
interacting with other road participants. Moreover, different strategies should
be made when the autonomous vehicle is interacting with drivers having
different level of cooperativeness. Whether the vehicle is on the merge-lane or
main-lane will also influence the driving maneuvers since drivers will behave
differently when they have the right-of-way than otherwise. Many traditional
methods have been proposed to solve decision making problems under merging
scenarios. However, these works either are incapable of modeling complicated
interactions or require implementing hand-designed rules which cannot properly
handle the uncertainties in real-world scenarios. In this paper, we proposed an
interaction-aware decision making with adaptive strategies (IDAS) approach that
can let the autonomous vehicle negotiate the road with other drivers by
leveraging their cooperativeness under merging scenarios. A single policy is
learned under the multi-agent reinforcement learning (MARL) setting via the
curriculum learning strategy, which enables the agent to automatically infer
other drivers' various behaviors and make decisions strategically. A masking
mechanism is also proposed to prevent the agent from exploring states that
violate common sense of human judgment and increase the learning efficiency. An
exemplar merging scenario was used to implement and examine the proposed
method.",arxiv
http://arxiv.org/abs/2102.05843v1,2021-02-11T04:33:43Z,2021-02-11T04:33:43Z,"Driving Style Representation in Convolutional Recurrent Neural Network
  Model of Driver Identification","Identifying driving styles is the task of analyzing the behavior of drivers
in order to capture variations that will serve to discriminate different
drivers from each other. This task has become a prerequisite for a variety of
applications, including usage-based insurance, driver coaching, driver action
prediction, and even in designing autonomous vehicles; because driving style
encodes essential information needed by these applications. In this paper, we
present a deep-neural-network architecture, we term D-CRNN, for building
high-fidelity representations for driving style, that combine the power of
convolutional neural networks (CNN) and recurrent neural networks (RNN). Using
CNN, we capture semantic patterns of driver behavior from trajectories (such as
a turn or a braking event). We then find temporal dependencies between these
semantic patterns using RNN to encode driving style. We demonstrate the
effectiveness of these techniques for driver identification by learning driving
style through extensive experiments conducted on several large, real-world
datasets, and comparing the results with the state-of-the-art deep-learning and
non-deep-learning solutions. These experiments also demonstrate a useful
example of bias removal, by presenting how we preprocess the input data by
sampling dissimilar trajectories for each driver to prevent spatial
memorization. Finally, this paper presents an analysis of the contribution of
different attributes for driver identification; we find that engine RPM, Speed,
and Acceleration are the best combination of features.",arxiv
http://arxiv.org/abs/1906.01562v2,2019-09-27T13:22:08Z,2019-06-04T16:27:47Z,Privacy-preserving Crowd-guided AI Decision-making in Ethical Dilemmas,"With the rapid development of artificial intelligence (AI), ethical issues
surrounding AI have attracted increasing attention. In particular, autonomous
vehicles may face moral dilemmas in accident scenarios, such as staying the
course resulting in hurting pedestrians or swerving leading to hurting
passengers. To investigate such ethical dilemmas, recent studies have adopted
preference aggregation, in which each voter expresses her/his preferences over
decisions for the possible ethical dilemma scenarios, and a centralized system
aggregates these preferences to obtain the winning decision. Although a useful
methodology for building ethical AI systems, such an approach can potentially
violate the privacy of voters since moral preferences are sensitive information
and their disclosure can be exploited by malicious parties. In this paper, we
report a first-of-its-kind privacy-preserving crowd-guided AI decision-making
approach in ethical dilemmas. We adopt the notion of differential privacy to
quantify privacy and consider four granularities of privacy protection by
taking voter-/record-level privacy protection and centralized/distributed
perturbation into account, resulting in four approaches VLCP, RLCP, VLDP, and
RLDP. Moreover, we propose different algorithms to achieve these privacy
protection granularities, while retaining the accuracy of the learned moral
preference model. Specifically, VLCP and RLCP are implemented with the data
aggregator setting a universal privacy parameter and perturbing the averaged
moral preference to protect the privacy of voters' data. VLDP and RLDP are
implemented in such a way that each voter perturbs her/his local moral
preference with a personalized privacy parameter. Extensive experiments on both
synthetic and real data demonstrate that the proposed approach can achieve high
accuracy of preference aggregation while protecting individual voter's privacy.",arxiv
http://arxiv.org/abs/2003.01886v1,2020-03-04T04:35:22Z,2020-03-04T04:35:22Z,"Efficient statistical validation with edge cases to evaluate Highly
  Automated Vehicles","The widescale deployment of Autonomous Vehicles (AV) seems to be imminent
despite many safety challenges that are yet to be resolved. It is well known
that there are no universally agreed Verification and Validation (VV)
methodologies to guarantee absolute safety, which is crucial for the acceptance
of this technology. Existing standards focus on deterministic processes where
the validation requires only a set of test cases that cover the requirements.
Modern autonomous vehicles will undoubtedly include machine learning and
probabilistic techniques that require a much more comprehensive testing regime
due to the non-deterministic nature of the operating design domain. A rigourous
statistical validation process is an essential component required to address
this challenge. Most research in this area focuses on evaluating system
performance in large scale real-world data gathering exercises (number of miles
travelled), or randomised test scenarios in simulation.
  This paper presents a new approach to compute the statistical characteristics
of a system's behaviour by biasing automatically generated test cases towards
the worst case scenarios, identifying potential unsafe edge cases.We use
reinforcement learning (RL) to learn the behaviours of simulated actors that
cause unsafe behaviour measured by the well established RSS safety metric. We
demonstrate that by using the method we can more efficiently validate a system
using a smaller number of test cases by focusing the simulation towards the
worst case scenario, generating edge cases that correspond to unsafe
situations.",arxiv
http://arxiv.org/abs/2101.06175v1,2021-01-15T15:36:22Z,2021-01-15T15:36:22Z,PaddleSeg: A High-Efficient Development Toolkit for Image Segmentation,"Image Segmentation plays an essential role in computer vision and image
processing with various applications from medical diagnosis to autonomous car
driving. A lot of segmentation algorithms have been proposed for addressing
specific problems. In recent years, the success of deep learning techniques has
tremendously influenced a wide range of computer vision areas, and the modern
approaches of image segmentation based on deep learning are becoming prevalent.
In this article, we introduce a high-efficient development toolkit for image
segmentation, named PaddleSeg. The toolkit aims to help both developers and
researchers in the whole process of designing segmentation models, training
models, optimizing performance and inference speed, and deploying models.
Currently, PaddleSeg supports around 20 popular segmentation models and more
than 50 pre-trained models from real-time and high-accuracy levels. With
modular components and backbone networks, users can easily build over one
hundred models for different requirements. Furthermore, we provide
comprehensive benchmarks and evaluations to show that these segmentation
algorithms trained on our toolkit have more competitive accuracy. Also, we
provide various real industrial applications and practical cases based on
PaddleSeg. All codes and examples of PaddleSeg are available at
https://github.com/PaddlePaddle/PaddleSeg.",arxiv
http://arxiv.org/abs/1801.02190v1,2018-01-07T13:46:03Z,2018-01-07T13:46:03Z,Approximate FPGA-based LSTMs under Computation Time Constraints,"Recurrent Neural Networks and in particular Long Short-Term Memory (LSTM)
networks have demonstrated state-of-the-art accuracy in several emerging
Artificial Intelligence tasks. However, the models are becoming increasingly
demanding in terms of computational and memory load. Emerging latency-sensitive
applications including mobile robots and autonomous vehicles often operate
under stringent computation time constraints. In this paper, we address the
challenge of deploying computationally demanding LSTMs at a constrained time
budget by introducing an approximate computing scheme that combines iterative
low-rank compression and pruning, along with a novel FPGA-based LSTM
architecture. Combined in an end-to-end framework, the approximation method's
parameters are optimised and the architecture is configured to address the
problem of high-performance LSTM execution in time-constrained applications.
Quantitative evaluation on a real-life image captioning application indicates
that the proposed methods required up to 6.5x less time to achieve the same
application-level accuracy compared to a baseline method, while achieving an
average of 25x higher accuracy under the same computation time constraints.",arxiv
http://arxiv.org/abs/2109.15286v1,2021-09-30T17:30:43Z,2021-09-30T17:30:43Z,Unsupervised Domain Adaptation for LiDAR Panoptic Segmentation,"Scene understanding is a pivotal task for autonomous vehicles to safely
navigate in the environment. Recent advances in deep learning enable accurate
semantic reconstruction of the surroundings from LiDAR data. However, these
models encounter a large domain gap while deploying them on vehicles equipped
with different LiDAR setups which drastically decreases their performance.
Fine-tuning the model for every new setup is infeasible due to the expensive
and cumbersome process of recording and manually labeling new data.
Unsupervised Domain Adaptation (UDA) techniques are thus essential to fill this
domain gap and retain the performance of models on new sensor setups without
the need for additional data labeling. In this paper, we propose AdaptLPS, a
novel UDA approach for LiDAR panoptic segmentation that leverages task-specific
knowledge and accounts for variation in the number of scan lines, mounting
position, intensity distribution, and environmental conditions. We tackle the
UDA task by employing two complementary domain adaptation strategies,
data-based and model-based. While data-based adaptations reduce the domain gap
by processing the raw LiDAR scans to resemble the scans in the target domain,
model-based techniques guide the network in extracting features that are
representative for both domains. Extensive evaluations on three pairs of
real-world autonomous driving datasets demonstrate that AdaptLPS outperforms
existing UDA approaches by up to 6.41 pp in terms of the PQ score.",arxiv
http://arxiv.org/abs/2001.03864v1,2020-01-12T06:06:03Z,2020-01-12T06:06:03Z,"Learning to drive via Apprenticeship Learning and Deep Reinforcement
  Learning","With the implementation of reinforcement learning (RL) algorithms, current
state-of-art autonomous vehicle technology have the potential to get closer to
full automation. However, most of the applications have been limited to game
domains or discrete action space which are far from the real world driving.
Moreover, it is very tough to tune the parameters of reward mechanism since the
driving styles vary a lot among the different users. For instance, an
aggressive driver may prefer driving with high acceleration whereas some
conservative drivers prefer a safer driving style. Therefore, we propose an
apprenticeship learning in combination with deep reinforcement learning
approach that allows the agent to learn the driving and stopping behaviors with
continuous actions. We use gradient inverse reinforcement learning (GIRL)
algorithm to recover the unknown reward function and employ REINFORCE as well
as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal
policy. The performance of our method is evaluated in simulation-based scenario
and the results demonstrate that the agent performs human like driving and even
better in some aspects after training.",arxiv
http://arxiv.org/abs/2011.01112v1,2020-11-02T16:43:04Z,2020-11-02T16:43:04Z,Scheduling Real-time Deep Learning Services as Imprecise Computations,"The paper presents an efficient real-time scheduling algorithm for
intelligent real-time edge services, defined as those that perform machine
intelligence tasks, such as voice recognition, LIDAR processing, or machine
vision, on behalf of local embedded devices that are themselves unable to
support extensive computations. The work contributes to a recent direction in
real-time computing that develops scheduling algorithms for machine
intelligence tasks with anytime prediction. We show that deep neural network
workflows can be cast as imprecise computations, each with a mandatory part and
(several) optional parts whose execution utility depends on input data. The
goal of the real-time scheduler is to maximize the average accuracy of deep
neural network outputs while meeting task deadlines, thanks to opportunistic
shedding of the least necessary optional parts. The work is motivated by the
proliferation of increasingly ubiquitous but resource-constrained embedded
devices (for applications ranging from autonomous cars to the Internet of
Things) and the desire to develop services that endow them with intelligence.
Experiments on recent GPU hardware and a state of the art deep neural network
for machine vision illustrate that our scheme can increase the overall accuracy
by 10%-20% while incurring (nearly) no deadline misses.",arxiv
http://arxiv.org/abs/2004.13866v1,2020-04-28T21:56:10Z,2020-04-28T21:56:10Z,Deflating Dataset Bias Using Synthetic Data Augmentation,"Deep Learning has seen an unprecedented increase in vision applications since
the publication of large-scale object recognition datasets and introduction of
scalable compute hardware. State-of-the-art methods for most vision tasks for
Autonomous Vehicles (AVs) rely on supervised learning and often fail to
generalize to domain shifts and/or outliers. Dataset diversity is thus key to
successful real-world deployment. No matter how big the size of the dataset,
capturing long tails of the distribution pertaining to task-specific
environmental factors is impractical. The goal of this paper is to investigate
the use of targeted synthetic data augmentation - combining the benefits of
gaming engine simulations and sim2real style transfer techniques - for filling
gaps in real datasets for vision tasks. Empirical studies on three different
computer vision tasks of practical use to AVs - parking slot detection, lane
detection and monocular depth estimation - consistently show that having
synthetic data in the training mix provides a significant boost in
cross-dataset generalization performance as compared to training on real data
only, for the same size of the training set.",arxiv
http://arxiv.org/abs/2011.07699v1,2020-11-16T02:56:13Z,2020-11-16T02:56:13Z,"Efficient falsification approach for autonomous vehicle validation using
  a parameter optimisation technique based on reinforcement learning","The widescale deployment of Autonomous Vehicles (AV) appears to be imminent
despite many safety challenges that are yet to be resolved. It is well-known
that there are no universally agreed Verification and Validation (VV)
methodologies guarantee absolute safety, which is crucial for the acceptance of
this technology. The uncertainties in the behaviour of the traffic participants
and the dynamic world cause stochastic reactions in advanced autonomous
systems. The addition of ML algorithms and probabilistic techniques adds
significant complexity to the process for real-world testing when compared to
traditional methods. Most research in this area focuses on generating
challenging concrete scenarios or test cases to evaluate the system performance
by looking at the frequency distribution of extracted parameters as collected
from the real-world data. These approaches generally employ Monte-Carlo
simulation and importance sampling to generate critical cases. This paper
presents an efficient falsification method to evaluate the System Under Test.
The approach is based on a parameter optimisation problem to search for
challenging scenarios. The optimisation process aims at finding the challenging
case that has maximum return. The method applies policy-gradient reinforcement
learning algorithm to enable the learning. The riskiness of the scenario is
measured by the well established RSS safety metric, euclidean distance, and
instance of a collision. We demonstrate that by using the proposed method, we
can more efficiently search for challenging scenarios which could cause the
system to fail in order to satisfy the safety requirements.",arxiv
http://arxiv.org/abs/1903.01712v1,2019-03-05T07:46:47Z,2019-03-05T07:46:47Z,"Deep Learning Based Motion Planning For Autonomous Vehicle Using
  Spatiotemporal LSTM Network","Motion Planning, as a fundamental technology of automatic navigation for the
autonomous vehicle, is still an open challenging issue in the real-life traffic
situation and is mostly applied by the model-based approaches. However, due to
the complexity of the traffic situations and the uncertainty of the edge cases,
it is hard to devise a general motion planning system for the autonomous
vehicle. In this paper, we proposed a motion planning model based on deep
learning (named as spatiotemporal LSTM network), which is able to generate a
real-time reflection based on spatiotemporal information extraction. To be
specific, the model based on spatiotemporal LSTM network has three main
structure. Firstly, the Convolutional Long-short Term Memory (Conv-LSTM) is
used to extract hidden features through sequential image data. Then, the 3D
Convolutional Neural Network(3D-CNN) is applied to extract the spatiotemporal
information from the multi-frame feature information. Finally, the fully
connected neural networks are used to construct a control model for autonomous
vehicle steering angle. The experiments demonstrated that the proposed method
can generate a robust and accurate visual motion planning results for the
autonomous vehicle.",arxiv
http://arxiv.org/abs/1904.00035v1,2019-03-29T18:15:24Z,2019-03-29T18:15:24Z,Autonomous Highway Driving using Deep Reinforcement Learning,"The operational space of an autonomous vehicle (AV) can be diverse and vary
significantly. This may lead to a scenario that was not postulated in the
design phase. Due to this, formulating a rule based decision maker for
selecting maneuvers may not be ideal. Similarly, it may not be effective to
design an a-priori cost function and then solve the optimal control problem in
real-time. In order to address these issues and to avoid peculiar behaviors
when encountering unforeseen scenario, we propose a reinforcement learning (RL)
based method, where the ego car, i.e., an autonomous vehicle, learns to make
decisions by directly interacting with simulated traffic. The decision maker
for AV is implemented as a deep neural network providing an action choice for a
given system state. In a critical application such as driving, an RL agent
without explicit notion of safety may not converge or it may need extremely
large number of samples before finding a reliable policy. To best address the
issue, this paper incorporates reinforcement learning with an additional short
horizon safety check (SC). In a critical scenario, the safety check will also
provide an alternate safe action to the agent provided if it exists. This leads
to two novel contributions. First, it generalizes the states that could lead to
undesirable ""near-misses"" or ""collisions "". Second, inclusion of safety check
can provide a safe and stable training environment. This significantly enhances
learning efficiency without inhibiting meaningful exploration to ensure safe
and optimal learned behavior. We demonstrate the performance of the developed
algorithm in highway driving scenario where the trained AV encounters varying
traffic density in a highway setting.",arxiv
http://arxiv.org/abs/1911.09592v1,2019-11-21T16:37:18Z,2019-11-21T16:37:18Z,"mm-Pose: Real-Time Human Skeletal Posture Estimation using mmWave Radars
  and CNNs","In this paper, mm-Pose, a novel approach to detect and track human skeletons
in real-time using an mmWave radar, is proposed. To the best of the authors'
knowledge, this is the first method to detect >15 distinct skeletal joints
using mmWave radar reflection signals. The proposed method would find several
applications in traffic monitoring systems, autonomous vehicles, patient
monitoring systems and defense forces to detect and track human skeleton for
effective and preventive decision making in real-time. The use of radar makes
the system operationally robust to scene lighting and adverse weather
conditions. The reflected radar point cloud in range, azimuth and elevation are
first resolved and projected in Range-Azimuth and Range-Elevation planes. A
novel low-size high-resolution radar-to-image representation is also presented,
that overcomes the sparsity in traditional point cloud data and offers
significant reduction in the subsequent machine learning architecture. The RGB
channels were assigned with the normalized values of range, elevation/azimuth
and the power level of the reflection signals for each of the points. A forked
CNN architecture was used to predict the real-world position of the skeletal
joints in 3-D space, using the radar-to-image representation. The proposed
method was tested for a single human scenario for four primary motions, (i)
Walking, (ii) Swinging left arm, (iii) Swinging right arm, and (iv) Swinging
both arms to validate accurate predictions for motion in range, azimuth and
elevation. The detailed methodology, implementation, challenges, and validation
results are presented.",arxiv
http://arxiv.org/abs/1804.03629v1,2018-04-10T17:05:53Z,2018-04-10T17:05:53Z,Probabilistic Prediction of Vehicle Semantic Intention and Motion,"Accurately predicting the possible behaviors of traffic participants is an
essential capability for future autonomous vehicles. The majority of current
researches fix the number of driving intentions by considering only a specific
scenario. However, distinct driving environments usually contain various
possible driving maneuvers. Therefore, a intention prediction method that can
adapt to different traffic scenarios is needed. To further improve the overall
vehicle prediction performance, motion information is usually incorporated with
classified intentions. As suggested in some literature, the methods that
directly predict possible goal locations can achieve better performance for
long-term motion prediction than other approaches due to their automatic
incorporation of environment constraints. Moreover, by obtaining the temporal
information of the predicted destinations, the optimal trajectories for
predicted vehicles as well as the desirable path for ego autonomous vehicle
could be easily generated. In this paper, we propose a Semantic-based Intention
and Motion Prediction (SIMP) method, which can be adapted to any driving
scenarios by using semantic-defined vehicle behaviors. It utilizes a
probabilistic framework based on deep neural network to estimate the
intentions, final locations, and the corresponding time information for
surrounding vehicles. An exemplar real-world scenario was used to implement and
examine the proposed method.",arxiv
http://arxiv.org/abs/1909.12217v4,2021-01-25T21:19:51Z,2019-09-26T16:15:37Z,"Visual Exploration and Energy-aware Path Planning via Reinforcement
  Learning","Visual exploration and smart data collection via autonomous vehicles is an
attractive topic in various disciplines. Disturbances like wind significantly
influence both the power consumption of the flying robots and the performance
of the camera. We propose a reinforcement learning approach which combines the
effects of the power consumption and the object detection modules to develop a
policy for object detection in large areas with limited battery life. The
learning model enables dynamic learning of the negative rewards of each action
based on the drag forces that is resulted by the motion of the flying robot
with respect to the wind field. The algorithm is implemented in a near-real
world simulation environment both for the planar motion and flight in different
altitudes. The trained agent often performed a trade-off between detecting the
objects with high accuracy and increasing the area coverage within its battery
life. The developed exploration policy outperformed the complete coverage
algorithm by minimizing the traveled path while finding the target objects. The
performance of the algorithms under various wind fields was evaluated in planar
and 3D motion. During an exploration task with sparsely distributed goals and
within a UAV's battery life, the proposed architecture could detect more than
twice the amount of goal objects compared to the coverage path planning
algorithm in moderate wind field. In high wind intensities, the energy-aware
algorithm could detect 4 times the amount of goal objects when compared to its
complete coverage counterpart.",arxiv
http://arxiv.org/abs/1911.03565v1,2019-11-08T22:28:53Z,2019-11-08T22:28:53Z,"Vision-Based Lane-Changing Behavior Detection Using Deep Residual Neural
  Network","Accurate lane localization and lane change detection are crucial in advanced
driver assistance systems and autonomous driving systems for safer and more
efficient trajectory planning. Conventional localization devices such as Global
Positioning System only provide road-level resolution for car navigation, which
is incompetent to assist in lane-level decision making. The state of art
technique for lane localization is to use Light Detection and Ranging sensors
to correct the global localization error and achieve centimeter-level accuracy,
but the real-time implementation and popularization for LiDAR is still limited
by its computational burden and current cost. As a cost-effective alternative,
vision-based lane change detection has been highly regarded for affordable
autonomous vehicles to support lane-level localization. A deep learning-based
computer vision system is developed to detect the lane change behavior using
the images captured by a front-view camera mounted on the vehicle and data from
the inertial measurement unit for highway driving. Testing results on
real-world driving data have shown that the proposed method is robust with
real-time working ability and could achieve around 87% lane change detection
accuracy. Compared to the average human reaction to visual stimuli, the
proposed computer vision system works 9 times faster, which makes it capable of
helping make life-saving decisions in time.",arxiv
http://arxiv.org/abs/2104.08862v1,2021-04-18T14:05:18Z,2021-04-18T14:05:18Z,"End-to-End Interactive Prediction and Planning with Optical Flow
  Distillation for Autonomous Driving","With the recent advancement of deep learning technology, data-driven
approaches for autonomous car prediction and planning have achieved
extraordinary performance. Nevertheless, most of these approaches follow a
non-interactive prediction and planning paradigm, hypothesizing that a
vehicle's behaviors do not affect others. The approaches based on such a
non-interactive philosophy typically perform acceptably in sparse traffic
scenarios but can easily fail in dense traffic scenarios. Therefore, we propose
an end-to-end interactive neural motion planner (INMP) for autonomous driving
in this paper. Given a set of past surrounding-view images and a high
definition map, our INMP first generates a feature map in bird's-eye-view
space, which is then processed to detect other agents and perform interactive
prediction and planning jointly. Also, we adopt an optical flow distillation
paradigm, which can effectively improve the network performance while still
maintaining its real-time inference speed. Extensive experiments on the
nuScenes dataset and in the closed-loop Carla simulation environment
demonstrate the effectiveness and efficiency of our INMP for the detection,
prediction, and planning tasks. Our project page is at
sites.google.com/view/inmp-ofd.",arxiv
http://arxiv.org/abs/1902.01084v2,2021-01-15T20:47:41Z,2019-02-04T08:51:50Z,Paracosm: A Language and Tool for Testing Autonomous Driving Systems,"Systematic testing of autonomous vehicles operating in complex real-world
scenarios is a difficult and expensive problem. We present Paracosm, a reactive
language for writing test scenarios for autonomous driving systems. Paracosm
allows users to programmatically describe complex driving situations with
specific visual features, e.g., road layout in an urban environment, as well as
reactive temporal behaviors of cars and pedestrians. Paracosm programs are
executed on top of a game engine that provides realistic physics simulation and
visual rendering. The infrastructure allows systematic exploration of the state
space, both for visual features (lighting, shadows, fog) and for reactive
interactions with the environment (pedestrians, other traffic). We define a
notion of test coverage for Paracosm configurations based on combinatorial
testing and low dispersion sequences. Paracosm comes with an automatic test
case generator that uses random sampling for discrete parameters and
deterministic quasi-Monte Carlo generation for continuous parameters. Through
an empirical evaluation, we demonstrate the modeling and testing capabilities
of Paracosm on a suite of autonomous driving systems implemented using deep
neural networks developed in research and education. We show how Paracosm can
expose incorrect behaviors or degraded performance.",arxiv
http://arxiv.org/abs/2003.07739v2,2020-07-12T15:00:40Z,2020-03-17T14:17:52Z,"Formal Scenario-Based Testing of Autonomous Vehicles: From Simulation to
  the Real World","We present a new approach to automated scenario-based testing of the safety
of autonomous vehicles, especially those using advanced artificial
intelligence-based components, spanning both simulation-based evaluation as
well as testing in the real world. Our approach is based on formal methods,
combining formal specification of scenarios and safety properties, algorithmic
test case generation using formal simulation, test case selection for track
testing, executing test cases on the track, and analyzing the resulting data.
Experiments with a real autonomous vehicle at an industrial testing facility
support our hypotheses that (i) formal simulation can be effective at
identifying test cases to run on the track, and (ii) the gap between simulated
and real worlds can be systematically evaluated and bridged.",arxiv
http://arxiv.org/abs/1912.03618v2,2020-06-06T03:56:29Z,2019-12-08T05:12:17Z,Efficient Black-box Assessment of Autonomous Vehicle Safety,"While autonomous vehicle (AV) technology has shown substantial progress, we
still lack tools for rigorous and scalable testing. Real-world testing, the
$\textit{de-facto}$ evaluation method, is dangerous to the public. Moreover,
due to the rare nature of failures, billions of miles of driving are needed to
statistically validate performance claims. Thus, the industry has largely
turned to simulation to evaluate AV systems. However, having a simulation stack
alone is not a solution. A simulation testing framework needs to prioritize
which scenarios to run, learn how the chosen scenarios provide coverage of
failure modes, and rank failure scenarios in order of importance. We implement
a simulation testing framework that evaluates an entire modern AV system as a
black box. This framework estimates the probability of accidents under a base
distribution governing standard traffic behavior. In order to accelerate
rare-event probability evaluation, we efficiently learn to identify and rank
failure scenarios via adaptive importance-sampling methods. Using this
framework, we conduct the first independent evaluation of a full-stack
commercial AV system, Comma AI's OpenPilot.",arxiv
http://arxiv.org/abs/2105.02613v1,2021-05-06T12:40:28Z,2021-05-06T12:40:28Z,"Challenges and Obstacles Towards Deploying Deep Learning Models on
  Mobile Devices","From computer vision and speech recognition to forecasting trajectories in
autonomous vehicles, deep learning approaches are at the forefront of so many
domains. Deep learning models are developed using plethora of high-level,
generic frameworks and libraries. Running those models on the mobile devices
require hardware-aware optimizations and in most cases converting the models to
other formats or using a third-party framework. In reality, most of the
developed models need to undergo a process of conversion, adaptation, and, in
some cases, full retraining to match the requirements and features of the
framework that is deploying the model on the target platform. Variety of
hardware platforms with heterogeneous computing elements, from wearable devices
to high-performance GPU clusters are used to run deep learning models. In this
paper, we present the existing challenges, obstacles, and practical solutions
towards deploying deep learning models on mobile devices.",arxiv
http://arxiv.org/abs/2009.14349v3,2020-12-07T20:08:44Z,2020-09-30T00:01:54Z,"Computing Systems for Autonomous Driving: State-of-the-Art and
  Challenges","The recent proliferation of computing technologies (e.g., sensors, computer
vision, machine learning, and hardware acceleration), and the broad deployment
of communication mechanisms (e.g., DSRC, C-V2X, 5G) have pushed the horizon of
autonomous driving, which automates the decision and control of vehicles by
leveraging the perception results based on multiple sensors. The key to the
success of these autonomous systems is making a reliable decision in real-time
fashion. However, accidents and fatalities caused by early deployed autonomous
vehicles arise from time to time. The real traffic environment is too
complicated for current autonomous driving computing systems to understand and
handle. In this paper, we present state-of-the-art computing systems for
autonomous driving, including seven performance metrics and nine key
technologies, followed by twelve challenges to realize autonomous driving. We
hope this paper will gain attention from both the computing and automotive
communities and inspire more research in this direction.",arxiv
http://arxiv.org/abs/2109.06668v2,2021-09-15T17:25:20Z,2021-09-14T13:16:33Z,Exploration in Deep Reinforcement Learning: A Comprehensive Survey,"Deep Reinforcement Learning (DRL) and Deep Multi-agent Reinforcement Learning
(MARL) have achieved significant success across a wide range of domains, such
as game AI, autonomous vehicles, robotics and finance. However, DRL and deep
MARL agents are widely known to be sample-inefficient and millions of
interactions are usually needed even for relatively simple game settings, thus
preventing the wide application in real-industry scenarios. One bottleneck
challenge behind is the well-known exploration problem, i.e., how to
efficiently explore the unknown environments and collect informative
experiences that could benefit the policy learning most.
  In this paper, we conduct a comprehensive survey on existing exploration
methods in DRL and deep MARL for the purpose of providing understandings and
insights on the critical problems and solutions. We first identify several key
challenges to achieve efficient exploration, which most of the exploration
methods aim at addressing. Then we provide a systematic survey of existing
approaches by classifying them into two major categories: uncertainty-oriented
exploration and intrinsic motivation-oriented exploration. The essence of
uncertainty-oriented exploration is to leverage the quantification of the
epistemic and aleatoric uncertainty to derive efficient exploration. By
contrast, intrinsic motivation-oriented exploration methods usually incorporate
different reward agnostic information for intrinsic exploration guidance.
Beyond the above two main branches, we also conclude other exploration methods
which adopt sophisticated techniques but are difficult to be classified into
the above two categories. In addition, we provide a comprehensive empirical
comparison of exploration methods for DRL on a set of commonly used benchmarks.
Finally, we summarize the open problems of exploration in DRL and deep MARL and
point out a few future directions.",arxiv
http://arxiv.org/abs/1812.10812v1,2018-12-27T19:55:54Z,2018-12-27T19:55:54Z,"DeepBillboard: Systematic Physical-World Testing of Autonomous Driving
  Systems","Deep Neural Networks (DNNs) have been widely applied in many autonomous
systems such as autonomous driving. Recently, DNN testing has been intensively
studied to automatically generate adversarial examples, which inject
small-magnitude perturbations into inputs to test DNNs under extreme
situations. While existing testing techniques prove to be effective, they
mostly focus on generating digital adversarial perturbations (particularly for
autonomous driving), e.g., changing image pixels, which may never happen in
physical world. There is a critical missing piece in the literature on
autonomous driving testing: understanding and exploiting both digital and
physical adversarial perturbation generation for impacting steering decisions.
In this paper, we present DeepBillboard, a systematic physical-world testing
approach targeting at a common and practical driving scenario: drive-by
billboards. DeepBillboard is capable of generating a robust and resilient
printable adversarial billboard, which works under dynamic changing driving
conditions including viewing angle, distance, and lighting. The objective is to
maximize the possibility, degree, and duration of the steering-angle errors of
an autonomous vehicle driving by the generated adversarial billboard. We have
extensively evaluated the efficacy and robustness of DeepBillboard through
conducting both digital and physical-world experiments. Results show that
DeepBillboard is effective for various steering models and scenes. Furthermore,
DeepBillboard is sufficiently robust and resilient for generating
physical-world adversarial billboard tests for real-world driving under various
weather conditions. To the best of our knowledge, this is the first study
demonstrating the possibility of generating realistic and continuous
physical-world tests for practical autonomous driving systems.",arxiv
http://arxiv.org/abs/2006.15110v1,2020-06-26T17:17:47Z,2020-06-26T17:17:47Z,"Learning predictive representations in autonomous driving to improve
  deep reinforcement learning","Reinforcement learning using a novel predictive representation is applied to
autonomous driving to accomplish the task of driving between lane markings
where substantial benefits in performance and generalization are observed on
unseen test roads in both simulation and on a real Jackal robot. The novel
predictive representation is learned by general value functions (GVFs) to
provide out-of-policy, or counter-factual, predictions of future lane
centeredness and road angle that form a compact representation of the state of
the agent improving learning in both online and offline reinforcement learning
to learn to drive an autonomous vehicle with methods that generalizes well to
roads not in the training data. Experiments in both simulation and the
real-world demonstrate that predictive representations in reinforcement
learning improve learning efficiency, smoothness of control and generalization
to roads that the agent was never shown during training, including damaged lane
markings. It was found that learning a predictive representation that consists
of several predictions over different time scales, or discount factors,
improves the performance and smoothness of the control substantially. The
Jackal robot was trained in a two step process where the predictive
representation is learned first followed by a batch reinforcement learning
algorithm (BCQ) from data collected through both automated and human-guided
exploration in the environment. We conclude that out-of-policy predictive
representations with GVFs offer reinforcement learning many benefits in
real-world problems.",arxiv
http://arxiv.org/abs/1808.06352v1,2018-08-20T09:06:21Z,2018-08-20T09:06:21Z,"Navigating the Landscape for Real-time Localisation and Mapping for
  Robotics and Virtual and Augmented Reality","Visual understanding of 3D environments in real-time, at low power, is a huge
computational challenge. Often referred to as SLAM (Simultaneous Localisation
and Mapping), it is central to applications spanning domestic and industrial
robotics, autonomous vehicles, virtual and augmented reality. This paper
describes the results of a major research effort to assemble the algorithms,
architectures, tools, and systems software needed to enable delivery of SLAM,
by supporting applications specialists in selecting and configuring the
appropriate algorithm and the appropriate hardware, and compilation pathway, to
meet their performance, accuracy, and energy consumption goals. The major
contributions we present are (1) tools and methodology for systematic
quantitative evaluation of SLAM algorithms, (2) automated,
machine-learning-guided exploration of the algorithmic and implementation
design space with respect to multiple objectives, (3) end-to-end simulation
tools to enable optimisation of heterogeneous, accelerated architectures for
the specific algorithmic requirements of the various SLAM algorithmic
approaches, and (4) tools for delivering, where appropriate, accelerated,
adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",arxiv
http://arxiv.org/abs/1808.05819v3,2020-03-04T06:35:56Z,2018-08-17T10:37:51Z,"Uncertainty-aware Short-term Motion Prediction of Traffic Actors for
  Autonomous Driving","We address one of the crucial aspects necessary for safe and efficient
operations of autonomous vehicles, namely predicting future state of traffic
actors in the autonomous vehicle's surroundings. We introduce a deep
learning-based approach that takes into account a current world state and
produces raster images of each actor's vicinity. The rasters are then used as
inputs to deep convolutional models to infer future movement of actors while
also accounting for and capturing inherent uncertainty of the prediction task.
Extensive experiments on real-world data strongly suggest benefits of the
proposed approach. Moreover, following completion of the offline tests the
system was successfully tested onboard self-driving vehicles.",arxiv
http://arxiv.org/abs/1909.10363v2,2019-09-26T17:08:30Z,2019-09-23T13:47:38Z,Shadow Transfer: Single Image Relighting For Urban Road Scenes,"Illumination effects in images, specifically cast shadows and shading, have
been shown to decrease the performance of deep neural networks on a large
number of vision-based detection, recognition and segmentation tasks in urban
driving scenes. A key factor that contributes to this performance gap is the
lack of `time-of-day' diversity within real, labeled datasets. There have been
impressive advances in the realm of image to image translation in transferring
previously unseen visual effects into a dataset, specifically in day to night
translation. However, it is not easy to constrain what visual effects, let
alone illumination effects, are transferred from one dataset to another during
the training process. To address this problem, we propose deep learning
framework, called Shadow Transfer, that can relight complex outdoor scenes by
transferring realistic shadow, shading, and other lighting effects onto a
single image. The novelty of the proposed framework is that it is both
self-supervised, and is designed to operate on sensor and label information
that is easily available in autonomous vehicle datasets. We show the
effectiveness of this method on both synthetic and real datasets, and we
provide experiments that demonstrate that the proposed method produces images
of higher visual quality than state of the art image to image translation
methods.",arxiv
http://arxiv.org/abs/1809.04734v2,2019-01-15T19:07:31Z,2018-09-13T01:36:55Z,"DispSegNet: Leveraging Semantics for End-to-End Learning of Disparity
  Estimation from Stereo Imagery","Recent work has shown that convolutional neural networks (CNNs) can be
applied successfully in disparity estimation, but these methods still suffer
from errors in regions of low-texture, occlusions and reflections.
Concurrently, deep learning for semantic segmentation has shown great progress
in recent years. In this paper, we design a CNN architecture that combines
these two tasks to improve the quality and accuracy of disparity estimation
with the help of semantic segmentation. Specifically, we propose a network
structure in which these two tasks are highly coupled. One key novelty of this
approach is the two-stage refinement process. Initial disparity estimates are
refined with an embedding learned from the semantic segmentation branch of the
network. The proposed model is trained using an unsupervised approach, in which
images from one half of the stereo pair are warped and compared against images
from the other camera. Another key advantage of the proposed approach is that a
single network is capable of outputting disparity estimates and semantic
labels. These outputs are of great use in autonomous vehicle operation; with
real-time constraints being key, such performance improvements increase the
viability of driving applications. Experiments on KITTI and Cityscapes datasets
show that our model can achieve state-of-the-art results and that leveraging
embedding learned from semantic segmentation improves the performance of
disparity estimation.",arxiv
http://arxiv.org/abs/1803.06077v2,2018-08-31T09:16:33Z,2018-03-16T05:29:12Z,"Real-time Detection, Tracking, and Classification of Moving and
  Stationary Objects using Multiple Fisheye Images","The ability to detect pedestrians and other moving objects is crucial for an
autonomous vehicle. This must be done in real-time with minimum system
overhead. This paper discusses the implementation of a surround view system to
identify moving as well as static objects that are close to the ego vehicle.
The algorithm works on 4 views captured by fisheye cameras which are merged
into a single frame. The moving object detection and tracking solution uses
minimal system overhead to isolate regions of interest (ROIs) containing moving
objects. These ROIs are then analyzed using a deep neural network (DNN) to
categorize the moving object. With deployment and testing on a real car in
urban environments, we have demonstrated the practical feasibility of the
solution. The video demos of our algorithm have been uploaded to Youtube:
https://youtu.be/vpoCfC724iA, https://youtu.be/2X4aqH2bMBs",arxiv
http://arxiv.org/abs/1711.06976v4,2019-08-14T11:17:00Z,2017-11-19T06:46:21Z,"MIT Advanced Vehicle Technology Study: Large-Scale Naturalistic Driving
  Study of Driver Behavior and Interaction with Automation","For the foreseeble future, human beings will likely remain an integral part
of the driving task, monitoring the AI system as it performs anywhere from just
over 0% to just under 100% of the driving. The governing objectives of the MIT
Autonomous Vehicle Technology (MIT-AVT) study are to (1) undertake large-scale
real-world driving data collection that includes high-definition video to fuel
the development of deep learning based internal and external perception
systems, (2) gain a holistic understanding of how human beings interact with
vehicle automation technology by integrating video data with vehicle state
data, driver characteristics, mental models, and self-reported experiences with
technology, and (3) identify how technology and other factors related to
automation adoption and use can be improved in ways that save lives. In
pursuing these objectives, we have instrumented 23 Tesla Model S and Model X
vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6
vehicles for both long-term (over a year per driver) and medium term (one month
per driver) naturalistic driving data collection. Furthermore, we are
continually developing new methods for analysis of the massive-scale dataset
collected from the instrumented vehicle fleet. The recorded data streams
include IMU, GPS, CAN messages, and high-definition video streams of the driver
face, the driver cabin, the forward roadway, and the instrument cluster (on
select vehicles). The study is on-going and growing. To date, we have 122
participants, 15,610 days of participation, 511,638 miles, and 7.1 billion
video frames. This paper presents the design of the study, the data collection
hardware, the processing of the data, and the computer vision algorithms
currently being used to extract actionable knowledge from the data.",arxiv
http://arxiv.org/abs/2006.00049v1,2020-05-29T19:42:25Z,2020-05-29T19:42:25Z,PointNet on FPGA for Real-Time LiDAR Point Cloud Processing,"LiDAR sensors have been widely used in many autonomous vehicle modalities,
such as perception, mapping, and localization. This paper presents an
FPGA-based deep learning platform for real-time point cloud processing targeted
on autonomous vehicles. The software driver for the Velodyne LiDAR sensor is
modified and moved into the on-chip processor system, while the programmable
logic is designed as a customized hardware accelerator. As the state-of-art
deep learning algorithm for point cloud processing, PointNet is successfully
implemented on the proposed FPGA platform. Targeted on a Xilinx Zynq
UltraScale+ MPSoC ZCU104 development board, the FPGA implementations of
PointNet achieve the computing performance of 182.1 GOPS and 280.0 GOPS for
classification and segmentation respectively. The proposed design can support
an input up to 4096 points per frame. The processing time is 19.8 ms for
classification and 34.6 ms for segmentation, which meets the real-time
requirement for most of the existing LiDAR sensors.",arxiv
http://arxiv.org/abs/2005.05441v2,2020-08-29T01:27:43Z,2020-05-11T21:21:50Z,"Delay-Aware Multi-Agent Reinforcement Learning for Cooperative and
  Competitive Environments","Action and observation delays exist prevalently in the real-world
cyber-physical systems which may pose challenges in reinforcement learning
design. It is particularly an arduous task when handling multi-agent systems
where the delay of one agent could spread to other agents. To resolve this
problem, this paper proposes a novel framework to deal with delays as well as
the non-stationary training issue of multi-agent tasks with model-free deep
reinforcement learning. We formally define the Delay-Aware Markov Game that
incorporates the delays of all agents in the environment. To solve Delay-Aware
Markov Games, we apply centralized training and decentralized execution that
allows agents to use extra information to ease the non-stationarity issue of
the multi-agent systems during training, without the need of a centralized
controller during execution. Experiments are conducted in multi-agent particle
environments including cooperative communication, cooperative navigation, and
competitive experiments. We also test the proposed algorithm in traffic
scenarios that require coordination of all autonomous vehicles to show the
practical value of delay-awareness. Results show that the proposed delay-aware
multi-agent reinforcement learning algorithm greatly alleviates the performance
degradation introduced by delay. Codes and demo videos are available at:
https://github.com/baimingc/delay-aware-MARL.",arxiv
http://arxiv.org/abs/2007.08501v1,2020-07-16T17:53:02Z,2020-07-16T17:53:02Z,Accelerating 3D Deep Learning with PyTorch3D,"Deep learning has significantly improved 2D image recognition. Extending into
3D may advance many new applications including autonomous vehicles, virtual and
augmented reality, authoring 3D content, and even improving 2D recognition.
However despite growing interest, 3D deep learning remains relatively
underexplored. We believe that some of this disparity is due to the engineering
challenges involved in 3D deep learning, such as efficiently processing
heterogeneous data and reframing graphics operations to be differentiable. We
address these challenges by introducing PyTorch3D, a library of modular,
efficient, and differentiable operators for 3D deep learning. It includes a
fast, modular differentiable renderer for meshes and point clouds, enabling
analysis-by-synthesis approaches. Compared with other differentiable renderers,
PyTorch3D is more modular and efficient, allowing users to more easily extend
it while also gracefully scaling to large meshes and images. We compare the
PyTorch3D operators and renderer with other implementations and demonstrate
significant speed and memory improvements. We also use PyTorch3D to improve the
state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D
images on ShapeNet. PyTorch3D is open-source and we hope it will help
accelerate research in 3D deep learning.",arxiv
http://arxiv.org/abs/2109.11661v1,2021-09-23T21:55:12Z,2021-09-23T21:55:12Z,Learning-Based Path Planning for Long-Range Autonomous Valet Parking,"In this paper, to reduce the congestion rate at the city center and increase
the quality of experience (QoE) of each user, the framework of long-range
autonomous valet parking (LAVP) is presented, where an Electric Autonomous
Vehicle (EAV) is deployed in the city, which can pick up, drop off users at
their required spots, and then drive to the car park out of city center
autonomously. In this framework, we aim to minimize the overall distance of the
EAV, while guarantee all users are served, i.e., picking up, and dropping off
users at their required spots through optimizing the path planning of the EAV
and number of serving time slots. To this end, we first propose a learning
based algorithm, which is named as Double-Layer Ant Colony Optimization
(DL-ACO) algorithm to solve the above problem in an iterative way. Then, to
make the real-time decision, while consider the dynamic environment (i.e., the
EAV may pick up and drop off users from different locations), we further
present a deep reinforcement learning (DRL) based algorithm, which is known as
deep Q network (DQN). The experimental results show that the DL-ACO and
DQN-based algorithms both achieve the considerable performance.",arxiv
http://arxiv.org/abs/2106.06369v1,2021-06-11T13:16:48Z,2021-06-11T13:16:48Z,"Courteous Behavior of Automated Vehicles at Unsignalized Intersections
  via Reinforcement Learning","The transition from today's mostly human-driven traffic to a purely automated
one will be a gradual evolution, with the effect that we will likely experience
mixed traffic in the near future. Connected and automated vehicles can benefit
human-driven ones and the whole traffic system in different ways, for example
by improving collision avoidance and reducing traffic waves. Many studies have
been carried out to improve intersection management, a significant bottleneck
in traffic, with intelligent traffic signals or exclusively automated vehicles.
However, the problem of how to improve mixed traffic at unsignalized
intersections has received less attention. In this paper, we propose a novel
approach to optimizing traffic flow at intersections in mixed traffic
situations using deep reinforcement learning. Our reinforcement learning agent
learns a policy for a centralized controller to let connected autonomous
vehicles at unsignalized intersections give up their right of way and yield to
other vehicles to optimize traffic flow. We implemented our approach and tested
it in the traffic simulator SUMO based on simulated and real traffic data. The
experimental evaluation demonstrates that our method significantly improves
traffic flow through unsignalized intersections in mixed traffic settings and
also provides better performance on a wide range of traffic situations compared
to the state-of-the-art traffic signal controller for the corresponding
signalized intersection.",arxiv
http://arxiv.org/abs/2109.02529v2,2021-09-07T04:46:01Z,2021-09-06T15:12:17Z,"ViSTA: a Framework for Virtual Scenario-based Testing of Autonomous
  Vehicles","In this paper, we present ViSTA, a framework for Virtual Scenario-based
Testing of Autonomous Vehicles (AV), developed as part of the 2021 IEEE
Autonomous Test Driving AI Test Challenge. Scenario-based virtual testing aims
to construct specific challenges posed for the AV to overcome, albeit in
virtual test environments that may not necessarily resemble the real world.
This approach is aimed at identifying specific issues that arise safety
concerns before an actual deployment of the AV on the road. In this paper, we
describe a comprehensive test case generation approach that facilitates the
design of special-purpose scenarios with meaningful parameters to form test
cases, both in automated and manual ways, leveraging the strength and
weaknesses of either. Furthermore, we describe how to automate the execution of
test cases, and analyze the performance of the AV under these test cases.",arxiv
http://arxiv.org/abs/2001.00048v1,2019-12-31T19:41:59Z,2019-12-31T19:41:59Z,"MIR-Vehicle: Cost-Effective Research Platform for Autonomous Vehicle
  Applications","This paper illustrates the MIR (Mobile Intelligent Robotics) Vehicle: a
feasible option of transforming an electric ride-on-car into a modular Graphics
Processing Unit (GPU) powered autonomous platform equipped with the capability
that supports test and deployment of various intelligent autonomous vehicles
algorithms. To use a platform for research, two components must be provided:
perception and control. The sensors such as incremental encoders, an Inertial
Measurement Unit (IMU), a camera, and a LIght Detection And Ranging (LIDAR)
must be able to be installed on the platform to add the capability of
environmental perception. A microcontroller-powered control box is designed to
properly respond to the environmental changes by regulating drive and steering
motors. This drive-by-wire capability is controlled by a GPU powered laptop
computer where high-level perception algorithms are processed and complex
actions are generated by various methods including behavior cloning using deep
neural networks. The main goal of this paper is to provide an adequate and
comprehensive approach for fabricating a cost-effective platform that would
contribute to the research quality from the wider community. The proposed
platform is to use a modular and hierarchical software architecture where the
lower and simpler motor controls are taken care of by microcontroller programs,
and the higher and complex algorithms are processed by a GPU powered laptop
computer. The platform uses the Robot Operating System (ROS) as middleware to
maintain the modularity of the perceptions and decision-making modules. It is
expected that the level three and above autonomous vehicle systems and Advanced
Driver Assistance Systems (ADAS) can be tested on and deployed to the platform
with a decent real-time system behavior due to the capabilities and
affordability of the proposed platform.",arxiv
http://arxiv.org/abs/2007.13371v1,2020-07-27T08:42:07Z,2020-07-27T08:42:07Z,"Building Trust in Autonomous Vehicles: Role of Virtual Reality Driving
  Simulators in HMI Design","The investigation of factors contributing at making humans trust Autonomous
Vehicles (AVs) will play a fundamental role in the adoption of such technology.
The user's ability to form a mental model of the AV, which is crucial to
establish trust, depends on effective user-vehicle communication; thus, the
importance of Human-Machine Interaction (HMI) is poised to increase. In this
work, we propose a methodology to validate the user experience in AVs based on
continuous, objective information gathered from physiological signals, while
the user is immersed in a Virtual Reality-based driving simulation. We applied
this methodology to the design of a head-up display interface delivering visual
cues about the vehicle' sensory and planning systems. Through this approach, we
obtained qualitative and quantitative evidence that a complete picture of the
vehicle's surrounding, despite the higher cognitive load, is conducive to a
less stressful experience. Moreover, after having been exposed to a more
informative interface, users involved in the study were also more willing to
test a real AV. The proposed methodology could be extended by adjusting the
simulation environment, the HMI and/or the vehicle's Artificial Intelligence
modules to dig into other aspects of the user experience.",arxiv
http://arxiv.org/abs/2008.00706v1,2020-08-03T08:19:57Z,2020-08-03T08:19:57Z,LiDAR point-cloud processing based on projection methods: a comparison,"An accurate and rapid-response perception system is fundamental for
autonomous vehicles to operate safely. 3D object detection methods handle point
clouds given by LiDAR sensors to provide accurate depth and position
information for each detection, together with its dimensions and
classification. The information is then used to track vehicles and other
obstacles in the surroundings of the autonomous vehicle, and also to feed
control units that guarantee collision avoidance and motion planning. Nowadays,
object detection systems can be divided into two main categories. The first
ones are the geometric based, which retrieve the obstacles using geometric and
morphological operations on the 3D points. The seconds are the deep
learning-based, which process the 3D points, or an elaboration of the 3D
point-cloud, with deep learning techniques to retrieve a set of obstacles. This
paper presents a comparison between those two approaches, presenting one
implementation of each class on a real autonomous vehicle. Accuracy of the
estimates of the algorithms has been evaluated with experimental tests carried
in the Monza ENI circuit. The position of the ego vehicle and the obstacle is
given by GPS sensors with RTK correction, which guarantees an accurate ground
truth for the comparison. Both algorithms have been implemented on ROS and run
on a consumer laptop.",arxiv
http://arxiv.org/abs/2002.10570v2,2020-06-27T14:29:00Z,2020-02-24T22:17:25Z,"Real-time Fusion Network for RGB-D Semantic Segmentation Incorporating
  Unexpected Obstacle Detection for Road-driving Images","Semantic segmentation has made striking progress due to the success of deep
convolutional neural networks. Considering the demands of autonomous driving,
real-time semantic segmentation has become a research hotspot these years.
However, few real-time RGB-D fusion semantic segmentation studies are carried
out despite readily accessible depth information nowadays. In this paper, we
propose a real-time fusion semantic segmentation network termed RFNet that
effectively exploits complementary cross-modal information. Building on an
efficient network architecture, RFNet is capable of running swiftly, which
satisfies autonomous vehicles applications. Multi-dataset training is leveraged
to incorporate unexpected small obstacle detection, enriching the recognizable
classes required to face unforeseen hazards in the real world. A comprehensive
set of experiments demonstrates the effectiveness of our framework. On
Cityscapes, Our method outperforms previous state-of-the-art semantic
segmenters, with excellent accuracy and 22Hz inference speed at the full
2048x1024 resolution, outperforming most existing RGB-D networks.",arxiv
http://arxiv.org/abs/2012.05032v1,2020-12-09T13:21:39Z,2020-12-09T13:21:39Z,"ReCoG: A Deep Learning Framework with Heterogeneous Graph for
  Interaction-Aware Trajectory Prediction","Predicting the future trajectory of surrounding vehicles is essential for the
navigation of autonomous vehicles in complex real-world driving scenarios. It
is challenging as a vehicle's motion is affected by many factors, including its
surrounding infrastructures and vehicles. In this work, we develop the ReCoG
(Recurrent Convolutional and Graph Neural Networks), which is a general scheme
that represents vehicle interactions with infrastructure information as a
heterogeneous graph and applies graph neural networks (GNNs) to model the
high-level interactions for trajectory prediction. Nodes in the graph contain
corresponding features, where a vehicle node contains its sequential feature
encoded using Recurrent Neural Network (RNN), and an infrastructure node
contains spatial feature encoded using Convolutional Neural Network (CNN). Then
the ReCoG predicts the future trajectory of the target vehicle by jointly
considering all of the features. Experiments are conducted by using the
INTERACTION dataset. Experimental results show that the proposed ReCoG
outperforms other state-of-the-art methods in terms of different types of
displacement error, validating the feasibility and effectiveness of the
developed approach.",arxiv
http://arxiv.org/abs/2009.03268v2,2020-10-10T14:16:31Z,2020-09-07T17:34:01Z,"Driving Tasks Transfer in Deep Reinforcement Learning for
  Decision-making of Autonomous Vehicles","Knowledge transfer is a promising concept to achieve real-time
decision-making for autonomous vehicles. This paper constructs a transfer deep
reinforcement learning framework to transform the driving tasks in
inter-section environments. The driving missions at the un-signalized
intersection are cast into a left turn, right turn, and running straight for
automated vehicles. The goal of the autonomous ego vehicle (AEV) is to drive
through the intersection situation efficiently and safely. This objective
promotes the studied vehicle to increase its speed and avoid crashing other
vehicles. The decision-making pol-icy learned from one driving task is
transferred and evaluated in another driving mission. Simulation results reveal
that the decision-making strategies related to similar tasks are transferable.
It indicates that the presented control framework could reduce the time
consumption and realize online implementation.",arxiv
http://arxiv.org/abs/2011.11190v1,2020-11-23T03:13:26Z,2020-11-23T03:13:26Z,"Attentional-GCNN: Adaptive Pedestrian Trajectory Prediction towards
  Generic Autonomous Vehicle Use Cases","Autonomous vehicle navigation in shared pedestrian environments requires the
ability to predict future crowd motion both accurately and with minimal delay.
Understanding the uncertainty of the prediction is also crucial. Most existing
approaches however can only estimate uncertainty through repeated sampling of
generative models. Additionally, most current predictive models are trained on
datasets that assume complete observability of the crowd using an aerial view.
These are generally not representative of real-world usage from a vehicle
perspective, and can lead to the underestimation of uncertainty bounds when the
on-board sensors are occluded. Inspired by prior work in motion prediction
using spatio-temporal graphs, we propose a novel Graph Convolutional Neural
Network (GCNN)-based approach, Attentional-GCNN, which aggregates information
of implicit interaction between pedestrians in a crowd by assigning attention
weight in edges of the graph. Our model can be trained to either output a
probabilistic distribution or faster deterministic prediction, demonstrating
applicability to autonomous vehicle use cases where either speed or accuracy
with uncertainty bounds are required. To further improve the training of
predictive models, we propose an automatically labelled pedestrian dataset
collected from an intelligent vehicle platform representative of real-world
use. Through experiments on a number of datasets, we show our proposed method
achieves an improvement over the state of art by 10% Average Displacement Error
(ADE) and 12% Final Displacement Error (FDE) with fast inference speeds.",arxiv
http://arxiv.org/abs/2101.04319v1,2021-01-12T06:42:45Z,2021-01-12T06:42:45Z,"DeepiSign: Invisible Fragile Watermark to Protect the Integrityand
  Authenticity of CNN","Convolutional Neural Networks (CNNs) deployed in real-life applications such
as autonomous vehicles have shown to be vulnerable to manipulation attacks,
such as poisoning attacks and fine-tuning. Hence, it is essential to ensure the
integrity and authenticity of CNNs because compromised models can produce
incorrect outputs and behave maliciously. In this paper, we propose a
self-contained tamper-proofing method, called DeepiSign, to ensure the
integrity and authenticity of CNN models against such manipulation attacks.
DeepiSign applies the idea of fragile invisible watermarking to securely embed
a secret and its hash value into a CNN model. To verify the integrity and
authenticity of the model, we retrieve the secret from the model, compute the
hash value of the secret, and compare it with the embedded hash value. To
minimize the effects of the embedded secret on the CNN model, we use a
wavelet-based technique to transform weights into the frequency domain and
embed the secret into less significant coefficients. Our theoretical analysis
shows that DeepiSign can hide up to 1KB secret in each layer with minimal loss
of the model's accuracy. To evaluate the security and performance of DeepiSign,
we performed experiments on four pre-trained models (ResNet18, VGG16, AlexNet,
and MobileNet) using three datasets (MNIST, CIFAR-10, and Imagenet) against
three types of manipulation attacks (targeted input poisoning, output
poisoning, and fine-tuning). The results demonstrate that DeepiSign is
verifiable without degrading the classification accuracy, and robust against
representative CNN manipulation attacks.",arxiv
http://arxiv.org/abs/1901.00466v1,2019-01-02T17:57:50Z,2019-01-02T17:57:50Z,Learning Generalizable Physical Dynamics of 3D Rigid Objects,"Humans have a remarkable ability to predict the effect of physical
interactions on the dynamics of objects. Endowing machines with this ability
would allow important applications in areas like robotics and autonomous
vehicles. In this work, we focus on predicting the dynamics of 3D rigid
objects, in particular an object's final resting position and total rotation
when subjected to an impulsive force. Different from previous work, our
approach is capable of generalizing to unseen object shapes - an important
requirement for real-world applications. To achieve this, we represent object
shape as a 3D point cloud that is used as input to a neural network, making our
approach agnostic to appearance variation. The design of our network is
informed by an understanding of physical laws. We train our model with data
from a physics engine that simulates the dynamics of a large number of shapes.
Experiments show that we can accurately predict the resting position and total
rotation for unseen object geometries.",arxiv
http://arxiv.org/abs/2011.09045v2,2021-05-24T00:38:06Z,2020-11-18T02:34:26Z,"Double-Prong ConvLSTM for Spatiotemporal Occupancy Prediction in Dynamic
  Environments","Predicting the future occupancy state of an environment is important to
enable informed decisions for autonomous vehicles. Common challenges in
occupancy prediction include vanishing dynamic objects and blurred predictions,
especially for long prediction horizons. In this work, we propose a
double-prong neural network architecture to predict the spatiotemporal
evolution of the occupancy state. One prong is dedicated to predicting how the
static environment will be observed by the moving ego vehicle. The other prong
predicts how the dynamic objects in the environment will move. Experiments
conducted on the real-world Waymo Open Dataset indicate that the fused output
of the two prongs is capable of retaining dynamic objects and reducing
blurriness in the predictions for longer time horizons than baseline models.",arxiv
http://arxiv.org/abs/2005.07460v1,2020-05-15T10:34:51Z,2020-05-15T10:34:51Z,"Collective Risk Minimization via a Bayesian Model for Statistical
  Software Testing","In the last four years, the number of distinct autonomous vehicles platforms
deployed in the streets of California increased 6-fold, while the reported
accidents increased 12-fold. This can become a trend with no signs of subsiding
as it is fueled by a constant stream of innovations in hardware sensors and
machine learning software. Meanwhile, if we expect the public and regulators to
trust the autonomous vehicle platforms, we need to find better ways to solve
the problem of adding technological complexity without increasing the risk of
accidents. We studied this problem from the perspective of reliability
engineering in which a given risk of an accident has severity and probability
of occurring. Timely information on accidents is important for engineers to
anticipate and reuse previous failures to approximate the risk of accidents in
a new city. However, this is challenging in the context of autonomous vehicles
because of the sparse nature of data on the operational scenarios (driving
trajectories in a new city). Our approach was to mitigate data sparsity by
reducing the state space through monitoring of multiple-vehicles operations. We
then minimized the risk of accidents by determining proper allocation of tests
for each equivalence class. Our contributions comprise (1) a set of strategies
to monitor the operational data of multiple autonomous vehicles, (2) a Bayesian
model that estimates changes in the risk of accidents, and (3) a feedback
control-loop that minimizes these risks by reallocating test effort. Our
results are promising in the sense that we were able to measure and control
risk for a diversity of changes in the operational scenarios. We evaluated our
models with data from two real cities with distinct traffic patterns and made
the data available for the community.",arxiv
http://arxiv.org/abs/1803.07913v1,2018-03-21T13:37:24Z,2018-03-21T13:37:24Z,"HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object
  Classification","Event-based cameras have recently drawn the attention of the Computer Vision
community thanks to their advantages in terms of high temporal resolution, low
power consumption and high dynamic range, compared to traditional frame-based
cameras. These properties make event-based cameras an ideal choice for
autonomous vehicles, robot navigation or UAV vision, among others. However, the
accuracy of event-based object classification algorithms, which is of crucial
importance for any reliable system working in real-world conditions, is still
far behind their frame-based counterparts. Two main reasons for this
performance gap are: 1. The lack of effective low-level representations and
architectures for event-based object classification and 2. The absence of large
real-world event-based datasets. In this paper we address both problems. First,
we introduce a novel event-based feature representation together with a new
machine learning architecture. Compared to previous approaches, we use local
memory units to efficiently leverage past temporal information and build a
robust event-based representation. Second, we release the first large
real-world event-based dataset for object classification. We compare our method
to the state-of-the-art with extensive experiments, showing better
classification performance and real-time computation.",arxiv
http://arxiv.org/abs/1805.08986v1,2018-05-23T07:36:48Z,2018-05-23T07:36:48Z,Deep Object Tracking on Dynamic Occupancy Grid Maps Using RNNs,"The comprehensive representation and understanding of the driving environment
is crucial to improve the safety and reliability of autonomous vehicles. In
this paper, we present a new approach to establish an environment model
containing a segmentation between static and dynamic background and parametric
modeled objects with shape, position and orientation. Multiple laser scanners
are fused into a dynamic occupancy grid map resulting in a 360{\deg} perception
of the environment. A single-stage deep convolutional neural network is
combined with a recurrent neural network, which takes a time series of the
occupancy grid map as input and tracks cell states and its corresponding object
hypotheses. The labels for training are created unsupervised with an automatic
label generation algorithm.
  The proposed methods are evaluated in real-world experiments in complex inner
city scenarios using the aforementioned 360{\deg} laser perception. The results
show a better object detection accuracy in comparison with our old approach as
well as an AUC score of 0.946 for the dynamic and static segmentation.
Furthermore, we gain an improved detection for occluded objects and a more
consistent size estimation due to the usage of time series as input and the
memory about previous states introduced by the recurrent neural network.",arxiv
http://arxiv.org/abs/2006.03463v2,2021-05-12T14:17:37Z,2020-06-05T14:10:09Z,Sponge Examples: Energy-Latency Attacks on Neural Networks,"The high energy costs of neural network training and inference led to the use
of acceleration hardware such as GPUs and TPUs. While this enabled us to train
large-scale neural networks in datacenters and deploy them on edge devices, the
focus so far is on average-case performance. In this work, we introduce a novel
threat vector against neural networks whose energy consumption or decision
latency are critical. We show how adversaries can exploit carefully crafted
$\boldsymbol{sponge}~\boldsymbol{examples}$, which are inputs designed to
maximise energy consumption and latency.
  We mount two variants of this attack on established vision and language
models, increasing energy consumption by a factor of 10 to 200. Our attacks can
also be used to delay decisions where a network has critical real-time
performance, such as in perception for autonomous vehicles. We demonstrate the
portability of our malicious inputs across CPUs and a variety of hardware
accelerator chips including GPUs, and an ASIC simulator. We conclude by
proposing a defense strategy which mitigates our attack by shifting the
analysis of energy consumption in hardware from an average-case to a worst-case
perspective.",arxiv
http://arxiv.org/abs/1808.10134v1,2018-08-30T06:29:10Z,2018-08-30T06:29:10Z,"Baidu Apollo Auto-Calibration System - An Industry-Level Data-Driven and
  Learning based Vehicle Longitude Dynamic Calibrating Algorithm","For any autonomous driving vehicle, control module determines its road
performance and safety, i.e. its precision and stability should stay within a
carefully-designed range. Nonetheless, control algorithms require vehicle
dynamics (such as longitudinal dynamics) as inputs, which, unfortunately, are
obscure to calibrate in real time. As a result, to achieve reasonable
performance, most, if not all, research-oriented autonomous vehicles do manual
calibrations in a one-by-one fashion. Since manual calibration is not
sustainable once entering into mass production stage for industrial purposes,
we here introduce a machine-learning based auto-calibration system for
autonomous driving vehicles. In this paper, we will show how we build a
data-driven longitudinal calibration procedure using machine learning
techniques. We first generated offline calibration tables from human driving
data. The offline table serves as an initial guess for later uses and it only
needs twenty-minutes data collection and process. We then used an
online-learning algorithm to appropriately update the initial table (the
offline table) based on real-time performance analysis. This longitudinal
auto-calibration system has been deployed to more than one hundred Baidu Apollo
self-driving vehicles (including hybrid family vehicles and electronic
delivery-only vehicles) since April 2018. By August 27, 2018, it had been
tested for more than two thousands hours, ten thousands kilometers (6,213
miles) and yet proven to be effective.",arxiv
http://arxiv.org/abs/2102.12967v2,2021-11-11T08:44:36Z,2021-02-25T16:14:47Z,"A statistical framework for efficient out of distribution detection in
  deep neural networks","Background. Commonly, Deep Neural Networks (DNNs) generalize well on samples
drawn from a distribution similar to that of the training set. However, DNNs'
predictions are brittle and unreliable when the test samples are drawn from a
dissimilar distribution. This is a major concern for deployment in real-world
applications, where such behavior may come at a considerable cost, such as
industrial production lines, autonomous vehicles, or healthcare applications.
Contributions. We frame Out Of Distribution (OOD) detection in DNNs as a
statistical hypothesis testing problem. Tests generated within our proposed
framework combine evidence from the entire network. Unlike previous OOD
detection heuristics, this framework returns a $p$-value for each test sample.
It is guaranteed to maintain the Type I Error (T1E - mistakenly identifying OOD
samples as ID) for test data. Moreover, this allows combining several detectors
while maintaining the T1E. Building on this framework, we suggest a novel OOD
procedure based on low-order statistics. Our method achieves comparable or
better results than state-of-the-art methods on well-accepted OOD benchmarks,
without retraining the network parameters or assuming prior knowledge on the
test distribution -- and at a fraction of the computational cost.",arxiv
http://arxiv.org/abs/2108.08448v1,2021-08-19T02:33:43Z,2021-08-19T02:33:43Z,"Prior Is All You Need to Improve the Robustness and Safety for the First
  Time Deployment of Meta RL","The field of Meta Reinforcement Learning (Meta-RL) has seen substantial
advancements recently. In particular, off-policy methods were developed to
improve the data efficiency of Meta-RL techniques. \textit{Probabilistic
embeddings for actor-critic RL} (PEARL) is currently one of the leading
approaches for multi-MDP adaptation problems. A major drawback of many existing
Meta-RL methods, including PEARL, is that they do not explicitly consider the
safety of the prior policy when it is exposed to a new task for the very first
time. This is very important for some real-world applications, including field
robots and Autonomous Vehicles (AVs). In this paper, we develop the PEARL PLUS
(PEARL$^+$) algorithm, which optimizes the policy for both prior safety and
posterior adaptation. Building on top of PEARL, our proposed PEARL$^+$
algorithm introduces a prior regularization term in the reward function and a
new Q-network for recovering the state-action value with prior context
assumption, to improve the robustness and safety of the trained network
exposing to a new task for the first time. The performance of the PEARL$^+$
method is demonstrated by solving three safety-critical decision-making
problems related to robots and AVs, including two MuJoCo benchmark problems.
From the simulation experiments, we show that the safety of the prior policy is
significantly improved compared to that of the original PEARL method.",arxiv
http://arxiv.org/abs/2111.03201v2,2021-11-09T08:02:35Z,2021-11-05T00:16:06Z,"Compressing Sensor Data for Remote Assistance of Autonomous Vehicles
  using Deep Generative Models","In the foreseeable future, autonomous vehicles will require human assistance
in situations they can not resolve on their own. In such scenarios, remote
assistance from a human can provide the required input for the vehicle to
continue its operation. Typical sensors used in autonomous vehicles include
camera and lidar sensors. Due to the massive volume of sensor data that must be
sent in real-time, highly efficient data compression is elementary to prevent
an overload of network infrastructure. Sensor data compression using deep
generative neural networks has been shown to outperform traditional compression
approaches for both image and lidar data, regarding compression rate as well as
reconstruction quality. However, there is a lack of research about the
performance of generative-neural-network-based compression algorithms for
remote assistance. In order to gain insights into the feasibility of deep
generative models for usage in remote assistance, we evaluate state-of-the-art
algorithms regarding their applicability and identify potential weaknesses.
Further, we implement an online pipeline for processing sensor data and
demonstrate its performance for remote assistance using the CARLA simulator.",arxiv
http://arxiv.org/abs/2012.14618v4,2021-08-06T11:04:09Z,2020-12-29T05:58:35Z,FPCC: Fast Point Cloud Clustering for Instance Segmentation,"Instance segmentation is an important pre-processing task in numerous
real-world applications, such as robotics, autonomous vehicles, and
human-computer interaction. Compared with the rapid development of deep
learning for two-dimensional (2D) image tasks, deep learning-based instance
segmentation of 3D point cloud still has a lot of room for development. In
particular, distinguishing a large number of occluded objects of the same class
is a highly challenging problem, which is seen in a robotic bin-picking. In a
usual bin-picking scene, many indentical objects are stacked together and the
model of the objects is known. Thus, the semantic information can be ignored;
instead, the focus in the bin-picking is put on the segmentation of instances.
Based on this task requirement, we propose a Fast Point Cloud Clustering (FPCC)
for instance segmentation of bin-picking scene. FPCC includes a network named
FPCC-Net and a fast clustering algorithm. FPCC-net has two subnets, one for
inferring the geometric centers for clustering and the other for describing
features of each point. FPCC-Net extracts features of each point and infers
geometric center points of each instance simultaneously. After that, the
proposed clustering algorithm clusters the remaining points to the closest
geometric center in feature embedding space. Experiments show that FPCC also
surpasses the existing works in bin-picking scenes and is more computationally
efficient. Our code and data are available at https://github.com/xyjbaal/FPCC.",arxiv
http://arxiv.org/abs/1405.5581v1,2014-05-22T00:30:20Z,2014-05-22T00:30:20Z,"Real-Time Predictive Modeling and Robust Avoidance of Pedestrians with
  Uncertain, Changing Intentions","To plan safe trajectories in urban environments, autonomous vehicles must be
able to quickly assess the future intentions of dynamic agents. Pedestrians are
particularly challenging to model, as their motion patterns are often uncertain
and/or unknown a priori. This paper presents a novel changepoint detection and
clustering algorithm that, when coupled with offline unsupervised learning of a
Gaussian process mixture model (DPGP), enables quick detection of changes in
intent and online learning of motion patterns not seen in prior training data.
The resulting long-term movement predictions demonstrate improved accuracy
relative to offline learning alone, in terms of both intent and trajectory
prediction. By embedding these predictions within a chance-constrained motion
planner, trajectories which are probabilistically safe to pedestrian motions
can be identified in real-time. Hardware experiments demonstrate that this
approach can accurately predict pedestrian motion patterns from onboard
sensor/perception data and facilitate robust navigation within a dynamic
environment.",arxiv
http://arxiv.org/abs/2109.12155v1,2021-09-24T19:20:00Z,2021-09-24T19:20:00Z,"Learning-based Initialization Strategy for Safety of Multi-Vehicle
  Systems","Multi-vehicle collision avoidance is a highly crucial problem due to the
soaring interests of introducing autonomous vehicles into the real world in
recent years. The safety of these vehicles while they complete their objectives
is of paramount importance. Hamilton-Jacobi (HJ) reachability is a promising
tool for guaranteeing safety for low-dimensional systems. However, due to its
exponential complexity in computation time, no reachability-based methods have
been able to guarantee safety for more than three vehicles successfully in
unstructured scenarios. For systems with four or more vehicles,we can only
empirically validate their safety performance.While reachability-based safety
methods enjoy a flexible least-restrictive control strategy, it is challenging
to reason about long-horizon trajectories online because safety at any given
state is determined by looking up its safety value in a pre-computed table that
does not exhibit favorable properties that continuous functions have. This
motivates the problem of improving the safety performance of unstructured
multi-vehicle systems when safety cannot be guaranteed given any
least-restrictive safety-aware collision avoidance algorithm while avoiding
online trajectory optimization. In this paper, we propose a novel approach
using supervised learning to enhance the safety of vehicles by proposing new
initial states in very close neighborhood of the original initial states of
vehicles. Our experiments demonstrate the effectiveness of our proposed
approach and show that vehicles are able to get to their goals with better
safety performance with our approach compared to a baseline approach in
wide-ranging scenarios.",arxiv
http://arxiv.org/abs/1907.05418v1,2019-07-11T17:59:13Z,2019-07-11T17:59:13Z,Adversarial Objects Against LiDAR-Based Autonomous Driving Systems,"Deep neural networks (DNNs) are found to be vulnerable against adversarial
examples, which are carefully crafted inputs with a small magnitude of
perturbation aiming to induce arbitrarily incorrect predictions. Recent studies
show that adversarial examples can pose a threat to real-world
security-critical applications: a ""physical adversarial Stop Sign"" can be
synthesized such that the autonomous driving cars will misrecognize it as
others (e.g., a speed limit sign). However, these image-space adversarial
examples cannot easily alter 3D scans of widely equipped LiDAR or radar on
autonomous vehicles. In this paper, we reveal the potential vulnerabilities of
LiDAR-based autonomous driving detection systems, by proposing an optimization
based approach LiDAR-Adv to generate adversarial objects that can evade the
LiDAR-based detection system under various conditions. We first show the
vulnerabilities using a blackbox evolution-based algorithm, and then explore
how much a strong adversary can do, using our gradient-based approach
LiDAR-Adv. We test the generated adversarial objects on the Baidu Apollo
autonomous driving platform and show that such physical systems are indeed
vulnerable to the proposed attacks. We also 3D-print our adversarial objects
and perform physical experiments to illustrate that such vulnerability exists
in the real world. Please find more visualizations and results on the anonymous
website: https://sites.google.com/view/lidar-adv.",arxiv
http://arxiv.org/abs/1911.04096v1,2019-11-11T06:18:04Z,2019-11-11T06:18:04Z,"UW-MARL: Multi-Agent Reinforcement Learning for Underwater Adaptive
  Sampling using Autonomous Vehicles","Near-real-time water-quality monitoring in uncertain environments such as
rivers, lakes, and water reservoirs of different variables is critical to
protect the aquatic life and to prevent further propagation of the potential
pollution in the water. In order to measure the physical values in a region of
interest, adaptive sampling is helpful as an energy- and time-efficient
technique since an exhaustive search of an area is not feasible with a single
vehicle. We propose an adaptive sampling algorithm using multiple autonomous
vehicles, which are well-trained, as agents, in a Multi-Agent Reinforcement
Learning (MARL) framework to make efficient sequence of decisions on the
adaptive sampling procedure. The proposed solution is evaluated using
experimental data, which is fed into a simulation framework. Experiments were
conducted in the Raritan River, Somerset and in Carnegie Lake, Princeton, NJ
during July 2019.",arxiv
http://arxiv.org/abs/1812.08273v1,2018-12-19T22:25:52Z,2018-12-19T22:25:52Z,Analog Signal Processing Using Stochastic Magnets,"We present a low barrier magnet based compact hardware unit for analog
stochastic neurons and demonstrate its use as a building-block for neuromorphic
hardware. By coupling circular magnetic tunnel junctions (MTJs) with a CMOS
based analog buffer, we show that these units can act as leaky-integrate-and
fire (LIF) neurons, a model of biological neural networks particularly suited
for temporal inferencing and pattern recognition. We demonstrate examples of
temporal sequence learning, processing, and prediction tasks in real time, as a
proof of concept demonstration of scalable and adaptive signal-processors.
Efficient non von-Neumann hardware implementation of such processors can open
up a pathway for integration of hardware based cognition in a wide variety of
emerging systems such as IoT, industrial controls, bio- and photo-sensors, and
Unmanned Autonomous Vehicles.",arxiv
http://arxiv.org/abs/1907.11394v1,2019-07-26T06:36:18Z,2019-07-26T06:36:18Z,"A Comparative Study of High-Recall Real-Time Semantic Segmentation Based
  on Swift Factorized Network","Semantic Segmentation (SS) is the task to assign a semantic label to each
pixel of the observed images, which is of crucial significance for autonomous
vehicles, navigation assistance systems for the visually impaired, and
augmented reality devices. However, there is still a long way for SS to be put
into practice as there are two essential challenges that need to be addressed:
efficiency and evaluation criterions for practical application. For specific
application scenarios, different criterions need to be adopted. Recall rate is
an important criterion for many tasks like autonomous vehicles. For autonomous
vehicles, we need to focus on the detection of the traffic objects like cars,
buses, and pedestrians, which should be detected with high recall rates. In
other words, it is preferable to detect it wrongly than miss it, because the
other traffic objects will be dangerous if the algorithm miss them and segment
them as safe roadways. In this paper, our main goal is to explore possible
methods to attain high recall rate. Firstly, we propose a real-time SS network
named Swift Factorized Network (SFN). The proposed network is adapted from
SwiftNet, whose structure is a typical U-shape structure with lateral
connections. Inspired by ERFNet and Global convolution Networks (GCNet), we
propose two different blocks to enlarge valid receptive field. They do not take
up too much calculation resources, but significantly enhance the performance
compared with the baseline network. Secondly, we explore three ways to achieve
higher recall rate, i.e. loss function, classifier and decision rules. We
perform a comprehensive set of experiments on state-of-the-art datasets
including CamVid and Cityscapes. We demonstrate that our SS convolutional
neural networks reach excellent performance. Furthermore, we make a detailed
analysis and comparison of the three proposed methods on the promotion of
recall rate.",arxiv
http://arxiv.org/abs/1808.03506v4,2019-03-05T23:50:01Z,2018-08-10T12:30:03Z,"ChipNet: Real-Time LiDAR Processing for Drivable Region Segmentation on
  an FPGA","This paper presents a field-programmable gate array (FPGA) design of a
segmentation algorithm based on convolutional neural network (CNN) that can
process light detection and ranging (LiDAR) data in real-time. For autonomous
vehicles, drivable region segmentation is an essential step that sets up the
static constraints for planning tasks. Traditional drivable region segmentation
algorithms are mostly developed on camera data, so their performance is
susceptible to the light conditions and the qualities of road markings. LiDAR
sensors can obtain the 3D geometry information of the vehicle surroundings with
high precision. However, it is a computational challenge to process a large
amount of LiDAR data in real-time. In this paper, a convolutional neural
network model is proposed and trained to perform semantic segmentation using
data from the LiDAR sensor. An efficient hardware architecture is proposed and
implemented on an FPGA that can process each LiDAR scan in 17.59 ms, which is
much faster than the previous works. Evaluated using Ford and KITTI road
detection benchmarks, the proposed solution achieves both high accuracy in
performance and real-time processing in speed.",arxiv
http://arxiv.org/abs/1711.02757v1,2017-11-07T22:42:09Z,2017-11-07T22:42:09Z,Real-Time Road Segmentation Using LiDAR Data Processing on an FPGA,"This paper presents the FPGA design of a convolutional neural network (CNN)
based road segmentation algorithm for real-time processing of LiDAR data. For
autonomous vehicles, it is important to perform road segmentation and obstacle
detection such that the drivable region can be identified for path planning.
Traditional road segmentation algorithms are mainly based on image data from
cameras, which is subjected to the light condition as well as the quality of
road markings. LiDAR sensor can obtain the 3D geometry information of the
vehicle surroundings with very high accuracy. However, it is a computational
challenge to process a large amount of LiDAR data at real-time. In this work, a
convolutional neural network model is proposed and trained to perform semantic
segmentation using the LiDAR sensor data. Furthermore, an efficient hardware
design is implemented on the FPGA that can process each LiDAR scan in 16.9ms,
which is much faster than the previous works. Evaluated using KITTI road
benchmarks, the proposed solution achieves high accuracy of road segmentation.",arxiv
http://arxiv.org/abs/2102.03326v1,2021-02-05T18:14:36Z,2021-02-05T18:14:36Z,"Fusion of neural networks, for LIDAR-based evidential road mapping","LIDAR sensors are usually used to provide autonomous vehicles with 3D
representations of their environment. In ideal conditions, geometrical models
could detect the road in LIDAR scans, at the cost of a manual tuning of
numerical constraints, and a lack of flexibility. We instead propose an
evidential pipeline, to accumulate road detection results obtained from neural
networks. First, we introduce RoadSeg, a new convolutional architecture that is
optimized for road detection in LIDAR scans. RoadSeg is used to classify
individual LIDAR points as either belonging to the road, or not. Yet, such
point-level classification results need to be converted into a dense
representation, that can be used by an autonomous vehicle. We thus secondly
present an evidential road mapping algorithm, that fuses consecutive road
detection results. We benefitted from a reinterpretation of logistic
classifiers, which can be seen as generating a collection of simple evidential
mass functions. An evidential grid map that depicts the road can then be
obtained, by projecting the classification results from RoadSeg into grid
cells, and by handling moving objects via conflict analysis. The system was
trained and evaluated on real-life data. A python implementation maintains a 10
Hz framerate. Since road labels were needed for training, a soft labelling
procedure, relying lane-level HD maps, was used to generate coarse training and
validation sets. An additional test set was manually labelled for evaluation
purposes. So as to reach satisfactory results, the system fuses road detection
results obtained from three variants of RoadSeg, processing different LIDAR
features.",arxiv
http://arxiv.org/abs/1907.01038v1,2019-07-01T19:42:37Z,2019-07-01T19:42:37Z,AVFI: Fault Injection for Autonomous Vehicles,"Autonomous vehicle (AV) technology is rapidly becoming a reality on U.S.
roads, offering the promise of improvements in traffic management, safety, and
the comfort and efficiency of vehicular travel. With this increasing popularity
and ubiquitous deployment, resilience has become a critical requirement for
public acceptance and adoption. Recent studies into the resilience of AVs have
shown that though the AV systems are improving over time, they have not reached
human levels of automation. Prior work in this area has studied the safety and
resilience of individual components of the AV system (e.g., testing of neural
networks powering the perception function). However, methods for holistic
end-to-end resilience assessment of AV systems are still non-existent.",arxiv
http://arxiv.org/abs/2106.02268v1,2021-06-04T05:27:09Z,2021-06-04T05:27:09Z,"A C-V2X Platform Using Transportation Data and Spectrum-Aware Sidelink
  Access","Intelligent transportation systems and autonomous vehicles are expected to
bring new experiences with enhanced efficiency and safety to road users in the
near future. However, an efficient and robust vehicular communication system
should act as a strong backbone to offer the needed infrastructure
connectivity. Deep learning (DL)-based algorithms are widely adopted recently
in various vehicular communication applications due to their achieved low
latency and fast reconfiguration properties. Yet, collecting actual and
sufficient transportation data to train DL-based vehicular communication models
is costly and complex. This paper introduces a cellular vehicle-to-everything
(C-V2X) verification platform based on an actual traffic simulator and
spectrum-aware access. This integrated platform can generate realistic
transportation and communication data, benefiting the development and
adaptivity of DL-based solutions. Accordingly, vehicular spectrum recognition
and management are further investigated to demonstrate the potentials of
dynamic slidelink access. Numerical results show that our platform can
effectively train and realize DL-based C-V2X algorithms. The developed
slidelink communication can adopt different operating bands with remarkable
spectrum detection performance, validating its practicality in real-world
vehicular environments.",arxiv
http://arxiv.org/abs/2104.06826v1,2021-04-14T12:57:40Z,2021-04-14T12:57:40Z,Towards Unsupervised Fine-Tuning for Edge Video Analytics,"Judging by popular and generic computer vision challenges, such as the
ImageNet or PASCAL VOC, neural networks have proven to be exceptionally
accurate in recognition tasks. However, state-of-the-art accuracy often comes
at a high computational price, requiring equally state-of-the-art and high-end
hardware acceleration to achieve anything near real-time performance. At the
same time, use cases such as smart cities or autonomous vehicles require an
automated analysis of images from fixed cameras in real-time. Due to the huge
and constant amount of network bandwidth these streams would generate, we
cannot rely on offloading compute to the omnipresent and omnipotent cloud.
Therefore, a distributed Edge Cloud must be in charge to process images
locally. However, the Edge Cloud is, by nature, resource-constrained, which
puts a limit on the computational complexity of the models executed in the
edge. Nonetheless, there is a need for a meeting point between the Edge Cloud
and accurate real-time video analytics. In this paper, we propose a method for
improving accuracy of edge models without any extra compute cost by means of
automatic model specialization. First, we show how the sole assumption of
static cameras allows us to make a series of considerations that greatly
simplify the scope of the problem. Then, we present Edge AutoTuner, a framework
that implements and brings these considerations together to automate the
end-to-end fine-tuning of models. Finally, we show that complex neural networks
- able to generalize better - can be effectively used as teachers to annotate
datasets for the fine-tuning of lightweight neural networks and tailor them to
the specific edge context, which boosts accuracy at constant computational
cost, and do so without any human interaction. Results show that our method can
automatically improve accuracy of pre-trained models by an average of 21%.",arxiv
http://arxiv.org/abs/1904.09007v2,2019-07-19T09:24:45Z,2019-04-18T20:41:10Z,"DeepLocalization: Landmark-based Self-Localization with Deep Neural
  Networks","We address the problem of vehicle self-localization from multi-modal sensor
information and a reference map. The map is generated off-line by extracting
landmarks from the vehicle's field of view, while the measurements are
collected similarly on the fly. Our goal is to determine the autonomous
vehicle's pose from the landmark measurements and map landmarks. To learn this
mapping, we propose DeepLocalization, a deep neural network that regresses the
vehicle's translation and rotation parameters from unordered and dynamic input
landmarks. The proposed network architecture is robust to changes of the
dynamic environment and can cope with a small number of extracted landmarks.
During the training process we rely on synthetically generated ground-truth. In
our experiments, we evaluate two inference approaches in real-world scenarios.
We show that DeepLocalization can be combined with regular GPS signals and
filtering algorithms such as the extended Kalman filter. Our approach achieves
state-of-the-art accuracy and is about ten times faster than the related work.",arxiv
http://arxiv.org/abs/2012.05228v2,2021-03-06T07:22:22Z,2020-12-09T18:49:24Z,Video Deblurring by Fitting to Test Data,"Motion blur in videos captured by autonomous vehicles and robots can degrade
their perception capability. In this work, we present a novel approach to video
deblurring by fitting a deep network to the test video. Our key observation is
that some frames in a video with motion blur are much sharper than others, and
thus we can transfer the texture information in those sharp frames to blurry
frames. Our approach heuristically selects sharp frames from a video and then
trains a convolutional neural network on these sharp frames. The trained
network often absorbs enough details in the scene to perform deblurring on all
the video frames. As an internal learning method, our approach has no domain
gap between training and test data, which is a problematic issue for existing
video deblurring approaches. The conducted experiments on real-world video data
show that our model can reconstruct clearer and sharper videos than
state-of-the-art video deblurring approaches. Code and data are available at
https://github.com/xrenaa/Deblur-by-Fitting.",arxiv
http://arxiv.org/abs/2101.06409v1,2021-01-16T09:00:34Z,2021-01-16T09:00:34Z,Shape Back-Projection In 3D Scenes,"In this work, we propose a novel framework shape back-projection for
computationally efficient point cloud processing in a probabilistic manner. The
primary component of the technique is shape histogram and a back-projection
procedure. The technique measures similarity between 3D surfaces, by analyzing
their geometrical properties. It is analogous to color back-projection which
measures similarity between images, simply by looking at their color
distributions. In the overall process, first, shape histogram of a sample
surface (e.g. planar) is computed, which captures the profile of surface
normals around a point in form of a probability distribution. Later, the
histogram is back-projected onto a test surface and a likelihood score is
obtained. The score depicts that how likely a point in the test surface behaves
similar to the sample surface, geometrically. Shape back-projection finds its
application in binary surface classification, high curvature edge detection in
unorganized point cloud, automated point cloud labeling for 3D-CNNs
(convolutional neural network) etc. The algorithm can also be used for
real-time robotic operations such as autonomous object picking in warehouse
automation, ground plane extraction for autonomous vehicles and can be deployed
easily on computationally limited platforms (UAVs).",arxiv
http://arxiv.org/abs/2005.07872v2,2020-08-16T08:07:49Z,2020-05-16T04:54:37Z,"Gentlemen on the Road: Understanding How Pedestrians Interpret Yielding
  Behavior of Autonomous Vehicles using Machine Learning","Autonomous vehicles (AVs) can prevent collisions by understanding pedestrian
intention. We conducted a virtual reality experiment with 39 participants and
measured crossing times (seconds) and head orientation (yaw degrees). We
manipulated AV yielding behavior (no-yield, slow-yield, and fast-yield) and the
AV size (small, medium, and large). Using machine learning approach, we
classified head orientation change of pedestrians by time into 6 clusters of
patterns. Results indicate that pedestrian head orientation change was
influenced by AV yielding behavior as well as the size of the AV. Participants
fixated on the front most of the time even when the car approached near.
Participants changed head orientation most frequently when a large size AV did
not yield (no-yield). In post-experiment interviews, participants reported that
yielding behavior and size affected their decision to cross and perceived
safety. For autonomous vehicles to be perceived more safe and trustful,
vehicle-specific factors such as size and yielding behavior should be
considered in the designing process.",arxiv
http://arxiv.org/abs/1712.02294v4,2018-07-12T14:11:40Z,2017-12-06T17:20:21Z,Joint 3D Proposal Generation and Object Detection from View Aggregation,"We present AVOD, an Aggregate View Object Detection network for autonomous
driving scenarios. The proposed neural network architecture uses LIDAR point
clouds and RGB images to generate features that are shared by two subnetworks:
a region proposal network (RPN) and a second stage detector network. The
proposed RPN uses a novel architecture capable of performing multimodal feature
fusion on high resolution feature maps to generate reliable 3D object proposals
for multiple object classes in road scenes. Using these proposals, the second
stage detection network performs accurate oriented 3D bounding box regression
and category classification to predict the extents, orientation, and
classification of objects in 3D space. Our proposed architecture is shown to
produce state of the art results on the KITTI 3D object detection benchmark
while running in real time with a low memory footprint, making it a suitable
candidate for deployment on autonomous vehicles. Code is at:
https://github.com/kujason/avod",arxiv
http://arxiv.org/abs/2010.08844v2,2021-06-11T00:42:30Z,2020-10-17T18:35:32Z,"Finding Physical Adversarial Examples for Autonomous Driving with Fast
  and Differentiable Image Compositing","There is considerable evidence that deep neural networks are vulnerable to
adversarial perturbations applied directly to their digital inputs. However, it
remains an open question whether this translates to vulnerabilities in real
systems. For example, an attack on self-driving cars would in practice entail
modifying the driving environment, which then impacts the video inputs to the
car's controller, thereby indirectly leading to incorrect driving decisions.
Such attacks require accounting for system dynamics and tracking viewpoint
changes. We propose a scalable approach for finding adversarial modifications
of a simulated autonomous driving environment using a differentiable
approximation for the mapping from environmental modifications (rectangles on
the road) to the corresponding video inputs to the controller neural network.
Given the parameters of the rectangles, our proposed differentiable mapping
composites them onto pre-recorded video streams of the original environment,
accounting for geometric and color variations. Moreover, we propose a multiple
trajectory sampling approach that enables our attacks to be robust to a car's
self-correcting behavior. When combined with a neural network-based controller,
our approach allows the design of adversarial modifications through end-to-end
gradient-based optimization. Using the Carla autonomous driving simulator, we
show that our approach is significantly more scalable and far more effective at
identifying autonomous vehicle vulnerabilities in simulation experiments than a
state-of-the-art approach based on Bayesian Optimization.",arxiv
http://arxiv.org/abs/2105.13289v1,2021-05-26T17:36:35Z,2021-05-26T17:36:35Z,"MTH-IDS: A Multi-Tiered Hybrid Intrusion Detection System for Internet
  of Vehicles","Modern vehicles, including connected vehicles and autonomous vehicles,
nowadays involve many electronic control units connected through intra-vehicle
networks to implement various functionalities and perform actions. Modern
vehicles are also connected to external networks through vehicle-to-everything
technologies, enabling their communications with other vehicles,
infrastructures, and smart devices. However, the improving functionality and
connectivity of modern vehicles also increase their vulnerabilities to
cyber-attacks targeting both intra-vehicle and external networks due to the
large attack surfaces. To secure vehicular networks, many researchers have
focused on developing intrusion detection systems (IDSs) that capitalize on
machine learning methods to detect malicious cyber-attacks. In this paper, the
vulnerabilities of intra-vehicle and external networks are discussed, and a
multi-tiered hybrid IDS that incorporates a signature-based IDS and an
anomaly-based IDS is proposed to detect both known and unknown attacks on
vehicular networks. Experimental results illustrate that the proposed system
can detect various types of known attacks with 99.99% accuracy on the
CAN-intrusion-dataset representing the intra-vehicle network data and 99.88%
accuracy on the CICIDS2017 dataset illustrating the external vehicular network
data. For the zero-day attack detection, the proposed system achieves high
F1-scores of 0.963 and 0.800 on the above two datasets, respectively. The
average processing time of each data packet on a vehicle-level machine is less
than 0.6 ms, which shows the feasibility of implementing the proposed system in
real-time vehicle systems. This emphasizes the effectiveness and efficiency of
the proposed IDS.",arxiv
http://arxiv.org/abs/2010.15441v1,2020-10-29T09:29:47Z,2020-10-29T09:29:47Z,"Self-awareness in intelligent vehicles: Feature based dynamic Bayesian
  models for abnormality detection","The evolution of Intelligent Transportation Systems in recent times
necessitates the development of self-awareness in agents. Before the intensive
use of Machine Learning, the detection of abnormalities was manually programmed
by checking every variable and creating huge nested conditions that are very
difficult to track. This paper aims to introduce a novel method to develop
self-awareness in autonomous vehicles that mainly focuses on detecting abnormal
situations around the considered agents. Multi-sensory time-series data from
the vehicles are used to develop the data-driven Dynamic Bayesian Network (DBN)
models used for future state prediction and the detection of dynamic
abnormalities. Moreover, an initial level collective awareness model that can
perform joint anomaly detection in co-operative tasks is proposed. The GNG
algorithm learns the DBN models' discrete node variables; probabilistic
transition links connect the node variables. A Markov Jump Particle Filter
(MJPF) is applied to predict future states and detect when the vehicle is
potentially misbehaving using learned DBNs as filter parameters. In this paper,
datasets from real experiments of autonomous vehicles performing various tasks
used to learn and test a set of switching DBN models.",arxiv
http://arxiv.org/abs/2107.07316v1,2021-07-15T13:36:55Z,2021-07-15T13:36:55Z,"Minimizing Safety Interference for Safe and Comfortable Automated
  Driving with Distributional Reinforcement Learning","Despite recent advances in reinforcement learning (RL), its application in
safety critical domains like autonomous vehicles is still challenging. Although
punishing RL agents for risky situations can help to learn safe policies, it
may also lead to highly conservative behavior. In this paper, we propose a
distributional RL framework in order to learn adaptive policies that can tune
their level of conservativity at run-time based on the desired comfort and
utility. Using a proactive safety verification approach, the proposed framework
can guarantee that actions generated from RL are fail-safe according to the
worst-case assumptions. Concurrently, the policy is encouraged to minimize
safety interference and generate more comfortable behavior. We trained and
evaluated the proposed approach and baseline policies using a high level
simulator with a variety of randomized scenarios including several corner cases
which rarely happen in reality but are very crucial. In light of our
experiments, the behavior of policies learned using distributional RL can be
adaptive at run-time and robust to the environment uncertainty. Quantitatively,
the learned distributional RL agent drives in average 8 seconds faster than the
normal DQN policy and requires 83\% less safety interference compared to the
rule-based policy with slightly increasing the average crossing time. We also
study sensitivity of the learned policy in environments with higher perception
noise and show that our algorithm learns policies that can still drive reliable
when the perception noise is two times higher than the training configuration
for automated merging and crossing at occluded intersections.",arxiv
http://arxiv.org/abs/1805.01195v1,2018-05-03T09:59:45Z,2018-05-03T09:59:45Z,BirdNet: a 3D Object Detection Framework from LiDAR information,"Understanding driving situations regardless the conditions of the traffic
scene is a cornerstone on the path towards autonomous vehicles; however,
despite common sensor setups already include complementary devices such as
LiDAR or radar, most of the research on perception systems has traditionally
focused on computer vision. We present a LiDAR-based 3D object detection
pipeline entailing three stages. First, laser information is projected into a
novel cell encoding for bird's eye view projection. Later, both object location
on the plane and its heading are estimated through a convolutional neural
network originally designed for image processing. Finally, 3D oriented
detections are computed in a post-processing phase. Experiments on KITTI
dataset show that the proposed framework achieves state-of-the-art results
among comparable methods. Further tests with different LiDAR sensors in real
scenarios assess the multi-device capabilities of the approach.",arxiv
http://arxiv.org/abs/2109.13751v1,2021-09-28T14:11:36Z,2021-09-28T14:11:36Z,StereoSpike: Depth Learning with a Spiking Neural Network,"Depth estimation is an important computer vision task, useful in particular
for navigation in autonomous vehicles, or for object manipulation in robotics.
Here we solved it using an end-to-end neuromorphic approach, combining two
event-based cameras and a Spiking Neural Network (SNN) with a slightly modified
U-Net-like encoder-decoder architecture, that we named StereoSpike. More
specifically, we used the Multi Vehicle Stereo Event Camera Dataset (MVSEC). It
provides a depth ground-truth, which was used to train StereoSpike in a
supervised manner, using surrogate gradient descent. We propose a novel readout
paradigm to obtain a dense analog prediction -- the depth of each pixel -- from
the spikes of the decoder. We demonstrate that this architecture generalizes
very well, even better than its non-spiking counterparts, leading to
state-of-the-art test accuracy. To the best of our knowledge, it is the first
time that such a large-scale regression problem is solved by a fully spiking
network. Finally, we show that low firing rates (<10%) can be obtained via
regularization, with a minimal cost in accuracy. This means that StereoSpike
could be efficiently implemented on neuromorphic chips, opening the door for
low power and real time embedded systems.",arxiv
http://arxiv.org/abs/2004.10049v1,2020-04-18T10:13:40Z,2020-04-18T10:13:40Z,"Learning Self-Awareness for Autonomous Vehicles: Exploring Multisensory
  Incremental Models","The technology for autonomous vehicles is close to replacing human drivers by
artificial systems endowed with high-level decision-making capabilities. In
this regard, systems must learn about the usual vehicle's behavior to predict
imminent difficulties before they happen. An autonomous agent should be capable
of continuously interacting with multi-modal dynamic environments while
learning unseen novel concepts. Such environments are not often available to
train the agent on it, so the agent should have an understanding of its own
capacities and limitations. This understanding is usually called
self-awareness. This paper proposes a multi-modal self-awareness modeling of
signals coming from different sources. This paper shows how different machine
learning techniques can be used under a generic framework to learn single
modality models by using Dynamic Bayesian Networks. In the presented case, a
probabilistic switching model and a bank of generative adversarial networks are
employed to model a vehicle's positional and visual information respectively.
Our results include experiments performed on a real vehicle, highlighting the
potentiality of the proposed approach at detecting abnormalities in real
scenarios.",arxiv
http://arxiv.org/abs/1807.01866v3,2021-08-26T09:51:33Z,2018-07-05T06:58:13Z,Multi-Task Trust Transfer for Human-Robot Interaction,"Trust is essential in shaping human interactions with one another and with
robots. This paper discusses how human trust in robot capabilities transfers
across multiple tasks. We first present a human-subject study of two distinct
task domains: a Fetch robot performing household tasks and a virtual reality
simulation of an autonomous vehicle performing driving and parking maneuvers.
The findings expand our understanding of trust and inspire new differentiable
models of trust evolution and transfer via latent task representations: (i) a
rational Bayes model, (ii) a data-driven neural network model, and (iii) a
hybrid model that combines the two. Experiments show that the proposed models
outperform prevailing models when predicting trust over unseen tasks and users.
These results suggest that (i) task-dependent functional trust models capture
human trust in robot capabilities more accurately, and (ii) trust transfer
across tasks can be inferred to a good degree. The latter enables
trust-mediated robot decision-making for fluent human-robot interaction in
multi-task settings.",arxiv
http://arxiv.org/abs/2005.06892v1,2020-05-14T11:54:04Z,2020-05-14T11:54:04Z,ZynqNet: An FPGA-Accelerated Embedded Convolutional Neural Network,"Image Understanding is becoming a vital feature in ever more applications
ranging from medical diagnostics to autonomous vehicles. Many applications
demand for embedded solutions that integrate into existing systems with tight
real-time and power constraints. Convolutional Neural Networks (CNNs) presently
achieve record-breaking accuracies in all image understanding benchmarks, but
have a very high computational complexity. Embedded CNNs thus call for small
and efficient, yet very powerful computing platforms. This master thesis
explores the potential of FPGA-based CNN acceleration and demonstrates a fully
functional proof-of-concept CNN implementation on a Zynq System-on-Chip. The
ZynqNet Embedded CNN is designed for image classification on ImageNet and
consists of ZynqNet CNN, an optimized and customized CNN topology, and the
ZynqNet FPGA Accelerator, an FPGA-based architecture for its evaluation.
ZynqNet CNN is a highly efficient CNN topology. Detailed analysis and
optimization of prior topologies using the custom-designed Netscope CNN
Analyzer have enabled a CNN with 84.5% top-5 accuracy at a computational
complexity of only 530 million multiplyaccumulate operations. The topology is
highly regular and consists exclusively of convolutional layers, ReLU
nonlinearities and one global pooling layer. The CNN fits ideally onto the FPGA
accelerator. The ZynqNet FPGA Accelerator allows an efficient evaluation of
ZynqNet CNN. It accelerates the full network based on a nested-loop algorithm
which minimizes the number of arithmetic operations and memory accesses. The
FPGA accelerator has been synthesized using High-Level Synthesis for the Xilinx
Zynq XC-7Z045, and reaches a clock frequency of 200MHz with a device
utilization of 80% to 90 %.",arxiv
http://arxiv.org/abs/1907.08009v1,2019-07-18T12:03:12Z,2019-07-18T12:03:12Z,"Real-Time Driver State Monitoring Using a CNN Based Spatio-Temporal
  Approach","Many road accidents occur due to distracted drivers. Today, driver monitoring
is essential even for the latest autonomous vehicles to alert distracted
drivers in order to take over control of the vehicle in case of emergency. In
this paper, a spatio-temporal approach is applied to classify drivers'
distraction level and movement decisions using convolutional neural networks
(CNNs). We approach this problem as action recognition to benefit from temporal
information in addition to spatial information. Our approach relies on features
extracted from sparsely selected frames of an action using a pre-trained
BN-Inception network. Experiments show that our approach outperforms the
state-of-the art results on the Distracted Driver Dataset (96.31%), with an
accuracy of 99.10% for 10-class classification while providing real-time
performance. We also analyzed the impact of fusion using RGB and optical flow
modalities with a very recent data level fusion strategy. The results on the
Distracted Driver and Brain4Cars datasets show that fusion of these modalities
further increases the accuracy.",arxiv
http://arxiv.org/abs/2107.08690v1,2021-07-19T08:53:35Z,2021-07-19T08:53:35Z,"ObserveNet Control: A Vision-Dynamics Learning Approach to Predictive
  Control in Autonomous Vehicles","A key component in autonomous driving is the ability of the self-driving car
to understand, track and predict the dynamics of the surrounding environment.
Although there is significant work in the area of object detection, tracking
and observations prediction, there is no prior work demonstrating that raw
observations prediction can be used for motion planning and control. In this
paper, we propose ObserveNet Control, which is a vision-dynamics approach to
the predictive control problem of autonomous vehicles. Our method is composed
of a: i) deep neural network able to confidently predict future sensory data on
a time horizon of up to 10s and ii) a temporal planner designed to compute a
safe vehicle state trajectory based on the predicted sensory data. Given the
vehicle's historical state and sensing data in the form of Lidar point clouds,
the method aims to learn the dynamics of the observed driving environment in a
self-supervised manner, without the need to manually specify training labels.
The experiments are performed both in simulation and real-life, using CARLA and
RovisLab's AMTU mobile platform as a 1:4 scaled model of a car. We evaluate the
capabilities of ObserveNet Control in aggressive driving contexts, such as
overtaking maneuvers or side cut-off situations, while comparing the results
with a baseline Dynamic Window Approach (DWA) and two state-of-the-art
imitation learning systems, that is, Learning by Cheating (LBC) and World on
Rails (WOR).",arxiv
http://arxiv.org/abs/2006.11684v1,2020-06-21T00:38:24Z,2020-06-21T00:38:24Z,"To Explain or Not to Explain: A Study on the Necessity of Explanations
  for Autonomous Vehicles","Explainable AI, in the context of autonomous systems, like self driving cars,
has drawn broad interests from researchers. Recent studies have found that
providing explanations for an autonomous vehicle actions has many benefits,
e.g., increase trust and acceptance, but put little emphasis on when an
explanation is needed and how the content of explanation changes with context.
In this work, we investigate which scenarios people need explanations and how
the critical degree of explanation shifts with situations and driver types.
Through a user experiment, we ask participants to evaluate how necessary an
explanation is and measure the impact on their trust in the self driving cars
in different contexts. We also present a self driving explanation dataset with
first person explanations and associated measure of the necessity for 1103
video clips, augmenting the Berkeley Deep Drive Attention dataset.
Additionally, we propose a learning based model that predicts how necessary an
explanation for a given situation in real time, using camera data inputs. Our
research reveals that driver types and context dictates whether or not an
explanation is necessary and what is helpful for improved interaction and
understanding.",arxiv
http://arxiv.org/abs/2001.11137v3,2021-02-08T07:43:45Z,2020-01-30T00:25:05Z,"Adversarial Attacks on Convolutional Neural Networks in Facial
  Recognition Domain","Numerous recent studies have demonstrated how Deep Neural Network (DNN)
classifiers can be fooled by adversarial examples, in which an attacker adds
perturbations to an original sample, causing the classifier to misclassify the
sample. Adversarial attacks that render DNNs vulnerable in real life represent
a serious threat in autonomous vehicles, malware filters, or biometric
authentication systems. In this paper, we apply Fast Gradient Sign Method to
introduce perturbations to a facial image dataset and then test the output on a
different classifier that we trained ourselves, to analyze transferability of
this method. Next, we craft a variety of different black-box attack algorithms
on a facial image dataset assuming minimal adversarial knowledge, to further
assess the robustness of DNNs in facial recognition. While experimenting with
different image distortion techniques, we focus on modifying single optimal
pixels by a large amount, or modifying all pixels by a smaller amount, or
combining these two attack approaches. While our single-pixel attacks achieved
about a 15% average decrease in classifier confidence level for the actual
class, the all-pixel attacks were more successful and achieved up to an 84%
average decrease in confidence, along with an 81.6% misclassification rate, in
the case of the attack that we tested with the highest levels of perturbation.
Even with these high levels of perturbation, the face images remained
identifiable to a human. Understanding how these noised and perturbed images
baffle the classification algorithms can yield valuable advances in the
training of DNNs against defense-aware adversarial attacks, as well as adaptive
noise reduction techniques. We hope our research may help to advance the study
of adversarial attacks on DNNs and defensive mechanisms to counteract them,
particularly in the facial recognition domain.",arxiv
