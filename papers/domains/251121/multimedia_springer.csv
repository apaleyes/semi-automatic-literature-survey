contentType,identifier,title,publicationName,doi,publisher,publicationDate,onlineDate,abstract,url,database
Chapter ReferenceWorkEntry,doi:10.1007/978-3-030-63416-2_848,Deep Learning Based 3D Vision,Computer Vision,10.1007/978-3-030-63416-2_848,Springer,2021-01-01,2021-10-13,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-63416-2_848,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-83723-5_10,Engineering a Digital Twin for Manual Assembling,"Leveraging Applications of Formal Methods, Verification and Validation: Tools and Trends",10.1007/978-3-030-83723-5_10,Springer,2021-01-01,2021-08-05,"The paper synthesizes our preliminary work on developing a digital twin, with learning capabilities, for a system that includes cyber, physical, and social components. The system is an industrial workstation for manual assembly tasks that uses several machine learning models implemented as microservices in a hybrid architecture, a combination between the orchestrated and the event stream approaches. These models have either similar objectives but context-dependent performance, or matching functionalities when the results are fused to support real-life decisions. Some of the models are descriptive but easy to transform in inductive models with extra tuning effort, while others are purely inductive, requiring intrinsic connection with the real world.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-83723-5_10,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-86383-8_49,Learning Traffic as Videos: A Spatio-Temporal VAE Approach for Traffic Data Imputation,Artificial Neural Networks and Machine Learning – ICANN 2021,10.1007/978-3-030-86383-8_49,Springer,2021-01-01,2021-09-07,"In the real world, data missing is inevitable in traffic data collection due to detector failures or signal interference. However, missing traffic data imputation is non-trivial since traffic data usually contains both temporal and spatial characteristics with inherent complex relations. In each time interval, the traffic measurements collected in all spatial regions can be regarded as an image with more or fewer channels. Therefore, the traffic raster data over time can be learned as videos. In this paper, we propose a novel unsupervised generative neural network for traffic raster data imputation called STVAE , which works well robustly even under different missing rates. The core idea of our model is to discover more complex spatio-temporal representations inside the traffic data under the architecture of variational autoencoder (VAE) with Sylvester normalizing flows (SNFs). After transforming the traffic raster data into multi-channel videos, a Detection-and-Calibration Block (DCB), which extends 3D gated convolution and multi-attention mechanism, is proposed to sense, extract and calibrate more flexible and accurate spatio-temporal dependencies of the original data. The experiments are employed on three real-world traffic flow datasets and demonstrate that our network STVAE achieves the lowest imputation errors and outperforms state-of-the-art traffic data imputation models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-86383-8_49,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-86517-7_29,Multi-task Learning for User Engagement and Adoption in Live Video Streaming Events,Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track,10.1007/978-3-030-86517-7_29,Springer,2021-01-01,2021-09-10,"Nowadays, live video streaming events have become a mainstay in viewer’s communication in large international enterprises. Provided that viewers are distributed worldwide, the main challenge resides on how to schedule the optimal event’s time so as to improve both the viewer’s engagement and adoption. In this paper we present a multi-task deep reinforcement learning model to select the time of a live video streaming event, aiming to optimize the viewer’s engagement and adoption at the same time. We consider the engagement and adoption of the viewers as independent tasks and formulate a unified loss function to learn a common policy. In addition, we account for the fact that each task might have different contribution to the training strategy of the agent. Therefore, to determine the contribution of each task to the agent’s training, we design a Transformer’s architecture for the state-action transitions of each task. We evaluate our proposed model on four real-world datasets, generated by the live video streaming events of four large enterprises spanning from January 2019 until March 2021. Our experiments demonstrate the effectiveness of the proposed model when compared with several state-of-the-art strategies. For reproduction purposes, our evaluation datasets and implementation are publicly available at https://github.com/stefanosantaris/merlin .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-86517-7_29,springer
Chapter,doi:10.1007/978-3-030-58197-8_7,6G Wireless Systems: Challenges and Opportunities,5G and Beyond,10.1007/978-3-030-58197-8_7,Springer,2021-01-01,2020-08-12,"The ongoing deployment of 5G cellular systems is continuously exposing the inherent limitations of this system, compared to its original premise as an enabler for Internet of Everything applications. These 5G drawbacks are spurring worldwide activities focused on defining the next-generation 6G wireless system that can truly integrate far-reaching applications ranging from autonomous systems to extended reality. To date, the fundamental architectural and performance components of 6G remain largely undefined and open to speculations. In this chapter, we present a holistic vision that identifies the main principles that can guide the design and development of a 6G system. In particular, we discuss, in detail, why 6G will not be a simple exploration of more spectrum at high-frequency bands such as terahertz frequencies, but it will rather be a convergence of a number of technological trends. 6G will also be largely driven by a new breed of exciting Internet of Everything services. To this end, we first outline the primary drivers of 6G systems, in terms of applications and accompanying technological trends. Then, we introduce a new set of service classes and expose their target 6G performance requirements. We then explore the enabling technologies for the introduced 6G services and define a comprehensive set of research problems that come hand in hand with the identified technologies. We conclude by providing our observations on the challenges and opportunities that will define the road toward 6G. Ultimately, this chapter can be used to stimulate comprehensive, out-of-the-box research around 6G technologies.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-58197-8_7,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-87034-8_42,User Experience Analysis Based on a Virtual Mark-up Approach,Creativity in Intelligent Technologies and Data Science,10.1007/978-3-030-87034-8_42,Springer,2021-01-01,2021-09-13,"The paper presents some results of interactive user interfaces implementation in practice. Virtual mark-up approach is used to classify the professional status of the users and respectively adapt the user interface. New software components are introduced as a part of Augmented Reality system that capture the user’s behavior, compare it to the typical patterns and generate virtual elements when necessary. The resulting solution is capable of providing alerts and notifications for novice users and hiding the redundant information for experts and thus personalizing the user interface. Artificial neural network provides classification based on the results of user performance in script execution according to pre-defined scenarios. The proposed approach is illustrated by an example of electrical meters surveying mobile application. Research results illustrate the possibility to improve and personalize the augmented reality user interfaces based on the analysis of user activity.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-87034-8_42,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-88007-1_9,Slice Sequential Network: A Lightweight Unsupervised Point Cloud Completion Network,Pattern Recognition and Computer Vision,10.1007/978-3-030-88007-1_9,Springer,2021-01-01,2021-10-22,"The point cloud is usually sparse and incomplete in reality, and the missing region of the point cloud is coherent when it is blocked by other objects. To tackle this problem, we propose a novel light-weight unsupervised model, namely the Slice Sequential Network, for point cloud completion. Our method only generates the missing parts with high fidelity, while many previous methods output the entire point cloud and leave out some important details. Specifically, we slice the incomplete point cloud and force the model to exploit the information lying between slices. In addition, we design a new algorithm for extracting geometric information, which can extract multi-scale features of points of the point cloud to enhance the use of slices. The qualitative and quantitative experiments show that our method is more lightweight and has better performance than the existing state-of-the-art methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-88007-1_9,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-89394-1_9,A Symbolic Machine Learning Approach for Cybersickness Potential-Cause Estimation,Entertainment Computing – ICEC 2021,10.1007/978-3-030-89394-1_9,Springer,2021-01-01,2021-10-22,"Virtual reality (VR) and head-mounted displays are constantly gaining popularity in various fields such as education, military, entertainment, and bio/medical informatics. Although such technologies provide a high sense of immersion, they can also trigger symptoms of discomfort. This condition is called cybersickness (CS) and is quite popular in recent publications in the virtual reality context. This work proposes a novel experimental analysis using symbolic machine learning that ranks potential causes for CS. We estimate the CS causes and rank them according to their impact on the classification capabilities of CS. The experiments are performed using two distinct virtual reality games. We were able to identify that acceleration triggered cybersickness more frequently in a race game in contrast to a flight game. Furthermore, participants less experienced with VR are more prone to feel discomfort and this variable has a greater impact in the race game in contrast to the flight game, where the acceleration is not controlled by the user.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-89394-1_9,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-69373-2_1,Cloud Networked Models of Knowledge-Based Intelligent Control Towards Manufacturing as a Service,"Service Oriented, Holonic and Multi-Agent Manufacturing Systems for Industry of the Future",10.1007/978-3-030-69373-2_1,Springer,2021-01-01,2021-03-03,"This paper describes a 10-year scientific journey in the area of Cloud-based manufacturing in the SOHOMA research community. The tour started in Paris on June 20, 2011 at École Nationale Supérieure d’Arts et Métiers, Paris and returns here on 1^st October 2020 after annual stops in Bucharest, Valenciennes, Nancy, Cambridge, Lisbon, Nantes, Bergamo and Valencia. Several stages in the evolution of Cloud manufacturing research are recalled in their historical order: vertical enterprise integration and networking; resource and product virtualization and cloud infrastructure design; batch optimization with cloud services; real time big shop floor data streaming, machine learning in the cloud for predictive production control, resource health monitoring and predictive maintenance. Major contributions of SOHOMA authors are evoked: extending the cloud computing model to on demand shop floor resource sharing, infrastructure sharing in cloud networked enterprises, MES workload virtualization, deploying cloud services in real time with virtual machine and containers, high availability solutions and software defined networking, machine learning for predictive manufacturing.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-69373-2_1,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-60577-3_31,Operational Visual Control of the Presence of Students in Training Sessions,"Advances in Neural Computation, Machine Learning, and Cognitive Research IV",10.1007/978-3-030-60577-3_31,Springer,2021-01-01,2020-10-02,"The problem of automating the students’ attendance in the classroom is solved by using computer vision. A convolutional neural network is used to recognize a person’s face. The recognition process is implemented in real time. Localization of faces in frames from a video camera is performed by the Viola-Jones method. The convolutional neural network of the VGGFace model forms the features of a person’s face. Identification of the person occurs by the facial features similarity. The software is implemented by using the Keras and OpenCV libraries. The control system performs the following functions: captures the faces of students on a video camera when entering the classroom, compares faces with a database of students, notes the presence at the lesson (or being late) in case of successful identification, saves the data in attendance register. For the convenience of video monitoring, the color of the student’s line changes depending on his condition: not present, present, late, absent. The system provides for manual editing of the electronic register and the choice of the subject name. The student’s photo can be uploaded into the database from a file or directly from the camera.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-60577-3_31,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-8458-9_40,3D Path Planning for UAV with Improved Double Deep Q-Network,Proceedings of 2020 Chinese Intelligent Systems Conference,10.1007/978-981-15-8458-9_40,Springer,2021-01-01,2020-09-30,"Unmanned aerial vehicle (UAV) has been widely used in civil and military fields due to its advantages such as zero casualties, low cost and strong maneuverability. Path planning in 3D obstacle environment is one of the fundamental capabilities of UAV for mission performing. In this paper, we propose a 3D path planning algorithm to learn a target-driven end-to-end model based on an improved double deep Q-network (DQN), where a greedy exploration strategy is applied to accelerate learning. The model takes target and obstacle message as input, and moving command of UAV as output. It can realize path planning successfully for UAV in 3D complex environment. Besides, the experimental results show that improved double DQN has better convergence speed compared with DQN and double DQN.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-8458-9_40,springer
Chapter ConferencePaper,doi:10.1007/978-981-16-7476-1_23,A Transfer Learning Method Based on ResNet Model,Data Mining and Big Data,10.1007/978-981-16-7476-1_23,Springer,2021-01-01,2021-10-31,"As countries around the world improve their garbage recycling and processing policies, the intelligently and efficiently garbage classification and identification has become a key point for implementing policies. However, traditional image recognition methods still have disadvantages, for instance, it needs a large amount of data annotation and a long time is required to train the model. In response to these drawbacks, this paper proposes a transfer learning method based on ResNet model, which aims to solve the problem of efficient classification of small-scale garbage image data sets. For the small sample image data set, after the data augmentation, the pre-training model ResNet50 is migrated to the data set through two migration learning methods of fine-tuning and pre-training model as the feature extractor, so as to realize the training of the target model. The experimental results show that the model classification effect after fine-tuning method and hyperparameter adjustment is better than the model without transfer learning, which can effectively improve the training speed and accuracy, and reduce the impact of over-fitting.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7476-1_23,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-87571-8_41,Application of Neural Network in Oracle Bone Inscriptions,Web Information Systems and Applications,10.1007/978-3-030-87571-8_41,Springer,2021-01-01,2021-09-17,"The recognition of Oracle Bone Inscriptions(OBIs) is of great significance to archaeology, history, and linguistics. To realize the fast and accurate retrieval of images for large-scale OBIs datasets and break through the limitations of current conventional retrieval methods, this paper proposes a convolutional neural network for OBIs recognition. The model is designed according to the characteristics of OBIs. The experimental results show that the improved network can better extract the features of OBIs characters, and the recognition rate reaches 84.45%, which is 13.74% higher than the network before the improvement.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-87571-8_41,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-71711-7_12,Evaluating Predictive Deep Learning Models,Intelligent Technologies and Applications,10.1007/978-3-030-71711-7_12,Springer,2021-01-01,2021-03-15,"Predicting the future using deep learning models is a research field of increasing interest. However, there is a lack of established evaluation methods for assessing their predictive abilities. Images and videos are targeted towards human observers, and since humans have individual perceptions of the world, evaluation of videos should take subjectivity into account. In this paper, we present a framework for evaluating predictive models using subjective data. The methodology is based on a mixed methods research design, and is applied in an experiment to measure the realism and accuracy of predictions of a visual traffic environment. Our method is shown to be uncorrelated with the predominant approach for evaluating predictive models, which is a frame-wise comparison between predictions and ground truth. These findings emphasise the importance of using subjective data in the assessment of predictive abilities of models and open up a new direction for evaluating predictive deep learning models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-71711-7_12,springer
Article,doi:10.1007/s11263-021-01530-3,Guest Editorial: Special Issue on Deep Learning for Video Analysis and Compression,International Journal of Computer Vision,10.1007/s11263-021-01530-3,Springer,2021-12-01,2021-10-15,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-021-01530-3,springer
Article,doi:10.1007/s12517-021-08909-z,Retraction Note to: Image recognition of coastal environment and aerobics sports based on remote sensing images based on deep learning,Arabian Journal of Geosciences,10.1007/s12517-021-08909-z,Springer,2021-11-12,2021-11-12,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12517-021-08909-z,springer
Article,doi:10.1007/s11042-021-11713-2,Real-time 2D/ 3D image processing with deep learning,Multimedia Tools and Applications,10.1007/s11042-021-11713-2,Springer,2021-11-06,2021-11-06,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11713-2,springer
Article,doi:10.1007/s10845-021-01769-0,Editorial: intelligent manufacturing systems towards industry 4.0 era,Journal of Intelligent Manufacturing,10.1007/s10845-021-01769-0,Springer,2021-10-01,2021-04-07,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10845-021-01769-0,springer
Article,doi:10.1007/s10489-021-02846-w,FATALRead - Fooling visual speech recognition models,Applied Intelligence,10.1007/s10489-021-02846-w,Springer,2021-11-12,2021-11-12,"Visual speech recognition is essential in understanding speech in several real-world applications such as surveillance systems and aiding differently-abled. It proliferates the research in the realm of visual speech recognition, also known as Automatic Lip Reading (ALR). In recent years, Deep Learning (DL) methods are being utilised for developing ALR systems. DL models tend to be vulnerable to adversarial attacks. Studying these attacks creates new research directions in designing robust DL systems. Existing attacks on images and videos classification models are not directly applicable to ALR systems. Since the ALR systems encompass temporal information, attacking these systems is comparatively more challenging and strenuous than attacking image classification models. Similarly, compared to other video classification tasks, the region-of-interest is smaller in the case of ALR systems. Despite these factors, our proposed method, Fooling AuTomAtic Lip Reading (FATALRead), can successfully perform adversarial attacks on state-of-the-art ALR systems. To the best of our knowledge, we are the first to successfully fool ALR systems for the word recognition task. We further demonstrate that the success of the attack is increased by incorporating logits instead of probabilities in the loss function. Our extensive experiments on a publicly available dataset, show that our attack successfully circumvents the well-known transformation based defences.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02846-w,springer
Article,doi:10.1007/s10055-021-00512-7,Predicting user visual attention in virtual reality with a deep learning model,Virtual Reality,10.1007/s10055-021-00512-7,Springer,2021-12-01,2021-04-05,"Recent studies show that user’s visual attention during virtual reality museum navigation can be effectively estimated with deep learning models. However, these models rely on large-scale datasets that usually are of high structure complexity and context specific, which is challenging for nonspecialist researchers and designers. Therefore, we present the deep learning model, ALRF, to generalise on real-time user visual attention prediction in virtual reality context. The model combines two parallel deep learning streams to process the compact dataset of temporal–spatial salient features of user’s eye movements and virtual object coordinates. The prediction accuracy outperformed the state-of-the-art deep learning models by reaching record high 91.03%. Importantly, with quick parametric tuning, the model showed flexible applicability across different environments of the virtual reality museum and outdoor scenes. Implications for how the proposed model may be implemented as a generalising tool for adaptive virtual reality application design and evaluation are discussed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-021-00512-7,springer
Article,doi:10.1007/s11548-021-02518-7,"Automatic, global registration in laparoscopic liver surgery",International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-021-02518-7,Springer,2021-10-26,2021-10-26,"Purpose The initial registration of a 3D pre-operative CT model to a 2D laparoscopic video image in augmented reality systems for liver surgery needs to be fast, intuitive to perform and with minimal interruptions to the surgical intervention. Several recent methods have focussed on using easily recognisable landmarks across modalities. However, these methods still need manual annotation or manual alignment. We propose a novel, fully automatic pipeline for 3D–2D global registration in laparoscopic liver interventions. Methods Firstly, we train a fully convolutional network for the semantic detection of liver contours in laparoscopic images. Secondly, we propose a novel contour-based global registration algorithm to estimate the camera pose without any manual input during surgery. The contours used are the anterior ridge and the silhouette of the liver. Results We show excellent generalisation of the semantic contour detection on test data from 8 clinical cases. In quantitative experiments, the proposed contour-based registration can successfully estimate a global alignment with as little as 30% of the liver surface, a visibility ratio which is characteristic of laparoscopic interventions. Moreover, the proposed pipeline showed very promising results in clinical data from 5 laparoscopic interventions. Conclusions Our proposed automatic global registration could make augmented reality systems more intuitive and usable for surgeons and easier to translate to operating rooms. Yet, as the liver is deformed significantly during surgery, it will be very beneficial to incorporate deformation into our method for more accurate registration.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-021-02518-7,springer
Article,doi:10.1186/s13634-021-00813-8,Speech enhancement from fused features based on deep neural network and gated recurrent unit network,EURASIP Journal on Advances in Signal Processing,10.1186/s13634-021-00813-8,Springer,2021-10-24,2021-10-24,"Speech is easily interfered by external environment in reality, which results in the loss of important features. Deep learning has become a popular speech enhancement method because of its superior potential in solving nonlinear mapping problems for complex features. However, the deficiency of traditional deep learning methods is the weak learning capability of important information from previous time steps and long-term event dependencies between the time-series data. To overcome this problem, we propose a novel speech enhancement method based on the fused features of deep neural networks (DNNs) and gated recurrent unit (GRU). The proposed method uses GRU to reduce the number of parameters of DNNs and acquire the context information of the speech, which improves the enhanced speech quality and intelligibility. Firstly, DNN with multiple hidden layers is used to learn the mapping relationship between the logarithmic power spectrum (LPS) features of noisy speech and clean speech. Secondly, the LPS feature of the deep neural network is fused with the noisy speech as the input of GRU network to compensate the missing context information. Finally, GRU network is performed to learn the mapping relationship between LPS features and log power spectrum features of clean speech spectrum. The proposed model is experimentally compared with traditional speech enhancement models, including DNN, CNN, LSTM and GRU. Experimental results demonstrate that the PESQ, SSNR and STOI of the proposed algorithm are improved by 30.72%, 39.84% and 5.53%, respectively, compared with the noise signal under the condition of matched noise. Under the condition of unmatched noise, the PESQ and STOI of the algorithm are improved by 23.8% and 37.36%, respectively. The advantage of the proposed method is that it uses the key information of features to suppress noise in both matched and unmatched noise cases and the proposed method outperforms other common methods in speech enhancement.",https://www.biomedcentral.com/openurl?doi=10.1186/s13634-021-00813-8,springer
Article,doi:10.1007/s12652-021-03525-x,"Framework for biometric iris recognition in video, by deep learning and quality assessment of the iris-pupil region",Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-021-03525-x,Springer,2021-10-05,2021-10-05,"In the current world scenario the influence of the COVID19 pandemic has reached universal proportions affecting almost all countries. In this sense, the need has arisen to wear gloves or to reduce direct contact with objects (such as sensors for capturing fingerprints or palm prints) as a sanitary measure to protect against the virus. In this new reality, it is necessary to have a biometric identification method that allows safe and rapid recognition of people at borders, or in quarantine controls, or in access to places of high biological risk, among others. In this scenario, iris biometric recognition has reached increasing relevance. This biometric modality avoids all the aforementioned inconveniences with proven high efficiency. However, there are still problems associated with the iris capturing and segmentation in real time that could affect the effectiveness of a System of this nature and that it is necessary to take into account. This work presents a framework for real time iris detection and segmentation in video as part of a biometric recognition system. Our proposal focuses on the stages of image capture, iris detection and segmentation in RGB video frames under controlled conditions (conditions of border and access controls, where people collaborate in the recognition process). The proposed framework is based on the direct detection of the iris-pupil region using the YOLO network, the evaluation of its quality and the semantic segmentation of iris by a Fully Convolutional Network. (FCN). The proposal of an evaluation step of the quality of the iris-pupil region reduce the passage to the system of images with problems of out of focus, blurring, occlusions, light changing and pose of the subject. For the evaluation of image quality, we propose a measure that combines parameters defined in ISO/IEC 19794-6 2005 and others derived from the systematization of the knowledge of the specialized literature. The experiments carried out in four different reference databases and an own video data set demonstrates the feasibility of its application under controlled conditions of border and access controls. The achieved results exceed or equal state-of-the-art methods under these working conditions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-021-03525-x,springer
Article,doi:10.1007/s11554-021-01144-5,A real-time person tracking system based on SiamMask network for intelligent video surveillance,Journal of Real-Time Image Processing,10.1007/s11554-021-01144-5,Springer,2021-10-01,2021-07-28,"Real-time video surveillance systems are widely deployed in various environments, including public areas, commercial buildings, and public infrastructures. Person detection is a key and crucial task in different video surveillance applications, such as person detection, segmentation, and tracking. Researchers presented different image processing and artificial intelligence-based approaches (including machine and deep learning) for person detection and tracking, but mainly comprised of frontal view camera perspective. A real-time person tracking and segmentation system is introduced in this work, using an overhead camera perspective. The system applied a deep learning-based algorithm, i.e., SiamMask, a simple, versatile, fast, and surpassing other real-time tracking algorithms. The algorithm also performs segmentation of the target person by combining a mask branch to the fully convolutional twin neural network for target or person tracking. First, the person video sequences are obtained from an overhead perspective, and then additional training is performed with the help of transfer learning. Finally, a comparison is performed with other tracking algorithms. The SiamMask algorithm delivers good results, with a tracking accuracy of 95%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-021-01144-5,springer
Article,doi:10.1007/s13369-021-06297-w,A Deep Learning Framework for Audio Deepfake Detection,Arabian Journal for Science and Engineering,10.1007/s13369-021-06297-w,Springer,2021-11-08,2021-11-08,"Audio deepfakes have been increasingly emerging as a potential source of deceit, with the development of avant-garde methods of synthetic speech generation. Hence, differentiating fake audio from the real one is becoming even more difficult owing to the increasing accuracy of text-to-speech models, posing a serious threat to speaker verification systems. Within the domain of audio deepfake detection, a majority of experiments have been based on the ASVSpoof or the AVSpoof dataset using various machine learning and deep learning approaches. In this work, experiments were performed on a more recent dataset, the Fake or Real (FoR) dataset which contains data generated using some of the best text to speech models. Two approaches have been adopted to the solve problem: feature-based approach and image-based approach. The feature-based approach involves converting audio data into a dataset consisting of various spectral features of the audio samples, which are fed to the machine learning algorithms for the classification of audio as fake or real. While in the image-based approach audio samples are converted into melspectrograms which are input into deep learning algorithms, namely Temporal Convolutional Network (TCN) and Spatial Transformer Network (STN). TCN has been implemented because it is a sequential model and has been shown to give good results on sequential data. A comparison between the performances of both the approaches has been made, and it is observed that deep learning algorithms, particularly TCN, outperforms the machine learning algorithms by a significant margin, with a 92 percent test accuracy. This solution presents a model for audio deepfake classification which has an accuracy comparable to the traditional CNN models like VGG16, XceptionNet, etc.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13369-021-06297-w,springer
Article,doi:10.1007/s00500-021-06258-3,Characteristics recognition and soft multimedia system for Japanese machine translation and edge-driven hardware implementations,Soft Computing,10.1007/s00500-021-06258-3,Springer,2021-10-06,2021-10-06,"With the development of recent economic globalization, international exchanges and cooperation are increasingly frequent and in-depth. In this process, there is a serious obstacle, that is, language differences. Therefore, the development of high-quality and practical machine translation system is of great significance. Japan has a close relationship with China, so it is necessary to acquire and process Japanese information. Japanese translation is the basis of Japanese information processing, which plays an important role in cross language information retrieval, machine translation, information extraction and other practical applications. In recent years, machine translation has made remarkable progress, but there is still much room for improvement in the quality of translation. Multimedia assisted instruction is an important application of computer technology. Therefore, this paper realizes the feature recognition and hardware structure of Japanese machine translation; for the efficient implementation of the model, the edge-driven model is combined. The proposed model is implemented through the edge devices and the performance of the model is validated through the testing. The recognition accuracy is much higher than the traditional models and the robustness is higher.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-021-06258-3,springer
Article,doi:10.1007/s12204-021-2348-7,Intelligent Analysis of Abnormal Vehicle Behavior Based on a Digital Twin,Journal of Shanghai Jiaotong University (Science),10.1007/s12204-021-2348-7,Springer,2021-10-01,2021-10-28,"Analyzing a vehicle’s abnormal behavior in surveillance videos is a challenging field, mainly due to the wide variety of anomaly cases and the complexity of surveillance videos. In this study, a novel intelligent vehicle behavior analysis framework based on a digital twin is proposed. First, detecting vehicles based on deep learning is implemented, and Kalman filtering and feature matching are used to track vehicles. Subsequently, the tracked vehicle is mapped to a digital-twin virtual scene developed in the Unity game engine, and each vehicle’s behavior is tested according to the customized detection conditions set up in the scene. The stored behavior data can be used to reconstruct the scene again in Unity for a secondary analysis. The experimental results using real videos from traffic cameras illustrate that the detection rate of the proposed framework is close to that of the state-of-the-art abnormal event detection systems. In addition, the implementation and analysis process show the usability, generalization, and effectiveness of the proposed framework.",http://link.springer.com/openurl/pdf?id=doi:10.1007/s12204-021-2348-7,springer
Article,doi:10.1007/s10489-021-02285-7,Detecting earthquakes: a novel deep learning-based approach for effective disaster response,Applied Intelligence,10.1007/s10489-021-02285-7,Springer,2021-11-01,2021-04-01,"In the present study, we present an intelligent earthquake signal detector that provides added assistance to automate traditional disaster responses. To effectively respond in a crisis scenario, additional sensors and automation are always necessary. Deep learning has achieved success in various low signal-to-noise ratio tasks, which motivated us to propose a novel 3-dimensional (3D) CNN-RNN-based earthquake detector from a demonstration paradigm to real-time implementation. Data taken from the ST anford EA rthquake D ataset (STEAD) are used to train the network. After preprocessing the raw earthquake signals, features such as log-mel spectrograms are extracted. Once the model has learned spatial and temporal information from low-frequency earthquake waves, it can be employed in real time to distinguish small and large earthquakes from seismic noise with an accuracy, sensitivity, and specificity of 99.057%, 98.488%, and 99.621%, respectively. We also observe that the choice of filters in log-mel spectrogram impacts the results much more than the model complexity. Furthermore, we implement and test the model on data collected continuously over two months by a personal seismometer in the laboratory. The inference speed for a single prediction is 2.27 seconds, and the system delivers a stable detection of all 63 major earthquakes from November 2019 to December 2019 reported by the Japan Meteorological Agency.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02285-7,springer
Article,doi:10.1007/s41095-021-0215-y,Real-time face view correction for front-facing cameras,Computational Visual Media,10.1007/s41095-021-0215-y,Springer,2021-12-01,2021-04-27,"Face views are particularly important in person-to-person communication. Differenes between the camera location and the face orientation can result in undesirable facial appearances of the participants during video conferencing. This phenomenon is particularly noticeable when using devices where the front-facing camera is placed in unconventional locations such as below the display or within the keyboard. In this paper, we take a video stream from a single RGB camera as input, and generate a video stream that emulates the view from a virtual camera at a designated location. The most challenging issue in this problem is that the corrected view often needs out-of-plane head rotations. To address this challenge, we reconstruct the 3D face shape and re-render it into synthesized frames according to the virtual camera location. To output the corrected video stream with natural appearance in real time, we propose several novel techniques including accurate eyebrow reconstruction, high-quality blending between the corrected face image and background, and template-based 3D reconstruction of glasses. Our system works well for different lighting conditions and skin tones, and can handle users wearing glasses. Extensive experiments and user studies demonstrate that our method provides high-quality results.",http://link.springer.com/openurl/pdf?id=doi:10.1007/s41095-021-0215-y,springer
Article,doi:10.1007/s40860-021-00131-8,Elephant–railway conflict minimisation using real-time video data and machine learning,Journal of Reliable Intelligent Environments,10.1007/s40860-021-00131-8,Springer,2021-12-01,2021-01-22,Elephant–train collision has been a major issue for both the railway as well as the forest departments. In this study real-time video data is analysed for detecting elephant to alert the train driver in case of elephants crossing the railway track in which the train is approaching. The HAAR feature extraction and adaptive boosting-based machine learning algorithm are used for detecting elephants from real-time video data. The experimental result shows the average precision of the proposed technique in detecting elephants using real-time video data is more than 96%.,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40860-021-00131-8,springer
Article,doi:10.1007/s11431-020-1777-4,Mobile phone recognition method based on bilinear convolutional neural network,Science China Technological Sciences,10.1007/s11431-020-1777-4,Springer,2021-11-01,2021-09-27,"Model recognition of second-hand mobile phones has been considered as an essential process to improve the efficiency of phone recycling. However, due to the diversity of mobile phone appearances, it is difficult to realize accurate recognition. To solve this problem, a mobile phone recognition method based on bilinear-convolutional neural network (B-CNN) is proposed in this paper. First, a feature extraction model, based on B-CNN, is designed to adaptively extract local features from the images of secondhand mobile phones. Second, a joint loss function, constructed by center distance and softmax, is developed to reduce the interclass feature distance during the training process. Third, a parameter downscaling method, derived from the kernel discriminant analysis algorithm, is introduced to eliminate redundant features in B-CNN. Finally, the experimental results demonstrate that the B-CNN method can achieve higher accuracy than some existing methods.",http://link.springer.com/openurl/pdf?id=doi:10.1007/s11431-020-1777-4,springer
Article,doi:10.1007/s13042-021-01342-4,PointCSE: Context-sensitive encoders for efficient 3D object detection from point cloud,International Journal of Machine Learning and Cybernetics,10.1007/s13042-021-01342-4,Springer,2021-10-29,2021-10-29,"Few modern 3D object detectors achieve fast inference speed and high accuracy at the same time. To achieve high performance, they usually directly operate on raw point clouds, or convert point clouds to 3D representation and then apply 3D convolution. However, those methods come with sizable computation overhead and complex operations. As for high-speed 2D-representation-based 3D detectors, their performance is still restricted. In this paper, we investigate how to leverage context knowledge to empower the 2D representation of point clouds for computation and memory-efficient 3D object detection with state-of-the-art performance. The proposed encoder has two parts: a context-sensitive point sampling network and a point set learning network. Specifically, our point sampling network samples points with dense localization information. With high-quality sampled points, we are allowed to utilize a deeper point set learning network to aggregate semantic details in a light manner. The proposed encoder is lightweight and very supportive of hardware acceleration like TensorRT and TVM. Extensive experiments on the KITTI benchmark show the proposed encoder called PointCSE outperforms prior real-time encoders by a large margin with 1.5 $$\times$$ × memory reduction; it also achieves state-of-the-art performance with 49 FPS inference speed ( 4 $$\times$$ × speedup on average compared to previous best methods).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13042-021-01342-4,springer
Article,doi:10.1007/s11042-021-10654-0,Neural network solution for a real-time no-reference video quality assessment of H.264/AVC video bitstreams,Multimedia Tools and Applications,10.1007/s11042-021-10654-0,Springer,2021-10-27,2021-10-27,"The ever-growing video streaming services require accurate quality assessment with often no reference to the original media. One primary challenge in developing no-reference (NR) video quality metrics is achieving real-timeliness while retaining the accuracy. A real-time no-reference video quality assessment (VQA) method is proposed for videos encoded by H.264/AVC codec. Temporal and spatial features are extracted from the encoded bit-stream and pixel values to train and validate a fully connected neural network. The hand-crafted features and network dynamics are designed in a manner to ensure a high correlation with human judgment of quality as well as minimizing the computational complexities. Proof-of-concept experiments are conducted via comparison with: 1) video sequences rated by a full-reference quality metric, and 2) H.264-encoded sequences from the LIVE video dataset which are subjectively evaluated through differential mean opinion scores (DMOS). The performance of the proposed method is verified by correlation measurements with the aforementioned objective and subjective scores. The framework achieves real-time execution while outperforming state-of-art full-reference and no-reference video quality assessment methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-10654-0,springer
Article,doi:10.1007/s11119-021-09803-0,Object-level classification of vegetable crops in 3D LiDAR point cloud using deep learning convolutional neural networks,Precision Agriculture,10.1007/s11119-021-09803-0,Springer,2021-10-01,2021-03-26,"Crop discrimination at the plant or patch level is vital for modern technology-enabled agriculture. Multispectral and hyperspectral remote sensing data have been widely used for crop classification. Even though spectral data are successful in classifying row-crops and orchards, they are limited in discriminating vegetable and cereal crops at plant or patch level. Terrestrial laser scanning is a potential remote sensing approach that offers distinct structural features useful for classification of crops at plant or patch level. The objective of this research is the improvement and application of an advanced deep learning framework for object-based classification of three vegetable crops: cabbage, tomato, and eggplant using high-resolution LiDAR point cloud. Point clouds from a terrestrial laser scanner (TLS) were acquired over experimental plots of the University of Agricultural Sciences, Bengaluru, India. As part of the methodology, a deep convolution neural network (CNN) model named CropPointNet is devised for the semantic segmentation of crops from a 3D perspective. The CropPointNet is an adaptation of the PointNet deep CNN model developed for the segmentation of indoor objects in a typical computer vision scenario. Apart from adapting to 3D point cloud segmentation of crops, the significant methodological improvements made in the CropPointNet are a random sampling scheme for training point cloud, and optimization of the network architecture to enable structural attribute-based segmentation of point clouds of unstructured objects such as TLS point clouds crops. The performance of the 3D crop classification has been validated and compared against two popular deep learning architectures: PointNet, and the Dynamic Graph-based Convolutional Neural Network (DGCNN). Results indicate consistent plant level object-based classification of crop point cloud with overall accuracies of 81% or better for all the three crops. The CropPointNet architecture proposed in this research can be generalized for segmentation and classification of other row crops and natural vegetation types.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11119-021-09803-0,springer
Article,doi:10.1007/s11554-021-01116-9,Real-time crowd behavior recognition in surveillance videos based on deep learning methods,Journal of Real-Time Image Processing,10.1007/s11554-021-01116-9,Springer,2021-10-01,2021-05-03,"Automatic video surveillance in public crowded places has been an active research area for security purposes. Traditional approaches try to solve the crowd behavior recognition task using a sequential two-stage pipeline as low-level feature extraction and classification. Lately, deep learning has shown promising results in comparison to traditional methods by extracting high-level representation and solving the problem in an end-to-end pipeline. In this paper, we investigate a deep architecture for crowd event recognition to detect seven behavior categories in PETS2009 event recognition dataset. More especially, we apply an integrated handcrafted and Conv-LSTM-AE method with optical flow images as input to extract a high-level representation of data and conduct classification. After achieving a latent representation of input optical flow image sequences in the bottleneck of autoencoder(AE), the architecture is split into two separate branches, one as AE decoder and the other as the classifier. The proposed architecture is jointly trained for representation and classification by defining two different losses. The experimental results in comparison to the state-of-the-art methods demonstrate that our algorithm can be promising for real-time event recognition and achieves a better performance in calculated metrics.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-021-01116-9,springer
Article,doi:10.1007/s00264-021-05191-2,"Ankle and foot surgery: from arthrodesis to arthroplasty, three dimensional printing, sensors, artificial intelligence, machine learning technology, digital twins, and cell therapy",International Orthopaedics,10.1007/s00264-021-05191-2,Springer,2021-09-01,2021-08-26,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00264-021-05191-2,springer
Article,doi:10.1038/s41528-021-00119-7,"All-weather, natural silent speech recognition via machine-learning-assisted tattoo-like electronics",npj Flexible Electronics,10.1038/s41528-021-00119-7,Nature,2021-08-13,2021-08-13,"The internal availability of silent speech serves as a translator for people with aphasia and keeps human–machine/human interactions working under various disturbances. This paper develops a silent speech strategy to achieve all-weather, natural interactions. The strategy requires few usage specialized skills like sign language but accurately transfers high-capacity information in complicated and changeable daily environments. In the strategy, the tattoo-like electronics imperceptibly attached on facial skin record high-quality bio-data of various silent speech, and the machine-learning algorithm deployed on the cloud recognizes accurately the silent speech and reduces the weight of the wireless acquisition module. A series of experiments show that the silent speech recognition system (SSRS) can enduringly comply with large deformation (~45%) of faces by virtue of the electricity-preferred tattoo-like electrodes and recognize up to 110 words covering daily vocabularies with a high average accuracy of 92.64% simply by use of small-sample machine learning. We successfully apply the SSRS to 1-day routine life, including daily greeting, running, dining, manipulating industrial robots in deafening noise, and expressing in darkness, which shows great promotion in real-world applications.",https://www.nature.com/articles/s41528-021-00119-7,springer
Article,doi:10.1007/s10055-021-00572-9,Immersive virtual reality as an empirical research tool: exploring the capability of a machine learning model for predicting construction workers’ safety behaviour,Virtual Reality,10.1007/s10055-021-00572-9,Springer,2021-09-02,2021-09-02,"In recent years, research has found that people have stable predispositions to engage in certain behavioural patterns to work safely or unsafely, which vary among individuals as a function of their personality features. In this regard, an innovative machine learning model has been recently developed to predict workers’ behavioural tendency based on personality factors. This paper presents an empirical evaluation of the model’s prediction performance (i.e. the degree to which the model can generate similar results compared to reality) to address the issue of the model’s usability before it is implemented in real situations. As virtual reality allows a good grip on fidelity resembling real-world situations, it can stimulate more natural behaviour responses from participants to increase ecological validity of experimental results. Thus, we implemented a virtual reality experimentation environment to assess workers’ safety behaviour. The model’s prediction capability was then evaluated by comparing the model prediction results and workers’ safety behaviour as assessed in virtual reality. The comparison results showed that the model predictions on two dimensions of workers’ safety behaviour (i.e. task and contextual performance) were in good agreement with the virtual reality experimental results, with Spearman correlation coefficients of 79.7% and 87.8%, respectively. The machine learning model thus proved to have good prediction capability, which allows the model to help identify vulnerable workers who are prone to undertake unsafe behaviours. The findings also suggest that virtual reality is a promising method for measuring workers’ safety behaviour as it can provide a realistic and safe environment for experimentation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-021-00572-9,springer
Article,doi:10.1007/s10055-020-00476-0,Implementation of escape room system based on augmented reality involving deep convolutional neural network,Virtual Reality,10.1007/s10055-020-00476-0,Springer,2021-09-01,2020-10-11,"Escape room is a live-action adventure game, where the players search clues, solve puzzles and achieve the assigned tasks. This paper proposed a novel escape room system combining augmented reality and deep learning technology. The system adopts a client–server architecture and can be divided into the server module, the smart glasses module and the player–hardware interaction module. The player–hardware interaction module consists of subsystems each of which includes a Raspberry Pi 3. HoloLens is used as the smart glasses in the experiment of the paper. The server communicates with all the Raspberry Pis and HoloLens through TCP/IP protocol and manages all the devices to achieve the game flow by following the process timeline. The smart glasses module provides two display modes, i.e., the AR 3D models display and the 2D text clues display. In the first mode, the SDK Vuforia is used for detection and tracking of markers. In the second mode, the scene images captured by HoloLens camera are sent to the pre-trained image classifier based on deep convolutional neural network. Considering both the image category and the game status value, the server decides the text clue image to be displayed on HoloLens. The accuracy of the image classification model reaches 94.9%, which can be correctly classified for a certain rotation angle and partial occlusion. The integration of AR, deep learning, electronics and escape room games opens up exciting new directions for the development of escape room. Finally, a built mini-escape room is analyzed to prove that the proposed system can support more complicated narratives showing the potential of achieving immersion.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-020-00476-0,springer
Article,doi:10.1038/s41598-021-98697-z,Automatic segmentation tool for 3D digital rocks by deep learning,Scientific Reports,10.1038/s41598-021-98697-z,Nature,2021-09-27,2021-09-27,"Obtaining an accurate segmentation of images obtained by computed microtomography (micro-CT) techniques is a non-trivial process due to the wide range of noise types and artifacts present in these images. Current methodologies are often time-consuming, sensitive to noise and artifacts, and require skilled people to give accurate results. Motivated by the rapid advancement of deep learning-based segmentation techniques in recent years, we have developed a tool that aims to fully automate the segmentation process in one step, without the need for any extra image processing steps such as noise filtering or artifact removal. To get a general model, we train our network using a dataset made of high-quality three-dimensional micro-CT images from different scanners, rock types, and resolutions. In addition, we use a domain-specific augmented training pipeline with various types of noise, synthetic artifacts, and image transformation/distortion. For validation, we use a synthetic dataset to measure accuracy and analyze noise/artifact sensitivity. The results show a robust and accurate segmentation performance for the most common types of noises present in real micro-CT images. We also compared the segmentation of our method and five expert users, using commercial and open software packages on real rock images. We found that most of the current tools fail to reduce the impact of local and global noises and artifacts. We quantified the variation on human-assisted segmentation results in terms of physical properties and observed a large variation. In comparison, the new method is more robust to local noises and artifacts, outperforming the human segmentation and giving consistent results. Finally, we compared the porosity of our model segmented images with experimental porosity measured in the laboratory for ten different untrained samples, finding very encouraging results.",https://www.nature.com/articles/s41598-021-98697-z,springer
Article,doi:10.1007/s13204-021-02068-z,Deep learning-based garbage image recognition algorithm,Applied Nanoscience,10.1007/s13204-021-02068-z,Springer,2021-09-08,2021-09-08,"To solve the problems of over-fitting, poor convergence, and reduced recall and accuracy of traditional image recognition algorithms, a junk image recognition algorithm based on deep learning is proposed. Dropout is introduced to overcome over fitting, adagrad adaptive method is used to debug the parameters of deep neural network, and ReLU is adopted to solve the gradient dispersion of neural network training, realize the centralized processing of garbage image data, and extract the edge features, color features and texture features of garbage image in the data set, respectively, shape features. The modified probability density function is used for image classification. According to the different characteristics of garbage image, the image to be recognized is divided into different categories to complete garbage image recognition. The experimental results show that the designed algorithm has good convergence, high recall and accuracy, and short recognition time, indicating that the algorithm is feasible and practical.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13204-021-02068-z,springer
Article,doi:10.1007/s10772-021-09885-1,Exploiting variable length segments with coarticulation effect in online speech recognition based on deep bidirectional recurrent neural network and context-sensitive segment,International Journal of Speech Technology,10.1007/s10772-021-09885-1,Springer,2021-08-28,2021-08-28,"Deep bidirectional recurrent network (DBRNN) is a powerful acoustic model that can capture the dynamics and coarticulation effect of speech signal. It can model the temporal sequences that depend on left and right contexts, whereas deep unidirectional recurrent neural network (or deep recurrent neural network) can model the temporal sequences that usually depend only on past information. When traditional DBRNNs are used, context-sensitive segments with carefully selected fixed length are exploited to balance recognition accuracy and latency for online speech recognition because the ASR decoder results in recognition latency, depending on the whole input sequence in each evaluation. On the other hand, acoustical realization of phoneme depends not only on the left-sided phoneme, but also on the right-sided phoneme, which should be considered in acoustic modeling for speech recognition. In this paper, we propose a DBRNN-based online speech recognition method that selects and exploits variable length chunks to take into account coarticulation effects appearing in speech production. In order to select variable length segments with the coarticulation effects, the vowel identification points predicted by a deep unidirectional recurrent neural network are used, and such variable length segments are used for training of DBRNN for online recognition. The deep unidirectional recurent neural network for predicting variable length segments is trained using the connectionist temporal classification (CTC) method. We show that the online recognizable DBRNN acoustic model constructed using variable length chunks with coarticulation effect in experiments on Korean speech recognition effectively limits recognition latency, resulting in performance comparable to traditional offline DBRNN, and provides improved performance than online recognition based on fixed-length context-sensitive chunks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-021-09885-1,springer
Article,doi:10.1038/s41598-021-95469-7,Author Correction: Automated rotator cuff tear classification using 3D convolutional neural network,Scientific Reports,10.1038/s41598-021-95469-7,Nature,2021-08-02,2021-08-02,,https://www.nature.com/articles/s41598-021-95469-7,springer
Article,doi:10.1007/s11042-021-11332-x,Machine learning for big multimedia analytics,Multimedia Tools and Applications,10.1007/s11042-021-11332-x,Springer,2021-08-01,2021-08-07,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11332-x,springer
Article,doi:10.1007/s00779-021-01554-z,"From AI, creativity and music to IoT, HCI, musical instrument design and audio interaction: a journey in sound",Personal and Ubiquitous Computing,10.1007/s00779-021-01554-z,Springer,2021-08-01,2021-04-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00779-021-01554-z,springer
Article,doi:10.1007/s00530-021-00814-5,Special issue on deep learning for emerging big multimedia super-resolution,Multimedia Systems,10.1007/s00530-021-00814-5,Springer,2021-08-01,2021-05-27,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-021-00814-5,springer
Article,doi:10.1007/s11554-021-01156-1,Special issue on deep learning for emerging embedded real-time image and video processing systems,Journal of Real-Time Image Processing,10.1007/s11554-021-01156-1,Springer,2021-08-01,2021-07-26,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-021-01156-1,springer
Article,doi:10.1007/s11042-021-10967-0,Correction to: Deep learning approaches for speech emotion recognition: state of the art and research challenges,Multimedia Tools and Applications,10.1007/s11042-021-10967-0,Springer,2021-07-01,2021-05-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-10967-0,springer
Article,doi:10.1007/s11042-021-11115-4,Correction to: Feature extraction and machine learning solutions for detecting motion vector data embedding in HEVC videos,Multimedia Tools and Applications,10.1007/s11042-021-11115-4,Springer,2021-07-01,2021-06-15,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11115-4,springer
Article,doi:10.1007/s42947-020-0138-5,Automatically detect and classify asphalt pavement raveling severity using 3D technology and machine learning,International Journal of Pavement Research and Technology,10.1007/s42947-020-0138-5,Springer,2021-07-01,2020-11-05,"Raveling is one of the most common asphalt pavement distresses that occur on US highway pavements. Raveling results in safety concerns such as loose stones and hydroplaning; poor ride quality and road/tire noise; and shortened pavement longevity. Traditional raveling survey methods involve manual visual inspection, which is time consuming, subjective, and hazardous to highway workers. With the research project competitively selected and sponsored by the National Cooperative Highway Research Program (NCHRP) Innovation Deserving Exploratory Analysis (IDEA) program, the objective of this study is to develop an accurate raveling detection and classification algorithm using 3D pavement data that has become mainstream technologies for state Department of Transportations (DOTs) in the US for pavement condition evaluation, and to comprehensively validate these methods using large-scale, real-world data based on actual transportation agencies’ distress protocol (Severity levels 1, 2, and 3). A total of 65 miles of 3 D pavement data was collected on I-85 and I-285 in Georgia for training and testing. Three supervised machine learning techniques —AdaBoost with decision trees, support vector machine (SVM) and random forests—were developed for the detection and classification of raveling in the collected data. The random forest classifier had the b est performance, with precision values ranging from 75.6% for level 3 raveling to 97.6% for level 0 (no) raveling and recall values ranging from 86.9% for level 1 raveling to 96.1% for level 0 raveling on real world large-scale data. The developed raveling detection and severity level classification method has been successfully implemented to entire Georgia’s interstate highway system with1452.5 survey miles of asphalt pavements after the large-scale validation and refinement. The proposed method for raveling detection can be deployed to other transportation agencies for safer and more efficient assessment of roadway raveling conditions.",http://link.springer.com/openurl/pdf?id=doi:10.1007/s42947-020-0138-5,springer
Article,doi:10.1007/s11042-021-11137-y,A survey of 3D object detection,Multimedia Tools and Applications,10.1007/s11042-021-11137-y,Springer,2021-08-01,2021-07-03,"Due to the rapid development of science and technology, object detection has become a promising research direction in computer vision. In recent years, most object detection frameworks proposed in the existing research are 2D. However, 2D object detection cannot take three-dimensional space into account, resulting in its inability to be used to solve problems in real world. Hence, we conduct this 3D object detection survey in the hope that 3D object detection methods can be better applied to the contexts of intelligent video surveillance, robot navigation and autonomous driving technology. There exist various 3D object detection methods while in this paper we only focus on the popular deep learning based methods. We divide these approaches into four categories according to the input data category. Besides, we discuss the innovations of these frames and compare their experimental results in terms of accuracy. Finally, we indicate the technical difficulties associated with current 3D object detection and discuss future research directions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11137-y,springer
Article,doi:10.3758/s13428-021-01623-4,Automated evaluation of psychotherapy skills using speech and language technologies,Behavior Research Methods,10.3758/s13428-021-01623-4,Springer,2021-08-03,2021-08-03,"With the growing prevalence of psychological interventions, it is vital to have measures which rate the effectiveness of psychological care to assist in training, supervision, and quality assurance of services. Traditionally, quality assessment is addressed by human raters who evaluate recorded sessions along specific dimensions, often codified through constructs relevant to the approach and domain. This is, however, a cost-prohibitive and time-consuming method that leads to poor feasibility and limited use in real-world settings. To facilitate this process, we have developed an automated competency rating tool able to process the raw recorded audio of a session, analyzing who spoke when, what they said, and how the health professional used language to provide therapy. Focusing on a use case of a specific type of psychotherapy called “motivational interviewing”, our system gives comprehensive feedback to the therapist, including information about the dynamics of the session (e.g., therapist’s vs. client’s talking time), low-level psychological language descriptors (e.g., type of questions asked), as well as other high-level behavioral constructs (e.g., the extent to which the therapist understands the clients’ perspective). We describe our platform and its performance using a dataset of more than 5000 recordings drawn from its deployment in a real-world clinical setting used to assist training of new therapists. Widespread use of automated psychotherapy rating tools may augment experts’ capabilities by providing an avenue for more effective training and skill improvement, eventually leading to more positive clinical outcomes.",http://link.springer.com/openurl/fulltext?id=doi:10.3758/s13428-021-01623-4,springer
Article,doi:10.1007/s11265-021-01683-x,Compute and Memory Efficient Universal Sound Source Separation,Journal of Signal Processing Systems,10.1007/s11265-021-01683-x,Springer,2021-07-30,2021-07-30,"Recent progress in audio source separation led by deep learning has enabled many neural network models to provide robust solutions to this fundamental estimation problem. In this study, we provide a family of efficient neural network architectures for general purpose audio source separation while focusing on multiple computational aspects that hinder the application of neural networks in real-world scenarios. The backbone structure of this convolutional network is the SUccessive DOwnsampling and Resampling of Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is performed through simple one-dimensional convolutions. This mechanism enables our models to obtain high fidelity signal separation in a wide variety of settings where a variable number of sources are present and with limited computational resources (e.g. floating point operations, memory footprint, number of parameters and latency). Our experiments show that SuDoRM-RF models perform comparably and even surpass several state-of-the-art benchmarks with significantly higher computational resource requirements. The causal variation of SuDoRM-RF is able to obtain competitive performance in real-time speech separation of around 10dB scale-invariant signal-to-distortion ratio improvement (SI-SDRi) while remaining up to 20 times faster than real-time on a laptop device.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11265-021-01683-x,springer
Article,doi:10.1007/s11554-021-01140-9,Convolution neural network with low operation FLOPS and high accuracy for image recognition,Journal of Real-Time Image Processing,10.1007/s11554-021-01140-9,Springer,2021-08-01,2021-06-19,"The convolution neural network makes deeper and wider for better accuracy, but requires higher computations. When the neural network goes deeper, some information loss is more. To improve this drawback, the residual structure was developed to connect the information of the previous layers. This is a good solution to prevent the loss of information, but it requires a huge amount of parameters for deeper layer operations. In this study, the fast computational algorithm is proposed to reduce the parameters and to save the operations with the modification of DenseNet deep layer block. With channel merging procedures, this solution can reduce the dilemma of multiple growth of the parameter quantity for deeper layer. This approach is not only to reduce the parameters and FLOPs, but also to keep high accuracy. Comparisons with the original DenseNet and RetNet-110, the parameters can be efficiency reduced about 30–70%, while the accuracy degrades little. The lightweight network can be implemented on a low-cost embedded system for real-time application.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-021-01140-9,springer
Article,doi:10.1007/s11042-020-09874-7,Deep learning approaches for speech emotion recognition: state of the art and research challenges,Multimedia Tools and Applications,10.1007/s11042-020-09874-7,Springer,2021-07-01,2021-01-02,"Speech emotion recognition (SER) systems identify emotions from the human voice in the areas of smart healthcare, driving a vehicle, call centers, automatic translation systems, and human-machine interaction. In the classical SER process, discriminative acoustic feature extraction is the most important and challenging step because discriminative features influence the classifier performance and decrease the computational time. Nonetheless, current handcrafted acoustic features suffer from limited capability and accuracy in constructing a SER system for real-time implementation. Therefore, to overcome the limitations of handcrafted features, in recent years, variety of deep learning techniques have been proposed and employed for automatic feature extraction in the field of emotion prediction from speech signals. However, to the best of our knowledge, there is no in-depth review study is available that critically appraises and summarizes the existing deep learning techniques with their strengths and weaknesses for SER. Hence, this study aims to present a comprehensive review of deep learning techniques, uniqueness, benefits and their limitations for SER. Moreover, this review study also presents speech processing techniques, performance measures and publicly available emotional speech databases. Furthermore, this review also discusses the significance of the findings of the primary studies. Finally, it also presents open research issues and challenges that need significant research efforts and enhancements in the field of SER systems.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-09874-7,springer
Article,doi:10.1007/s10489-021-02631-9,A novel deep pixel restoration video prediction algorithm integrating attention mechanism,Applied Intelligence,10.1007/s10489-021-02631-9,Springer,2021-08-02,2021-08-02,"With the rapid development of deep learning, in recent years, many excellent deep learning models have been developed to solve the problem of video frame prediction. Among them, most models directly generate predicted target frames. However, the predicted frames obtained in this way are often fuzzy and not realistic enough. In order to solve this problem, this paper first attempts to integrate the attention mechanism with Convolutional Long Short-Term Memory, and correspondingly proposes a new deep learning model, abbreviated as AttConvLSTM. One prominent and original characteristic of this newly constructed model is that, each of its layer calculates the attention weight of the obtained information, focusing on the information key part. Although the proposed AttConvLSTM model effectively improves the prediction accuracy, it still does not solve the problem that the prediction frames directly generated by classical deep learning models are often fuzzy and not realistic. Therefore, inspired by the concept of optical flow, this work further develops a novel Deep Pixel Restoration AttConvLSTM (DPRAConvLSTM) model. This model cleverly uses the input frames and the end-to-end characteristics of deep learning. We innovatively restore the pixels of the input frames to the predicted frames, thereby avoiding the defects that typical deep learning models can easily cause, when directly generating the predicted frames. The experimental results effectively confirm that the finally formed DPRAConvLSTM model can not only improve the accuracy of prediction, but also obtain clearer and more realistic prediction frames.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02631-9,springer
Article,doi:10.1007/s11554-021-01071-5,Predicting behavioral competencies automatically from facial expressions in real-time video-recorded interviews,Journal of Real-Time Image Processing,10.1007/s11554-021-01071-5,Springer,2021-08-01,2021-01-27,"This work aims to develop a real-time image and video processor enabled with an artificial intelligence (AI) agent that can predict a job candidate’s behavioral competencies according to his or her facial expressions. This is accomplished using a real-time video-recorded interview with a histogram of oriented gradients and support vector machine (HOG-SVM) plus convolutional neural network (CNN) recognition. Different from the classical view of recognizing emotional states, this prototype system was developed to automatically decode a job candidate’s behaviors by their microexpressions based on the behavioral ecology view of facial displays (BECV) in the context of employment interviews using a real-time video-recorded interview. An experiment was conducted at a Fortune 500 company, and the video records and competency scores were collected from the company’s employees and hiring managers. The results indicated that our proposed system can provide better predictive power than can human-structured interviews, personality inventories, occupation interest testing, and assessment centers. As such, our proposed approach can be utilized as an effective screening method using a personal-value-based competency model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-021-01071-5,springer
Article,doi:10.1007/s11042-021-10930-z,Real-time detection method of driver fatigue state based on deep learning of face video,Multimedia Tools and Applications,10.1007/s11042-021-10930-z,Springer,2021-07-01,2021-04-17,"The use of face video information for driver fatigue detection has received extensive attention because of its low cost and non-invasiveness. However, the current vehicle-mounted embedded device has insufficient memory and limited computing power, which cannot complete the real-time detection of driver fatigue based on deep learning. Therefore, this paper designs a lightweight neural network model to solve this problem. The model includes object detection and fatigue detection. First, a lightweight object detection network is designed, which can quickly identify the opening and closing states of the driver’s eyes and mouth in the time series video. Secondly, the EYE-MOUTH (EM) driver fatigue detection model is designed, which encodes the driver’s eye and mouth opening and closing state, and calculates the driver’s PERCLOS (Percentage of Eyelid Closure over the Pupil) and FOM (Frequency of Open Mouth) according to the coding sequence. Finally, the multi-feature fusion judgment algorithm is used to realize the judgment of the driver’s fatigue state. The experimental results show that our method has an accuracy rate of 98.30% for drowsiness and yawning behaviors in a real vehicle environment, and a detection speed of 27FPS, which is better than other advanced methods and meets the requirements of real-time detection.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-10930-z,springer
Article,doi:10.1007/s11042-021-11079-5,Remote sensing image recognition based on dual-channel deep learning network,Multimedia Tools and Applications,10.1007/s11042-021-11079-5,Springer,2021-07-01,2021-05-22,"In the face of remote sensing images with diversified information, the recognition of remote sensing images only through local features or global features is limited. Traditionally, it is difficult to achieve good image modeling. To solve this problem, this paper proposes a recognition framework based on dual-channel deep learning(DCDL). The purpose of this framework is to mine global feature information and local feature information at the same time. On the first channel, this paper uses a multi-scale convolution residual network to perform local mining and residual calculations on the image to generate local features; secondly, the local attention mechanism is used to assign weight coefficients to the local features to make the information more contained. More features are more prominent; finally, after several times of mining features and assigning weights, more representative deep features are produced. On the other channel, we introduce the global attention mechanism to realize the weight coefficient distribution of global features; then use the multi-scale dilated convolution to expand the receptive field to obtain a larger range of feature information; then, use the Sigmoid function to achieve 0 to 1 for all features The weight distribution of, further expands the difference between global features; finally, the deep mining of features is realized through 2-layer convolution. In this paper, through the experimental results of the three sub-data sets in the NWPU-RESISC45 data set, we can see that our proposed algorithm has achieved higher recognition accuracy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11079-5,springer
Article,doi:10.1007/s10288-021-00473-2,Controlling the emotional expressiveness of synthetic speech: a deep learning approach,4OR,10.1007/s10288-021-00473-2,Springer,2021-06-24,2021-06-24,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10288-021-00473-2,springer
Article,doi:10.1007/s11042-021-10503-0,Machine learning and soft computing applications in multimedia,Multimedia Tools and Applications,10.1007/s11042-021-10503-0,Springer,2021-06-01,2021-02-04,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-10503-0,springer
Article,doi:10.1007/s11042-021-10923-y,Special issue on content-based multimedia indexing in the era of artificial intelligence,Multimedia Tools and Applications,10.1007/s11042-021-10923-y,Springer,2021-06-01,2021-05-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-10923-y,springer
Article,doi:10.1007/s12599-021-00689-w,Virtual Sensors,Business & Information Systems Engineering,10.1007/s12599-021-00689-w,Springer,2021-06-01,2021-03-09,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12599-021-00689-w,springer
Article,doi:10.1007/s11432-019-2956-6,3DPF-FBN: video inpainting by jointly 3D-patch filling and neural network refinement,Science China Information Sciences,10.1007/s11432-019-2956-6,Springer,2021-05-14,2021-05-14,,http://link.springer.com/openurl/pdf?id=doi:10.1007/s11432-019-2956-6,springer
Article,doi:10.1007/s10055-020-00468-0,"RUN: rational ubiquitous navigation, a model for automated navigation and searching in virtual environments",Virtual Reality,10.1007/s10055-020-00468-0,Springer,2021-06-01,2020-09-18,"By now, the realm of virtual reality is abuzz with high-quality visuals, enough to simulate a real-world scene. The use of intelligence in virtual reality systems, however, is a milestone yet to be achieved to make possible seamless realism in a virtual environment. This paper presents a model, rational ubiquitous navigation to improve believability of a virtual environment. The model intends to augment maturity of a virtual agent by inculcating in it the human-like learning capability. A novel approach for automated navigation and searching is proposed by incorporating machine learning in virtual reality. An intelligent virtual agent learns objects of interest along with the paths followed for navigation. A mental map is molded dynamically as a user navigates in the environment. The map is followed by the agent during self-directed navigation to access any known object. After reaching at a location where an object of interest resides, the required object is selected on the basis of front-facet feature. The model is implemented in a case-study project learn objects on path (LOOP). Twelve users evaluated the model in the immersive maze-like environment of LOOP. Results of the evaluation assure applicability of the model in various cross-modality applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-020-00468-0,springer
Article,doi:10.1007/s00521-020-05405-5,Three-dimensional CNN-inspired deep learning architecture for Yoga pose recognition in the real-world environment,Neural Computing and Applications,10.1007/s00521-020-05405-5,Springer,2021-06-01,2020-10-09,"Existing techniques for Yoga pose recognition build classifiers based on sophisticated handcrafted features computed from the raw inputs captured in a controlled environment. These techniques often fail in complex real-world situations and thus, pose limitations on the practical applicability of existing Yoga pose recognition systems. This paper presents an alternative computationally efficient approach for Yoga pose recognition in complex real-world environments using deep learning. To this end, a Yoga pose dataset was created with the participation of 27 individual (8 males and 19 females), which consists of ten Yoga poses, namely Malasana, Ananda Balasana, Janu Sirsasana, Anjaneyasana, Tadasana, Kumbhakasana, Hasta Uttanasana, Paschimottanasana, Uttanasana, and Dandasana. To capture the videos, we used smartphone cameras having 4 K resolution and 30 fps frame rate. For the recognition of Yoga poses in real time, a three-dimensional convolutional neural network (3D CNN) architecture is designed and implemented. The designed architecture is a modified version of the C3D architecture initially introduced for the recognition of human actions. In the proposed modified C3D architecture, the computationally intensive fully connected layers are pruned, and supplementary layers such as the batch normalization and average pooling were introduced for computational efficiency. To the best of our knowledge, this is among the first studies, which utilized the inherent spatial–temporal relationship among Yoga poses for their recognition. The designed 3D CNN architecture achieved test recognition accuracy of 91.15% on the in-house prepared Yoga pose dataset consisting of ten Yoga poses. Furthermore, on the publicly available dataset, the designed architecture achieved competitive test recognition accuracy of 99.39%, along with multifold improvement in the execution speed compared to the existing state-of-the-art technique. To promote further study, we will make the in-house created Yoga pose dataset publicly available to the research community.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-020-05405-5,springer
Article,doi:10.1007/s11416-020-00376-6,Audio signal processing for Android malware detection and family identification,Journal of Computer Virology and Hacking Techniques,10.1007/s11416-020-00376-6,Springer,2021-06-01,2021-01-07,"Mobile malware is increasing in complexity and maliciousness, with particular regard to the malicious samples targeting the Android platform, currently the most widespread operating system for mobile devices. In this scenario antimalware technologies are not able to detect the so-called zero-day malware, because they are able to detect mobile malware only once their malicious signature is stored in the antimalware repository (i.e., the so-called signature based approach). From these considerations, in this paper an approach for detecting Android malware is proposed. Moreover the proposed approach aims to detect the belonging family of the malicious sample under analysis. We represent the executable of the application in term of audio file and, exploiting audio signal processing techniques, we extract a set of numerical features from each sample. Thus, we build several machine learning models and we evaluate their effectiveness in terms of malware detection and family identification. We experiment the method we propose on a data-set composed by 50,000 Android real-world samples (24,553 malicious among 71 families and 25,447 legitimate), by reaching an accuracy equal to 0.952 in Android malware detection and of 0.922 in family detection.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11416-020-00376-6,springer
Article,doi:10.1007/s00521-021-06072-w,Video analysis of intelligent teaching based on machine learning and virtual reality technology,Neural Computing and Applications,10.1007/s00521-021-06072-w,Springer,2021-05-08,2021-05-08,"In this paper, we study and analyse the teaching video of oil painting art through machine learning combined with virtual reality computing. Since the current oil painting, image acquisition method cannot meet the user's demand for multi-dimensional image description, and at the same time the retrieval method is too simple to perform high flexibility retrieval, we try to adopt the deep learning-based object extraction fusion method. Also, objects with poor performance quality are not suitable for further interception and cutting. We first pre-process the images in the image library to filter out the relatively high-quality images and then filter out the objects whose saliency and clarity do not reach the queue value by judging the saliency and clarity values of the images. Next, a series of aesthetic criteria, such as visual balance, visual triangulation, and centrosymmetric diagonal composition criteria, used to further filter the objects with relatively poor quality and low ratings. Then, we expand the areas with high saliency, match the contours of the segmented image elements with the contours of the user-drawn image to return an optimal matching value, and finally improve the quality and naturalness of the image by learning the deeper features of the image based on the style migration. The experimental framework based on TensorFlow is a new application of deep learning in the field of image synthesis, which has a very good improvement in the implementation efficiency compared with the traditional method. Using virtual reality technology to carry out teaching practice and analyse the effect of teaching practice, students can immerse themselves in art appreciation teaching activities, accept multiculturalism, learn through experience, and improve aesthetic quality.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06072-w,springer
Article,doi:10.1007/s11740-021-01020-y,Modeling Fused Filament Fabrication using Artificial Neural Networks,Production Engineering,10.1007/s11740-021-01020-y,Springer,2021-06-01,2021-02-07,"With industries pushing towards digitalized production, adaption to expectations and increasing requirements for modern applications, has brought additive manufacturing (AM) to the forefront of Industry 4.0. In fact, AM is a main accelerator for digital production with its possibilities in structural design, such as topology optimization, production flexibility, customization, product development, to name a few. Fused Filament Fabrication (FFF) is a widespread and practical tool for rapid prototyping that also demonstrates the importance of AM technologies through its accessibility to the general public by creating cost effective desktop solutions. An increasing integration of systems in an intelligent production environment also enables the generation of large-scale data to be used for process monitoring and process control. Deep learning as a form of artificial intelligence (AI) and more specifically, a method of machine learning (ML) is ideal for handling big data. This study uses a trained artificial neural network (ANN) model as a digital shadow to predict the force within the nozzle of an FFF printer using filament speed and nozzle temperatures as input data. After the ANN model was tested using data from a theoretical model it was implemented to predict the behavior using real-time printer data. For this purpose, an FFF printer was equipped with sensors that collect real time printer data during the printing process. The ANN model reflected the kinematics of melting and flow predicted by models currently available for various speeds of printing. The model allows for a deeper understanding of the influencing process parameters which ultimately results in the determination of the optimum combination of process speed and print quality.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11740-021-01020-y,springer
Article,doi:10.1007/s00170-021-07156-6,An image-based approach to predict instantaneous cutting forces using convolutional neural networks in end milling operation,The International Journal of Advanced Manufacturing Technology,10.1007/s00170-021-07156-6,Springer,2021-07-01,2021-05-16,"Cutting force detection can contribute to predicting the productivity and quality of end milling operations. Instantaneous cutting force prediction of digital twins in end milling operations should be near real-time and accurate. This paper proposes an image-based approach that can contain more useful information due to a higher dimension and simplify the complexity of computing geometric data. The cutter frame image (CFI) is utilized as one of the inputs of a convolutional neural network (CNN) to predict instantaneous cutting forces. Considering the convenience of capturing massive data, the approach uses cutting forces generated from a mechanistic force model instead of experimental cutting forces to train the CNN. The correlation coefficient R ^2 value between predicted results and simulated results is 0.9999 and the average time cost per image is 0.057 s in a cutting condition, which validates the possibility to use the image-based method to predict instantaneous cutting forces accurately and efficiently in the digital twin.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00170-021-07156-6,springer
Article,doi:10.1007/s11042-019-07752-5,Analysis of synchronized storage method for multimedia key areas based on machine learning,Multimedia Tools and Applications,10.1007/s11042-019-07752-5,Springer,2021-06-01,2019-05-18,"Aiming at the problems of high energy consumption, small amount of stored data, large standard deviation of storage space, high failure rate of storage nodes and poor quality of data storage in existing multimedia key area synchronous storage methods, a machine learning based multimedia key area synchronous storage method is proposed. Using genetic algorithm to calculate the distance between data in multimedia critical area and cluster center, and redistribute cluster set, K-Mean is realized by M-R parallel computing model. Different amounts of data are allocated to the clustered sample data storage nodes to complete the synchronous storage of multimedia key area data. The experimental results show that compared with other methods, under the network regulation of 2800 m × 800 m, the storage energy consumption of the proposed method is in the range of 10 × 10^3NJ-1250 × 10^3NJ, and the storage energy consumption is low; The maximum number of data storage is 570, and the amount of stored data is large; the standard deviation of data storage space in the key area of multimedia changes within the range of 1.3–4, and the standard deviation of storage space is small. The proposed method lays a foundation for the further development of data storage technology.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-019-07752-5,springer
Article,doi:10.1007/s00170-021-06976-w,Digital twin–driven aero-engine intelligent predictive maintenance,The International Journal of Advanced Manufacturing Technology,10.1007/s00170-021-06976-w,Springer,2021-06-01,2021-04-30,"Aero-engine is one of the most important components of an aircraft. The development of maintenance has undergone the transition from “post-event maintenance” and “preventive maintenance” to “predictive maintenance”, and the future development direction is precise maintenance, which aims to achieve the collaborative optimization goal of ensuring operational safety and reducing operating costs. To improve the effect of predictive engine maintenance, the aero-engine predictive maintenance framework driven by digital twin (DT) is studied, and the implicit digital twin (IDT) model is mined. The validity of the model is verified by the consistency evaluation of virtual and real data assets. Combining the data-driven with LSTM model of deep learning method and taking an aero-engine as an example can show that the method is effective. Experimental results show that when the data set used for model training is 80%, the model prediction has high accuracy, and the RMSE predicted by aero-engine RUL is 13.12, which is better than other experimental schemes.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00170-021-06976-w,springer
Article,doi:10.1007/s00530-020-00724-y,Video-based driver action recognition via hybrid spatial–temporal deep learning framework,Multimedia Systems,10.1007/s00530-020-00724-y,Springer,2021-06-01,2021-01-22,"Driver action recognition aims to distinguish normal driver action and some abnormal driver actions such as leaving the wheel, talking on the phone, diving with smoking, etc. For the purpose of traffic safety, studies on the computer vision technologies for driver action recognition have become especially meaningful. However, this issue is far from being solved, mainly due to the subtle variations between different driver action classes. In this paper, we present a new video-based driver action recognition approach based on the hybrid spatial–temporal deep learning framework. Specifically, we first design an encoder–decoder spatial–temporal convolutional neural network (EDSTCNN) to capture short-term spatial–temporal representation of driver actions jointly with optical flow prediction. Second, we exploit the feature refinement network (FRN) to refine the short-term driver action feature. Then, convolutional long short-term memory network (ConvLSTM) is employed for long-term spatial–temporal fusion. Finally, the fully connected neural network (FCNN) is used for final driver action recognition. In our experiment, we validate the performance of the proposed framework on our self-created datasets, including a simulated driving dataset and a real driving dataset. Extensive experimental results illustrate that the proposed hybrid spatial–temporal deep learning framework obtains the highest accuracy in multiple driver action recognition datasets (98.9% on SEU-DAR-V1 dataset and 97.0% on SEU-DAR-V2 dataset).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-020-00724-y,springer
Article,doi:10.1007/s11227-020-03504-7,Efficient covering of target areas using a location prediction-based algorithm,The Journal of Supercomputing,10.1007/s11227-020-03504-7,Springer,2021-06-01,2020-11-19,"Due to the rapid development of the high-speed wired and wireless Internet, image contents including objects with exposed personal information are being distributed freely, which is a social problem. In this paper, we introduce a method of robustly detecting a target object with facial region exposed from an image that is quickly entered using skin color and a deep learning algorithm and effectively covering the detected target object through prediction. The proposed method in this paper accurately detects the target object containing facial region exposed from the image entered by applying an image adaptive skin color model and a CNN-based deep learning algorithm. Subsequently, the location prediction algorithm is used to quickly track the detected object. A mosaic is overlapped over the target object area to effectively protect the object area where the facial region is exposed. The experimental results show that the proposed approach accurately detects the target object including the facial region exposed from the continuously entered video, and efficiently covers the detected object through mosaic processing while quickly tracking it using a prediction-based tracking algorithm. The tracking-based target covering method proposed in this study is expected to be useful in various practical applications related to pattern recognition and image security, such as content-based image retrieval, real-time surveillance, human–computer interaction, and face detection.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-020-03504-7,springer
Article,doi:10.1007/s11042-020-10280-2,Multimodal deep learning for multimedia understanding and reasoning,Multimedia Tools and Applications,10.1007/s11042-020-10280-2,Springer,2021-05-01,2021-01-06,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-10280-2,springer
Article,doi:10.1007/s11042-020-10282-0,FiM’s DE - the communication package for the creative pipeline,Multimedia Tools and Applications,10.1007/s11042-020-10282-0,Springer,2021-05-01,2021-02-16,"The FotoInMotion ( FiM ) project is building a novel media creation platform, leveraging the use of semi-automated analysis and editing tools to empower creators to easily transform static visual acquisitions of real-world events into rich, animated and engaging objects, distributable through common channels. FiM transforms the content creative chain into an integrated pipeline across which media and metadata seamlessly flow and are exploited to produce more complex media objects. One of the addressed challenges consists the need for a seamless and efficient communication across such pipeline and on how to preserve, in a structured manner, all of the involved media and metadata. Existing standardized metadata tools and content wrappers are limited in expressivity and scope and incapable of fully supporting the needs of the content creative pipeline. This paper describes FiM ’s new structured data object, i.e. the Digital Event (DE), which acts as a universal vehicle for media and metadata. It builds on well-established and emergent MPEG standards (MPEG-21, MPEG-V, MPEG-7 and MPEG HEIF), to support data diversity, interoperability, packaging and sharing, within complex, Machine Learning enhanced, creative pipelines. Our solution has been validated by creative professionals (photojournalism, fashion marketing and festivals), who have conducted experiments within the context of different creative workflows in real world scenarios. DE’s employment revealed to be advantageous, particularly in the homogenization of the media and metadata representation and packaging and in the normalization of the interaction between different pipeline components.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-10282-0,springer
Article,doi:10.1186/s12893-021-01123-4,Supporting laparoscopic general surgery training with digital technology: The United Kingdom and Ireland paradigm,BMC Surgery,10.1186/s12893-021-01123-4,BioMed Central,2021-03-08,2021-03-08,"Surgical training in the UK and Ireland has faced challenges following the implementation of the European Working Time Directive and postgraduate training reform. The health services are undergoing a digital transformation; digital technology is remodelling the delivery of surgical care and surgical training. This review aims to critically evaluate key issues in laparoscopic general surgical training and the digital technology such as virtual and augmented reality, telementoring and automated workflow analysis and surgical skills assessment. We include pre-clinical, proof of concept research and commercial systems that are being developed to provide solutions. Digital surgical technology is evolving through interdisciplinary collaboration to provide widespread access to high-quality laparoscopic general surgery training and assessment. In the future this could lead to integrated, context-aware systems that support surgical teams in providing safer surgical care.",https://www.biomedcentral.com/openurl?doi=10.1186/s12893-021-01123-4,springer
Article,doi:10.1038/s41586-020-03152-0,Towards real-time photorealistic 3D holography with deep neural networks,Nature,10.1038/s41586-020-03152-0,Nature,2021-03-11,2021-03-10,"The ability to present three-dimensional (3D) scenes with continuous depth sensation has a profound impact on virtual and augmented reality, human–computer interaction, education and training. Computer-generated holography (CGH) enables high-spatio-angular-resolution 3D projection via numerical simulation of diffraction and interference^ 1 . Yet, existing physically based methods fail to produce holograms with both per-pixel focal control and accurate occlusion^ 2 , 3 . The computationally taxing Fresnel diffraction simulation further places an explicit trade-off between image quality and runtime, making dynamic holography impractical^ 4 . Here we demonstrate a deep-learning-based CGH pipeline capable of synthesizing a photorealistic colour 3D hologram from a single RGB-depth image in real time. Our convolutional neural network (CNN) is extremely memory efficient (below 620 kilobytes) and runs at 60 hertz for a resolution of 1,920 × 1,080 pixels on a single consumer-grade graphics processing unit. Leveraging low-power on-device artificial intelligence acceleration chips, our CNN also runs interactively on mobile (iPhone 11 Pro at 1.1 hertz) and edge (Google Edge TPU at 2.0 hertz) devices, promising real-time performance in future-generation virtual and augmented-reality mobile headsets. We enable this pipeline by introducing a large-scale CGH dataset (MIT-CGH-4K) with 4,000 pairs of RGB-depth images and corresponding 3D holograms. Our CNN is trained with differentiable wave-based loss functions^ 5 and physically approximates Fresnel diffraction. With an anti-aliasing phase-only encoding method, we experimentally demonstrate speckle-free, natural-looking, high-resolution 3D holograms. Our learning-based approach and the Fresnel hologram dataset will help to unlock the full potential of holography and enable applications in metasurface design^ 6 , 7 , optical and acoustic tweezer-based microscopic manipulation^ 8 – 10 , holographic microscopy^ 11 and single-exposure volumetric 3D printing^ 12 , 13 . A deep-learning-based approach using a convolutional neural network is used to synthesize photorealistic colour three-dimensional holograms from a single RGB-depth image in real time, and termed tensor holography.",https://www.nature.com/articles/s41586-020-03152-0,springer
Article,doi:10.1038/s41467-021-22696-x,Neural network based 3D tracking with a graphene transparent focal stack imaging system,Nature Communications,10.1038/s41467-021-22696-x,Nature,2021-04-23,2021-04-23,"Recent years have seen the rapid growth of new approaches to optical imaging, with an emphasis on extracting three-dimensional (3D) information from what is normally a two-dimensional (2D) image capture. Perhaps most importantly, the rise of computational imaging enables both new physical layouts of optical components and new algorithms to be implemented. This paper concerns the convergence of two advances: the development of a transparent focal stack imaging system using graphene photodetector arrays, and the rapid expansion of the capabilities of machine learning including the development of powerful neural networks. This paper demonstrates 3D tracking of point-like objects with multilayer feedforward neural networks and the extension to tracking positions of multi-point objects. Computer simulations further demonstrate how this optical system can track extended objects in 3D, highlighting the promise of combining nanophotonic devices, new optical system designs, and machine learning for new frontiers in 3D imaging. Transparent photodetectors based on graphene stacked vertically along the optical axis have shown promising potential for light field reconstruction. Here, the authors develop transparent photodetector arrays and implement a neural network for real-time 3D optical imaging and object tracking.",https://www.nature.com/articles/s41467-021-22696-x,springer
Article,doi:10.1007/s11042-020-08858-x,Behavioral features fusion for ethological CNN classification of open field test videos,Multimedia Tools and Applications,10.1007/s11042-020-08858-x,Springer,2021-05-01,2020-04-14,"In both ethological and pharmacological experiments, open field test (OFT) is a classic experiment for measuring mouse general activity and exploratory behaviors. The OFT manual analysis is usually a time consuming and costly process. In this paper, we propose a convolutional neural network (CNN) based classification, which can automatically sort out two groups of mice with different behaviors. We first extract the multi-modality features from both spatial and time domain of the OFT video. Then, all extracted features are regularized and fused to a 2D behavior feature map. Finally, we train a CNN network to distinguish mice with different behaviors. On real OFT datasets experiments, the proposed classification significantly outperforms other methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-08858-x,springer
Article,doi:10.1007/s10772-021-09848-6,Nonlinear acoustic noise cancellation based automatic speech recognition system (NANC-ASR) with convolutional neural networks,International Journal of Speech Technology,10.1007/s10772-021-09848-6,Springer,2021-04-30,2021-04-30,"Automatic Speech Recognition (ASR) is a self-governing, computer-based spoken language transcript for real-time applications. It is used in various real time applications and it listens the speech signals through a microphone, identifies the words, and assists a network in converting the written text. When we use the ASR system in multiple environments there is a possibility of ambient noise captured by a microphone unit and ASR system doesn’t predicting correct words. The Non-linear Acoustic Noise Cancellation (NANC) approach based automatic speech recognition method focused on the properties of non-linear sound noise cancellation. There are several distinct small segments in this approach, such as speech signal sounds, syllables, and so on. As an acrylic symbol associated with organs, these units analyze syllables to find acoustic properties of speech signals. This experimental study has adopted Convolutional Neural Network (CNN) based noise reduction in the speech recognition system with an accuracy of 98.5%. Finally, a speech signal has been identified through the ASR's vocabulary, which has been obtained with correct words after all phonetic signs are present.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-021-09848-6,springer
Article,doi:10.1007/s00371-021-02135-0,"
            
              
            
            $$\hbox {PISEP}{^2}$$
            
              
                PISEP
                
                  
                  2
                
              
            
          : pseudo-image sequence evolution-based 3D pose prediction",The Visual Computer,10.1007/s00371-021-02135-0,Springer,2021-04-24,2021-04-24,"Pose prediction is to predict future poses given a window of previous poses. In this paper, we propose a new problem that predicts poses using 3D positions of skeletal sequences.Different from the traditional pose prediction based on mocap frames, this problem is convenient to use in real applications due to its simple sensors to capture data. We also present a new framework, pseudo-image sequence evolution-based 3D pose prediction, to address this new problem. Specifically, a skeletal representation is proposed by transforming a 3D skeletal sequence into an image sequence, which can model different correlations among different joints. With this image-based skeletal representation, we model the pose prediction as the evolution of an image sequence. Moreover, a novel inference network is proposed to predict multiple future poses in a non-recursive manner using decoders with independent parameters. In contrast to the recursive sequence-to-sequence model, we can improve the computational efficiency and avoid error accumulations significantly. Extensive experiments are carried out on two benchmark datasets (e.g., G3D and FNTU). The proposed method achieves state-of-the-art performance on both datasets, which demonstrates the effectiveness of our proposed method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02135-0,springer
Article,doi:10.1007/s00521-020-05226-6,Multi-scale generative adversarial network for improved evaluation of cell–cell interactions observed in organ-on-chip experiments,Neural Computing and Applications,10.1007/s00521-020-05226-6,Springer,2021-04-01,2020-07-30,"Organs On a Chip (OOCs) represent a sophisticated approach for exploring biological mechanisms and developing therapeutic agents. In conjunction with high-quality time-lapse microscopy (TLM), OOCs allow for the visualization of reconstituted complex biological processes, such as multi-cell-type migration and cell–cell interactions. In this context, increasing the frame rate is desirable to reconstruct accurately cell-interaction dynamics. However, a trade-off between high resolution and carried information content is required to reduce the overall data volume. Moreover, high frame rates increase photobleaching and phototoxicity. As a possible solution for these problems, we report a new hybrid-imaging paradigm based on the integration of OOC/TLMs with a Multi-scale Generative Adversarial Network (GAN) predicting interleaved video frames with the aim to provide high-throughput videos. We tested the performance of the predictive capability of GAN on synthetic videos, as well as on real OOC experiments dealing with tumor–immune cell interactions. The proposed approach offers the possibility to acquire a reduced number of high-quality TLM images without any major loss of information on the phenomena under investigation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-020-05226-6,springer
Chapter,doi:10.1007/978-3-030-76387-9_2,Cyber-Physical-Social Systems: An Overview,Smart Connected World,10.1007/978-3-030-76387-9_2,Springer,2021-01-01,2021-09-28,"For the past few years, devices or systems have been transformed into smart connected devices or systems, which are widely named as Internet of Things (IoT) and cyber-physical systems (CPSs). Integrating social networks to CPSs results in a new paradigm called cyber-physical-social systems (CPSSs). CPSSs include the human-to-device communications and device-to-device communications and create continuous interaction of human-device relationships. The fifth-generation (5G) wireless communication networks, wireless sensor networks, big data, artificial intelligence (AI), virtual reality (VR), and augmented reality (AR) are the supporting technologies of CPSSs. This chapter discusses the concept of the CPSSs, their supporting technologies, and the implementation of CPSSs in different domains. It also observes current challenges such as privacy concerns, ethical issues, safety, security, interdependence, and compatibility.",http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-030-76387-9_2,springer
Article,doi:10.1007/s12065-020-00559-6,An improved optimal algorithm for collision detection of hybrid hierarchical bounding box,Evolutionary Intelligence,10.1007/s12065-020-00559-6,Springer,2021-02-01,2021-02-01,"Collision detection is currently a hot issue in virtual reality and other fields. The efficiency and accuracy of collision detection directly affect the real-time update effect of the virtual reality environment, and it is also an important indicator that affects the user's interactive experience. In a complex virtual reality scene, if the traditional collision detection algorithm (Sphere-OBB) is adopted and the tree structure traversal is used to realize the bounding box traversal detection, the accuracy remains unchanged, but the detection complexity is reduced. If the RAPID collision detection algorithm is used, the separated axis test method and the two-layer hybrid hierarchical surrounding tree structure are used, although the amount of calculation is large, the detection efficiency is improved. Using the separation axis (SAT) algorithm and using the separation axis theorem to determine the vector axis can save a lot of calculation time. The purpose of this research is to propose an improved hybrid-level bounding box collision detection optimization algorithm (ASO) based on the traditional hybrid-level bounding box collision detection algorithm. Firstly, based on the spatio-temporal correlation theory, the hybrid hierarchical bounding box hierarchical tree structure is improved to AABB and OBB from top to bottom. The synchronous descent rule is used to realize the traversal of nodes, and then the triangle area weighting method is used to improve the calculation method of the bottom OBB bounding box node center, solve the bounding box vertex covariance matrix, and improve the efficiency and accuracy of collision detection. The experimental results show that the algorithm proposed in this paper is 35.6% faster than the RAPID detection speed and 29.9% faster than the separation axis (SAT) detection speed under the same accuracy. In the multi-object collision detection, compared with the latest research, the algorithm in this paper shortens the intersection detection time, improves the collision detection efficiency, and meets the real-time update requirements of complex virtual reality scenes.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12065-020-00559-6,springer
Article,doi:10.1186/s40537-020-00401-x,Multiclass emotion prediction using heart rate and virtual reality stimuli,Journal of Big Data,10.1186/s40537-020-00401-x,Springer,2021-01-07,2021-01-07,"Background Emotion prediction is a method that recognizes the human emotion derived from the subject’s psychological data. The problem in question is the limited use of heart rate (HR) as the prediction feature through the use of common classifiers such as Support Vector Machine (SVM), K-Nearest Neighbor (KNN) and Random Forest (RF) in emotion prediction. This paper aims to investigate whether HR signals can be utilized to classify four-class emotions using the emotion model from Russell’s in a virtual reality (VR) environment using machine learning. Method An experiment was conducted using the Empatica E4 wristband to acquire the participant’s HR, a VR headset as the display device for participants to view the 360° emotional videos, and the Empatica E4 real-time application was used during the experiment to extract and process the participant's recorded heart rate. Findings For intra-subject classification, all three classifiers SVM, KNN, and RF achieved 100% as the highest accuracy while inter-subject classification achieved 46.7% for SVM, 42.9% for KNN and 43.3% for RF. Conclusion The results demonstrate the potential of SVM, KNN and RF classifiers to classify HR as a feature to be used in emotion prediction in four distinct emotion classes in a virtual reality environment. The potential applications include interactive gaming, affective entertainment, and VR health rehabilitation.",https://www.biomedcentral.com/openurl?doi=10.1186/s40537-020-00401-x,springer
Article,doi:10.1007/s00138-020-01158-2,Online inspection of narrow overlap weld quality using two-stage convolution neural network image recognition,Machine Vision and Applications,10.1007/s00138-020-01158-2,Springer,2021-01-03,2021-01-03,"In narrow overlap welding, serious defects in the weld will lead to band breakage accident, and the whole hot dip galvanizing unit will be shut down. Laser vision inspection hardware is used to collect real-time image of weld surface, and image defect recognition and evaluation system is developed to real-time detect quality. Firstly, region division is implemented. In view of the characteristics of gray image such as large information, low contrast and blurred edge, an improved image segmentation algorithm is proposed by using image convolution to enhance edge features and combining with integral image, which can quickly and accurately extract the weld edge and divide the region, and the processing time can meet the real-time requirements. Then comparing the effect of traditional method and convolution neural network in identifying defects, VGG16 is used to identify weld defects. In order to ensure real-time performance, a two-stage weld defect recognition is proposed. First, the large defective area is identified, and then, the defect is accurately identified in the area. This method can quickly extract defect areas and complete weld defect classification. Experiments show that the accuracy can reach 97% and average running time takes 3.2 s, meeting the online detection requirements.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00138-020-01158-2,springer
Article,doi:10.1186/s13636-021-00199-3,Accent modification for speech recognition of non-native speakers using neural style transfer,"EURASIP Journal on Audio, Speech, and Music Processing",10.1186/s13636-021-00199-3,Springer,2021-02-18,2021-02-18,"Nowadays automatic speech recognition (ASR) systems can achieve higher and higher accuracy rates depending on the methodology applied and datasets used. The rate decreases significantly when the ASR system is being used with a non-native speaker of the language to be recognized. The main reason for this is specific pronunciation and accent features related to the mother tongue of that speaker, which influence the pronunciation. At the same time, an extremely limited volume of labeled non-native speech datasets makes it difficult to train, from the ground up, sufficiently accurate ASR systems for non-native speakers.In this research, we address the problem and its influence on the accuracy of ASR systems, using the style transfer methodology. We designed a pipeline for modifying the speech of a non-native speaker so that it more closely resembles the native speech. This paper covers experiments for accent modification using different setups and different approaches, including neural style transfer and autoencoder. The experiments were conducted on English language pronounced by Japanese speakers ( UME-ERJ dataset). The results show that there is a significant relative improvement in terms of the speech recognition accuracy. Our methodology reduces the necessity of training new algorithms for non-native speech (thus overcoming the obstacle related to the data scarcity) and can be used as a wrapper for any existing ASR system. The modification can be performed in real time, before a sample is passed into the speech recognition system itself.",https://www.biomedcentral.com/openurl?doi=10.1186/s13636-021-00199-3,springer
Article,doi:10.1007/s11042-020-09964-6,A deep learning approach to building an intelligent video surveillance system,Multimedia Tools and Applications,10.1007/s11042-020-09964-6,Springer,2021-02-01,2020-10-07,"Recent advances in the field of object detection and face recognition have made it possible to develop practical video surveillance systems with embedded object detection and face recognition functionalities that are accurate and fast enough for commercial uses. In this paper, we compare some of the latest approaches to object detection and face recognition and provide reasons why they may or may not be amongst the best to be used in video surveillance applications in terms of both accuracy and speed. It is discovered that Faster R-CNN with Inception ResNet V2 is able to achieve some of the best accuracies while maintaining real-time rates. Single Shot Detector (SSD) with MobileNet, on the other hand, is incredibly fast and still accurate enough for most applications. As for face recognition, FaceNet with Multi-task Cascaded Convolutional Networks (MTCNN) achieves higher accuracy than advances such as DeepFace and DeepID2+ while being faster. An end-to-end video surveillance system is also proposed which could be used as a starting point for more complex systems. Various experiments have also been attempted on trained models with observations explained in detail. We finish by discussing video object detection and video salient object detection approaches which could potentially be used as future improvements to the proposed system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-09964-6,springer
Article,doi:10.1007/s00500-021-05586-8,Detection of shilling attack in recommender system for YouTube video statistics using machine learning techniques,Soft Computing,10.1007/s00500-021-05586-8,Springer,2021-01-28,2021-01-28,"Literature survey shows that the recommendation systems have been largely adapted and evaluated in various domains. Due to low performances from various cyber attacks, the adoption of recommender system is in the initial stage of defense systems. One of the most common attacks for recommender system is shilling attack. There are some existing techniques for identifying the shilling attacks built in the user ratings patterns. The performance of ratings on target items differs between the attack user profiles and actual user profiles. To differentiate the certain profiles, the affected profiles are known as attack profiles. Besides the shilling attacks, real cyber attacks are taking place in the community which are being solved by Petri Net methods. These attacks can be falsely predicted (shilling attacks) by the users which can raise security threats. For identifying various shilling attacks without a priori knowledge, Recommendation System suffers from low accuracy. Basically, recommendation attack is split into nuke and push attack that encourage and discourage the recommended target item. The strength of shilling attack is usually measured by filler size and attack size. An experiment over unsupervised machine learning algorithms with filler size 3% over 3%, 5%, 8% and 10% attack sizes is presented for Netflix dataset. Furthermore, we conducted an experiment on data of 26 K videos on the Trending YouTube Video Statistics, to predict the user preferences for a particular genre of videos using Machine Learning Algorithms. Based on the results, it observed that the Boosted Decision tree performs the best with an accuracy of 99 percent.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-021-05586-8,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-77246-8_6,Bidirectional Octonion Long-Short Term Memory Recurrent Neural Networks for Speech Recognition,Artificial Intelligence Systems and the Internet of Things in the Digital Era,10.1007/978-3-030-77246-8_6,Springer,2021-01-01,2021-05-29,"Over the few decades onwards all the researchers got marvelous attention on machine learning due to a lot of applications in different fields like image processing, speech processing, etc. Automatic Speech Recognition (ASR) is an application of speech processing that achieved tremendous results due to the usage of recurrent neural networks (RNN). A unique type of recurrent neural network is long short-term memory (LSTM), to keeps away from the problem of long term dependencies. In this paper, multidimensional octonion algebra is used to process the input entities with multidimensions efficiently, when compared to real-valued models octonion numbers and octonion neural networks solve many tasks with less learning parameters. We propose a new octonion value long-short term memory (OVLSTM) to efficiently represent long-term dependencies among features in speech sequences prediction. The TIMIT dataset taken for experiments for speech recognition and results are compared with QLSTMS and LSTMs, OVLSTM with less learning parameters reaches better results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-77246-8_6,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-8677-4_2,An Automatic Violence Detection Technique Using 3D Convolutional Neural Network,Sustainable Communication Networks and Application,10.1007/978-981-15-8677-4_2,Springer,2021-01-01,2021-01-26,"In the field of machine learning, the deep learning technique plays a very important role as it is useful in various real-life domains. As various crimes and misdeeds are occurring in various public places because of lack of proper monitoring, a number of methods have been proposed for detecting violence from videos. Automatic violence detection has gained increased research importance in case of video surveillance. However, they suffer from various limitations and most of the times it depends on special criteria. In this perspective, this paper proposes an effective violence detection method from videos using 3D convolutional neural network. The proposed methodology uses machine learning and deep learning techniques for improving the accuracy. Comprehensive performance analyses have proven that the proposed method achieves high performance in case of detecting violence from videos. The experimental results also prove that the proposed technique outperforms various other existing methods for detecting violence from videos.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-8677-4_2,springer
Chapter ConferencePaper,doi:10.1007/978-981-16-0942-8_48,Comparative Analysis of Machine Learning and Deep Learning Algorithms for Detection of Online Hate Speech,Advances in Mechanical Engineering,10.1007/978-981-16-0942-8_48,Springer,2021-01-01,2021-06-27,"In the day and age of social media, users have become prone to online hate speech. Several attempts have been made to classify hate speech using machine learning, but the state-of-the-art models are not robust enough for practical applications. This is attributed to the use of primitive NLP feature engineering techniques. In this paper, we explored various feature engineering techniques ranging from different embeddings to conventional NLP algorithms. We also experimented with combinations of different features. From our experimentation, we realized that robustly optimized BERT approach (RoBERTa)-based sentence embeddings classified using decision trees gives the best results of 0.9998 F _1 score. In our paper, we concluded that BERT based embeddings give the most useful features for this problem and have the capacity to be made into a practical robust model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-0942-8_48,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-8354-4_24,Augmented Reality in Visual Learning,ICT Analysis and Applications,10.1007/978-981-15-8354-4_24,Springer,2021-01-01,2020-12-16,"Augmented reality (AR) is one of the upcoming technologies in the area of information technology. It is gaining wide momentum and is being deployed in various fields of advertising, marketing, manufacturing, health care, tourism, etc. With the increase in the usage of smart phones, Internet and cloud connectivity, mobile technology and augmented reality can be integrated to provide a richer learning experience in the field of higher education and research. Complex concepts, practical hands-on and laboratory experiments can be captured within customized AR application to enhance learning and provide in-depth understanding to students. This paper elaborates customized software design and development for mobile-based AR application to capture details specific to Indian medicinal plants. This application deployed on smart phones overlays digital information when a smart phone camera points to specific botanical plants. This application relies on Vuforia’s Image Target recognition that is a type of marker-based algorithm.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-8354-4_24,springer
Chapter ReferenceWorkEntry,doi:10.1007/978-3-030-58080-3_164-1,AIM in Endoscopy Procedures,Artificial Intelligence in Medicine,10.1007/978-3-030-58080-3_164-1,Springer,2021-01-01,2021-08-26,"Artificial intelligence (AI) is revolutionizing the way medicine is practiced. In this context, the application of AI algorithms in endoscopy is gaining increasing attention so that modern endoscopy is moving towards more and more assisted/automatic solutions. Several approaches have been carried out in order to improve accuracy in diagnosis and surgical procedures. In this chapter, a general overview of the main contributions in the field is surveyed. Four main categories of applications were identified, namely, (i) detection and diagnosis during endoscopic procedure, (ii) informative frame selection, (iii) mosaicking and surface reconstruction, (iv) augmented reality systems for intraoperative assistance and surgeon training. Discussions on future research directions and implementation in clinical practice are provided.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-58080-3_164-1,springer
Chapter,doi:10.1007/978-3-030-54564-2_27,A Machine Learning Approach to Automatic Phobia Therapy with Virtual Reality,Modern Approaches to Augmentation of Brain Function,10.1007/978-3-030-54564-2_27,Springer,2021-01-01,2021-08-26,"Phobia is an anxiety disorder that affects 13% of the world’s population and it is manifested through an extreme and irrational fear toward objects or situations. A lot of research has been done on studying the contributing factors to the onset, development, and maintenance of phobias, underlying cognitive and behavioral processes, physical manifestation, and treatment methods. In this chapter, we present a new automated approach to phobias therapy, based on the integration of virtual reality technology, artificial intelligence, and affective computing—the ability of computational machines to recognize, adapt, and respond intelligently to human emotions. First, we introduce the results obtained by applying various machine learning algorithms for classifying the six basic emotions—anger, disgust, fear, joy, sadness, and surprise—based on the physiological recordings and self-reported ratings from the DEAP database. Then we present the stages of development and evaluation of a virtual environment for treating acrophobia that relies on gradual exposure to stimuli, accompanied by physiological signals monitoring. In a pilot experiment, we proposed a model for automatically adapting exposure scenarios according to the users’ emotional states, which has been tested on four acrophobic subjects. Here, the results obtained pass through a critical analysis in which we discuss the progress of the research and current limitations. Finally, we introduce the conceptual design of a system for phobias treatment that can be used as a therapy tool in clinical setups and also for home training, integrating virtual reality, machine learning, gamification techniques, biophysical data recording, emotion recognition, and automatic scenario adaptation. The system uses an intelligent virtual therapist that can understand emotions based on physiological signals, provides encouragement, and changes the level of exposure according to the user’s affective states.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-54564-2_27,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-53980-1_112,3D Modeling System of Lidar Point Cloud Processing Algorithm Based on Artificial Intelligence,2020 International Conference on Applications and Techniques in Cyber Intelligence,10.1007/978-3-030-53980-1_112,Springer,2021-01-01,2020-08-13,"With the continuous expansion of the application field of artificial intelligence-based unmanned vehicles and the rapid development of lidar scanning technology, the application of lidar gradually spread to many artificial intelligence-based unmanned vehicles such as environmental perception, augmented reality, and environmental modeling Technology area. Therefore, the research of lidar in the application of artificial intelligence-based unmanned vehicles has become an inevitable trend in the field of unmanned vehicles. At the same time, the research of lidar data processing technology is of great significance to the development of artificial intelligence unmanned vehicles. This article is based on the research of lidar-based 3D environment modeling technology based on lidar. The research content mainly involves vehicle lidar point cloud 3D environment modeling method, adaptive lidar point cloud data matching algorithm, lidar point cloud-based 3D map modeling application. Through theoretical research and experimental verification of related technical issues, an in-depth study of the three-dimensional terrain modeling technology of unmanned vehicles based on artificial intelligence based on lidar is carried out. This paper proposes a three-dimensional environment modeling method for vehicle lidar point cloud. First, preprocessing processes such as data filtering and terrain segmentation are performed on the original lidar data, and then a three-dimensional geometric model of the environment including surface obstacles and terrain is established through data interpolation and gridding. The verification of the measured data on the typical environment shows the effectiveness of the method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-53980-1_112,springer
Chapter ConferencePaper,doi:10.1007/978-981-33-4367-2_18,Semantics Exploration for Automatic Bangla Speech Recognition,Emerging Technologies in Data Mining and Information Security,10.1007/978-981-33-4367-2_18,Springer,2021-01-01,2021-05-05,"Islam , S. M. Zahidul Akteruzzaman, xxxx Abujar, Sheikh ASRBangla is a proposed model that can recognize Bangla speech or voice automatically by using the Mel Frequency Cepstral Coefficient (MFCC) and Recurrent Neural Network (RNN). Speech Recognition is the power of a process of an instrument or program that puts word and phrases into words and converts them into a machine-meaningful arrangement. ASR are already implemented in many other language but not in Bengali language properly. In this worlds about 15% population which is almost one billion who are physically disabled. For this physically disabled people we used this technology like blind, handicapped, deaf. This really reduces their dependency. In this research work we pursuits at building a model in deep learning way to deal with Automatic Speech Recognition in Bangla. Our proposed methodology there used MFCC ‘for feature extraction, RNN ’ for training dataset. This proposed model we trained a LSTM model to know the most possible phonemes. Our proposed model get 20% error rate in word detection on Bangla-Word from audio dataset.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-33-4367-2_18,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-88007-1_20,Unsupervised Learning Framework for 3D Reconstruction from Face Sketch,Pattern Recognition and Computer Vision,10.1007/978-3-030-88007-1_20,Springer,2021-01-01,2021-10-22,"Increasingly attention has been paid to 3D understanding and reconstruction recently, while the inputs of most existing models are chromatic photos. 3D shape modelling from the monochromatic input, such as sketch, largely remains under-explored. One of the major challenges is the lack of paired training data, since it is costly to collect such a database with one-to-one mapping instances of two modalities, e.g., a 2D sketch and its corresponding 3D shape. In this work, we attempt to attack the problem of 3D face reconstruction using 2D sketch in an unsupervised setting. In particular, an end-to-end learning framework is proposed. There are two key modules of the network, the 2D translation network and the 3D reconstruction network. The 2D translation network is utilized to translate an input sketch face into a form of realistic chromatic 2D image. Then an unsupervised 3D reconstruction network is proposed to further transform the 2D image obtained in the previous step into a 3D face shape. In addition, because there is no existing sketch-3D face dataset available, two synthetic datasets are constructed based on BFM and CelebA, namely SynBFM and SynCelebA, to facilitate the evaluation. Extensive experiments conducted on these two synthetic datasets validate the effectiveness of our proposed approach.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-88007-1_20,springer
Article,doi:10.1007/s11042-020-09722-8,Single image 3D object reconstruction based on deep learning: A review,Multimedia Tools and Applications,10.1007/s11042-020-09722-8,Springer,2021-01-01,2020-09-03,"The reconstruction of 3D object from a single image is an important task in the field of computer vision. In recent years, 3D reconstruction of single image using deep learning technology has achieved remarkable results. Traditional methods to reconstruct 3D object from a single image require prior knowledge and assumptions, and the reconstruction object is limited to a certain category or it is difficult to accomplish a good reconstruction from a real image. Although deep learning can solve these problems well with its own powerful learning ability, it also faces many problems. In this paper, we first discuss the challenges faced by applying the deep learning method to reconstruct 3D objects from a single image. Second, we comprehensively review encoders, decoders and training details used in 3D reconstruction of a single image. Then, the common datasets and evaluation metrics of single image 3D object reconstruction in recent years are introduced. In order to analyze the advantages and disadvantages of different 3D reconstruction methods, a series of experiments are used for comparison. In addition, we simply give some related application examples involving 3D reconstruction of a single image. Finally, we summarize this paper and discuss the future directions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-09722-8,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-70665-4_15,Speech Enhancement Based on Deep Neural Network and Recurrent Neural Network,"Advances in Natural Computation, Fuzzy Systems and Knowledge Discovery",10.1007/978-3-030-70665-4_15,Springer,2021-01-01,2021-06-27,"Deep Neural Networks (DNNs) have been successfully adopted as a regression model in speech enhancement. However, the performance in the battlefield environment is not always satisfactory because the noise energy is often dominating in certain speech segments causing speech distortion. For the speech enhancement in complicated battlefield environment where multiple noises can simultaneously corrupt speech, such as gunshots and explosions, we propose an enhanced method to improve the existing DNN-based speech enhancement by using Recurrent Neural Networks (RNNs). This RNN model judges whether each frame is in a low SNR state, and then fuses two DNN-based speech enhancement models. The proposed method is compared with existing DNN-based speech enhancement techniques through the perceptual evaluation of speech quality (PESQ) and the short-time objective intelligibility (STOI) scores in various noisy speech conditions. The experimental results demonstrate significant improvements over the state-of-the-art techniques and reflect the usability of the method in a real battlefield environment.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-70665-4_15,springer
Chapter,doi:10.1007/978-981-15-5093-5_16,Psychological Stress Detection by 2D and 3D Facial Image Processing,Progresses in Artificial Intelligence and Neural Systems,10.1007/978-981-15-5093-5_16,Springer,2021-01-01,2020-07-10,"This work aims to identify people psychological stress through the capture of micro modifications and motions within their facial expression. Exogenous and endogenous causes of stress, from environment and/or psychological conditions that could induce stress, have been reproduced in the experimental test involving real subjects, and their face expressions have been recorded by 2D and 3D image capturing tools to create a sample of emotional database. Successively, 2D and 3D analyses have been performed on recorded data according to the respective protocols, by deep learning and machine learning techniques, and a data driven model of the databases has been developed by neural network approach, to classify the psycho-behavioral answers to the different kinds of stress conditions induced on tested people. The ultimate aim of the study is to demonstrate the possibility to analyze data collected on participants from 2D shooting and 3D scans in a consistent way by means of deep learning and machine learning techniques, so that to provide a methodology to identify and classify some of the subtle facial micro-expressions of people involved in stressing activities.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-5093-5_16,springer
Chapter,doi:10.1007/978-3-030-59475-6_8,Setting the Stage for 3D Compositing with Machine Learning,Artificial Intelligence and the Arts,10.1007/978-3-030-59475-6_8,Springer,2021-01-01,2021-10-14,,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-030-59475-6_8,springer
Chapter ReferenceWorkEntry,doi:10.1007/978-3-030-63416-2_300354,Machine Learning for 3D Vision,Computer Vision,10.1007/978-3-030-63416-2_300354,Springer,2021-01-01,2021-10-13,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-63416-2_300354,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-7533-4_48,Style Transfer for Videos with Audio,"Proceedings of the International Conference on Paradigms of Computing, Communication and Data Sciences",10.1007/978-981-15-7533-4_48,Springer,2021-01-01,2021-02-20,"In the art of painting, from early era of beginning of the human civilization, human beings have been creating artistic images with content from the real world but style from their imagination. Consider one such art called The Starry Night by painter van Gogh. Here the mountains, moon and houses are content taken from the real world but the style of painting is totally from the painter’s imagination and is unique. Style Transfer is the problem of taking content of an image and style of other image to create third image having content of the former but style of the latter. Clearly, such work cannot be obtained by simple overlapping the two images. Until recently, due to not so optimized GPUs and slower hardware, image processing was a time consuming computation problem. But now we can use technological optimizations to use Convolutional Neural Networks (CNNs) to do the Style Transfer. In this paper, we discuss how style transfer can be done in videos having audio in them. We shall also compare one of the existing methods with our implementation. Our proposed work has potential applications in the domains of social media communication, entertainment industry and mobile applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-7533-4_48,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-86514-6_22,Automatic Acoustic Mosquito Tagging with Bayesian Neural Networks,Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track,10.1007/978-3-030-86514-6_22,Springer,2021-01-01,2021-09-10,"Deep learning models are now widely used in decision-making applications. These models must be robust to noise and carefully map to the underlying uncertainty in the data. Standard deterministic neural networks are well known to be poor at providing reliable estimates of uncertainty and often lack the robustness that is required for real-world deployment. In this paper, we work with an application that requires accurate uncertainty estimates in addition to good predictive performance. In particular, we consider the task of detecting a mosquito from its acoustic signature. We use Bayesian neural networks (BNNs) to infer predictive distributions over outputs and incorporate this uncertainty as part of an automatic labelling process. We demonstrate the utility of BNNs by performing the first fully automated data collection procedure to identify acoustic mosquito data on over 1,500 h of unlabelled field data collected with low-cost smartphones in Tanzania. We use uncertainty metrics such as predictive entropy and mutual information to help with the labelling process. We show how to bridge the gap between theory and practice by describing our pipeline from data preprocessing to model output visualisation. Additionally, we supply all of our data and code. The successful autonomous detection of mosquitoes allows us to perform analysis which is critical to the project goals of tackling mosquito-borne diseases such as malaria and dengue fever.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-86514-6_22,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-86230-5_47,Deploying a Speech Therapy Game Using a Deep Neural Network Sibilant Consonants Classifier,Progress in Artificial Intelligence,10.1007/978-3-030-86230-5_47,Springer,2021-01-01,2021-09-03,"Speech therapy games present a relevant application of business intelligence to real-world problems. However many such models are only studied in a research environment and lack the discussion on the practical issues related to their deployment. In this article, we depict the main aspects that are critical to the deployment of a real-time sound recognition neural model. We have previously presented a classifier of a serious game for mobile platforms that allows children to practice their isolated sibilants exercises at home to correct sibilant distortions, which was further motivated by the Covid-19 pandemic present at the time this article is posted. Since the current classifier reached an accuracy of over 95%, we conducted a study on the ongoing issues for deploying the game. Such issues include pruning and optimization of the current classifier to ensure near real-time classifications and silence detection to prevent sending silence segment requests to the classifier. To analyze if the classification is done in a tolerable amount of time, several requests were done to the server with pre-defined time intervals and the interval of time between the request and response was recorded. Deploying a program presents new obstacles, from choosing host providers to ensuring everything runs smoothly and on time. This paper proposes a guide to deploying an application containing a neural network classifier to free- and controlled-cost cloud servers to motivate further deployment research.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-86230-5_47,springer
Chapter ConferencePaper,doi:10.1007/978-981-16-1649-5_14,Are You Speaking with a Mask? An Investigation on Attention Based Deep Temporal Convolutional Neural Networks for Mask Detection Task,Proceedings of the 8th Conference on Sound and Music Technology,10.1007/978-981-16-1649-5_14,Springer,2021-01-01,2021-04-25,"When writing this article, COVID-19 as a global epidemic, has affected more than 200 countries and territories globally and lead to more than 694,000 deaths. Wearing a mask is one of most convenient, cheap, and efficient precautions. Moreover, guaranteeing a quality of the speech under the condition of wearing a mask is crucial in real-world telecommunication technologies. To this line, the goal of the ComParE 2020 Mask condition recognition of speakers subchallenge is to recognize the states of speakers with or without facial masks worn. In this work, we present three modeling methods under the deep neural network framework, namely Convolutional Recurrent Neural Network(CRNN), Convolutional Temporal Convolutional Network(CTCNs) and CTCNs combined with utterance level features, respectively. Furthermore, we use cycle mode to fill the samples to further enhance the system performance. In the CTCNs model, we tried different network depths. Finally, the experimental results demonstrate the effectiveness of the CTCNs network structure, which can reach an unweighted average recall (UAR) at 66.4% on the development set. This is higher than the result of baseline, which is 64.4% in S2SAE+SVM nerwork(a significance level at $$p < 0.001$$ p < 0.001 by one-tailed z-test). It demonstrates the good performance of our proposed network.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-1649-5_14,springer
Chapter,doi:10.1007/978-3-030-70009-6_11,AR and VR in Manufacturing,Futuristic Trends in Intelligent Manufacturing,10.1007/978-3-030-70009-6_11,Springer,2021-01-01,2021-06-01,"Digitization is the backbone and major cause in the era of Industry Industry 4.0. Manufacturing Manufacturing industries Industry are in the midst of a transformation regarding the method of manufacturing Manufacturing processing and delivering of goods to customers. Until now the production Production of various goods done by the manufacturing industries Industry uses the traditional method of old machines and human labor but looking into the plan of industry Industry 4.0 it gives tremendous outcomes in terms of economical aspect as well as safety-wise in the long run.. In the first revolution, the manufacturing sector was mainly based on using mechanical human-operated machines which needed a lot of labors to complete the work, Second revolution is the use of mass production Production and assembly lines with the help of electricity. The third revolution was mainly based on advancements made in machines by introducing computers and reducing the burden on human shoulders. But now the beginning of industry Industry 4.0 has started and many manufacturing Manufacturing sectors are getting benefited from this. Industry Industry 4.0 is the automation Automation simulation smart Smart system based environment where Augmented Augmented reality(AR) and Virtual reality(VR) comes into play. Industry Industry 4.0 has started to change the complete industrial experience and with the introduction of AR and VR in manufacturing Manufacturing sectors the profit-economy growth graph is starting to rise hence establishing a strong foundation and helping to deliver good sufficient advanced products that customer desires with the help of purely Automation Automation and Artificial intelligence Artificial intelligence . Industry Industry 4.0 is based on data the way it can be gathered, analyzed and deployed in the manufacturing Manufacturing sector with help of Augmented Augmented and Virtual reality.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-70009-6_11,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-68780-9_34,Increased-Confidence Adversarial Examples for Deep Learning Counter-Forensics,Pattern Recognition. ICPR International Workshops and Challenges,10.1007/978-3-030-68780-9_34,Springer,2021-01-01,2021-02-25,"Transferability of adversarial examples is a key issue to apply this kind of attacks against multimedia forensics (MMF) techniques based on Deep Learning (DL) in a real-life setting. Adversarial example transferability, in fact, would open the way to the deployment of successful counter forensics attacks also in cases where the attacker does not have a full knowledge of the to-be-attacked system. Some preliminary works have shown that adversarial examples against CNN-based image forensics detectors are in general non-transferrable, at least when the basic versions of the attacks implemented in the most popular libraries are adopted. In this paper, we introduce a general strategy to increase the strength of the attacks and evaluate their transferability when such a strength varies. We experimentally show that, in this way, attack transferability can be largely increased, at the expense of a larger distortion. Our research confirms the security threats posed by the existence of adversarial examples even in multimedia forensics scenarios, thus calling for new defense strategies to improve the security of DL-based MMF techniques.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-68780-9_34,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-87202-1_64,SurgeonAssist-Net: Towards Context-Aware Head-Mounted Display-Based Augmented Reality for Surgical Guidance,Medical Image Computing and Computer Assisted Intervention – MICCAI 2021,10.1007/978-3-030-87202-1_64,Springer,2021-01-01,2021-09-21,"We present SurgeonAssist-Net: a lightweight framework making action-and-workflow-driven virtual assistance, for a set of predefined surgical tasks, accessible to commercially available optical see-through head-mounted displays (OST-HMDs). On a widely used benchmark dataset for laparoscopic surgical workflow, our implementation competes with state-of-the-art approaches in prediction accuracy for automated task recognition, and yet requires $$7.4\times $$ 7.4 × fewer parameters, $$10.2\times $$ 10.2 × fewer floating point operations per second (FLOPS), is $$7.0\times $$ 7.0 × faster for inference on a CPU, and is capable of near real-time performance on the Microsoft HoloLens 2 OST-HMD. To achieve this, we make use of an efficient convolutional neural network (CNN) backbone to extract discriminative features from image data, and a low-parameter recurrent neural network (RNN) architecture to learn long-term temporal dependencies. To demonstrate the feasibility of our approach for inference on the HoloLens 2 we created a sample dataset that included video of several surgical tasks recorded from a user-centric point-of-view. After training, we deployed our model and cataloged its performance in an online simulated surgical scenario for the prediction of the current surgical task. The utility of our approach is explored in the discussion of several relevant clinical use-cases. Our code is publicly available at https://github.com/doughtmw/surgeon-assist-net .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-87202-1_64,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-4488-0_56,Speech Recognition Using Neural Network for Mobile Robot Navigation,Trends in Mechanical and Biomedical Design,10.1007/978-981-15-4488-0_56,Springer,2021-01-01,2020-08-21,"Automatic speech recognition (ASR) has gained a lot of popularity in the mobile robotics, where the commands could be provided to the robot wirelessly to maneuver. A navigation system combined with ASR is a complex system to carry out, because the system has difficulty in recognizing the voice commands when the environment involved already has disturbances like road noise, air conditioner, music, and passengers. The objective of this research is to operate a mobile robot with a single-arm manipulator, where the robot can perceive the speech and it can react to the individual speech commands provided by the operator swiftly and precisely. In order to recognize the speech, mel-frequency cepstral coefficient (MFCC) speech recognition algorithm is chosen and implemented in MATLAB. Various training and testing have been done in MFCC algorithm where it has to carry out the real-time processing of speech data and respond to it. Based on both the training and testing the voice commands collected from the five test subjects both male and female, the speech recognition system achieved 89% efficiency for the test database.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-4488-0_56,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-55506-1_28,Abnormal Interference Recognition Based on Rolling Prediction Average Algorithm,Advances in Computer Science for Engineering and Education III,10.1007/978-3-030-55506-1_28,Springer,2021-01-01,2020-08-06,"Aiming at the problems that the traditional camera abnormal interference recognition methods have a single identification type, the low recognition accuracy and reliability, the generalization ability is not strong due to the prediction of flicker. A method for camera abnormal interference recognition based on the rolling prediction average algorithm is proposed. The core ideas of this method include the following steps. Firstly, the ImageNet pre-trained ResNet50 network on the self-built abnormal interference image training set is fine-tuned. Then a model for camera abnormal interference image classification and recognition is trained. Finally, the rolling prediction average algorithm is applied to the model to realize the classification and recognition of the camera abnormal interference video online or offline. The experimental results on the self-built abnormal interference image verification set show that the proposed method can correctly identify the normal images, occlusion images, blurred images and the camera rotated images. The recognition accuracy reaches 97%. Test accuracy reaches 95% when using test set videos test, which fully verifies the feasibility and effectiveness of the proposed method. Compared with other abnormal interference recognition methods, not only the accuracy rate is high, but also the false recognition rate and missed detection rate are relatively low. The proposed method can be applied to actual monitoring scenarios to assist the work of monitoring center staff.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-55506-1_28,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-4624-2_48,Linking Seakeeping Performance Predictions with Onboard Measurements for Surface Platform Digital Twins,Practical Design of Ships and Other Floating Structures,10.1007/978-981-15-4624-2_48,Springer,2021-01-01,2020-10-06,"Despite the hype surrounding digital twin technology and its implementation in other fields, the marine industry has published very few successful, full-scale applications of this technology to date. The development of useful digital twin technology within the industry requires fundamental exploration of data fusion techniques in the marine context. Future development of a useful surface platform digital twin that incorporates advanced machine learning techniques requires preliminary experimentation with more basic, well-understood learning models using full-scale, real-world data. The goal of this work was to lay a foundation for addressing the current knowledge gap through fusion of seakeeping predictions with full-scale measured motions using linear least-squares and neural network corrections. Motion data were obtained from an oceanographic research cruise and these correction methods were applied to frequency-domain response predictions. The success of these relatively simplistic correction models provided insight for the next steps of surface platform data fusion algorithm development.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-4624-2_48,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-67667-4_14,An Advert Creation System for 3D Product Placements,Machine Learning and Knowledge Discovery in Databases: Applied Data Science Track,10.1007/978-3-030-67667-4_14,Springer,2021-01-01,2021-02-25,"Over the past decade, the evolution of video-sharing platforms has attracted a significant amount of investments on contextual advertising. The common contextual advertising platforms utilize the information provided by users to integrate 2D visual ads into videos. The existing platforms face many technical challenges such as ad integration with respect to occluding objects and 3D ad placement. This paper presents a Video Advertisement Placement & Integration (Adverts) framework, which is capable of perceiving the 3D geometry of the scene and camera motion to blend 3D virtual objects in videos and create the illusion of reality. The proposed framework contains several modules such as monocular depth estimation, object segmentation, background-foreground separation, alpha matting and camera tracking. Our experiments conducted using Adverts framework indicates the significant potential of this system in contextual ad integration, and pushing the limits of advertising industry using mixed reality technologies.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-67667-4_14,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-89820-5_4,Improving a Conversational Speech Recognition System Using Phonetic and Neural Transcript Correction,Advances in Soft Computing,10.1007/978-3-030-89820-5_4,Springer,2021-01-01,2021-10-21,"This article describes the successful implementation of a conversational speech recognition system applied to telephonic sales performed by an autonomous agent. Our implementation uses a post-processing corrector based on phonetic representations of text and subsequent neural network classifier. The classifier assesses the proposed correction’s relevance to reduce the errors in the transcript sent to a downstream Natural Language Understanding engine. The experiments were carried on correcting transcripts from real audios of orders placed by customers of a large bottling company. We measured the Word Error Rate of the corrected transcripts against human-annotated ground-truth to verify the improvement produced by the system. To evaluate the corrections’ impact on the entities detected by the Natural Language Understanding engine, we used Jaccard distance, Precision, Recall, and $$F_1$$ F 1 . Results show that the implemented system and architecture enhance the transcript relative Word Error Rate on a 39% and Jaccard distance on 13% in comparison to the Automatic Speech Recognition baseline, making them suitable for real-time telephonic sales systems implementation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-89820-5_4,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-53021-1_42,Audio Scene Classification Based on Convolutional Neural Networks: An Evaluation of Multiple Features and Topologies in Short Time Segments,AETA 2019 - Recent Advances in Electrical Engineering and Related Sciences: Theory and Application,10.1007/978-3-030-53021-1_42,Springer,2021-01-01,2020-08-11,"The classification of acoustic scenes has been gaining importance in recent years, either because of the applications that it may have or because of the challenge of implementing a computational tool that allows the proper detection of complex and diverse sounds, such as those presented in real environments. In this work a convolutional neuronal network is implemented, trained with features such as mel-frequency cepstral coefficients, gamma tone y discrete Fourier transform, extracted to the sounds in segments of 10 s and 1 s. Cross validation is used with 80% of the data for training and 20% for validation with the DCASE2018 database evaluating the performance for different topologies of the neural network.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-53021-1_42,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-5788-0_70,Fog-Based Video Surveillance System for Smart City Applications,Evolution in Computational Intelligence,10.1007/978-981-15-5788-0_70,Springer,2021-01-01,2020-09-09,"Natesha, B. V. Guddeti, Ram Mohana Reddy With the rapid growth in the use of IoT devices in monitoring and surveillance environment, the amount of data generated by these devices is increased exponentially. There is a need for efficient computing architecture to push the intelligence and data processing close to the data source nodes. Fog computing will help us to process and analyze the video at the edge of the network and thus reduces the service latency and network congestion. In this paper, we develop fog computing infrastructure which uses the deep learning models to process the video feed generated by the surveillance cameras. The preliminary experimental results show that using different deep learning models (DNN and SNN) at the different levels of fog infrastructure helps to process the video and classify the vehicle in real time and thus service the delay-sensitive applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-5788-0_70,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-58669-0_6,Video Captioning Using Attention Based Visual Fusion with Bi-temporal Context and Bi-modal Semantic Feature Learning,Proceedings of the International Conference on Advanced Intelligent Systems and Informatics 2020,10.1007/978-3-030-58669-0_6,Springer,2021-01-01,2020-09-20,"Video captioning is a recent emerging task that describes a video through generating a natural language sentence. Practically videos are untrimmed where both localizing and describing the event of interest is crucial for many vision based- real life applications. This paper proposes a deep neural network framework for effective video event localization through using a bidirectional Long Short Term Memory (LSTM) that encodes past, current and future context information. Our framework adopts an encoder decoder network that accepts the event proposal with highest temporal intersection with ground truth for captioning. Our encoder is fed with attentively fused visual features, extracted by a two stream 3D convolution neural network, along with the proposal’s context information for generating an effective representation. Our decoder accepts learnt semantic features that represent bi-modal (two modes) high-level semantic concepts. We conduct experiments to demonstrate that utilizing both semantic features and contextual information provides better captioning performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-58669-0_6,springer
Chapter ReferenceWorkEntry,doi:10.1007/978-3-030-63416-2_848,Deep Learning Based 3D Vision,Computer Vision,10.1007/978-3-030-63416-2_848,Springer,2021-01-01,2021-10-13,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-63416-2_848,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-83723-5_10,Engineering a Digital Twin for Manual Assembling,"Leveraging Applications of Formal Methods, Verification and Validation: Tools and Trends",10.1007/978-3-030-83723-5_10,Springer,2021-01-01,2021-08-05,"The paper synthesizes our preliminary work on developing a digital twin, with learning capabilities, for a system that includes cyber, physical, and social components. The system is an industrial workstation for manual assembly tasks that uses several machine learning models implemented as microservices in a hybrid architecture, a combination between the orchestrated and the event stream approaches. These models have either similar objectives but context-dependent performance, or matching functionalities when the results are fused to support real-life decisions. Some of the models are descriptive but easy to transform in inductive models with extra tuning effort, while others are purely inductive, requiring intrinsic connection with the real world.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-83723-5_10,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-86383-8_49,Learning Traffic as Videos: A Spatio-Temporal VAE Approach for Traffic Data Imputation,Artificial Neural Networks and Machine Learning – ICANN 2021,10.1007/978-3-030-86383-8_49,Springer,2021-01-01,2021-09-07,"In the real world, data missing is inevitable in traffic data collection due to detector failures or signal interference. However, missing traffic data imputation is non-trivial since traffic data usually contains both temporal and spatial characteristics with inherent complex relations. In each time interval, the traffic measurements collected in all spatial regions can be regarded as an image with more or fewer channels. Therefore, the traffic raster data over time can be learned as videos. In this paper, we propose a novel unsupervised generative neural network for traffic raster data imputation called STVAE , which works well robustly even under different missing rates. The core idea of our model is to discover more complex spatio-temporal representations inside the traffic data under the architecture of variational autoencoder (VAE) with Sylvester normalizing flows (SNFs). After transforming the traffic raster data into multi-channel videos, a Detection-and-Calibration Block (DCB), which extends 3D gated convolution and multi-attention mechanism, is proposed to sense, extract and calibrate more flexible and accurate spatio-temporal dependencies of the original data. The experiments are employed on three real-world traffic flow datasets and demonstrate that our network STVAE achieves the lowest imputation errors and outperforms state-of-the-art traffic data imputation models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-86383-8_49,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-86517-7_29,Multi-task Learning for User Engagement and Adoption in Live Video Streaming Events,Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track,10.1007/978-3-030-86517-7_29,Springer,2021-01-01,2021-09-10,"Nowadays, live video streaming events have become a mainstay in viewer’s communication in large international enterprises. Provided that viewers are distributed worldwide, the main challenge resides on how to schedule the optimal event’s time so as to improve both the viewer’s engagement and adoption. In this paper we present a multi-task deep reinforcement learning model to select the time of a live video streaming event, aiming to optimize the viewer’s engagement and adoption at the same time. We consider the engagement and adoption of the viewers as independent tasks and formulate a unified loss function to learn a common policy. In addition, we account for the fact that each task might have different contribution to the training strategy of the agent. Therefore, to determine the contribution of each task to the agent’s training, we design a Transformer’s architecture for the state-action transitions of each task. We evaluate our proposed model on four real-world datasets, generated by the live video streaming events of four large enterprises spanning from January 2019 until March 2021. Our experiments demonstrate the effectiveness of the proposed model when compared with several state-of-the-art strategies. For reproduction purposes, our evaluation datasets and implementation are publicly available at https://github.com/stefanosantaris/merlin .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-86517-7_29,springer
Chapter,doi:10.1007/978-3-030-58197-8_7,6G Wireless Systems: Challenges and Opportunities,5G and Beyond,10.1007/978-3-030-58197-8_7,Springer,2021-01-01,2020-08-12,"The ongoing deployment of 5G cellular systems is continuously exposing the inherent limitations of this system, compared to its original premise as an enabler for Internet of Everything applications. These 5G drawbacks are spurring worldwide activities focused on defining the next-generation 6G wireless system that can truly integrate far-reaching applications ranging from autonomous systems to extended reality. To date, the fundamental architectural and performance components of 6G remain largely undefined and open to speculations. In this chapter, we present a holistic vision that identifies the main principles that can guide the design and development of a 6G system. In particular, we discuss, in detail, why 6G will not be a simple exploration of more spectrum at high-frequency bands such as terahertz frequencies, but it will rather be a convergence of a number of technological trends. 6G will also be largely driven by a new breed of exciting Internet of Everything services. To this end, we first outline the primary drivers of 6G systems, in terms of applications and accompanying technological trends. Then, we introduce a new set of service classes and expose their target 6G performance requirements. We then explore the enabling technologies for the introduced 6G services and define a comprehensive set of research problems that come hand in hand with the identified technologies. We conclude by providing our observations on the challenges and opportunities that will define the road toward 6G. Ultimately, this chapter can be used to stimulate comprehensive, out-of-the-box research around 6G technologies.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-58197-8_7,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-87034-8_42,User Experience Analysis Based on a Virtual Mark-up Approach,Creativity in Intelligent Technologies and Data Science,10.1007/978-3-030-87034-8_42,Springer,2021-01-01,2021-09-13,"The paper presents some results of interactive user interfaces implementation in practice. Virtual mark-up approach is used to classify the professional status of the users and respectively adapt the user interface. New software components are introduced as a part of Augmented Reality system that capture the user’s behavior, compare it to the typical patterns and generate virtual elements when necessary. The resulting solution is capable of providing alerts and notifications for novice users and hiding the redundant information for experts and thus personalizing the user interface. Artificial neural network provides classification based on the results of user performance in script execution according to pre-defined scenarios. The proposed approach is illustrated by an example of electrical meters surveying mobile application. Research results illustrate the possibility to improve and personalize the augmented reality user interfaces based on the analysis of user activity.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-87034-8_42,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-88007-1_9,Slice Sequential Network: A Lightweight Unsupervised Point Cloud Completion Network,Pattern Recognition and Computer Vision,10.1007/978-3-030-88007-1_9,Springer,2021-01-01,2021-10-22,"The point cloud is usually sparse and incomplete in reality, and the missing region of the point cloud is coherent when it is blocked by other objects. To tackle this problem, we propose a novel light-weight unsupervised model, namely the Slice Sequential Network, for point cloud completion. Our method only generates the missing parts with high fidelity, while many previous methods output the entire point cloud and leave out some important details. Specifically, we slice the incomplete point cloud and force the model to exploit the information lying between slices. In addition, we design a new algorithm for extracting geometric information, which can extract multi-scale features of points of the point cloud to enhance the use of slices. The qualitative and quantitative experiments show that our method is more lightweight and has better performance than the existing state-of-the-art methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-88007-1_9,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-89394-1_9,A Symbolic Machine Learning Approach for Cybersickness Potential-Cause Estimation,Entertainment Computing – ICEC 2021,10.1007/978-3-030-89394-1_9,Springer,2021-01-01,2021-10-22,"Virtual reality (VR) and head-mounted displays are constantly gaining popularity in various fields such as education, military, entertainment, and bio/medical informatics. Although such technologies provide a high sense of immersion, they can also trigger symptoms of discomfort. This condition is called cybersickness (CS) and is quite popular in recent publications in the virtual reality context. This work proposes a novel experimental analysis using symbolic machine learning that ranks potential causes for CS. We estimate the CS causes and rank them according to their impact on the classification capabilities of CS. The experiments are performed using two distinct virtual reality games. We were able to identify that acceleration triggered cybersickness more frequently in a race game in contrast to a flight game. Furthermore, participants less experienced with VR are more prone to feel discomfort and this variable has a greater impact in the race game in contrast to the flight game, where the acceleration is not controlled by the user.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-89394-1_9,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-69373-2_1,Cloud Networked Models of Knowledge-Based Intelligent Control Towards Manufacturing as a Service,"Service Oriented, Holonic and Multi-Agent Manufacturing Systems for Industry of the Future",10.1007/978-3-030-69373-2_1,Springer,2021-01-01,2021-03-03,"This paper describes a 10-year scientific journey in the area of Cloud-based manufacturing in the SOHOMA research community. The tour started in Paris on June 20, 2011 at École Nationale Supérieure d’Arts et Métiers, Paris and returns here on 1^st October 2020 after annual stops in Bucharest, Valenciennes, Nancy, Cambridge, Lisbon, Nantes, Bergamo and Valencia. Several stages in the evolution of Cloud manufacturing research are recalled in their historical order: vertical enterprise integration and networking; resource and product virtualization and cloud infrastructure design; batch optimization with cloud services; real time big shop floor data streaming, machine learning in the cloud for predictive production control, resource health monitoring and predictive maintenance. Major contributions of SOHOMA authors are evoked: extending the cloud computing model to on demand shop floor resource sharing, infrastructure sharing in cloud networked enterprises, MES workload virtualization, deploying cloud services in real time with virtual machine and containers, high availability solutions and software defined networking, machine learning for predictive manufacturing.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-69373-2_1,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-60577-3_31,Operational Visual Control of the Presence of Students in Training Sessions,"Advances in Neural Computation, Machine Learning, and Cognitive Research IV",10.1007/978-3-030-60577-3_31,Springer,2021-01-01,2020-10-02,"The problem of automating the students’ attendance in the classroom is solved by using computer vision. A convolutional neural network is used to recognize a person’s face. The recognition process is implemented in real time. Localization of faces in frames from a video camera is performed by the Viola-Jones method. The convolutional neural network of the VGGFace model forms the features of a person’s face. Identification of the person occurs by the facial features similarity. The software is implemented by using the Keras and OpenCV libraries. The control system performs the following functions: captures the faces of students on a video camera when entering the classroom, compares faces with a database of students, notes the presence at the lesson (or being late) in case of successful identification, saves the data in attendance register. For the convenience of video monitoring, the color of the student’s line changes depending on his condition: not present, present, late, absent. The system provides for manual editing of the electronic register and the choice of the subject name. The student’s photo can be uploaded into the database from a file or directly from the camera.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-60577-3_31,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-8458-9_40,3D Path Planning for UAV with Improved Double Deep Q-Network,Proceedings of 2020 Chinese Intelligent Systems Conference,10.1007/978-981-15-8458-9_40,Springer,2021-01-01,2020-09-30,"Unmanned aerial vehicle (UAV) has been widely used in civil and military fields due to its advantages such as zero casualties, low cost and strong maneuverability. Path planning in 3D obstacle environment is one of the fundamental capabilities of UAV for mission performing. In this paper, we propose a 3D path planning algorithm to learn a target-driven end-to-end model based on an improved double deep Q-network (DQN), where a greedy exploration strategy is applied to accelerate learning. The model takes target and obstacle message as input, and moving command of UAV as output. It can realize path planning successfully for UAV in 3D complex environment. Besides, the experimental results show that improved double DQN has better convergence speed compared with DQN and double DQN.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-8458-9_40,springer
Chapter ConferencePaper,doi:10.1007/978-981-16-7476-1_23,A Transfer Learning Method Based on ResNet Model,Data Mining and Big Data,10.1007/978-981-16-7476-1_23,Springer,2021-01-01,2021-10-31,"As countries around the world improve their garbage recycling and processing policies, the intelligently and efficiently garbage classification and identification has become a key point for implementing policies. However, traditional image recognition methods still have disadvantages, for instance, it needs a large amount of data annotation and a long time is required to train the model. In response to these drawbacks, this paper proposes a transfer learning method based on ResNet model, which aims to solve the problem of efficient classification of small-scale garbage image data sets. For the small sample image data set, after the data augmentation, the pre-training model ResNet50 is migrated to the data set through two migration learning methods of fine-tuning and pre-training model as the feature extractor, so as to realize the training of the target model. The experimental results show that the model classification effect after fine-tuning method and hyperparameter adjustment is better than the model without transfer learning, which can effectively improve the training speed and accuracy, and reduce the impact of over-fitting.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7476-1_23,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-87571-8_41,Application of Neural Network in Oracle Bone Inscriptions,Web Information Systems and Applications,10.1007/978-3-030-87571-8_41,Springer,2021-01-01,2021-09-17,"The recognition of Oracle Bone Inscriptions(OBIs) is of great significance to archaeology, history, and linguistics. To realize the fast and accurate retrieval of images for large-scale OBIs datasets and break through the limitations of current conventional retrieval methods, this paper proposes a convolutional neural network for OBIs recognition. The model is designed according to the characteristics of OBIs. The experimental results show that the improved network can better extract the features of OBIs characters, and the recognition rate reaches 84.45%, which is 13.74% higher than the network before the improvement.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-87571-8_41,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-71711-7_12,Evaluating Predictive Deep Learning Models,Intelligent Technologies and Applications,10.1007/978-3-030-71711-7_12,Springer,2021-01-01,2021-03-15,"Predicting the future using deep learning models is a research field of increasing interest. However, there is a lack of established evaluation methods for assessing their predictive abilities. Images and videos are targeted towards human observers, and since humans have individual perceptions of the world, evaluation of videos should take subjectivity into account. In this paper, we present a framework for evaluating predictive models using subjective data. The methodology is based on a mixed methods research design, and is applied in an experiment to measure the realism and accuracy of predictions of a visual traffic environment. Our method is shown to be uncorrelated with the predominant approach for evaluating predictive models, which is a frame-wise comparison between predictions and ground truth. These findings emphasise the importance of using subjective data in the assessment of predictive abilities of models and open up a new direction for evaluating predictive deep learning models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-71711-7_12,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-71906-7_6,Cloud-Edge Microservice Architecture for DNN-based Distributed Multimedia Event Processing,Advances in Service-Oriented and Cloud Computing,10.1007/978-3-030-71906-7_6,Springer,2021-01-01,2021-03-14,"The rise of Big Data, Internet of Multimedia Things (IoMT), and Deep Neural Network (DNN) enabled the growth of DNN-based Computer Vision solutions to Multimedia Event Processing (MEP) applications. When these are applied to a real-world scenario we notice the importance of having a system with a satisfactory speed that can fit in the limited resources of most IoMT devices. However, most solutions for distributed MEP are dependent on a Cloud architecture, which makes these applications migration to the Edge more challenging. As a response to this, we present a microservice architecture for DNN-based distributed MEP over heterogeneous Cloud-Edge environments. We describe our solution that allows for an easier deployment both on the Edge and on the Cloud. We show that choosing the proper tools for an Edge-Friendly solution can lead to 100 times less resource utilisation. Our preliminary investigation shows promising results, with a reduction in energy consumption by 8% with a minor drawback of 15% in throughput in the Edge and a negligible increase in energy consumption on the Cloud.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-71906-7_6,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-79757-7_18,A Sequenced Edge Grid Image Technique for Sign Language Recognition,Recent Advances in Information and Communication Technology 2021,10.1007/978-3-030-79757-7_18,Springer,2021-01-01,2021-06-25,"Currently there are many techniques and methods continuously proposed by researchers for Sign language recognition systems based-on machine learning. For data preprocessing for sign language, majority of researchers use single image of hands like static gesture images. Using only static hand images may not be efficient for real-world applications. In this paper, we propose an innovative technique for video processing called Sequenced Edge Grid Images (SEGI) for Sign Language recognition. The proposed SEGI is composed of images that represent the movement of hands within a single image, which can be applied to recognize a word or a sentence. To proof the concept, we have done several experiments with Thai sign language data collected from internet. SEGI was with existing techniques. Data are the Thai sign language learning video clips that are vocabularies to use in daily life. The proposed technique was implemented with convolutional neural network (CNN). For normal CNN, the experiments show that the SEGI with CNN increases of test accuracy rate with approximately 27% when compared to static gesture images.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-79757-7_18,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-5113-0_31,FastV2C-HandNet: Fast Voxel to Coordinate Hand Pose Estimation with 3D Convolutional Neural Networks,International Conference on Innovative Computing and Communications,10.1007/978-981-15-5113-0_31,Springer,2021-01-01,2020-08-02,"Lekhwani, Rohan Singh, Bhupendra Hand pose estimation from monocular depth images has been an important and challenging problem in the Computer Vision community. In this paper, we present a novel approach to estimate 3D hand joint locations from 2D depth images. Unlike most of the previous methods, our model using a voxel to coordinate(V2C) approach captures the 3D spatial information from a depth image using 3D CNNs thereby giving it a greater understanding of the input. We voxelize the input depth map to capture the 3D features of the input and perform 3D data augmentations to make our network robust to real-world scenarios. Our network is trained in an end-to-end manner which mitigates time and space complexity significantly when compared to other methods. Through comprehensive experiments, it is shown that our model outperforms state-of-the-art methods with respect to the time it takes to train and predict 3D hand joint locations. This makes our method more suitable for real-world hand pose estimation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-5113-0_31,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-80568-5_37,Using Artificial Neural Network to Provide Realistic Lifting Capacity in the Mobile Crane Simulation,Proceedings of the 22nd Engineering Applications of Neural Networks Conference,10.1007/978-3-030-80568-5_37,Springer,2021-01-01,2021-07-01,"Simulations are often used for training novice operators to avoid accidents, while they are still polishing their skills. To ensure the experience gained in the simulation be applicable in real-world scenarios, the simulation has to be made as realistic as possible. This paper investigated how to make the lifting capacity of a virtual mobile crane behave similarly like its real counterpart. We initially planned to use information from the load charts, which document how the lifting capacity of a mobile crane works, but the data in the load charts were very limited. To mitigate this issue, we trained an artificial neural network (ANN) using 90% of random data from two official load charts of a real mobile crane. The trained model could predict the lifting capacity based on the real-time states of the boom length, the load radius, and the counterweight of the virtual mobile crane. To evaluate the accuracy of the ANN predictions, we conducted a real-time experiment inside the simulation, where we compared the lifting capacity predicted by the ANN and the remaining 10% of the data from the load charts. The results showed that the ANN could predict the lifting capacity with small deviation rates. The deviation rates also had no significant impact on the lifting capacity, except when both boom length and load radius were approaching their maximum states. Therefore, the predicted lifting capacity generated by the ANN could be assumed to be close enough to the values in the load charts.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-80568-5_37,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-87986-0_41,Intelligent Virtual Environments with Assessment of User Experiences,Artificial Intelligence and Soft Computing,10.1007/978-3-030-87986-0_41,Springer,2021-01-01,2021-10-05,"Virtual reality (VR) is a powerful modern medium. The advent of low-cost head-mounted display (HMD) devices made this technology accessible at large and featured VR with possibilities to monitor interactions and user’s motion. However, due to lack of real-time feedback mechanism at present, the level of intelligence for virtual environments is still not sufficient to assist the experience and make group or individual assessments towards VR based applications. In this paper, we present our findings related to the problem of real-time feedback that focus on behavioral data by employing the novel feedback mechanism. Virtual-world coordinates, motions and interactions are tracked and captured in real-time while the user experiences particular application. Captured data is investigated to target the issue of complementing VR applications with features derived from real-time behavioral analysis. In our experiment, we also use collected data and provide a methodology to predict virtual-location by the nonlinear auto-regressive neural network with exogenous inputs (NARX). Results suggest employed neural network model is suitable for performing prediction which can be used to obtain a virtual environment with adaptive intelligence.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-87986-0_41,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-80285-1_50,Advanced Cyber and Physical Situation Awareness in Urban Smart Spaces,Advances in Neuroergonomics and Cognitive Engineering,10.1007/978-3-030-80285-1_50,Springer,2021-01-01,2021-07-04,"The ever-growing adoption of big data technologies, smart sensing, data science and artificial intelligence is enabling the development of new intelligent urban spaces with real-time monitoring and advanced cyber-physical situational awareness capabilities. In the S4AllCities international research project, the advancement of cyber-physical situational awareness will be experimented for achieving safer smart city spaces in Europe and beyond. The deployment of digital twins will lead to understanding real-time situation awareness and risks of potential physical and/or cyber-attacks on urban critical infrastructure specifically. The critical extraction of knowledge using digital twins, which ingest, process and fuse observation data and information, prior to machine reasoning is performed in S4AllCities. In this paper, a cyber behavior detection module, which identifies unusualness in cyber traffic networks is described. Also, a physical behaviour detection module is introduced. The two modules function within the so-called Malicious Attacks Information Detection System (MAIDS) digital twin.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-80285-1_50,springer
Chapter ConferencePaper,doi:10.1007/978-981-16-1244-2_11,Feedback-Based Real—Time Surveillance for MidDay Meal Scheme,International Virtual Conference on Industry 4.0,10.1007/978-981-16-1244-2_11,Springer,2021-01-01,2021-08-04,"There is a growing demand of a proper supervision of the midday meal scheme that is enforced in India. Last decade statistics and news articles tend to suggest that there are many loopholes in the current system that needs immediate attention to help effective enforcement of the scheme at the grassroots level. Our proposed system provides a holistic approach towards supervision of the scheme. This system, when implemented in compliance with the Government regulations, can help the concerned authorities in monitoring the execution of the midday meal scheme eliminating the scope of error. Our proposed system is capable of performing real-time food detection, counting number of students through state-of-the-art surveillance cameras with machine learning capabilities. The application is trained on Indian food that can be deployed in Indian School. In case of discrepancy, the user can report discrepancy with a thorough follow-up mechanism that enables authorities to take quick action against the reported complaint. The proposed system enhances the current monitoring process. The system is robust to the changes in the menu. This helps in monitoring process supervised by the government authority. This standalone application can help the governing authorities to have an all-round inclusive approach and magnify the supervision process under a single platform.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-1244-2_11,springer
Chapter ConferencePaper,doi:10.1007/978-981-16-0275-7_41,Design and Implementation of a Small Hand Held Camera Detector on FPGA,"Proceeding of Fifth International Conference on Microelectronics, Computing and Communication Systems",10.1007/978-981-16-0275-7_41,Springer,2021-01-01,2021-09-10,"Nowadays, abuse of camera is high due to the rapid development technology. In a video surveillance system, wireless camera is commonly used. Even though, privacy issues related to illegal videos have recently increased. Due to accuracy limitation, conventional technique is not recommended for practical real-time implementations. In this paper, we propose A Small Hand Held Hidden Camera detector to detect the Hidden cameras in public places like trial rooms, hotels, and restrooms. This device uses a camera to scan the areas to capture the lights emitted by the components of the hidden camera that maybe the LED indicator or maybe the IR light emitted by the camera lenses. In this project, an adaptive thresholding method-based segmentation algorithm will segment the abnormal portion of the image and send it to the monitor via a Video Graphics Array (VGA) port. A hidden camera detector prototype is implemented using a Field Programmable Gate Array (FPGA) and Raspberry Pi Board. Around 98% of classification accuracy is obtained using proposed method which is high when compared to the existing techniques.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-0275-7_41,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-8599-9_49,Recognition of Grape Species with Small Samples Based on Attention Mechanism,Artificial Intelligence in China,10.1007/978-981-15-8599-9_49,Springer,2021-01-01,2021-02-09,"Take Turpan as an example, in recent years, the grape industry in Turpan has become one of the most important pillar industries for rural economic development and farmers’ income increase in Turpan. However, there are still many problems in the development of grape industry in Turpan area. Due to many and complicated grape varieties, it is difficult to identify them, and current identification efficiency is far from enough. Moreover, there is no large-scale open data set in grape recognition at present, and each image itself taken for the grape has much noise effect, which leads to low recognition accuracy. In this paper, a small sample of grape variety recognition method based on attention mechanism is used to fine-tune the CNN model and process the dataset image differently, and is compared with the traditional method. The experiment results show that the method can recognize different grape image types accurately with an accuracy of 93.72% under the condition of small sample. By the method, we can not only improve the efficiency of intelligent recognition, but also reduce the manpower cost, and thus realize the intelligent recognition of grape types.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-8599-9_49,springer
Article,doi:10.1007/s00034-020-01468-w,"
              
                
              
              $$hf_0$$
              
                
                  h
                  
                    f
                    0
                  
                
              
            : A Hybrid Pitch Extraction Method for Multimodal Voice","Circuits, Systems, and Signal Processing",10.1007/s00034-020-01468-w,Springer,2021-01-01,2020-06-15,"Pitch or fundamental frequency ( $$f_0$$ f 0 ) estimation is a fundamental problem extensively studied for its potential speech and clinical applications. The existing $$f_0$$ f 0 estimation methods degrade in performance when applied over real-time audio signals with varying $$f_0$$ f 0 modulations and high SNR environment. In this work, a $$f_0$$ f 0 estimation method using both signal processing and deep learning approaches is developed. Specifically, we train a convolutional neural network to map the periodicity-rich input representation to pitch classes, such that the number of pitch classes is drastically reduced compared to existing deep learning approaches. Then, the accurate $$f_0$$ f 0 is estimated from the nominal pitch classes based on signal processing approaches. The observations from the experimental results showed that the proposed method generalizes to unseen modulations of speech and noisy signals (with various types of noise) for large-scale datasets. Also, the proposed hybrid model significantly reduces the learning parameters required to train the model compared to other methods. Furthermore, the evaluation measures showed that the proposed method performs significantly better than the state-of-the-art signal processing and deep learning approaches.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00034-020-01468-w,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-64594-6_9,Concrete Crack Detection from Video Footage for Structural Health Monitoring,European Workshop on Structural Health Monitoring,10.1007/978-3-030-64594-6_9,Springer,2021-01-01,2021-01-11,"Non-destructive imaging is largely encouraged as a preliminary investigation for damage identification on concrete structural surfaces. Cracks are basic signatures for any structure to initiate the damage. As the whole world is currently connected with lot of cameras all around for various purposes either it be for traffic studies, accident analysis, thefts, natural or human disasters. Alternatively, the same video frames obtained from cameras located in or on the structure can be analysed even for the structural health monitoring. This study aims at identifying the cracks from images mined out of the video frames apart from the crack propagation and length of the crack. Convolution Neural Network is used to train over the images from the video captured during the laboratory compressive strength experiment on a concrete cube to examine and estimate the crack properties. This methodology can be extended to the real-life scenario to alert the damages caused in the structures.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-64594-6_9,springer
Article,doi:10.1007/s11042-020-10022-4,Multimedia and machine learning approaches for data analytics,Multimedia Tools and Applications,10.1007/s11042-020-10022-4,Springer,2020-12-01,2020-10-31,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-10022-4,springer
Article,doi:10.1007/s11554-020-01026-2,Advances in deep learning for real-time image and video reconstruction and processing,Journal of Real-Time Image Processing,10.1007/s11554-020-01026-2,Springer,2020-12-01,2020-09-30,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-020-01026-2,springer
Article,doi:10.1007/s11042-020-09232-7,Artificial intelligence in deep learning algorithms for multimedia analysis,Multimedia Tools and Applications,10.1007/s11042-020-09232-7,Springer,2020-12-01,2020-07-16,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-09232-7,springer
Article,doi:10.1007/s11548-020-02240-w,Augmented reality for inner ear procedures: visualization of the cochlear central axis in microscopic videos,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-020-02240-w,Springer,2020-10-01,2020-07-31,"Purpose Visualization of the cochlea is impossible due to the delicate and intricate ear anatomy. Augmented reality may be used to perform auditory nerve implantation by transmodiolar approach in patients with profound hearing loss. Methods We present an augmented reality system for the visualization of the cochlear axis in surgical videos. The system starts with an automatic anatomical landmark detection in preoperative computed tomography images based on deep reinforcement learning. These landmarks are used to register the preoperative geometry with the real-time microscopic video captured inside the auditory canal. Three-dimensional pose of the cochlear axis is determined using the registration projection matrices. In addition, the patient microscope movements are tracked using an image feature-based tracking process. Results The landmark detection stage yielded an average localization error of $$2.18 \pm 1.44$$ 2.18 ± 1.44  mm ( $$n = 8$$ n = 8 ). The target registration error was $$0.31 \pm 0.10$$ 0.31 ± 0.10  mm for the cochlear apex and $$15.10 \pm 1.28 ^{\circ }$$ 15.10 ± 1 . 28 ∘ for the cochlear axis. Conclusion We developed an augmented reality system to visualize the cochlear axis in intraoperative videos. The system yielded millimetric accuracy and remained stable throughout the experimental study despite camera movements throughout the procedure in experimental conditions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-020-02240-w,springer
Article,doi:10.1007/s10772-020-09690-2,Pattern recognition and features selection for speech emotion recognition model using deep learning,International Journal of Speech Technology,10.1007/s10772-020-09690-2,Springer,2020-12-01,2020-09-08,"Automatic speaker recognizing models consists of a foundation on building various models of speaker characterization, pattern analyzing and engineering. The effect of classification and feature selection methods for the speech emotion recognition is focused. The process of selecting the exact parameter in arrangement with the classifier is an important part of minimizing the difficulty of system computing. This process becomes essential particularly for the models which undergo deployment in real time scenario. In this paper, a new deep learning speech based recognition model is presented for automatically recognizes the speech words. The superiority of an input source, i.e. speech sound in this state has straight impact on a classifier correctness attaining process. The Berlin database consist around 500 demonstrations to media persons that is both male and female. On the applied dataset, the presented model achieves a maximum accuracy of 94.21%, 83.54%, 83.65% and 78.13% under MFCC, prosodic, LSP and LPC features. The presented model offered better recognition performance over the other methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-020-09690-2,springer
Article,doi:10.1007/s11265-020-01562-x,eBrainII: a 3 kW Realtime Custom 3D DRAM Integrated ASIC Implementation of a Biologically Plausible Model of a Human Scale Cortex,Journal of Signal Processing Systems,10.1007/s11265-020-01562-x,Springer,2020-11-01,2020-07-07,"The Artificial Neural Networks (ANNs), like CNN/DNN and LSTM, are not biologically plausible. Despite their initial success, they cannot attain the cognitive capabilities enabled by the dynamic hierarchical associative memory systems of biological brains. The biologically plausible spiking brain models, e.g., cortex, basal ganglia, and amygdala, have a greater potential to achieve biological brain like cognitive capabilities. Bayesian Confidence Propagation Neural Network (BCPNN) is a biologically plausible spiking model of the cortex. A human-scale model of BCPNN in real-time requires 162 TFlop/s, 50 TBs of synaptic weight storage to be accessed with a bandwidth of 200 TBs. The spiking bandwidth is relatively modest at 250 GBs/s. A hand-optimized implementation of rodent scale BCPNN has been done on Tesla K80 GPUs require 3 kWs, we extrapolate from that a human scale network will require 3 MWs. These power numbers rule out such implementations for field deployment as cognition engines in embedded systems. The key innovation that this paper reports is that it is feasible and affordable to implement real-time BCPNN as a custom tiled application-specific integrated circuit (ASIC) in 28 nm technology with custom 3D DRAM - eBrainII - that consumes 3 kW for human scale and 12 watts for rodent scale. Such implementations eminently fulfill the demands for field deployment.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11265-020-01562-x,springer
Article,doi:10.1007/s11042-020-09847-w,Research on real-time data transmission and multi-scale video image decomposition of embedded optical sensor array based on machine learning,Multimedia Tools and Applications,10.1007/s11042-020-09847-w,Springer,2020-10-06,2020-10-06,"Aiming at the research of real-time data transmission and multi-scale image decomposition of embedded optical sensor array, the principle, method and fusion strategy of multi-sensor image fusion are studied comprehensively, thoroughly and systematically by combining the imaging characteristics of source image with multi-scale geometric analysis tools using machine learning algorithm. A new quality scalable video image coding framework is also proposed in this paper, which is implemented by a multi-scale online dictionary learning algorithm based on structured sparse video signals. For the purpose of different types of images and image fusion, a new high quality scalable video image coding framework based on machine learning algorithm is proposed on the basis of comprehensive analysis of prior information such as imaging mechanism of image sensor and imaging characteristics of source image. A multi-scale online dictionary learning algorithm based on machine learning for sparse video signal structure is proposed. Through the hierarchical structure of wavelet decomposition, the searching domain of online learning is optimized to a hierarchical sparse block, and its sparse representation coefficients are obtained by using machine learning sparse coding idea. The real-time data transmission of embedded optical sensor array based on machine learning and multi-scale image decomposition algorithm proposed in this paper have good fusion performance, which is of great significance for further research and engineering application of image fusion technology.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-09847-w,springer
Article,doi:10.1007/s10845-020-01539-4,The architecture development of Industry 4.0 compliant smart machine tool system (SMTS),Journal of Intelligent Manufacturing,10.1007/s10845-020-01539-4,Springer,2020-12-01,2020-01-29,"As part of the fourth industrial revolution, the movement to apply various enabling technologies under the name of Industry 4.0 is being promoted worldwide. Because of the wide range of applications and the capacity of manufacturing workpieces flexibly, machine tools are regarded as essential industrial elements. Hence, much research has been concerned with applying various enabling technologies such as cyber-physical systems to machine tools. To realize a machine tool suitable for Industry 4.0, development should be done in a systematic manner rather than the ad-hoc application of enabling technologies. In this paper, we propose a functional architecture for the Industry 4.0 version of machine tools, namely smart machine tool system. To reflect the voices of various stakeholders, stakeholder requirements are identified and transformed into design considerations. The design considerations are incorporated into the conceptual model and functional modeling, both of which are used to derive the functional architecture. The implementation procedure and an illustrative case study are presented for the application of the functional architecture.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10845-020-01539-4,springer
Article,doi:10.1007/s11517-020-02251-4,Fast body part segmentation and tracking of neonatal video data using deep learning,Medical & Biological Engineering & Computing,10.1007/s11517-020-02251-4,Springer,2020-12-01,2020-10-23,"Photoplethysmography imaging (PPGI) for non-contact monitoring of preterm infants in the neonatal intensive care unit (NICU) is a promising technology, as it could reduce medical adhesive-related skin injuries and associated complications. For practical implementations of PPGI, a region of interest has to be detected automatically in real time. As the neonates’ body proportions differ significantly from adults, existing approaches may not be used in a straightforward way, and color-based skin detection requires RGB data, thus prohibiting the use of less-intrusive near-infrared (NIR) acquisition. In this paper, we present a deep learning-based method for segmentation of neonatal video data. We augmented an existing encoder-decoder semantic segmentation method with a modified version of the ResNet-50 encoder. This reduced the computational time by a factor of 7.5, so that 30 frames per second can be processed at 960 × 576 pixels. The method was developed and optimized on publicly available databases with segmentation data from adults. For evaluation, a comprehensive dataset consisting of RGB and NIR video recordings from 29 neonates with various skin tones recorded in two NICUs in Germany and India was used. From all recordings, 643 frames were manually segmented. After pre-training the model on the public adult data, parts of the neonatal data were used for additional learning and left-out neonates are used for cross-validated evaluation. On the RGB data, the head is segmented well (82% intersection over union, 88% accuracy), and performance is comparable with those achieved on large, public, non-neonatal datasets. On the other hand, performance on the NIR data was inferior. By employing data augmentation to generate additional virtual NIR data for training, results could be improved and the head could be segmented with 62% intersection over union and 65% accuracy. The method is in theory capable of performing segmentation in real time and thus it may provide a useful tool for future PPGI applications. Graphical Abstract This work presents the development of a customized, real-time capable Deep Learning architecture for segmenting of neonatal videos recorded in the intensive care unit. In addition to hand-annotated data, transfer learning is exploited to improve performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11517-020-02251-4,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-78710-3_40,High-Speed Simulation of the 3D Behavior of Myocardium Using a Neural Network PDE Approach,Functional Imaging and Modeling of the Heart,10.1007/978-3-030-78710-3_40,Springer,2021-01-01,2021-06-18,"The full characterization of three-dimensional (3D) mechanical behaviour of myocardium is essential in understanding their function in health and disease. The hierarchical structure of myocardium results in their highly anisotropic mechanical behaviors, with the spatial variations in fiber structure giving rise to heterogeneity. The optimal set of loading paths has been used to estimate the constitutive parameters of myocardium using a novel numerical-experimental approach with full 3D kinematically controlled (triaxial) experiments [ 1 , 2 ]. Due to the natural variations in soft tissue structures, the mechanical behaviors of myocardium can vary dramatically within the same organ. To alleviate the associated computational costs for obtaining responses of myocardium under a range of loading conditions with a given realization of structure, we developed a neural network-based method integrated with finite elements. The boundary conditions were parameterized. The neural network generated a corresponding trial solution of the underling hyperelasticity problem for each boundary condition. Thus, the neural network approximated the parameter-to-state map. A physics-informed approach was used to train the neural network. Due to their learnability characteristics, the neural network was able to predict solutions for a range of boundary conditions with given individual specimen fiber structures. The neural network was validated with finite element solutions. This method will provide efficient and robust computational models for clinical evaluation to improve patient outcomes.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-78710-3_40,springer
Article,doi:10.1007/s11042-020-08781-1,Unsupervised learning on multimedia data: a Cultural Heritage case study,Multimedia Tools and Applications,10.1007/s11042-020-08781-1,Springer,2020-12-01,2020-03-14,"Integrating and analyzing a large amount of data extracted from different sources can be considered a key asset for businesses, organizations, research institutions that also deal with the Cultural Heritage domain. In the last decade, Internet of Things (IoT) technologies and the massive use of mobile devices contributed to generate an enormous flow of multimedia data, whose collection, analysis and interpretation allows for real-time analysis related to the behaviours, preferences and opinions of users. In this paper we present and discuss an unsupervised learning approach on multimedia features of a dataset coming from an Internet of Things framework. The main research objective of this work is to assess how the collection of behavioural IoT data coming from the Cultural Heritage domain can be opportunely exploited by means of unsupervised learning techniques in order to produce useful insights for the stakeholders, especially considering the multimedia features of such data. The presented experimental results, executed in a real case study, assess how the Cultural Heritage domain, and the related stakeholders, can benefit from these kind of services and applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-08781-1,springer
Article,doi:10.1007/s40192-020-00189-x,Unsupervised Deep Learning for Laboratory-Based Diffraction Contrast Tomography,Integrating Materials and Manufacturing Innovation,10.1007/s40192-020-00189-x,Springer,2020-12-01,2020-10-19,"An important leap forward for the 3D community is the possibility to perform non-destructive 3D microstructural imaging in the home laboratories. This possibility is profiled by a recently developed technique—laboratory X-ray diffraction contrast tomography (LabDCT). As diffraction spots in LabDCT images are the basis for 3D reconstruction of microstructures, it is critical to get their identification as precise as possible. In the present work we use a deep learning (DL) routine to optimize the identification of the spots. It is shown that by adding an artificial simple constant background noise to a series of forward simulated LabDCT diffraction images, DL can be trained and then learn to remove high frequency noise and low frequency radial gradients in brightness in the real experimental LabDCT images. The training of the DL routine is unsupervised in the sense that no human intervention is needed for labelling the data. The reduction in high frequency noise and low frequency radial gradients in brightness is demonstrated by comparing line profile scans through the experimental and the DL output images. Finally, the implications of this reduction procedure on the spot identification are analysed and possible improvements are discussed.",https://www.biomedcentral.com/openurl?doi=10.1007/s40192-020-00189-x,springer
Article,doi:10.1007/s11042-020-08998-0,Multi modal human action recognition for video content matching,Multimedia Tools and Applications,10.1007/s11042-020-08998-0,Springer,2020-12-01,2020-05-29,"Human action recognition (HAR)in videos is a challenging task in computer vision. Conventional methods are prone to explore the spatiotemporal or optical representations for video actions. However, optical representation might be inefficient in some real-life situations, such as object occlusion and dim light. To address this issue, this paper presents a novel approach for human action recognition by jointly exploiting video and Wi-Fi clues. We leverage the fact that Wi-Fi signals carry discriminative information of human actions, which is robust to optical limitations. To validate this innovative thought, we conceive a practical framework for HAR and setup a dataset containing both video clips and Wi-Fi Channel State Information of human actions. The 3D convolutional neural network was used to extract the video features and the statistical algorithms were used to extract radio features. A classical linear support vector machine is employed as the classifier after the video and radio feature fusion. Comprehensive experiments on this dataset achieved desirable results with the maximum improvement in accuracy by 10%. This demonstrates our promising findings: with the aid of Wi-Fi Channel State Information, the performance of the video action recognition methods can be improved significantly, even under the optical limitation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-08998-0,springer
Article,doi:10.1007/s11042-020-09870-x,Video smoke detection base on dense optical flow and convolutional neural network,Multimedia Tools and Applications,10.1007/s11042-020-09870-x,Springer,2020-10-06,2020-10-06,"Fire is one of the disasters with the highest probability among natural disasters and social disasters. It poses a serious threat to human life and life safety. In order to reduce fire losses, a reliable fire warning method is particularly important. But due to huge variations of smoke in color, shapes, and texture and complex application environments, the existing methods still do not meet the application requirements well. To solve these problems, in this paper, we propose a two-stage real-time video smoke detection method base on dense optical flow and convolutional neural network. In the first stage, we propose a fast pre-positioning module to obtain suspicious smoke areas through the dynamic characteristics of smoke which can greatly reduce the subsequent computational complexity, and only extract the moving optical flow of suspicious smoke areas as the dynamic features of the smoke which reduce the subsequent processing time cost. Instead of simply using moving optical flow as the dynamic characteristics of smoke, we found that the optical flow of the blue channel (OFBC) can effectively reflect the motion characteristics of smoke, so we combine the OFBC of suspicious smoke areas with its three RGB color channels to form a quaternion matrix for subsequent classification. In the second stage, we choose ResNet as our pre-classifier, and a temporal enhanced adjustment algorithm was proposed as the pre-classified follow-up fine optimization module, which can fully utilize the characteristics of the smoke movement in the video to improve detection rate. The experimental results show that compared with the existing smoke detection methods, our proposed method achieves high detection rate and low false alarm rate.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-09870-x,springer
Article,doi:10.1007/s00586-019-06054-6,Augmented reality and artificial intelligence-based navigation during percutaneous vertebroplasty: a pilot randomised clinical trial,European Spine Journal,10.1007/s00586-019-06054-6,Springer,2020-07-01,2019-07-02,"Purpose To assess technical feasibility, accuracy, safety and patient radiation exposure of a novel navigational tool integrating augmented reality (AR) and artificial intelligence (AI), during percutaneous vertebroplasty of patients with vertebral compression fractures (VCFs). Material and methods This prospective parallel randomised open trial compared the trans-pedicular access phase of percutaneous vertebroplasty across two groups of 10 patients, electronically randomised, with symptomatic single-level VCFs. Trocar insertion was performed using AR/AI-guidance with motion compensation in Group A, and standard fluoroscopy in Group B. The primary endpoint was technical feasibility in Group A. Secondary outcomes included the comparison of Groups A and B in terms of accuracy of trocar placement (distance between planned/actual trajectory on sagittal/coronal fluoroscopic images); complications; time for trocar deployment; and radiation dose/fluoroscopy time. Results Technical feasibility in Group A was 100%. Accuracy in Group A was 1.68 ± 0.25 mm (skin entry point), and 1.02 ± 0.26 mm (trocar tip) in the sagittal plane, and 1.88 ± 0.28 mm (skin entry point) and 0.86 ± 0.17 mm (trocar tip) in the coronal plane, without any significant difference compared to Group B ( p  > 0.05). No complications were observed in the entire population. Time for trocar deployment was significantly longer in Group A (642 ± 210 s) than in Group B (336 ± 60 s; p  = 0.001). Dose–area product and fluoroscopy time were significantly lower in Group A (182.6 ± 106.7 mGy cm^2 and 5.2 ± 2.6 s) than in Group B (367.8 ± 184.7 mGy cm^2 and 10.4 ± 4.1 s; p  = 0.025 and 0.005), respectively. Conclusion AR/AI-guided percutaneous vertebroplasty appears feasible, accurate and safe, and facilitates lower patient radiation exposure compared to standard fluoroscopic guidance. Graphic abstract These slides can be retrieved under Electronic Supplementary Material.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00586-019-06054-6,springer
Article,doi:10.1186/s40537-020-00322-9,Four-class emotion classification in virtual reality using pupillometry,Journal of Big Data,10.1186/s40537-020-00322-9,Springer,2020-07-06,2020-07-06,"Background Emotion classification remains a challenging problem in affective computing. The large majority of emotion classification studies rely on electroencephalography (EEG) and/or electrocardiography (ECG) signals and only classifies the emotions into two or three classes. Moreover, the stimuli used in most emotion classification studies utilize either music or visual stimuli that are presented through conventional displays such as computer display screens or television screens. This study reports on a novel approach to recognizing emotions using pupillometry alone in the form of pupil diameter data to classify emotions into four distinct classes according to Russell’s Circumplex Model of Emotions, utilizing emotional stimuli that are presented in a virtual reality (VR) environment. The stimuli used in this experiment are 360° videos presented using a VR headset. Using an eye-tracker, pupil diameter is acquired as the sole classification feature. Three classifiers were used for the emotion classification which are Support Vector Machine (SVM), k-Nearest Neighbor (KNN), and Random Forest (RF). Findings SVM achieved the best performance for the four-class intra-subject classification task at an average of 57.05% accuracy, which is more than twice the accuracy of a random classifier. Although the accuracy can still be significantly improved, this study reports on the first systematic study on the use of eye-tracking data alone without any other supplementary sensor modalities to perform human emotion classification and demonstrates that even with a single feature of pupil diameter alone, emotions could be classified into four distinct classes to a certain level of accuracy. Moreover, the best performance for recognizing a particular class was 70.83%, which was achieved by the KNN classifier for Quadrant 3 emotions. Conclusion This study presents the first systematic investigation on the use of pupillometry as the sole feature to classify emotions into four distinct classes using VR stimuli. The ability to conduct emotion classification using pupil data alone represents a promising new approach to affective computing as new applications could be developed using readily-available webcams on laptops and other mobile devices that are equipped with cameras without the need for specialized and costly equipment such as EEG and/or ECG as the sensor modality.",https://www.biomedcentral.com/openurl?doi=10.1186/s40537-020-00322-9,springer
Article,doi:10.1007/s42979-020-00304-x,Real-Time Processing of High-Resolution Video and 3D Model-Based Tracking for Remote Towers,SN Computer Science,10.1007/s42979-020-00304-x,Nature,2020-09-08,2020-09-08,"During the past decade, a new approach to providing air traffic services to airports from a remote location has been established, known as remote or digital tower. High quality video data is a core component in remote tower operations as it inherently contains a huge amount of information on which a controller can base decisions. The total resolution of a typical remote tower setup often exceeds 25 million RGB pixels and is captured at 30 frames per second or more. It is thus a challenge to efficiently process all the data in such a way as to provide relevant real-time enhancements to the controller. In this paper we describe the development of number of improvements and discuss how they can be implemented efficiently on a single workstation by decoupling processes, implementing attention mechanisms and utilizing hardware for parallel computing.",https://www.nature.com/articles/s42979-020-00304-x,springer
Article,doi:10.1186/s13636-020-00176-2,A depthwise separable convolutional neural network for keyword spotting on an embedded system,"EURASIP Journal on Audio, Speech, and Music Processing",10.1186/s13636-020-00176-2,Springer,2020-06-25,2020-06-25,"A keyword spotting algorithm implemented on an embedded system using a depthwise separable convolutional neural network classifier is reported. The proposed system was derived from a high-complexity system with the goal to reduce complexity and to increase efficiency. In order to meet the requirements set by hardware resource constraints, a limited hyper-parameter grid search was performed, which showed that network complexity could be drastically reduced with little effect on classification accuracy. It was furthermore found that quantization of pre-trained networks using mixed and dynamic fixed point principles could reduce the memory footprint and computational requirements without lowering classification accuracy. Data augmentation techniques were used to increase network robustness in unseen acoustic conditions by mixing training data with realistic noise recordings. Finally, the system’s ability to detect keywords in a continuous audio stream was successfully demonstrated.",https://www.biomedcentral.com/openurl?doi=10.1186/s13636-020-00176-2,springer
Article,doi:10.1007/s11554-019-00854-1,Fast video encoding based on random forests,Journal of Real-Time Image Processing,10.1007/s11554-019-00854-1,Springer,2020-08-01,2019-02-05,"Machine learning approaches have been increasingly used to reduce the high computational complexity of high-efficiency video coding (HEVC), as this is a major limiting factor for real-time implementations, due to the decision process required to find optimal coding modes and partition sizes for the quad-tree data structures defined by the standard. This paper proposes a systematic approach to reduce the computational complexity of HEVC based on an ensemble of online and offline Random Forests classifiers. A reduced set of features for training the Random Forests classifier is proposed, based on the rankings obtained from information gain and a wrapper-based approach. The best model parameters are also obtained through a consistent and generalizable method. The proposed Random Forests classifier is used to model the coding unit and transform unit-splitting decision and the SKIP-mode prediction, as binary classification problems, taking advantage from the combination of online and offline approaches, which adapts better to the dynamic characteristics of video content. Experimental results show that, on average, the proposed approach reduces the computational complexity of HEVC by 62.64% for the random access (RA) profile and 54.57% for the low-delay (LD) main profile, with an increase in BD-Rate of 2.58% for RA and 2.97% for LD, respectively. These results outperform the previous works also using ensemble classifiers for the same purpose.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-019-00854-1,springer
Article,doi:10.1007/s00521-019-04506-0,Real-time monitoring of driver drowsiness on mobile platforms using 3D neural networks,Neural Computing and Applications,10.1007/s00521-019-04506-0,Springer,2020-07-01,2019-10-13,"Driver drowsiness increases crash risk, leading to substantial road trauma each year. Drowsiness detection methods have received considerable attention, but few studies have investigated the implementation of a detection approach on a mobile phone. Phone applications reduce the need for specialised hardware and hence, enable a cost-effective roll-out of the technology across the driving population. While it has been shown that three-dimensional (3D) operations are more suitable for spatiotemporal feature learning, current methods for drowsiness detection commonly use frame-based, multi-step approaches. However, computationally expensive techniques that achieve superior results on action recognition benchmarks (e.g. 3D convolutions, optical flow extraction) create bottlenecks for real-time, safety-critical applications on mobile devices. Here, we show how depthwise separable 3D convolutions, combined with an early fusion of spatial and temporal information, can achieve a balance between high prediction accuracy and real-time inference requirements. In particular, increased accuracy is achieved when assessment requires motion information, for example, when sunglasses conceal the eyes. Further, a custom TensorFlow-based smartphone application shows the true impact of various approaches on inference times and demonstrates the effectiveness of real-time monitoring based on out-of-sample data to alert a drowsy driver. Our model is pre-trained on ImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness Detection dataset. Fine-tuning on large naturalistic driving datasets could further improve accuracy to obtain robust in-vehicle performance. Overall, our research is a step towards practical deep learning applications, potentially preventing micro-sleeps and reducing road trauma.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-019-04506-0,springer
Article,doi:10.1007/s00138-020-01117-x,Detection of 3D bounding boxes of vehicles using perspective transformation for accurate speed measurement,Machine Vision and Applications,10.1007/s00138-020-01117-x,Springer,2020-09-04,2020-09-04,"Detection and tracking of vehicles captured by traffic surveillance cameras is a key component of intelligent transportation systems. We present an improved version of our algorithm for detection of 3D bounding boxes of vehicles, their tracking and subsequent speed estimation. Our algorithm utilizes the known geometry of vanishing points in the surveilled scene to construct a perspective transformation. The transformation enables an intuitive simplification of the problem of detecting 3D bounding boxes to detection of 2D bounding boxes with one additional parameter using a standard 2D object detector. Main contribution of this paper is an improved construction of the perspective transformation which is more robust and fully automatic and an extended experimental evaluation of speed estimation. We test our algorithm on the speed estimation task of the BrnoCompSpeed dataset. We evaluate our approach with different configurations to gauge the relationship between accuracy and computational costs and benefits of 3D bounding box detection over 2D detection. All of the tested configurations run in real time and are fully automatic. Compared to other published state-of-the-art fully automatic results, our algorithm reduces the mean absolute speed measurement error by 32% (1.10 km/h to 0.75 km/h) and the absolute median error by 40% (0.97 km/h to 0.58 km/h).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00138-020-01117-x,springer
Article,doi:10.1007/s11265-019-01511-3,An End-to-End Approach to Automatic Speech Assessment for Cantonese-speaking People with Aphasia,Journal of Signal Processing Systems,10.1007/s11265-019-01511-3,Springer,2020-08-01,2020-02-18,"Conventional automatic assessment of pathological speech usually follows two main steps: (1) extraction of pathology-specific features; (2) classification or regression on extracted features. Given the great variety of speech and language disorders, feature design is never a straightforward task, and yet it is most crucial to the performance of assessment. This paper presents an end-to-end approach to automatic speech assessment for Cantonese-speaking People With Aphasia (PWA). The assessment is formulated as a binary classification task to discriminate PWA with high scores of subjective assessment from those with low scores. The 2-layer Gated Recurrent Unit (GRU) and Convolutional Neural Network (CNN) models are applied to realize the end-to-end mapping from basic speech features to the classification outcome. The pathology-specific features used for assessment are learned implicitly by the neural network model. The Class Activation Mapping (CAM) method is utilized to visualize how the learned features contribute to the assessment result. Experimental results show that the end-to-end approach can achieve comparable performance to the conventional two-step approach in the classification task, and the CNN model is able to learn impairment-related features that are similar to the hand-crafted features. The experimental results also indicate that CNN model performs better than 2-layer GRU model in this specific task.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11265-019-01511-3,springer
Article,doi:10.1038/s41569-020-0375-y,Estimating ejection fraction by video-based AI,Nature Reviews Cardiology,10.1038/s41569-020-0375-y,Nature,2020-06-01,2020-04-07,,https://www.nature.com/articles/s41569-020-0375-y,springer
Article,doi:10.1007/s13735-020-00194-y,Special issue on deep learning in image and video retrieval,International Journal of Multimedia Information Retrieval,10.1007/s13735-020-00194-y,Springer,2020-06-01,2020-05-16,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13735-020-00194-y,springer
Article,doi:10.1007/s11042-020-08785-x,Applications of deep learning for multimedia,Multimedia Tools and Applications,10.1007/s11042-020-08785-x,Springer,2020-04-01,2020-03-07,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-08785-x,springer
Article,doi:10.1007/s11042-017-5360-z,RETRACTED ARTICLE: Utilizing a deep learning model to enhance video credibility verification system,Multimedia Tools and Applications,10.1007/s11042-017-5360-z,Springer,2020-04-01,2017-11-16,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-017-5360-z,springer
Article,doi:10.1007/s11042-017-4821-8,RETRACTED ARTICLE: Image quality tendency modeling by fusing multiple visual cues,Multimedia Tools and Applications,10.1007/s11042-017-4821-8,Springer,2020-04-01,2017-06-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-017-4821-8,springer
Article,doi:10.1007/s12559-019-09653-z,"A Novel Real-Time, Lightweight Chaotic-Encryption Scheme for Next-Generation Audio-Visual Hearing Aids",Cognitive Computation,10.1007/s12559-019-09653-z,Springer,2020-05-01,2019-11-13,"Next-generation audio-visual (AV) hearing aids stand as a major enabler to realize more intelligible audio. However, high data rate, low latency, low computational complexity, and privacy are some of the major bottlenecks to the successful deployment of such advanced hearing aids. To address these challenges, we propose an integration of 5G Cloud-Radio Access Network (C-RAN), Internet of Things (IoT), and strong privacy algorithms to fully benefit from the possibilities these technologies have to offer. Existing audio-only hearing aids are known to perform poorly in noisy situations where overwhelming noise is present. Current devices make the signal more audible but remain deficient in restoring intelligibility. Thus, there is a need for hearing aids that can selectively amplify the attended talker or filter out acoustic clutter. The proposed 5G IoT-enabled AV hearing-aid framework transmits the encrypted compressed AV information and receives encrypted enhanced reconstructed speech in real time to address cybersecurity attacks such as location privacy and eavesdropping. For security implementation, a real-time lightweight AV encryption is proposed, based on a piece-wise linear chaotic map (PWLSM), Chebyshev map, and a secure hash and S-Box algorithm. For speech enhancement, the received secure AV (including lip-reading) information in the cloud is used to filter noisy audio using both deep learning and analytical acoustic modelling. To offload the computational complexity and real-time optimization issues, the framework runs deep learning and big data optimization processes in the background, on the cloud. The effectiveness and security of the proposed 5G-IoT-enabled AV hearing-aid framework are extensively evaluated using widely known security metrics. Our newly reported, deep learning-driven lip-reading approach for speech enhancement is evaluated under four different dynamic real-world scenarios (cafe, street, public transport, pedestrian area) using benchmark Grid and ChiME3 corpora. Comparative critical analysis in terms of both speech enhancement and AV encryption demonstrates the potential of the envisioned technology to deliver high-quality speech reconstruction and secure mobile AV hearing aid communication. We believe our proposed 5G IoT enabled AV hearing aid framework is an effective and feasible solution and represents a step change in the development of next-generation multimodal digital hearing aids. The ongoing and future work includes more extensive evaluation and comparison with benchmark lightweight encryption algorithms and hardware prototype implementation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12559-019-09653-z,springer
Article,doi:10.1007/s11042-019-7323-z,Effective multiple person recognition in random video sequences using a convolutional neural network,Multimedia Tools and Applications,10.1007/s11042-019-7323-z,Springer,2020-04-01,2019-02-09,"Effective and efficient face recognition through pervasive networks of surveillance cameras is one of the most challenging objectives of advanced computer vision. This study developed a real-time person recognition system (PRS) for the effective identification of multiple people in video sequences. We focused on identifying approximately 9000 celebrities by intelligent preprocessing, training, and deployment of a deep-learning convolutional neural network (CNN). The proposed PRS method comprises the following three major steps. In the first step, multiple faces present in a given frame as well as their associated landmarks are detected. This must be precise because the accuracy of this step dictates the accuracy of the complete PRS. In the second step, the extracted facial regions of interest are then aligned using affine warping, based on their respective identified landmark positions. The alignment process is meant to ensure correct identification of a person, because a wide range of faces entails intrinsic interclass similarities. Finally, in the third step, a VGG-19 CNN is trained to classify the aligned facial images for person recognition. In the training phase of the PRS, we utilized images from the CASIA WebFace database, which contains nearly 9000 classes, and aligned them using their respective facial landmarks. Subsequently, we used the aligned images to train a VGG-19 CNN classifier. For the purpose of validation, the trained classifier was tested with the standard Labelled Faces in the Wild (LFW) database by extracting the features for the LFW images using the trained VGG. Specifically, the VGG-extracted LFW features were used to train support vector machine classifiers, and the obtained resultant classification accuracy of approximately 96% was very close to the currently existing benchmark for the LFW database. During the testing phase, alternate frames of the input video were extracted and the identified faces (post-alignment) were used as inputs into the trained VGG to recognize the people in a given frame. When tested on random samples of video images, the proposed PRS offered robust recognition performance for most of the facial regions that had reasonable facial orientations and sizes. Furthermore, the average recognition time per person was approximately 370 milliseconds. The proposed deep learning-based PRS is the first of its kind to exhibit real-time performance for person recognition with significant accuracy, without involving any prior knowledge of the people involved in a video.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-019-7323-z,springer
Article,doi:10.1186/s42833-020-00011-0,Multi-scale X-ray tomography and machine learning algorithms to study MoNi_4 electrocatalysts anchored on MoO_2 cuboids aligned on Ni foam,BMC Materials,10.1186/s42833-020-00011-0,BioMed Central,2020-03-06,2020-03-06,"For a systematic materials selection and for design and synthesis of systems for electrochemical energy conversion with specific properties, it is essential to clarify the general relationship between physicochemical properties of the materials and the electrocatalytic performance and stability of the system or device. The design of highly performant and durable 3D electrocatalysts requires an optimized hierarchical morphology and surface structures with high activity. A systematic approach to determine the 3D morphology of hierarchically structured materials with high accuracy is described, based on a multi-scale X-ray tomography study. It is applied to a novel transition-metal-based materials system: MoNi_4 electrocatalysts anchored on MoO_2 cuboids aligned on Ni foam. The high accuracy of 3D morphological data of the formed micro- and nanostructures is ensured by applying machine learning algorithms for the correction of imaging artefacts of high-resolution X-ray tomography such as beam hardening and for the compensation of experimental inaccuracies such as misalignment and motions of samples and tool components. This novel approach is validated based on the comparison of virtual cross-sections through the MoNi_4 electrocatalysts and real FIB cross-sections imaged in the SEM.",https://www.biomedcentral.com/openurl?doi=10.1186/s42833-020-00011-0,springer
Article,doi:10.1007/s11042-019-08407-1,Surface defect detection of voltage-dependent resistors using convolutional neural networks,Multimedia Tools and Applications,10.1007/s11042-019-08407-1,Springer,2020-03-01,2019-12-16,"Surface defect detection is an important way to improve the production quality of voltage-dependent resistors (VDRs). To improve the accuracy and efficiency of VDR surface quality detection, an end-to-end surface quality detection method based on deep convolutional neural networks (CNNs) was proposed. The method includes four stages: data preparation, convolution neural network design, CNN training, and testing. First, images of VDRs were acquired from three perspectives, i.e., the front, back, and side, and then training, validation and testing sets were obtained. Second, the proposed CNN models for VDR surface defect detection were constructed. Third, during the training stage, the images with class labels from the established training sets were input to the proposed network for training and validation. Finally, in the testing stage, test images from a total of 408 samples of two VDR models were used to test the trained network. The sensitivity, specificity, accuracy, precision and F measure of the proposed algorithm were compared with those of state-of-the-art methods, and the experimental results showed that the proposed method has a high recognition speed and accuracy and meets the requirements of online real-time detection.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-019-08407-1,springer
Article,doi:10.1007/s11042-018-6963-8,Editorial Note: Machine Learning for Visual Analysis of Multimedia Data,Multimedia Tools and Applications,10.1007/s11042-018-6963-8,Springer,2020-02-01,2018-12-14,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-018-6963-8,springer
Article,doi:10.1007/s11042-019-08560-7,Artificial intelligence in multimedia computing,Multimedia Tools and Applications,10.1007/s11042-019-08560-7,Springer,2020-02-01,2020-01-02,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-019-08560-7,springer
Article,doi:10.1007/s00521-019-04166-0,The emergence of deep learning: new opportunities for music and audio technologies,Neural Computing and Applications,10.1007/s00521-019-04166-0,Springer,2020-02-01,2019-04-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-019-04166-0,springer
Article,doi:10.1007/s11432-019-2737-0,SynthText3D: synthesizing scene text images from 3D virtual worlds,Science China Information Sciences,10.1007/s11432-019-2737-0,Springer,2020-01-15,2020-01-15,"With the development of deep neural networks, the demand for a significant amount of annotated training data becomes the performance bottlenecks in many fields of research and applications. Image synthesis can generate annotated images automatically and freely, which gains increasing attention recently. In this paper, we propose to synthesize scene text images from the 3D virtual worlds, where the precise descriptions of scenes, editable illumination/visibility, and realistic physics are provided. Different from the previous methods which paste the rendered text on static 2D images, our method can render the 3D virtual scene and text instances as an entirety. In this way, real-world variations, including complex perspective transformations, various illuminations, and occlusions, can be realized in our synthesized scene text images. Moreover, the same text instances with various viewpoints can be produced by randomly moving and rotating the virtual camera, which acts as human eyes. The experiments on the standard scene text detection benchmarks using the generated synthetic data demonstrate the effectiveness and superiority of the proposed method.",http://link.springer.com/openurl/pdf?id=doi:10.1007/s11432-019-2737-0,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-34986-8_4,Speech Evaluation Based on Deep Learning Audio Caption,Advances in E-Business Engineering for Ubiquitous Computing,10.1007/978-3-030-34986-8_4,Springer,2020-01-01,2019-11-28,"Speech evaluation is an essential process of language learning. Traditionally, speech evaluation is done by experts evaluate voice and pronunciation from testers, which lack of efficiency and standards. In this paper, we propose a novel approach, based on deep learning and audio caption, to evaluate speeches instead of linguistic experts. First, the proposed approach extracts audio features from the speech. Then, the relationships between audio features expert evaluations are learned by deep learning. At last, an LSTM model is applied to predict expert evaluations. The experiment is done in a real-world dataset collected by our collaborative company. The result shows the proposed approach achieves excellent performance and has high potentials in the application.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-34986-8_4,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-50729-9_22,A Robot Agent that Learns Group Interaction Through a Team-Based Virtual Reality Game Using Affective Reward Reinforcement Learning,HCI International 2020 - Posters,10.1007/978-3-030-50729-9_22,Springer,2020-01-01,2020-07-10,"In the near future, robots are expected to be integrated into people’s lives, interacting with them. To develop better robotics and artificial intelligence, this research focuses on the concept of teamwork. A robot agent was implemented in a virtual reality(VR) game to play the sport roundnet, a team-based sport similar to table tennis and volleyball [ 2 ]. The agent is trained with reinforcement learning with EDA skin sensor data [ 6 ] of players. The system is evaluated using a questionnaire on the player’s feeling during the experiment and compared with agents not trained with affective data. The system is implemented in Unity3D’s ML-Agents Toolkit.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-50729-9_22,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-3250-4_225,AR Augmented Reality Intelligent Image Recognition System Based on the Artificial Intelligence,Frontier Computing,10.1007/978-981-15-3250-4_225,Springer,2020-01-01,2020-02-26,"With the rise of the AR technology, the effect of the virtual image enhancement has also been improved. The experimental results show that the proposed virtual reality simulation system model can effectively achieve the ideal goal of the image enhancement in the virtual reality images, and has the remarkable advantage of low memory consumption, which meets the international standards of the virtual image testing, and the effect is close to reality. Therefore, the active combination of the AR technology is helpful to improve the effect of the virtual image enhancement and has the good application value.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-3250-4_225,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-2568-1_76,Landscape Sand Table System Based on Deep Learning and Augmented Reality Technology,Big Data Analytics for Cyber-Physical System in Smart City,10.1007/978-981-15-2568-1_76,Springer,2020-01-01,2020-01-12,"Landscape sand table has been widely used in many fields, and higher requirements have been put forward for landscape sand table system. Therefore, the traditional landscape sand table system cannot meet the needs of current use. In addition, the traditional research has great limitations and cannot meet the requirements of high restoration and precision of the landscape sand table. In this paper, on the basis of deep study and augmented reality, combine the two, the in-depth study in augmented reality, and with landscape as the research object, combined with image processing and network technology knowledge, based on extensive analysis of the data source, this paper proposes a method to build a landscape sand table system with higher efficiency and restoration degree. This method is a deep learning algorithm constructed through parameter optimization and experimental verification, which can accurately identify scene information and realize augmented reality effect. In this paper, the stochastic gradient descent algorithm was used to conduct an experiment to improve the accuracy of the target detection in the landscape sand table. The experimental results also showed that the augmented reality technology could make the landscape sand table with high reproducibility and stereoscopic effect, and the use of the augmented reality effect could make the landscape sand table show better development.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-2568-1_76,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-49282-3_28,iVision: An Assistive System for the Blind Based on Augmented Reality and Machine Learning,Universal Access in Human-Computer Interaction. Design Approaches and Supporting Technologies,10.1007/978-3-030-49282-3_28,Springer,2020-01-01,2020-07-10,"In this paper, an assistive system iVision for the blind is proposed, which can solve one of the main problems faced by the visually impaired. We use augmented reality technology to create a three-dimensional model of the surrounding space and machine learning model to find the object of interest in the image taken by the camera. Then we combine these data to get the actual spatial location of the objects. This location information can be used by visually impaired people to search for objects of interest and avoid dangerous obstacle. The system uses speech and audio to feedback the spatial location to the user. Voice user interfaces (VUIs) is designed for iVision which allow the user to interact with a system through voice or speech commands. We verified the usability of the proposed system in evaluation experiments and noted its innovations and limitations.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-49282-3_28,springer
Article,doi:10.1007/s10044-018-00770-3,Understanding temporal structure for video captioning,Pattern Analysis and Applications,10.1007/s10044-018-00770-3,Springer,2020-02-01,2019-01-05,"Recent research in convolutional and recurrent neural networks has fueled incredible advances in video understanding. We propose a video captioning framework that achieves the performance and quality necessary to be deployed in distributed surveillance systems. Our method combines an efficient hierarchical architecture with novel attention mechanisms at both the local and global levels. By shifting focus to different spatiotemporal locations, attention mechanisms correlate sequential outputs with activation maps, offering a clever way to adaptively combine multiple frames and locations of video. As soft attention mixing weights are solved via back-propagation, the number of weights or input frames needs to be known in advance. To remove this restriction, our video understanding framework combines continuous attention mechanisms over a family of Gaussian distributions. Our efficient multistream hierarchical model combines a recurrent architecture with a soft hierarchy layer using both equally spaced and dynamically localized boundary cuts. As opposed to costly volumetric attention approaches, we use video attributes to steer temporal attention. Our fully learnable end-to-end approach helps predict salient temporal regions of action/objects in the video. We demonstrate state-of-the-art captioning results on the popular MSVD, MSR-VTT and M-VAD video datasets and compare several variants of the algorithm suitable for real-time applications. By adjusting the frame rate, we show a single computer can generate effective video captions for 100 simultaneous cameras. We additionally perform studies to show how bit rate compression modifies captioning results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10044-018-00770-3,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-17798-0_22,Accident Recognition via 3D CNNs for Automated Traffic Monitoring in Smart Cities,Advances in Computer Vision,10.1007/978-3-030-17798-0_22,Springer,2020-01-01,2019-04-24,"Automatic recognition of road accidents in traffic videos can improve road safety. Smart cities can deploy accident recognition systems to promote urban traffic safety and efficiency. This work reviews existing approaches for automatic accident detection and highlights a number of challenges that make accident detection a difficult task. Furthermore, we propose to implement a 3D Convolutional Neural Network (CNN) based accident detection system. We customize a video game to generate road traffic video data in a variety of weather and lighting conditions. The generated data is preprocessed using optical flow method and injected with noise to focus only on motion and introduce further variations in the data, respectively. The resulting data is used to train the model, which was then tested on real-life traffic videos from YouTube. The experiments demonstrate that the performance of the proposed algorithm is comparable to that of the existing models, but unlike them, it is not dependent on a large volume of real-life video data for training and does not require manual tuning of any thresholds.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-17798-0_22,springer
Chapter,doi:10.1007/978-3-030-18732-3_1,"The Convergence of Digital Twin, IoT, and Machine Learning: Transforming Data into Action",Digital Twin Technologies and Smart Cities,10.1007/978-3-030-18732-3_1,Springer,2020-01-01,2019-07-23,"Digital twins, Internet of Things (IoT), block chains, and Artificial Intelligence (AI) may redefine our imagination and future vision of globalization. Digital Twin will likely affect most of the enterprises worldwide as it duplicates the physical model for remote monitoring, viewing, and controlling based on the digital format. It is actually the living model of the physical system which continuously adapts to operational changes based on the real-time data from various IoT sensors and devices and forecasts the future of the corresponding physical counterparts with the help of machine learning/artificial intelligence. We have investigated the architecture, applications, and challenges in the implementation of digital twin with IoT capabilities. Some of the major research areas like big data and cloud, data fusion, and security in digital twins have been explored. AI facilitates the development of new models and technology systems in the domain of intelligent manufacturing.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-18732-3_1,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-60884-2_7,Click Event Sound Detection Using Machine Learning in Automotive Industry,Advances in Soft Computing,10.1007/978-3-030-60884-2_7,Springer,2020-01-01,2020-10-07,"Artificial intelligence has been playing an important role when it comes to the automotive industry and its quality of assemblies in the production line, this is because since the arrival of the industry 4.0 it has been subject to change and continuous improvement. In the past, we’ve observed how many machine learning architectures have been used to create environmental sound classification systems in order to improve traditional systems, thus overcoming efficiency issues with great results. In this work, we present a machine learning solution/approach for click event sound detection using audio sensors that are used in the assembly of electric harnesses for engines, this being done on an automotive production line, where we divided our workflow into: data collection, pre-processing, feature extraction, training and inference and finally the detection of the click event sounds. We created a dataset that is composed by 25,000 audio files that have an average duration of 0.025 seconds per click sound with the purpose of training a Multi-layer Perceptron and bring it into the inference phase. In order to test this approach, we’ve performed various implementations in a laboratory and in the real automotive industry. We obtained 95.23% in F1-Score Metric in a laboratory, while in real conditions, we obtained less reliable results, as 84.00% as the best results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-60884-2_7,springer
Article,doi:10.1007/s13369-019-03969-6,Efficient and Fast Objects Detection Technique for Intelligent Video Surveillance Using Transfer Learning and Fine-Tuning,Arabian Journal for Science and Engineering,10.1007/s13369-019-03969-6,Springer,2020-03-01,2019-06-28,"Intelligent video surveillance systems require effective techniques in order to detect objects accurately and rapidly. The most suitable algorithms for performing this task are based on convolutional neural networks. Existing approaches encounter a wide range of difficulties in terms of dealing with different sizes, high definition, or colored images turning these latter slower and less precise. The real-time sensitive application offers an interesting challenge for the optimization of the quality and quantity of previous approaches, thus obtaining an efficient system with regard to surveillance environment. This paper presents a novel, fast, and precise technique for advanced object detection as far as intelligent video surveillance systems are concerned. Thus, we propose the transfer learning of an efficient pre-trained network to appropriate datasets for our application and its integration in the architecture of our algorithm. Accordingly, we implement a fine-tuning on this pre-trained model via replacing the softmax layer and running backpropagation. Then, we compare the results of the previous algorithms using common evaluation parameters. The experimental results reveal that with this technique, we can enhance the precision and the accuracy of object detection in video surveillance scenes to more than $$90 \%$$ 90 % . Furthermore, along with dealing with different input dimensions, the detector runs in real time. To conclude, our application of machine learning for intelligent video surveillance systems maximizes their efficiency in highly difficult situations.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13369-019-03969-6,springer
Article,doi:10.1007/s00034-019-01234-7,Multi-object Detection and Tracking (MODT) Machine Learning Model for Real-Time Video Surveillance Systems,"Circuits, Systems, and Signal Processing",10.1007/s00034-019-01234-7,Springer,2020-02-01,2019-08-20,"Recently, video surveillance has garnered considerable attention in various real-time applications. Due to advances in the field of machine learning, numerous techniques have been developed for multi-object detection and tracking (MODT). This paper introduces a new MODT methodology. The proposed method uses an optimal Kalman filtering technique to track the moving objects in video frames. The video clips were converted based on the number of frames into morphological operations using the region growing model. After distinguishing the objects, Kalman filtering was applied for parameter optimization using the probability-based grasshopper algorithm. Using the optimal parameters, the selected objects were tracked in each frame by a similarity measure. Finally, the proposed MODT framework was executed, and the results were assessed. The experiments showed that the MODT framework achieved maximum detection and tracking accuracies of 76.23% and 86.78%, respectively. The results achieved with Kalman filtering in the MODT process are compared with the results of previous studies.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00034-019-01234-7,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-58545-7_17,3D Human Shape and Pose from a Single Low-Resolution Image with Self-Supervised Learning,Computer Vision – ECCV 2020,10.1007/978-3-030-58545-7_17,Springer,2020-01-01,2020-11-05,"3D human shape and pose estimation from monocular images has been an active area of research in computer vision, having a substantial impact on the development of new applications, from activity recognition to creating virtual avatars. Existing deep learning methods for 3D human shape and pose estimation rely on relatively high-resolution input images; however, high-resolution visual content is not always available in several practical scenarios such as video surveillance and sports broadcasting. Low-resolution images in real scenarios can vary in a wide range of sizes, and a model trained in one resolution does not typically degrade gracefully across resolutions. Two common approaches to solve the problem of low-resolution input are applying super-resolution techniques to the input images which may result in visual artifacts, or simply training one model for each resolution, which is impractical in many realistic applications. To address the above issues, this paper proposes a novel algorithm called RSC-Net, which consists of a Resolution-aware network, a Self-supervision loss, and a Contrastive learning scheme. The proposed network is able to learn the 3D body shape and pose across different resolutions with a single model. The self-supervision loss encourages scale-consistency of the output, and the contrastive learning scheme enforces scale-consistency of the deep features. We show that both these new training losses provide robustness when learning 3D shape and pose in a weakly-supervised manner. Extensive experiments demonstrate that the RSC-Net can achieve consistently better results than the state-of-the-art methods for challenging low-resolution images.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-58545-7_17,springer
Chapter,doi:10.1007/978-3-030-48118-6_3,A Survey: Implemented Architectures of 3D Convolutional Neural Networks,Cognitive Computing in Human Cognition,10.1007/978-3-030-48118-6_3,Springer,2020-01-01,2020-06-19,"Convolutional Neural Networks are those types of deep neural networks that have shown great performance on a varied set of benchmarks. This powerful learning ability of a convolutional neural network is mainly due to the use of several hidden layers that can automatically extract all the important features from the data. Due to the availability of datasets in large amount and discovery for better hardware units has extended the research in CNNs to a great extent, and due to such extensive research work, a large number of architectures have been reported and implemented on real-world data. The introduction of 3D sensors encouraged pursuing research in visionary aspect of computers for many real-world application areas including virtual-reality, augmented-reality and medical imaging. Recently, many 2D and 3D classification methods depend on CNNs, which have a reliable resource in extracting features. But, they cannot find out all the dimensional relationships between features due to the max-pooling layers, and they require a vast amount of data for training. In this paper, we survey on different implementations of 3D Convolutional neural networks and their respective accuracies for different datasets. We then compare all the architectures to find which one is most suitable to perform flexibly on CBCT scanned images.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-48118-6_3,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-58465-8_21,AI4AR: An AI-Based Mobile Application for the Automatic Generation of AR Contents,"Augmented Reality, Virtual Reality, and Computer Graphics",10.1007/978-3-030-58465-8_21,Springer,2020-01-01,2020-08-31,"Augmented reality (AR) is the process of using technology to superimpose images, text or sounds on top of what a person can already see. Art galleries and museums started to develop AR applications to increase engagement and provide an entirely new kind of exploration experience. However, the creation of contents results a very time consuming process, thus requiring an ad-hoc development for each painting to be increased. In fact, for the creation of an AR experience on any painting, it is necessary to choose the points of interest, to create digital content and then to develop the application. If this is affordable for the great masterpieces of an art gallery, it would be impracticable for an entire collection. In this context, the idea of this paper is to develop AR applications based on Artificial Intelligence. In particular, automatic captioning techniques are the key core for the implementation of AR application for improving the user experience in front of a painting or an artwork in general. The study has demonstrated the feasibility through a proof of concept application, implemented for hand held devices, and adds to the body of knowledge in mobile AR application as this approach has not been applied in this field before.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-58465-8_21,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-67070-2_34,LightNet: Deep Learning Based Illumination Estimation from Virtual Images,Computer Vision – ECCV 2020 Workshops,10.1007/978-3-030-67070-2_34,Springer,2020-01-01,2021-01-30,"In the era of virtual reality (VR), estimating illumination with lighting direction and lighting virtual objects has been a challenging problem. In VR, poor estimation of illumination and lighting direction makes any virtual objects into unrealistic. The inaccurate estimation of lighting can also cause strong artifacts in relighting of the virtual images. Inspired by these issues, the main objective of this paper is to enrich visual rationality of single image by providing accurate assessments of real illumination and lighting direction. We proposed a LightNet architecture by modelling Denseset121 network to estimate the light direction and color temperature level in any virtual reality images. We present quantitative results on VIDIT dataset to evaluate the performance and achieved good results in all the performance metrics. The experimental results proved that the proposed model is robust and provides a good level of accuracy in estimating illumination and lighting direction.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-67070-2_34,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-59725-2_38,Enhanced Detection of Fetal Pose in 3D MRI by Deep Reinforcement Learning with Physical Structure Priors on Anatomy,Medical Image Computing and Computer Assisted Intervention – MICCAI 2020,10.1007/978-3-030-59725-2_38,Springer,2020-01-01,2020-09-29,"Fetal MRI is heavily constrained by unpredictable and substantial fetal motion that causes image artifacts and limits the set of viable diagnostic image contrasts. Current mitigation of motion artifacts is predominantly performed by fast, single-shot MRI and retrospective motion correction. Estimation of fetal pose in real time during MRI stands to benefit prospective methods to detect and mitigate fetal motion artifacts where inferred fetal motion is combined with online slice prescription with low-latency decision making. Current developments of deep reinforcement learning (DRL), offer a novel approach for fetal landmarks detection. In this task 15 agents are deployed to detect 15 landmarks simultaneously by DRL. The optimization is challenging, and here we propose an improved DRL that incorporates priors on physical structure of the fetal body. First, we use graph communication layers to improve the communication among agents based on a graph where each node represents a fetal-body landmark. Further, additional reward based on the distance between agents and physical structures such as the fetal limbs is used to fully exploit physical structure. Evaluation of this method on a repository of 3-mm resolution in vivo data demonstrates a mean accuracy of landmark estimation 10 mm of ground truth as 87.3%, and a mean error of 6.9 mm. The proposed DRL for fetal pose landmark search demonstrates a potential clinical utility for online detection of fetal motion that guides real-time mitigation of motion artifacts as well as health diagnosis during MRI of the pregnant mother.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-59725-2_38,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-60700-5_40,An Empirical Study on Feature Extraction in DNN-Based Speech Emotion Recognition,HCI International 2020 – Late Breaking Posters,10.1007/978-3-030-60700-5_40,Springer,2020-01-01,2020-11-08,"The current empirical study focuses on speech emotion recognition using speech data extracted from video clips. Although many studies reported speech emotion recognition, the majority of the studies presented were based on using acted and clean speech. A more challenging and realistic task would be using spontaneous noisy speech from video clips. In the current study, the modern and state-of-the-art i-vector features are applied and experimentally evaluated. Comparisons with the widely used low-level descriptors (LLDs) and functionals are also presented. To improve the classification accuracy, a method based on late fusion is investigated. Using the proposed method, higher accuracies were achieved compared to the sole use of individual features. For classification, a fully connected deep neural network (DNN) with several hidden layers was used.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-60700-5_40,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-1468-5_216,3D Reconstruction Method of Waterfront Recreational Landscape Space Based on Artificial Intelligence and Fuzzy Algorithms,Data Processing Techniques and Applications for Cyber-Physical Systems (DPTA 2019),10.1007/978-981-15-1468-5_216,Springer,2020-01-01,2020-02-04,"The paper proposes a method of building three-dimensional landscape entity model based on digital measurement. First, we use the relationship between the three-dimensional landscape images to build a rendezvous model, then get the coordinates of the landscape points systematically. Third, we calculate the translation and rotation relationship between the camera coordinate system and the world coordinate system and correct the distorted image points. Finally, we use the corrected three-dimensional coordinate points to realize the three-dimensional landscape entity model. The experimental results show that the proposed model has the characteristics of high accuracy and fast speed, which provides a feasible basis for the construction of digital.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-1468-5_216,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-3250-4_178,Urban Road Water Recognition Based on Deep Learning,Frontier Computing,10.1007/978-981-15-3250-4_178,Springer,2020-01-01,2020-02-26,"The urban road water recognition algorithm based on deep learning can achieve the automatic recognition of road water and timely report to relevant departments, thereby eliminating traffic accidents and road congestion caused by road water, and the method effectively reduces hardware resources cost. In this paper, an image recognition algorithm based on ResNet50 is proposed to accomplish the real-time intelligent recognition of urban road water. Firstly, the ResNet50 network is used to extract image features, and then the model training is carried out on the dataset of urban road water. Finally, the trained model is applied to the surveillance video of urban road to recognize the water by optimizing the model parameters. On the dataset of urban road water, the accuracy of the water state recognition of the algorithm is 94.10%, the recognition accuracy of the water level is 59.75%, and the processing speed reaches 15 FPS. The experimental results show that the algorithm has basically achieved real-time recognition of urban road water.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-3250-4_178,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-3992-3_58,Using Images for Real-Time Violence Detection in the Edge,Advances in Communication Systems and Networks,10.1007/978-981-15-3992-3_58,Springer,2020-01-01,2020-06-14,"Surveillance cameras have become commonplace in urban scenarios. These cameras are currently proving to be a valuable aid in apprehending criminals after the camera feed is analyzed. Their utility can be further enhanced if detection capabilities are provided, via additional hardware for realizing specific functionality. Currently, Deep Neural Networks are the most popular set of tools for image classification and have also been successfully adapted for other modalities such as video, speech and ECG signals. However, many of these Deep Nets are designed for very complex multi-class problems and hence do not allow for real-time functionality on an embedded platform, due to the large number of parameters involved during the classification task. In this work, we use a modified version of an existing dataset that can be easily trained with published networks to give reasonably good detection accuracy. It is shown via experiments that even smaller nets perform as well as state-of-the-art networks and provide the same level of detection accuracy as the more complex networks. Hence, the simpler networks can be used for a detection task by downloading it to an embedded system and allowing for classification to happen in real time.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-3992-3_58,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-58462-7_2,Audio Events Detection in Noisy Embedded Railway Environments,Dependable Computing - EDCC 2020 Workshops,10.1007/978-3-030-58462-7_2,Springer,2020-01-01,2020-08-31,"Ensuring passengers’ safety is one of the daily concerns of railway operators. To do this, various image and sound processing techniques have been proposed in the scientific community. Since the beginning of the 2010s, the development of deep learning made it possible to develop these research areas in the railway field included. Thus, this article deals with the audio events detection task (screams, glass breaks, gunshots, sprays) using deep learning techniques. It describes the methodology for designing a deep learning architecture that is both suitable for audio detection and optimised for embedded railway systems. We will describe how we designed from scratch two CRNN (Convolutional Recurrent Neural Network) for the detection task. And since the creation of a large and varied training database is one of the challenges of deep learning, this article also deals with the innovative methodology used to build a database of audio events in the railway environment. Finally, we will show the very promising results obtained during the experimentation in real of the model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-58462-7_2,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-57881-7_36,A Deep Reinforcement Learning Framework for Vehicle Detection and Pose Estimation in 3D Point Clouds,Artificial Intelligence and Security,10.1007/978-3-030-57881-7_36,Springer,2020-01-01,2020-09-01,"As for autonomous driving in urban environments, it is of significance to accurately capture the position and pose of vehicles. Those information can assist self-driving system in making right decisions to avoid potential risks. Currently, 3D point clouds captured by laser scanners are widely used in self-driving systems to sense the real environment. Therefore, in this paper, we propose a deep reinforcement learning framework for vehicle detection and pose estimation by using 3D point clouds. Specifically, to estimate the pose of vehicles, we propose to design a rotation action in Deep Q Network (DQN). In addition, by considering the whole detection procedure as Markov Decision Process (MDP), our intermediate detected results can further improve the detection performance of our proposed method. The evaluations are carried on outdoor point cloud scenes captured by VMX450 laser scanning system. The experimental results demonstrate the satisfied performance on vehicle detection and pose estimation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-57881-7_36,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-57884-8_6,Deep Learning Video Action Recognition Method Based on Key Frame Algorithm,Artificial Intelligence and Security,10.1007/978-3-030-57884-8_6,Springer,2020-01-01,2020-09-01,"In order to solve the problem of action recognition in short video and capture the key information of video, this paper first proposes a KGAF-means method for key frame extraction. The KGAF-means method is based on the clustering principle and combines the K-means algorithm with the artificial fish swarm algorithm to realize the key frame sequence extraction. Based on the extracted key frame sequence, the RGB image and the optical flow image are separately extracted by the improved dual-stream variable convolution network. Then, using the cascading method, the image feature vector and the optical flow feature vector are fused to obtain the fused feature vector for action recognition. The selected data set is the Charades data set. The experimental results show that the mAP value of the method is 22.9 on the public dataset Charades. And the results show that the proposed method has better robustness than other network models and improves the short video action recognition effect.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-57884-8_6,springer
Chapter ReferenceWorkEntry,doi:10.1007/978-3-030-03243-2_848-1,Deep Learning Based 3D Vision,Computer Vision,10.1007/978-3-030-03243-2_848-1,Springer,2020-01-01,2020-11-20,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-03243-2_848-1,springer
Chapter,doi:10.1007/978-3-030-35252-3_8,Using Artificial Intelligence to Bring Accurate Real-Time Simulation to Virtual Reality,Advanced Computational Intelligence Techniques for Virtual Reality in Healthcare,10.1007/978-3-030-35252-3_8,Springer,2020-01-01,2019-12-12,"There always has been an excruciating gap between theoretical possibilities, clinical trial and real world applications in the Medical Industry. Any new research, experimentation or training in this sector has always been subject to extreme scrutiny and legal intricacies, due to the complexity of the human body and any resulting complications that might arise from the application of prematurely tested techniques or tools. The introduction of Virtual Reality in the Medical Industry is bringing all these troubles to their heel. Simulations generated by virtual reality are currently being explored to impart education and practical medical experience to students and doctors alike, generate engaging environments for patients and thus assisting in various aspects ranging from treatment of medical conditions to rehabilitation. This book chapter aims to develop an understanding on how virtual reality is being applied in the healthcare industry. A formal study of various solutions for reducing the latency is presented along with research being done in the area for improving the performance and making the experience more immersive. It is evident that motion to photons latency plays a crucial role in determining a genuine virtual reality experience. Among many, foveated rendering and gaze tracking systems seem to be the most promising in creating exciting opportunities for virtual reality systems in the future.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-35252-3_8,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-32564-0_54,Contribution to Research the Applied Engineering Protocol to Implement a Fuzzy Regulator for Autonomous Driving of an Automotive Model Implemented in Virtual Reality,The 30th SIAR International Congress of Automotive and Transport Engineering,10.1007/978-3-030-32564-0_54,Springer,2020-01-01,2019-10-14,"Monitoring and controlling the autonomous driving in automotive sector where high traffic density and risk factors are recorded it’s today a common trend in global research and high potential for artificial intelligence implementation. But reaching to the large scale market and the series production in the same time it’s a quite difficult problem that has to be approached and solved by engineers. The objective of the present research paper is mainly to innovate the application of a fuzzy regulator and to model a specific vehicle type in virtual reality through multidisciplinary components. It aims to implement some authentic methods for data collection, transfer and whole digital management, in the vehicle control system for automotive sector correlated with the virtual reality protocols, meaning that the algorithms and protocols of the vehicle in different conditions will be modeled and digitally tested in a special environment a priori to their implementation on full scale model. Specific objectives consist in a short technical presentation of the fuzzy regulator operational protocol for system communication method in the case of an road vehicle with autonomous driving capabilities and a practical set-up conducted in the Laboratory of Artificial Intelligence for Automotive Sector at Technical University from Cluj-N with the technical data retrieved from virtual reality tested model. Artificial intelligence applied to the control system is integrated in the model when protocols are defined in virtual reality for optimizing the driving conditions and improving the stability and steering, thus positively impacting the environment, society and autonomous vehicles.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-32564-0_54,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-59000-0_17,A Hybrid Data Fusion Architecture for BINDI: A Wearable Solution to Combat Gender-Based Violence,"Multimedia Communications, Services and Security",10.1007/978-3-030-59000-0_17,Springer,2020-01-01,2020-09-24,"Currently, most of the affective computing research is about modifying and adapting the machine behavior based on the human emotional state. Although, the use of the affective state inference can be extended to provide a tool for other fields more society related such as gender violence detection, which is a real global emergency. Based on the World Health Organization (WHO) statistics, one in three women worldwide experiences gender-based violence, often from an intimate partner. Due to this motivation, the authors developed BINDI, which is a wearable solution for detecting automatically those situations. It uses affective computing together with short-term physiological and physical observations. It represents a step toward an autonomous, embedded, non-intrusive, and wearable system for detecting those situations and connecting the victim with a trusted circle. In this work, and as a response for improving the detection capability of BINDI, a novel hybrid data fusion architecture is proposed. This new architecture is intended to improve the already implemented decision level fusion architecture. Further details of the uni-modal systems and the different approaches needed to be explored in the future are given.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-59000-0_17,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-59725-2_28,PolypSeg: An Efficient Context-Aware Network for Polyp Segmentation from Colonoscopy Videos,Medical Image Computing and Computer Assisted Intervention – MICCAI 2020,10.1007/978-3-030-59725-2_28,Springer,2020-01-01,2020-09-29,"Polyp segmentation from colonoscopy videos is of great importance for improving the quantitative analysis of colon cancer. However, it remains a challenging task due to (1) the large size and shape variation of polyps, (2) the low contrast between polyps and background, and (3) the inherent real-time requirement of this application, where the segmentation results should be immediately presented to the doctors during the colonoscopy procedures for their prompt decision and action. It is difficult to develop a model with powerful representation capability, yielding satisfactory segmentation results in a real-time manner. We propose a novel and efficient context-aware network, named PolypSeg , in order to comprehensively address these challenges. The proposed PolypSeg consists of two key components: adaptive scale context module (ASCM) and semantic global context module (SGCM). The ASCM aggregates the multi-scale context information and takes advantage of an improved attention mechanism to make the network focus on the target regions and hence improve the feature representation. The SGCM enriches the semantic information and excludes the background noise in the low-level features, which enhances the feature fusion between high-level and low-level features. In addition, we introduce the deep separable convolution into our PolypSeg to replace the traditional convolution operations in order to reduce parameters and computational costs to make the PolypSeg run in a real-time manner. We conducted extensive experiments on a famous public available dataset for polyp segmentation task. Experimental results demonstrate that the proposed PolypSeg achieves much better segmentation results than state-of-the-art methods with a much faster speed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-59725-2_28,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-65736-9_38,A 3D Flower Modeling Method Based on a Single Image,Entertainment Computing – ICEC 2020,10.1007/978-3-030-65736-9_38,Springer,2020-01-01,2021-01-05,"Since the structure of the flower is too complex, the modeling of the flower faces huge challenges. This paper collects 3D scenes containing flower models, uses 3dsMax to extract flower models, and constructs flower dataset. This paper proposes an encoder-decoder network structure called MVF3D and adopted the trained MVF3D network to predict the missing perspective information, use a single RGB image to generate a depth map of flowers from different perspectives, and finally use the depth maps to reconstruct the flower models. To evaluate the performance of our proposed method, for simple flowers, the average chamfer distance between the reconstructed 3D model and the real model is 0.27, The experimental results have shown that our proposed method can preserve the true structure of the flower.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-65736-9_38,springer
Chapter ConferencePaper,doi:10.1007/978-981-33-4601-7_1,Multi-scale Graph Convolutional Neural Network for Object Recognition from Point Cloud Data,Urban Intelligence and Applications,10.1007/978-981-33-4601-7_1,Springer,2020-01-01,2020-12-19,"How to make robots understand the point cloud data which is collected from the 3D sensor and complete the recognition has become a hot research direction in recent years. In this paper, we propose a new approach to improve the critical robotic capability, semantic understanding of the environment (i.e., 3D object recognition). The convolutional neural network (CNN) method has a very good recognition result in the 2D image domain, but it has certain difficulty in applying irregular and unordered 3D point clouds data. The network for point cloud data generally uses the convolution to realize the extraction of point cloud features by finding the neighborhood features on the point set. Due to the different neighborhood scales caused by the irregularity of 3D point cloud data, we propose a CNN structure that combines multi-scale features. By finding multiple neighborhoods of the point set and establishing local graph extraction features, the stable expression of the local neighborhood is obtained. At the same time, the key point calibration method is added, so that the network can dynamically focus on key point features to improve the recognition result. In a series of analytical experiments, we demonstrate competing results that demonstrate the effectiveness of the network structure.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-33-4601-7_1,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-58598-3_21,Beyond 3DMM Space: Towards Fine-Grained 3D Face Reconstruction,Computer Vision – ECCV 2020,10.1007/978-3-030-58598-3_21,Springer,2020-01-01,2020-11-07,"Recently, deep learning based 3D face reconstruction methods have shown promising results in both quality and efficiency. However, most of their training data is constructed by 3D Morphable Model, whose space spanned is only a small part of the shape space. As a result, the reconstruction results lose the fine-grained geometry and look different from real faces. To alleviate this issue, we first propose a solution to construct large-scale fine-grained 3D data from RGB-D images, which are expected to be massively collected as the proceeding of hand-held depth camera. A new dataset Fine-Grained 3D face (FG3D) with 200k samples is constructed to provide sufficient data for neural network training. Secondly, we propose a Fine-Grained reconstruction Network (FGNet) that can concentrate on shape modification by warping the network input and output to the UV space. Through FG3D and FGNet, we successfully generate reconstruction results with fine-grained geometry. The experiments on several benchmarks validate the effectiveness of our method compared to several baselines and other state-of-the-art methods. The proposed method and code will be available at https://github.com/XiangyuZhu-open/Beyond3DMM .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-58598-3_21,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-1286-5_62,Detection of Garbage Disposal from a Mobile Vehicle Using Image Processing,International Conference on Innovative Computing and Communications,10.1007/978-981-15-1286-5_62,Springer,2020-01-01,2020-02-29,"One of the major causes of dispersal of various diseases is illegal disposal of waste materials. Littering also causes pollution and waste of resources. Therefore, to develop strategies and models to prevent littering is one of the interesting research area. The presented exposition focusses on real-time detection of roadside littering done by passengers in mobile vehicles. The proposed model consists of three major steps, namely, detection of vehicle, frame extraction and its analysis and detection of garbage. In the first step, the targeted vehicle frame detection is done using the Haar-cascade classifier method followed by the detection of a foreign particle motion originating from the vehicle using background subtraction and frame differencing method. Finally, the garbage is detected by calculating and analyzing the nature of curve of the foreign particle obtained from the processing of consecutive images of the target vehicle. Therefore, the experimental results indicate that if a parabolic or linear curve is observed in the motion near the detected vehicle, then it is considered to be the motion caused due to the garbage. A very high recognition rate is observed in the results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-1286-5_62,springer
Chapter ConferencePaper,doi:10.1007/978-981-33-4336-8_4,Numeric CNNs: CNNs for Reading Numeric Characters on Meters,"Cyberspace Data and Intelligence, and Cyber-Living, Syndrome, and Health",10.1007/978-981-33-4336-8_4,Springer,2020-01-01,2020-12-01,"Nowadays, Internet of Things (IoT) is becoming an irreplaceable role in human life. Moreover, the meter reading is becoming the procedure composed of picture collection and image recognition. Previous works treat meter reading as a problem of image classification. They only focus on the accuracy of classification but ignore numerical accuracy, which is the measurement’s essential performance. In this paper, we address that the meter reading is a hybrid regression and classification (HRC) problem. Under this definition, the resulting algorithm considers the targets of both measurement and digits recognition. To solve the HRC problem, we designed a hybrid regression and classification loss function and a multi-branch convolutional neural networks for numbers (N-CNNs). To further verify the effectiveness of the model’s classification and regression, we constructed two kinds of datasets: standard dataset and carry dataset. The N-CNNs establishes new state-of-the-art metrics both on regression and classification. Notably, the numerical precision of N-CNNs outperforms the classification-based methods. The numerical accuracy of the model has one order of magnitude higher than other models. Furthermore, we deployed N-CNNs in a realistic meter reading system based on smart meter shells and cloud computing.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-33-4336-8_4,springer
Chapter ConferencePaper,doi:10.1007/978-981-13-9683-0_5,Two Stream Convolutional Neural Networks for Anomaly Detection in Surveillance Videos,Smart Computing Paradigms: New Progresses and Challenges,10.1007/978-981-13-9683-0_5,Springer,2020-01-01,2019-12-01,"In this paper we propose Jamadandi, Adarsh  a deep learning framework to identify anomalous events Kotturshettar, Sunidhi  in surveillance videos. Anomalous events Mudenagudi, Uma  are those which do not adhere to normal behaviour. We propose to use two discriminatively trained Convolutional Neural Networks, to capture the spatial and temporal features of videos, the classification scores obtained from the two streams are later fused to assign one final score. Since our approach is scenario-based, this eliminates the need for adopting a particular definition of anomaly. We show that the Two Stream CNNs perfectly capture the intricacies involved in modelling a video data by demonstrating the framework on airport and mall surveillance datasets respectively. We achieve a final test accuracy of 99.1% for spatial stream and 91% for temporal stream for airport scenario and an accuracy of 94.7% for spatial and 90.1% for the temporal stream for the mall scenario. Our framework can be easily implemented in real-time and is capable of detecting anomaly in each frame fed by a live surveillance system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-13-9683-0_5,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-41005-6_26,Enhancing Speech Recorded from a Wearable Sensor Using a Collection of Autoencoders,High Performance Computing,10.1007/978-3-030-41005-6_26,Springer,2020-01-01,2020-02-12,"Assistive Technology (AT) is a concept which includes the use of technological devices to improve the learning process or the general capabilities of people with disabilities. One of the major tasks of the AT is the development of devices that offer alternative or augmentative communication capabilities. In this work, we implemented a simple AT device with a low-cost sensor for registering speech signals, in which the sound is perceived as low quality and corrupted. Thus, it is not suitable to integrate into speech recognition systems, automatic transcription or general recognition of vocal-tract sounds for people with disabilities. We propose the use of a group of artificial neural networks that improve different aspects of the signal. In the study of the speech enhancement, it is normal to focus on how to make improvements in specific conditions of the signal, such as background noise, reverberation, natural noises, among others. In this case, the conditions that degrade the sound are unknown. This uncertainty represents a bigger challenge for the enhancement of the speech, in a real-life application. The results show the capacity of the artificial neural networks to enhance the quality of the sound, under several objective evaluation measurements. Therefore, this proposal can become a way of treating these kinds of signals to improve robust speech recognition systems and increase the real possibilities for implementing low-cost AT devices.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-41005-6_26,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-0637-6_28,MHAD: Multi-Human Action Dataset,Fourth International Congress on Information and Communication Technology,10.1007/978-981-15-0637-6_28,Springer,2020-01-01,2019-12-01,"This paper presents a framework for a multi-action recognition method. In this framework, we introduce a new approach to detect and recognize the action of several persons within one scene. Also, considering the scarcity of related data, we provide a new data set involving many persons performing different actions in the same video. Our multi-action recognition method is based on a three-dimensional convolution neural network, and it involves a preprocessing phase to prepare the data to be recognized using the 3DCNN model. The new representation of data consists in extracting each person’s sequence during its presence in the scene. Then, we analyze each sequence to detect the actions in it. The experimental results proved to be accurate, efficient, and robust in real-time multi-human action recognition.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-0637-6_28,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-4018-9_15,Video Based Deception Detection Using Deep Recurrent Convolutional Neural Network,Computer Vision and Image Processing,10.1007/978-981-15-4018-9_15,Springer,2020-01-01,2020-03-29,"Automatic deception detection has been extensively studied considering their applicability in various real-life applications. Since humans will express the deception through non-verbal behavior that can be recorded in a non-intrusive manner, the deception detection from video using automatic techniques can be devised. In this paper, we present a novel technique for the video-based deception technique using Deep Recurrent Convolutional Neural Network. The proposed method uses the sequential input that can capture the spatiotemporal information to capture the non-verbal behavior from the video. The deep features are extracted from the sequence of frames using a pre-trained GoogleNet CNN. To effectively learn the extended sequence, the bi-directional LSTMs are connected to the GoogleNet and can be jointly trained to learn the perceptual representation. Extensive experiments are carried out on a publicly available dataset [ 5 ] with 121 deceptive and truthful video clips reflecting a real-life scenario. Obtained results demonstrate the outstanding performance of the proposed method when compared with the four different state-of-the-art techniques.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-4018-9_15,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-60276-5_30,Multi-corpus Experiment on Continuous Speech Emotion Recognition: Convolution or Recurrence?,Speech and Computer,10.1007/978-3-030-60276-5_30,Springer,2020-01-01,2020-09-29,"Extraction of semantic information from real-life speech, such as emotions, is a challenging task that has grown in popularity over the last few years. Recently, emotion processing in speech moved from discrete emotional categories to continuous affective dimensions. This trend helps in the design of systems that predict the dynamic evolution of affect in speech. However, no standard annotation guidelines exist for these dimensions thus making cross-corpus studies hard to achieve. Deep neural networks are nowadays predominant in the task of emotion recognition. Almost all systems use recurrent architectures, but convolutional networks were recently reassessed as they are faster to train and have less parameters than recurrent ones. This paper aims at investigating pros and cons of the aforementioned architectures using cross-corpus experiments to highlight the issue of corpus variability. We also explore the best suitable acoustic representation for continuous emotion, together with loss functions. We concluded that recurrent networks are robust to corpus variability and we confirm the power of cepstral features for continuous Speech Emotion Recognition (SER), especially for satisfaction prediction. A final post-treatment applied on prediction brings very nice result ( ccc = 0.719) on AlloSat and achieves new state of the art.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-60276-5_30,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-51100-5_14,A Local Occlusion Face Image Recognition Algorithm Based on the Recurrent Neural Network,Multimedia Technology and Enhanced Learning,10.1007/978-3-030-51100-5_14,Springer,2020-01-01,2020-07-19,"The recognition rate of traditional face recognition algorithm to the face image with occlusion is not high, resulting in poor recognition effect. Therefore, this paper proposes a partial occlusion face recognition algorithm based on recurrent neural network. According to the different light sources, the high filtering function is used to analyze the halo effect of the image, realize the preprocessing of partially occluded face image, set up the global face feature area and the local face feature area according to the image features, and extract the global and local features of the image; based on the time and structure features of the recursive neural network, establish the local subspace, and realize the local face image recognition Law. The experimental results show that: compared with the traditional algorithm, the face recognition algorithm studied in this paper has a higher recognition rate, and can accurately recognize the partially occluded face image, which meets the basic requirements of the current face image recognition.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-51100-5_14,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-62463-7_29,Research on Image Recognition Method of Ethnic Costume Based on VGG,Machine Learning for Cyber Security,10.1007/978-3-030-62463-7_29,Springer,2020-01-01,2020-11-11,"This paper uses deep learning related algorithms to classify and recognize the ethnic costume images of the Wa and Yi people. In the deep learning framework Tensorflow, the deep convolution network VGG model was migrated to the ethnic costume recognition task, and the image recognition of ethnic costume based on VGG was realized. By adjusting the size of the convolution kernel and the number of convolution layers of the VGGNet network model, a neural network model of ethnic costume image recognition suitable for the classification and recognition of the Wa and Yi people is constructed. By training the images in the ethnic costume image library, iteratively adjusting the parameters of Batch_size, Epoch, Dropout and other parameters of the network model, the comparative experiments are on, and the classification recognition rate of ethnic costume images under different network model parameters is analyzed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-62463-7_29,springer
Chapter ConferencePaper,doi:10.1007/978-981-15-9129-7_29,Detection of Various Speech Forgery Operations Based on Recurrent Neural Network,Security and Privacy in Digital Economy,10.1007/978-981-15-9129-7_29,Springer,2020-01-01,2020-10-22,"Most existed algorithms of speech forensics have been proposed to detect specific forgery operations. In realistic scenes, however, it is difficult to predict the type of the forgery. Since the suspicious speech might have been processed by some unknown forgery operation, it will give a confusing result based on a classifier for a specific forgery operation. To this end, a forensic algorithm based on recurrent neural network (RNN) and linear frequency cepstrum coefficients (LFCC) is proposed to detect four common forgery operations. The LFCC with its derivative coefficients is determined as the forensic feature. An RNN frame with two-layer LSTM is designed with preliminary experiments. Extensive experiments on TIMIT and UME databases show that the detection accuracy for the intra-database evaluation can achieve about 99%, and the detection accuracy for the cross-database can achieve higher than 88%. Finally, compared with the previous algorithm, better performance is obtained by the proposed algorithm.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-9129-7_29,springer
Article,doi:10.1007/s11432-018-9828-3,Multi-view based neural network for semantic segmentation on 3D scenes,Science China Information Sciences,10.1007/s11432-018-9828-3,Springer,2019-09-04,2019-09-04,,http://link.springer.com/openurl/pdf?id=doi:10.1007/s11432-018-9828-3,springer
Article,doi:10.1007/s00521-018-3579-x,Sitcom-star-based clothing retrieval for video advertising: a deep learning framework,Neural Computing and Applications,10.1007/s00521-018-3579-x,Springer,2019-11-01,2018-06-07,"This paper presents a novel learning-based framework for video content-based advertising, DeepLink, which aims at linking Sitcom-stars and online shops with clothing retrieval by using state-of-the-art deep convolutional neural networks (CNNs). Specifically, several deep CNN models are adopted for composing multiple sub-modules in DeepLink, including human-body detection, human pose selection, face verification, clothing detection and retrieval from advertisements (ads) pool that is constructed by clothing images crawled from real-world online shops. For clothing detection and retrieval from ad-images, we firstly transfer the state-of-the-art deep CNN models to our data domain, and then train corresponding models based on our constructed large-scale clothes datasets. Extensive experimental results demonstrate the feasibility and efficacy of our proposed clothing-based video advertising system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-018-3579-x,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-58539-6_12,Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring,Computer Vision – ECCV 2020,10.1007/978-3-030-58539-6_12,Springer,2020-01-01,2020-11-07,"Real-time video deblurring still remains a challenging task due to the complexity of spatially and temporally varying blur itself and the requirement of low computational cost. To improve the network efficiency, we adopt residual dense blocks into RNN cells, so as to efficiently extract the spatial features of the current frame. Furthermore, a global spatio-temporal attention module is proposed to fuse the effective hierarchical features from past and future frames to help better deblur the current frame. For evaluation, we also collect a novel dataset with paired blurry/sharp video clips by using a co-axis beam splitter system. Through experiments on synthetic and realistic datasets, we show that our proposed method can achieve better deblurring performance both quantitatively and qualitatively with less computational cost against state-of-the-art video deblurring methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-58539-6_12,springer
Article,doi:10.1007/s11042-019-08102-1,CaR-PLive: Cloud-assisted reinforcement learning based P2P live video streaming: a hybrid approach,Multimedia Tools and Applications,10.1007/s11042-019-08102-1,Springer,2019-12-01,2019-10-09,"In recent years, live video streaming has become one of the most popular and prevalent applications of the Internet. The Peer-to-Peer (P2P) and Content Delivery Network (CDN) are popular approaches to stream video contents. These approaches respectively have faced some drastic challenges such as obtaining the desired Quality of Service (QoS) level and minimizing economic cost. The cloud computing infrastructures can reveal proper solutions to these problems. The P2P systems can eliminate their bandwidth shortage by renting resources from the cloud environment. This paper depicts CaR-PLive as a hybrid cloud-assisted P2P live streaming system. CaR-PLive uses video servers such as Amazon EC2 from cloud to stream video contents and rents Cloud Storage Services (CSSs) such as Amazon S3 to assist P2P live streaming system to reach the desired playback continuity. In CaR-PLive, we proposed two stages (sub-windows) sliding window for buffer management that a sub-window belongs to the P2P system and another one belongs to CSS. The objective of CAR-PLive is to optimize the size of sub-windows to minimize the overall rental cost of CSS restricted to a desired QoS level. We formulate this problem as an optimization problem and model it with Markov Decision Process (MDP) and then propose a reinforcement learning based algorithm to solve this problem. Finally, we evaluate the performance of CaR-PLive by performing extensive simulations and experiments with realistic settings. Simulation results demonstrate that CaR-PLive efficiently mitigates overall CSS billing cost in different system configurations and provides desired playback continuity in different system settings.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-019-08102-1,springer
Article,doi:10.1007/s10278-019-00206-2,High Efficiency Video Coding (HEVC)–Based Surgical Telementoring System Using Shallow Convolutional Neural Network,Journal of Digital Imaging,10.1007/s10278-019-00206-2,Springer,2019-12-01,2019-04-12,"Surgical telementoring systems have gained lots of interest, especially in remote locations. However, bandwidth constraint has been the primary bottleneck for efficient telementoring systems. This study aims to establish an efficient surgical telementoring system, where the qualified surgeon (mentor) provides real-time guidance and technical assistance for surgical procedures to the on-spot physician (surgeon). High Efficiency Video Coding (HEVC/H.265)–based video compression has shown promising results for telementoring applications. However, there is a trade-off between the bandwidth resources required for video transmission and quality of video received by the remote surgeon. In order to efficiently compress and transmit real-time surgical videos, a hybrid lossless-lossy approach is proposed where surgical incision region is coded in high quality whereas the background region is coded in low quality based on distance from the surgical incision region. For surgical incision region extraction, state-of-the-art deep learning (DL) architectures for semantic segmentation can be used. However, the computational complexity of these architectures is high resulting in large training and inference times. For telementoring systems, encoding time is crucial; therefore, very deep architectures are not suitable for surgical incision extraction. In this study, we propose a shallow convolutional neural network (S-CNN)–based segmentation approach that consists of encoder network only for surgical region extraction. The segmentation performance of S-CNN is compared with one of the state-of-the-art image segmentation networks (SegNet), and results demonstrate the effectiveness of the proposed network. The proposed telementoring system is efficient and explicitly considers the physiological nature of the human visual system to encode the video by providing good overall visual impact in the location of surgery. The results of the proposed S-CNN-based segmentation demonstrated a pixel accuracy of 97% and a mean intersection over union accuracy of 79%. Similarly, HEVC experimental results showed that the proposed surgical region–based encoding scheme achieved an average bitrate reduction of 88.8% at high-quality settings in comparison with default full-frame HEVC encoding. The average gain in encoding performance (signal-to-noise) of the proposed algorithm is 11.5 dB in the surgical region. The bitrate saving and visual quality of the proposed optimal bit allocation scheme are compared with the mean shift segmentation–based coding scheme for fair comparison. The results show that the proposed scheme maintains high visual quality in surgical incision region along with achieving good bitrate saving. Based on comparison and results, the proposed encoding algorithm can be considered as an efficient and effective solution for surgical telementoring systems for low-bandwidth networks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10278-019-00206-2,springer
Article,doi:10.1007/s11042-019-7700-7,Computer vision based working environment monitoring to analyze Generalized Anxiety Disorder (GAD),Multimedia Tools and Applications,10.1007/s11042-019-7700-7,Springer,2019-11-01,2019-05-16,"Ever advancing development in Computer Vision and Deep Learning has increased the efficacy of smart monitoring by analyzing and predicting the physical abnormalities and generating time-sensitive results. Based on the improved principles of smart monitoring and data processing, a novel computer vision assisted deep learning based posture monitoring system is proposed to predict Generalized Anxiety Disorder (GAD) oriented physical abnormalities of an individual from their working environment. We used deep learning-assisted 3D Convolutional Neural Network (CNN) technology for spatio-temporal feature extraction and Gated Recurrent Unit (GRU) model to exploit the extracted temporal dynamics for adversity scale determination. The alert-based decisions with the deliverance of the physical state helps to increase the utility of the proposed system in the healthcare or assistive-care domain. The proficiency of the system is also enhanced by storing the predicted anomaly scores in the local database of the system which can be further used for therapeutic purposes. To validate the prediction performance of the proposed system, extensive experiments are conducted on three challenging datasets, NTU RGB+D, UTD-MHAD and HMDB51. The proposed methodology achieved comparable performance by obtaining the mean accuracy of 91.88%, 94.28%, and 70.33%, respectively. Furthermore, the average prediction time taken by the proposed methodology is approximately 1.13 seconds which demonstrates the real-time monitoring efficiency of the system. The calculated outcomes show that the proposed methodology performs better contrasted with other contemporary studies for activity prediction, data processing cost, error rate, and time complexity.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-019-7700-7,springer
Article,doi:10.1007/s10586-018-2422-6,Multimodal computer image recognition based on depth neural network,Cluster Computing,10.1007/s10586-018-2422-6,Springer,2019-11-01,2018-03-13,"In this paper, multi-modal computer image is identified based on deep neural network. First, we propose two rules for reduction. The first reduction rule is used to reduce the number of similarity calculations between adjacent vertices. The second reduction rule reduces calculation time by inaccurately calculating the similarity between adjacent vertices. Secondly, a structured graph based on GraphX in Spark is proposed Class Algorithm GXDSGC. The algorithm does not require a large amount of disk I/O overhead during operation. Finally, experiments on a large number of real data sets and synthetic data sets, confirm the effectiveness of the proposed GXDSGC algorithm. The GXDSGC algorithm is more than 30 times faster than the algorithm based on the MapReduce framework in Hadoop, which can significantly improve the efficiency of computer image recognition in big data analysis.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10586-018-2422-6,springer
Article,doi:10.1007/s11042-019-7703-4,Affective social big data generation algorithm for autonomous controls by CRNN-based end-to-end controls,Multimedia Tools and Applications,10.1007/s11042-019-7703-4,Springer,2019-10-15,2019-05-04,"Affective social multimedia computing provides us the opportunity to improve our daily lives. Various things, such as devices in ubiquitous computing environments and autonomous vehicles in real environments considering human beings, can be controlled by analyzing and learning affective social big data. Deep learning is a core learning algorithm for autonomous control; however, it requires huge amounts of learning data, and the process of collecting various types of learning data is expensive. The collection limit of affective social videos for deep learning is resolved by analyzing affective social videos, such as YouTube and Closed Circuit Television (CCTV) videos collected in advance, and generating new affective social videos more as learning data without human beings autonomously controlling other cameras. The control signals of the cameras are generated by Convolutional Neural Network (CNN)-based end-to-end controls. However, images captured consecutively need to be analyzed to improve the quality of the generated control signals. This paper proposes a system that generates affective social videos for deep learning by Convolutional Recurrent Neural Network (CRNN)-based end-to-end controls. The extracted images in affective social videos are utilized for calculating the control signals based on the CRNN. Additional affective social videos are then generated by the extracted consecutive images and camera control signals. The effectiveness of the proposed method was verified in the experiments by comparing the results obtained using the proposed method with those obtained using the traditional CNN. The results showed that the accuracy of the control signals obtained using the proposed method was 56.30% higher than that of the control signals obtained using the traditional CNN.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-019-7703-4,springer
Article,doi:10.1007/s00138-019-01043-7,One-shot learning hand gesture recognition based on modified 3d convolutional neural networks,Machine Vision and Applications,10.1007/s00138-019-01043-7,Springer,2019-10-01,2019-08-01,"Though deep neural networks have played a very important role in the field of vision-based hand gesture recognition, however, it is challenging to acquire large numbers of annotated samples to support its deep learning or training. Furthermore, in practical applications it often encounters some case with only one single sample for a new gesture class so that conventional recognition method cannot be qualified with a satisfactory classification performance. In this paper, the methodology of transfer learning is employed to build an effective network architecture of one-shot learning so as to deal with such intractable problem. Then some useful knowledge from deep training with big dataset of relative objects can be transferred and utilized to strengthen one-shot learning hand gesture recognition (OSLHGR) rather than to train a network from scratch. According to this idea a well-designed convolutional network architecture with deeper layers, C3D (Tran et al. in: ICCV, pp 4489–4497, 2015 ), is modified as an effective tool to extract spatiotemporal feature by deep learning. Then continuous fine-tune training is performed on a sample of new classes to complete one-shot learning. Moreover, the test of classification is carried out by Softmax classifier and geometrical classification based on Euclidean distance. Finally, a series of experiments and tests on two benchmark datasets, VIVA (Vision for Intelligent Vehicles and Applications) and SKIG (Sheffield Kinect Gesture) are conducted to demonstrate its state-of-the-art recognition accuracy of our proposed method. Meanwhile, a special dataset of gestures, BSG, is built using SoftKinetic DS325 for the test of OSLHGR, and a series of test results verify and validate its well classification performance and real-time response speed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00138-019-01043-7,springer
Article,doi:10.1007/s00034-019-01161-7,Guest Editorial: Algorithms and Architectures for Machine Learning Based Speech Processing,"Circuits, Systems, and Signal Processing",10.1007/s00034-019-01161-7,Springer,2019-08-15,2019-06-26,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00034-019-01161-7,springer
Article,doi:10.1186/s40537-019-0212-5,Intelligent video surveillance: a review through deep learning techniques for crowd analysis,Journal of Big Data,10.1186/s40537-019-0212-5,Springer,2019-06-06,2019-06-06,"Big data applications are consuming most of the space in industry and research area. Among the widespread examples of big data, the role of video streams from CCTV cameras is equally important as other sources like social media data, sensor data, agriculture data, medical data and data evolved from space research. Surveillance videos have a major contribution in unstructured big data. CCTV cameras are implemented in all places where security having much importance. Manual surveillance seems tedious and time consuming. Security can be defined in different terms in different contexts like theft identification, violence detection, chances of explosion etc. In crowded public places the term security covers almost all type of abnormal events. Among them violence detection is difficult to handle since it involves group activity. The anomalous or abnormal activity analysis in a crowd video scene is very difficult due to several real world constraints. The paper includes a deep rooted survey which starts from object recognition, action recognition, crowd analysis and finally violence detection in a crowd environment. Majority of the papers reviewed in this survey are based on deep learning technique. Various deep learning methods are compared in terms of their algorithms and models. The main focus of this survey is application of deep learning techniques in detecting the exact count, involved persons and the happened activity in a large crowd at all climate conditions. Paper discusses the underlying deep learning implementation technology involved in various crowd video analysis methods. Real time processing, an important issue which is yet to be explored more in this field is also considered. Not many methods are there in handling all these issues simultaneously. The issues recognized in existing methods are identified and summarized. Also future direction is given to reduce the obstacles identified. The survey provides a bibliographic summary of papers from ScienceDirect, IEEE Xplore and ACM digital library.",https://www.biomedcentral.com/openurl?doi=10.1186/s40537-019-0212-5,springer
Article,doi:10.1038/s41591-019-0539-7,An augmented reality microscope with real-time artificial intelligence integration for cancer diagnosis,Nature Medicine,10.1038/s41591-019-0539-7,Nature,2019-09-01,2019-08-12,"The microscopic assessment of tissue samples is instrumental for the diagnosis and staging of cancer, and thus guides therapy. However, these assessments demonstrate considerable variability and many regions of the world lack access to trained pathologists. Though artificial intelligence (AI) promises to improve the access and quality of healthcare, the costs of image digitization in pathology and difficulties in deploying AI solutions remain as barriers to real-world use. Here we propose a cost-effective solution: the augmented reality microscope (ARM). The ARM overlays AI-based information onto the current view of the sample in real time, enabling seamless integration of AI into routine workflows. We demonstrate the utility of ARM in the detection of metastatic breast cancer and the identification of prostate cancer, with latency compatible with real-time use. We anticipate that the ARM will remove barriers towards the use of AI designed to improve the accuracy and efficiency of cancer diagnosis. An artificial intelligence-powered microscope able to detect tumor cells in histopathology slides holds promise for accelerating pathology workflows for cancer diagnosis",https://www.nature.com/articles/s41591-019-0539-7,springer
Article,doi:10.1007/s12559-018-9607-4,Cognitively Inspired Feature Extraction and Speech Recognition for Automated Hearing Loss Testing,Cognitive Computation,10.1007/s12559-018-9607-4,Springer,2019-08-15,2019-02-13,"Hearing loss, a partial or total inability to hear, is one of the most commonly reported disabilities. A hearing test can be carried out by an audiologist to assess a patient’s auditory system. However, the procedure requires an appointment, which can result in delays and practitioner fees. In addition, there are often challenges associated with the unavailability of equipment and qualified practitioners, particularly in remote areas. This paper presents a novel idea that automatically identifies any hearing impairment based on a cognitively inspired feature extraction and speech recognition approach. The proposed system uses an adaptive filter bank with weighted Mel-frequency cepstral coefficients for feature extraction. The adaptive filter bank implementation is inspired by the principle of spectrum sensing in cognitive radio that is aware of its environment and adapts to statistical variations in the input stimuli by learning from the environment. Comparative performance evaluation demonstrates the potential of our automated hearing test method to achieve comparable results to the clinical ground truth, established by the expert audiologist’s tests. The overall absolute error of the proposed model when compared with the expert audiologist test is less than 4.9 dB and 4.4 dB for the pure tone and speech audiometry tests, respectively. The overall accuracy achieved is 96.67% with a hidden Markov model (HMM). The proposed method potentially offers a second opinion to audiologists, and serves as a cost-effective pre-screening test to predict hearing loss at an early stage. In future work, authors intend to explore the application of advanced deep learning and optimization approaches to further enhance the performance of the automated testing prototype considering imperfect datasets with real-world background noise.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12559-018-9607-4,springer
Article,doi:10.1007/s00779-019-01225-0,Gesture recognition algorithm based on image information fusion in virtual reality,Personal and Ubiquitous Computing,10.1007/s00779-019-01225-0,Springer,2019-07-17,2019-05-02,"Combining image information fusion theory with machine learning for biometric recognition is an important field in computer vision research in recent years. Based on this, a gesture recognition algorithm based on image information fusion in virtual reality is proposed. Firstly, it introduces the basic concepts and principles of virtual reality and information fusion technology, analyzes the characteristics and basic components of virtual environment system, points out the relationship between human and virtual environment and the impact of virtual environment on people, and gives a virtual reality. Then, the multi-sensor information fusion model of the virtual environment for gesture recognition is proposed. The membership degree and template matching algorithm are further selected for data correlation and gesture recognition in the fusion model. Finally, the design comparison experiment verifies the proposed method. The results show that the proposed multi-sensor information fusion model in the interactive virtual environment achieves the highest recognition success rate of 96.17% and is better than several comparison machine learning methods in recognition time.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00779-019-01225-0,springer
Article,doi:10.1631/FITEE.1800469,Binary neural networks for speech recognition,Frontiers of Information Technology & Electronic Engineering,10.1631/FITEE.1800469,Springer,2019-05-01,2019-06-18,"Recently, deep neural networks (DNNs) significantly outperform Gaussian mixture models in acoustic modeling for speech recognition. However, the substantial increase in computational load during the inference stage makes deep models difficult to directly deploy on low-power embedded devices. To alleviate this issue, structure sparseness and low precision fixed-point quantization have been applied widely. In this work, binary neural networks for speech recognition are developed to reduce the computational cost during the inference stage. A fast implementation of binary matrix multiplication is introduced. On modern central processing unit (CPU) and graphics processing unit (GPU) architectures, a 5–7 times speedup compared with full precision floatingpoint matrix multiplication can be achieved in real applications. Several kinds of binary neural networks and related model optimization algorithms are developed for large vocabulary continuous speech recognition acoustic modeling. In addition, to improve the accuracy of binary models, knowledge distillation from the normal full precision floating-point model to the compressed binary model is explored. Experiments on the standard Switchboard speech recognition task show that the proposed binary neural networks can deliver 3–4 times speedup over the normal full precision deep models. With the knowledge distillation from the normal floating-point models, the binary DNNs or binary convolutional neural networks (CNNs) can restrict the word error rate (WER) degradation to within 15.0%, compared to the normal full precision floating-point DNNs or CNNs, respectively. Particularly for the binary CNN with binarization only on the convolutional layers, the WER degradation is very small and is almost negligible with the proposed approach.",http://link.springer.com/openurl/pdf?id=doi:10.1631/FITEE.1800469,springer
Article,doi:10.1007/s10553-019-01055-z,Depth Image Super Resolution for 3D Reconstruction of Oil Reflnery Buildings,Chemistry and Technology of Fuels and Oils,10.1007/s10553-019-01055-z,Springer,2019-09-01,2019-11-07,"Time-of-Flight (ToF) camera can collect the depth data of dynamic scene surface in real time, which has been applied to 3D reconstruction of refinery buildings. However; due to the limitations of sensor hardware, the resolution of the depth image obtained is very low, so it cannot meet the requirements of dense depth of scene in practical applications such as 3D reconstruction. Therefore, it is necessary to make a breakthrough in software and design a good algorithm to improve the resolution of depth image. We propose of an algorithm of depth image super-resolution by using fusion of multiple progressive convolution neural networks, which uses a context-based network fusion framework to fuse multiple different progressive networks, so as to improve individual network performance and efficiency while maintaining the simplicity of network training. Finally, we have carried out experiments on the public data set, and the experimental results show that the proposed algorithm has reached or even exceeded the most advanced algorithms at present.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10553-019-01055-z,springer
Article,doi:10.1007/s00779-019-01296-z,Research of multi-object detection and tracking using machine learning based on knowledge for video surveillance system,Personal and Ubiquitous Computing,10.1007/s00779-019-01296-z,Springer,2019-08-28,2019-08-28,"Recently, as the risk of crime and accidents increases, interest in security and surveillance of individuals and the public is increasing rapidly, and video surveillance system technology is continuously developing. Reliable object detection in the system is the basis of all elements using image information and it is used in various applications using the information, so accurate object detection and tracking are needed. Therefore, we propose a system for analyzing images with a knowledge-based deep learning technology for multi-object recognition and tracking enhancement. Algorithms for recognizing objects using existing convolution neural network (CNN) classifiers have a problem that it is difficult to process in real time because the processing time is increased when there are a lot of objects to be classified in the image. Therefore, we propose an algorithm that combines optical flow while maintaining the recognition performance through a knowledge-based CNN. An optical flow-based tracker can forecast the position of objects in the next frame based on the position of objects in the current frame. A CNN-based detector can detect the position of objects through a knowledge-based mining method between the two images. CNN-based detectors also carry out mining method on current frame information. This detector can select more capacity features based on the background to more accurately forecast the location of the tracked targets and targets. The fusion of the tracker and detector compensates for accumulated errors that can occur in the tracker and for drift from the detector. The experimental results show that the proposed algorithm combining CNN and optical flow can detect and trace multiple objects in a video stream, and can carry out robust detection and tracing even in a complex environment.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00779-019-01296-z,springer
Article,doi:10.1007/s00034-019-01141-x,Automatic Hypernasality Detection in Cleft Palate Speech Using CNN,"Circuits, Systems, and Signal Processing",10.1007/s00034-019-01141-x,Springer,2019-08-15,2019-05-20,"Automatic hypernasality detection in cleft palate speech can facilitate diagnosis by speech-language pathologists. This paper describes a feature-independent end-to-end algorithm that uses a convolutional neural network (CNN) to detect hypernasality in cleft palate speech. A speech spectrogram is adopted as the input. The average F1-scores for the hypernasality detection task are 0.9485 and 0.9746 using a dataset that is spoken by children and a dataset that is spoken by adults, respectively. The experiments explore the influence of the spectral resolution on the hypernasality detection performance in cleft palate speech. Higher spectral resolution can highlight the vocal tract parameters of hypernasality, such as formants and spectral zeros. The CNN learns efficient features via a two-dimensional filtering operation, while the feature extraction performance of shallow classifiers is limited. Compared with deep neural network and shallow classifiers, CNN realizes the highest F1-score of 0.9485. Comparing various network architectures, the convolutional filter of size 1 × 8 achieves the highest F1-score in the hypernasality detection task. The selected filter size of 1 × 8 considers more frequency information and is more suitable for hypernasality detection than the filters of size 3 × 3, 4 × 4, 5 × 5, and 6 × 6. According to an analysis of hypernasality-sensitive vowels, the experimental result concludes that the vowel /i/ is the most sensitive vowel to hypernasality. Compared with state-of-the-art literature, the proposed CNN-based system realizes a better detection performance. The results of an experiment that is conducted on a heterogeneous corpus demonstrate that CNN can better handle the speech variability compared with the shallow classifiers.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00034-019-01141-x,springer
Article,doi:10.1007/s10921-019-0601-x,Real-Time Video Surveillance Based Structural Health Monitoring of Civil Structures Using Artificial Neural Network,Journal of Nondestructive Evaluation,10.1007/s10921-019-0601-x,Springer,2019-07-01,2019-07-01,"Modern world’s incessantly increasing outdoor traffic load has eventually led to structural health concern and continuous health monitoring of large scale civil structures such as bridges, roads, highways, etc. In this paper, we propose a computer vision based non-destructive structural health monitoring (SHM) method using high speed camera system combined with the brilliance of artificial intelligence. A number of appreciable SHM techniques had been reported that utilizes wired or wireless smart sensors, but the use of nondestructive techniques, such as, digital high speed imaging were rarely employed for detection of dynamic vibrations of civil structures. In the current research, we have developed a high speed video imaging based structural health monitoring system that utilizes blob detection based motion tracking algorithm. It provides factual information regarding localization and displacement of the target object or an existing feature in the civil structure. The modal parameters were subsequently extracted to analyze the level of severity of structural damage within the civil structures. Also, an artificial neural network is trained to infer the qualitative characteristics of structural vibrations based on vibration intensity and the network inferences can be correlated with the conditions of the structure. The efficacy of our vision system in remote measurement of dynamic displacements was demonstrated through a shaking table and a slip desk experiment. The experimental results demonstrate real-time output with satisfactory performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10921-019-0601-x,springer
Article,doi:10.1007/s40436-019-00254-5,Machine auscultation: enabling machine diagnostics using convolutional neural networks and large-scale machine audio data,Advances in Manufacturing,10.1007/s40436-019-00254-5,Springer,2019-06-06,2019-05-21,"Acoustic signals play an essential role in machine state monitoring. Efficient processing of real-time machine acoustic signals improves production quality. However, generating semantically useful information from sound signals is an ill-defined problem that exhibits a highly non-linear relationship between sound and subjective perceptions. This paper outlines two neural network models to analyze and classify acoustic signals emanating from machines: (i) a backpropagation neural network (BP-NN); and (ii) a convolutional neural network (CNN). Microphones are used to collect acoustic data for training models from a computer numeric control (CNC) lathe. Numerical experiments demonstrate that CNN performs better than the BP-NN.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40436-019-00254-5,springer
Article,doi:10.1007/s11548-019-01960-y,Catheter localization in 3D ultrasound using voxel-of-interest-based ConvNets for cardiac intervention,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-019-01960-y,Springer,2019-06-01,2019-04-09,"Purpose Efficient image-based catheter localization in 3D US during cardiac interventions is highly desired, since it facilitates the operation procedure, reduces the patient risk and improves the outcome. Current image-based catheter localization methods are not efficient or accurate enough for real clinical use. Methods We propose a catheter localization method for 3D cardiac ultrasound (US). The catheter candidate voxels are first pre-selected by the Frangi vesselness filter with adaptive thresholding, after which a triplanar-based ConvNet is applied to classify the remaining voxels as catheter or not. We propose a Share-ConvNet for 3D US, which reduces the computation complexity by sharing a single ConvNet for all orthogonal slices. To boost the performance of ConvNet, we also employ two-stage training with weighted cross-entropy. Using the classified voxels, the catheter is localized by a model fitting algorithm. Results To validate our method, we have collected challenging ex vivo datasets. Extensive experiments show that the proposed method outperforms state-of-the-art methods and can localize the catheter with an average error of 2.1 mm in around 10 s per volume. Conclusion Our method can automatically localize the cardiac catheter in challenging 3D cardiac US images. The efficiency and accuracy localization of the proposed method are considered promising for catheter detection and localization during clinical interventions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-019-01960-y,springer
Article,doi:10.1007/s10772-018-09579-1,Enhancement of esophageal speech obtained by a voice conversion technique using time dilated Fourier cepstra,International Journal of Speech Technology,10.1007/s10772-018-09579-1,Springer,2019-03-15,2018-12-12,"This paper presents a novel speaking-aid system for enhancing esophageal speech (ES). The method adopted in this paper aims to improve the quality of esophageal speech using a combination of a voice conversion technique and a time dilation algorithm. In the proposed system, a Deep Neural Network (DNN) is used as a nonlinear mapping function for vocal tract vector transformation. Then the converted frames are used to determine realistic excitation and phase vectors from the target training space using a frame selection algorithm. Next, in order to preserve speaker identity of the esophageal speakers, we use the source vocal tract features and propose to apply on them a time dilation algorithm to reduce the unpleasant esophageal noises. Finally the converted speech is reconstructed using the dilated source vocal tract frames and the predicted excitation and phase. DNN and Gaussian mixture model (GMM) based voice conversion systems have been evaluated using objective and subjective measures. Such an experimental study has been realized also in order to evaluate the changes in speech quality and intelligibility of the transformed signals. Experimental results demonstrate that the proposed methods provide considerable improvement in intelligibility and naturalness of the converted esophageal speech.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-018-09579-1,springer
Chapter ConferencePaper,doi:10.1007/978-3-319-99353-9_30,Recognition of Assembly Parts by Convolutional Neural Networks,Advances in Manufacturing Engineering and Materials,10.1007/978-3-319-99353-9_30,Springer,2019-01-01,2018-09-15,"The paper describes the experiments with the use of deep neural networks (CNN) for robust identification of assembly parts (screws, nuts) and assembly features (holes), to speed up any assembly process with augmented reality application. The simple image processing tasks with static camera and recognized parts can be handled by standard image processing algorithms (threshold, Hough line/circle detection and contour detection), but the augmented reality devices require dynamic recognition of features detected in various distances and angles. The problem can be solved by deep learning CNN which is robust to orientation, scale and in cases when element is not fully visible. We tested two pretrained CNN models Mobilenet V1 and SSD Fast RCNN Inception V2 SSD extension have been tested to detect exact position. The results obtained were very promising in comparison to standard image processing techniques.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-319-99353-9_30,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-34885-4_26,An Investigation of the Accuracy of Real Time Speech Emotion Recognition,Artificial Intelligence XXXVI,10.1007/978-3-030-34885-4_26,Springer,2019-01-01,2019-11-19,"This paper presents an investigation of speech emotion systems and how the accuracy can be further improved by exploring machine learning algorithms and hybrid solutions. The accuracy of machine learning algorithms and speech noise reduction techniques are investigated on an embedded system. Research suggests improvements could be made to the feature selection from speech signals and pattern recognition algorithms for emotion recognition. The system deployed to perform the experiments is EmotionPi, using the Raspberry Pi 3 B+. Pattern recognition is investigated by using K-Nearest Neighbour (K-NN), Support Vector Machine (SVM), Random Forest Classifier (RFC), Multi-Layer Perception (MLP) and Convolutional Neural Networks (CNN) algorithms. Experiments are conducted to determine the accuracy of the speech emotion system using the speech database and our own recorded dataset. We propose a hybrid solution which has proven to increase the accuracy of the emotion recognition results. Results obtained from testing, show the system needs to be trained using real cases rather than using speech databases (as it is more accurate in detecting the user’s emotion).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-34885-4_26,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-05198-3_13,Awale Game: Application Programming Interface and Augmented Reality Interface,Emerging Technologies for Developing Countries,10.1007/978-3-030-05198-3_13,Springer,2019-01-01,2018-12-14,"Awale game is one of the famous board games from Africa with many variants and is now played worldwide in various forms. In this paper, we propose an open-source Application Programming Interface ( API ) for developers to allow an easy implementation of the various variants of Awale as well as artificial intelligence based players. The API is available online at https://github.com/Machine-Intelligence-For-You/Awale . Based on this API , we propose a PC Awale game, a mobile Awale game, and an Augmented Reality Game. The Awale API , PC game, and mobile game are implemented in the programming language Java while the game in Augmented Reality is realized with the C# programming language, Unity 3D game engine and the Vuforia Augmented Reality SDK . The various tests carried out show that the API and the different games are totally functional. This API was also used for the first edition of MAIC, an Artificial Intelligence contest https://mify-ai.com/maic2017/ .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-05198-3_13,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-33617-2_3,Multimodal Web Based Video Annotator with Real-Time Human Pose Estimation,Intelligent Data Engineering and Automated Learning – IDEAL 2019,10.1007/978-3-030-33617-2_3,Springer,2019-01-01,2019-10-18,"This paper presents a multi-platform Web-based video annotator to support multimodal annotation that can be applied to several working areas, such as dance rehearsals, among others. The CultureMoves’ “Motion-Notes” Annotator was designed to assist the creative and exploratory processes of both professional and amateur users, working with a digital device for personal annotations. This prototype is being developed for any device capable of running in a modern Web browser. It is a real-time multimodal video annotator based on keyboard, touch and voice inputs. Five different ways of adding annotations have been already implemented: voice, draw, text, web URL, and mark annotations. Pose estimation functionality uses machine learning techniques to identify a person skeleton in the video frames, which gives the user another resource to identify possible annotations.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-33617-2_3,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-30490-4_30,Neuro-Spectral Audio Synthesis: Exploiting Characteristics of the Discrete Fourier Transform in the Real-Time Simulation of Musical Instruments Using Parallel Neural Networks,Artificial Neural Networks and Machine Learning – ICANN 2019: Text and Time Series,10.1007/978-3-030-30490-4_30,Springer,2019-01-01,2019-09-09,"Two main approaches are currently prevalent in the digital emulation of musical instruments: manipulation of pre-recorded samples and techniques of real-time synthesis, generally based on physical models with varying degrees of accuracy. Concerning the first, while the processing power of present-day computers enables their use in real-time, many restrictions arising from this sample-based design persist; the huge on disk space requirements and the stiffness of musical articulations being the most prominent. On the other side of the spectrum, pure synthesis approaches, while offering greater flexibility, fail to capture and reproduce certain nuances central to the verisimilitude of the generated sound, offering a dry, synthetic output, at a high computational cost. We propose a method where ensembles of lightweight neural networks working in parallel are learned, from crafted frequency-domain features of an instrument sound spectra, an arbitrary instrument’s voice and articulations realistically and efficiently. We find that our method, while retaining perceptual sound quality on par with sampled approaches, exhibits 1/10 of latency times of industry standard real-time synthesis algorithms, and 1/100 of the disk space requirements of industry standard sample-based digital musical instruments. This method can, therefore, serve as a basis for more efficient implementations in dedicated devices, such as keyboards and electronic drumkits and in general purpose platforms, like desktops and tablets or open-source hardware like Arduino and Raspberry Pi. From a conceptual point of view, this work highlights the advantages of a closer integration of machine learning with other subjects, especially in the endeavor of new product development. Exploiting the synergy between neural networks, digital signal processing techniques and physical modelling, we illustrate the proposed method via the implementation of two virtual instruments: a conventional grand piano and a hibrid stringed instrument.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-30490-4_30,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-29904-0_10,Video Intelligence as a Component of a Global Security System,Artificial Intelligence for Knowledge Management,10.1007/978-3-030-29904-0_10,Springer,2019-01-01,2019-09-12,"This paper describes the evolution of our research from video analytics to a global security system with focus on the video surveillance component. Indeed video surveillance has evolved from a commodity security tool up to the most efficient way of tracking perpetrators when terrorism hits our modern urban centers. As number of cameras soars, one could expect the system to leverage the huge amount of data carried through the video streams to provide fast access to video evidences, actionable intelligence for monitoring real-time events and enabling predictive capacities to assist operators in their surveillance tasks. This research explores a hybrid platform for video intelligence capture, automated data extraction, supervised Machine Learning for intelligently assisted urban video surveillance; Extension to other components of a global security system are discussed. Applying Knowledge Management principles in this research helps with deep problem understanding and facilitates the implementation of efficient information and experience sharing decision support systems providing assistance to people on the field as well as in operations centers. The originality of this work is also the creation of “common” human-machine and machine to machine language and a security ontology.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-29904-0_10,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-27538-9_46,3D Pose Estimation of Robot Arm with RGB Images Based on Deep Learning,Intelligent Robotics and Applications,10.1007/978-3-030-27538-9_46,Springer,2019-01-01,2019-08-03,"In the field of human-robot interaction, robot collision avoidance with the human in a shared workspace remains a challenge. Many researchers use visual methods to detect the collision between robots and obstacles on the assumption that the robot pose is known because the information about the robot is obtained from the controller and hand-eye calibration is conducted. Therefore, they focus on the motion prediction of obstacles. In this paper, a real-time method based on deep learning is proposed to directly estimate the 3D pose of the robot arm using a color image. The method aims to remove the hand-eye calibration when the system needs to be reconfigured and increase the flexibility of the system by eliminating the requirement that the camera fixed relative to the robot. Our approach has two main contributions. One is that the method estimates the 3D position of the robot base and the relative 3D positions of the predefined key points of the robot to the robot base separately different from other deep learning methods considering the limitations of the dataset. The other is that some datasets are collected through another trained network to avoid tedious calibration process, and the trained network will be reused in the pose estimation task. Finally, the experiments are conducted. The results show that a fully trained system provides an accurate 3D pose estimation for the robot arm in the camera coordinate system. The average errors of the 3D positions of the robot base and the predefined key points are 2.35 cm and 1.99 cm respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-27538-9_46,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-33723-0_9,3D Object Recognition with Ensemble Learning—A Study of Point Cloud-Based Deep Learning Models,Advances in Visual Computing,10.1007/978-3-030-33723-0_9,Springer,2019-01-01,2019-10-21,"In this study, we present an analysis of model-based ensemble learning for 3D point-cloud object classification. An ensemble of multiple model instances is known to outperform a single model instance, but there is little study of the topic of ensemble learning for 3D point clouds. First, an ensemble of multiple model instances trained on the same part of the ModelNet40 dataset was tested for seven deep learning, point cloud-based classification algorithms: PointNet , PointNet++ , SO-Net , KCNet , DeepSets , DGCNN , and PointCNN . Second, the ensemble of different architectures was tested. Results of our experiments show that the tested ensemble learning methods improve over state-of-the-art on the ModelNet40 dataset, from 92.65% to 93.64% for the ensemble of single architecture instances, 94.03% for two different architectures, and 94.15% for five different architectures. We show that the ensemble of two models with different architectures can be as effective as the ensemble of 10 models with the same architecture. Third, a study on classic bagging ( i.e. with different subsets used for training multiple model instances ) was tested and sources of ensemble accuracy growth were investigated for best-performing architecture, i.e. SO-Net . We measure the inference time of all 3D classification architectures on a Nvidia Jetson TX2 , a common embedded computer for mobile robots, to allude to the use of these models in real-life applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-33723-0_9,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-11012-3_45,Learning Spatiotemporal 3D Convolution with Video Order Self-supervision,Computer Vision – ECCV 2018 Workshops,10.1007/978-3-030-11012-3_45,Springer,2019-01-01,2019-01-29,"The purpose of this work is to explore self-supervised learning (SSL) strategy to capture a better feature with spatiotemporal 3D convolution. Although one of the next frontier in video recognition must be spatiotemporal 3D CNN, the convergence of the 3D convolutions is really difficult because of their enormous parameters or missing temporal(motion) feature. One of the effective solutions is to collect a $$10^5$$ 10 5 -order video database such as Kinetics/Moments in Time. However, this is not an efficient with burden of manual annotations. In the paper, we train 3D CNN on wrong video-sequence detection tasks in a self-supervised manner (without any manual annotation). The shuffling and verification of consecutive video-frame-order is effective for 3D CNN to capture temporal feature and get a good start point of parameters to be fine-tuned. In the experimental section, we verify that our pretrained 3D CNN on wrong clip detection improves the level of performance on UCF101 ( $$+3.99\%$$ + 3.99 % better than baseline, namely training 3D convolution from scratch).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-11012-3_45,springer
Chapter ConferencePaper,doi:10.1007/978-981-13-2517-5_57,Predicting Knee Angles from Video: An Initial Experiment with Machine Learning,XXVI Brazilian Congress on Biomedical Engineering,10.1007/978-981-13-2517-5_57,Springer,2019-01-01,2019-05-16,"Machine Learning (ML) has drawn a lot of attention these days due to its capability to automate processes that where very complicated and/or only performed by humans before. It is done by not having the need to write hard coded rules to solve problems, letting the machine find the rules by itself, and for being able to universally approximate functions with certain algorithms. On the other side, motion capture systems are quite expensive, making it more difficult to health professionals and physicians to have a more precise way to analyze the gait of patients and the diseases related to it. Having that said, this paper aimed to show that is possible to predict (generate) the angles of the right knee of patients directly from a common video of their walks, reducing costs and opening the opportunity for a more complete and affordable system to realize motion capture. Thus, this study is an initial experiment and the first step towards a possible future more complete product. As an initial study, the data of three patients were acquired and the resulting model, a Multilayer Perceptron Artificial Neural Network (MLP), acquainted a score of 92.2% when predicting the angles of the right knee for unknown data.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-13-2517-5_57,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-22971-9_26,MCU-Based Isolated Appealing Words Detecting Method with AI Techniques,Artificial Intelligence for Communications and Networks,10.1007/978-3-030-22971-9_26,Springer,2019-01-01,2019-07-05,"Bullying in campus has attracted more and more attention in recent years. By analyzing typical campus bullying events, it can be found that the victims often use the words “help” and some other appealing or begging words, that is to say, by using the artificial intelligence of speech recognition, we can find the occurrence of campus bullying events in time, and take measures to avoid further harm. The main purpose of this study is to help the guardians discover the occurrence of campus bullying in time by real-time monitoring of the keywords of campus bullying, and take corresponding measures in the first time to minimize the harm of campus bullying. On the basis of Sunplus MCU and speech recognition technology, by using the MFCC acoustic features and an efficient DTW classifier, we were able to realize the detection of common vocabulary of campus bullying for the specific human voice. After repeated experiments, and finally combining the voice signal processing functions of Sunplus MCU, the recognition procedure of specific isolated words was completed. On the basis of realizing the isolated word detection of specific human voice, we got an average accuracy of 99% of appealing words for the dedicated speaker and the misrecognition rate of other words and other speakers was very low.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-22971-9_26,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-11018-5_25,Training Compact Deep Learning Models for Video Classification Using Circulant Matrices,Computer Vision – ECCV 2018 Workshops,10.1007/978-3-030-11018-5_25,Springer,2019-01-01,2019-01-23,"In real world scenarios, model accuracy is hardly the only factor to consider. Large models consume more memory and are computationally more intensive, which make them difficult to train and to deploy, especially on mobile devices. In this paper, we build on recent results at the crossroads of Linear Algebra and Deep Learning which demonstrate how imposing a structure on large weight matrices can be used to reduce the size of the model. Building on these results, we propose very compact models for video classification based on state-of-the-art network architectures such as Deep Bag-of-Frames , NetVLAD and NetFisherVectors . We then conduct thorough experiments using the large YouTube-8M video classification dataset. As we will show, the circulant DBoF embedding achieves an excellent trade-off between size and accuracy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-11018-5_25,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-05710-7_31,Detecting Tampered Videos with Multimedia Forensics and Deep Learning,MultiMedia Modeling,10.1007/978-3-030-05710-7_31,Springer,2019-01-01,2018-12-08,"User-Generated Content (UGC) has become an integral part of the news reporting cycle. As a result, the need to verify videos collected from social media and Web sources is becoming increasingly important for news organisations. While video verification is attracting a lot of attention, there has been limited effort so far in applying video forensics to real-world data. In this work we present an approach for automatic video manipulation detection inspired by manual verification approaches. In a typical manual verification setting, video filter outputs are visually interpreted by human experts. We use two such forensics filters designed for manual verification, one based on Discrete Cosine Transform (DCT) coefficients and a second based on video requantization errors, and combine them with Deep Convolutional Neural Networks (CNN) designed for image classification. We compare the performance of the proposed approach to other works from the state of the art, and discover that, while competing approaches perform better when trained with videos from the same dataset, one of the proposed filters demonstrates superior performance in cross-dataset settings. We discuss the implications of our work and the limitations of the current experimental setup, and propose directions for future research in this area.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-05710-7_31,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-05297-3_6,Automating Engineering Educational Practical Electronics Laboratories for Designing Engaging Learning Experiences,Human Work Interaction Design. Designing Engaging Automation,10.1007/978-3-030-05297-3_6,Springer,2019-01-01,2019-01-01,"This paper presents a work on understanding the effect of automated systems on learning experiences of students in practical electronics laboratory sessions. Here automation refers to the ability to provide students with contextualized information and instructions to rectify mistakes made while conducting practical experiment. A system employing mobile augmented reality (AR) and a debugging tool to assist students with physical circuit prototyping was developed. The AR provides active visualization to students regarding practical experiment. The debugger tool senses errors made while prototyping of electronic circuits on breadboard. The proposed system, named Smart Learning System, has shown to improve students’ engagement in practical laboratory sessions and improve laboratory dynamics by reducing the workload of instructors.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-05297-3_6,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-34995-0_53,A Deep Network for Automatic Video-Based Food Bite Detection,Computer Vision Systems,10.1007/978-3-030-34995-0_53,Springer,2019-01-01,2019-11-23,"Past research has now provided compelling evidence pointing towards correlations among individual eating styles and the development of (un)healthy eating patterns, obesity and other medical conditions. In this setting, an automatic, non-invasive food bite detection system can be a really useful tool in the hands of nutritionists, dietary experts and medical doctors in order to explore real-life eating behaviors and dietary habits. Unfortunately, the automatic detection of food bites can be challenging due to occlusions between hands and mouth, use of different kitchen utensils and personalized eating habits. On the other hand, although accurate, manual bite detection is time-consuming for the annotator, making it infeasible for large scale experimental deployments or real-life settings. To this regard, we propose a novel deep learning methodology that relies solely on human body and face motion data extracted from videos depicting people eating meals. The purpose is to develop a system that can accurately, robustly and automatically identify food bite instances, with the long-term goal to complement or even replace manual bite-annotation protocols currently in use. The experimental results on a large dataset reveal the superb classification performance of the proposed methodology on the task of bite detection and paves the way for additional research on automatic bite detection systems.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-34995-0_53,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-24299-2_21,Robust Detection of Outdoor Urban Advertising Panels in Static Images,Highlights of Practical Applications of Survivable Agents and Multi-Agent Systems. The PAAMS Collection,10.1007/978-3-030-24299-2_21,Springer,2019-01-01,2019-06-22,"One interesting publicity application for Smart City environments is recognizing brand information contained in urban advertising panels. For such a purpose, a previous stage is to accurately detect and locate the position of these panels in images . This work presents an effective solution to this problem using a Single Shot Detector (SSD) based on a deep neural network architecture that minimizes the number of false detections under multiple variable conditions regarding the panels and the scene. Achieved experimental results using the Intersection over Union (IoU) accuracy metric make this proposal applicable in real complex urban images.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-24299-2_21,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-05716-9_17,Realtime Human Segmentation in Video,MultiMedia Modeling,10.1007/978-3-030-05716-9_17,Springer,2019-01-01,2018-12-11,"Human segmentation from a single image using deep learning models has obtained significant performance improvements. However, when directly adopting a deep human segmentation model on video human segmentation, the performance is unsatisfactory due to some issues, e.g., the segmentation results of video frames are discontinuous, and the speed of segmentation process is slow. To address these issues, we propose a new real-time video-based human segmentation framework which is designed for the single person from videos to produces smoothing, accurate and fast human segmentation results. The proposed framework for video human segmentation consists of a fully convolutional network and a tracking module based on a level set algorithm, where the fully convolutional network segments the human part in the first frame of the video sequence, and the tracking module obtains the segmentation results of other frames using the segmentation result of the last frame as the initial segmentation. The fully convolutional network is trained using human images datasets. To evaluate the proposed framework for video human segmentation, we have created and annotated a new single person video dataset. The experimental results demonstrate very accurate and smoothing human segmentation with very higher speed only using a deep human segmentation model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-05716-9_17,springer
Chapter ConferencePaper,doi:10.1007/978-3-030-34120-6_47,Densenet-Based Multi-scale Recurrent Network for Video Restoration with Gaussian Blur,Image and Graphics,10.1007/978-3-030-34120-6_47,Springer,2019-01-01,2019-11-28,"Video cameras are now commonplace and available, and videos can be obtained almost everywhere at anytime. However, due to turbulence or thermal effects of air, blurring occurs during image acquisition. Removing these artifacts from the blurry recordings is a highly ill-posed problem as neither the sharp image nor the blur kernel is known. Propagating information between multiple consecutive blurry observations can help restore the desired sharp video. In this work, we propose an efficient approach to produce a significant amount of realistic training data and introduce a novel multi-scale recurrent network architecture to deblur frames taking temporal information into account. The experimental results demonstrate the effectiveness of the proposed method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-34120-6_47,springer
Article,doi:10.1007/s11554-013-0331-8,Fast image recognition based on n-tuple neural networks implemented in an FPGA,Journal of Real-Time Image Processing,10.1007/s11554-013-0331-8,Springer,2016-01-01,2013-02-26,This paper deals with the design and the implementation of an image recognition system based on FPGA devices. It explores an n -tuple methodology using node ‘grouping’ and the possible advantages offered by this little-known technique. The paper is based on the implementation of this concept by an FPGA device. A novel approach to the organization of the neural networks data in the n -tuple memory is introduced. The system was tested on a real-world recognition task—the recognition of road signs. The test results are presented and discussed. It is concluded that the designed system may be a powerful part of more complex equipment for the solution of many recognition issues.,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-013-0331-8,springer
Chapter ConferencePaper,doi:10.1007/978-3-319-43955-6_12,Human-Machine Speech-Based Interfaces with Augmented Reality and Interactive Systems for Controlling Mobile Cranes,Interactive Collaborative Robotics,10.1007/978-3-319-43955-6_12,Springer,2016-01-01,2016-08-14,"In this paper, an overview of human-machine interactive communication for controlling lifting devices is presented, covering also the integration with vision and sensorial systems. Following a general concept, and motivation towards intelligent human-machine communication through artificial neural networks, selected methods are proposed, which provide further directions both of recent as well as of future research on human-machine interaction. The aim of the experimental research is to design a prototype of an innovative interaction system, equipped with a speech interface in a natural language, augmented reality and interactive manipulators with force feedback.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-319-43955-6_12,springer
Chapter ConferencePaper,doi:10.1007/978-3-319-44781-0_62,Intelligent Speech-Based Interactive Communication Between Mobile Cranes and Their Human Operators,Artificial Neural Networks and Machine Learning – ICANN 2016,10.1007/978-3-319-44781-0_62,Springer,2016-01-01,2016-08-13,"In this paper, an overview of human-machine interactive communication for controlling lifting devices is presented, covering also the integration with vision and sensorial systems. Following a general concept, and motivation towards intelligent human-machine communication through artificial neural networks, selected methods are proposed, which provide further directions both of recent as well as of future research on human-machine interaction. The aim of the experimental research is to design a prototype of an innovative interaction system, equipped with a speech interface in a natural language, augmented reality and interactive manipulators with force feedback. The presented research offers the possibility of motivating and inspiring further development of the intelligent speech interaction system and methods that have been elaborated in this paper.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-319-44781-0_62,springer
Chapter,doi:10.1007/978-3-319-28047-9_6,Where Speech Recognition Is Going: Conclusion and Future Scope,"Emotion, Affect and Personality in Speech",10.1007/978-3-319-28047-9_6,Springer,2016-01-01,2015-12-23,"Today, voice and natural language processing are at the forefront of any human machine interaction environment. The chapter emphasizes the tremendous progress that has taken place in machine learning, statistical data-mining and pattern recognition approaches that can help in making speech interfaces more versatile and pervasive. The growing requirements of speech interfaces also warn against the impediments that may come in the way of successful implementation of acoustically robust natural interfaces. Finally, the chapter underlines the technical advances and research efforts to be undertaken for high performance real-time speech recognition that will completely change the way humans interact with their computing devices.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-319-28047-9_6,springer
Article,doi:10.1186/1471-2202-14-S1-P210,Learning speech recognition from songbirds,BMC Neuroscience,10.1186/1471-2202-14-S1-P210,BioMed Central,2013-07-08,2013-07-08,,https://www.biomedcentral.com/openurl?doi=10.1186/1471-2202-14-S1-P210,springer
Article,doi:10.1186/1471-2202-14-S1-P158,A novel 3D visualization tool for large-scale neural networks,BMC Neuroscience,10.1186/1471-2202-14-S1-P158,BioMed Central,2013-07-08,2013-07-08,,https://www.biomedcentral.com/openurl?doi=10.1186/1471-2202-14-S1-P158,springer
Article,doi:10.1007/s11390-013-1349-x,Manifold Constrained Transfer of Facial Geometric Knowledge for 3D Caricature Reconstruction,Journal of Computer Science and Technology,10.1007/s11390-013-1349-x,Springer,2013-05-01,2013-05-01,"3D caricatures are important attractive elements of the interface in virtual environment such as online game. However, very limited 3D caricatures exist in the real world. Meanwhile, creating 3D caricatures manually is rather costly, and even professional skills are needed. This paper proposes a novel and effective manifold transfer algorithm to reconstruct 3D caricatures according to their original 2D caricatures. We first manually create a small dataset with only 100 3D caricature models and use them to initialize the whole 3D dataset. After that, manifold transfer algorithm is carried out to refine the dataset. The algorithm comprises of two steps. The first is to perform manifold alignment between 2D and 3D caricatures to get a “standard” manifold map; the second is to reconstruct all the 3D caricatures based on the manifold map. The proposed approach utilizes and transfers knowledge of 2D caricatures to the target 3D caricatures well. Comparative experiments show that the approach reconstructs 3D caricatures more effectively and the results conform more to the styles of the original 2D caricatures than the Principal Components Analysis (PCA) based method.",http://link.springer.com/openurl/pdf?id=doi:10.1007/s11390-013-1349-x,springer
Chapter ConferencePaper,doi:10.1007/978-3-319-24078-7_33,Real-Time Human Action Recognition Using CNN Over Temporal Images for Static Video Surveillance Cameras,Advances in Multimedia Information Processing -- PCM 2015,10.1007/978-3-319-24078-7_33,Springer,2015-01-01,2015-12-15,"This paper proposes a real-time human action recognition approach to static video surveillance systems. This approach predicts human actions using temporal images and convolutional neural networks (CNN). CNN is a type of deep learning model that can automatically learn features from training videos. Although the state-of-the-art methods have shown high accuracy, they consume a lot of computational resources. Another problem is that many methods assume that exact knowledge of human positions. Moreover, most of the current methods build complex handcrafted features for specific classifiers. Therefore, these kinds of methods are difficult to apply in real-world applications. In this paper, a novel CNN model based on temporal images and a hierarchical action structure is developed for real-time human action recognition. The hierarchical action structure includes three levels: action layer, motion layer, and posture layer. The top layer represents subtle actions; the bottom layer represents posture. Each layer contains one CNN, which means that this model has three CNNs working together; layers are combined to represent many different kinds of action with a large degree of freedom. The developed approach was implemented and achieved superior performance for the ICVL action dataset; the algorithm can run at around 20 frames per second.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-319-24078-7_33,springer
Article,doi:10.1007/s00138-013-0550-9,Mixture of Merged Gaussian Algorithm using RTDENN,Machine Vision and Applications,10.1007/s00138-013-0550-9,Springer,2014-07-01,2013-10-12,"Computer vision has been a widely developed research area in the last years, and it has been used for a broad range of applications, including surveillance systems. In the pursuit of an autonomous and smart motion detection system, a reliable segmentation algorithm is required. The main problems of present segmentation solutions are their high execution time and the lack of robustness against changes in the environment due to variations in lighting, shadows, occlusions or the movement of secondary objects. This paper proposes a new algorithm named Mixture of Merged Gaussian Algorithm (MMGA) that aims to achieve a substantial improvement in execution speed to enable real-time implementation, without compromising the reliability and accuracy of the segmentation. The MMGA is based on the combination of a probabilistic model for the background, similar to the Mixture of Gaussian Model (MGM), with the learning processes of Real-Time Dynamic Ellipsoidal Neural Networks (RTDENN) for the update of the model. The proposed algorithm has been tested for different videos and compared to the MGM and SDGM algorithms. Results show a reduction of 30 to 50 % in execution times. Furthermore, the segmentation is more robust against the effect of noise and adapts faster to lighting changes.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00138-013-0550-9,springer
Article,doi:10.1186/1687-4722-2014-13,Single-channel dereverberation by feature mapping using cascade neural networks for robust distant speaker identification and speech recognition,"EURASIP Journal on Audio, Speech, and Music Processing",10.1186/1687-4722-2014-13,Springer,2014-04-10,2014-04-10,"We present a feature enhancement method that uses neural networks (NNs) to map the reverberant feature in a log-melspectral domain to its corresponding anechoic feature. The mapping is done by cascade NNs trained using Cascade2 algorithm with an implementation of segment-based normalization. Experiments using speaker identification (SID) and automatic speech recognition (ASR) systems were conducted to evaluate the method. The experiments of SID system was conducted by using our own simulated and real reverberant datasets, while the CENSREC-4 evaluation framework was used as the evaluation for the ASR system. The proposed method could remarkably improve the performance of both systems by using limited stereo data and low speaker-variant data as the training data. From the evaluation using SID, we reached 26.0% and 34.8% of error rate reduction (ERR) relative to the baseline by using simulated and real data, respectively, by using only one pair of utterances for matched condition cases. Then, by using combined dataset containing 15 pairs of utterances by one speaker from three positions in a room, we could reach 93.7% of average identification rate (three known and two unknown positions), which was 42.2% of ERR relative to the use of cepstral mean normalization (CMN). From the evaluation using ASR, by using 40 pairs of utterances as the NN training data, we could reach 78.4% of ERR relative to the baseline by using simulated utterances by five speakers. Moreover, we could reach 75.4% and 71.6% of ERR relative to the baseline by using real utterances by five speakers and one speaker, respectively.",https://www.biomedcentral.com/openurl?doi=10.1186/1687-4722-2014-13,springer
Chapter ConferencePaper,doi:10.1007/978-3-319-13650-9_16,A Hierarchical Reinforcement Learning Based Artificial Intelligence for Non-Player Characters in Video Games,Nature-Inspired Computation and Machine Learning,10.1007/978-3-319-13650-9_16,Springer,2014-01-01,,"Nowadays, video games conforms a huge industry that is always developing new technology. In particular, artificial intelligence techniques have been used broadly in the well-known non-player characters (NPC) given the opportunity to users to feel video games more real. This paper proposes the usage of the MaxQ-Q hierarchical reinforcement learning algorithm in non-player characters in order to increase the experience of the user in terms of naturalness. A case study of an NPC with the proposed artificial intelligence based algorithm in a first personal shooter video game was developed. Experimental results show that this implementation improves naturalness from the user’s point of view. In addition, the proposed MaxQ-Q based algorithm in NPCs allow to programmers a robust way to give artificial intelligence to them.",http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-319-13650-9_16,springer
Chapter ConferencePaper,doi:10.1007/978-3-319-26532-2_15,Learning to Reconstruct 3D Structure from Object Motion,Neural Information Processing,10.1007/978-3-319-26532-2_15,Springer,2015-01-01,2015-11-12,"In this paper, we propose a new approach for reconstructing 3D structure from motion parallax. Instead of obtaining 3D structure from multi-view geometry or factorization, a Deep Neural Network (DNN) based method is proposed without assuming the camera model explicitly. In the proposed method, the targets are first split into connected 3D corners, and then the DNN regressor is trained to estimate the relative 3D structure of each corner from the target rotation. Finally, a temporal integration is performed to further improve the reconstruction accuracy. The effectiveness of the method is proved by a typical experiment of the Kinetic Depth Effect (KDE) in human visual system, in which the DNN regressor reconstructs the structure of a rotating 3D bent wire. The proposed method is also applied to reconstruct another two real targets. Experimental results on both synthetic and real images show that the proposed method is accurate and effective.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-319-26532-2_15,springer
Chapter ConferencePaper,doi:10.1007/978-3-319-05951-8_11,Real-Time Statistical Speech Translation,"New Perspectives in Information Systems and Technologies, Volume 1",10.1007/978-3-319-05951-8_11,Springer,2014-01-01,,"This research investigates the Statistical Machine Translation approaches to translate speech in real time automatically. Such systems can be used in a pipeline with speech recognition and synthesis software in order to produce a real-time voice communication system between foreigners. We obtained three main data sets from spoken proceedings that represent three different types of human speech. TED, Europarl, and OPUS parallel text corpora were used as the basis for training of language models, for developmental tuning and testing of the translation system. We also conducted experiments involving part of speech tagging, compound splitting, linear language model interpolation, TrueCasing and morphosyntactic analysis. We evaluated the effects of variety of data preparations on the translation results using the BLEU, NIST, METEOR and TER metrics and tried to give answer which metric is most suitable for PL-EN language pair.",http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-319-05951-8_11,springer
Chapter ConferencePaper,doi:10.1007/978-3-319-11179-7_27,Chinese Image Character Recognition Using DNN and Machine Simulated Training Samples,Artificial Neural Networks and Machine Learning – ICANN 2014,10.1007/978-3-319-11179-7_27,Springer,2014-01-01,,"Inspired by the success of deep neural network (DNN) models in solving challenging visual problems, this paper studies the task of Chinese Image Character Recognition (ChnICR) by leveraging DNN model and huge machine simulated training samples. To generate the samples, clean machine born Chinese characters are extracted and are plus with common variations of image characters such as changes in size, font, boldness, shift and complex backgrounds, which in total produces over 28 million character images, covering the vast majority of occurrences of Chinese character in real life images. Based on these samples, a DNN training procedure is employed to learn the appropriate Chinese character recognizer, where the width and depth of DNN, and the volume of samples are empirically discussed. Parallel to this, a holistic Chinese image text recognition system is developed. Encouraging experimental results on text from 13 TV channels demonstrate the effectiveness of the learned recognizer, from which significant performance gains are observed compared to the baseline system.",http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-319-11179-7_27,springer
Chapter,doi:10.1007/978-3-642-35467-0_42,Machine Learning and Soft Computing Methodologies for Music Emotion Recognition,Neural Nets and Surroundings,10.1007/978-3-642-35467-0_42,Springer,2013-01-01,"Social interaction is one of the main channels to access reality and information about people. In this last years there is a growing interest in community websites that combine social interaction with music and entertainment exploration. Music is a language of emotions and music emotional recognition has been addressed by different disciplines (psychology, cognitive science and musicology). Aim of this work is to introduce a framework for music emotion recognition based on machine learning and soft computing techniques. First, musical emotional features are extract from audio songs and successively they are elaborated for classification or clustering. One user can submit a target song, representing his conceptual emotion, and to obtain a playlist of audio songs with similar emotional content. In the case of classification, a playlist is obtained from the songs of the same class. In the other case, the playlist is suggested by system exploiting the content of the audio songs and it could also contain songs of different classes. Several experiments are proposed to show the performance of the developed framework.",,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-642-35467-0_42,springer
Article,doi:10.1007/s10470-012-9828-5,Analog image recognition arrays design by using co-fabricated MOSFET and MESFETs on a 0.25 μm SOS process,Analog Integrated Circuits and Signal Processing,10.1007/s10470-012-9828-5,Springer,2012-08-01,"This paper presents an analog image recognition system with a novel MESFET device fabricated on a fully depleted (FD) CMOS process. An analog image recognition system with a power consumption of 2.4 mW/cell and a settling time of 6.5 μs was designed, fabricated and characterized. A CNN is employed to realize a core cell of the proposed image recognition system. While a CNN benefits from its regular structure, it faces challenges due to its power consumption, speed, and size in their CMOS implementations. SOS MESFETs can deal with the challenges associated with CMOS-based CNNs. Advantages of SOS MESFETs associated with nonlinear signal processing include lower power consumption and higher operating speeds compared to similar geometry MOSFETs carrying the same current. SOS MESFET-based analog image recognition systems were fabricated and the transient response is characterized in both simulation using a TOM3 SPICE model extracted from SOS MESFETs and in experiment using image testing lab equipment. Settling times of 3.5 and 6.5 μs for one-by-four and one-by-eight arrays, respectively, were achieved with line recognition template. The corresponding power consumption for the two arrays was 9.6 and 19.2 mW, respectively.",2012-01-14,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10470-012-9828-5,springer
Chapter ConferencePaper,doi:10.1007/978-3-642-33885-4_9,Monocular Camera Fall Detection System Exploiting 3D Measures: A Semi-supervised Learning Approach,Computer Vision – ECCV 2012. Workshops and Demonstrations,10.1007/978-3-642-33885-4_9,Springer,2012-01-01,"Falls have been reported as the leading cause of injury-related visits to emergency departments and the primary etiology of accidental deaths in elderly. The system presented in this article addresses the fall detection problem through visual cues. The proposed methodology utilize a fast, real-time background subtraction algorithm based on motion information in the scene and capable to operate properly in dynamically changing visual conditions, in order to detect the foreground object and, at the same time, it exploits 3D space’s measures, through automatic camera calibration, to increase the robustness of fall detection algorithm which is based on semi-supervised learning. The above system uses a single monocular camera and is characterized by minimal computational cost and memory requirements that make it suitable for real-time large scale implementations.",,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-642-33885-4_9,springer
Chapter ConferencePaper,doi:10.1007/978-1-4471-2467-2_86,Activity Analysis in Video Scenes Using an Unsupervised Learning Approach,"Electrical, Information Engineering and Mechatronics 2011",10.1007/978-1-4471-2467-2_86,Springer,2012-01-01,"In this paper, we introduce an unsupervised learning framework for automatically detecting motion patterns in the real video scenes containing multiple groups of heterogeneous objects. First we quantize the vectors of optical flow as the visual features from video scenes, and then these visual features are clustered using hierarchical dirichlet process to discover the key motion patterns in video scenes. We test our approach on two datasets, and experimental results show that this framework can effectively detect the motion patterns in complex scenes.",2012-03-13,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-1-4471-2467-2_86,springer
Article,doi:10.1007/s00500-008-0325-9,Guest editorial: bio-inspired information hiding,Soft Computing,10.1007/s00500-008-0325-9,Springer,2009-02-01,2008-05-30,,http://link.springer.com/openurl/pdf?id=doi:10.1007/s00500-008-0325-9,springer
Chapter ConferencePaper,doi:10.1007/978-3-642-10467-1_42,A Neural Network Based Framework for Audio Scene Analysis in Audio Sensor Networks,Advances in Multimedia Information Processing - PCM 2009,10.1007/978-3-642-10467-1_42,Springer,2009-01-01,,"In recent years, the audio sensor networks have been paid much attention. One of the most important applications of audio sensor networks is audio scene analysis. In this paper, we present a neural network based framework for analyzing the audio scene in the audio sensor networks. In the proposed framework, basic audio events are modeled and detected by Hidden Markov Models (HMMs) in the audio sensor nodes. The cluster head collects the sensory information in its cluster, and then a neural network based approach is proposed to discover the high-level semantic content of the audio context. With the neural network based approach, human knowledge and machine learning are effectively combined together in the semantic inference. That is, the model parameters are learned by statistical learning and then modified manually based on the prior knowledge. We deploy the proposed framework on an audio sensor network and do a series of experiments to evaluate its performance. The experimental results show that our method can work well in the complex real-world situations.",http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-642-10467-1_42,springer
Article,doi:10.1007/s00500-010-0631-x,Learning a tensor subspace for semi-supervised dimensionality reduction,Soft Computing,10.1007/s00500-010-0631-x,Springer,2011-02-01,2010-06-20,"The high-dimensional data is frequently encountered and processed in real-world applications and unlabeled samples are readily available, but labeled or pairwise constrained ones are fairly expensive to capture. Traditionally, when a pattern itself is an n _1 ×  n _2 image, the image first has to be vectorized to the vector pattern in $$ \Re^{{n_{1} \times n_{2} }} $$ by concatenating its pixels. However, such a vector representation fails to take into account the spatial locality of pixels in the images, which are intrinsically matrices. In this paper, we propose a tensor subspace learning-based semi-supervised dimensionality reduction algorithm (TS^2DR), in which an image is naturally represented as a second-order tensor in $$ \Re^{{n_{1} }} \otimes \Re^{{n_{2} }} $$ and domain knowledge in the forms of pairwise similarity and dissimilarity constraints is used to specify whether pairs of instances belong to the same class or different classes. TS^2DR has an analytic form of the global structure preserving embedding transformation, which can be easily computed based on eigen-decomposition. We also verify the efficiency of TS^2DR by conducting unbalanced data classification experiments based on the benchmark real-word databases. Numerical results show that TS^2DR tends to capture the intrinsic structure characteristics of the given data and achieves better classification accuracy, while being much more efficient.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-010-0631-x,springer
Chapter ConferencePaper,doi:10.1007/978-3-642-24028-7_61,3D Object Modeling with Graphics Hardware Acceleration and Unsupervised Neural Networks,Advances in Visual Computing,10.1007/978-3-642-24028-7_61,Springer,2011-01-01,,"This paper presents a methodology for reaching higher performances when modeling 3D virtualized reality objects using Self-Organizing Maps (SOM) and Neural Gas Networks (NGN). Our aim is to improve the training speed of unsupervised neural networks when modeling 3D objects using a parallel implementation in a Graphic Process Unit (GPU). Experimental tests were performed over several virtualized reality objects as phantom brain tumors, archaeological items, faces and fruits. In this research, the classic SOM and NGN algorithms were adapted to the data–parallel GPU, and were compared to a similar implementation in an only–CPU platform. We present evidence that rates NGN as a better neural architecture, in quality terms, compared to SOM in the task of 3D object modeling. In order to combine the NGN accuracy with the SOM faster training, we propose and implement a hybrid neural network based on NGN using SOM as seed. Our experimental results show a considerable reduction in the training time without affecting the representation accuracy.",http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-642-24028-7_61,springer
Article,doi:10.1007/s00268-009-0319-5,Multimedia Abstract Generation of Intensive Care Data: The Automation of Clinical Processes Through AI Methodologies,World Journal of Surgery,10.1007/s00268-009-0319-5,Springer,2010-04-01,2009-12-12,"Medical errors from communication failures are enormous during the perioperative period of cardiac surgical patients. As caregivers change shifts or surgical patients change location within the hospital, key information is lost or misconstrued. After a baseline cognitive study of information need and caregiver workflow, we implemented an advanced clinical decision support tool of intelligent agents, medical logic modules, and text generators called the “Inference Engine” to summarize individual patient’s raw medical data elements into procedural milestones, illness severity, and care therapies. The system generates two displays: 1) the continuum of care, multimedia abstract generation of intensive care data (MAGIC)—an expert system that would automatically generate a physician briefing of a cardiac patient’s operative course in a multimodal format; and 2) the isolated point in time, “Inference Engine”—a system that provides a real-time, high-level, summarized depiction of a patient’s clinical status. In our studies, system accuracy and efficacy was judged against clinician performance in the workplace. To test the automated physician briefing, “MAGIC,” the patient’s intraoperative course, was reviewed in the intensive care unit before patient arrival. It was then judged against the actual physician briefing and that given in a cohort of patients where the system was not used. To test the real-time representation of the patient’s clinical status, system inferences were judged against clinician decisions. Changes in workflow and situational awareness were assessed by questionnaires and process evaluation. MAGIC provides 200% more information, twice the accuracy, and enhances situational awareness. This study demonstrates that the automation of clinical processes through AI methodologies yields positive results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00268-009-0319-5,springer
Chapter,doi:10.1007/978-3-642-15111-8_24,Comparison of Fuzzy Edge Detectors Based on the Image Recognition Rate as Performance Index Calculated with Neural Networks,Soft Computing for Recognition Based on Biometrics,10.1007/978-3-642-15111-8_24,Springer,2010-01-01,,"Edge detection is a process usually applied to image sets before the training phase in recognition systems. This preprocessing step helps to extract the most important shapes in an image, ignoring the homogeneous regions and remarking the real objective to classify or recognize. Many traditional and fuzzy edge detectors can be used, but it’s very difficult to demonstrate which one is better before the recognition results. In this work we present an experiment where several edge detectors were used to preprocess the same image sets. Each resultant image set was used as training data for neural network recognition system, and the recognition rates were compared. The goal of this experiment is to find the better edge detector that can be used as training data on a neural network for image recognition.",http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-642-15111-8_24,springer
Chapter ReferenceWorkEntry,doi:10.1007/978-0-387-78414-4_56,Neural Networks in Multimedia Content Adaptation,Encyclopedia of Multimedia,10.1007/978-0-387-78414-4_56,Springer,2008-01-01,,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-0-387-78414-4_56,springer
Chapter ReferenceWorkEntry,doi:10.1007/978-0-387-78414-4_242,Video Content Analysis Using Machine Learning Tools,Encyclopedia of Multimedia,10.1007/978-0-387-78414-4_242,Springer,2008-01-01,,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-0-387-78414-4_242,springer
Chapter,doi:10.1007/978-3-540-71078-3_2,Multimedia Content Protection Based on Chaotic Neural Networks,Computational Intelligence in Information Assurance and Security,10.1007/978-3-540-71078-3_2,Springer,2007-01-01,,,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-540-71078-3_2,springer
Chapter ConferencePaper,doi:10.1007/978-3-642-02812-0_21,Real-Time Emotional State Estimator for Adaptive Virtual Reality Stimulation,Foundations of Augmented Cognition. Neuroergonomics and Operational Neuroscience,10.1007/978-3-642-02812-0_21,Springer,2009-01-01,"The paper presents design and evaluation of emotional state estimator based on artificial neural networks for physiology-driven adaptive virtual reality (VR) stimulation. Real-time emotional state estimation from physiological signals enables adapting the stimulations to the emotional response of each individual. Estimation is first evaluated on artificial subjects, which are convenient during software development and testing of physiology-driven adaptive VR stimulation. Artificial subjects are implemented in the form of parameterized skin conductance and heart rate generators that respond to emotional inputs. Emotional inputs are a temporal sequence of valence/arousal annotations, which quantitatively express emotion along unpleasant-pleasant and calm-aroused axes. Preliminary evaluation of emotional state estimation is also performed with a limited set of humans. Human physiological signals are acquired during simultaneous presentation of static pictures and sounds from valence/arousal-annotated International Affective Picture System and International Affective Digitized Sounds databases.",,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-642-02812-0_21,springer
Chapter ConferencePaper,doi:10.1007/978-3-540-73007-1_123,Neural Network Based Virtual Reality Spaces for Visual Data Mining of Cancer Data: An Unsupervised Perspective,Computational and Ambient Intelligence,10.1007/978-3-540-73007-1_123,Springer,2007-01-01,"Unsupervised neural networks are used for constructing virtual reality spaces for visual data mining of gene expression cancer data. Datasets representative of three of the most important types of cancer considered in modern medicine (liver, lung and stomach) are considered in the study. They are composed of samples from normal and tumor tissues, described in terms of tens of thousands of variables, which are the corresponding gene expression intensities measured in microarray experiments. Despite the very high dimensionality of the studied patterns, high quality visual representations in the form of structure-preserving virtual spaces are obtained using SAMANN neural networks, which enables the differentiation of cancerous and noncancerous tissues. The same networks could be used as nonlinear feature generators in a preprocessing step for other data mining procedures.",,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-540-73007-1_123,springer
Article,doi:10.1007/s11554-007-0029-x,Real-time approach for cloth simulation,Journal of Real-Time Image Processing,10.1007/s11554-007-0029-x,Springer,2007-07-01,"This paper presents a neural technique for simulating deformable objects. For the sake of achieving real-time and reducing the computational time costs, the fuzzy Kohonen clustering network (FKCN) is used for modeling and visualization. However, determining the physical parameters of differential equation is very difficult and sometimes different equation should be solved to produce animations, which become ill-conditioned. The finite element model is normally very time-consuming, since an appropriate structure must be defined. The proposed method can be more easily implemented than the finite element method. The simulation results show that the neural network method produced plausible animation results.",2007-06-16,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-007-0029-x,springer
Chapter ConferencePaper,doi:10.1007/978-3-540-89853-5_14,A Speech Recognition System for Urdu Language,"Wireless Networks, Information Processing and Systems",10.1007/978-3-540-89853-5_14,Springer,2009-01-01,"This paper investigates use of a machine learnt model for recognition of individually words spoken in Urdu language. Speech samples from many different speakers were utilized for modeling. Original time-domain samples are normalized and pre-processed by applying discrete Fourier transformation for speech feature extraction. In frequency domain, high degree of correlation was found for the same words spoken by different speakers. This helped produce models with high recognition accuracy. Details of model realization in MATLAB are included in this paper. Current work is being extended using linear predictive coding for efficient hardware implementation.",,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-540-89853-5_14,springer
Chapter ConferencePaper,doi:10.1007/978-3-540-88309-8_11,A Dipolar Competitive Neural Network for Video Segmentation,Advances in Artificial Intelligence – IBERAMIA 2008,10.1007/978-3-540-88309-8_11,Springer,2008-01-01,"This paper present a video segmentation method which separate pixels corresponding to foreground from those corresponding to background. The proposed background model consists of a competitive neural network based on dipoles, which is used to classify the pixels as background or foreground. Using this kind of neural networks permits an easy hardware implementation to achieve a real time processing with good results. The dipolar representation is designed to deal with the problem of estimating the directionality of data. Experimental results are provided by using the standard PETS dataset and compared with the mixture of Gaussians and background subtraction methods.",,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-540-88309-8_11,springer
Chapter ConferencePaper,doi:10.1007/978-3-540-69812-8_15,A Neural Network Approach for Video Object Segmentation in Traffic Surveillance,Image Analysis and Recognition,10.1007/978-3-540-69812-8_15,Springer,2008-01-01,This paper presents a neural background modeling based on subtraction approach for video object segmentation. A competitive neural network is proposed to form a background model for traffic surveillance. The unsupervised neural classifier handles the segmentation in natural traffic sequences with changes in illumination. The segmentation performance of the proposed neural network is qualitatively examined and compared to mixture of Gaussian models. The proposed algorithm is designed to enable efficient hardware implementation and to achieve real-time processing at great frame rates.,,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-540-69812-8_15,springer
Chapter ConferencePaper,doi:10.1007/978-3-540-79547-6_45,Learning Contextual Variations for Video Segmentation,Computer Vision Systems,10.1007/978-3-540-79547-6_45,Springer,2008-01-01,"This paper deals with video segmentation in vision systems. We focus on the maintenance of background models in long-term videos of changing environment which is still a real challenge in video surveillance. We propose an original weakly supervised method for learning contextual variations in videos. Our approach uses a clustering algorithm to automatically identify different contexts based on image content analysis. Then, state-of-the-art video segmentation algorithms (e.g. codebook, MoG) are trained on each cluster. The goal is to achieve a dynamic selection of background models. We have experimented our approach on a long video sequence (24 hours). The presented results show the segmentation improvement of our approach compared to codebook and MoG.",,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-540-79547-6_45,springer
Chapter,doi:10.1007/978-1-84628-659-9_10,"New Frontiers For Computer Ethics Artificial Intelligence, Cyberspace, And Virtual Reality",Ethical and Social Issues in the Information Age,10.1007/978-1-84628-659-9_10,Springer,2007-01-01,"Florence Yozefu is a brilliant scientist who heads a robotics research laboratory at a top ten research university. Florence has been developing wearable robotics gear that can take over the driving functions of a vehicle from a human operator when it is worn by the driver. In laboratory tests, the robot, nicknamed Catchmenot, has performed successfully whenever Florence and her assistants have worn the robot. However, no real-life experiment has ever been conducted outside the lab. Florence has been meaning to try it out one day, but has not got a chance as yet to do so.",,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-1-84628-659-9_10,springer
Article,doi:10.1007/s00530-006-0040-2,Guest Editors' introduction to the special issue: machine learning approaches to multimedia information retrieval,Multimedia Systems,10.1007/s00530-006-0040-2,Springer,2006-08-01,2006-07-26,,http://link.springer.com/openurl/pdf?id=doi:10.1007/s00530-006-0040-2,springer
Chapter ConferencePaper,doi:10.1007/3-540-34783-6_44,Proposal of Holographic 3D-Movie Generation Using Coherent Neural-Network Interpolation,"Computational Intelligence, Theory and Applications",10.1007/3-540-34783-6_44,Springer,2006-01-01,,,http://link.springer.com/openurl/pdf?id=doi:10.1007/3-540-34783-6_44,springer
Chapter ReferenceWorkEntry,doi:10.1007/0-387-30038-4_245,Video Content Analysis Using Machine Learning Tools,Encyclopedia of Multimedia,10.1007/0-387-30038-4_245,Springer,2006-01-01,,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/0-387-30038-4_245,springer
Article,doi:10.1007/s10994-005-1399-6,Guest Editors Introduction: Machine Learning in Speech and Language Technologies,Machine Learning,10.1007/s10994-005-1399-6,Springer,2005-09-01,,,http://link.springer.com/openurl/pdf?id=doi:10.1007/s10994-005-1399-6,springer
Chapter,doi:10.1007/0-387-21575-1_6,Automatic Speech Recognition: An Auditory Perspective,Speech Processing in the Auditory System,10.1007/0-387-21575-1_6,Springer,2004-01-01,,,http://link.springer.com/openurl/pdf?id=doi:10.1007/0-387-21575-1_6,springer
Chapter,doi:10.1007/0-387-21575-1_1,Speech Processing in the Auditory System: An Overview,Speech Processing in the Auditory System,10.1007/0-387-21575-1_1,Springer,2004-01-01,,,http://link.springer.com/openurl/pdf?id=doi:10.1007/0-387-21575-1_1,springer
Chapter,doi:10.1007/0-387-21575-1_5,The Perception of Speech Under Adverse Conditions,Speech Processing in the Auditory System,10.1007/0-387-21575-1_5,Springer,2004-01-01,,,http://link.springer.com/openurl/pdf?id=doi:10.1007/0-387-21575-1_5,springer
Article,doi:10.1007/s10055-005-0005-3,Neural network-based calibration of electromagnetic tracking systems,Virtual Reality,10.1007/s10055-005-0005-3,Springer,2005-12-01,2005-09-27,"Electromagnetic tracking systems are a common component of many virtual reality installations. Their accuracy, however, suffers from the distortions of the electromagnetic field used in calculating the tracker sensor’s position. We have developed a tracker calibration technique based on a neural network that effectively compensates for the errors in both tracked location and orientation. This case study discusses our implementation of the calibration algorithm and compares the results with traditional calibration methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-005-0005-3,springer
Chapter ConferencePaper,doi:DOItmp_0558_031242,A Real-Time Multimedia Data Transmission Rate Control Using a Neural Network Prediction Algorithm,Web Technologies Research and Development - APWeb 2005,DOItmp_0558_031242,Springer,2005-01-01,2005-03-10,"Both the real-time transmission and the amount of valid transmitted data are important factors in real-time multimedia transmission over the Internet. They are mainly affected by the channel bandwidth, delay time and packet loss. In this paper, we propose a predictive rate control system for data transmission, which is designed to improve the number of valid transmitted packets for the transmission of real-time multimedia through the Internet. A real-time multimedia transmission system was implemented using a TCP-Friendly algorithm, in order to obtain the measurement data needed for the proposed system. Neural network modeling was performed using the collected data, which consisted of the round-trip time delay and packet loss rate. The experiment results show that the algorithm proposed in this study increases the number of valid packets compare to the TCP-Friendly algorithm.",http://link.springer.com/openurl/pdf?id=doi:DOItmp_0558_031242,springer
Chapter ConferencePaper,doi:10.1007/978-3-540-31849-1_98,A Real-Time Multimedia Data Transmission Rate Control Using a Neural Network Prediction Algorithm,Web Technologies Research and Development - APWeb 2005,10.1007/978-3-540-31849-1_98,Springer,2005-01-01,,"Both the real-time transmission and the amount of valid transmitted data are important factors in real-time multimedia transmission over the Internet. They are mainly affected by the channel bandwidth, delay time and packet loss. In this paper, we propose a predictive rate control system for data transmission, which is designed to improve the number of valid transmitted packets for the transmission of real-time multimedia through the Internet. A real-time multimedia transmission system was implemented using a TCP-Friendly algorithm, in order to obtain the measurement data needed for the proposed system. Neural network modeling was performed using the collected data, which consisted of the round-trip time delay and packet loss rate. The experiment results show that the algorithm proposed in this study increases the number of valid packets compare to the TCP-Friendly algorithm.",http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-540-31849-1_98,springer
Chapter ConferencePaper,doi:10.1007/11494669_81,Neural Network Based 3D Model Reconstruction  with Highly Distorted Stereoscopic Sensors,Computational Intelligence and Bioinspired Systems,10.1007/11494669_81,Springer,2005-01-01,,"In stereoscopic vision, there are two artificial eyes implemented so that it can obtain two separate views of the scene and simulate the binocular depth perception of human beings. Traditionally, camera calibration and 3D reconstruction of such a vision sensor are performed by geometrical solutions. However, the traditional camera model is very complicated since nonlinear factors in it and needs to approximate the light projection scheme by a number of parameters. It is even very difficult to model some highly distorted vision sensors, such as fish-eye lens. In order to simplify both the camera calibration and 3D reconstruction procedures, this work presents a method based on neural networks which is brought forward according to the characteristics of neural network and stereoscopic vision. The relation between spatial points and image points is established by training the network without the parameters of the cameras, such as focus, distortions besides the geometry of the system. The training set for our neural network consists of a variety of stereo-pair images and corresponding 3D world coordinates. Then the 3D reconstruction of a new s cene is simply using the trained network. Such a method is more similar to how human’s eyes work. Simulations and real data are used to demonstrate and evaluate the procedure. We observe that the errors obtained our experimentation are accurate enough for most machine-vision applications.",http://link.springer.com/openurl/pdf?id=doi:10.1007/11494669_81,springer
Article,doi:10.1023/B:IJST.0000037076.86366.8d,Trainable Articulatory Control Models for Visual Speech Synthesis,International Journal of Speech Technology,10.1023/B:IJST.0000037076.86366.8d,Springer,2004-10-01,,"This paper deals with the problem of modelling the dynamics of articulation for a parameterised talking head based on phonetic input. Four different models are implemented and trained to reproduce the articulatory patterns of a real speaker, based on a corpus of optical measurements. Two of the models, (“Cohen-Massaro” and “Öhman”) are based on coarticulation models from speech production theory and two are based on artificial neural networks, one of which is specially intended for streaming real-time applications. The different models are evaluated through comparison between predicted and measured trajectories, which shows that the Cohen-Massaro model produces trajectories that best matches the measurements. A perceptual intelligibility experiment is also carried out, where the four data-driven models are compared against a rule-based model as well as an audio-alone condition. Results show that all models give significantly increased speech intelligibility over the audio-alone case, with the rule-based model yielding highest intelligibility score.",http://link.springer.com/openurl/pdf?id=doi:10.1023/B:IJST.0000037076.86366.8d,springer
Article,doi:10.1007/s10845-005-1662-5,Neurovision-based logic control of an experimental manufacturing plant using neural net le-net5 and automation Petri nets,Journal of Intelligent Manufacturing,10.1007/s10845-005-1662-5,Springer,2005-10-01,,"In this paper, Petri nets and neural networks are used together in the development of an intelligent logic controller for an experimental manufacturing plant to provide the flexibility and intelligence required from this type of dynamic systems. In the experimental setup, among deformed and good parts to be processed, there are four different part types to be recognised and selected. To distinguish the correct part types, a convolutional neural net le-net5 based on-line image recognition system is established. Then, the necessary information to be used within the logic control system is produced by this on-line image recognition system. Using the information about the correct part types and Automation Petri nets, a logic control system is designed. To convert the resulting Automation Petri net model of the controller into the related ladder logic diagram (LLD), the token passing logic (TPL) method is used. Finally, the implementation of the control logic as an LDD for the real time control of the manufacturing system is accomplished by using a commercial programmable logic controller (PLC).",http://link.springer.com/openurl/pdf?id=doi:10.1007/s10845-005-1662-5,springer
Chapter ConferencePaper,doi:10.1007/11861461_19,Machine Learning for Spoken Dialogue Management: An Experiment with Speech-Based Database Querying,"Artificial Intelligence: Methodology, Systems, and Applications",10.1007/11861461_19,Springer,2006-01-01,,"Although speech and language processing techniques achieved a relative maturity during the last decade, designing a spoken dialogue system is still a tailoring task because of the great variability of factors to take into account. Rapid design and reusability across tasks of previous work is made very difficult. For these reasons, machine learning methods applied to dialogue strategy optimization has become a leading subject of researches since the mid 90’s. In this paper, we describe an experiment of reinforcement learning applied to the optimization of speech-based database querying. We will especially emphasize on the sensibility of the method relatively to the dialogue modeling parameters in the framework of the Markov decision processes, namely the state space and the reinforcement signal. The evolution of the design will be exposed as well as results obtained on a simple real application.",http://link.springer.com/openurl/pdf?id=doi:10.1007/11861461_19,springer
Chapter ConferencePaper,doi:10.1007/11892755_41,Combining Neural Networks and Clustering Techniques for Object Recognition in Indoor Video Sequences,"Progress in Pattern Recognition, Image Analysis and Applications",10.1007/11892755_41,Springer,2006-01-01,,"This paper presents the results obtained in a real experiment for object recognition in a sequence of images captured by a mobile robot in an indoor environment. Objects are simply represented as an unstructured set of spots (image regions) for each frame, which are obtained from the result of an image segmentation algorithm applied on the whole sequence. In a previous work, neural networks were used to classify the spots independently as belonging to one of the objects of interest or the background from different spot features (color, size and invariant moments). In this work, clustering techniques are applied afterwards taking into account both the neural net outputs (class probabilities) and geometrical data (spot mass centers). In this way, context information is exploited to improve the classification performance. The experimental results of this combined approach are quite promising and better than the ones obtained using only the neural nets.",http://link.springer.com/openurl/pdf?id=doi:10.1007/11892755_41,springer
Chapter ConferencePaper,doi:10.1007/11760023_22,Multimodal Priority Verification of Face and Speech Using Momentum Back-Propagation Neural Network,Advances in Neural Networks - ISNN 2006,10.1007/11760023_22,Springer,2006-01-01,,"In this paper, we propose a priority verification method for multimodal biometric features by using a momentum back-propagation artificial neural network (MBP-ANN). We also propose a personal verification method using both face and speech to improve the rate of single biometric verification. False acceptance rate (FAR) and false rejection rate (FRR) have been a fundamental bottleneck of real-time personal verification. The proposed multimodal biometric method is to improve both verification rate and reliability in real-time by overcoming technical limitations of single biometric verification methods. The proposed method uses principal component analysis (PCA) for face recognition and hidden markov model (HMM) for speech recognition. It also uses MBP-ANN for the final decision of personal verification. Based on experimental results, the proposed system can reduce FAR down to 0.0001%, which proves that the proposed method overcomes the limitation of single biometric system and proves stable personal verification in real-time.",http://link.springer.com/openurl/pdf?id=doi:10.1007/11760023_22,springer
Chapter ConferencePaper,doi:10.1007/11427445_55,Speech Recognition of Finite Words Based on Multi-weight Neural Network,Advances in Neural Networks – ISNN 2005,10.1007/11427445_55,Springer,2005-01-01,,"Under the guide of the novel biomimetics pattern recognition theory that is based on the character of human’s recognition, combining the character of traditional neural network, a new multi-weight neural network is constructed to realize the idea of that theory. Extraordinary results are obtained with the first use of the new multi-weight neural network in speech recognition of finite words. The experiment results show that the multi-weight neural network can not only recognize finite words correctly and promptly, but also keep quite high correct recognition rate under the circumstance of small-number samples.",http://link.springer.com/openurl/pdf?id=doi:10.1007/11427445_55,springer
Chapter ConferencePaper,doi:10.1007/11578079_10,Object Recognition in Indoor Video Sequences by Classifying Image Segmentation Regions Using Neural Networks,"Progress in Pattern Recognition, Image Analysis and Applications",10.1007/11578079_10,Springer,2005-01-01,,"This paper presents the results obtained in a real experiment for object recognition in a sequence of images captured by a mobile robot in an indoor environment. The purpose is that the robot learns to identify and locate objects of interest in its environment from samples of different views of the objects taken from video sequences. In this work, objects are simply represented as an unstructured set of spots (image regions) for each frame, which are obtained from the result of an image segmentation algorithm applied on the whole sequence. Each spot is semi-automatically assigned to a class (one of the objects or the background) and different features (color, size and invariant moments) are computed for it. These labeled data are given to a feed-forward neural network which is trained to classify the spots. The results obtained with all the features, several feature subsets and a backward selection method show the feasibility of the approach and point to color as the fundamental feature for discriminative ability.",http://link.springer.com/openurl/pdf?id=doi:10.1007/11578079_10,springer
Chapter ConferencePaper,doi:10.1007/978-3-540-28633-2_30,Elastic Learning Rate on Error Backpropagation of Online Update,PRICAI 2004: Trends in Artificial Intelligence,10.1007/978-3-540-28633-2_30,Springer,2004-01-01,,"The error-backpropagation (EBP) algorithm for learning multilayer perceptrons (MLPs) is known to have good features of robustness and economical efficiency. However, the algorithm has difficulty in selecting an optimal constant learning rate and thus results in non-optimal learning speed and inflexible operation for working data. This paper introduces an elastic learning rate that guarantees convergence of learning and its local realization by online update of MLP parameters into the original EBP algorithm in order to complement the non-optimality. The results of experiments on a speaker verification system with Korean speech database are presented and discussed to demonstrate the performance improvement of the proposed method in terms of learning speed and flexibility for working data of the original EBP algorithm.",http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-540-28633-2_30,springer
Chapter,doi:10.1007/0-387-21575-1_7,Hearing Aids and Hearing Impairment,Speech Processing in the Auditory System,10.1007/0-387-21575-1_7,Springer,2004-01-01,,,http://link.springer.com/openurl/pdf?id=doi:10.1007/0-387-21575-1_7,springer
Article,doi:10.1023/A:1021087603673,Editorial: Speech Synthesis Comes of Age,International Journal of Speech Technology,10.1023/A:1021087603673,Springer,2003-01-01,,,http://link.springer.com/openurl/pdf?id=doi:10.1023/A:1021087603673,springer
Article,doi:10.1007/s003710100137,Detection of human faces in a compressed domain for video stratification,The Visual Computer,10.1007/s003710100137,Springer,2002-04-01,,2014-03-01,http://link.springer.com/openurl/pdf?id=doi:10.1007/s003710100137,springer
Article,doi:10.1007/s005210170004,A Mixture of Recurrent Neural Networks for Speaker Normalisation,Neural Computing & Applications,10.1007/s005210170004,Springer,2001-05-01,,,http://link.springer.com/openurl/pdf?id=doi:10.1007/s005210170004,springer
Article,doi:10.1007/s005210070006,Emotion Recognition in Speech Using Neural Networks,Neural Computing & Applications,10.1007/s005210070006,Springer,2000-12-01,,,http://link.springer.com/openurl/pdf?id=doi:10.1007/s005210070006,springer
Article,doi:10.1007/s005210070007,"A Set of Neural Tools for Human-Computer Interactions: Application to the Handwritten Character Recognition, and Visual Speech Recognition Problems",Neural Computing & Applications,10.1007/s005210070007,Springer,2000-12-01,,,http://link.springer.com/openurl/pdf?id=doi:10.1007/s005210070007,springer
Article,doi:10.1007/s100440050024,A Neural Network Approach to Planar-Object Recognition in 3D Space,Pattern Analysis & Applications,10.1007/s100440050024,Springer,1999-06-01,"Most existing 2D object recognition algorithms are not perspective (or projective) invariant, and hence are not suitable for many real-world applications. By contrast, one of the primary goals of this research is to develop a flat object matching system that can identify and localise an object, even when seen from different viewpoints in 3D space. In addition, we also strive to achieve good scale invariance and robustness against partial occlusion as in any practical 2D object recognition system. The proposed system uses multi-view model representations and objects are recognised by self-organised dynamic link matching. The merit of this approach is that it offers a compact framework for concurrent assessments of multiple match hypotheses by promoting competitions and/or co-operations among several local mappings of model and test image feature correspondences. Our experiments show that the system is very successful in recognising object to perspective distortion, even in rather cluttered scenes.",2014-03-04,http://link.springer.com/openurl/pdf?id=doi:10.1007/s100440050024,springer
Chapter ConferencePaper,doi:10.1007/978-3-540-24598-8_50,A Video System for Recognizing Gestures by Artificial Neural Networks for Expressive Musical Control,Gesture-Based Communication in Human-Computer Interaction,10.1007/978-3-540-24598-8_50,Springer,2004-01-01,In this paper we describe a system to recognize gestures to control musical processes. For that we applied a Time Delay Neuronal Network to match gestures processed as variation of luminance information in video streams. This resulted in recognition rates of about 90% for 3 different types of hand gestures and it is presented here as a prototype for a gestural recognition system that is tolerant to ambient conditions and environments. The neural network can be trained to recognize gestures difficult to be described by postures or sign language. This can be used to adapt to unique gestures of a performer or video sequences of arbitrary moving objects. We will discuss the outcome of extending the system to learn successfully a set of 17 hand gestures. The application was implemented in jMax to achieve real-time conditions and easy integration into a musical environment. We will describe the design and learning procedure of the using the Stuttgart Neuronal Network Simulator. The system aims to integrate into an environment that enables expressive control of musical parameters (KANSEI).,,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-540-24598-8_50,springer
Chapter ConferencePaper,doi:DOItmp_0558_010604,A Video System for Recognizing Gestures by Artificial Neural Networks for Expressive Musical Control,Gesture-Based Communication in Human-Computer Interaction,DOItmp_0558_010604,Springer,2004-01-01,In this paper we describe a system to recognize gestures to control musical processes. For that we applied a Time Delay Neuronal Network to match gestures processed as variation of luminance information in video streams. This resulted in recognition rates of about 90% for 3 different types of hand gestures and it is presented here as a prototype for a gestural recognition system that is tolerant to ambient conditions and environments. The neural network can be trained to recognize gestures difficult to be described by postures or sign language. This can be used to adapt to unique gestures of a performer or video sequences of arbitrary moving objects. We will discuss the outcome of extending the system to learn successfully a set of 17 hand gestures. The application was implemented in jMax to achieve real-time conditions and easy integration into a musical environment. We will describe the design and learning procedure of the using the Stuttgart Neuronal Network Simulator. The system aims to integrate into an environment that enables expressive control of musical parameters (KANSEI).,2004-02-06,http://link.springer.com/openurl/pdf?id=doi:DOItmp_0558_010604,springer
Article,doi:10.1155/S1110865702000720,Audio Classification in Speech and Music: A Comparison between a Statistical and a Neural Approach,EURASIP Journal on Advances in Signal Processing,10.1155/S1110865702000720,Springer,2002-04-30,"We focus the attention on the problem of audio classification in speech and music for multimedia applications. In particular, we present a comparison between two different techniques for speech/music discrimination. The first method is based on Zero crossing rate and Bayesian classification. It is very simple from a computational point of view, and gives good results in case of pure music or speech. The simulation results show that some performance degradation arises when the music segment contains also some speech superimposed on music, or strong rhythmic components. To overcome these problems, we propose a second method, that uses more features, and is based on neural networks (specifically a multi-layer Perceptron). In this case we obtain better performance, at the expense of a limited growth in the computational complexity. In practice, the proposed neural network is simple to be implemented if a suitable polynomial is used as the activation function, and a real-time implementation is possible even if low-cost embedded systems are used.",2002-04-30,https://www.biomedcentral.com/openurl/pdf?id=doi:10.1155/S1110865702000720,springer
Chapter ConferencePaper,doi:10.1007/3-540-48239-3_26,Speech Recognition Using Elman Neural Networks,"Text, Speech and Dialogue",10.1007/3-540-48239-3_26,Springer,1999-01-01,The main topic of this paper was to research the use of recurrent neural networks in the process of automatic speech processing. The starting point was a modified version of RECNET as developed by Robinson. This phoneme recognizer is based on a RNN but post-processing is based on Hidden Markov Models. A parallel version of RECNET was implemented on a parallel computer (nCUBE2) and Elman RNN were used as postprocessor. Word segmentation is also realized using an Elman RNN. The network models and results of testing are reported in this paper.,1999-10-01,http://link.springer.com/openurl/pdf?id=doi:10.1007/3-540-48239-3_26,springer
Chapter ConferencePaper,doi:10.1007/3-540-46084-5_212,Neural Networks Retraining for Unsupervised Video Object Segmentation of Videoconference Sequences,Artificial Neural Networks — ICANN 2002,10.1007/3-540-46084-5_212,Springer,2002-01-01,"In this paper efficient performance generalization of neural network classifiers is accomplished, for unsupervised video object segmentation in videoconference/videophone sequences. Each time conditions change, a retraining phase is activated and the neural network classifier is adapted to the new environment. During retraining both the former and current knowledge are utilized so that good network generalization is achieved. The retraining algorithm results in the minimization of a convex function subject to linear constraints, leading to very fast network weight adaptation. Current knowledge is unsupervisedly extracted using a face-body detector, based on Gaussian p.d.f models. A binary template matching technique is also incorporated, which imposes shape constraints to candidate face regions. Finally the retrained network performs video object segmentation to the new environment. Several experiments on real sequences indicate the promising performance of the proposed adaptive neural network as efficient video object segmentation tool.",2002-08-21,http://link.springer.com/openurl/pdf?id=doi:10.1007/3-540-46084-5_212,springer
Article,doi:10.1007/BF00631073,Group speech acts,Linguistics and Philosophy,10.1007/BF00631073,Springer,1984-11-01,,,http://link.springer.com/openurl/pdf?id=doi:10.1007/BF00631073,springer
Article,doi:10.1023/A:1007933917719,Human-Machine Collaboration in Robotics: Integrating Virtual Tools with a Collision Avoidance Concept using Conglomerates of Spheres,Journal of Intelligent and Robotic Systems,10.1023/A:1007933917719,Springer,1997-04-01,"This paper describes how virtual tools that represent real robot end-effectors are used in conjunction with a generalized conglomerate-of-spheres approach to collision avoidance in such a way that telerobotic trajectory planning can be accomplished using simple gesture phrases such as ‘put that there while avoiding that’. In this concept, an operator (or set of collaborators) need not train for cumbersome telemanipulation on several multiple-link robots, nor do robots need a priori knowledge of operator intent and exhaustive algorithms for evaluating every aspect of a detailed environment model. The human does what humans do best during task specification, while the robot does what machines do best during trajectory planning and execution. Four telerobotic stages were implemented to demonstrate this strategic supervision concept that will facilitate collaborative control between humans and machines. In the first stage, virtual reality tools are selected from a ‘toolbox’ by the operator(s) and then these virtual tools are computationally interwoven into the live video scene with depth correlation. Each virtual tool is a graphic representation of a robot end-effector (gripper, cutter, or other robot tool) that carries tool-use attributes on how to perform a task. An operator uses an instrumented glove to virtually retrieve the disembodied tool, in the shared scene, and place it near objects and obstacles while giving key-point gesture directives, such as ‘cut there while avoiding that’. Collaborators on a network may alter the plan by changing tools or tool positioning to achieve preferred results from their own perspectives. When parties agree, from wherever they reside geographically, the robot(s) create and execute appropriate trajectories suitable to their own particular links and joints. Stage two generates standard joint-interpolated trajectories, and later creates potential field trajectories if necessary. Stage three tests for collisions with obstacles identified by the operator and modeled as conglomerates of spheres. Stage four involves automatic grasping (or cutting etc.) once the robot camera acquires a close-up view of the object during approach. In this paper particular emphasis is placed on the conglomerate-of-spheres approach to collision detection as integrated with the virtual tools concept for a Puma 560 robot by the Virtual Tools and Robotics Group in the Computer Integrated Manufacturing Laboratory at The Pennsylvania State University (Penn State).",,http://link.springer.com/openurl/pdf?id=doi:10.1023/A:1007933917719,springer
Chapter ConferencePaper,doi:10.1007/978-3-7091-9433-1_12,The Virtual Treadmill: A Naturalistic Metaphor for Navigation in Immersive Virtual Environments,Virtual Environments ’95,10.1007/978-3-7091-9433-1_12,Springer,1995-01-01,"This paper describes a metaphor that allows people to move around an immersive virtual environment by “walking in place”. Positional data of participants’ head movements are obtained from a tracking sensor on a head-mounted display during a training session, where they alternate between walking in place and a range of other activities. The data is fed to a neural net pattern recogniser that learns to recognise the person’s walking in place behaviour. This is used in a virtual reality system to allow people to move through the virtual environment by simulating the kinds of kinesthetic actions and sensory perceptions involved in walking. An experiment was carried out to compare this method of navigation with the familiar alternative that involves using a hand-held pointing device, such as a 3D mouse. The experiment suggests that the walking in place method may enhance the participant’s sense of presence, but that it is not advantageous with respect to the efficiency of navigation.",,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-7091-9433-1_12,springer
Chapter ConferencePaper,doi:10.1007/BFb0032583,Simulation of MPEG video traffic using neural networks,Biological and Artificial Computation: From Neuroscience to Technology,10.1007/BFb0032583,Springer,1997-01-01,"A new model for the simulation of MPEG video traffic is presented. The model, implemented on neural networks, is capable of accurately adjusting the autocorrelation and probability distribution functions of a given video traffic. This adjustment is performed by capturing the projected conditioned histogram of the real traffic, so that the neural model will be able to yield a simulated traffic at its output, just as a function of an input white noise. Moreover, using neural networks we benefit from their inherent capacities for working in real time, because of their parallelism, and interpolating unknown functions. These interpolations avoid the need of searching in transition matrices of other histogram-based methods as well as they reduce the amount of stored information. Results are presented for a real MPEG video source.",2005-06-18,http://link.springer.com/openurl/pdf?id=doi:10.1007/BFb0032583,springer
Article,doi:10.1007/BF02747523,"Pattern recognition, image processing and computer vision in fifth generation computer systems",Sadhana,10.1007/BF02747523,Springer,1986-09-01,"It is well-known that one of the goals of research for the last two decades or so in pattern recognition and its sub-areas, such as processing, analysis and understanding of image, speech and natural language, and computer vision techniques etc., has always been to develop fundamental techniques for flexible interactive intelligent man-machine interfaces for computers. In this paper, the author tries to argue that for the evolution of fifth generation computer systems (FGCS) as defined by Japanese scientists, some of the things required are realisation and implementation of the advances in pattern recognition and its sub-areas, not only to achieve the man-machine interface with a natural mode of communication, but also for the realisation of the basic mechanisms of inference, association and learning, which are inherent both in pattern recognition and in the core functions of FGCS. The next generation computers will be knowledge-based systems, which form a subdomain of artificial intelligence ( a 1) techniques, and so a 1 provides the essential link between pattern recognition domains and different application systems. No attempt is made to discuss other essential conceptual building blocks, such as software engineering, computer architecture and very large scale integration technology unless these become very relevant in the discussions of concerned topics of the paper. A section on limitations of perception, learning and knowledge for computing machines is also included.",,http://link.springer.com/openurl/pdf?id=doi:10.1007/BF02747523,springer
Chapter,doi:10.1007/978-3-662-13015-5_33,Time Delay Neural Networks for Articulatory Estimation from Speech: Suitable Subjective Evaluation Protocols,Speechreading by Humans and Machines,10.1007/978-3-662-13015-5_33,Springer,1996-01-01,"The approach described in this paper is based on the use of time-delay neural networks for solving the task of articulatory estimation from acoustic speech and on image vector quantization as far as the visual synthesis is concerned. Once the system has been trained on a reference speaker, the association of visual cues is performed in real-time to each 20 ms of incoming speech. Preliminary results are reported with reference to the on-going experimentation both with normal hearing people and with deaf persons to estimate some of the many perceptual thresholds involved in the complex task of speechreading from synthetic images. This experimental phase is carried on in cooperation with FIADDA, the italian association of the families of hearing impaired children, and is based on a flexible simulation environment.",,http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-662-13015-5_33,springer
