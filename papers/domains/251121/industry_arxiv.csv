id,updated,published,title,summary,database
http://arxiv.org/abs/2007.03580v1,2020-07-07T15:58:10Z,2020-07-07T15:58:10Z,Using Semantic Web Services for AI-Based Research in Industry 4.0,"The transition to Industry 4.0 requires smart manufacturing systems that are
easily configurable and provide a high level of flexibility during
manufacturing in order to achieve mass customization or to support cloud
manufacturing. To realize this, Cyber-Physical Systems (CPSs) combined with
Artificial Intelligence (AI) methods find their way into manufacturing shop
floors. For using AI methods in the context of Industry 4.0, semantic web
services are indispensable to provide a reasonable abstraction of the
underlying manufacturing capabilities. In this paper, we present semantic web
services for AI-based research in Industry 4.0. Therefore, we developed more
than 300 semantic web services for a physical simulation factory based on Web
Ontology Language for Web Services (OWL-S) and Web Service Modeling Ontology
(WSMO) and linked them to an already existing domain ontology for intelligent
manufacturing control. Suitable for the requirements of CPS environments, our
pre- and postconditions are verified in near real-time by invoking other
semantic web services in contrast to complex reasoning within the knowledge
base. Finally, we evaluate our implementation by executing a cyber-physical
workflow composed of semantic web services using a workflow management system.",arxiv
http://arxiv.org/abs/2005.13669v1,2020-05-27T21:32:44Z,2020-05-27T21:32:44Z,"The Manufacturing Data and Machine Learning Platform: Enabling Real-time
  Monitoring and Control of Scientific Experiments via IoT","IoT devices and sensor networks present new opportunities for measuring,
monitoring, and guiding scientific experiments. Sensors, cameras, and
instruments can be combined to provide previously unachievable insights into
the state of ongoing experiments. However, IoT devices can vary greatly in the
type, volume, and velocity of data they generate, making it challenging to
fully realize this potential. Indeed, synergizing diverse IoT data streams in
near-real time can require the use of machine learning (ML). In addition, new
tools and technologies are required to facilitate the collection, aggregation,
and manipulation of sensor data in order to simplify the application of ML
models and in turn, fully realize the utility of IoT devices in laboratories.
Here we will demonstrate how the use of the Argonne-developed Manufacturing
Data and Machine Learning (MDML) platform can analyze and use IoT devices in a
manufacturing experiment. MDML is designed to standardize the research and
operational environment for advanced data analytics and AI-enabled automated
process optimization by providing the infrastructure to integrate AI in
cyber-physical systems for in situ analysis. We will show that MDML is capable
of processing diverse IoT data streams, using multiple computing resources, and
integrating ML models to guide an experiment.",arxiv
http://arxiv.org/abs/1908.02150v3,2019-10-22T02:23:42Z,2019-08-04T05:19:43Z,Industrial Artificial Intelligence,"Artificial Intelligence (AI) is a cognitive science to enables human to
explore many intelligent ways to model our sensing and reasoning processes.
Industrial AI is a systematic discipline to enable engineers to systematically
develop and deploy AI algorithms with repeating and consistent successes. In
this paper, the key enablers for this transformative technology along with
their significant advantages are discussed. In addition, this research explains
Lighthouse Factories as an emerging status applying to the top manufacturers
that have implemented Industrial AI in their manufacturing ecosystem and gained
significant financial benefits. It is believed that this research will work as
a guideline and roadmap for researchers and industries towards the real-world
implementation of Industrial AI.",arxiv
http://arxiv.org/abs/2012.09610v1,2020-10-30T20:33:05Z,2020-10-30T20:33:05Z,Validate and Enable Machine Learning in Industrial AI,"Industrial Artificial Intelligence (Industrial AI) is an emerging concept
which refers to the application of artificial intelligence to industry.
Industrial AI promises more efficient future industrial control systems.
However, manufacturers and solution partners need to understand how to
implement and integrate an AI model into the existing industrial control
system. A well-trained machine learning (ML) model provides many benefits and
opportunities for industrial control optimization; however, an inferior
Industrial AI design and integration limits the capability of ML models. To
better understand how to develop and integrate trained ML models into the
traditional industrial control system, test the deployed AI control system, and
ultimately outperform traditional systems, manufacturers and their AI solution
partners need to address a number of challenges. Six top challenges, which were
real problems we ran into when deploying Industrial AI, are explored in the
paper. The Petuum Optimum system is used as an example to showcase the
challenges in making and testing AI models, and more importantly, how to
address such challenges in an Industrial AI system.",arxiv
http://arxiv.org/abs/2001.05703v1,2020-01-16T09:13:31Z,2020-01-16T09:13:31Z,"A Markerless Deep Learning-based 6 Degrees of Freedom PoseEstimation for
  with Mobile Robots using RGB Data","Augmented Reality has been subject to various integration efforts within
industries due to its ability to enhance human machine interaction and
understanding. Neural networks have achieved remarkable results in areas of
computer vision, which bear great potential to assist and facilitate an
enhanced Augmented Reality experience. However, most neural networks are
computationally intensive and demand huge processing power thus, are not
suitable for deployment on Augmented Reality devices. In this work we propose a
method to deploy state of the art neural networks for real time 3D object
localization on augmented reality devices. As a result, we provide a more
automated method of calibrating the AR devices with mobile robotic systems. To
accelerate the calibration process and enhance user experience, we focus on
fast 2D detection approaches which are extracting the 3D pose of the object
fast and accurately by using only 2D input. The results are implemented into an
Augmented Reality application for intuitive robot control and sensor data
visualization. For the 6D annotation of 2D images, we developed an annotation
tool, which is, to our knowledge, the first open source tool to be available.
We achieve feasible results which are generally applicable to any AR device
thus making this work promising for further research in combining high
demanding neural networks with Internet of Things devices.",arxiv
http://arxiv.org/abs/2108.04465v1,2021-08-10T06:20:18Z,2021-08-10T06:20:18Z,"Industrial Digital Twins at the Nexus of NextG Wireless Networks and
  Computational Intelligence: A Survey","By amalgamating recent communication and control technologies, computing and
data analytics techniques, and modular manufacturing, Industry~4.0 promotes
integrating cyber-physical worlds through cyber-physical systems (CPS) and
digital twin (DT) for monitoring, optimization, and prognostics of industrial
processes. A DT is an emerging but conceptually different construct than CPS.
Like CPS, DT relies on communication to create a highly-consistent,
synchronized digital mirror image of the objects or physical processes. DT, in
addition, uses built-in models on this precise image to simulate, analyze,
predict, and optimize their real-time operation using feedback. DT is rapidly
diffusing in the industries with recent advances in the industrial Internet of
things (IIoT), edge and cloud computing, machine learning, artificial
intelligence, and advanced data analytics. However, the existing literature
lacks in identifying and discussing the role and requirements of these
technologies in DT-enabled industries from the communication and computing
perspective. In this article, we first present the functional aspects, appeal,
and innovative use of DT in smart industries. Then, we elaborate on this
perspective by systematically reviewing and reflecting on recent research in
next-generation (NextG) wireless technologies (e.g., 5G and beyond networks),
various tools (e.g., age of information, federated learning, data analytics),
and other promising trends in networked computing (e.g., edge and cloud
computing). Moreover, we discuss the DT deployment strategies at different
industrial communication layers to meet the monitoring and control requirements
of industrial applications. We also outline several key reflections and future
research challenges and directions to facilitate industrial DT's adoption.",arxiv
http://arxiv.org/abs/2103.11879v2,2021-09-13T07:07:50Z,2021-03-22T14:16:16Z,Real-time End-to-End Federated Learning: An Automotive Case Study,"With the development and the increasing interests in ML/DL fields, companies
are eager to apply Machine Learning/Deep Learning approaches to increase
service quality and customer experience. Federated Learning was implemented as
an effective model training method for distributing and accelerating
time-consuming model training while protecting user data privacy. However,
common Federated Learning approaches, on the other hand, use a synchronous
protocol to conduct model aggregation, which is inflexible and unable to adapt
to rapidly changing environments and heterogeneous hardware settings in
real-world scenarios. In this paper, we present an approach to real-time
end-to-end Federated Learning combined with a novel asynchronous model
aggregation protocol. Our method is validated in an industrial use case in the
automotive domain, focusing on steering wheel angle prediction for autonomous
driving. Our findings show that asynchronous Federated Learning can
significantly improve the prediction performance of local edge models while
maintaining the same level of accuracy as centralized machine learning.
Furthermore, by using a sliding training window, the approach can minimize
communication overhead, accelerate model training speed and consume real-time
streaming data, proving high efficiency when deploying ML/DL components to
heterogeneous real-world embedded systems.",arxiv
http://arxiv.org/abs/2009.03645v1,2020-09-08T11:31:43Z,2020-09-08T11:31:43Z,"Detection of Anomalies and Faults in Industrial IoT Systems by Data
  Mining: Study of CHRIST Osmotron Water Purification System","Industry 4.0 will make manufacturing processes smarter but this smartness
requires more environmental awareness, which in case of Industrial Internet of
Things, is realized by the help of sensors. This article is about industrial
pharmaceutical systems and more specifically, water purification systems.
Purified water which has certain conductivity is an important ingredient in
many pharmaceutical products. Almost every pharmaceutical company has a water
purifying unit as a part of its interdependent systems. Early detection of
faults right at the edge can significantly decrease maintenance costs and
improve safety and output quality, and as a result, lead to the production of
better medicines. In this paper, with the help of a few sensors and data mining
approaches, an anomaly detection system is built for CHRIST Osmotron water
purifier. This is a practical research with real-world data collected from
SinaDarou Labs Co. Data collection was done by using six sensors over two-week
intervals before and after system overhaul. This gave us normal and faulty
operation samples. Given the data, we propose two anomaly detection approaches
to build up our edge fault detection system. The first approach is based on
supervised learning and data mining e.g. by support vector machines. However,
since we cannot collect all possible faults data, an anomaly detection approach
is proposed based on normal system identification which models the system
components by artificial neural networks. Extensive experiments are conducted
with the dataset generated in this study to show the accuracy of the
data-driven and model-based anomaly detection methods.",arxiv
http://arxiv.org/abs/1810.07829v1,2018-10-17T23:06:06Z,2018-10-17T23:06:06Z,"Quality 4.0: Let's Get Digital - The many ways the fourth industrial
  revolution is reshaping the way we think about quality","The technology landscape is richer and more promising than ever before. In
many ways, cloud computing, big data, virtual reality (VR), augmented reality
(AR), blockchain, additive manufacturing, artificial intelligence (AI), machine
learning (ML), Internet Protocol Version 6 (IPv6), cyber-physical systems and
the Internet of Things (IoT) all represent new frontiers. These technologies
can help improve product and service quality, and organizational performance.
In many regions, the internet is now as ubiquitous as electricity. Components
are relatively cheap. A robust ecosystem of open-source software libraries
means that engineers can solve problems 100 times faster than just two decades
ago. This digital transformation is leading us toward connected intelligent
automation: smart, hyperconnected agents deployed in environments where humans
and machines cooperate, and leverage data, to achieve shared goals. This is not
the worlds first industrial revolution. In fact, it is its fourth, and the
disruptive changes it will bring suggest we will need a fresh perspective on
quality to adapt to it.",arxiv
http://arxiv.org/abs/1705.00346v1,2017-04-30T17:17:44Z,2017-04-30T17:17:44Z,Deep Learning in the Automotive Industry: Applications and Tools,"Deep Learning refers to a set of machine learning techniques that utilize
neural networks with many hidden layers for tasks, such as image
classification, speech recognition, language understanding. Deep learning has
been proven to be very effective in these domains and is pervasively used by
many Internet services. In this paper, we describe different automotive uses
cases for deep learning in particular in the domain of computer vision. We
surveys the current state-of-the-art in libraries, tools and infrastructures
(e.\,g.\ GPUs and clouds) for implementing, training and deploying deep neural
networks. We particularly focus on convolutional neural networks and computer
vision use cases, such as the visual inspection process in manufacturing plants
and the analysis of social media data. To train neural networks, curated and
labeled datasets are essential. In particular, both the availability and scope
of such datasets is typically very limited. A main contribution of this paper
is the creation of an automotive dataset, that allows us to learn and
automatically recognize different vehicle properties. We describe an end-to-end
deep learning application utilizing a mobile app for data collection and
process support, and an Amazon-based cloud backend for storage and training.
For training we evaluate the use of cloud and on-premises infrastructures
(including multiple GPUs) in conjunction with different neural network
architectures and frameworks. We assess both the training times as well as the
accuracy of the classifier. Finally, we demonstrate the effectiveness of the
trained classifier in a real world setting during manufacturing process.",arxiv
http://arxiv.org/abs/1911.05726v2,2020-05-14T22:57:58Z,2019-11-05T16:04:45Z,"Cyber Risk at the Edge: Current and future trends on Cyber Risk
  Analytics and Artificial Intelligence in the Industrial Internet of Things
  and Industry 4.0 Supply Chains","Digital technologies have changed the way supply chain operations are
structured. In this article, we conduct systematic syntheses of literature on
the impact of new technologies on supply chains and the related cyber risks. A
taxonomic/cladistic approach is used for the evaluations of progress in the
area of supply chain integration in the Industrial Internet of Things and
Industry 4.0, with a specific focus on the mitigation of cyber risks. An
analytical framework is presented, based on a critical assessment with respect
to issues related to new types of cyber risk and the integration of supply
chains with new technologies. This paper identifies a dynamic and self-adapting
supply chain system supported with Artificial Intelligence and Machine Learning
(AI/ML) and real-time intelligence for predictive cyber risk analytics. The
system is integrated into a cognition engine that enables predictive cyber risk
analytics with real-time intelligence from IoT networks at the edge. This
enhances capacities and assist in the creation of a comprehensive understanding
of the opportunities and threats that arise when edge computing nodes are
deployed, and when AI/ML technologies are migrated to the periphery of IoT
networks.",arxiv
http://arxiv.org/abs/2103.13058v1,2021-03-24T10:13:20Z,2021-03-24T10:13:20Z,"Machine Learning based Indicators to Enhance Process Monitoring by
  Pattern Recognition","In industrial manufacturing, modern high-tech equipment delivers an
increasing volume of data, which exceeds the capacities of human observers.
Complex data formats like images make the detection of critical events
difficult and require pattern recognition, which is beyond the scope of
state-of-the-art process monitoring systems. Approaches that bridge the gap
between conventional statistical tools and novel machine learning (ML)
algorithms are required, but insufficiently studied. We propose a novel
framework for ML based indicators combining both concepts by two components:
pattern type and intensity. Conventional tools implement the intensity
component, while the pattern type accounts for error modes and tailors the
indicator to the production environment. In a case-study from semiconductor
industry, our framework goes beyond conventional process control and achieves
high quality experimental results. Thus, the suggested concept contributes to
the integration of ML in real-world process monitoring problems and paves the
way to automated decision support in manufacturing.",arxiv
http://arxiv.org/abs/2110.01167v1,2021-10-04T03:20:39Z,2021-10-04T03:20:39Z,Trustworthy AI: From Principles to Practices,"Fast developing artificial intelligence (AI) technology has enabled various
applied systems deployed in the real world, impacting people's everyday lives.
However, many current AI systems were found vulnerable to imperceptible
attacks, biased against underrepresented groups, lacking in user privacy
protection, etc., which not only degrades user experience but erodes the
society's trust in all AI systems. In this review, we strive to provide AI
practitioners a comprehensive guide towards building trustworthy AI systems. We
first introduce the theoretical framework of important aspects of AI
trustworthiness, including robustness, generalization, explainability,
transparency, reproducibility, fairness, privacy preservation, alignment with
human values, and accountability. We then survey leading approaches in these
aspects in the industry. To unify the current fragmented approaches towards
trustworthy AI, we propose a systematic approach that considers the entire
lifecycle of AI systems, ranging from data acquisition to model development, to
development and deployment, finally to continuous monitoring and governance. In
this framework, we offer concrete action items to practitioners and societal
stakeholders (e.g., researchers and regulators) to improve AI trustworthiness.
Finally, we identify key opportunities and challenges in the future development
of trustworthy AI systems, where we identify the need for paradigm shift
towards comprehensive trustworthy AI systems.",arxiv
http://arxiv.org/abs/1912.04900v1,2019-12-10T13:26:53Z,2019-12-10T13:26:53Z,Datamorphic Testing: A Methodology for Testing AI Applications,"With the rapid growth of the applications of machine learning (ML) and other
artificial intelligence (AI) techniques, adequate testing has become a
necessity to ensure their quality. This paper identifies the characteristics of
AI applications that distinguish them from traditional software, and analyses
the main difficulties in applying existing testing methods. Based on this
analysis, we propose a new method called datamorphic testing and illustrate the
method with an example of testing face recognition applications. We also report
an experiment with four real industrial application systems of face recognition
to validate the proposed approach.",arxiv
http://arxiv.org/abs/2004.03264v3,2020-08-21T04:12:15Z,2020-04-07T11:00:29Z,"Inspector Gadget: A Data Programming-based Labeling System for
  Industrial Images","As machine learning for images becomes democratized in the Software 2.0 era,
one of the serious bottlenecks is securing enough labeled data for training.
This problem is especially critical in a manufacturing setting where smart
factories rely on machine learning for product quality control by analyzing
industrial images. Such images are typically large and may only need to be
partially analyzed where only a small portion is problematic (e.g., identifying
defects on a surface). Since manual labeling these images is expensive, weak
supervision is an attractive alternative where the idea is to generate weak
labels that are not perfect, but can be produced at scale. Data programming is
a recent paradigm in this category where it uses human knowledge in the form of
labeling functions and combines them into a generative model. Data programming
has been successful in applications based on text or structured data and can
also be applied to images usually if one can find a way to convert them into
structured data. In this work, we expand the horizon of data programming by
directly applying it to images without this conversion, which is a common
scenario for industrial applications. We propose Inspector Gadget, an image
labeling system that combines crowdsourcing, data augmentation, and data
programming to produce weak labels at scale for image classification. We
perform experiments on real industrial image datasets and show that Inspector
Gadget obtains better performance than other weak-labeling techniques: Snuba,
GOGGLES, and self-learning baselines using convolutional neural networks (CNNs)
without pre-training.",arxiv
http://arxiv.org/abs/1902.02236v1,2019-02-06T15:38:06Z,2019-02-06T15:38:06Z,Dynamic Pricing for Airline Ancillaries with Customer Context,"Ancillaries have become a major source of revenue and profitability in the
travel industry. Yet, conventional pricing strategies are based on business
rules that are poorly optimized and do not respond to changing market
conditions. This paper describes the dynamic pricing model developed by Deepair
solutions, an AI technology provider for travel suppliers. We present a pricing
model that provides dynamic pricing recommendations specific to each customer
interaction and optimizes expected revenue per customer. The unique nature of
personalized pricing provides the opportunity to search over the market space
to find the optimal price-point of each ancillary for each customer, without
violating customer privacy. In this paper, we present and compare three
approaches for dynamic pricing of ancillaries, with increasing levels of
sophistication: (1) a two-stage forecasting and optimization model using a
logistic mapping function; (2) a two-stage model that uses a deep neural
network for forecasting, coupled with a revenue maximization technique using
discrete exhaustive search; (3) a single-stage end-to-end deep neural network
that recommends the optimal price. We describe the performance of these models
based on both offline and online evaluations. We also measure the real-world
business impact of these approaches by deploying them in an A/B test on an
airline's internet booking website. We show that traditional machine learning
techniques outperform human rule-based approaches in an online setting by
improving conversion by 36% and revenue per offer by 10%. We also provide
results for our offline experiments which show that deep learning algorithms
outperform traditional machine learning techniques for this problem. Our
end-to-end deep learning model is currently being deployed by the airline in
their booking system.",arxiv
http://arxiv.org/abs/2011.09926v2,2021-01-18T10:34:53Z,2020-11-18T16:20:28Z,Challenges in Deploying Machine Learning: a Survey of Case Studies,"In recent years, machine learning has received increased interest both as an
academic research field and as a solution for real-world business problems.
However, the deployment of machine learning models in production systems can
present a number of issues and concerns. This survey reviews published reports
of deploying machine learning solutions in a variety of use cases, industries
and applications and extracts practical considerations corresponding to stages
of the machine learning deployment workflow. Our survey shows that
practitioners face challenges at each stage of the deployment. The goal of this
paper is to layout a research agenda to explore approaches addressing these
challenges.",arxiv
http://arxiv.org/abs/2012.05410v1,2020-12-10T02:08:47Z,2020-12-10T02:08:47Z,Artificial Intelligence at the Edge,"The Internet of Things (IoT) and edge computing applications aim to support a
variety of societal needs, including the global pandemic situation that the
entire world is currently experiencing and responses to natural disasters.
  The need for real-time interactive applications such as immersive video
conferencing, augmented/virtual reality, and autonomous vehicles, in education,
healthcare, disaster recovery and other domains, has never been higher. At the
same time, there have been recent technological breakthroughs in highly
relevant fields such as artificial intelligence (AI)/machine learning (ML),
advanced communication systems (5G and beyond), privacy-preserving
computations, and hardware accelerators. 5G mobile communication networks
increase communication capacity, reduce transmission latency and error, and
save energy -- capabilities that are essential for new applications. The
envisioned future 6G technology will integrate many more technologies,
including for example visible light communication, to support groundbreaking
applications, such as holographic communications and high precision
manufacturing. Many of these applications require computations and analytics
close to application end-points: that is, at the edge of the network, rather
than in a centralized cloud. AI techniques applied at the edge have tremendous
potential both to power new applications and to need more efficient operation
of edge infrastructure. However, it is critical to understand where to deploy
AI systems within complex ecosystems consisting of advanced applications and
the specific real-time requirements towards AI systems.",arxiv
http://arxiv.org/abs/2012.01913v1,2020-12-03T13:51:05Z,2020-12-03T13:51:05Z,Transfer Learning as an Enabler of the Intelligent Digital Twin,"Digital Twins have been described as beneficial in many areas, such as
virtual commissioning, fault prediction or reconfiguration planning. Equipping
Digital Twins with artificial intelligence functionalities can greatly expand
those beneficial applications or open up altogether new areas of application,
among them cross-phase industrial transfer learning. In the context of machine
learning, transfer learning represents a set of approaches that enhance
learning new tasks based upon previously acquired knowledge. Here, knowledge is
transferred from one lifecycle phase to another in order to reduce the amount
of data or time needed to train a machine learning algorithm. Looking at common
challenges in developing and deploying industrial machinery with deep learning
functionalities, embracing this concept would offer several advantages: Using
an intelligent Digital Twin, learning algorithms can be designed, configured
and tested in the design phase before the physical system exists and real data
can be collected. Once real data becomes available, the algorithms must merely
be fine-tuned, significantly speeding up commissioning and reducing the
probability of costly modifications. Furthermore, using the Digital Twin's
simulation capabilities virtually injecting rare faults in order to train an
algorithm's response or using reinforcement learning, e.g. to teach a robot,
become practically feasible. This article presents several cross-phase
industrial transfer learning use cases utilizing intelligent Digital Twins. A
real cyber physical production system consisting of an automated welding
machine and an automated guided vehicle equipped with a robot arm is used to
illustrate the respective benefits.",arxiv
http://arxiv.org/abs/1804.05839v4,2019-11-05T13:12:43Z,2018-04-16T12:04:03Z,BigDL: A Distributed Deep Learning Framework for Big Data,"This paper presents BigDL (a distributed deep learning framework for Apache
Spark), which has been used by a variety of users in the industry for building
deep learning applications on production big data platforms. It allows deep
learning applications to run on the Apache Hadoop/Spark cluster so as to
directly process the production data, and as a part of the end-to-end data
analysis pipeline for deployment and management. Unlike existing deep learning
frameworks, BigDL implements distributed, data parallel training directly on
top of the functional compute model (with copy-on-write and coarse-grained
operations) of Spark. We also share real-world experience and ""war stories"" of
users that have adopted BigDL to address their challenges(i.e., how to easily
build end-to-end data analysis and deep learning pipelines for their production
data).",arxiv
http://arxiv.org/abs/1909.13343v2,2019-10-01T16:06:39Z,2019-09-29T19:15:08Z,"ISTHMUS: Secure, Scalable, Real-time and Robust Machine Learning
  Platform for Healthcare","In recent times, machine learning (ML) and artificial intelligence (AI) based
systems have evolved and scaled across different industries such as finance,
retail, insurance, energy utilities, etc. Among other things, they have been
used to predict patterns of customer behavior, to generate pricing models, and
to predict the return on investments. But the successes in deploying machine
learning models at scale in those industries have not translated into the
healthcare setting. There are multiple reasons why integrating ML models into
healthcare has not been widely successful, but from a technical perspective,
general-purpose commercial machine learning platforms are not a good fit for
healthcare due to complexities in handling data quality issues, mandates to
demonstrate clinical relevance, and a lack of ability to monitor performance in
a highly regulated environment with stringent security and privacy needs. In
this paper, we describe Isthmus, a turnkey, cloud-based platform which
addresses the challenges above and reduces time to market for operationalizing
ML/AI in healthcare. Towards the end, we describe three case studies which shed
light on Isthmus capabilities. These include (1) supporting an end-to-end
lifecycle of a model which predicts trauma survivability at hospital trauma
centers, (2) bringing in and harmonizing data from disparate sources to create
a community data platform for inferring population as well as patient level
insights for Social Determinants of Health (SDoH), and (3) ingesting
live-streaming data from various IoT sensors to build models, which can
leverage real-time and longitudinal information to make advanced time-sensitive
predictions.",arxiv
http://arxiv.org/abs/1905.10090v1,2019-05-24T08:45:56Z,2019-05-24T08:45:56Z,Deploying AI Frameworks on Secure HPC Systems with Containers,"The increasing interest in the usage of Artificial Intelligence techniques
(AI) from the research community and industry to tackle ""real world"" problems,
requires High Performance Computing (HPC) resources to efficiently compute and
scale complex algorithms across thousands of nodes. Unfortunately, typical data
scientists are not familiar with the unique requirements and characteristics of
HPC environments. They usually develop their applications with high-level
scripting languages or frameworks such as TensorFlow and the installation
process often requires connection to external systems to download open source
software during the build. HPC environments, on the other hand, are often based
on closed source applications that incorporate parallel and distributed
computing API's such as MPI and OpenMP, while users have restricted
administrator privileges, and face security restrictions such as not allowing
access to external systems. In this paper we discuss the issues associated with
the deployment of AI frameworks in a secure HPC environment and how we
successfully deploy AI frameworks on SuperMUC-NG with Charliecloud.",arxiv
http://arxiv.org/abs/2008.08525v1,2020-08-19T16:05:58Z,2020-08-19T16:05:58Z,"""Name that manufacturer"". Relating image acquisition bias with task
  complexity when training deep learning models: experiments on head CT","As interest in applying machine learning techniques for medical images
continues to grow at a rapid pace, models are starting to be developed and
deployed for clinical applications. In the clinical AI model development
lifecycle (described by Lu et al. [1]), a crucial phase for machine learning
scientists and clinicians is the proper design and collection of the data
cohort. The ability to recognize various forms of biases and distribution
shifts in the dataset is critical at this step. While it remains difficult to
account for all potential sources of bias, techniques can be developed to
identify specific types of bias in order to mitigate their impact. In this work
we analyze how the distribution of scanner manufacturers in a dataset can
contribute to the overall bias of deep learning models. We evaluate
convolutional neural networks (CNN) for both classification and segmentation
tasks, specifically two state-of-the-art models: ResNet [2] for classification
and U-Net [3] for segmentation. We demonstrate that CNNs can learn to
distinguish the imaging scanner manufacturer and that this bias can
substantially impact model performance for both classification and segmentation
tasks. By creating an original synthesis dataset of brain data mimicking the
presence of more or less subtle lesions we also show that this bias is related
to the difficulty of the task. Recognition of such bias is critical to develop
robust, generalizable models that will be crucial for clinical applications in
real-world data distributions.",arxiv
http://arxiv.org/abs/2009.00351v1,2020-09-01T11:10:13Z,2020-09-01T11:10:13Z,"Advancing from Predictive Maintenance to Intelligent Maintenance with AI
  and IIoT","As Artificial Intelligent (AI) technology advances and increasingly large
amounts of data become readily available via various Industrial Internet of
Things (IIoT) projects, we evaluate the state of the art of predictive
maintenance approaches and propose our innovative framework to improve the
current practice. The paper first reviews the evolution of reliability
modelling technology in the past 90 years and discusses major technologies
developed in industry and academia. We then introduce the next generation
maintenance framework - Intelligent Maintenance, and discuss its key
components. This AI and IIoT based Intelligent Maintenance framework is
composed of (1) latest machine learning algorithms including probabilistic
reliability modelling with deep learning, (2) real-time data collection,
transfer, and storage through wireless smart sensors, (3) Big Data
technologies, (4) continuously integration and deployment of machine learning
models, (5) mobile device and AR/VR applications for fast and better
decision-making in the field. Particularly, we proposed a novel probabilistic
deep learning reliability modelling approach and demonstrate it in the Turbofan
Engine Degradation Dataset.",arxiv
http://arxiv.org/abs/2004.10251v1,2020-04-21T19:40:16Z,2020-04-21T19:40:16Z,"Industrial Robot Grasping with Deep Learning using a Programmable Logic
  Controller (PLC)","Universal grasping of a diverse range of previously unseen objects from heaps
is a grand challenge in e-commerce order fulfillment, manufacturing, and home
service robotics. Recently, deep learning based grasping approaches have
demonstrated results that make them increasingly interesting for industrial
deployments. This paper explores the problem from an automation systems
point-of-view. We develop a robotics grasping system using Dex-Net, which is
fully integrated at the controller level. Two neural networks are deployed on a
novel industrial AI hardware acceleration module close to a PLC with a power
footprint of less than 10 W for the overall system. The software is tightly
integrated with the hardware allowing for fast and efficient data processing
and real-time communication. The success rate of grasping an object form a bin
is up to 95 percent with more than 350 picks per hour, if object and receptive
bins are in close proximity. The system was presented at the Hannover Fair 2019
(world s largest industrial trade fair) and other events, where it performed
over 5,000 grasps per event.",arxiv
http://arxiv.org/abs/1908.08998v2,2019-10-23T14:39:47Z,2019-08-13T10:15:39Z,AIBench: An Industry Standard Internet Service AI Benchmark Suite,"Today's Internet Services are undergoing fundamental changes and shifting to
an intelligent computing era where AI is widely employed to augment services.
In this context, many innovative AI algorithms, systems, and architectures are
proposed, and thus the importance of benchmarking and evaluating them rises.
However, modern Internet services adopt a microservice-based architecture and
consist of various modules. The diversity of these modules and complexity of
execution paths, the massive scale and complex hierarchy of datacenter
infrastructure, the confidential issues of data sets and workloads pose great
challenges to benchmarking. In this paper, we present the first
industry-standard Internet service AI benchmark suite---AIBench with seventeen
industry partners, including several top Internet service providers. AIBench
provides a highly extensible, configurable, and flexible benchmark framework
that contains loosely coupled modules. We identify sixteen prominent AI problem
domains like learning to rank, each of which forms an AI component benchmark,
from three most important Internet service domains: search engine, social
network, and e-commerce, which is by far the most comprehensive AI benchmarking
effort. On the basis of the AIBench framework, abstracting the real-world data
sets and workloads from one of the top e-commerce providers, we design and
implement the first end-to-end Internet service AI benchmark, which contains
the primary modules in the critical paths of an industry scale application and
is scalable to deploy on different cluster scales. The specifications, source
code, and performance numbers are publicly available from the benchmark council
web site http://www.benchcouncil.org/AIBench/index.html.",arxiv
http://arxiv.org/abs/2101.03747v1,2021-01-11T08:14:35Z,2021-01-11T08:14:35Z,Cognitive Visual Inspection Service for LCD Manufacturing Industry,"With the rapid growth of display devices, quality inspection via machine
vision technology has become increasingly important for flat-panel displays
(FPD) industry. This paper discloses a novel visual inspection system for
liquid crystal display (LCD), which is currently a dominant type in the FPD
industry. The system is based on two cornerstones: robust/high-performance
defect recognition model and cognitive visual inspection service architecture.
A hybrid application of conventional computer vision technique and the latest
deep convolutional neural network (DCNN) leads to an integrated defect
detection, classfication and impact evaluation model that can be economically
trained with only image-level class annotations to achieve a high inspection
accuracy. In addition, the properly trained model is robust to the variation of
the image qulity, significantly alleviating the dependency between the model
prediction performance and the image aquisition environment. This in turn
justifies the decoupling of the defect recognition functions from the front-end
device to the back-end serivce, motivating the design and realization of the
cognitive visual inspection service architecture. Empirical case study is
performed on a large-scale real-world LCD dataset from a manufacturing line
with different layers and products, which shows the promising utility of our
system, which has been deployed in a real-world LCD manufacturing line from a
major player in the world.",arxiv
http://arxiv.org/abs/1907.11778v1,2019-07-26T20:03:26Z,2019-07-26T20:03:26Z,"An Encoder-Decoder Based Approach for Anomaly Detection with Application
  in Additive Manufacturing","We present a novel unsupervised deep learning approach that utilizes the
encoder-decoder architecture for detecting anomalies in sequential sensor data
collected during industrial manufacturing. Our approach is designed not only to
detect whether there exists an anomaly at a given time step, but also to
predict what will happen next in the (sequential) process. We demonstrate our
approach on a dataset collected from a real-world testbed. The dataset contains
images collected under both normal conditions and synthetic anomalies. We show
that the encoder-decoder model is able to identify the injected anomalies in a
modern manufacturing process in an unsupervised fashion. In addition, it also
gives hints about the temperature non-uniformity of the testbed during
manufacturing, which is what we are not aware of before doing the experiment.",arxiv
http://arxiv.org/abs/2103.02938v1,2021-03-04T10:38:46Z,2021-03-04T10:38:46Z,FootApp: an AI-Powered System for Football Match Annotation,"In the last years, scientific and industrial research has experienced a
growing interest in acquiring large annotated data sets to train artificial
intelligence algorithms for tackling problems in different domains. In this
context, we have observed that even the market for football data has
substantially grown. The analysis of football matches relies on the annotation
of both individual players' and team actions, as well as the athletic
performance of players. Consequently, annotating football events at a
fine-grained level is a very expensive and error-prone task. Most existing
semi-automatic tools for football match annotation rely on cameras and computer
vision. However, those tools fall short in capturing team dynamics, and in
extracting data of players who are not visible in the camera frame. To address
these issues, in this manuscript we present FootApp, an AI-based system for
football match annotation. First, our system relies on an advanced and mixed
user interface that exploits both vocal and touch interaction. Second, the
motor performance of players is captured and processed by applying machine
learning algorithms to data collected from inertial sensors worn by players.
Artificial intelligence techniques are then used to check the consistency of
generated labels, including those regarding the physical activity of players,
to automatically recognize annotation errors. Notably, we implemented a full
prototype of the proposed system, performing experiments to show its
effectiveness in a real-world adoption scenario.",arxiv
http://arxiv.org/abs/1810.09957v1,2018-10-08T04:30:44Z,2018-10-08T04:30:44Z,NSML: Meet the MLaaS platform with a real-world case study,"The boom of deep learning induced many industries and academies to introduce
machine learning based approaches into their concern, competitively. However,
existing machine learning frameworks are limited to sufficiently fulfill the
collaboration and management for both data and models. We proposed NSML, a
machine learning as a service (MLaaS) platform, to meet these demands. NSML
helps machine learning work be easily launched on a NSML cluster and provides a
collaborative environment which can afford development at enterprise scale.
Finally, NSML users can deploy their own commercial services with NSML cluster.
In addition, NSML furnishes convenient visualization tools which assist the
users in analyzing their work. To verify the usefulness and accessibility of
NSML, we performed some experiments with common examples. Furthermore, we
examined the collaborative advantages of NSML through three competitions with
real-world use cases.",arxiv
http://arxiv.org/abs/2110.11573v1,2021-10-22T03:52:45Z,2021-10-22T03:52:45Z,"ModEL: A Modularized End-to-end Reinforcement Learning Framework for
  Autonomous Driving","Heated debates continue over the best autonomous driving framework. The
classic modular pipeline is widely adopted in the industry owing to its great
interpretability and stability, whereas the end-to-end paradigm has
demonstrated considerable simplicity and learnability along with the rise of
deep learning. We introduce a new modularized end-to-end reinforcement learning
framework (ModEL) for autonomous driving, which combines the merits of both
previous approaches. The autonomous driving stack of ModEL is decomposed into
perception, planning, and control module, leveraging scene understanding,
end-to-end reinforcement learning, and PID control respectively. Furthermore,
we build a fully functional autonomous vehicle to deploy this framework.
Through extensive simulation and real-world experiments, our framework has
shown great generalizability to various complicated scenarios and outperforms
the competing baselines.",arxiv
http://arxiv.org/abs/2107.12433v1,2021-07-26T18:52:00Z,2021-07-26T18:52:00Z,"The Graph Neural Networking Challenge: A Worldwide Competition for
  Education in AI/ML for Networks","During the last decade, Machine Learning (ML) has increasingly become a hot
topic in the field of Computer Networks and is expected to be gradually adopted
for a plethora of control, monitoring and management tasks in real-world
deployments. This poses the need to count on new generations of students,
researchers and practitioners with a solid background in ML applied to
networks. During 2020, the International Telecommunication Union (ITU) has
organized the ""ITU AI/ML in 5G challenge'', an open global competition that has
introduced to a broad audience some of the current main challenges in ML for
networks. This large-scale initiative has gathered 23 different challenges
proposed by network operators, equipment manufacturers and academia, and has
attracted a total of 1300+ participants from 60+ countries. This paper narrates
our experience organizing one of the proposed challenges: the ""Graph Neural
Networking Challenge 2020''. We describe the problem presented to participants,
the tools and resources provided, some organization aspects and participation
statistics, an outline of the top-3 awarded solutions, and a summary with some
lessons learned during all this journey. As a result, this challenge leaves a
curated set of educational resources openly available to anyone interested in
the topic.",arxiv
http://arxiv.org/abs/2011.09463v3,2021-08-20T07:24:05Z,2020-11-18T18:41:27Z,"EasyTransfer -- A Simple and Scalable Deep Transfer Learning Platform
  for NLP Applications","The literature has witnessed the success of leveraging Pre-trained Language
Models (PLMs) and Transfer Learning (TL) algorithms to a wide range of Natural
Language Processing (NLP) applications, yet it is not easy to build an
easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the
EasyTransfer platform is designed to develop deep TL algorithms for NLP
applications. EasyTransfer is backended with a high-performance and scalable
engine for efficient training and inference, and also integrates comprehensive
deep TL algorithms, to make the development of industrial-scale TL applications
easier. In EasyTransfer, the built-in data and model parallelism strategies,
combined with AI compiler optimization, show to be 4.0x faster than the
community version of distributed training. EasyTransfer supports various NLP
models in the ModelZoo, including mainstream PLMs and multi-modality models. It
also features various in-house developed TL algorithms, together with the
AppZoo for NLP applications. The toolkit is convenient for users to quickly
start model training, evaluation, and online deployment. EasyTransfer is
currently deployed at Alibaba to support a variety of business scenarios,
including item recommendation, personalized search, conversational question
answering, etc. Extensive experiments on real-world datasets and online
applications show that EasyTransfer is suitable for online production with
cutting-edge performance for various applications. The source code of
EasyTransfer is released at Github (https://github.com/alibaba/EasyTransfer).",arxiv
http://arxiv.org/abs/2012.01148v2,2021-01-01T18:06:59Z,2020-11-30T05:46:14Z,Applied Machine Learning for Games: A Graduate School Course,"The game industry is moving into an era where old-style game engines are
being replaced by re-engineered systems with embedded machine learning
technologies for the operation, analysis and understanding of game play. In
this paper, we describe our machine learning course designed for graduate
students interested in applying recent advances of deep learning and
reinforcement learning towards gaming. This course serves as a bridge to foster
interdisciplinary collaboration among graduate schools and does not require
prior experience designing or building games. Graduate students enrolled in
this course apply different fields of machine learning techniques such as
computer vision, natural language processing, computer graphics, human computer
interaction, robotics and data analysis to solve open challenges in gaming.
Student projects cover use-cases such as training AI-bots in gaming benchmark
environments and competitions, understanding human decision patterns in gaming,
and creating intelligent non-playable characters or environments to foster
engaging gameplay. Projects demos can help students open doors for an industry
career, aim for publications, or lay the foundations of a future product. Our
students gained hands-on experience in applying state of the art machine
learning techniques to solve real-life problems in gaming.",arxiv
http://arxiv.org/abs/2007.01097v1,2020-07-01T08:47:46Z,2020-07-01T08:47:46Z,"PrototypeML: A Neural Network Integrated Design and Development
  Environment","Neural network architectures are most often conceptually designed and
described in visual terms, but are implemented by writing error-prone code.
PrototypeML is a machine learning development environment that bridges the
dichotomy between the design and development processes: it provides a highly
intuitive visual neural network design interface that supports (yet abstracts)
the full capabilities of the PyTorch deep learning framework, reduces model
design and development time, makes debugging easier, and automates many
framework and code writing idiosyncrasies. In this paper, we detail the deep
learning development deficiencies that drove the implementation of PrototypeML,
and propose a hybrid approach to resolve these issues without limiting network
expressiveness or reducing code quality. We demonstrate the real-world benefits
of a visual approach to neural network design for research, industry and
teaching. Available at https://PrototypeML.com",arxiv
http://arxiv.org/abs/1811.02213v1,2018-11-06T08:05:24Z,2018-11-06T08:05:24Z,"Hybrid Approach to Automation, RPA and Machine Learning: a Method for
  the Human-centered Design of Software Robots","One of the more prominent trends within Industry 4.0 is the drive to employ
Robotic Process Automation (RPA), especially as one of the elements of the Lean
approach. The full implementation of RPA is riddled with challenges relating
both to the reality of everyday business operations, from SMEs to SSCs and
beyond, and the social effects of the changing job market. To successfully
address these points there is a need to develop a solution that would adjust to
the existing business operations and at the same time lower the negative social
impact of the automation process.
  To achieve these goals we propose a hybrid, human-centered approach to the
development of software robots. This design and implementation method combines
the Living Lab approach with empowerment through participatory design to
kick-start the co-development and co-maintenance of hybrid software robots
which, supported by variety of AI methods and tools, including interactive and
collaborative ML in the cloud, transform menial job posts into higher-skilled
positions, allowing former employees to stay on as robot co-designers and
maintainers, i.e. as co-programmers who supervise the machine learning
processes with the use of tailored high-level RPA Domain Specific Languages
(DSLs) to adjust the functioning of the robots and maintain operational
flexibility.",arxiv
http://arxiv.org/abs/2004.13401v1,2020-04-28T10:13:00Z,2020-04-28T10:13:00Z,CmnRec: Sequential Recommendations with Chunk-accelerated Memory Network,"Recently, Memory-based Neural Recommenders (MNR) have demonstrated superior
predictive accuracy in the task of sequential recommendations, particularly for
modeling long-term item dependencies. However, typical MNR requires complex
memory access operations, i.e., both writing and reading via a controller
(e.g., RNN) at every time step. Those frequent operations will dramatically
increase the network training time, resulting in the difficulty in being
deployed on industrial-scale recommender systems. In this paper, we present a
novel general Chunk framework to accelerate MNR significantly. Specifically,
our framework divides proximal information units into chunks, and performs
memory access at certain time steps, whereby the number of memory operations
can be greatly reduced. We investigate two ways to implement effective
chunking, i.e., PEriodic Chunk (PEC) and Time-Sensitive Chunk (TSC), to
preserve and recover important recurrent signals in the sequence. Since
chunk-accelerated MNR models take into account more proximal information units
than that from a single timestep, it can remove the influence of noise in the
item sequence to a large extent, and thus improve the stability of MNR. In this
way, the proposed chunk mechanism can lead to not only faster training and
prediction, but even slightly better results. The experimental results on three
real-world datasets (weishi, ml-10M and ml-latest) show that our chunk
framework notably reduces the running time (e.g., with up to 7x for training &
10x for inference on ml-latest) of MNR, and meantime achieves competitive
performance.",arxiv
http://arxiv.org/abs/2004.10908v4,2021-09-06T18:36:40Z,2020-04-23T00:21:05Z,"Taskflow: A Lightweight Parallel and Heterogeneous Task Graph Computing
  System","Taskflow aims to streamline the building of parallel and heterogeneous
applications using a lightweight task graph-based approach. Taskflow introduces
an expressive task graph programming model to assist developers in the
implementation of parallel and heterogeneous decomposition strategies on a
heterogeneous computing platform. Our programming model distinguishes itself as
a very general class of task graph parallelism with in-graph control flow to
enable end-to-end parallel optimization. To support our model with high
performance, we design an efficient system runtime that solves many of the new
scheduling challenges arising out of our models and optimizes the performance
across latency, energy efficiency, and throughput. We have demonstrated the
promising performance of Taskflow in real-world applications. As an example,
Taskflow solves a large-scale machine learning workload up to 29% faster, 1.5x
less memory, and 1.9x higher throughput than the industrial system, oneTBB, on
a machine of 40 CPUs and 4 GPUs. We have opened the source of Taskflow and
deployed it to large numbers of users in the open-source community.",arxiv
http://arxiv.org/abs/1709.09480v2,2018-02-06T10:59:19Z,2017-09-27T13:03:52Z,A Benchmark Environment Motivated by Industrial Control Problems,"In the research area of reinforcement learning (RL), frequently novel and
promising methods are developed and introduced to the RL community. However,
although many researchers are keen to apply their methods on real-world
problems, implementing such methods in real industry environments often is a
frustrating and tedious process. Generally, academic research groups have only
limited access to real industrial data and applications. For this reason, new
methods are usually developed, evaluated and compared by using artificial
software benchmarks. On one hand, these benchmarks are designed to provide
interpretable RL training scenarios and detailed insight into the learning
process of the method on hand. On the other hand, they usually do not share
much similarity with industrial real-world applications. For this reason we
used our industry experience to design a benchmark which bridges the gap
between freely available, documented, and motivated artificial benchmarks and
properties of real industrial problems. The resulting industrial benchmark (IB)
has been made publicly available to the RL community by publishing its Java and
Python code, including an OpenAI Gym wrapper, on Github. In this paper we
motivate and describe in detail the IB's dynamics and identify prototypic
experimental settings that capture common situations in real-world industry
control problems.",arxiv
http://arxiv.org/abs/2101.03889v2,2021-02-16T13:44:56Z,2020-12-21T10:25:27Z,A Comprehensive Survey of 6G Wireless Communications,"While fifth-generation (5G) communications are being rolled out worldwide,
sixth-generation (6G) communications have attracted much attention from both
the industry and the academia. Compared with 5G, 6G will have a wider frequency
band, higher transmission rate, spectrum efficiency, greater connection
capacity, shorter delay, broader coverage, and more robust anti-interference
capability to satisfy various network requirements. This survey presents an
insightful understanding of 6G wireless communications by introducing
requirements, features, critical technologies, challenges, and applications.
First, we give an overview of 6G from perspectives of technologies, security
and privacy, and applications. Subsequently, we introduce various 6G
technologies and their existing challenges in detail, e.g., artificial
intelligence (AI), intelligent surfaces, THz, space-air-ground-sea integrated
network, cell-free massive MIMO, etc. Because of these technologies, 6G is
expected to outperform existing wireless communication systems regarding the
transmission rate, latency, global coverage, etc. Next, we discuss security and
privacy techniques that can be applied to protect data in 6G. Since edge
devices are expected to gain popularity soon, the vast amount of generated data
and frequent data exchange make the leakage of data easily. Finally, we predict
real-world applications built on the technologies and features of 6G; for
example, smart healthcare, smart city, and smart manufacturing will be
implemented by taking advantage of AI.",arxiv
http://arxiv.org/abs/1505.04813v1,2015-05-19T01:17:47Z,2015-05-19T01:17:47Z,"What is Learning? A primary discussion about information and
  Representation","Nowadays, represented by Deep Learning techniques, the field of machine
learning is experiencing unprecedented prosperity and its influence is
demonstrated in academia, industry and civil society. ""Intelligent"" has become
a label which could not be neglected for most applications; celebrities and
scientists also warned that the development of full artificial intelligence may
spell the end of the human race. It seems that the answer to building a
computer system that could automatically improve with experience is right on
the next corner. While for AI and machine learning researchers, it is a
consensus that we are not anywhere near the core technique which could bring
the Terminator, Number 5 or R2D2 into real life, and there is not even a formal
definition about what is intelligence, or one of its basic properties:
Learning. Therefore, even though researchers know these concerns are not
necessary currently, there is no generalized explanation about why these
concerns are not necessary, and what properties people should take into account
that would make these concerns to be necessary. In this paper, starts from
analysing the relation between information and its representation, a necessary
condition for a model to be a learning model is proposed. This condition and
related future works could be used to verify whether a system is able to learn
or not, and enrich our understanding of learning: one important property of
Intelligence.",arxiv
http://arxiv.org/abs/1901.10281v1,2019-01-29T13:43:57Z,2019-01-29T13:43:57Z,Structural Material Property Tailoring Using Deep Neural Networks,"Advances in robotics, artificial intelligence, and machine learning are
ushering in a new age of automation, as machines match or outperform human
performance. Machine intelligence can enable businesses to improve performance
by reducing errors, improving sensitivity, quality and speed, and in some cases
achieving outcomes that go beyond current resource capabilities. Relevant
applications include new product architecture design, rapid material
characterization, and life-cycle management tied with a digital strategy that
will enable efficient development of products from cradle to grave. In
addition, there are also challenges to overcome that must be addressed through
a major, sustained research effort that is based solidly on both inferential
and computational principles applied to design tailoring of functionally
optimized structures. Current applications of structural materials in the
aerospace industry demand the highest quality control of material
microstructure, especially for advanced rotational turbomachinery in aircraft
engines in order to have the best tailored material property. In this paper,
deep convolutional neural networks were developed to accurately predict
processing-structure-property relations from materials microstructures images,
surpassing current best practices and modeling efforts. The models
automatically learn critical features, without the need for manual
specification and/or subjective and expensive image analysis. Further, in
combination with generative deep learning models, a framework is proposed to
enable rapid material design space exploration and property identification and
optimization. The implementation must take account of real-time decision cycles
and the trade-offs between speed and accuracy.",arxiv
http://arxiv.org/abs/2008.10713v1,2020-08-24T21:29:56Z,2020-08-24T21:29:56Z,"Dynamic Dispatching for Large-Scale Heterogeneous Fleet via Multi-agent
  Deep Reinforcement Learning","Dynamic dispatching is one of the core problems for operation optimization in
traditional industries such as mining, as it is about how to smartly allocate
the right resources to the right place at the right time. Conventionally, the
industry relies on heuristics or even human intuitions which are often
short-sighted and sub-optimal solutions. Leveraging the power of AI and
Internet of Things (IoT), data-driven automation is reshaping this area.
However, facing its own challenges such as large-scale and heterogenous trucks
running in a highly dynamic environment, it can barely adopt methods developed
in other domains (e.g., ride-sharing). In this paper, we propose a novel Deep
Reinforcement Learning approach to solve the dynamic dispatching problem in
mining. We first develop an event-based mining simulator with parameters
calibrated in real mines. Then we propose an experience-sharing Deep Q Network
with a novel abstract state/action representation to learn memories from
heterogeneous agents altogether and realizes learning in a centralized way. We
demonstrate that the proposed methods significantly outperform the most widely
adopted approaches in the industry by $5.56\%$ in terms of productivity. The
proposed approach has great potential in a broader range of industries (e.g.,
manufacturing, logistics) which have a large-scale of heterogenous equipment
working in a highly dynamic environment, as a general framework for dynamic
resource allocation.",arxiv
http://arxiv.org/abs/2103.16938v1,2021-03-31T09:43:38Z,2021-03-31T09:43:38Z,"Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein
  GANs","Real-time estimation of actual environment depth is an essential module for
various autonomous system tasks such as localization, obstacle detection and
pose estimation. During the last decade of machine learning, extensive
deployment of deep learning methods to computer vision tasks yielded successful
approaches for realistic depth synthesis out of a simple RGB modality. While
most of these models rest on paired depth data or availability of video
sequences and stereo images, there is a lack of methods facing single-image
depth synthesis in an unsupervised manner. Therefore, in this study, latest
advancements in the field of generative neural networks are leveraged to fully
unsupervised single-image depth synthesis. To be more exact, two
cycle-consistent generators for RGB-to-depth and depth-to-RGB transfer are
implemented and simultaneously optimized using the Wasserstein-1 distance. To
ensure plausibility of the proposed method, we apply the models to a self
acquised industrial data set as well as to the renown NYU Depth v2 data set,
which allows comparison with existing approaches. The observed success in this
study suggests high potential for unpaired single-image depth estimation in
real world applications.",arxiv
http://arxiv.org/abs/2008.04130v1,2020-08-10T13:46:28Z,2020-08-10T13:46:28Z,Bilevel Learning Model Towards Industrial Scheduling,"Automatic industrial scheduling, aiming at optimizing the sequence of jobs
over limited resources, is widely needed in manufacturing industries. However,
existing scheduling systems heavily rely on heuristic algorithms, which either
generate ineffective solutions or compute inefficiently when job scale
increases. Thus, it is of great importance to develop new large-scale
algorithms that are not only efficient and effective, but also capable of
satisfying complex constraints in practice. In this paper, we propose a Bilevel
Deep reinforcement learning Scheduler, \textit{BDS}, in which the higher level
is responsible for exploring an initial global sequence, whereas the lower
level is aiming at exploitation for partial sequence refinements, and the two
levels are connected by a sliding-window sampling mechanism. In the
implementation, a Double Deep Q Network (DDQN) is used in the upper level and
Graph Pointer Network (GPN) lies within the lower level. After the theoretical
guarantee for the convergence of BDS, we evaluate it in an industrial automatic
warehouse scenario, with job number up to $5000$ in each production line. It is
shown that our proposed BDS significantly outperforms two most used heuristics,
three strong deep networks, and another bilevel baseline approach. In
particular, compared with the most used greedy-based heuristic algorithm in
real world which takes nearly an hour, our BDS can decrease the makespan by
27.5\%, 28.6\% and 22.1\% for 3 largest datasets respectively, with
computational time less than 200 seconds.",arxiv
http://arxiv.org/abs/2008.13223v2,2020-10-25T17:47:42Z,2020-08-30T17:29:43Z,"Deep Reinforcement Learning for Contact-Rich Skills Using Compliant
  Movement Primitives","In recent years, industrial robots have been installed in various industries
to handle advanced manufacturing and high precision tasks. However, further
integration of industrial robots is hampered by their limited flexibility,
adaptability and decision making skills compared to human operators. Assembly
tasks are especially challenging for robots since they are contact-rich and
sensitive to even small uncertainties. While reinforcement learning (RL) offers
a promising framework to learn contact-rich control policies from scratch, its
applicability to high-dimensional continuous state-action spaces remains rather
limited due to high brittleness and sample complexity. To address those issues,
we propose different pruning methods that facilitate convergence and
generalization. In particular, we divide the task into free and contact-rich
sub-tasks, perform the control in Cartesian rather than joint space, and
parameterize the control policy. Those pruning methods are naturally
implemented within the framework of dynamic movement primitives (DMP). To
handle contact-rich tasks, we extend the DMP framework by introducing a
coupling term that acts like the human wrist and provides active compliance
under contact with the environment. We demonstrate that the proposed method can
learn insertion skills that are invariant to space, size, shape, and closely
related scenarios, while handling large uncertainties. Finally we demonstrate
that the learned policy can be easily transferred from simulations to real
world and achieve similar performance on UR5e robot.",arxiv
http://arxiv.org/abs/1910.12695v2,2019-11-25T17:07:00Z,2019-10-28T14:11:26Z,AI Ethics in Industry: A Research Framework,"Artificial Intelligence (AI) systems exert a growing influence on our
society. As they become more ubiquitous, their potential negative impacts also
become evident through various real-world incidents. Following such early
incidents, academic and public discussion on AI ethics has highlighted the need
for implementing ethics in AI system development. However, little currently
exists in the way of frameworks for understanding the practical implementation
of AI ethics. In this paper, we discuss a research framework for implementing
AI ethics in industrial settings. The framework presents a starting point for
empirical studies into AI ethics but is still being developed further based on
its practical utilization.",arxiv
http://arxiv.org/abs/2004.14850v1,2020-04-30T15:02:08Z,2020-04-30T15:02:08Z,6G White Paper on Edge Intelligence,"In this white paper we provide a vision for 6G Edge Intelligence. Moving
towards 5G and beyond the future 6G networks, intelligent solutions utilizing
data-driven machine learning and artificial intelligence become crucial for
several real-world applications including but not limited to, more efficient
manufacturing, novel personal smart device environments and experiences, urban
computing and autonomous traffic settings. We present edge computing along with
other 6G enablers as a key component to establish the future 2030 intelligent
Internet technologies as shown in this series of 6G White Papers.
  In this white paper, we focus in the domains of edge computing infrastructure
and platforms, data and edge network management, software development for edge,
and real-time and distributed training of ML/AI algorithms, along with
security, privacy, pricing, and end-user aspects. We discuss the key enablers
and challenges and identify the key research questions for the development of
the Intelligent Edge services. As a main outcome of this white paper, we
envision a transition from Internet of Things to Intelligent Internet of
Intelligent Things and provide a roadmap for development of 6G Intelligent
Edge.",arxiv
http://arxiv.org/abs/2102.11492v2,2021-02-24T04:05:07Z,2021-02-23T04:55:12Z,"DeepThermal: Combustion Optimization for Thermal Power Generating Units
  Using Offline Reinforcement Learning","Thermal power generation plays a dominant role in the world's electricity
supply. It consumes large amounts of coal worldwide, and causes serious air
pollution. Optimizing the combustion efficiency of a thermal power generating
unit (TPGU) is a highly challenging and critical task in the energy industry.
We develop a new data-driven AI system, namely DeepThermal, to optimize the
combustion control strategy for TPGUs. At its core, is a new model-based
offline reinforcement learning (RL) framework, called MORE, which leverages
logged historical operational data of a TPGU to solve a highly complex
constrained Markov decision process problem via purely offline training. MORE
aims at simultaneously improving the long-term reward (increase combustion
efficiency and reduce pollutant emission) and controlling operational risks
(safety constraints satisfaction). In DeepThermal, we first learn a data-driven
combustion process simulator from the offline dataset. The RL agent of MORE is
then trained by combining real historical data as well as carefully filtered
and processed simulation data through a novel restrictive exploration scheme.
DeepThermal has been successfully deployed in four large coal-fired thermal
power plants in China. Real-world experiments show that DeepThermal effectively
improves the combustion efficiency of a TPGU. We also report and demonstrate
the superior performance of MORE by comparing with the state-of-the-art
algorithms on the standard offline RL benchmarks. To the best knowledge of the
authors, DeepThermal is the first AI application that has been used to solve
real-world complex mission-critical control tasks using the offline RL
approach.",arxiv
http://arxiv.org/abs/2012.06753v2,2020-12-20T13:23:54Z,2020-12-12T08:08:47Z,"Towards Neurohaptics: Brain-Computer Interfaces for Decoding Intuitive
  Sense of Touch","Noninvasive brain-computer interface (BCI) is widely used to recognize users'
intentions. Especially, BCI related to tactile and sensation decoding could
provide various effects on many industrial fields such as manufacturing
advanced touch displays, controlling robotic devices, and more immersive
virtual reality or augmented reality. In this paper, we introduce haptic and
sensory perception-based BCI systems called neurohaptics. It is a preliminary
study for a variety of scenarios using actual touch and touch imagery
paradigms. We designed a novel experimental environment and a device that could
acquire brain signals under touching designated materials to generate natural
touch and texture sensations. Through the experiment, we collected the
electroencephalogram (EEG) signals with respect to four different texture
objects. Seven subjects were recruited for the experiment and evaluated
classification performances using machine learning and deep learning
approaches. Hence, we could confirm the feasibility of decoding actual touch
and touch imagery on EEG signals to develop practical neurohaptics.",arxiv
http://arxiv.org/abs/1711.09279v1,2017-11-25T20:11:41Z,2017-11-25T20:11:41Z,A Big Data Analysis Framework Using Apache Spark and Deep Learning,"With the spreading prevalence of Big Data, many advances have recently been
made in this field. Frameworks such as Apache Hadoop and Apache Spark have
gained a lot of traction over the past decades and have become massively
popular, especially in industries. It is becoming increasingly evident that
effective big data analysis is key to solving artificial intelligence problems.
Thus, a multi-algorithm library was implemented in the Spark framework, called
MLlib. While this library supports multiple machine learning algorithms, there
is still scope to use the Spark setup efficiently for highly time-intensive and
computationally expensive procedures like deep learning. In this paper, we
propose a novel framework that combines the distributive computational
abilities of Apache Spark and the advanced machine learning architecture of a
deep multi-layer perceptron (MLP), using the popular concept of Cascade
Learning. We conduct empirical analysis of our framework on two real world
datasets. The results are encouraging and corroborate our proposed framework,
in turn proving that it is an improvement over traditional big data analysis
methods that use either Spark or Deep learning as individual elements.",arxiv
http://arxiv.org/abs/1909.02129v1,2019-09-04T21:50:03Z,2019-09-04T21:50:03Z,"Towards Precise Robotic Grasping by Probabilistic Post-grasp
  Displacement Estimation","Precise robotic grasping is important for many industrial applications, such
as assembly and palletizing, where the location of the object needs to be
controlled and known. However, achieving precise grasps is challenging due to
noise in sensing and control, as well as unknown object properties. We propose
a method to plan robotic grasps that are both robust and precise by training
two convolutional neural networks - one to predict the robustness of a grasp
and another to predict a distribution of post-grasp object displacements. Our
networks are trained with depth images in simulation on a dataset of over 1000
industrial parts and were successfully deployed on a real robot without having
to be further fine-tuned. The proposed displacement estimator achieves a mean
prediction errors of 0.68cm and 3.42deg on novel objects in real world
experiments.",arxiv
http://arxiv.org/abs/1908.10001v1,2019-08-27T03:13:53Z,2019-08-27T03:13:53Z,Real-world Conversational AI for Hotel Bookings,"In this paper, we present a real-world conversational AI system to search for
and book hotels through text messaging. Our architecture consists of a
frame-based dialogue management system, which calls machine learning models for
intent classification, named entity recognition, and information retrieval
subtasks. Our chatbot has been deployed on a commercial scale, handling tens of
thousands of hotel searches every day. We describe the various opportunities
and challenges of developing a chatbot in the travel industry.",arxiv
http://arxiv.org/abs/1904.05724v1,2019-03-06T12:59:01Z,2019-03-06T12:59:01Z,"Improving SIEM for Critical SCADA Water Infrastructures Using Machine
  Learning","Network Control Systems (NAC) have been used in many industrial processes.
They aim to reduce the human factor burden and efficiently handle the complex
process and communication of those systems. Supervisory control and data
acquisition (SCADA) systems are used in industrial, infrastructure and facility
processes (e.g. manufacturing, fabrication, oil and water pipelines, building
ventilation, etc.) Like other Internet of Things (IoT) implementations, SCADA
systems are vulnerable to cyber-attacks, therefore, a robust anomaly detection
is a major requirement. However, having an accurate anomaly detection system is
not an easy task, due to the difficulty to differentiate between cyber-attacks
and system internal failures (e.g. hardware failures). In this paper, we
present a model that detects anomaly events in a water system controlled by
SCADA. Six Machine Learning techniques have been used in building and
evaluating the model. The model classifies different anomaly events including
hardware failures (e.g. sensor failures), sabotage and cyber-attacks (e.g. DoS
and Spoofing). Unlike other detection systems, our proposed work focuses on
notifying the operator when an anomaly occurs with a probability of the event
occurring. This additional information helps in accelerating the mitigation
process. The model is trained and tested using a real-world dataset.",arxiv
http://arxiv.org/abs/2008.11070v1,2020-08-25T14:43:30Z,2020-08-25T14:43:30Z,An Economic Perspective on Predictive Maintenance of Filtration Units,"This paper provides an economic perspective on the predictive maintenance of
filtration units. The rise of predictive maintenance is possible due to the
growing trend of industry 4.0 and the availability of inexpensive sensors.
However, the adoption rate for predictive maintenance by companies remains low.
The majority of companies are sticking to corrective and preventive
maintenance. This is not due to a lack of information on the technical
implementation of predictive maintenance, with an abundance of research papers
on state-of-the-art machine learning algorithms that can be used effectively.
The main issue is that most upper management has not yet been fully convinced
of the idea of predictive maintenance. The economic value of the implementation
has to be linked to the predictive maintenance program for better justification
by the management. In this study, three machine learning models were trained to
demonstrate the economic value of predictive maintenance. Data was collected
from a testbed located at the Singapore University of Technology and Design.
The testbed closely resembles a real-world water treatment plant. A
cost-benefit analysis coupled with Monte Carlo simulation was proposed. It
provided a structured approach to document potential costs and savings by
implementing a predictive maintenance program. The simulation incorporated
real-world risk into a financial model. Financial figures were adapted from
CITIC Envirotech Ltd, a leading membrane-based integrated environmental
solutions provider. Two scenarios were used to elaborate on the economic values
of predictive maintenance. Overall, this study seeks to bridge the gap between
technical and business domains of predictive maintenance.",arxiv
http://arxiv.org/abs/2101.04086v1,2021-01-11T18:29:50Z,2021-01-11T18:29:50Z,"System Design for a Data-driven and Explainable Customer Sentiment
  Monitor","The most important goal of customer services is to keep the customer
satisfied. However, service resources are always limited and must be
prioritized. Therefore, it is important to identify customers who potentially
become unsatisfied and might lead to escalations. Today this prioritization of
customers is often done manually. Data science on IoT data (esp. log data) for
machine health monitoring, as well as analytics on enterprise data for customer
relationship management (CRM) have mainly been researched and applied
independently. In this paper, we present a framework for a data-driven decision
support system which combines IoT and enterprise data to model customer
sentiment. Such decision support systems can help to prioritize customers and
service resources to effectively troubleshoot problems or even avoid them. The
framework is applied in a real-world case study with a major medical device
manufacturer. This includes a fully automated and interpretable machine
learning pipeline designed to meet the requirements defined with domain experts
and end users. The overall framework is currently deployed, learns and
evaluates predictive models from terabytes of IoT and enterprise data to
actively monitor the customer sentiment for a fleet of thousands of high-end
medical devices. Furthermore, we provide an anonymized industrial benchmark
dataset for the research community.",arxiv
http://arxiv.org/abs/2103.09430v3,2021-10-20T22:40:40Z,2021-03-17T04:08:03Z,OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs,"Enabling effective and efficient machine learning (ML) over large-scale graph
data (e.g., graphs with billions of edges) can have a great impact on both
industrial and scientific applications. However, existing efforts to advance
large-scale graph ML have been largely limited by the lack of a suitable public
benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of
three real-world datasets for facilitating the advancements in large-scale
graph ML. The OGB-LSC datasets are orders of magnitude larger than existing
ones, covering three core graph learning tasks -- link prediction, graph
regression, and node classification. Furthermore, we provide dedicated baseline
experiments, scaling up expressive graph ML models to the massive datasets. We
show that expressive models significantly outperform simple scalable baselines,
indicating an opportunity for dedicated efforts to further improve graph ML at
scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and
attracted more than 500 team registrations globally, during which significant
performance improvements were made by a variety of innovative techniques. We
summarize the common techniques used by the winning solutions and highlight the
current best practices in large-scale graph ML. Finally, we describe how we
have updated the datasets after the KDD Cup to further facilitate research
advances. The OGB-LSC datasets, baseline code, and all the information about
the KDD Cup are available at https://ogb.stanford.edu/docs/lsc/ .",arxiv
http://arxiv.org/abs/2004.04710v2,2021-01-21T16:05:23Z,2020-04-09T17:44:34Z,"Prune2Edge: A Multi-Phase Pruning Pipelines to Deep Ensemble Learning in
  IIoT","Most recently, with the proliferation of IoT devices, computational nodes in
manufacturing systems IIoT(Industrial-Internet-of-things) and the lunch of 5G
networks, there will be millions of connected devices generating a massive
amount of data. In such an environment, the controlling systems need to be
intelligent enough to deal with a vast amount of data to detect defects in a
real-time process. Driven by such a need, artificial intelligence models such
as deep learning have to be deployed into IIoT systems. However, learning and
using deep learning models are computationally expensive, so an IoT device with
limited computational power could not run such models. To tackle this issue,
edge intelligence had emerged as a new paradigm towards running Artificial
Intelligence models on edge devices. Although a considerable amount of studies
have been proposed in this area, the research is still in the early stages. In
this paper, we propose a novel edge-based multi-phase pruning pipelines to
ensemble learning on IIoT devices. In the first phase, we generate a diverse
ensemble of pruned models, then we apply integer quantisation, next we prune
the generated ensemble using a clustering-based technique. Finally, we choose
the best representative from each generated cluster to be deployed to a
distributed IoT environment. On CIFAR-100 and CIFAR-10, our proposed approach
was able to outperform the predictability levels of a baseline model (up to
7%), more importantly, the generated learners have small sizes (up to 90%
reduction in the model size) that minimise the required computational
capabilities to make an inference on the resource-constraint devices.",arxiv
http://arxiv.org/abs/1807.02340v2,2018-10-03T15:42:51Z,2018-07-06T10:17:44Z,Testing Untestable Neural Machine Translation: An Industrial Case,"Neural Machine Translation (NMT) has been widely adopted recently due to its
advantages compared with the traditional Statistical Machine Translation (SMT).
However, an NMT system still often produces translation failures due to the
complexity of natural language and sophistication in designing neural networks.
While in-house black-box system testing based on reference translations (i.e.,
examples of valid translations) has been a common practice for NMT quality
assurance, an increasingly critical industrial practice, named in-vivo testing,
exposes unseen types or instances of translation failures when real users are
using a deployed industrial NMT system. To fill the gap of lacking test oracle
for in-vivo testing of an NMT system, in this paper, we propose a new approach
for automatically identifying translation failures, without requiring reference
translations for a translation task; our approach can directly serve as a test
oracle for in-vivo testing. Our approach focuses on properties of natural
language translation that can be checked systematically and uses information
from both the test inputs (i.e., the texts to be translated) and the test
outputs (i.e., the translations under inspection) of the NMT system. Our
evaluation conducted on real-world datasets shows that our approach can
effectively detect targeted property violations as translation failures. Our
experiences on deploying our approach in both production and development
environments of WeChat (a messenger app with over one billion monthly active
users) demonstrate high effectiveness of our approach along with high industry
impact.",arxiv
http://arxiv.org/abs/2105.06543v1,2021-05-13T20:39:02Z,2021-05-13T20:39:02Z,"Policy Optimization in Bayesian Network Hybrid Models of
  Biomanufacturing Processes","Biopharmaceutical manufacturing is a rapidly growing industry with impact in
virtually all branches of medicine. Biomanufacturing processes require close
monitoring and control, in the presence of complex bioprocess dynamics with
many interdependent factors, as well as extremely limited data due to the high
cost and long duration of experiments. We develop a novel model-based
reinforcement learning framework that can achieve human-level control in
low-data environments. The model uses a probabilistic knowledge graph to
capture causal interdependencies between factors in the underlying stochastic
decision process, leveraging information from existing kinetic models from
different unit operations while incorporating real-world experimental data. We
then present a computationally efficient, provably convergent stochastic
gradient method for policy optimization. Validation is conducted on a realistic
application with a multi-dimensional, continuous state variable.",arxiv
http://arxiv.org/abs/2108.02565v1,2021-07-30T12:17:36Z,2021-07-30T12:17:36Z,"Dependable Neural Networks Through Redundancy, A Comparison of Redundant
  Architectures","With edge-AI finding an increasing number of real-world applications,
especially in industry, the question of functionally safe applications using AI
has begun to be asked. In this body of work, we explore the issue of achieving
dependable operation of neural networks. We discuss the issue of dependability
in general implementation terms before examining lockstep solutions. We intuit
that it is not necessarily a given that two similar neural networks generate
results at precisely the same time and that synchronization between the
platforms will be required. We perform some preliminary measurements that may
support this intuition and introduce some work in implementing lockstep neural
network engines.",arxiv
http://arxiv.org/abs/2001.02103v1,2020-01-06T13:13:21Z,2020-01-06T13:13:21Z,Self learning robot using real-time neural networks,"With the advancements in high volume, low precision computational technology
and applied research on cognitive artificially intelligent heuristic systems,
machine learning solutions through neural networks with real-time learning has
seen an immense interest in the research community as well the industry. This
paper involves research, development and experimental analysis of a neural
network implemented on a robot with an arm through which evolves to learn to
walk in a straight line or as required. The neural network learns using the
algorithms of Gradient Descent and Backpropagation. Both the implementation and
training of the neural network is done locally on the robot on a raspberry pi 3
so that its learning process is completely independent. The neural network is
first tested on a custom simulator developed on MATLAB and then implemented on
the raspberry computer. Data at each generation of the evolving network is
stored, and analysis both mathematical and graphical is done on the data.
Impact of factors like the learning rate and error tolerance on the learning
process and final output is analyzed.",arxiv
http://arxiv.org/abs/2111.09478v1,2021-11-18T02:18:27Z,2021-11-18T02:18:27Z,"Software Engineering for Responsible AI: An Empirical Study and
  Operationalised Patterns","Although artificial intelligence (AI) is solving real-world challenges and
transforming industries, there are serious concerns about its ability to behave
and make decisions in a responsible way. Many AI ethics principles and
guidelines for responsible AI have been recently issued by governments,
organisations, and enterprises. However, these AI ethics principles and
guidelines are typically high-level and do not provide concrete guidance on how
to design and develop responsible AI systems. To address this shortcoming, we
first present an empirical study where we interviewed 21 scientists and
engineers to understand the practitioners' perceptions on AI ethics principles
and their implementation. We then propose a template that enables AI ethics
principles to be operationalised in the form of concrete patterns and suggest a
list of patterns using the newly created template. These patterns provide
concrete, operationalised guidance that facilitate the development of
responsible AI systems.",arxiv
http://arxiv.org/abs/2003.07583v1,2020-03-17T08:47:34Z,2020-03-17T08:47:34Z,"Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow
  Based QoE","With the merit of containing full panoramic content in one camera, Virtual
Reality (VR) and 360-degree videos have attracted more and more attention in
the field of industrial cloud manufacturing and training. Industrial Internet
of Things (IoT), where many VR terminals needed to be online at the same time,
can hardly guarantee VR's bandwidth requirement. However, by making use of
users' quality of experience (QoE) awareness factors, including the relative
moving speed and depth difference between the viewpoint and other content,
bandwidth consumption can be reduced. In this paper, we propose OFB-VR (Optical
Flow Based VR), an interactive method of VR streaming that can make use of VR
users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable
Difference through Optical Flow Estimation (JND-OFE) is explored to quantify
users' awareness of quality distortion in 360-degree videos. Accordingly, a
novel 360-degree videos QoE metric based on PSNR and JND-OFE (PSNR-OF) is
proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling
scheme to lessen the tiling overhead. A Reinforcement Learning(RL) method is
implemented to make use of historical data to perform Adaptive BitRate(ABR).
For evaluation, we take two prior VR streaming schemes, Pano and Plato, as
baselines. Vast evaluations show that our system can increase the mean PSNR-OF
score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano
and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that
OFB-VR is a promising prototype for actual interactive industrial VR. A
prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.",arxiv
http://arxiv.org/abs/2101.04808v1,2021-01-13T00:02:49Z,2021-01-13T00:02:49Z,MLGO: a Machine Learning Guided Compiler Optimizations Framework,"Leveraging machine-learning (ML) techniques for compiler optimizations has
been widely studied and explored in academia. However, the adoption of ML in
general-purpose, industry strength compilers has yet to happen. We propose
MLGO, a framework for integrating ML techniques systematically in an industrial
compiler -- LLVM. As a case study, we present the details and results of
replacing the heuristics-based inlining-for-size optimization in LLVM with
machine learned models. To the best of our knowledge, this work is the first
full integration of ML in a complex compiler pass in a real-world setting. It
is available in the main LLVM repository. We use two different ML algorithms:
Policy Gradient and Evolution Strategies, to train the inlining-for-size model,
and achieve up to 7\% size reduction, when compared to state of the art LLVM
-Oz. The same model, trained on one corpus, generalizes well to a diversity of
real-world targets, as well as to the same set of targets after months of
active development. This property of the trained models is beneficial to deploy
ML techniques in real-world settings.",arxiv
http://arxiv.org/abs/2002.08333v1,2020-02-11T15:32:28Z,2020-02-11T15:32:28Z,"Towards Intelligent Pick and Place Assembly of Individualized Products
  Using Reinforcement Learning","Individualized manufacturing is becoming an important approach as a means to
fulfill increasingly diverse and specific consumer requirements and
expectations. While there are various solutions to the implementation of the
manufacturing process, such as additive manufacturing, the subsequent automated
assembly remains a challenging task. As an approach to this problem, we aim to
teach a collaborative robot to successfully perform pick and place tasks by
implementing reinforcement learning. For the assembly of an individualized
product in a constantly changing manufacturing environment, the simulated
geometric and dynamic parameters will be varied. Using reinforcement learning
algorithms capable of meta-learning, the tasks will first be trained in
simulation. They will then be performed in a real-world environment where new
factors are introduced that were not simulated in training to confirm the
robustness of the algorithms. The robot will gain its input data from tactile
sensors, area scan cameras, and 3D cameras used to generate heightmaps of the
environment and the objects. The selection of machine learning algorithms and
hardware components as well as further research questions to realize the
outlined production scenario are the results of the presented work.",arxiv
http://arxiv.org/abs/2102.08936v2,2021-04-02T20:06:57Z,2021-02-17T18:51:51Z,"Deep Learning Anomaly Detection for Cellular IoT with Applications in
  Smart Logistics","The number of connected Internet of Things (IoT) devices within
cyber-physical infrastructure systems grows at an increasing rate. This poses
significant device management and security challenges to current IoT networks.
Among several approaches to cope with these challenges, data-based methods
rooted in deep learning (DL) are receiving an increased interest. In this
paper, motivated by the upcoming surge of 5G IoT connectivity in industrial
environments, we propose to integrate a DL-based anomaly detection (AD) as a
service into the 3GPP mobile cellular IoT architecture. The proposed
architecture embeds autoencoder based anomaly detection modules both at the IoT
devices (ADM-EDGE) and in the mobile core network (ADM-FOG), thereby balancing
between the system responsiveness and accuracy. We design, integrate,
demonstrate and evaluate a testbed that implements the above service in a
real-world deployment integrated within the 3GPP Narrow-Band IoT (NB-IoT)
mobile operator network.",arxiv
http://arxiv.org/abs/2012.08174v2,2021-03-29T17:15:00Z,2020-12-15T09:49:22Z,"Towards open and expandable cognitive AI architectures for large-scale
  multi-agent human-robot collaborative learning","Learning from Demonstration (LfD) constitutes one of the most robust
methodologies for constructing efficient cognitive robotic systems. Despite the
large body of research works already reported, current key technological
challenges include those of multi-agent learning and long-term autonomy.
Towards this direction, a novel cognitive architecture for multi-agent LfD
robotic learning is introduced, targeting to enable the reliable deployment of
open, scalable and expandable robotic systems in large-scale and complex
environments. In particular, the designed architecture capitalizes on the
recent advances in the Artificial Intelligence (AI) field, by establishing a
Federated Learning (FL)-based framework for incarnating a multi-human
multi-robot collaborative learning environment. The fundamental
conceptualization relies on employing multiple AI-empowered cognitive processes
(implementing various robotic tasks) that operate at the edge nodes of a
network of robotic platforms, while global AI models (underpinning the
aforementioned robotic tasks) are collectively created and shared among the
network, by elegantly combining information from a large number of human-robot
interaction instances. Regarding pivotal novelties, the designed cognitive
architecture a) introduces a new FL-based formalism that extends the
conventional LfD learning paradigm to support large-scale multi-agent
operational settings, b) elaborates previous FL-based self-learning robotic
schemes so as to incorporate the human in the learning loop and c) consolidates
the fundamental principles of FL with additional sophisticated AI-enabled
learning methodologies for modelling the multi-level inter-dependencies among
the robotic tasks. The applicability of the proposed framework is explained
using an example of a real-world industrial case study for agile
production-based Critical Raw Materials (CRM) recovery.",arxiv
http://arxiv.org/abs/2012.01153v1,2020-12-02T12:53:19Z,2020-12-02T12:53:19Z,Towards Intelligent Reconfigurable Wireless Physical Layer (PHY),"Next-generation wireless networks are getting significant attention because
they promise 10-factor enhancement in mobile broadband along with the potential
to enable new heterogeneous services. Services include massive machine type
communications desired for Industrial 4.0 along with ultra-reliable low latency
services for remote healthcare and vehicular communications. In this paper, we
present the design of an intelligent and reconfigurable physical layer (PHY) to
bring these services to reality. First, we design and implement the
reconfigurable PHY via a hardware-software co-design approach on system-on-chip
consisting of the ARM processor and field-programmable gate array (FPGA). The
reconfigurable PHY is then made intelligent by augmenting it with online
machine learning (OML) based decision-making algorithm. Such PHY can learn the
environment (for example, wireless channel) and dynamically adapt the
transceivers' configuration (i.e., modulation scheme, word-length) and select
the wireless channel on-the-fly. Since the environment is unknown and changes
with time, we make the OML architecture reconfigurable to enable dynamic switch
between various OML algorithms on-the-fly. We have demonstrated the functional
correctness of the proposed architecture for different environments and
word-lengths. The detailed throughput, latency, and complexity analysis
validate the feasibility and importance of the proposed intelligent and
reconfigurable PHY in next-generation networks.",arxiv
http://arxiv.org/abs/2111.06981v1,2021-11-12T23:29:43Z,2021-11-12T23:29:43Z,"Soft-Sensing ConFormer: A Curriculum Learning-based Convolutional
  Transformer","Over the last few decades, modern industrial processes have investigated
several cost-effective methodologies to improve the productivity and yield of
semiconductor manufacturing. While playing an essential role in facilitating
real-time monitoring and control, the data-driven soft-sensors in industries
have provided a competitive edge when augmented with deep learning approaches
for wafer fault-diagnostics. Despite the success of deep learning methods
across various domains, they tend to suffer from bad performance on
multi-variate soft-sensing data domains. To mitigate this, we propose a
soft-sensing ConFormer (CONvolutional transFORMER) for wafer fault-diagnostic
classification task which primarily consists of multi-head convolution modules
that reap the benefits of fast and light-weight operations of convolutions, and
also the ability to learn the robust representations through multi-head design
alike transformers. Another key issue is that traditional learning paradigms
tend to suffer from low performance on noisy and highly-imbalanced soft-sensing
data. To address this, we augment our soft-sensing ConFormer model with a
curriculum learning-based loss function, which effectively learns easy samples
in the early phase of training and difficult ones later. To further demonstrate
the utility of our proposed architecture, we performed extensive experiments on
various toolsets of Seagate Technology's wafer manufacturing process which are
shared openly along with this work. To the best of our knowledge, this is the
first time that curriculum learning-based soft-sensing ConFormer architecture
has been proposed for soft-sensing data and our results show strong promise for
future use in soft-sensing research domain.",arxiv
http://arxiv.org/abs/2012.10610v3,2021-02-16T17:31:15Z,2020-12-19T07:00:09Z,"SpaceML: Distributed Open-source Research with Citizen Scientists for
  the Advancement of Space Technology for NASA","Traditionally, academic labs conduct open-ended research with the primary
focus on discoveries with long-term value, rather than direct products that can
be deployed in the real world. On the other hand, research in the industry is
driven by its expected commercial return on investment, and hence focuses on a
real world product with short-term timelines. In both cases, opportunity is
selective, often available to researchers with advanced educational
backgrounds. Research often happens behind closed doors and may be kept
confidential until either its publication or product release, exacerbating the
problem of AI reproducibility and slowing down future research by others in the
field. As many research organizations tend to exclusively focus on specific
areas, opportunities for interdisciplinary research reduce. Undertaking
long-term bold research in unexplored fields with non-commercial yet great
public value is hard due to factors including the high upfront risk, budgetary
constraints, and a lack of availability of data and experts in niche fields.
Only a few companies or well-funded research labs can afford to do such
long-term research. With research organizations focused on an exploding array
of fields and resources spread thin, opportunities for the maturation of
interdisciplinary research reduce. Apart from these exigencies, there is also a
need to engage citizen scientists through open-source contributors to play an
active part in the research dialogue. We present a short case study of SpaceML,
an extension of the Frontier Development Lab, an AI accelerator for NASA.
SpaceML distributes open-source research and invites volunteer citizen
scientists to partake in development and deployment of high social value
products at the intersection of space and AI.",arxiv
http://arxiv.org/abs/2111.06123v1,2021-11-11T10:01:01Z,2021-11-11T10:01:01Z,"Spatio-Temporal Scene-Graph Embedding for Autonomous Vehicle Collision
  Prediction","In autonomous vehicles (AVs), early warning systems rely on collision
prediction to ensure occupant safety. However, state-of-the-art methods using
deep convolutional networks either fail at modeling collisions or are too
expensive/slow, making them less suitable for deployment on AV edge hardware.
To address these limitations, we propose sg2vec, a spatio-temporal scene-graph
embedding methodology that uses Graph Neural Network (GNN) and Long Short-Term
Memory (LSTM) layers to predict future collisions via visual scene perception.
We demonstrate that sg2vec predicts collisions 8.11% more accurately and 39.07%
earlier than the state-of-the-art method on synthesized datasets, and 29.47%
more accurately on a challenging real-world collision dataset. We also show
that sg2vec is better than the state-of-the-art at transferring knowledge from
synthetic datasets to real-world driving datasets. Finally, we demonstrate that
sg2vec performs inference 9.3x faster with an 88.0% smaller model, 32.4% less
power, and 92.8% less energy than the state-of-the-art method on the
industry-standard Nvidia DRIVE PX 2 platform, making it more suitable for
implementation on the edge.",arxiv
http://arxiv.org/abs/1911.07391v3,2021-11-14T16:58:11Z,2019-11-18T01:15:24Z,Justification-Based Reliability in Machine Learning,"With the advent of Deep Learning, the field of machine learning (ML) has
surpassed human-level performance on diverse classification tasks. At the same
time, there is a stark need to characterize and quantify reliability of a
model's prediction on individual samples. This is especially true in
application of such models in safety-critical domains of industrial control and
healthcare. To address this need, we link the question of reliability of a
model's individual prediction to the epistemic uncertainty of the model's
prediction. More specifically, we extend the theory of Justified True Belief
(JTB) in epistemology, created to study the validity and limits of
human-acquired knowledge, towards characterizing the validity and limits of
knowledge in supervised classifiers. We present an analysis of neural network
classifiers linking the reliability of its prediction on an input to
characteristics of the support gathered from the input and latent spaces of the
network. We hypothesize that the JTB analysis exposes the epistemic uncertainty
(or ignorance) of a model with respect to its inference, thereby allowing for
the inference to be only as strong as the justification permits. We explore
various forms of support (for e.g., k-nearest neighbors (k-NN) and l_p-norm
based) generated for an input, using the training data to construct a
justification for the prediction with that input. Through experiments conducted
on simulated and real datasets, we demonstrate that our approach can provide
reliability for individual predictions and characterize regions where such
reliability cannot be ascertained.",arxiv
http://arxiv.org/abs/1709.03008v1,2017-09-09T21:27:06Z,2017-09-09T21:27:06Z,"Identifying Irregular Power Usage by Turning Predictions into
  Holographic Spatial Visualizations","Power grids are critical infrastructure assets that face non-technical losses
(NTL) such as electricity theft or faulty meters. NTL may range up to 40% of
the total electricity distributed in emerging countries. Industrial NTL
detection systems are still largely based on expert knowledge when deciding
whether to carry out costly on-site inspections of customers. Electricity
providers are reluctant to move to large-scale deployments of automated systems
that learn NTL profiles from data due to the latter's propensity to suggest a
large number of unnecessary inspections. In this paper, we propose a novel
system that combines automated statistical decision making with expert
knowledge. First, we propose a machine learning framework that classifies
customers into NTL or non-NTL using a variety of features derived from the
customers' consumption data. The methodology used is specifically tailored to
the level of noise in the data. Second, in order to allow human experts to feed
their knowledge in the decision loop, we propose a method for visualizing
prediction results at various granularity levels in a spatial hologram. Our
approach allows domain experts to put the classification results into the
context of the data and to incorporate their knowledge for making the final
decisions of which customers to inspect. This work has resulted in appreciable
results on a real-world data set of 3.6M customers. Our system is being
deployed in a commercial NTL detection software.",arxiv
http://arxiv.org/abs/2002.05648v3,2020-04-26T04:59:52Z,2020-02-01T01:15:39Z,Politics of Adversarial Machine Learning,"In addition to their security properties, adversarial machine-learning
attacks and defenses have political dimensions. They enable or foreclose
certain options for both the subjects of the machine learning systems and for
those who deploy them, creating risks for civil liberties and human rights. In
this paper, we draw on insights from science and technology studies,
anthropology, and human rights literature, to inform how defenses against
adversarial attacks can be used to suppress dissent and limit attempts to
investigate machine learning systems. To make this concrete, we use real-world
examples of how attacks such as perturbation, model inversion, or membership
inference can be used for socially desirable ends. Although the predictions of
this analysis may seem dire, there is hope. Efforts to address human rights
concerns in the commercial spyware industry provide guidance for similar
measures to ensure ML systems serve democratic, not authoritarian ends",arxiv
http://arxiv.org/abs/2110.04003v1,2021-10-08T09:59:12Z,2021-10-08T09:59:12Z,Learning to Centralize Dual-Arm Assembly,"Even though industrial manipulators are widely used in modern manufacturing
processes, deployment in unstructured environments remains an open problem. To
deal with variety, complexity and uncertainty of real world manipulation tasks
a general framework is essential. In this work we want to focus on assembly
with humanoid robots by providing a framework for dual-arm peg-in-hole
manipulation. As we aim to contribute towards an approach which is not limited
to dual-arm peg-in-hole, but dual-arm manipulation in general, we keep modeling
effort at a minimum. While reinforcement learning has shown great results for
single-arm robotic manipulation in recent years, research focusing on dual-arm
manipulation is still rare. Solving such tasks often involves complex modeling
of interaction between two manipulators and their coupling at a control level.
In this paper, we explore the applicability of model-free reinforcement
learning to dual-arm manipulation based on a modular approach with two
decentralized single-arm controllers and a single centralized policy. We reduce
modeling effort to a minimum by using sparse rewards only. We demonstrate the
effectiveness of the framework on dual-arm peg-in-hole and analyze sample
efficiency and success rates for different action spaces. Moreover, we compare
results on different clearances and showcase disturbance recovery and
robustness, when dealing with position uncertainties. Finally we zero-shot
transfer policies trained in simulation to the real-world and evaluate their
performance.",arxiv
http://arxiv.org/abs/1911.08090v1,2019-11-19T04:33:05Z,2019-11-19T04:33:05Z,Deep Detector Health Management under Adversarial Campaigns,"Machine learning models are vulnerable to adversarial inputs that induce
seemingly unjustifiable errors. As automated classifiers are increasingly used
in industrial control systems and machinery, these adversarial errors could
grow to be a serious problem. Despite numerous studies over the past few years,
the field of adversarial ML is still considered alchemy, with no practical
unbroken defenses demonstrated to date, leaving PHM practitioners with few
meaningful ways of addressing the problem. We introduce turbidity detection as
a practical superset of the adversarial input detection problem, coping with
adversarial campaigns rather than statistically invisible one-offs. This
perspective is coupled with ROC-theoretic design guidance that prescribes an
inexpensive domain adaptation layer at the output of a deep learning model
during an attack campaign. The result aims to approximate the Bayes optimal
mitigation that ameliorates the detection model's degraded health. A
proactively reactive type of prognostics is achieved via Monte Carlo simulation
of various adversarial campaign scenarios, by sampling from the model's own
turbidity distribution to quickly deploy the correct mitigation during a
real-world campaign.",arxiv
http://arxiv.org/abs/2103.15245v3,2021-04-03T14:55:20Z,2021-03-28T23:36:56Z,"Game Theory Based Privacy Preserving Approach for Collaborative Deep
  Learning in IoT","The exponential growth of Internet of Things (IoT) has become a transcending
force in creating innovative smart devices and connected domains including
smart homes, healthcare, transportation and manufacturing. With billions of IoT
devices, there is a huge amount of data continuously being generated,
transmitted, and stored at various points in the IoT architecture. Deep
learning is widely being used in IoT applications to extract useful insights
from IoT data. However, IoT users have security and privacy concerns and prefer
not to share their personal data with third party applications or stakeholders.
In order to address user privacy concerns, Collaborative Deep Learning (CDL)
has been largely employed in data-driven applications which enables multiple
IoT devices to train their models locally on edge gateways. In this chapter, we
first discuss different types of deep learning approaches and how these
approaches can be employed in the IoT domain. We present a privacy-preserving
collaborative deep learning approach for IoT devices which can achieve benefits
from other devices in the system. This learning approach is analyzed from the
behavioral perspective of mobile edge devices using a game-theoretic model. We
analyze the Nash Equilibrium in N-player static game model. We further present
a novel fair collaboration strategy among edge IoT devices using cluster based
approach to solve the CDL game, which enforces mobile edge devices for
cooperation. We also present implementation details and evaluation analysis in
a real-world smart home deployment.",arxiv
http://arxiv.org/abs/2105.13183v2,2021-05-30T08:34:59Z,2021-05-27T14:37:08Z,An Efficient Style Virtual Try on Network for Clothing Business Industry,"With the increasing development of garment manufacturing industry, the method
of combining neural network with industry to reduce product redundancy has been
paid more and more attention.In order to reduce garment redundancy and achieve
personalized customization, more researchers have appeared in the field of
virtual trying on.They try to transfer the target clothing to the reference
figure, and then stylize the clothes to meet user's requirements for
fashion.But the biggest problem of virtual try on is that the shape and motion
blocking distort the clothes, causing the patterns and texture on the clothes
to be impossible to restore. This paper proposed a new stylized virtual try on
network, which can not only retain the authenticity of clothing texture and
pattern, but also obtain the undifferentiated stylized try on. The network is
divided into three sub-networks, the first is the user image, the front of the
target clothing image, the semantic segmentation image and the posture heat map
to generate a more detailed human parsing map. Second, UV position map and
dense correspondence are used to map patterns and textures to the deformed
silhouettes in real time, so that they can be retained in real time, and the
rationality of spatial structure can be guaranteed on the basis of improving
the authenticity of images. Third,Stylize and adjust the generated virtual try
on image. Through the most subtle changes, users can choose the texture, color
and style of clothing to improve the user's experience.",arxiv
http://arxiv.org/abs/2102.10498v1,2021-02-21T03:25:26Z,2021-02-21T03:25:26Z,"Customized Slicing for 6G: Enforcing Artificial Intelligence on Resource
  Management","Next generation wireless networks are expected to support diverse vertical
industries and offer countless emerging use cases. To satisfy stringent
requirements of diversified services, network slicing is developed, which
enables service-oriented resource allocation by tailoring the infrastructure
network into multiple logical networks. However, there are still some
challenges in cross-domain multi-dimensional resource management for end-to-end
(E2E) slices under the dynamic and uncertain environment. Trading off the
revenue and cost of resource allocation while guaranteeing service quality is
significant to tenants. Therefore, this article introduces a hierarchical
resource management framework, utilizing deep reinforcement learning in
admission control of resource requests from different tenants and resource
adjustment within admitted slices for each tenant. Particularly, we first
discuss the challenges in customized resource management of 6G. Second, the
motivation and background are presented to explain why artificial intelligence
(AI) is applied in resource customization of multi-tenant slicing. Third, E2E
resource management is decomposed into two problems, multi-dimensional resource
allocation decision based on slice-level feedback and real-time slice adaption
aimed at avoiding service quality degradation. Simulation results demonstrate
the effectiveness of AI-based customized slicing. Finally, several significant
challenges that need to be addressed in practical implementation are
investigated.",arxiv
http://arxiv.org/abs/1910.11779v1,2019-10-25T15:03:11Z,2019-10-25T15:03:11Z,"Toward a better trade-off between performance and fairness with
  kernel-based distribution matching","As recent literature has demonstrated how classifiers often carry unintended
biases toward some subgroups, deploying machine learned models to users demands
careful consideration of the social consequences. How should we address this
problem in a real-world system? How should we balance core performance and
fairness metrics? In this paper, we introduce a MinDiff framework for
regularizing classifiers toward different fairness metrics and analyze a
technique with kernel-based statistical dependency tests. We run a thorough
study on an academic dataset to compare the Pareto frontier achieved by
different regularization approaches, and apply our kernel-based method to two
large-scale industrial systems demonstrating real-world improvements.",arxiv
http://arxiv.org/abs/2007.15724v1,2020-07-30T20:14:42Z,2020-07-30T20:14:42Z,"MAPPER: Multi-Agent Path Planning with Evolutionary Reinforcement
  Learning in Mixed Dynamic Environments","Multi-agent navigation in dynamic environments is of great industrial value
when deploying a large scale fleet of robot to real-world applications. This
paper proposes a decentralized partially observable multi-agent path planning
with evolutionary reinforcement learning (MAPPER) method to learn an effective
local planning policy in mixed dynamic environments. Reinforcement
learning-based methods usually suffer performance degradation on long-horizon
tasks with goal-conditioned sparse rewards, so we decompose the long-range
navigation task into many easier sub-tasks under the guidance of a global
planner, which increases agents' performance in large environments. Moreover,
most existing multi-agent planning approaches assume either perfect information
of the surrounding environment or homogeneity of nearby dynamic agents, which
may not hold in practice. Our approach models dynamic obstacles' behavior with
an image-based representation and trains a policy in mixed dynamic environments
without homogeneity assumption. To ensure multi-agent training stability and
performance, we propose an evolutionary training approach that can be easily
scaled to large and complex environments. Experiments show that MAPPER is able
to achieve higher success rates and more stable performance when exposed to a
large number of non-cooperative dynamic obstacles compared with traditional
reaction-based planner LRA* and the state-of-the-art learning-based method.",arxiv
http://arxiv.org/abs/2109.05821v1,2021-09-13T09:46:50Z,2021-09-13T09:46:50Z,Cyber-Security in the Emerging World of Smart Everything,"The fourth industrial revolution (4IR) is a revolution many authors believe
have come to stay. It is a revolution that has been fast blurring the line
between physical, digital and biological technologies. These disruptive
technologies largely rely on high-speed internet connectivity, Cloud
technologies, Augmented Reality, Additive Manufacturing, Data science and
Artificial Intelligence. Most developed economies have embraced the it while
the developing economies are struggling to adopt 4IR because they lack the
requisite skills, knowledge and technology. Thus, this study investigates
Nigeria as one of the developing economies to understand her readiness for 4IR
and the level of preparedness to mitigate the sophisticated cyber-attacks that
comes with it. The investigation adopted quantitative research approach and
developed an online questionnaire that was shared amongst the population of
interest that includes academic, industry experts and relevant stakeholders.
The questionnaire returned 116 valid responses which were analysed with
descriptive statistical tools in SPSS. Results suggest that 60 of the
respondents opined that Nigerian government at are not showing enough evidence
to demonstrate her preparedness to leverage these promised potentials by
developing 4IR relevant laws, strong institutional frameworks and policies.
They lack significant development capacity to mitigate risks associated with
digital ecosystem and cyber ecosystem that are ushered in by the 4IR. In the
universities, 52 of the courses offered at the undergraduate and 42 at the
post-graduate levels are relevant in the development of skills required in the
revolution. The study recommends that the government at all levels make
adequate efforts in developing the countrys intangible assets. In all, this
paper posits that successful implementation of these could equip Nigeria to
embrace the 4IR in all its aspects.",arxiv
http://arxiv.org/abs/1909.10270v1,2019-09-23T10:37:59Z,2019-09-23T10:37:59Z,"Pose Estimation for Texture-less Shiny Objects in a Single RGB Image
  Using Synthetic Training Data","In the industrial domain, the pose estimation of multiple texture-less shiny
parts is a valuable but challenging task. In this particular scenario, it is
impractical to utilize keypoints or other texture information because most of
them are not actual features of the target but the reflections of surroundings.
Moreover, the similarity of color also poses a challenge in segmentation. In
this article, we propose to divide the pose estimation process into three
stages: object detection, features detection and pose optimization. A
convolutional neural network was utilized to perform object detection.
Concerning the reliability of surface texture, we leveraged the contour
information for estimating pose. Since conventional contour-based methods are
inapplicable to clustered metal parts due to the difficulties in segmentation,
we use the dense discrete points along the metal part edges as semantic
keypoints for contour detection. Afterward, we exploit both keypoint
information and CAD model to calculate the 6D pose of each object in view. A
typical implementation of deep learning methods not only requires a large
amount of training data, but also relies on intensive human labor for labeling
the datasets. Therefore, we propose an approach to generate datasets and label
them automatically. Despite not using any real-world photos for training, a
series of experiments showed that the algorithm built on synthetic data perform
well in the real environment.",arxiv
http://arxiv.org/abs/1708.08559v2,2018-03-20T06:10:24Z,2017-08-28T23:26:14Z,"DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous
  Cars","Recent advances in Deep Neural Networks (DNNs) have led to the development of
DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can
drive without any human intervention. Most major manufacturers including Tesla,
GM, Ford, BMW, and Waymo/Google are working on building and testing different
types of autonomous vehicles. The lawmakers of several US states including
California, Texas, and New York have passed new legislation to fast-track the
process of testing and deployment of autonomous vehicles on their roads.
  However, despite their spectacular progress, DNNs, just like traditional
software, often demonstrate incorrect or unexpected corner case behaviors that
can lead to potentially fatal collisions. Several such real-world accidents
involving autonomous cars have already happened including one which resulted in
a fatality. Most existing testing techniques for DNN-driven vehicles are
heavily dependent on the manual collection of test data under different driving
conditions which become prohibitively expensive as the number of test
conditions increases.
  In this paper, we design, implement and evaluate DeepTest, a systematic
testing tool for automatically detecting erroneous behaviors of DNN-driven
vehicles that can potentially lead to fatal crashes. First, our tool is
designed to automatically generated test cases leveraging real-world changes in
driving conditions like rain, fog, lighting conditions, etc. DeepTest
systematically explores different parts of the DNN logic by generating test
inputs that maximize the numbers of activated neurons. DeepTest found thousands
of erroneous behaviors under different realistic driving conditions (e.g.,
blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in
three top performing DNNs in the Udacity self-driving car challenge.",arxiv
http://arxiv.org/abs/1911.02987v1,2019-11-08T02:13:21Z,2019-11-08T02:13:21Z,The Pitfall of Evaluating Performance on Emerging AI Accelerators,"In recent years, domain-specific hardware has brought significant performance
improvements in deep learning (DL). Both industry and academia only focus on
throughput when evaluating these AI accelerators, which usually are custom
ASICs deployed in datacenter to speed up the inference phase of DL workloads.
Pursuing higher hardware throughput such as OPS (Operation Per Second) using
various optimizations seems to be their main design target. However, they
ignore the importance of accuracy in the DL nature. Motivated by this, this
paper argue that a single throughput metric can not comprehensively reflect the
real-world performance of AI accelerators. To reveal this pitfall, we evaluates
several frequently-used optimizations on a typical AI accelerator and
quantifies their impact on accuracy and throughout under representative DL
inference workloads. Based on our experimental results, we find that some
optimizations cause significant loss on accuracy in some workloads, although it
can improves the throughout. Furthermore, our results show the importance of
end-to-end evaluation in DL.",arxiv
http://arxiv.org/abs/2107.13252v1,2021-07-28T10:28:05Z,2021-07-28T10:28:05Z,"Multi Agent System for Machine Learning Under Uncertainty in Cyber
  Physical Manufacturing System","Recent advancements in predictive machine learning has led to its application
in various use cases in manufacturing. Most research focused on maximising
predictive accuracy without addressing the uncertainty associated with it.
While accuracy is important, focusing primarily on it poses an overfitting
danger, exposing manufacturers to risk, ultimately hindering the adoption of
these techniques. In this paper, we determine the sources of uncertainty in
machine learning and establish the success criteria of a machine learning
system to function well under uncertainty in a cyber-physical manufacturing
system (CPMS) scenario. Then, we propose a multi-agent system architecture
which leverages probabilistic machine learning as a means of achieving such
criteria. We propose possible scenarios for which our proposed architecture is
useful and discuss future work. Experimentally, we implement Bayesian Neural
Networks for multi-tasks classification on a public dataset for the real-time
condition monitoring of a hydraulic system and demonstrate the usefulness of
the system by evaluating the probability of a prediction being accurate given
its uncertainty. We deploy these models using our proposed agent-based
framework and integrate web visualisation to demonstrate its real-time
feasibility.",arxiv
http://arxiv.org/abs/1808.09794v2,2018-08-30T19:40:56Z,2018-08-29T13:25:11Z,"Correlated Time Series Forecasting using Deep Neural Networks: A Summary
  of Results","Cyber-physical systems often consist of entities that interact with each
other over time. Meanwhile, as part of the continued digitization of industrial
processes, various sensor technologies are deployed that enable us to record
time-varying attributes (a.k.a., time series) of such entities, thus producing
correlated time series. To enable accurate forecasting on such correlated time
series, this paper proposes two models that combine convolutional neural
networks (CNNs) and recurrent neural networks (RNNs). The first model employs a
CNN on each individual time series, combines the convoluted features, and then
applies an RNN on top of the convoluted features in the end to enable
forecasting. The second model adds additional auto-encoders into the individual
CNNs, making the second model a multi-task learning model, which provides
accurate and robust forecasting. Experiments on two real-world correlated time
series data set suggest that the proposed two models are effective and
outperform baselines in most settings.
  This report extends the paper ""Correlated Time Series Forecasting using
Multi-Task Deep Neural Networks,"" to appear in ACM CIKM 2018, by providing
additional experimental results.",arxiv
http://arxiv.org/abs/2011.14925v1,2020-11-26T02:37:39Z,2020-11-26T02:37:39Z,"Autonomous Graph Mining Algorithm Search with Best Speed/Accuracy
  Trade-off","Graph data is ubiquitous in academia and industry, from social networks to
bioinformatics. The pervasiveness of graphs today has raised the demand for
algorithms that can answer various questions: Which products would a user like
to purchase given her order list? Which users are buying fake followers to
increase their public reputation? Myriads of new graph mining algorithms are
proposed every year to answer such questions - each with a distinct problem
formulation, computational time, and memory footprint. This lack of unity makes
it difficult for a practitioner to compare different algorithms and pick the
most suitable one for a specific application. These challenges - even more
severe for non-experts - create a gap in which state-of-the-art techniques
developed in academic settings fail to be optimally deployed in real-world
applications. To bridge this gap, we propose AUTOGM, an automated system for
graph mining algorithm development. We first define a unified framework
UNIFIEDGM that integrates various message-passing based graph algorithms,
ranging from conventional algorithms like PageRank to graph neural networks.
Then UNIFIEDGM defines a search space in which five parameters are required to
determine a graph algorithm. Under this search space, AUTOGM explicitly
optimizes for the optimal parameter set of UNIFIEDGM using Bayesian
Optimization. AUTOGM defines a novel budget-aware objective function for the
optimization to incorporate a practical issue - finding the best speed-accuracy
trade-off under a computation budget - into the graph algorithm generation
problem. Experiments on real-world benchmark datasets demonstrate that AUTOGM
generates novel graph mining algorithms with the best speed/accuracy trade-off
compared to existing models with heuristic parameters.",arxiv
http://arxiv.org/abs/1601.06473v2,2016-01-26T04:43:20Z,2016-01-25T03:31:24Z,Teaching Robots to Do Object Assembly using Multi-modal 3D Vision,"The motivation of this paper is to develop a smart system using multi-modal
vision for next-generation mechanical assembly. It includes two phases where in
the first phase human beings teach the assembly structure to a robot and in the
second phase the robot finds objects and grasps and assembles them using AI
planning. The crucial part of the system is the precision of 3D visual
detection and the paper presents multi-modal approaches to meet the
requirements: AR markers are used in the teaching phase since human beings can
actively control the process. Point cloud matching and geometric constraints
are used in the robot execution phase to avoid unexpected noises. Experiments
are performed to examine the precision and correctness of the approaches. The
study is practical: The developed approaches are integrated with graph
model-based motion planning, implemented on an industrial robots and applicable
to real-world scenarios.",arxiv
http://arxiv.org/abs/1911.04099v1,2019-11-11T06:26:13Z,2019-11-11T06:26:13Z,"Beyond Similarity: Relation Embedding with Dual Attentions for
  Item-based Recommendation","Given the effectiveness and ease of use, Item-based Collaborative Filtering
(ICF) methods have been broadly used in industry in recent years. The key of
ICF lies in the similarity measurement between items, which however is a
coarse-grained numerical value that can hardly capture users' fine-grained
preferences toward different latent aspects of items from a representation
learning perspective. In this paper, we propose a model called REDA (latent
Relation Embedding with Dual Attentions) to address this challenge. REDA is
essentially a deep learning based recommendation method that employs an item
relation embedding scheme through a neural network structure for inter-item
relations representation. A relational user embedding is then proposed by
aggregating the relation embeddings between all purchased items of a user,
which not only better characterizes user preferences but also alleviates the
data sparsity problem. Moreover, to capture valid meta-knowledge that reflects
users' desired latent aspects and meanwhile suppress their explosive growth
towards overfitting, we further propose a dual attentions mechanism, including
a memory attention and a weight attention. A relation-wise optimization method
is finally developed for model inference by constructing a personalized ranking
loss for item relations. Extensive experiments are implemented on real-world
datasets and the proposed model is shown to greatly outperform state-of-the-art
methods, especially when the data is sparse.",arxiv
http://arxiv.org/abs/2006.16911v1,2020-06-19T10:59:32Z,2020-06-19T10:59:32Z,"Turbulence on the Global Economy influenced by Artificial Intelligence
  and Foreign Policy Inefficiencies","It is said that Data and Information are the new oil. One, who handles the
data, handles the emerging future of the global economy. Complex algorithms and
intelligence-based filter programs are utilized to manage, store, handle and
maneuver vast amounts of data for the fulfillment of specific purposes. This
paper seeks to find the bridge between artificial intelligence and its impact
on the international policy implementation in the light of geopolitical
influence, global economy and the future of labor markets. We hypothesize that
the distortion in the labor markets caused by artificial intelligence can be
mitigated by a collaborative international foreign policy on the deployment of
AI in the industrial circles. We, in this paper, then proceed to propose a
disposition for the essentials of AI-based foreign policy and implementation,
while asking questions such as 'could AI become the real Invisible Hand
discussed by economists?'.",arxiv
http://arxiv.org/abs/1903.12457v3,2019-06-05T07:35:08Z,2019-03-29T11:57:24Z,"Towards Knowledge-Based Personalized Product Description Generation in
  E-commerce","Quality product descriptions are critical for providing competitive customer
experience in an e-commerce platform. An accurate and attractive description
not only helps customers make an informed decision but also improves the
likelihood of purchase. However, crafting a successful product description is
tedious and highly time-consuming. Due to its importance, automating the
product description generation has attracted considerable interests from both
research and industrial communities. Existing methods mainly use templates or
statistical methods, and their performance could be rather limited. In this
paper, we explore a new way to generate the personalized product description by
combining the power of neural networks and knowledge base. Specifically, we
propose a KnOwledge Based pErsonalized (or KOBE) product description generation
model in the context of e-commerce. In KOBE, we extend the encoder-decoder
framework, the Transformer, to a sequence modeling formulation using
self-attention. In order to make the description both informative and
personalized, KOBE considers a variety of important factors during text
generation, including product aspects, user categories, and knowledge base,
etc. Experiments on real-world datasets demonstrate that the proposed method
out-performs the baseline on various metrics. KOBE can achieve an improvement
of 9.7% over state-of-the-arts in terms of BLEU. We also present several case
studies as the anecdotal evidence to further prove the effectiveness of the
proposed approach. The framework has been deployed in Taobao, the largest
online e-commerce platform in China.",arxiv
http://arxiv.org/abs/2109.13602v1,2021-09-28T10:23:46Z,2021-09-28T10:23:46Z,"SafetyNet: Safe planning for real-world self-driving vehicles using
  machine-learned policies","In this paper we present the first safe system for full control of
self-driving vehicles trained from human demonstrations and deployed in
challenging, real-world, urban environments. Current industry-standard
solutions use rule-based systems for planning. Although they perform reasonably
well in common scenarios, the engineering complexity renders this approach
incompatible with human-level performance. On the other hand, the performance
of machine-learned (ML) planning solutions can be improved by simply adding
more exemplar data. However, ML methods cannot offer safety guarantees and
sometimes behave unpredictably. To combat this, our approach uses a simple yet
effective rule-based fallback layer that performs sanity checks on an ML
planner's decisions (e.g. avoiding collision, assuring physical feasibility).
This allows us to leverage ML to handle complex situations while still assuring
the safety, reducing ML planner-only collisions by 95%. We train our ML planner
on 300 hours of expert driving demonstrations using imitation learning and
deploy it along with the fallback layer in downtown San Francisco, where it
takes complete control of a real vehicle and navigates a wide variety of
challenging urban driving scenarios.",arxiv
http://arxiv.org/abs/2008.13585v1,2020-08-26T14:03:25Z,2020-08-26T14:03:25Z,At Your Service: Coffee Beans Recommendation From a Robot Assistant,"With advances in the field of machine learning, precisely algorithms for
recommendation systems, robot assistants are envisioned to become more present
in the hospitality industry. Additionally, the COVID-19 pandemic has also
highlighted the need to have more service robots in our everyday lives, to
minimise the risk of human to-human transmission. One such example would be
coffee shops, which have become intrinsic to our everyday lives. However,
serving an excellent cup of coffee is not a trivial feat as a coffee blend
typically comprises rich aromas, indulgent and unique flavours and a lingering
aftertaste. Our work addresses this by proposing a computational model which
recommends optimal coffee beans resulting from the user's preferences.
Specifically, given a set of coffee bean properties (objective features), we
apply different supervised learning techniques to predict coffee qualities
(subjective features). We then consider an unsupervised learning method to
analyse the relationship between coffee beans in the subjective feature space.
Evaluated on a real coffee beans dataset based on digitised reviews, our
results illustrate that the proposed computational model gives up to 92.7
percent recommendation accuracy for coffee beans prediction. From this, we
propose how this computational model can be deployed on a service robot to
reliably predict customers' coffee bean preferences, starting from the user
inputting their coffee preferences to the robot recommending the coffee beans
that best meet the user's likings.",arxiv
http://arxiv.org/abs/1811.07315v1,2018-11-18T11:28:24Z,2018-11-18T11:28:24Z,"Learning to infer: RL-based search for DNN primitive selection on
  Heterogeneous Embedded Systems","Deep Learning is increasingly being adopted by industry for computer vision
applications running on embedded devices. While Convolutional Neural Networks'
accuracy has achieved a mature and remarkable state, inference latency and
throughput are a major concern especially when targeting low-cost and low-power
embedded platforms. CNNs' inference latency may become a bottleneck for Deep
Learning adoption by industry, as it is a crucial specification for many
real-time processes. Furthermore, deployment of CNNs across heterogeneous
platforms presents major compatibility issues due to vendor-specific technology
and acceleration libraries. In this work, we present QS-DNN, a fully automatic
search based on Reinforcement Learning which, combined with an inference engine
optimizer, efficiently explores through the design space and empirically finds
the optimal combinations of libraries and primitives to speed up the inference
of CNNs on heterogeneous embedded devices. We show that, an optimized
combination can achieve 45x speedup in inference latency on CPU compared to a
dependency-free baseline and 2x on average on GPGPU compared to the best vendor
library. Further, we demonstrate that, the quality of results and time
""to-solution"" is much better than with Random Search and achieves up to 15x
better results for a short-time search.",arxiv
http://arxiv.org/abs/1609.09296v1,2016-09-29T11:03:21Z,2016-09-29T11:03:21Z,"Comprehensive Evaluation of OpenCL-based Convolutional Neural Network
  Accelerators in Xilinx and Altera FPGAs","Deep learning has significantly advanced the state of the art in artificial
intelligence, gaining wide popularity from both industry and academia. Special
interest is around Convolutional Neural Networks (CNN), which take inspiration
from the hierarchical structure of the visual cortex, to form deep layers of
convolutional operations, along with fully connected classifiers. Hardware
implementations of these deep CNN architectures are challenged with memory
bottlenecks that require many convolution and fully-connected layers demanding
large amount of communication for parallel computation. Multi-core CPU based
solutions have demonstrated their inadequacy for this problem due to the memory
wall and low parallelism. Many-core GPU architectures show superior performance
but they consume high power and also have memory constraints due to
inconsistencies between cache and main memory. FPGA design solutions are also
actively being explored, which allow implementing the memory hierarchy using
embedded BlockRAM. This boosts the parallel use of shared memory elements
between multiple processing units, avoiding data replicability and
inconsistencies. This makes FPGAs potentially powerful solutions for real-time
classification of CNNs. Both Altera and Xilinx have adopted OpenCL co-design
framework from GPU for FPGA designs as a pseudo-automatic development solution.
In this paper, a comprehensive evaluation and comparison of Altera and Xilinx
OpenCL frameworks for a 5-layer deep CNN is presented. Hardware resources,
temporal performance and the OpenCL architecture for CNNs are discussed. Xilinx
demonstrates faster synthesis, better FPGA resource utilization and more
compact boards. Altera provides multi-platforms tools, mature design community
and better execution times.",arxiv
http://arxiv.org/abs/2105.12899v1,2021-05-27T01:16:00Z,2021-05-27T01:16:00Z,Learning to Optimize Industry-Scale Dynamic Pickup and Delivery Problems,"The Dynamic Pickup and Delivery Problem (DPDP) is aimed at dynamically
scheduling vehicles among multiple sites in order to minimize the cost when
delivery orders are not known a priori. Although DPDP plays an important role
in modern logistics and supply chain management, state-of-the-art DPDP
algorithms are still limited on their solution quality and efficiency. In
practice, they fail to provide a scalable solution as the numbers of vehicles
and sites become large. In this paper, we propose a data-driven approach,
Spatial-Temporal Aided Double Deep Graph Network (ST-DDGN), to solve
industry-scale DPDP. In our method, the delivery demands are first forecast
using spatial-temporal prediction method, which guides the neural network to
perceive spatial-temporal distribution of delivery demand when dispatching
vehicles. Besides, the relationships of individuals such as vehicles are
modelled by establishing a graph-based value function. ST-DDGN incorporates
attention-based graph embedding with Double DQN (DDQN). As such, it can make
the inference across vehicles more efficiently compared with traditional
methods. Our method is entirely data driven and thus adaptive, i.e., the
relational representation of adjacent vehicles can be learned and corrected by
ST-DDGN from data periodically. We have conducted extensive experiments over
real-world data to evaluate our solution. The results show that ST-DDGN reduces
11.27% number of the used vehicles and decreases 13.12% total transportation
cost on average over the strong baselines, including the heuristic algorithm
deployed in our UAT (User Acceptance Test) environment and a variety of vanilla
DRL methods. We are due to fully deploy our solution into our online logistics
system and it is estimated that millions of USD logistics cost can be saved per
year.",arxiv
http://arxiv.org/abs/1805.04752v1,2018-05-12T17:05:56Z,2018-05-12T17:05:56Z,"Generating Rescheduling Knowledge using Reinforcement Learning in a
  Cognitive Architecture","In order to reach higher degrees of flexibility, adaptability and autonomy in
manufacturing systems, it is essential to develop new rescheduling
methodologies which resort to cognitive capabilities, similar to those found in
human beings. Artificial cognition is important for designing planning and
control systems that generate and represent knowledge about heuristics for
repair-based scheduling. Rescheduling knowledge in the form of decision rules
is used to deal with unforeseen events and disturbances reactively in real
time, and take advantage of the ability to act interactively with the user to
counteract the effects of disruptions. In this work, to achieve the
aforementioned goals, a novel approach to generate rescheduling knowledge in
the form of dynamic first-order logical rules is proposed. The proposed
approach is based on the integration of reinforcement learning with artificial
cognitive capabilities involving perception and reasoning/learning skills
embedded in the Soar cognitive architecture. An industrial example is discussed
showing that the approach enables the scheduling system to assess its
operational range in an autonomic way, and to acquire experience through
intensive simulation while performing repair tasks.",arxiv
http://arxiv.org/abs/2011.02738v1,2020-11-05T10:16:54Z,2020-11-05T10:16:54Z,"Switching Scheme: A Novel Approach for Handling Incremental Concept
  Drift in Real-World Data Sets","Machine learning models nowadays play a crucial role for many applications in
business and industry. However, models only start adding value as soon as they
are deployed into production. One challenge of deployed models is the effect of
changing data over time, which is often described with the term concept drift.
Due to their nature, concept drifts can severely affect the prediction
performance of a machine learning system. In this work, we analyze the effects
of concept drift in the context of a real-world data set. For efficient concept
drift handling, we introduce the switching scheme which combines the two
principles of retraining and updating of a machine learning model. Furthermore,
we systematically analyze existing regular adaptation as well as triggered
adaptation strategies. The switching scheme is instantiated on New York City
taxi data, which is heavily influenced by changing demand patterns over time.
We can show that the switching scheme outperforms all other baselines and
delivers promising prediction results.",arxiv
http://arxiv.org/abs/1912.00778v1,2019-11-27T15:48:26Z,2019-11-27T15:48:26Z,"Learning a faceted customer segmentation for discovering new business
  opportunities at Intel","For sales and marketing organizations within large enterprises, identifying
and understanding new markets, customers and partners is a key challenge.
Intel's Sales and Marketing Group (SMG) faces similar challenges while growing
in new markets and domains and evolving its existing business. In today's
complex technological and commercial landscape, there is need for intelligent
automation supporting a fine-grained understanding of businesses in order to
help SMG sift through millions of companies across many geographies and
languages and identify relevant directions. We present a system developed in
our company that mines millions of public business web pages, and extracts a
faceted customer representation. We focus on two key customer aspects that are
essential for finding relevant opportunities: industry segments (ranging from
broad verticals such as healthcare, to more specific fields such as 'video
analytics') and functional roles (e.g., 'manufacturer' or 'retail'). To address
the challenge of labeled data collection, we enrich our data with external
information gleaned from Wikipedia, and develop a semi-supervised multi-label,
multi-lingual deep learning model that parses customer website texts and
classifies them into their respective facets. Our system scans and indexes
companies as part of a large-scale knowledge graph that currently holds tens of
millions of connected entities with thousands being fetched, enriched and
connected to the graph by the hour in real time, and also supports knowledge
and insight discovery. In experiments conducted in our company, we are able to
significantly boost the performance of sales personnel in the task of
discovering new customers and commercial partnership opportunities.",arxiv
http://arxiv.org/abs/2106.00241v1,2021-06-01T05:46:22Z,2021-06-01T05:46:22Z,"Reinforced Iterative Knowledge Distillation for Cross-Lingual Named
  Entity Recognition","Named entity recognition (NER) is a fundamental component in many
applications, such as Web Search and Voice Assistants. Although deep neural
networks greatly improve the performance of NER, due to the requirement of
large amounts of training data, deep neural networks can hardly scale out to
many languages in an industry setting. To tackle this challenge, cross-lingual
NER transfers knowledge from a rich-resource language to languages with low
resources through pre-trained multilingual language models. Instead of using
training data in target languages, cross-lingual NER has to rely on only
training data in source languages, and optionally adds the translated training
data derived from source languages. However, the existing cross-lingual NER
methods do not make good use of rich unlabeled data in target languages, which
is relatively easy to collect in industry applications. To address the
opportunities and challenges, in this paper we describe our novel practice in
Microsoft to leverage such large amounts of unlabeled data in target languages
in real production settings. To effectively extract weak supervision signals
from the unlabeled data, we develop a novel approach based on the ideas of
semi-supervised learning and reinforcement learning. The empirical study on
three benchmark data sets verifies that our approach establishes the new
state-of-the-art performance with clear edges. Now, the NER techniques reported
in this paper are on their way to become a fundamental component for Web
ranking, Entity Pane, Answers Triggering, and Question Answering in the
Microsoft Bing search engine. Moreover, our techniques will also serve as part
of the Spoken Language Understanding module for a commercial voice assistant.
We plan to open source the code of the prototype framework after deployment.",arxiv
http://arxiv.org/abs/2103.00821v1,2021-03-01T07:41:34Z,2021-03-01T07:41:34Z,"Rethinking complexity for software code structures: A pioneering study
  on Linux kernel code repository","The recent progress of artificial intelligence(AI) has shown great potentials
for alleviating human burden in various complex tasks. From the view of
software engineering, AI techniques can be seen in many fundamental aspects of
development, such as source code comprehension, in which state-of-the-art
models are implemented to extract and express the meaning of code snippets
automatically. However, such technologies are still struggling to tackle and
comprehend the complex structures within industrial code, thus far from
real-world applications. In the present work, we built an innovative and
systematical framework, emphasizing the problem of complexity in code
comprehension and further software engineering. Upon automatic data collection
from the latest Linux kernel source code, we modeled code structures as complex
networks through token extraction and relation parsing. Comprehensive analysis
of complexity further revealed the density and scale of network-based code
representations. Our work constructed the first large-scale dataset from
industrial-strength software code for downstream software engineering tasks
including code comprehension, and incorporated complex network theory into
code-level investigations of software development for the first time. In the
longer term, the proposed methodology could play significant roles in the
entire software engineering process, powering software design, coding,
debugging, testing, and sustaining by redefining and embracing complexity.",arxiv
http://arxiv.org/abs/2104.09876v1,2021-04-20T10:16:04Z,2021-04-20T10:16:04Z,"IIoT-Enabled Health Monitoring for Integrated Heat Pump System Using
  Mixture Slow Feature Analysis","The sustaining evolution of sensing and advancement in communications
technologies have revolutionized prognostics and health management for various
electrical equipment towards data-driven ways. This revolution delivers a
promising solution for the health monitoring problem of heat pump (HP) system,
a vital device widely deployed in modern buildings for heating use, to timely
evaluate its operation status to avoid unexpected downtime. Many HPs were
practically manufactured and installed many years ago, resulting in fewer
sensors available due to technology limitations and cost control at that time.
It raises a dilemma to safeguard HPs at an affordable cost. We propose a hybrid
scheme by integrating industrial Internet-of-Things (IIoT) and intelligent
health monitoring algorithms to handle this challenge. To start with, an IIoT
network is constructed to sense and store measurements. Specifically,
temperature sensors are properly chosen and deployed at the inlet and outlet of
the water tank to measure water temperature. Second, with temperature
information, we propose an unsupervised learning algorithm named mixture slow
feature analysis (MSFA) to timely evaluate the health status of the integrated
HP. Characterized by frequent operation switches of different HPs due to the
variable demand for hot water, various heating patterns with different heating
speeds are observed. Slowness, a kind of dynamics to measure the varying speed
of steady distribution, is properly considered in MSFA for both heating pattern
division and health evaluation. Finally, the efficacy of the proposed method is
verified through a real integrated HP with five connected HPs installed ten
years ago. The experimental results show that MSFA is capable of accurately
identifying health status of the system, especially failure at a preliminary
stage compared to its competing algorithms.",arxiv
http://arxiv.org/abs/2101.00703v1,2021-01-03T20:56:56Z,2021-01-03T20:56:56Z,"Automatic Defect Detection of Print Fabric Using Convolutional Neural
  Network","Automatic defect detection is a challenging task because of the variability
in texture and type of fabric defects. An effective defect detection system
enables manufacturers to improve the quality of processes and products.
Automation across the textile manufacturing systems would reduce fabric wastage
and increase profitability by saving cost and resources. There are different
contemporary research on automatic defect detection systems using image
processing and machine learning techniques. These techniques differ from each
other based on the manufacturing processes and defect types. Researchers have
also been able to establish real-time defect detection system during weaving.
Although, there has been research on patterned fabric defect detection, these
defects are related to weaving faults such as holes, and warp and weft defects.
But, there has not been any research that is designed to detect defects that
arise during such as spot and print mismatch. This research has fulfilled this
gap by developing a print fabric database and implementing deep convolutional
neural network (CNN).",arxiv
http://arxiv.org/abs/1908.01853v1,2019-08-02T01:13:50Z,2019-08-02T01:13:50Z,DELTA: A DEep learning based Language Technology plAtform,"In this paper we present DELTA, a deep learning based language technology
platform. DELTA is an end-to-end platform designed to solve industry level
natural language and speech processing problems. It integrates most popular
neural network models for training as well as comprehensive deployment tools
for production. DELTA aims to provide easy and fast experiences for using,
deploying, and developing natural language processing and speech models for
both academia and industry use cases. We demonstrate the reliable performance
with DELTA on several natural language processing and speech tasks, including
text classification, named entity recognition, natural language inference,
speech recognition, speaker verification, etc. DELTA has been used for
developing several state-of-the-art algorithms for publications and delivering
real production to serve millions of users.",arxiv
http://arxiv.org/abs/2006.00429v1,2020-05-31T03:55:41Z,2020-05-31T03:55:41Z,Pseudo-Representation Labeling Semi-Supervised Learning,"In recent years, semi-supervised learning (SSL) has shown tremendous success
in leveraging unlabeled data to improve the performance of deep learning
models, which significantly reduces the demand for large amounts of labeled
data. Many SSL techniques have been proposed and have shown promising
performance on famous datasets such as ImageNet and CIFAR-10. However, some
exiting techniques (especially data augmentation based) are not suitable for
industrial applications empirically. Therefore, this work proposes the
pseudo-representation labeling, a simple and flexible framework that utilizes
pseudo-labeling techniques to iteratively label a small amount of unlabeled
data and use them as training data. In addition, our framework is integrated
with self-supervised representation learning such that the classifier gains
benefits from representation learning of both labeled and unlabeled data. This
framework can be implemented without being limited at the specific model
structure, but a general technique to improve the existing model. Compared with
the existing approaches, the pseudo-representation labeling is more intuitive
and can effectively solve practical problems in the real world. Empirically, it
outperforms the current state-of-the-art semi-supervised learning methods in
industrial types of classification problems such as the WM-811K wafer map and
the MIT-BIH Arrhythmia dataset.",arxiv
http://arxiv.org/abs/2008.00181v2,2021-01-14T00:15:23Z,2020-08-01T06:02:16Z,"Relation-aware Meta-learning for Market Segment Demand Prediction with
  Limited Records","E-commerce business is revolutionizing our shopping experiences by providing
convenient and straightforward services. One of the most fundamental problems
is how to balance the demand and supply in market segments to build an
efficient platform. While conventional machine learning models have achieved
great success on data-sufficient segments, it may fail in a large-portion of
segments in E-commerce platforms, where there are not sufficient records to
learn well-trained models. In this paper, we tackle this problem in the context
of market segment demand prediction. The goal is to facilitate the learning
process in the target segments by leveraging the learned knowledge from
data-sufficient source segments. Specifically, we propose a novel algorithm,
RMLDP, to incorporate a multi-pattern fusion network (MPFN) with a
meta-learning paradigm. The multi-pattern fusion network considers both local
and seasonal temporal patterns for segment demand prediction. In the
meta-learning paradigm, transferable knowledge is regarded as the model
parameter initialization of MPFN, which are learned from diverse source
segments. Furthermore, we capture the segment relations by combining
data-driven segment representation and segment knowledge graph representation
and tailor the segment-specific relations to customize transferable model
parameter initialization. Thus, even with limited data, the target segment can
quickly find the most relevant transferred knowledge and adapt to the optimal
parameters. We conduct extensive experiments on two large-scale industrial
datasets. The results justify that our RMLDP outperforms a set of
state-of-the-art baselines. Besides, RMLDP has been deployed in Taobao, a
real-world E-commerce platform. The online A/B testing results further
demonstrate the practicality of RMLDP.",arxiv
http://arxiv.org/abs/2003.11637v1,2020-02-24T13:26:34Z,2020-02-24T13:26:34Z,Bio-inspired Optimization: metaheuristic algorithms for optimization,"In today's day and time solving real-world complex problems has become
fundamentally vital and critical task. Many of these are combinatorial
problems, where optimal solutions are sought rather than exact solutions.
Traditional optimization methods are found to be effective for small scale
problems. However, for real-world large scale problems, traditional methods
either do not scale up or fail to obtain optimal solutions or they end-up
giving solutions after a long running time. Even earlier artificial
intelligence based techniques used to solve these problems could not give
acceptable results. However, last two decades have seen many new methods in AI
based on the characteristics and behaviors of the living organisms in the
nature which are categorized as bio-inspired or nature inspired optimization
algorithms. These methods, are also termed meta-heuristic optimization methods,
have been proved theoretically and implemented using simulation as well used to
create many useful applications. They have been used extensively to solve many
industrial and engineering complex problems due to being easy to understand,
flexible, simple to adapt to the problem at hand and most importantly their
ability to come out of local optima traps. This local optima avoidance property
helps in finding global optimal solutions. This paper is aimed at understanding
how nature has inspired many optimization algorithms, basic categorization of
them, major bio-inspired optimization algorithms invented in recent time with
their applications.",arxiv
http://arxiv.org/abs/1709.01648v1,2017-09-06T01:36:12Z,2017-09-06T01:36:12Z,"Boosting Deep Learning Risk Prediction with Generative Adversarial
  Networks for Electronic Health Records","The rapid growth of Electronic Health Records (EHRs), as well as the
accompanied opportunities in Data-Driven Healthcare (DDH), has been attracting
widespread interests and attentions. Recent progress in the design and
applications of deep learning methods has shown promising results and is
forcing massive changes in healthcare academia and industry, but most of these
methods rely on massive labeled data. In this work, we propose a general deep
learning framework which is able to boost risk prediction performance with
limited EHR data. Our model takes a modified generative adversarial network
namely ehrGAN, which can provide plausible labeled EHR data by mimicking real
patient records, to augment the training dataset in a semi-supervised learning
manner. We use this generative model together with a convolutional neural
network (CNN) based prediction model to improve the onset prediction
performance. Experiments on two real healthcare datasets demonstrate that our
proposed framework produces realistic data samples and achieves significant
improvements on classification tasks with the generated data over several
stat-of-the-art baselines.",arxiv
http://arxiv.org/abs/1910.02078v4,2020-08-13T12:21:07Z,2019-10-04T16:43:06Z,"I'm sorry Dave, I'm afraid I can't do that, Deep Q-learning from
  forbidden action","The use of Reinforcement Learning (RL) is still restricted to simulation or
to enhance human-operated systems through recommendations. Real-world
environments (e.g. industrial robots or power grids) are generally designed
with safety constraints in mind implemented in the shape of valid actions masks
or contingency controllers. For example, the range of motion and the angles of
the motors of a robot can be limited to physical boundaries. Violating
constraints thus results in rejected actions or entering in a safe mode driven
by an external controller, making RL agents incapable of learning from their
mistakes. In this paper, we propose a simple modification of a state-of-the-art
deep RL algorithm (DQN), enabling learning from forbidden actions. To do so,
the standard Q-learning update is enhanced with an extra safety loss inspired
by structured classification. We empirically show that it reduces the number of
hit constraints during the learning phase and accelerates convergence to
near-optimal policies compared to using standard DQN. Experiments are done on a
Visual Grid World Environment and Text-World domain.",arxiv
http://arxiv.org/abs/1904.01719v1,2019-04-03T00:56:14Z,2019-04-03T00:56:14Z,"Empirical Evaluations of Active Learning Strategies in Legal Document
  Review","One type of machine learning, text classification, is now regularly applied
in the legal matters involving voluminous document populations because it can
reduce the time and expense associated with the review of those documents. One
form of machine learning - Active Learning - has drawn attention from the legal
community because it offers the potential to make the machine learning process
even more effective. Active Learning, applied to legal documents, is considered
a new technology in the legal domain and is continuously applied to all
documents in a legal matter until an insignificant number of relevant documents
are left for review. This implementation is slightly different than traditional
implementations of Active Learning where the process stops once achieving
acceptable model performance. The purpose of this paper is twofold: (i) to
question whether Active Learning actually is a superior learning methodology
and (ii) to highlight the ways that Active Learning can be most effectively
applied to real legal industry data. Unlike other studies, our experiments were
performed against large data sets taken from recent, real-world legal matters
covering a variety of areas. We conclude that, although these experiments show
the Active Learning strategy popularly used in legal document review can
quickly identify informative training documents, it becomes less effective over
time. In particular, our findings suggest this most popular form of Active
Learning in the legal arena, where the highest-scoring documents are selected
as training examples, is in fact not the most efficient approach in most
instances. Ultimately, a different Active Learning strategy may be best suited
to initiate the predictive modeling process but not to continue through the
entire document review.",arxiv
http://arxiv.org/abs/2101.10876v1,2021-01-26T15:33:22Z,2021-01-26T15:33:22Z,Blind Image Denoising and Inpainting Using Robust Hadamard Autoencoders,"In this paper, we demonstrate how deep autoencoders can be generalized to the
case of inpainting and denoising, even when no clean training data is
available. In particular, we show how neural networks can be trained to perform
all of these tasks simultaneously. While, deep autoencoders implemented by way
of neural networks have demonstrated potential for denoising and anomaly
detection, standard autoencoders have the drawback that they require access to
clean data for training. However, recent work in Robust Deep Autoencoders
(RDAEs) shows how autoencoders can be trained to eliminate outliers and noise
in a dataset without access to any clean training data. Inspired by this work,
we extend RDAEs to the case where data are not only noisy and have outliers,
but also only partially observed. Moreover, the dataset we train the neural
network on has the properties that all entries have noise, some entries are
corrupted by large mistakes, and many entries are not even known. Given such an
algorithm, many standard tasks, such as denoising, image inpainting, and
unobserved entry imputation can all be accomplished simultaneously within the
same framework. Herein we demonstrate these techniques on standard machine
learning tasks, such as image inpainting and denoising for the MNIST and
CIFAR10 datasets. However, these approaches are not only applicable to image
processing problems, but also have wide ranging impacts on datasets arising
from real-world problems, such as manufacturing and network processing, where
noisy, partially observed data naturally arise.",arxiv
http://arxiv.org/abs/1806.05434v1,2018-06-14T09:44:59Z,2018-06-14T09:44:59Z,"Transfer Learning for Context-Aware Question Matching in
  Information-seeking Conversations in E-commerce","Building multi-turn information-seeking conversation systems is an important
and challenging research topic. Although several advanced neural text matching
models have been proposed for this task, they are generally not efficient for
industrial applications. Furthermore, they rely on a large amount of labeled
data, which may not be available in real-world applications. To alleviate these
problems, we study transfer learning for multi-turn information seeking
conversations in this paper. We first propose an efficient and effective
multi-turn conversation model based on convolutional neural networks. After
that, we extend our model to adapt the knowledge learned from a resource-rich
domain to enhance the performance. Finally, we deployed our model in an
industrial chatbot called AliMe Assist
(https://consumerservice.taobao.com/online-help) and observed a significant
improvement over the existing online model.",arxiv
http://arxiv.org/abs/1906.08864v1,2019-06-01T18:49:57Z,2019-06-01T18:49:57Z,"Accurate and Energy-Efficient Classification with Spiking Random Neural
  Network: Corrected and Expanded Version","Artificial Neural Network (ANN) based techniques have dominated
state-of-the-art results in most problems related to computer vision, audio
recognition, and natural language processing in the past few years, resulting
in strong industrial adoption from all leading technology companies worldwide.
One of the major obstacles that have historically delayed large scale adoption
of ANNs is the huge computational and power costs associated with training and
testing (deploying) them. In the mean-time, Neuromorphic Computing platforms
have recently achieved remarkable performance running more bio-realistic
Spiking Neural Networks at high throughput and very low power consumption
making them a natural alternative to ANNs. Here, we propose using the Random
Neural Network (RNN), a spiking neural network with both theoretical and
practical appealing properties, as a general purpose classifier that can match
the classification power of ANNs on a number of tasks while enjoying all the
features of a spiking neural network. This is demonstrated on a number of
real-world classification datasets.",arxiv
http://arxiv.org/abs/2005.05815v1,2020-05-12T14:30:03Z,2020-05-12T14:30:03Z,One-Shot Recognition of Manufacturing Defects in Steel Surfaces,"Quality control is an essential process in manufacturing to make the product
defect-free as well as to meet customer needs. The automation of this process
is important to maintain high quality along with the high manufacturing
throughput. With recent developments in deep learning and computer vision
technologies, it has become possible to detect various features from the images
with near-human accuracy. However, many of these approaches are data intensive.
Training and deployment of such a system on manufacturing floors may become
expensive and time-consuming. The need for large amounts of training data is
one of the limitations of the applicability of these approaches in real-world
manufacturing systems. In this work, we propose the application of a Siamese
convolutional neural network to do one-shot recognition for such a task. Our
results demonstrate how one-shot learning can be used in quality control of
steel by identification of defects on the steel surface. This method can
significantly reduce the requirements of training data and can also be run in
real-time.",arxiv
http://arxiv.org/abs/2105.08649v3,2021-08-22T05:41:15Z,2021-05-18T16:27:20Z,"DCAP: Deep Cross Attentional Product Network for User Response
  Prediction","User response prediction, which aims to predict the probability that a user
will provide a predefined positive response in a given context such as clicking
on an ad or purchasing an item, is crucial to many industrial applications such
as online advertising, recommender systems, and search ranking. However, due to
the high dimensionality and super sparsity of the data collected in these
tasks, handcrafting cross features is inevitably time expensive. Prior studies
in predicting user response leveraged the feature interactions by enhancing
feature vectors with products of features to model second-order or high-order
cross features, either explicitly or implicitly. Nevertheless, these existing
methods can be hindered by not learning sufficient cross features due to model
architecture limitations or modeling all high-order feature interactions with
equal weights. This work aims to fill this gap by proposing a novel
architecture Deep Cross Attentional Product Network (DCAP), which keeps cross
network's benefits in modeling high-order feature interactions explicitly at
the vector-wise level. Beyond that, it can differentiate the importance of
different cross features in each network layer inspired by the multi-head
attention mechanism and Product Neural Network (PNN), allowing practitioners to
perform a more in-depth analysis of user behaviors. Additionally, our proposed
model can be easily implemented and train in parallel. We conduct comprehensive
experiments on three real-world datasets. The results have robustly
demonstrated that our proposed model DCAP achieves superior prediction
performance compared with the state-of-the-art models. Public codes are
available at https://github.com/zachstarkk/DCAP.",arxiv
http://arxiv.org/abs/2006.03616v2,2020-10-26T15:31:56Z,2020-06-05T18:11:14Z,"High-level Modeling of Manufacturing Faults in Deep Neural Network
  Accelerators","The advent of data-driven real-time applications requires the implementation
of Deep Neural Networks (DNNs) on Machine Learning accelerators. Google's
Tensor Processing Unit (TPU) is one such neural network accelerator that uses
systolic array-based matrix multiplication hardware for computation in its
crux. Manufacturing faults at any state element of the matrix multiplication
unit can cause unexpected errors in these inference networks. In this paper, we
propose a formal model of permanent faults and their propagation in a TPU using
the Discrete-Time Markov Chain (DTMC) formalism. The proposed model is analyzed
using the probabilistic model checking technique to reason about the likelihood
of faulty outputs. The obtained quantitative results show that the
classification accuracy is sensitive to the type of permanent faults as well as
their location, bit position and the number of layers in the neural network.
The conclusions from our theoretical model have been validated using
experiments on a digit recognition-based DNN.",arxiv
http://arxiv.org/abs/2006.06082v3,2021-02-13T23:23:08Z,2020-06-10T21:54:27Z,Towards Integrating Fairness Transparently in Industrial Applications,"Numerous Machine Learning (ML) bias-related failures in recent years have led
to scrutiny of how companies incorporate aspects of transparency and
accountability in their ML lifecycles. Companies have a responsibility to
monitor ML processes for bias and mitigate any bias detected, ensure business
product integrity, preserve customer loyalty, and protect brand image.
Challenges specific to industry ML projects can be broadly categorized into
principled documentation, human oversight, and need for mechanisms that enable
information reuse and improve cost efficiency. We highlight specific roadblocks
and propose conceptual solutions on a per-category basis for ML practitioners
and organizational subject matter experts. Our systematic approach tackles
these challenges by integrating mechanized and human-in-the-loop components in
bias detection, mitigation, and documentation of projects at various stages of
the ML lifecycle. To motivate the implementation of our system -- SIFT (System
to Integrate Fairness Transparently) -- we present its structural primitives
with an example real-world use case on how it can be used to identify potential
biases and determine appropriate mitigation strategies in a participatory
manner.",arxiv
http://arxiv.org/abs/0706.1051v1,2007-06-07T18:13:59Z,2007-06-07T18:13:59Z,"Improved Neural Modeling of Real-World Systems Using Genetic Algorithm
  Based Variable Selection","Neural network models of real-world systems, such as industrial processes,
made from sensor data must often rely on incomplete data. System states may not
all be known, sensor data may be biased or noisy, and it is not often known
which sensor data may be useful for predictive modelling. Genetic algorithms
may be used to help to address this problem by determining the near optimal
subset of sensor variables most appropriate to produce good models. This paper
describes the use of genetic search to optimize variable selection to determine
inputs into the neural network model. We discuss genetic algorithm
implementation issues including data representation types and genetic operators
such as crossover and mutation. We present the use of this technique for neural
network modelling of a typical industrial application, a liquid fed ceramic
melter, and detail the results of the genetic search to optimize the neural
network model for this application.",arxiv
http://arxiv.org/abs/2012.10489v2,2021-02-24T04:38:47Z,2020-12-18T19:54:19Z,"XAI4Wind: A Multimodal Knowledge Graph Database for Explainable Decision
  Support in Operations & Maintenance of Wind Turbines","Condition-based monitoring (CBM) has been widely utilised in the wind
industry for monitoring operational inconsistencies and failures in turbines,
with techniques ranging from signal processing and vibration analysis to
artificial intelligence (AI) models using Supervisory Control & Acquisition
(SCADA) data. However, existing studies do not present a concrete basis to
facilitate explainable decision support in operations and maintenance (O&M),
particularly for automated decision support through recommendation of
appropriate maintenance action reports corresponding to failures predicted by
CBM techniques. Knowledge graph databases (KGs) model a collection of
domain-specific information and have played an intrinsic role for real-world
decision support in domains such as healthcare and finance, but have seen very
limited attention in the wind industry. We propose XAI4Wind, a multimodal
knowledge graph for explainable decision support in real-world operational
turbines and demonstrate through experiments several use-cases of the proposed
KG towards O&M planning through interactive query and reasoning and providing
novel insights using graph data science algorithms. The proposed KG combines
multimodal knowledge like SCADA parameters and alarms with natural language
maintenance actions, images etc. By integrating our KG with an Explainable AI
model for anomaly prediction, we show that it can provide effective
human-intelligible O&M strategies for predicted operational inconsistencies in
various turbine sub-components. This can help instil better trust and
confidence in conventionally black-box AI models. We make our KG publicly
available and envisage that it can serve as the building ground for providing
autonomous decision support in the wind industry.",arxiv
http://arxiv.org/abs/2104.13190v1,2021-04-27T16:21:48Z,2021-04-27T16:21:48Z,Extending Isolation Forest for Anomaly Detection in Big Data via K-Means,"Industrial Information Technology (IT) infrastructures are often vulnerable
to cyberattacks. To ensure security to the computer systems in an industrial
environment, it is required to build effective intrusion detection systems to
monitor the cyber-physical systems (e.g., computer networks) in the industry
for malicious activities. This paper aims to build such intrusion detection
systems to protect the computer networks from cyberattacks. More specifically,
we propose a novel unsupervised machine learning approach that combines the
K-Means algorithm with the Isolation Forest for anomaly detection in industrial
big data scenarios. Since our objective is to build the intrusion detection
system for the big data scenario in the industrial domain, we utilize the
Apache Spark framework to implement our proposed model which was trained in
large network traffic data (about 123 million instances of network traffic)
stored in Elasticsearch. Moreover, we evaluate our proposed model on the live
streaming data and find that our proposed system can be used for real-time
anomaly detection in the industrial setup. In addition, we address different
challenges that we face while training our model on large datasets and
explicitly describe how these issues were resolved. Based on our empirical
evaluation in different use-cases for anomaly detection in real-world network
traffic data, we observe that our proposed system is effective to detect
anomalies in big data scenarios. Finally, we evaluate our proposed model on
several academic datasets to compare with other models and find that it
provides comparable performance with other state-of-the-art approaches.",arxiv
http://arxiv.org/abs/2008.06448v1,2020-08-14T16:17:54Z,2020-08-14T16:17:54Z,"Loghub: A Large Collection of System Log Datasets towards Automated Log
  Analytics","Logs have been widely adopted in software system development and maintenance
because of the rich system runtime information they contain. In recent years,
the increase of software size and complexity leads to the rapid growth of the
volume of logs. To handle these large volumes of logs efficiently and
effectively, a line of research focuses on intelligent log analytics powered by
AI (artificial intelligence) techniques. However, only a small fraction of
these techniques have reached successful deployment in industry because of the
lack of public log datasets and necessary benchmarking upon them. To fill this
significant gap between academia and industry and also facilitate more research
on AI-powered log analytics, we have collected and organized loghub, a large
collection of log datasets. In particular, loghub provides 17 real-world log
datasets collected from a wide range of systems, including distributed systems,
supercomputers, operating systems, mobile systems, server applications, and
standalone software. In this paper, we summarize the statistics of these
datasets, introduce some practical log usage scenarios, and present a case
study on anomaly detection to demonstrate how loghub facilitates the research
and practice in this field. Up to the time of this paper writing, loghub
datasets have been downloaded over 15,000 times by more than 380 organizations
from both industry and academia.",arxiv
http://arxiv.org/abs/1905.13118v1,2019-05-30T15:50:14Z,2019-05-30T15:50:14Z,"Standing on the Shoulders of Giants: AI-driven Calibration of
  Localisation Technologies","High accuracy localisation technologies exist but are prohibitively expensive
to deploy for large indoor spaces such as warehouses, factories, and
supermarkets to track assets and people. However, these technologies can be
used to lend their highly accurate localisation capabilities to low-cost,
commodity, and less-accurate technologies. In this paper, we bridge this link
by proposing a technology-agnostic calibration framework based on artificial
intelligence to assist such low-cost technologies through highly accurate
localisation systems. A single-layer neural network is used to calibrate less
accurate technology using more accurate one such as BLE using UWB and UWB using
a professional motion tracking system. On a real indoor testbed, we demonstrate
an increase in accuracy of approximately 70% for BLE and 50% for UWB. Not only
the proposed approach requires a very short measurement campaign, the low
complexity of the single-layer neural network also makes it ideal for
deployment on constrained devices typically for localisation purposes.",arxiv
http://arxiv.org/abs/2005.06342v1,2020-05-09T05:54:28Z,2020-05-09T05:54:28Z,"sCrop: A Internet-of-Agro-Things (IoAT) Enabled Solar Powered Smart
  Device for Automatic Plant Disease Prediction","Internet-of-Things (IoT) is omnipresent, ranging from home solutions to
turning wheels for the fourth industrial revolution. This article presents the
novel concept of Internet-of-Agro-Things (IoAT) with an example of automated
plant disease prediction. It consists of solar enabled sensor nodes which help
in continuous sensing and automating agriculture. The existing solutions have
implemented a battery powered sensor node. On the contrary, the proposed system
has adopted the use of an energy efficient way of powering using solar energy.
It is observed that around 80% of the crops are attacked with microbial
diseases in traditional agriculture. To prevent this, a health maintenance
system is integrated with the sensor node, which captures the image of the crop
and performs an analysis with the trained Convolutional Neural Network (CNN)
model. The deployment of the proposed system is demonstrated in a real-time
environment using a microcontroller, solar sensor nodes with a camera module,
and an mobile application for the farmers visualization of the farms. The
deployed prototype was deployed for two months and has achieved a robust
performance by sustaining in varied weather conditions and continued to remain
rust-free. The proposed deep learning framework for plant disease prediction
has achieved an accuracy of 99.2% testing accuracy.",arxiv
http://arxiv.org/abs/2107.00401v1,2021-07-01T12:20:48Z,2021-07-01T12:20:48Z,"CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous
  Cars on the Loihi Neuromorphic Research Processor","Autonomous Driving (AD) related features provide new forms of mobility that
are also beneficial for other kind of intelligent and autonomous systems like
robots, smart transportation, and smart industries. For these applications, the
decisions need to be made fast and in real-time. Moreover, in the quest for
electric mobility, this task must follow low power policy, without affecting
much the autonomy of the mean of transport or the robot. These two challenges
can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed
on a specialized neuromorphic hardware, SNNs can achieve high performance with
low latency and low power consumption. In this paper, we use an SNN connected
to an event-based camera for facing one of the key problems for AD, i.e., the
classification between cars and other objects. To consume less power than
traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS). The
experiments are made following an offline supervised learning rule, followed by
mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip. Our
best experiment achieves an accuracy on offline implementation of 86%, that
drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware
implementation has maximum 0.72 ms of latency for every sample, and consumes
only 310 mW. To the best of our knowledge, this work is the first
implementation of an event-based car classifier on a Neuromorphic Chip.",arxiv
http://arxiv.org/abs/1906.08834v2,2019-06-24T07:57:58Z,2019-06-20T20:30:39Z,"Deep Learning in the Automotive Industry: Recent Advances and
  Application Examples","One of the most exciting technology breakthroughs in the last few years has
been the rise of deep learning. State-of-the-art deep learning models are being
widely deployed in academia and industry, across a variety of areas, from image
analysis to natural language processing. These models have grown from fledgling
research subjects to mature techniques in real-world use. The increasing scale
of data, computational power and the associated algorithmic innovations are the
main drivers for the progress we see in this field. These developments also
have a huge potential for the automotive industry and therefore the interest in
deep learning-based technology is growing. A lot of the product innovations,
such as self-driving cars, parking and lane-change assist or safety functions,
such as autonomous emergency braking, are powered by deep learning algorithms.
Deep learning is poised to offer gains in performance and functionality for
most ADAS (Advanced Driver Assistance System) solutions. Virtual sensing for
vehicle dynamics application, vehicle inspection/heath monitoring, automated
driving and data-driven product development are key areas that are expected to
get the most attention. This article provides an overview of the recent
advances and some associated challenges in deep learning techniques in the
context of automotive applications.",arxiv
http://arxiv.org/abs/1602.08350v2,2017-07-25T04:44:12Z,2016-02-26T14:49:29Z,Large-Scale Detection of Non-Technical Losses in Imbalanced Data Sets,"Non-technical losses (NTL) such as electricity theft cause significant harm
to our economies, as in some countries they may range up to 40% of the total
electricity distributed. Detecting NTLs requires costly on-site inspections.
Accurate prediction of NTLs for customers using machine learning is therefore
crucial. To date, related research largely ignore that the two classes of
regular and non-regular customers are highly imbalanced, that NTL proportions
may change and mostly consider small data sets, often not allowing to deploy
the results in production. In this paper, we present a comprehensive approach
to assess three NTL detection models for different NTL proportions in large
real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and
Support Vector Machine. This work has resulted in appreciable results that are
about to be deployed in a leading industry solution. We believe that the
considerations and observations made in this contribution are necessary for
future smart meter research in order to report their effectiveness on
imbalanced and large real world data sets.",arxiv
http://arxiv.org/abs/2104.02980v1,2021-04-07T08:07:57Z,2021-04-07T08:07:57Z,"Synthetic training data generation for deep learning based quality
  inspection","Deep learning is now the gold standard in computer vision-based quality
inspection systems. In order to detect defects, supervised learning is often
utilized, but necessitates a large amount of annotated images, which can be
costly: collecting, cleaning, and annotating the data is tedious and limits the
speed at which a system can be deployed as everything the system must detect
needs to be observed first. This can impede the inspection of rare defects,
since very few samples can be collected by the manufacturer. In this work, we
focus on simulations to solve this issue. We first present a generic simulation
pipeline to render images of defective or healthy (non defective) parts. As
metallic parts can be highly textured with small defects like holes, we design
a texture scanning and generation method. We assess the quality of the
generated images by training deep learning networks and by testing them on real
data from a manufacturer. We demonstrate that we can achieve encouraging
results on real defect detection using purely simulated data. Additionally, we
are able to improve global performances by concatenating simulated and real
data, showing that simulations can complement real images to boost
performances. Lastly, using domain adaptation techniques helps improving
slightly our final results.",arxiv
http://arxiv.org/abs/1705.07262v2,2017-07-27T15:34:21Z,2017-05-20T05:31:52Z,"Batch Reinforcement Learning on the Industrial Benchmark: First
  Experiences","The Particle Swarm Optimization Policy (PSO-P) has been recently introduced
and proven to produce remarkable results on interacting with academic
reinforcement learning benchmarks in an off-policy, batch-based setting. To
further investigate the properties and feasibility on real-world applications,
this paper investigates PSO-P on the so-called Industrial Benchmark (IB), a
novel reinforcement learning (RL) benchmark that aims at being realistic by
including a variety of aspects found in industrial applications, like
continuous state and action spaces, a high dimensional, partially observable
state space, delayed effects, and complex stochasticity. The experimental
results of PSO-P on IB are compared to results of closed-form control policies
derived from the model-based Recurrent Control Neural Network (RCNN) and the
model-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not
only of interest for academic benchmarks, but also for real-world industrial
applications, since it also yielded the best performing policy in our IB
setting. Compared to other well established RL techniques, PSO-P produced
outstanding results in performance and robustness, requiring only a relatively
low amount of effort in finding adequate parameters or making complex design
decisions.",arxiv
http://arxiv.org/abs/2104.13114v1,2021-04-27T11:29:02Z,2021-04-27T11:29:02Z,"One Backward from Ten Forward, Subsampling for Large-Scale Deep Learning","Deep learning models in large-scale machine learning systems are often
continuously trained with enormous data from production environments. The sheer
volume of streaming training data poses a significant challenge to real-time
training subsystems and ad-hoc sampling is the standard practice. Our key
insight is that these deployed ML systems continuously perform forward passes
on data instances during inference, but ad-hoc sampling does not take advantage
of this substantial computational effort. Therefore, we propose to record a
constant amount of information per instance from these forward passes. The
extra information measurably improves the selection of which data instances
should participate in forward and backward passes. A novel optimization
framework is proposed to analyze this problem and we provide an efficient
approximation algorithm under the framework of Mini-batch gradient descent as a
practical solution. We also demonstrate the effectiveness of our framework and
algorithm on several large-scale classification and regression tasks, when
compared with competitive baselines widely used in industry.",arxiv
http://arxiv.org/abs/2010.12837v3,2021-06-17T08:03:30Z,2020-10-24T08:37:04Z,"XDM: Improving Sequential Deep Matching with Unclicked User Behaviors
  for Recommender System","Deep learning-based sequential recommender systems have recently attracted
increasing attention from both academia and industry. Most of industrial
Embedding-Based Retrieval (EBR) system for recommendation share the similar
ideas with sequential recommenders. Among them, how to comprehensively capture
sequential user interest is a fundamental problem. However, most existing
sequential recommendation models take as input clicked or purchased behavior
sequences from user-item interactions. This leads to incomprehensive user
representation and sub-optimal model performance, since they ignore the
complete user behavior exposure data, i.e., items impressed yet unclicked by
users. In this work, we attempt to incorporate and model those unclicked item
sequences using a new learning approach in order to explore better sequential
recommendation technique. An efficient triplet metric learning algorithm is
proposed to appropriately learn the representation of unclicked items. Our
method can be simply integrated with existing sequential recommendation models
by a confidence fusion network and further gain better user representation. The
offline experimental results based on real-world E-commerce data demonstrate
the effectiveness and verify the importance of unclicked items in sequential
recommendation. Moreover we deploy our new model (named XDM) into EBR of
recommender system at Taobao, outperforming the deployed previous generation
SDM.",arxiv
http://arxiv.org/abs/1904.01723v1,2019-04-03T01:00:41Z,2019-04-03T01:00:41Z,"Empirical Study of Deep Learning for Text Classification in Legal
  Document Review","Predictive coding has been widely used in legal matters to find relevant or
privileged documents in large sets of electronically stored information. It
saves the time and cost significantly. Logistic Regression (LR) and Support
Vector Machines (SVM) are two popular machine learning algorithms used in
predictive coding. Recently, deep learning received a lot of attentions in many
industries. This paper reports our preliminary studies in using deep learning
in legal document review. Specifically, we conducted experiments to compare
deep learning results with results obtained using a SVM algorithm on the four
datasets of real legal matters. Our results showed that CNN performed better
with larger volume of training dataset and should be a fit method in the text
classification in legal industry.",arxiv
http://arxiv.org/abs/1811.00260v5,2019-09-04T19:30:00Z,2018-11-01T07:02:45Z,Horizon: Facebook's Open Source Applied Reinforcement Learning Platform,"In this paper we present Horizon, Facebook's open source applied
reinforcement learning (RL) platform. Horizon is an end-to-end platform
designed to solve industry applied RL problems where datasets are large
(millions to billions of observations), the feedback loop is slow (vs. a
simulator), and experiments must be done with care because they don't run in a
simulator. Unlike other RL platforms, which are often designed for fast
prototyping and experimentation, Horizon is designed with production use cases
as top of mind. The platform contains workflows to train popular deep RL
algorithms and includes data preprocessing, feature transformation, distributed
training, counterfactual policy evaluation, optimized serving, and a
model-based data understanding tool. We also showcase and describe real
examples where reinforcement learning models trained with Horizon significantly
outperformed and replaced supervised learning systems at Facebook.",arxiv
http://arxiv.org/abs/1912.12397v1,2019-12-28T04:05:15Z,2019-12-28T04:05:15Z,"Natural language processing of MIMIC-III clinical notes for identifying
  diagnosis and procedures with neural networks","Coding diagnosis and procedures in medical records is a crucial process in
the healthcare industry, which includes the creation of accurate billings,
receiving reimbursements from payers, and creating standardized patient care
records. In the United States, Billing and Insurance related activities cost
around $471 billion in 2012 which constitutes about 25% of all the U.S hospital
spending. In this paper, we report the performance of a natural language
processing model that can map clinical notes to medical codes, and predict
final diagnosis from unstructured entries of history of present illness,
symptoms at the time of admission, etc. Previous studies have demonstrated that
deep learning models perform better at such mapping when compared to
conventional machine learning models. Therefore, we employed state-of-the-art
deep learning method, ULMFiT on the largest emergency department clinical notes
dataset MIMIC III which has 1.2M clinical notes to select for the top-10 and
top-50 diagnosis and procedure codes. Our models were able to predict the
top-10 diagnoses and procedures with 80.3% and 80.5% accuracy, whereas the
top-50 ICD-9 codes of diagnosis and procedures are predicted with 70.7% and
63.9% accuracy. Prediction of diagnosis and procedures from unstructured
clinical notes benefit human coders to save time, eliminate errors and minimize
costs. With promising scores from our present model, the next step would be to
deploy this on a small-scale real-world scenario and compare it with human
coders as the gold standard. We believe that further research of this approach
can create highly accurate predictions that can ease the workflow in a clinical
setting.",arxiv
http://arxiv.org/abs/2002.10853v1,2020-02-25T13:36:15Z,2020-02-25T13:36:15Z,Learning Machines from Simulation to Real World,"Learning Machines is developing a flexible, cross-industry, advanced
analytics platform, targeted during stealth-stage at a limited number of
specific vertical applications. In this paper, we aim to integrate a general
machine system to learn a variant of tasks from simulation to real world. In
such a machine system, it involves real-time robot vision, sensor fusion, and
learning algorithms (reinforcement learning). To this end, we demonstrate the
general machine system on three fundamental tasks including obstacle avoidance,
foraging, and predator-prey robot. The proposed solutions are implemented on
Robobo robots with mobile device (smartphone with camera) as interface and
built-in infrared (IR) sensors. The agent is trained in a virtual environment.
In order to assess its performance, the learned agent is tested in the virtual
environment and reproduce the same results in a real environment. The results
show that the reinforcement learning algorithm can be reliably used for a
variety of tasks in unknown environments.",arxiv
http://arxiv.org/abs/1811.04871v1,2018-11-12T17:32:24Z,2018-11-12T17:32:24Z,Characterizing machine learning process: A maturity framework,"Academic literature on machine learning modeling fails to address how to make
machine learning models work for enterprises. For example, existing machine
learning processes cannot address how to define business use cases for an AI
application, how to convert business requirements from offering managers into
data requirements for data scientists, and how to continuously improve AI
applications in term of accuracy and fairness, and how to customize general
purpose machine learning models with industry, domain, and use case specific
data to make them more accurate for specific situations etc. Making AI work for
enterprises requires special considerations, tools, methods and processes. In
this paper we present a maturity framework for machine learning model lifecycle
management for enterprises. Our framework is a re-interpretation of the
software Capability Maturity Model (CMM) for machine learning model development
process. We present a set of best practices from our personal experience of
building large scale real-world machine learning models to help organizations
achieve higher levels of maturity independent of their starting point.",arxiv
http://arxiv.org/abs/2005.03459v4,2021-09-05T13:44:38Z,2020-05-06T01:24:25Z,AIBench Scenario: Scenario-distilling AI Benchmarking,"Modern real-world application scenarios like Internet services consist of a
diversity of AI and non-AI modules with huge code sizes and long and
complicated execution paths, which raises serious benchmarking or evaluating
challenges. Using AI components or micro benchmarks alone can lead to
error-prone conclusions. This paper presents a methodology to attack the above
challenge. We formalize a real-world application scenario as a Directed Acyclic
Graph-based model and propose the rules to distill it into a permutation of
essential AI and non-AI tasks, which we call a scenario benchmark. Together
with seventeen industry partners, we extract nine typical scenario benchmarks.
We design and implement an extensible, configurable, and flexible benchmark
framework. We implement two Internet service AI scenario benchmarks based on
the framework as proxies to two real-world application scenarios. We consider
scenario, component, and micro benchmarks as three indispensable parts for
evaluating. Our evaluation shows the advantage of our methodology against using
component or micro AI benchmarks alone. The specifications, source code,
testbed, and results are publicly available from
\url{https://www.benchcouncil.org/aibench/scenario/}.",arxiv
http://arxiv.org/abs/1204.0262v2,2012-04-26T23:17:34Z,2012-04-01T20:17:32Z,"Managing contextual artificial neural networks with a service-based
  mediator","Today, a wide variety of probabilistic and expert AI systems used to analyze
real world inputs such as unstructured text, sounds, images, and statistical
data. However, all these systems exist on different platforms, with different
implementations, and with very different, often very specific goals in mind.
This paper introduces a concept for a mediator framework for such systems and
seeks to show several architectures which would support it, potential benefits
in combining the signals of disparate networks for formalized, high level logic
and signal processing, and its possible academic and industrial uses.",arxiv
http://arxiv.org/abs/2102.10430v1,2021-02-20T20:08:32Z,2021-02-20T20:08:32Z,"Cybersecurity Awareness Platform with Virtual Coach and Automated
  Challenge Assessment","Over the last years, the number of cyber-attacks on industrial control
systems has been steadily increasing. Among several factors, proper software
development plays a vital role in keeping these systems secure. To achieve
secure software, developers need to be aware of secure coding guidelines and
secure coding best practices. This work presents a platform geared towards
software developers in the industry that aims to increase awareness of secure
software development. The authors also introduce an interactive game component,
a virtual coach, which implements a simple artificial intelligence engine based
on the laddering technique for interviews. Through a survey, a preliminary
evaluation of the implemented artifact with real-world players (from academia
and industry) shows a positive acceptance of the developed platform.
Furthermore, the players agree that the platform is adequate for training their
secure coding skills. The impact of our work is to introduce a new automatic
challenge evaluation method together with a virtual coach to improve existing
cybersecurity awareness training programs. These training workshops can be
easily held remotely or off-line.",arxiv
http://arxiv.org/abs/2008.04461v1,2020-08-11T00:33:15Z,2020-08-11T00:33:15Z,SafetyOps,"Safety assurance is a paramount factor in the large-scale deployment of
various autonomous systems (e.g., self-driving vehicles). However, the
execution of safety engineering practices and processes have been challenged by
an increasing complexity of modern safety-critical systems. This attribute has
become more critical for autonomous systems that involve artificial
intelligence (AI) and data-driven techniques along with the complex
interactions of the physical world and digital computing platforms. In this
position paper, we highlight some challenges of applying current safety
processes to modern autonomous systems. Then, we introduce the concept of
SafetyOps - a set of practices, which combines DevOps, TestOps, DataOps, and
MLOps to provide an efficient, continuous and traceable system safety
lifecycle. We believe that SafetyOps can play a significant role in scalable
integration and adaptation of safety engineering into various industries
relying on AI and data.",arxiv
http://arxiv.org/abs/2005.04726v1,2020-05-10T17:37:38Z,2020-05-10T17:37:38Z,Knowledge Graph semantic enhancement of input data for improving AI,"Intelligent systems designed using machine learning algorithms require a
large number of labeled data. Background knowledge provides complementary, real
world factual information that can augment the limited labeled data to train a
machine learning algorithm. The term Knowledge Graph (KG) is in vogue as for
many practical applications, it is convenient and useful to organize this
background knowledge in the form of a graph. Recent academic research and
implemented industrial intelligent systems have shown promising performance for
machine learning algorithms that combine training data with a knowledge graph.
In this article, we discuss the use of relevant KGs to enhance input data for
two applications that use machine learning -- recommendation and community
detection. The KG improves both accuracy and explainability.",arxiv
http://arxiv.org/abs/2101.02644v2,2021-01-08T12:26:17Z,2021-01-07T17:32:56Z,Data Poisoning Attacks to Deep Learning Based Recommender Systems,"Recommender systems play a crucial role in helping users to find their
interested information in various web services such as Amazon, YouTube, and
Google News. Various recommender systems, ranging from neighborhood-based,
association-rule-based, matrix-factorization-based, to deep learning based,
have been developed and deployed in industry. Among them, deep learning based
recommender systems become increasingly popular due to their superior
performance.
  In this work, we conduct the first systematic study on data poisoning attacks
to deep learning based recommender systems. An attacker's goal is to manipulate
a recommender system such that the attacker-chosen target items are recommended
to many users. To achieve this goal, our attack injects fake users with
carefully crafted ratings to a recommender system. Specifically, we formulate
our attack as an optimization problem, such that the injected ratings would
maximize the number of normal users to whom the target items are recommended.
However, it is challenging to solve the optimization problem because it is a
non-convex integer programming problem. To address the challenge, we develop
multiple techniques to approximately solve the optimization problem. Our
experimental results on three real-world datasets, including small and large
datasets, show that our attack is effective and outperforms existing attacks.
Moreover, we attempt to detect fake users via statistical analysis of the
rating patterns of normal and fake users. Our results show that our attack is
still effective and outperforms existing attacks even if such a detector is
deployed.",arxiv
http://arxiv.org/abs/2106.04008v2,2021-06-09T16:58:52Z,2021-06-07T23:31:47Z,Widening Access to Applied Machine Learning with TinyML,"Broadening access to both computational and educational resources is critical
to diffusing machine-learning (ML) innovation. However, today, most ML
resources and experts are siloed in a few countries and organizations. In this
paper, we describe our pedagogical approach to increasing access to applied ML
through a massive open online course (MOOC) on Tiny Machine Learning (TinyML).
We suggest that TinyML, ML on resource-constrained embedded devices, is an
attractive means to widen access because TinyML both leverages low-cost and
globally accessible hardware, and encourages the development of complete,
self-contained applications, from data collection to deployment. To this end, a
collaboration between academia (Harvard University) and industry (Google)
produced a four-part MOOC that provides application-oriented instruction on how
to develop solutions using TinyML. The series is openly available on the edX
MOOC platform, has no prerequisites beyond basic programming, and is designed
for learners from a global variety of backgrounds. It introduces pupils to
real-world applications, ML algorithms, data-set engineering, and the ethical
considerations of these technologies via hands-on programming and deployment of
TinyML applications in both the cloud and their own microcontrollers. To
facilitate continued learning, community building, and collaboration beyond the
courses, we launched a standalone website, a forum, a chat, and an optional
course-project competition. We also released the course materials publicly,
hoping they will inspire the next generation of ML practitioners and educators
and further broaden access to cutting-edge ML technologies.",arxiv
http://arxiv.org/abs/2012.02298v2,2021-06-15T06:28:13Z,2020-11-25T17:23:52Z,"Exploration in Online Advertising Systems with Deep Uncertainty-Aware
  Learning","Modern online advertising systems inevitably rely on personalization methods,
such as click-through rate (CTR) prediction. Recent progress in CTR prediction
enjoys the rich representation capabilities of deep learning and achieves great
success in large-scale industrial applications. However, these methods can
suffer from lack of exploration. Another line of prior work addresses the
exploration-exploitation trade-off problem with contextual bandit methods,
which are recently less studied in the industry due to the difficulty in
extending their flexibility with deep models. In this paper, we propose a novel
Deep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on
Gaussian processes, which can provide predictive uncertainty estimations while
maintaining the flexibility of deep neural networks. DUAL can be easily
implemented on existing models and deployed in real-time systems with minimal
extra computational overhead. By linking the predictive uncertainty estimation
ability of DUAL to well-known bandit algorithms, we further present DUAL-based
Ad-ranking strategies to boost up long-term utilities such as the social
welfare in advertising systems. Experimental results on several public datasets
demonstrate the effectiveness of our methods. Remarkably, an online A/B test
deployed in the Alibaba display advertising platform shows an 8.2% social
welfare improvement and an 8.0% revenue lift.",arxiv
http://arxiv.org/abs/1909.10976v1,2019-09-24T14:58:07Z,2019-09-24T14:58:07Z,"Synthetic dataset generation for object-to-model deep learning in
  industrial applications","The availability of large image data sets has been a crucial factor in the
success of deep learning-based classification and detection methods. While data
sets for everyday objects are widely available, data for specific industrial
use-cases (e.g. identifying packaged products in a warehouse) remains scarce.
In such cases, the data sets have to be created from scratch, placing a crucial
bottleneck on the deployment of deep learning techniques in industrial
applications.
  We present work carried out in collaboration with a leading UK online
supermarket, with the aim of creating a computer vision system capable of
detecting and identifying unique supermarket products in a warehouse setting.
To this end, we demonstrate a framework for using synthetic data to create an
end-to-end deep learning pipeline, beginning with real-world objects and
culminating in a trained model.
  Our method is based on the generation of a synthetic dataset from 3D models
obtained by applying photogrammetry techniques to real-world objects. Using
100k synthetic images generated from 60 real images per class, an InceptionV3
convolutional neural network (CNN) was trained, which achieved classification
accuracy of 95.8% on a separately acquired test set of real supermarket product
images. The image generation process supports automatic pixel annotation. This
eliminates the prohibitively expensive manual annotation typically required for
detection tasks. Based on this readily available data, a one-stage RetinaNet
detector was trained on the synthetic, annotated images to produce a detector
that can accurately localize and classify the specimen products in real-time.",arxiv
http://arxiv.org/abs/2011.09849v1,2020-11-16T06:32:31Z,2020-11-16T06:32:31Z,"Budgeted Online Selection of Candidate IoT Clients to Participate in
  Federated Learning","Machine Learning (ML), and Deep Learning (DL) in particular, play a vital
role in providing smart services to the industry. These techniques however
suffer from privacy and security concerns since data is collected from clients
and then stored and processed at a central location. Federated Learning (FL),
an architecture in which model parameters are exchanged instead of client data,
has been proposed as a solution to these concerns. Nevertheless, FL trains a
global model by communicating with clients over communication rounds, which
introduces more traffic on the network and increases the convergence time to
the target accuracy. In this work, we solve the problem of optimizing accuracy
in stateful FL with a budgeted number of candidate clients by selecting the
best candidate clients in terms of test accuracy to participate in the training
process. Next, we propose an online stateful FL heuristic to find the best
candidate clients. Additionally, we propose an IoT client alarm application
that utilizes the proposed heuristic in training a stateful FL global model
based on IoT device type classification to alert clients about unauthorized IoT
devices in their environment. To test the efficiency of the proposed online
heuristic, we conduct several experiments using a real dataset and compare the
results against state-of-the-art algorithms. Our results indicate that the
proposed heuristic outperforms the online random algorithm with up to 27% gain
in accuracy. Additionally, the performance of the proposed online heuristic is
comparable to the performance of the best offline algorithm.",arxiv
http://arxiv.org/abs/2005.05287v2,2020-05-25T12:16:12Z,2020-05-11T17:40:58Z,"Using Computer Vision to enhance Safety of Workforce in Manufacturing in
  a Post COVID World","The COVID-19 pandemic forced governments across the world to impose lockdowns
to prevent virus transmissions. This resulted in the shutdown of all economic
activity and accordingly the production at manufacturing plants across most
sectors was halted. While there is an urgency to resume production, there is an
even greater need to ensure the safety of the workforce at the plant site.
Reports indicate that maintaining social distancing and wearing face masks
while at work clearly reduces the risk of transmission. We decided to use
computer vision on CCTV feeds to monitor worker activity and detect violations
which trigger real time voice alerts on the shop floor. This paper describes an
efficient and economic approach of using AI to create a safe environment in a
manufacturing setup. We demonstrate our approach to build a robust social
distancing measurement algorithm using a mix of modern-day deep learning and
classic projective geometry techniques. We have deployed our solution at
manufacturing plants across the Aditya Birla Group (ABG). We have also
described our face mask detection approach which provides a high accuracy
across a range of customized masks.",arxiv
http://arxiv.org/abs/1806.08946v1,2018-06-23T11:12:12Z,2018-06-23T11:12:12Z,"Multilevel Wavelet Decomposition Network for Interpretable Time Series
  Analysis","Recent years have witnessed the unprecedented rising of time series from
almost all kindes of academic and industrial fields. Various types of deep
neural network models have been introduced to time series analysis, but the
important frequency information is yet lack of effective modeling. In light of
this, in this paper we propose a wavelet-based neural network structure called
multilevel Wavelet Decomposition Network (mWDN) for building frequency-aware
deep learning models for time series analysis. mWDN preserves the advantage of
multilevel discrete wavelet decomposition in frequency learning while enables
the fine-tuning of all parameters under a deep neural network framework. Based
on mWDN, we further propose two deep learning models called Residual
Classification Flow (RCF) and multi-frequecy Long Short-Term Memory (mLSTM) for
time series classification and forecasting, respectively. The two models take
all or partial mWDN decomposed sub-series in different frequencies as input,
and resort to the back propagation algorithm to learn all the parameters
globally, which enables seamless embedding of wavelet-based frequency analysis
into deep learning frameworks. Extensive experiments on 40 UCR datasets and a
real-world user volume dataset demonstrate the excellent performance of our
time series models based on mWDN. In particular, we propose an importance
analysis method to mWDN based models, which successfully identifies those
time-series elements and mWDN layers that are crucially important to time
series analysis. This indeed indicates the interpretability advantage of mWDN,
and can be viewed as an indepth exploration to interpretable deep learning.",arxiv
http://arxiv.org/abs/1612.07448v6,2017-06-27T02:05:09Z,2016-12-22T05:41:27Z,Towards Linear Algebra over Normalized Data,"Providing machine learning (ML) over relational data is a mainstream
requirement for data analytics systems. While almost all the ML tools require
the input data to be presented as a single table, many datasets are
multi-table, which forces data scientists to join those tables first, leading
to data redundancy and runtime waste. Recent works on ""factorized"" ML mitigate
this issue for a few specific ML algorithms by pushing ML through joins. But
their approaches require a manual rewrite of ML implementations. Such piecemeal
methods create a massive development overhead when extending such ideas to
other ML algorithms. In this paper, we show that it is possible to mitigate
this overhead by leveraging a popular formal algebra to represent the
computations of many ML algorithms: linear algebra. We introduce a new logical
data type to represent normalized data and devise a framework of algebraic
rewrite rules to convert a large set of linear algebra operations over
denormalized data into operations over normalized data. We show how this
enables us to automatically ""factorize"" several popular ML algorithms, thus
unifying and generalizing several prior works. We prototype our framework in
the popular ML environment R and an industrial R-over-RDBMS tool. Experiments
with both synthetic and real normalized data show that our framework also
yields significant speed-ups, up to 36x on real data.",arxiv
http://arxiv.org/abs/1607.03611v2,2016-10-08T05:21:00Z,2016-07-13T07:15:30Z,Characterizing Driving Styles with Deep Learning,"Characterizing driving styles of human drivers using vehicle sensor data,
e.g., GPS, is an interesting research problem and an important real-world
requirement from automotive industries. A good representation of driving
features can be highly valuable for autonomous driving, auto insurance, and
many other application scenarios. However, traditional methods mainly rely on
handcrafted features, which limit machine learning algorithms to achieve a
better performance. In this paper, we propose a novel deep learning solution to
this problem, which could be the first attempt of extending deep learning to
driving behavior analysis based on GPS data. The proposed approach can
effectively extract high level and interpretable features describing complex
driving patterns. It also requires significantly less human experience and
work. The power of the learned driving style representations are validated
through the driver identification problem using a large real dataset.",arxiv
http://arxiv.org/abs/2012.14668v2,2021-02-04T11:49:57Z,2020-12-29T09:01:47Z,Reinforcement Learning for Control of Valves,"This paper is a study of reinforcement learning (RL) as an optimal-control
strategy for control of nonlinear valves. It is evaluated against the PID
(proportional-integral-derivative) strategy, using a unified framework. RL is
an autonomous learning mechanism that learns by interacting with its
environment. It is gaining increasing attention in the world of control systems
as a means of building optimal-controllers for challenging dynamic and
nonlinear processes. Published RL research often uses open-source tools (Python
and OpenAI Gym environments). We use MATLAB's recently launched (R2019a)
Reinforcement Learning Toolbox to develop the valve controller; trained using
the DDPG (Deep Deterministic Policy-Gradient) algorithm and Simulink to
simulate the nonlinear valve and create the experimental test-bench for
evaluation. Simulink allows industrial engineers to quickly adapt and
experiment with other systems of their choice. Results indicate that the RL
controller is extremely good at tracking the signal with speed and produces a
lower error with respect to the reference signal. The PID, however, is better
at disturbance rejection and hence provides a longer life for the valves.
Successful machine learning involves tuning many hyperparameters requiring
significant investment of time and efforts. We introduce ""Graded Learning"" as a
simplified, application oriented adaptation of the more formal and algorithmic
""Curriculum for Reinforcement Learning"". It is shown via experiments that it
helps converge the learning task of complex non-linear real world systems.
Finally, experiential learnings gained from this research are corroborated
against published research.",arxiv
http://arxiv.org/abs/1911.05771v1,2019-11-13T19:25:53Z,2019-11-13T19:25:53Z,"Machine Learning Based Network Vulnerability Analysis of Industrial
  Internet of Things","It is critical to secure the Industrial Internet of Things (IIoT) devices
because of potentially devastating consequences in case of an attack. Machine
learning and big data analytics are the two powerful leverages for analyzing
and securing the Internet of Things (IoT) technology. By extension, these
techniques can help improve the security of the IIoT systems as well. In this
paper, we first present common IIoT protocols and their associated
vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the
utilization of machine learning in countering these susceptibilities. Following
that, a literature review of the available intrusion detection solutions using
machine learning models is presented. Finally, we discuss our case study, which
includes details of a real-world testbed that we have built to conduct
cyber-attacks and to design an intrusion detection system (IDS). We deploy
backdoor, command injection, and Structured Query Language (SQL) injection
attacks against the system and demonstrate how a machine learning based anomaly
detection system can perform well in detecting these attacks. We have evaluated
the performance through representative metrics to have a fair point of view on
the effectiveness of the methods.",arxiv
http://arxiv.org/abs/1908.03571v1,2019-08-09T13:46:48Z,2019-08-09T13:46:48Z,LSTM-based Flow Prediction,"In this paper, a method of prediction on continuous time series variables
from the production or flow -- an LSTM algorithm based on multivariate tuning
-- is proposed. The algorithm improves the traditional LSTM algorithm and
converts the time series data into supervised learning sequences regarding
industrial data's features. The main innovation of this paper consists in
introducing the concepts of periodic measurement and time window in the
industrial prediction problem, especially considering industrial data with time
series characteristics. Experiments using real-world datasets show that the
prediction accuracy is improved, 54.05% higher than that of traditional LSTM
algorithm.",arxiv
http://arxiv.org/abs/2004.05898v1,2020-04-10T14:26:00Z,2020-04-10T14:26:00Z,Exposing Hardware Building Blocks to Machine Learning Frameworks,"There are a plethora of applications that demand high throughput and low
latency algorithms leveraging machine learning methods. This need for real time
processing can be seen in industries ranging from developing neural network
based pre-distortors for enhanced mobile broadband to designing FPGA-based
triggers in major scientific efforts by CERN for particle physics. In this
thesis, we explore how niche domains can benefit vastly if we look at neurons
as a unique boolean function of the form $f:B^{I} \rightarrow B^{O}$, where $B
= \{0,1\}$. We focus on how to design topologies that complement such a view of
neurons, how to automate such a strategy of neural network design, and
inference of such networks on Xilinx FPGAs. Major hardware borne constraints
arise when designing topologies that view neurons as unique boolean functions.
Fundamentally, realizing such topologies on hardware asserts a strict limit on
the 'fan-in' bits of a neuron due to the doubling of permutations possible with
every increment in input bit-length. We address this limit by exploring
different methods of implementing sparsity and explore activation quantization.
Further, we develop a library that supports training a neural network with
custom sparsity and quantization. This library also supports conversion of
trained Sparse Quantized networks from PyTorch to VERILOG code which is then
synthesized using Vivado, all of which is part of the LogicNet tool-flow. To
aid faster prototyping, we also support calculation of the worst-case hardware
cost of any given topology. We hope that our insights into the behavior of
extremely sparse quantized neural networks are of use to the research community
and by extension allow people to use the LogicNet design flow to deploy highly
efficient neural networks.",arxiv
http://arxiv.org/abs/2108.13381v1,2021-08-30T17:04:04Z,2021-08-30T17:04:04Z,"Trustworthy AI for Process Automation on a Chylla-Haase Polymerization
  Reactor","In this paper, genetic programming reinforcement learning (GPRL) is utilized
to generate human-interpretable control policies for a Chylla-Haase
polymerization reactor. Such continuously stirred tank reactors (CSTRs) with
jacket cooling are widely used in the chemical industry, in the production of
fine chemicals, pigments, polymers, and medical products. Despite appearing
rather simple, controlling CSTRs in real-world applications is quite a
challenging problem to tackle. GPRL utilizes already existing data from the
reactor and generates fully automatically a set of optimized simplistic control
strategies, so-called policies, the domain expert can choose from. Note that
these policies are white-box models of low complexity, which makes them easy to
validate and implement in the target control system, e.g., SIMATIC PCS 7.
However, despite its low complexity the automatically-generated policy yields a
high performance in terms of reactor temperature control deviation, which we
empirically evaluate on the original reactor template.",arxiv
http://arxiv.org/abs/2002.07443v1,2020-02-18T09:28:14Z,2020-02-18T09:28:14Z,"An Evaluation of Monte Carlo-Based Hyper-Heuristic for Interaction
  Testing of Industrial Embedded Software Applications","Hyper-heuristic is a new methodology for the adaptive hybridization of
meta-heuristic algorithms to derive a general algorithm for solving
optimization problems. This work focuses on the selection type of
hyper-heuristic, called the Exponential Monte Carlo with Counter (EMCQ).
Current implementations rely on the memory-less selection that can be
counterproductive as the selected search operator may not (historically) be the
best performing operator for the current search instance. Addressing this
issue, we propose to integrate the memory into EMCQ for combinatorial t-wise
test suite generation using reinforcement learning based on the Q-learning
mechanism, called Q-EMCQ. The limited application of combinatorial test
generation on industrial programs can impact the use of such techniques as
Q-EMCQ. Thus, there is a need to evaluate this kind of approach against
relevant industrial software, with a purpose to show the degree of interaction
required to cover the code as well as finding faults. We applied Q-EMCQ on 37
real-world industrial programs written in Function Block Diagram (FBD)
language, which is used for developing a train control management system at
Bombardier Transportation Sweden AB. The results of this study show that Q-EMCQ
is an efficient technique for test case generation. Additionally, unlike the
t-wise test suite generation, which deals with the minimization problem, we
have also subjected Q-EMCQ to a maximization problem involving the general
module clustering to demonstrate the effectiveness of our approach.",arxiv
http://arxiv.org/abs/1905.12443v1,2019-05-28T09:20:31Z,2019-05-28T09:20:31Z,"Implementing SCADA Scenarios and Introducing Attacks to Obtain Training
  Data for Intrusion Detection Methods","There are hardly any data sets publicly available that can be used to
evaluate intrusion detection algorithms. The biggest threat for industrial
applications arises from state-sponsored and criminal groups. Often, formerly
unknown exploits are employed by these attackers, so-called 0-day exploits.
They cannot be discovered with signature-based intrusion detection. Thus,
statistical or machine learning based anomaly detection lends itself readily.
These methods especially, however, need a large amount of labelled training
data. In this work, an exemplary industrial use case with real-world industrial
hardware is presented. Siemens S7 Programmable Logic Controllers are used to
control a real world-based control application using the OPC UA protocol: A
pump, filling and emptying water tanks. This scenario is used to generate
application specific network data. Furthermore, attacks are introduced into
this data set. This is done in three ways: First, the normal process is
monitored and captured. Common attacks are then synthetically introduced into
this data set. Second, malicious behaviour is implemented on the Programmable
Logic Controller program and executed live, the traffic is captured as well.
Third, malicious behaviour is implemented on the Programmable Logic Controller
while still keeping the same output behaviour as in normal operation. An
attacker could exploit an application but forge valid sensor output so that no
anomaly is detected. Sensors are employed, capturing temperature, sound and
flow of water to create data that can be correlated to the network data and
used to still detect the attack. All data is labelled, containing the ground
truth, meaning all attacks are known and no unknown attacks occur. This makes
them perfect for training of anomaly detection algorithms. The data is
published to enable security researchers to evaluate intrusion detection
solutions.",arxiv
http://arxiv.org/abs/1711.08149v3,2018-10-15T15:26:47Z,2017-11-22T06:32:13Z,"Accurate Real Time Localization Tracking in A Clinical Environment using
  Bluetooth Low Energy and Deep Learning","Deep learning has started to revolutionize several different industries, and
the applications of these methods in medicine are now becoming more
commonplace. This study focuses on investigating the feasibility of tracking
patients and clinical staff wearing Bluetooth Low Energy (BLE) tags in a
radiation oncology clinic using artificial neural networks (ANNs) and
convolutional neural networks (CNNs). The performance of these networks was
compared to relative received signal strength indicator (RSSI) thresholding and
triangulation. By utilizing temporal information, a combined CNN+ANN network
was capable of correctly identifying the location of the BLE tag with an
accuracy of 99.9%. It outperformed a CNN model (accuracy = 94%), a thresholding
model employing majority voting (accuracy = 95%), and a triangulation
classifier utilizing majority voting (accuracy = 95%). Future studies will seek
to deploy this affordable real time location system in hospitals to improve
clinical workflow, efficiency, and patient safety.",arxiv
http://arxiv.org/abs/1801.05627v2,2018-04-03T09:06:42Z,2018-01-17T11:48:18Z,"On the Reduction of Biases in Big Data Sets for the Detection of
  Irregular Power Usage","In machine learning, a bias occurs whenever training sets are not
representative for the test data, which results in unreliable models. The most
common biases in data are arguably class imbalance and covariate shift. In this
work, we aim to shed light on this topic in order to increase the overall
attention to this issue in the field of machine learning. We propose a scalable
novel framework for reducing multiple biases in high-dimensional data sets in
order to train more reliable predictors. We apply our methodology to the
detection of irregular power usage from real, noisy industrial data. In
emerging markets, irregular power usage, and electricity theft in particular,
may range up to 40% of the total electricity distributed. Biased data sets are
of particular issue in this domain. We show that reducing these biases
increases the accuracy of the trained predictors. Our models have the potential
to generate significant economic value in a real world application, as they are
being deployed in a commercial software for the detection of irregular power
usage.",arxiv
http://arxiv.org/abs/1401.3875v1,2014-01-16T05:10:17Z,2014-01-16T05:10:17Z,"On-line Planning and Scheduling: An Application to Controlling Modular
  Printers","We present a case study of artificial intelligence techniques applied to the
control of production printing equipment. Like many other real-world
applications, this complex domain requires high-speed autonomous
decision-making and robust continual operation. To our knowledge, this work
represents the first successful industrial application of embedded
domain-independent temporal planning. Our system handles execution failures and
multi-objective preferences. At its heart is an on-line algorithm that combines
techniques from state-space planning and partial-order scheduling. We suggest
that this general architecture may prove useful in other applications as more
intelligent systems operate in continual, on-line settings. Our system has been
used to drive several commercial prototypes and has enabled a new product
architecture for our industrial partner. When compared with state-of-the-art
off-line planners, our system is hundreds of times faster and often finds
better plans. Our experience demonstrates that domain-independent AI planning
based on heuristic search can flexibly handle time, resources, replanning, and
multiple objectives in a high-speed practical application without requiring
hand-coded control knowledge.",arxiv
http://arxiv.org/abs/2012.07938v1,2020-12-14T20:55:48Z,2020-12-14T20:55:48Z,NVIDIA SimNet^{TM}: an AI-accelerated multi-physics simulation framework,"We present SimNet, an AI-driven multi-physics simulation framework, to
accelerate simulations across a wide range of disciplines in science and
engineering. Compared to traditional numerical solvers, SimNet addresses a wide
range of use cases - coupled forward simulations without any training data,
inverse and data assimilation problems. SimNet offers fast turnaround time by
enabling parameterized system representation that solves for multiple
configurations simultaneously, as opposed to the traditional solvers that solve
for one configuration at a time. SimNet is integrated with parameterized
constructive solid geometry as well as STL modules to generate point clouds.
Furthermore, it is customizable with APIs that enable user extensions to
geometry, physics and network architecture. It has advanced network
architectures that are optimized for high-performance GPU computing, and offers
scalable performance for multi-GPU and multi-Node implementation with
accelerated linear algebra as well as FP32, FP64 and TF32 computations. In this
paper we review the neural network solver methodology, the SimNet architecture,
and the various features that are needed for effective solution of the PDEs. We
present real-world use cases that range from challenging forward multi-physics
simulations with turbulence and complex 3D geometries, to industrial design
optimization and inverse problems that are not addressed efficiently by the
traditional solvers. Extensive comparisons of SimNet results with open source
and commercial solvers show good correlation.",arxiv
http://arxiv.org/abs/2011.09902v1,2020-11-17T04:11:31Z,2020-11-17T04:11:31Z,"Low-latency Federated Learning and Blockchain for Edge Association in
  Digital Twin empowered 6G Networks","Emerging technologies such as digital twins and 6th Generation mobile
networks (6G) have accelerated the realization of edge intelligence in
Industrial Internet of Things (IIoT). The integration of digital twin and 6G
bridges the physical system with digital space and enables robust instant
wireless connectivity. With increasing concerns on data privacy, federated
learning has been regarded as a promising solution for deploying distributed
data processing and learning in wireless networks. However, unreliable
communication channels, limited resources, and lack of trust among users,
hinder the effective application of federated learning in IIoT. In this paper,
we introduce the Digital Twin Wireless Networks (DTWN) by incorporating digital
twins into wireless networks, to migrate real-time data processing and
computation to the edge plane. Then, we propose a blockchain empowered
federated learning framework running in the DTWN for collaborative computing,
which improves the reliability and security of the system, and enhances data
privacy. Moreover, to balance the learning accuracy and time cost of the
proposed scheme, we formulate an optimization problem for edge association by
jointly considering digital twin association, training data batch size, and
bandwidth allocation. We exploit multi-agent reinforcement learning to find an
optimal solution to the problem. Numerical results on real-world dataset show
that the proposed scheme yields improved efficiency and reduced cost compared
to benchmark learning method.",arxiv
http://arxiv.org/abs/1804.05497v1,2018-04-16T03:55:42Z,2018-04-16T03:55:42Z,"Deep Learning on Key Performance Indicators for Predictive Maintenance
  in SAP HANA","With a new era of cloud and big data, Database Management Systems (DBMSs)
have become more crucial in numerous enterprise business applications in all
the industries. Accordingly, the importance of their proactive and preventive
maintenance has also increased. However, detecting problems by predefined rules
or stochastic modeling has limitations, particularly when analyzing the data on
high-dimensional Key Performance Indicators (KPIs) from a DBMS. In recent
years, Deep Learning (DL) has opened new opportunities for this complex
analysis. In this paper, we present two complementary DL approaches to detect
anomalies in SAP HANA. A temporal learning approach is used to detect abnormal
patterns based on unlabeled historical data, whereas a spatial learning
approach is used to classify known anomalies based on labeled data. We
implement a system in SAP HANA integrated with Google TensorFlow. The
experimental results with real-world data confirm the effectiveness of the
system and models.",arxiv
http://arxiv.org/abs/1907.09511v1,2019-07-22T18:17:56Z,2019-07-22T18:17:56Z,Universal Person Re-Identification,"Most state-of-the-art person re-identification (re-id) methods depend on
supervised model learning with a large set of cross-view identity labelled
training data. Even worse, such trained models are limited to only the
same-domain deployment with significantly degraded cross-domain generalization
capability, i.e. ""domain specific"". To solve this limitation, there are a
number of recent unsupervised domain adaptation and unsupervised learning
methods that leverage unlabelled target domain training data. However, these
methods need to train a separate model for each target domain as supervised
learning methods. This conventional ""{\em train once, run once}"" pattern is
unscalable to a large number of target domains typically encountered in
real-world deployments. We address this problem by presenting a ""train once,
run everywhere"" pattern industry-scale systems are desperate for. We formulate
a ""universal model learning' approach enabling domain-generic person re-id
using only limited training data of a ""{\em single}"" seed domain. Specifically,
we train a universal re-id deep model to discriminate between a set of
transformed person identity classes. Each of such classes is formed by applying
a variety of random appearance transformations to the images of that class,
where the transformations simulate the camera viewing conditions of any domains
for making the model training domain generic. Extensive evaluations show the
superiority of our method for universal person re-id over a wide variety of
state-of-the-art unsupervised domain adaptation and unsupervised learning re-id
methods on five standard benchmarks: Market-1501, DukeMTMC, CUHK03, MSMT17, and
VIPeR.",arxiv
http://arxiv.org/abs/2003.13174v1,2020-03-30T00:59:31Z,2020-03-30T00:59:31Z,"MIP An AI Distributed Architectural Model to Introduce Cognitive
  computing capabilities in Cyber Physical Systems (CPS)","This paper introduces the MIP Platform architecture model, a novel AI-based
cognitive computing platform architecture. The goal of the proposed application
of MIP is to reduce the implementation burden for the usage of AI algorithms
applied to cognitive computing and fluent HMI interactions within the
manufacturing process in a cyber-physical production system. The cognitive
inferencing engine of MIP is a deterministic cognitive module that processes
declarative goals, identifies Intents and Entities, selects suitable actions
and associated algorithms, and invokes for the execution a processing logic
(Function) configured in the internal Function-as-aService or Connectivity
Engine. Constant observation and evaluation against performance criteria assess
the performance of Lambda(s) for many and varying scenarios. The modular design
with well-defined interfaces enables the reusability and extensibility of FaaS
components. An integrated BigData platform implements this modular design
supported by technologies such as Docker, Kubernetes for virtualization and
orchestration of the individual components and their communication. The
implementation of the architecture is evaluated using a real-world use case
later discussed in this paper.",arxiv
http://arxiv.org/abs/1902.07316v2,2019-04-15T10:21:01Z,2019-02-17T19:37:33Z,Deep Modulation Embedding,"Deep neural network has recently shown very promising applications in
different research directions and attracted the industry attention as well.
Although the idea was introduced in the past but just recently the main
limitation of using this class of algorithms is solved by enabling parallel
computing on GPU hardware. Opening the possibility of hardware prototyping with
proven superiority of this class of algorithm, trigger several research
directions in communication system too. Among them cognitive radio, modulation
recognition, learning based receiver and transceiver are already given very
interesting result in simulation and real experimental evaluation implemented
on software defined radio. Specifically, modulation recognition is mostly
approached as a classification problem which is a supervised learning
framework. But it is here addressed as an unsupervised problem with introducing
new features for training, a new loss function and investigating the robustness
of the pipeline against several mismatch conditions.",arxiv
http://arxiv.org/abs/2008.05221v4,2021-06-13T17:47:28Z,2020-08-12T10:42:14Z,Compression of Deep Learning Models for Text: A Survey,"In recent years, the fields of natural language processing (NLP) and
information retrieval (IR) have made tremendous progress thanksto deep learning
models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and
Long Short-Term Memory (LSTMs)networks, and Transformer [120] based models like
Bidirectional Encoder Representations from Transformers (BERT) [24],
GenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network
(MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer
transformer (T5) [95], T-NLG [98] and GShard [63]. But these models are
humongous in size. On the other hand,real world applications demand small model
size, low response times and low computational power wattage. In this survey,
wediscuss six different types of methods (Pruning, Quantization, Knowledge
Distillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic
Transformer based methods) for compression of such models to enable their
deployment in real industry NLP projects.Given the critical need of building
applications with efficient and small models, and the large amount of recently
published work inthis area, we believe that this survey organizes the plethora
of work done by the 'deep learning for NLP' community in the past fewyears and
presents it as a coherent story.",arxiv
http://arxiv.org/abs/1208.5554v1,2012-08-28T04:26:35Z,2012-08-28T04:26:35Z,Soft Computing approaches on the Bandwidth Problem,"The Matrix Bandwidth Minimization Problem (MBMP) seeks for a simultaneous
reordering of the rows and the columns of a square matrix such that the nonzero
entries are collected within a band of small width close to the main diagonal.
The MBMP is a NP-complete problem, with applications in many scientific
domains, linear systems, artificial intelligence, and real-life situations in
industry, logistics, information recovery. The complex problems are hard to
solve, that is why any attempt to improve their solutions is beneficent.
Genetic algorithms and ant-based systems are Soft Computing methods used in
this paper in order to solve some MBMP instances. Our approach is based on a
learning agent-based model involving a local search procedure. The algorithm is
compared with the classical Cuthill-McKee algorithm, and with a hybrid genetic
algorithm, using several instances from Matrix Market collection. Computational
experiments confirm a good performance of the proposed algorithms for the
considered set of MBMP instances. On Soft Computing basis, we also propose a
new theoretical Reinforcement Learning model for solving the MBMP problem.",arxiv
http://arxiv.org/abs/2003.05622v1,2020-03-12T05:15:48Z,2020-03-12T05:15:48Z,"Distributed Hierarchical GPU Parameter Server for Massive Scale Deep
  Learning Ads Systems","Neural networks of ads systems usually take input from multiple resources,
e.g., query-ad relevance, ad features and user portraits. These inputs are
encoded into one-hot or multi-hot binary features, with typically only a tiny
fraction of nonzero feature values per example. Deep learning models in online
advertising industries can have terabyte-scale parameters that do not fit in
the GPU memory nor the CPU main memory on a computing node. For example, a
sponsored online advertising system can contain more than $10^{11}$ sparse
features, making the neural network a massive model with around 10 TB
parameters. In this paper, we introduce a distributed GPU hierarchical
parameter server for massive scale deep learning ads systems. We propose a
hierarchical workflow that utilizes GPU High-Bandwidth Memory, CPU main memory
and SSD as 3-layer hierarchical storage. All the neural network training
computations are contained in GPUs. Extensive experiments on real-world data
confirm the effectiveness and the scalability of the proposed system. A 4-node
hierarchical GPU parameter server can train a model more than 2X faster than a
150-node in-memory distributed parameter server in an MPI cluster. In addition,
the price-performance ratio of our proposed system is 4-9 times better than an
MPI-cluster solution.",arxiv
http://arxiv.org/abs/2104.14870v1,2021-04-30T09:53:28Z,2021-04-30T09:53:28Z,"Action in Mind: A Neural Network Approach to Action Recognition and
  Segmentation","Recognizing and categorizing human actions is an important task with
applications in various fields such as human-robot interaction, video analysis,
surveillance, video retrieval, health care system and entertainment industry.
This thesis presents a novel computational approach for human action
recognition through different implementations of multi-layer architectures
based on artificial neural networks. Each system level development is designed
to solve different aspects of the action recognition problem including online
real-time processing, action segmentation and the involvement of objects. The
analysis of the experimental results are illustrated and described in six
articles. The proposed action recognition architecture of this thesis is
composed of several processing layers including a preprocessing layer, an
ordered vector representation layer and three layers of neural networks. It
utilizes self-organizing neural networks such as Kohonen feature maps and
growing grids as the main neural network layers. Thus the architecture presents
a biological plausible approach with certain features such as topographic
organization of the neurons, lateral interactions, semi-supervised learning and
the ability to represent high dimensional input space in lower dimensional
maps. For each level of development the system is trained with the input data
consisting of consecutive 3D body postures and tested with generalized input
data that the system has never met before. The experimental results of
different system level developments show that the system performs well with
quite high accuracy for recognizing human actions.",arxiv
http://arxiv.org/abs/2003.02454v4,2020-03-16T01:23:30Z,2020-03-05T06:54:33Z,AGL: a Scalable System for Industrial-purpose Graph Machine Learning,"Machine learning over graphs have been emerging as powerful learning tools
for graph data. However, it is challenging for industrial communities to
leverage the techniques, such as graph neural networks (GNNs), and solve
real-world problems at scale because of inherent data dependency in the graphs.
As such, we cannot simply train a GNN with classic learning systems, for
instance parameter server that assumes data parallel. Existing systems store
the graph data in-memory for fast accesses either in a single machine or graph
stores from remote. The major drawbacks are in three-fold. First, they cannot
scale because of the limitations on the volume of the memory, or the bandwidth
between graph stores and workers. Second, they require extra development of
graph stores without well exploiting mature infrastructures such as MapReduce
that guarantee good system properties. Third, they focus on training but ignore
the optimization of inference over graphs, thus makes them an unintegrated
system.
  In this paper, we design AGL, a scalable, fault-tolerance and integrated
system, with fully-functional training and inference for GNNs. Our system
design follows the message passing scheme underlying the computations of GNNs.
We design to generate the $k$-hop neighborhood, an information-complete
subgraph for each node, as well as do the inference simply by merging values
from in-edge neighbors and propagating values to out-edge neighbors via
MapReduce. In addition, the $k$-hop neighborhood contains information-complete
subgraphs for each node, thus we simply do the training on parameter servers
due to data independency. Our system AGL, implemented on mature
infrastructures, can finish the training of a 2-layer graph attention network
on a graph with billions of nodes and hundred billions of edges in 14 hours,
and complete the inference in 1.2 hour.",arxiv
http://arxiv.org/abs/1802.08365v6,2018-10-23T15:20:56Z,2018-02-23T02:29:06Z,"Budget Constrained Bidding by Model-free Reinforcement Learning in
  Display Advertising","Real-time bidding (RTB) is an important mechanism in online display
advertising, where a proper bid for each page view plays an essential role for
good marketing results. Budget constrained bidding is a typical scenario in RTB
where the advertisers hope to maximize the total value of the winning
impressions under a pre-set budget constraint. However, the optimal bidding
strategy is hard to be derived due to the complexity and volatility of the
auction environment. To address these challenges, in this paper, we formulate
budget constrained bidding as a Markov Decision Process and propose a
model-free reinforcement learning framework to resolve the optimization
problem. Our analysis shows that the immediate reward from environment is
misleading under a critical resource constraint. Therefore, we innovate a
reward function design methodology for the reinforcement learning problems with
constraints. Based on the new reward design, we employ a deep neural network to
learn the appropriate reward so that the optimal policy can be learned
effectively. Different from the prior model-based work, which suffers from the
scalability problem, our framework is easy to be deployed in large-scale
industrial applications. The experimental evaluations demonstrate the
effectiveness of our framework on large-scale real datasets.",arxiv
http://arxiv.org/abs/2108.03044v1,2021-08-06T10:48:00Z,2021-08-06T10:48:00Z,"Molecule Generation Experience: An Open Platform of Material Design for
  Public Users","Artificial Intelligence (AI)-driven material design has been attracting great
attentions as a groundbreaking technology across a wide spectrum of industries.
Molecular design is particularly important owing to its broad application
domains and boundless creativity attributed to progresses in generative models.
The recent maturity of molecular generative models has stimulated expectations
for practical use among potential users, who are not necessarily familiar with
coding or scripting, such as experimental engineers and students in chemical
domains. However, most of the existing molecular generative models are Python
libraries on GitHub, that are accessible for only IT-savvy users. To fill this
gap, we newly developed a graphical user interface (GUI)-based web application
of molecular generative models, Molecule Generation Experience, that is open to
the general public. This is the first web application of molecular generative
models enabling users to work with built-in datasets to carry out molecular
design. In this paper, we describe the background technology extended from our
previous work. Our new online evaluation and structural filtering algorithms
significantly improved the generation speed by 30 to 1,000 times with a wider
structural variety, satisfying chemical stability and synthetic reality. We
also describe in detail our Kubernetes-based scalable cloud architecture and
user-oriented GUI that are necessary components to achieve a public service.
Finally, we present actual use cases in industrial research to design new
photoacid generators (PAGs) as well as release cases in educational events.",arxiv
http://arxiv.org/abs/1810.12027v3,2019-10-29T06:41:48Z,2018-10-29T09:41:52Z,"Deep Reinforcement Learning based Recommendation with Explicit User-Item
  Interactions Modeling","Recommendation is crucial in both academia and industry, and various
techniques are proposed such as content-based collaborative filtering, matrix
factorization, logistic regression, factorization machines, neural networks and
multi-armed bandits. However, most of the previous studies suffer from two
limitations: (1) considering the recommendation as a static procedure and
ignoring the dynamic interactive nature between users and the recommender
systems, (2) focusing on the immediate feedback of recommended items and
neglecting the long-term rewards. To address the two limitations, in this paper
we propose a novel recommendation framework based on deep reinforcement
learning, called DRR. The DRR framework treats recommendation as a sequential
decision making procedure and adopts an ""Actor-Critic"" reinforcement learning
scheme to model the interactions between the users and recommender systems,
which can consider both the dynamic adaptation and long-term rewards.
Furthermore, a state representation module is incorporated into DRR, which can
explicitly capture the interactions between items and users. Three
instantiation structures are developed. Extensive experiments on four
real-world datasets are conducted under both the offline and online evaluation
settings. The experimental results demonstrate the proposed DRR method indeed
outperforms the state-of-the-art competitors.",arxiv
http://arxiv.org/abs/2110.11290v1,2021-10-21T17:18:52Z,2021-10-21T17:18:52Z,Physical Side-Channel Attacks on Embedded Neural Networks: A Survey,"During the last decade, Deep Neural Networks (DNN) have progressively been
integrated on all types of platforms, from data centers to embedded systems
including low-power processors and, recently, FPGAs. Neural Networks (NN) are
expected to become ubiquitous in IoT systems by transforming all sorts of
real-world applications, including applications in the safety-critical and
security-sensitive domains. However, the underlying hardware security
vulnerabilities of embedded NN implementations remain unaddressed. In
particular, embedded DNN implementations are vulnerable to Side-Channel
Analysis (SCA) attacks, which are especially important in the IoT and edge
computing contexts where an attacker can usually gain physical access to the
targeted device. A research field has therefore emerged and is rapidly growing
in terms of the use of SCA including timing, electromagnetic attacks and power
attacks to target NN embedded implementations. Since 2018, research papers have
shown that SCA enables an attacker to recover inference models architectures
and parameters, to expose industrial IP and endangers data confidentiality and
privacy. Without a complete review of this emerging field in the literature so
far, this paper surveys state-of-the-art physical SCA attacks relative to the
implementation of embedded DNNs on micro-controllers and FPGAs in order to
provide a thorough analysis on the current landscape. It provides a taxonomy
and a detailed classification of current attacks. It first discusses mitigation
techniques and then provides insights for future research leads.",arxiv
http://arxiv.org/abs/1604.06195v1,2016-04-21T06:55:42Z,2016-04-21T06:55:42Z,Articulated Hand Pose Estimation Review,"With the increase number of companies focusing on commercializing Augmented
Reality (AR), Virtual Reality (VR) and wearable devices, the need for a hand
based input mechanism is becoming essential in order to make the experience
natural, seamless and immersive. Hand pose estimation has progressed
drastically in recent years due to the introduction of commodity depth cameras.
  Hand pose estimation based on vision is still a challenging problem due to
its complexity from self-occlusion (between fingers), close similarity between
fingers, dexterity of the hands, speed of the pose and the high dimension of
the hand kinematic parameters. Articulated hand pose estimation is still an
open problem and under intensive research from both academia and industry.
  The 2 approaches used for hand pose estimation are: discriminative and
generative. Generative approach is a model based that tries to fit a hand model
to the observed data. Discriminative approach is appearance based, usually
implemented with machine learning (ML) and require a large amount of training
data. Recent hand pose estimation uses hybrid approach by combining both
discriminative and generative methods into a single hand pipeline.
  In this paper, we focus on reviewing recent progress of hand pose estimation
from depth sensor. We will survey discriminative methods, generative methods
and hybrid methods. This paper is not a comprehensive review of all hand pose
estimation techniques, it is a subset of some of the recent state-of-the-art
techniques.",arxiv
http://arxiv.org/abs/2103.03793v2,2021-09-13T12:48:02Z,2021-03-05T16:50:57Z,"Learning Collision-free and Torque-limited Robot Trajectories based on
  Alternative Safe Behaviors","This paper presents an approach to learn online generation of collision-free
and torque-limited robot trajectories. In order to generate future motions, a
neural network is periodically invoked. Based on the current kinematic state of
the robot and the network prediction, a trajectory for the current time
interval can be calculated. The main idea of our paper is to execute the
predicted motion only if a collision-free and torque-limited way to continue
the trajectory is known. In practice, the motion predicted for the current time
interval is extended by a braking trajectory and simulated using a physics
engine. If the simulated trajectory complies with all safety constraints, the
predicted motion is carried out. Otherwise, the braking trajectory calculated
in the previous time interval serves as an alternative safe behavior. Given a
task-specific reward function, the neural network is trained using
reinforcement learning. The design of the action space used for reinforcement
learning ensures that all predicted trajectories comply with kinematic joint
limits. For our evaluation, simulated industrial robots and humanoid robots are
trained to reach as many randomly placed target points as possible. We show
that our method reliably prevents collisions with static obstacles and
collisions between the robot arms, while generating motions that respect both
torque limits and kinematic joint limits. Experiments with a real robot
demonstrate that safe trajectories can be generated in real-time.",arxiv
http://arxiv.org/abs/1806.02424v1,2018-06-06T20:59:40Z,2018-06-06T20:59:40Z,Action4D: Real-time Action Recognition in the Crowd and Clutter,"Recognizing every person's action in a crowded and cluttered environment is a
challenging task. In this paper, we propose a real-time action recognition
method, Action4D, which gives reliable and accurate results in the real-world
settings. We propose to tackle the action recognition problem using a holistic
4D ""scan"" of a cluttered scene to include every detail about the people and
environment. Recognizing multiple people's actions in the cluttered 4D
representation is a new problem. In this paper, we propose novel methods to
solve this problem. We propose a new method to track people in 4D, which can
reliably detect and follow each person in real time. We propose a new deep
neural network, the Action4D-Net, to recognize the action of each tracked
person. The Action4D-Net's novel structure uses both the global feature and the
focused attention to achieve state-of-the-art result. Our real-time method is
invariant to camera view angles, resistant to clutter and able to handle crowd.
The experimental results show that the proposed method is fast, reliable and
accurate. Our method paves the way to action recognition in the real-world
applications and is ready to be deployed to enable smart homes, smart factories
and smart stores.",arxiv
http://arxiv.org/abs/2011.11305v1,2020-11-23T10:05:50Z,2020-11-23T10:05:50Z,"Industrial object, machine part and defect recognition towards fully
  automated industrial monitoring employing deep learning. The case of
  multilevel VGG19","Modern industry requires modern solutions for monitoring the automatic
production of goods. Smart monitoring of the functionality of the mechanical
parts of technology systems or machines is mandatory for a fully automatic
production process. Although Deep Learning has been advancing, allowing for
real-time object detection and other tasks, little has been investigated about
the effectiveness of specially designed Convolutional Neural Networks for
defect detection and industrial object recognition. In the particular study, we
employed six publically available industrial-related datasets containing defect
materials and industrial tools or engine parts, aiming to develop a specialized
model for pattern recognition. Motivated by the recent success of the Virtual
Geometry Group (VGG) network, we propose a modified version of it, called
Multipath VGG19, which allows for more local and global feature extraction,
while the extra features are fused via concatenation. The experiments verified
the effectiveness of MVGG19 over the traditional VGG19. Specifically, top
classification performance was achieved in five of the six image datasets,
while the average classification improvement was 6.95%.",arxiv
http://arxiv.org/abs/2010.16307v1,2020-10-30T14:56:54Z,2020-10-30T14:56:54Z,"Automatic Counting and Identification of Train Wagons Based on Computer
  Vision and Deep Learning","In this work, we present a robust and efficient solution for counting and
identifying train wagons using computer vision and deep learning. The proposed
solution is cost-effective and can easily replace solutions based on
radiofrequency identification (RFID), which are known to have high installation
and maintenance costs. According to our experiments, our two-stage methodology
achieves impressive results on real-world scenarios, i.e., 100% accuracy in the
counting stage and 99.7% recognition rate in the identification one. Moreover,
the system is able to automatically reject some of the train wagons
successfully counted, as they have damaged identification codes. The results
achieved were surprising considering that the proposed system requires low
processing power (i.e., it can run in low-end setups) and that we used a
relatively small number of images to train our Convolutional Neural Network
(CNN) for character recognition. The proposed method is registered, under
number BR512020000808-9, with the National Institute of Industrial Property
(Brazil).",arxiv
http://arxiv.org/abs/2105.08886v2,2021-10-30T14:21:58Z,2021-05-19T02:11:43Z,"Towards Trusted and Intelligent Cyber-Physical Systems: A
  Security-by-Design Approach","The complexity of cyberattacks in Cyber-Physical Systems (CPSs) calls for a
mechanism that can evaluate the operational behaviour and security without
negatively affecting the operation of live systems. In this regard, Digital
Twins (DTs) are revolutionizing the CPSs. DTs strengthen the security of CPSs
throughout the product lifecycle, while assuming that the DT data is trusted,
providing agility to predict and respond to real-time changes. However,
existing DTs solutions in CPS are constrained with untrustworthy data
dissemination among multiple stakeholders and timely course correction. Such
limitations reinforce the significance of designing trustworthy distributed
solutions with the ability to create actionable insights in real-time. To do
so, we propose a framework that focuses on trusted and intelligent DT by
integrating blockchain and Artificial Intelligence (AI). Following a hybrid
approach, the proposed framework not only acquires process knowledge from the
specifications of the CPS, but also relies on AI to learn security threats
based on sensor data. Furthermore, we integrate blockchain to safeguard product
lifecycle data. We discuss the applicability of the proposed framework for the
automotive industry as a CPS use case. Finally, we identify the open challenges
that impede the implementation of intelligence-driven architectures in CPSs.",arxiv
http://arxiv.org/abs/2103.06326v2,2021-07-05T03:46:03Z,2021-03-10T20:13:21Z,"S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement
  Learning","Offline reinforcement learning proposes to learn policies from large
collected datasets without interacting with the physical environment. These
algorithms have made it possible to learn useful skills from data that can then
be deployed in the environment in real-world settings where interactions may be
costly or dangerous, such as autonomous driving or factories. However, current
algorithms overfit to the dataset they are trained on and exhibit poor
out-of-distribution generalization to the environment when deployed. In this
paper, we study the effectiveness of performing data augmentations on the state
space, and study 7 different augmentation schemes and how they behave with
existing offline RL algorithms. We then combine the best data performing
augmentation scheme with a state-of-the-art Q-learning technique, and improve
the function approximation of the Q-networks by smoothening out the learned
state-action space. We experimentally show that using this Surprisingly Simple
Self-Supervision technique in RL (S4RL), we significantly improve over the
current state-of-the-art algorithms on offline robot learning environments such
as MetaWorld [1] and RoboSuite [2,3], and benchmark datasets such as D4RL [4].",arxiv
http://arxiv.org/abs/2109.15239v2,2021-10-01T18:46:58Z,2021-09-30T16:18:30Z,Multi Scale Graph Wavenet for Wind Speed Forecasting,"Geometric deep learning has gained tremendous attention in both academia and
industry due to its inherent capability of representing arbitrary structures.
Due to exponential increase in interest towards renewable sources of energy,
especially wind energy, accurate wind speed forecasting has become very
important. . In this paper, we propose a novel deep learning architecture,
Multi Scale Graph Wavenet for wind speed forecasting. It is based on a graph
convolutional neural network and captures both spatial and temporal
relationships in multivariate time series weather data for wind speed
forecasting. We especially took inspiration from dilated convolutions, skip
connections and the inception network to capture temporal relationships and
graph convolutional networks for capturing spatial relationships in the data.
We conducted experiments on real wind speed data measured at different cities
in Denmark and compared our results with the state-of-the-art baseline models.
Our novel architecture outperformed the state-of-the-art methods on wind speed
forecasting for multiple forecast horizons by 4-5%.",arxiv
http://arxiv.org/abs/2106.06180v1,2021-06-11T05:48:00Z,2021-06-11T05:48:00Z,"Gas Detection and Identification Using Multimodal Artificial
  Intelligence Based Sensor Fusion","With the rapid industrialization and technological advancements, innovative
engineering technologies which are cost effective, faster and easier to
implement are essential. One such area of concern is the rising number of
accidents happening due to gas leaks at coal mines, chemical industries, home
appliances etc. In this paper we propose a novel approach to detect and
identify the gaseous emissions using the multimodal AI fusion techniques. Most
of the gases and their fumes are colorless, odorless, and tasteless, thereby
challenging our normal human senses. Sensing based on a single sensor may not
be accurate, and sensor fusion is essential for robust and reliable detection
in several real-world applications. We manually collected 6400 gas samples
(1600 samples per class for four classes) using two specific sensors: the
7-semiconductor gas sensors array, and a thermal camera. The early fusion
method of multimodal AI, is applied The network architecture consists of a
feature extraction module for individual modality, which is then fused using a
merged layer followed by a dense layer, which provides a single output for
identifying the gas. We obtained the testing accuracy of 96% (for fused model)
as opposed to individual model accuracies of 82% (based on Gas Sensor data
using LSTM) and 93% (based on thermal images data using CNN model). Results
demonstrate that the fusion of multiple sensors and modalities outperforms the
outcome of a single sensor.",arxiv
http://arxiv.org/abs/2007.12002v1,2020-07-23T13:25:36Z,2020-07-23T13:25:36Z,Grale: Designing Networks for Graph Learning,"How can we find the right graph for semi-supervised learning? In real world
applications, the choice of which edges to use for computation is the first
step in any graph learning process. Interestingly, there are often many types
of similarity available to choose as the edges between nodes, and the choice of
edges can drastically affect the performance of downstream semi-supervised
learning systems. However, despite the importance of graph design, most of the
literature assumes that the graph is static. In this work, we present Grale, a
scalable method we have developed to address the problem of graph design for
graphs with billions of nodes. Grale operates by fusing together different
measures of(potentially weak) similarity to create a graph which exhibits high
task-specific homophily between its nodes. Grale is designed for running on
large datasets. We have deployed Grale in more than 20 different industrial
settings at Google, including datasets which have tens of billions of nodes,
and hundreds of trillions of potential edges to score. By employing locality
sensitive hashing techniques,we greatly reduce the number of pairs that need to
be scored, allowing us to learn a task specific model and build the associated
nearest neighbor graph for such datasets in hours, rather than the days or even
weeks that might be required otherwise. We illustrate this through a case study
where we examine the application of Grale to an abuse classification problem on
YouTube with hundreds of million of items. In this application, we find that
Grale detects a large number of malicious actors on top of hard-coded rules and
content classifiers, increasing the total recall by 89% over those approaches
alone.",arxiv
http://arxiv.org/abs/2007.13404v2,2020-10-29T16:23:12Z,2020-07-27T09:50:11Z,"YOLOpeds: Efficient Real-Time Single-Shot Pedestrian Detection for Smart
  Camera Applications","Deep Learning-based object detectors can enhance the capabilities of smart
camera systems in a wide spectrum of machine vision applications including
video surveillance, autonomous driving, robots and drones, smart factory, and
health monitoring. Pedestrian detection plays a key role in all these
applications and deep learning can be used to construct accurate
state-of-the-art detectors. However, such complex paradigms do not scale easily
and are not traditionally implemented in resource-constrained smart cameras for
on-device processing which offers significant advantages in situations when
real-time monitoring and robustness are vital. Efficient neural networks can
not only enable mobile applications and on-device experiences but can also be a
key enabler of privacy and security allowing a user to gain the benefits of
neural networks without needing to send their data to the server to be
evaluated. This work addresses the challenge of achieving a good trade-off
between accuracy and speed for efficient deployment of deep-learning-based
pedestrian detection in smart camera applications. A computationally efficient
architecture is introduced based on separable convolutions and proposes
integrating dense connections across layers and multi-scale feature fusion to
improve representational capacity while decreasing the number of parameters and
operations. In particular, the contributions of this work are the following: 1)
An efficient backbone combining multi-scale feature operations, 2) a more
elaborate loss function for improved localization, 3) an anchor-less approach
for detection, The proposed approach called YOLOpeds is evaluated using the
PETS2009 surveillance dataset on 320x320 images. Overall, YOLOpeds provides
real-time sustained operation of over 30 frames per second with detection rates
in the range of 86% outperforming existing deep learning models.",arxiv
http://arxiv.org/abs/1810.09076v1,2018-10-22T04:13:17Z,2018-10-22T04:13:17Z,"CSI Neural Network: Using Side-channels to Recover Your Artificial
  Neural Network Information","Machine learning has become mainstream across industries. Numerous examples
proved the validity of it for security applications. In this work, we
investigate how to reverse engineer a neural network by using only power
side-channel information. To this end, we consider a multilayer perceptron as
the machine learning architecture of choice and assume a non-invasive and
eavesdropping attacker capable of measuring only passive side-channel leakages
like power consumption, electromagnetic radiation, and reaction time.
  We conduct all experiments on real data and common neural net architectures
in order to properly assess the applicability and extendability of those
attacks. Practical results are shown on an ARM CORTEX-M3 microcontroller. Our
experiments show that the side-channel attacker is capable of obtaining the
following information: the activation functions used in the architecture, the
number of layers and neurons in the layers, the number of output classes, and
weights in the neural network. Thus, the attacker can effectively reverse
engineer the network using side-channel information.
  Next, we show that once the attacker has the knowledge about the neural
network architecture, he/she could also recover the inputs to the network with
only a single-shot measurement. Finally, we discuss several mitigations one
could use to thwart such attacks.",arxiv
http://arxiv.org/abs/2104.03613v1,2021-04-08T08:50:44Z,2021-04-08T08:50:44Z,Uncertainty-aware Remaining Useful Life predictor,"Remaining Useful Life (RUL) estimation is the problem of inferring how long a
certain industrial asset can be expected to operate within its defined
specifications. Deploying successful RUL prediction methods in real-life
applications is a prerequisite for the design of intelligent maintenance
strategies with the potential of drastically reducing maintenance costs and
machine downtimes. In light of their superior performance in a wide range of
engineering fields, Machine Learning (ML) algorithms are natural candidates to
tackle the challenges involved in the design of intelligent maintenance
systems. In particular, given the potentially catastrophic consequences or
substantial costs associated with maintenance decisions that are either too
late or too early, it is desirable that ML algorithms provide uncertainty
estimates alongside their predictions. However, standard data-driven methods
used for uncertainty estimation in RUL problems do not scale well to large
datasets or are not sufficiently expressive to model the high-dimensional
mapping from raw sensor data to RUL estimates. In this work, we consider Deep
Gaussian Processes (DGPs) as possible solutions to the aforementioned
limitations. We perform a thorough evaluation and comparison of several
variants of DGPs applied to RUL predictions. The performance of the algorithms
is evaluated on the N-CMAPSS (New Commercial Modular Aero-Propulsion System
Simulation) dataset from NASA for aircraft engines. The results show that the
proposed methods are able to provide very accurate RUL predictions along with
sensible uncertainty estimates, providing more reliable solutions for
(safety-critical) real-life industrial applications.",arxiv
http://arxiv.org/abs/2110.11073v1,2021-10-18T12:48:02Z,2021-10-18T12:48:02Z,"RL4RS: A Real-World Benchmark for Reinforcement Learning based
  Recommender System","Reinforcement learning based recommender systems (RL-based RS) aims at
learning a good policy from a batch of collected data, with casting sequential
recommendation to multi-step decision-making tasks. However, current RL-based
RS benchmarks commonly have a large reality gap, because they involve
artificial RL datasets or semi-simulated RS datasets, and the trained policy is
directly evaluated in the simulation environment. In real-world situations, not
all recommendation problems are suitable to be transformed into reinforcement
learning problems. Unlike previous academic RL researches, RL-based RS suffer
from extrapolation error and the difficulties of being well validated before
deployment. In this paper, we introduce the RL4RS (Reinforcement Learning for
Recommender Systems) benchmark - a new resource fully collected from industrial
applications to train and evaluate RL algorithms with special concerns on the
above issues. It contains two datasets, tuned simulation environments, related
advanced RL baselines, data understanding tools, and counterfactual policy
evaluation algorithms. The RL4RS suit can be found at
https://github.com/fuxiAIlab/RL4RS. In addition to the RL-based recommender
systems, we expect the resource to contribute to research in reinforcement
learning and neural combinatorial optimization.",arxiv
http://arxiv.org/abs/1911.10290v1,2019-11-23T01:11:29Z,2019-11-23T01:11:29Z,Scalable sim-to-real transfer of soft robot designs,"The manual design of soft robots and their controllers is notoriously
challenging, but it could be augmented---or, in some cases, entirely
replaced---by automated design tools. Machine learning algorithms can
automatically propose, test, and refine designs in simulation, and the most
promising ones can then be manufactured in reality (sim2real). However, it is
currently not known how to guarantee that behavior generated in simulation can
be preserved when deployed in reality. Although many previous studies have
devised training protocols that facilitate sim2real transfer of control
polices, little to no work has investigated the simulation-reality gap as a
function of morphology. This is due in part to an overall lack of tools capable
of systematically designing and rapidly manufacturing robots. Here we introduce
a low cost, open source, and modular soft robot design and construction kit,
and use it to simulate, fabricate, and measure the simulation-reality gap of
minimally complex yet soft, locomoting machines. We prove the scalability of
this approach by transferring an order of magnitude more robot designs from
simulation to reality than any other method. The kit and its instructions can
be found here: https://github.com/skriegman/sim2real4designs",arxiv
http://arxiv.org/abs/1812.00679v1,2018-12-03T11:35:53Z,2018-12-03T11:35:53Z,Data Driven Chiller Plant Energy Optimization with Domain Knowledge,"Refrigeration and chiller optimization is an important and well studied topic
in mechanical engineering, mostly taking advantage of physical models, designed
on top of over-simplified assumptions, over the equipments. Conventional
optimization techniques using physical models make decisions of online
parameter tuning, based on very limited information of hardware specifications
and external conditions, e.g., outdoor weather. In recent years, new generation
of sensors is becoming essential part of new chiller plants, for the first time
allowing the system administrators to continuously monitor the running status
of all equipments in a timely and accurate way. The explosive growth of data
flowing to databases, driven by the increasing analytical power by machine
learning and data mining, unveils new possibilities of data-driven approaches
for real-time chiller plant optimization. This paper presents our research and
industrial experience on the adoption of data models and optimizations on
chiller plant and discusses the lessons learnt from our practice on real world
plants. Instead of employing complex machine learning models, we emphasize the
incorporation of appropriate domain knowledge into data analysis tools, which
turns out to be the key performance improver over state-of-the-art deep
learning techniques by a significant margin. Our empirical evaluation on a real
world chiller plant achieves savings by more than 7% on daily power
consumption.",arxiv
http://arxiv.org/abs/2007.04154v1,2020-07-08T14:33:17Z,2020-07-08T14:33:17Z,Robust pricing and hedging via neural SDEs,"Mathematical modelling is ubiquitous in the financial industry and drives key
decision processes. Any given model provides only a crude approximation to
reality and the risk of using an inadequate model is hard to detect and
quantify. By contrast, modern data science techniques are opening the door to
more robust and data-driven model selection mechanisms. However, most machine
learning models are ""black-boxes"" as individual parameters do not have
meaningful interpretation. The aim of this paper is to combine the above
approaches achieving the best of both worlds. Combining neural networks with
risk models based on classical stochastic differential equations (SDEs), we
find robust bounds for prices of derivatives and the corresponding hedging
strategies while incorporating relevant market data. The resulting model called
neural SDE is an instantiation of generative models and is closely linked with
the theory of causal optimal transport. Neural SDEs allow consistent
calibration under both the risk-neutral and the real-world measures. Thus the
model can be used to simulate market scenarios needed for assessing risk
profiles and hedging strategies. We develop and analyse novel algorithms needed
for efficient use of neural SDEs. We validate our approach with numerical
experiments using both local and stochastic volatility models.",arxiv
http://arxiv.org/abs/1903.08536v3,2019-06-11T10:07:09Z,2019-03-20T15:03:17Z,Segmentation-Based Deep-Learning Approach for Surface-Defect Detection,"Automated surface-anomaly detection using machine learning has become an
interesting and promising area of research, with a very high and direct impact
on the application domain of visual inspection. Deep-learning methods have
become the most suitable approaches for this task. They allow the inspection
system to learn to detect the surface anomaly by simply showing it a number of
exemplar images. This paper presents a segmentation-based deep-learning
architecture that is designed for the detection and segmentation of surface
anomalies and is demonstrated on a specific domain of surface-crack detection.
The design of the architecture enables the model to be trained using a small
number of samples, which is an important requirement for practical
applications. The proposed model is compared with the related deep-learning
methods, including the state-of-the-art commercial software, showing that the
proposed approach outperforms the related methods on the specific domain of
surface-crack detection. The large number of experiments also shed light on the
required precision of the annotation, the number of required training samples
and on the required computational cost. Experiments are performed on a newly
created dataset based on a real-world quality control case and demonstrates
that the proposed approach is able to learn on a small number of defected
surfaces, using only approximately 25-30 defective training samples, instead of
hundreds or thousands, which is usually the case in deep-learning applications.
This makes the deep-learning method practical for use in industry where the
number of available defective samples is limited. The dataset is also made
publicly available to encourage the development and evaluation of new methods
for surface-defect detection.",arxiv
http://arxiv.org/abs/2010.09254v1,2020-10-19T06:48:40Z,2020-10-19T06:48:40Z,Query-aware Tip Generation for Vertical Search,"As a concise form of user reviews, tips have unique advantages to explain the
search results, assist users' decision making, and further improve user
experience in vertical search scenarios. Existing work on tip generation does
not take query into consideration, which limits the impact of tips in search
scenarios. To address this issue, this paper proposes a query-aware tip
generation framework, integrating query information into encoding and
subsequent decoding processes. Two specific adaptations of Transformer and
Recurrent Neural Network (RNN) are proposed. For Transformer, the query impact
is incorporated into the self-attention computation of both the encoder and the
decoder. As for RNN, the query-aware encoder adopts a selective network to
distill query-relevant information from the review, while the query-aware
decoder integrates the query information into the attention computation during
decoding. The framework consistently outperforms the competing methods on both
public and real-world industrial datasets. Last but not least, online
deployment experiments on Dianping demonstrate the advantage of the proposed
framework for tip generation as well as its online business values.",arxiv
http://arxiv.org/abs/2003.13652v1,2020-03-18T01:26:36Z,2020-03-18T01:26:36Z,"Machine Learning enabled Spectrum Sharing in Dense LTE-U/Wi-Fi
  Coexistence Scenarios","The application of Machine Learning (ML) techniques to complex engineering
problems has proved to be an attractive and efficient solution. ML has been
successfully applied to several practical tasks like image recognition,
automating industrial operations, etc. The promise of ML techniques in solving
non-linear problems influenced this work which aims to apply known ML
techniques and develop new ones for wireless spectrum sharing between Wi-Fi and
LTE in the unlicensed spectrum. In this work, we focus on the LTE-Unlicensed
(LTE-U) specification developed by the LTE-U Forum, which uses the duty-cycle
approach for fair coexistence. The specification suggests reducing the duty
cycle at the LTE-U base-station (BS) when the number of co-channel Wi-Fi basic
service sets (BSSs) increases from one to two or more. However, without
decoding the Wi-Fi packets, detecting the number of Wi-Fi BSSs operating on the
channel in real-time is a challenging problem. In this work, we demonstrate a
novel ML-based approach which solves this problem by using energy values
observed during the LTE-U OFF duration. It is relatively straightforward to
observe only the energy values during the LTE-U BS OFF time compared to
decoding the entire Wi-Fi packet, which would require a full Wi-Fi receiver at
the LTE-U base-station. We implement and validate the proposed ML-based
approach by real-time experiments and demonstrate that there exist distinct
patterns between the energy distributions between one and many Wi-Fi AP
transmissions. The proposed ML-based approach results in a higher accuracy
(close to 99\% in all cases) as compared to the existing auto-correlation (AC)
and energy detection (ED) approaches.",arxiv
http://arxiv.org/abs/2011.08512v1,2020-11-17T08:55:14Z,2020-11-17T08:55:14Z,"Preventing Repeated Real World AI Failures by Cataloging Incidents: The
  AI Incident Database","Mature industrial sectors (e.g., aviation) collect their real world failures
in incident databases to inform safety improvements. Intelligent systems
currently cause real world harms without a collective memory of their failings.
As a result, companies repeatedly make the same mistakes in the design,
development, and deployment of intelligent systems. A collection of intelligent
system failures experienced in the real world (i.e., incidents) is needed to
ensure intelligent systems benefit people and society. The AI Incident Database
is an incident collection initiated by an industrial/non-profit cooperative to
enable AI incident avoidance and mitigation. The database supports a variety of
research and development use cases with faceted and full text search on more
than 1,000 incident reports archived to date.",arxiv
http://arxiv.org/abs/2010.03173v1,2020-10-05T19:13:10Z,2020-10-05T19:13:10Z,A Study on Trees's Knots Prediction from their Bark Outer-Shape,"In the industry, the value of wood-logs strongly depends on their internal
structure and more specifically on the knots' distribution inside the trees. As
of today, CT-scanners are the prevalent tool to acquire accurate images of the
trees internal structure. However, CT-scanners are expensive, and slow, making
their use impractical for most industrial applications. Knowing where the knots
are within a tree could improve the efficiency of the overall tree industry by
reducing waste and improving the quality of wood-logs by-products. In this
paper we evaluate different deep-learning based architectures to predict the
internal knots distribution of a tree from its outer-shape, something that has
never been done before. Three types of techniques based on Convolutional Neural
Networks (CNN) will be studied.
  The architectures are tested on both real and synthetic CT-scanned trees.
With these experiments, we demonstrate that CNNs can be used to predict
internal knots distribution based on the external surface of the trees. The
goal being to show that these inexpensive and fast methods could be used to
replace the CT-scanners.
  Additionally, we look into the performance of several off-the-shelf
object-detectors to detect knots inside CT-scanned images. This method is used
to autonomously label part of our real CT-scanned trees alleviating the need to
manually segment the whole of the images.",arxiv
http://arxiv.org/abs/2012.13099v1,2020-12-24T04:52:29Z,2020-12-24T04:52:29Z,"Cooperative Policy Learning with Pre-trained Heterogeneous Observation
  Representations","Multi-agent reinforcement learning (MARL) has been increasingly explored to
learn the cooperative policy towards maximizing a certain global reward. Many
existing studies take advantage of graph neural networks (GNN) in MARL to
propagate critical collaborative information over the interaction graph, built
upon inter-connected agents. Nevertheless, the vanilla GNN approach yields
substantial defects in dealing with complex real-world scenarios since the
generic message passing mechanism is ineffective between heterogeneous vertices
and, moreover, simple message aggregation functions are incapable of accurately
modeling the combinational interactions from multiple neighbors. While adopting
complex GNN models with more informative message passing and aggregation
mechanisms can obviously benefit heterogeneous vertex representations and
cooperative policy learning, it could, on the other hand, increase the training
difficulty of MARL and demand more intense and direct reward signals compared
to the original global reward. To address these challenges, we propose a new
cooperative learning framework with pre-trained heterogeneous observation
representations. Particularly, we employ an encoder-decoder based graph
attention to learn the intricate interactions and heterogeneous representations
that can be more easily leveraged by MARL. Moreover, we design a pre-training
with local actor-critic algorithm to ease the difficulty in cooperative policy
learning. Extensive experiments over real-world scenarios demonstrate that our
new approach can significantly outperform existing MARL baselines as well as
operational research solutions that are widely-used in industry.",arxiv
http://arxiv.org/abs/2103.03227v1,2021-03-02T21:14:44Z,2021-03-02T21:14:44Z,"Graph Computing for Financial Crime and Fraud Detection: Trends,
  Challenges and Outlook","The rise of digital payments has caused consequential changes in the
financial crime landscape. As a result, traditional fraud detection approaches
such as rule-based systems have largely become ineffective. AI and machine
learning solutions using graph computing principles have gained significant
interest in recent years. Graph-based techniques provide unique solution
opportunities for financial crime detection. However, implementing such
solutions at industrial-scale in real-time financial transaction processing
systems has brought numerous application challenges to light. In this paper, we
discuss the implementation difficulties current and next-generation graph
solutions face. Furthermore, financial crime and digital payments trends
indicate emerging challenges in the continued effectiveness of the detection
techniques. We analyze the threat landscape and argue that it provides key
insights for developing graph-based solutions.",arxiv
http://arxiv.org/abs/1802.09756v2,2018-09-11T13:53:10Z,2018-02-27T07:52:35Z,"Real-Time Bidding with Multi-Agent Reinforcement Learning in Display
  Advertising","Real-time advertising allows advertisers to bid for each impression for a
visiting user. To optimize specific goals such as maximizing revenue and return
on investment (ROI) led by ad placements, advertisers not only need to estimate
the relevance between the ads and user's interests, but most importantly
require a strategic response with respect to other advertisers bidding in the
market. In this paper, we formulate bidding optimization with multi-agent
reinforcement learning. To deal with a large number of advertisers, we propose
a clustering method and assign each cluster with a strategic bidding agent. A
practical Distributed Coordinated Multi-Agent Bidding (DCMAB) has been proposed
and implemented to balance the tradeoff between the competition and cooperation
among advertisers. The empirical study on our industry-scaled real-world data
has demonstrated the effectiveness of our methods. Our results show
cluster-based bidding would largely outperform single-agent and bandit
approaches, and the coordinated bidding achieves better overall objectives than
purely self-interested bidding agents.",arxiv
http://arxiv.org/abs/2103.14407v2,2021-04-13T12:48:31Z,2021-03-26T11:32:27Z,Bellman: A Toolbox for Model-Based Reinforcement Learning in TensorFlow,"In the past decade, model-free reinforcement learning (RL) has provided
solutions to challenging domains such as robotics. Model-based RL shows the
prospect of being more sample-efficient than model-free methods in terms of
agent-environment interactions, because the model enables to extrapolate to
unseen situations. In the more recent past, model-based methods have shown
superior results compared to model-free methods in some challenging domains
with non-linear state transitions. At the same time, it has become apparent
that RL is not market-ready yet and that many real-world applications are going
to require model-based approaches, because model-free methods are too
sample-inefficient and show poor performance in early stages of training. The
latter is particularly important in industry, e.g. in production systems that
directly impact a company's revenue. This demonstrates the necessity for a
toolbox to push the boundaries for model-based RL. While there is a plethora of
toolboxes for model-free RL, model-based RL has received little attention in
terms of toolbox development. Bellman aims to fill this gap and introduces the
first thoroughly designed and tested model-based RL toolbox using
state-of-the-art software engineering practices. Our modular approach enables
to combine a wide range of environment models with generic model-based agent
classes that recover state-of-the-art algorithms. We also provide an experiment
harness to compare both model-free and model-based agents in a systematic
fashion w.r.t. user-defined evaluation metrics (e.g. cumulative reward). This
paves the way for new research directions, e.g. investigating uncertainty-aware
environment models that are not necessarily neural-network-based, or developing
algorithms to solve industrially-motivated benchmarks that share
characteristics with real-world problems.",arxiv
http://arxiv.org/abs/2101.07462v1,2021-01-19T04:38:58Z,2021-01-19T04:38:58Z,"Deep Reinforcement Learning for Producing Furniture Layout in Indoor
  Scenes","In the industrial interior design process, professional designers plan the
size and position of furniture in a room to achieve a satisfactory design for
selling. In this paper, we explore the interior scene design task as a Markov
decision process (MDP), which is solved by deep reinforcement learning. The
goal is to produce an accurate position and size of the furniture
simultaneously for the indoor layout task. In particular, we first formulate
the furniture layout task as a MDP problem by defining the state, action, and
reward function. We then design the simulated environment and train
reinforcement learning agents to produce the optimal layout for the MDP
formulation. We conduct our experiments on a large-scale real-world interior
layout dataset that contains industrial designs from professional designers.
Our numerical results demonstrate that the proposed model yields higher-quality
layouts as compared with the state-of-art model. The developed simulator and
codes are available at \url{https://github.com/CODE-SUBMIT/simulator1}.",arxiv
http://arxiv.org/abs/2102.09137v1,2021-02-18T03:20:35Z,2021-02-18T03:20:35Z,"Multi-Agent Reinforcement Learning of 3D Furniture Layout Simulation in
  Indoor Graphics Scenes","In the industrial interior design process, professional designers plan the
furniture layout to achieve a satisfactory 3D design for selling. In this
paper, we explore the interior graphics scenes design task as a Markov decision
process (MDP) in 3D simulation, which is solved by multi-agent reinforcement
learning. The goal is to produce furniture layout in the 3D simulation of the
indoor graphics scenes. In particular, we firstly transform the 3D interior
graphic scenes into two 2D simulated scenes. We then design the simulated
environment and apply two reinforcement learning agents to learn the optimal 3D
layout for the MDP formulation in a cooperative way. We conduct our experiments
on a large-scale real-world interior layout dataset that contains industrial
designs from professional designers. Our numerical results demonstrate that the
proposed model yields higher-quality layouts as compared with the state-of-art
model. The developed simulator and codes are available at
\url{https://github.com/CODE-SUBMIT/simulator2}.",arxiv
http://arxiv.org/abs/2110.03785v1,2021-10-07T20:38:14Z,2021-10-07T20:38:14Z,"Addressing practical challenges in Active Learning via a hybrid query
  strategy","Active Learning (AL) is a powerful tool to address modern machine learning
problems with significantly fewer labeled training instances. However,
implementation of traditional AL methodologies in practical scenarios is
accompanied by multiple challenges due to the inherent assumptions. There are
several hindrances, such as unavailability of labels for the AL algorithm at
the beginning; unreliable external source of labels during the querying
process; or incompatible mechanisms to evaluate the performance of Active
Learner. Inspired by these practical challenges, we present a hybrid query
strategy-based AL framework that addresses three practical challenges
simultaneously: cold-start, oracle uncertainty and performance evaluation of
Active Learner in the absence of ground truth. While a pre-clustering approach
is employed to address the cold-start problem, the uncertainty surrounding the
expertise of labeler and confidence in the given labels is incorporated to
handle oracle uncertainty. The heuristics obtained during the querying process
serve as the fundamental premise for accessing the performance of Active
Learner. The robustness of the proposed AL framework is evaluated across three
different environments and industrial settings. The results demonstrate the
capability of the proposed framework to tackle practical challenges during AL
implementation in real-world scenarios.",arxiv
http://arxiv.org/abs/1712.02975v1,2017-12-08T08:06:03Z,2017-12-08T08:06:03Z,Recruitment Market Trend Analysis with Sequential Latent Variable Models,"Recruitment market analysis provides valuable understanding of
industry-specific economic growth and plays an important role for both
employers and job seekers. With the rapid development of online recruitment
services, massive recruitment data have been accumulated and enable a new
paradigm for recruitment market analysis. However, traditional methods for
recruitment market analysis largely rely on the knowledge of domain experts and
classic statistical models, which are usually too general to model large-scale
dynamic recruitment data, and have difficulties to capture the fine-grained
market trends. To this end, in this paper, we propose a new research paradigm
for recruitment market analysis by leveraging unsupervised learning techniques
for automatically discovering recruitment market trends based on large-scale
recruitment data. Specifically, we develop a novel sequential latent variable
model, named MTLVM, which is designed for capturing the sequential dependencies
of corporate recruitment states and is able to automatically learn the latent
recruitment topics within a Bayesian generative framework. In particular, to
capture the variability of recruitment topics over time, we design hierarchical
dirichlet processes for MTLVM. These processes allow to dynamically generate
the evolving recruitment topics. Finally, we implement a prototype system to
empirically evaluate our approach based on real-world recruitment data in
China. Indeed, by visualizing the results from MTLVM, we can successfully
reveal many interesting findings, such as the popularity of LBS related jobs
reached the peak in the 2nd half of 2014, and decreased in 2015.",arxiv
http://arxiv.org/abs/1910.01568v2,2019-10-06T18:47:26Z,2019-10-03T16:14:57Z,"Incremental learning for the detection and classification of
  GAN-generated images","Current developments in computer vision and deep learning allow to
automatically generate hyper-realistic images, hardly distinguishable from real
ones. In particular, human face generation achieved a stunning level of
realism, opening new opportunities for the creative industry but, at the same
time, new scary scenarios where such content can be maliciously misused.
Therefore, it is essential to develop innovative methodologies to automatically
tell apart real from computer generated multimedia, possibly able to follow the
evolution and continuous improvement of data in terms of quality and realism.
In the last few years, several deep learning-based solutions have been proposed
for this problem, mostly based on Convolutional Neural Networks (CNNs).
Although results are good in controlled conditions, it is not clear how such
proposals can adapt to real-world scenarios, where learning needs to
continuously evolve as new types of generated data appear. In this work, we
tackle this problem by proposing an approach based on incremental learning for
the detection and classification of GAN-generated images. Experiments on a
dataset comprising images generated by several GAN-based architectures show
that the proposed method is able to correctly perform discrimination when new
GANs are presented to the network",arxiv
http://arxiv.org/abs/2109.13375v1,2021-09-27T22:37:55Z,2021-09-27T22:37:55Z,"Automated Estimation of Construction Equipment Emission using Inertial
  Sensors and Machine Learning Models","The construction industry is one of the main producers of greenhouse gasses
(GHG). Quantifying the amount of air pollutants including GHG emissions during
a construction project has become an additional project objective to
traditional metrics such as time, cost, and safety in many parts of the world.
A major contributor to air pollution during construction is the use of heavy
equipment and thus their efficient operation and management can substantially
reduce the harm to the environment. Although the on-road vehicle emission
prediction is a widely researched topic, construction equipment emission
measurement and reduction have received very little attention. This paper
describes the development and deployment of a novel framework that uses machine
learning (ML) methods to predict the level of emissions from heavy construction
equipment monitored via an Internet of Things (IoT) system comprised of
accelerometer and gyroscope sensors. The developed framework was validated
using an excavator performing real-world construction work. A portable emission
measurement system (PEMS) was employed along with the inertial sensors to
record data including the amount of CO, NOX, CO2, SO2, and CH4 pollutions
emitted by the equipment. Different ML algorithms were developed and compared
to identify the best model to predict emission levels from inertial sensors
data. The results showed that Random Forest with the coefficient of
determination (R2) of 0.94, 0.91 and 0.94 for CO, NOX, CO2, respectively was
the best algorithm among different models evaluated in this study.",arxiv
http://arxiv.org/abs/1107.5462v1,2011-07-27T13:07:39Z,2011-07-27T13:07:39Z,HyFlex: A Benchmark Framework for Cross-domain Heuristic Search,"Automating the design of heuristic search methods is an active research field
within computer science, artificial intelligence and operational research. In
order to make these methods more generally applicable, it is important to
eliminate or reduce the role of the human expert in the process of designing an
effective methodology to solve a given computational search problem.
Researchers developing such methodologies are often constrained on the number
of problem domains on which to test their adaptive, self-configuring
algorithms; which can be explained by the inherent difficulty of implementing
their corresponding domain specific software components.
  This paper presents HyFlex, a software framework for the development of
cross-domain search methodologies. The framework features a common software
interface for dealing with different combinatorial optimisation problems, and
provides the algorithm components that are problem specific. In this way, the
algorithm designer does not require a detailed knowledge the problem domains,
and thus can concentrate his/her efforts in designing adaptive general-purpose
heuristic search algorithms. Four hard combinatorial problems are fully
implemented (maximum satisfiability, one dimensional bin packing, permutation
flow shop and personnel scheduling), each containing a varied set of instance
data (including real-world industrial applications) and an extensive set of
problem specific heuristics and search operators. The framework forms the basis
for the first International Cross-domain Heuristic Search Challenge (CHeSC),
and it is currently in use by the international research community. In summary,
HyFlex represents a valuable new benchmark of heuristic search generality, with
which adaptive cross-domain algorithms are being easily developed, and reliably
compared.",arxiv
http://arxiv.org/abs/1708.09099v1,2017-08-30T03:32:56Z,2017-08-30T03:32:56Z,"Watch Me, but Don't Touch Me! Contactless Control Flow Monitoring via
  Electromagnetic Emanations","Trustworthy operation of industrial control systems depends on secure and
real-time code execution on the embedded programmable logic controllers (PLCs).
The controllers monitor and control the critical infrastructures, such as
electric power grids and healthcare platforms, and continuously report back the
system status to human operators. We present Zeus, a contactless embedded
controller security monitor to ensure its execution control flow integrity.
Zeus leverages the electromagnetic emission by the PLC circuitry during the
execution of the controller programs. Zeus's contactless execution tracking
enables non-intrusive monitoring of security-critical controllers with tight
real-time constraints. Those devices often cannot tolerate the cost and
performance overhead that comes with additional traditional hardware or
software monitoring modules. Furthermore, Zeus provides an air-gap between the
monitor (trusted computing base) and the target (potentially compromised) PLC.
This eliminates the possibility of the monitor infection by the same attack
vectors. Zeus monitors for control flow integrity of the PLC program execution.
Zeus monitors the communications between the human-machine interface and the
PLC, and captures the control logic binary uploads to the PLC. Zeus exercises
its feasible execution paths, and fingerprints their emissions using an
external electromagnetic sensor. Zeus trains a neural network for legitimate
PLC executions, and uses it at runtime to identify the control flow based on
PLC's electromagnetic emissions. We implemented Zeus on a commercial Allen
Bradley PLC, which is widely used in industry, and evaluated it on real-world
control program executions. Zeus was able to distinguish between different
legitimate and malicious executions with 98.9% accuracy and with zero overhead
on PLC execution by design.",arxiv
http://arxiv.org/abs/2105.01064v1,2021-05-04T03:14:30Z,2021-05-04T03:14:30Z,"Alternate Model Growth and Pruning for Efficient Training of
  Recommendation Systems","Deep learning recommendation systems at scale have provided remarkable gains
through increasing model capacity (i.e. wider and deeper neural networks), but
it comes at significant training cost and infrastructure cost. Model pruning is
an effective technique to reduce computation overhead for deep neural networks
by removing redundant parameters. However, modern recommendation systems are
still thirsty for model capacity due to the demand for handling big data. Thus,
pruning a recommendation model at scale results in a smaller model capacity and
consequently lower accuracy. To reduce computation cost without sacrificing
model capacity, we propose a dynamic training scheme, namely alternate model
growth and pruning, to alternatively construct and prune weights in the course
of training. Our method leverages structured sparsification to reduce
computational cost without hurting the model capacity at the end of offline
training so that a full-size model is available in the recurring training stage
to learn new data in real-time. To the best of our knowledge, this is the first
work to provide in-depth experiments and discussion of applying structural
dynamics to recommendation systems at scale to reduce training cost. The
proposed method is validated with an open-source deep-learning recommendation
model (DLRM) and state-of-the-art industrial-scale production models.",arxiv
http://arxiv.org/abs/2105.11216v1,2021-05-24T11:51:46Z,2021-05-24T11:51:46Z,"CONECT4: Desarrollo de componentes basados en Realidad Mixta, Realidad
  Virtual Y Conocimiento Experto para generación de entornos de aprendizaje
  Hombre-Máquina","This work presents the results of project CONECT4, which addresses the
research and development of new non-intrusive communication methods for the
generation of a human-machine learning ecosystem oriented to predictive
maintenance in the automotive industry. Through the use of innovative
technologies such as Augmented Reality, Virtual Reality, Digital Twin and
expert knowledge, CONECT4 implements methodologies that allow improving the
efficiency of training techniques and knowledge management in industrial
companies. The research has been supported by the development of content and
systems with a low level of technological maturity that address solutions for
the industrial sector applied in training and assistance to the operator. The
results have been analyzed in companies in the automotive sector, however, they
are exportable to any other type of industrial sector. -- --
  En esta publicaci\'on se presentan los resultados del proyecto CONECT4, que
aborda la investigaci\'on y desarrollo de nuevos m\'etodos de comunicaci\'on no
intrusivos para la generaci\'on de un ecosistema de aprendizaje
hombre-m\'aquina orientado al mantenimiento predictivo en la industria de
automoci\'on. A trav\'es del uso de tecnolog\'ias innovadoras como la Realidad
Aumentada, la Realidad Virtual, el Gemelo Digital y conocimiento experto,
CONECT4 implementa metodolog\'ias que permiten mejorar la eficiencia de las
t\'ecnicas de formaci\'on y gesti\'on de conocimiento en las empresas
industriales. La investigaci\'on se ha apoyado en el desarrollo de contenidos y
sistemas con un nivel de madurez tecnol\'ogico bajo que abordan soluciones para
el sector industrial aplicadas en la formaci\'on y asistencia al operario. Los
resultados han sido analizados en empresas del sector de automoci\'on, no
obstante, son exportables a cualquier otro tipo de sector industrial.",arxiv
http://arxiv.org/abs/2107.11972v1,2021-07-26T05:52:42Z,2021-07-26T05:52:42Z,"Trade When Opportunity Comes: Price Movement Forecasting via
  Locality-Aware Attention and Adaptive Refined Labeling","Price movement forecasting aims at predicting the future trends of financial
assets based on the current market conditions and other relevant information.
Recently, machine learning(ML) methods have become increasingly popular and
achieved promising results for price movement forecasting in both academia and
industry. Most existing ML solutions formulate the forecasting problem as a
classification(to predict the direction) or a regression(to predict the return)
problem in the entire set of training data. However, due to the extremely low
signal-to-noise ratio and stochastic nature of financial data, good trading
opportunities are extremely scarce. As a result, without careful selection of
potentially profitable samples, such ML methods are prone to capture the
patterns of noises instead of real signals. To address the above issues, we
propose a novel framework-LARA(Locality-Aware Attention and Adaptive Refined
Labeling), which contains the following three components: 1)Locality-aware
attention automatically extracts the potentially profitable samples by
attending to their label information in order to construct a more accurate
classifier on these selected samples. 2)Adaptive refined labeling further
iteratively refines the labels, alleviating the noise of samples. 3)Equipped
with metric learning techniques, Locality-aware attention enjoys task-specific
distance metrics and distributes attention on potentially profitable samples in
a more effective way. To validate our method, we conduct comprehensive
experiments on three real-world financial markets: ETFs, the China's A-share
stock market, and the cryptocurrency market. LARA achieves superior performance
compared with the time-series analysis methods and a set of machine learning
based competitors on the Qlib platform. Extensive ablation studies and
experiments demonstrate that LARA indeed captures more reliable trading
opportunities.",arxiv
http://arxiv.org/abs/2004.14404v2,2020-05-23T01:42:24Z,2020-04-29T18:00:22Z,Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks,"Robotic insertion tasks are characterized by contact and friction mechanics,
making them challenging for conventional feedback control methods due to
unmodeled physical effects. Reinforcement learning (RL) is a promising approach
for learning control policies in such settings. However, RL can be unsafe
during exploration and might require a large amount of real-world training
data, which is expensive to collect. In this paper, we study how to use
meta-reinforcement learning to solve the bulk of the problem in simulation by
solving a family of simulated industrial insertion tasks and then adapt
policies quickly in the real world. We demonstrate our approach by training an
agent to successfully perform challenging real-world insertion tasks using less
than 20 trials of real-world experience. Videos and other material are
available at https://pearl-insertion.github.io/",arxiv
http://arxiv.org/abs/1904.13001v1,2019-04-30T00:24:06Z,2019-04-30T00:24:06Z,"Encoding Categorical Variables with Conjugate Bayesian Models for WeWork
  Lead Scoring Engine","Applied Data Scientists throughout various industries are commonly faced with
the challenging task of encoding high-cardinality categorical features into
digestible inputs for machine learning algorithms. This paper describes a
Bayesian encoding technique developed for WeWork's lead scoring engine which
outputs the probability of a person touring one of our office spaces based on
interaction, enrichment, and geospatial data. We present a paradigm for
ensemble modeling which mitigates the need to build complicated preprocessing
and encoding schemes for categorical variables. In particular, domain-specific
conjugate Bayesian models are employed as base learners for features in a
stacked ensemble model. For each column of a categorical feature matrix we fit
a problem-specific prior distribution, for example, the Beta distribution for a
binary classification problem. In order to analytically derive the moments of
the posterior distribution, we update the prior with the conjugate likelihood
of the corresponding target variable for each unique value of the given
categorical feature. This function of column and value encodes the categorical
feature matrix so that the final learner in the ensemble model ingests
low-dimensional numerical input. Experimental results on both curated and real
world datasets demonstrate impressive accuracy and computational efficiency on
a variety of problem archetypes. Particularly, for the lead scoring engine at
WeWork -- where some categorical features have as many as 300,000 levels -- we
have seen an AUC improvement from 0.87 to 0.97 through implementing conjugate
Bayesian model encoding.",arxiv
http://arxiv.org/abs/1907.01882v2,2019-08-05T15:51:47Z,2019-07-03T12:26:38Z,An Experimental Evaluation of Large Scale GBDT Systems,"Gradient boosting decision tree (GBDT) is a widely-used machine learning
algorithm in both data analytic competitions and real-world industrial
applications. Further, driven by the rapid increase in data volume, efforts
have been made to train GBDT in a distributed setting to support large-scale
workloads. However, we find it surprising that the existing systems manage the
training dataset in different ways, but none of them have studied the impact of
data management. To that end, this paper aims to study the pros and cons of
different data management methods regarding the performance of distributed
GBDT. We first introduce a quadrant categorization of data management policies
based on data partitioning and data storage. Then we conduct an in-depth
systematic analysis and summarize the advantageous scenarios of the quadrants.
Based on the analysis, we further propose a novel distributed GBDT system named
Vero, which adopts the unexplored composition of vertical partitioning and
row-store and suits for many large-scale cases. To validate our analysis
empirically, we implement different quadrants in the same code base and compare
them under extensive workloads, and finally compare Vero with other
state-of-the-art systems over a wide range of datasets. Our theoretical and
experimental results provide a guideline on choosing a proper data management
policy for a given workload.",arxiv
http://arxiv.org/abs/2002.01711v4,2020-02-10T08:49:39Z,2020-02-05T10:25:02Z,"A Reinforcement Learning Framework for Time-Dependent Causal Effects
  Evaluation in A/B Testing","A/B testing, or online experiment is a standard business strategy to compare
a new product with an old one in pharmaceutical, technological, and traditional
industries. Major challenges arise in online experiments where there is only
one unit that receives a sequence of treatments over time. In those
experiments, the treatment at a given time impacts current outcome as well as
future outcomes. The aim of this paper is to introduce a reinforcement learning
framework for carrying A/B testing, while characterizing the long-term
treatment effects. Our proposed testing procedure allows for sequential
monitoring and online updating, so it is generally applicable to a variety of
treatment designs in different industries. In addition, we systematically
investigate the theoretical properties (e.g., asymptotic distribution and
power) of our testing procedure. Finally, we apply our framework to both
synthetic datasets and a real-world data example obtained from a ride-sharing
company to illustrate its usefulness.",arxiv
http://arxiv.org/abs/2009.09926v1,2020-09-17T02:36:52Z,2020-09-17T02:36:52Z,"Cross-Modal Alignment with Mixture Experts Neural Network for
  Intral-City Retail Recommendation","In this paper, we introduce Cross-modal Alignment with mixture experts Neural
Network (CameNN) recommendation model for intral-city retail industry, which
aims to provide fresh foods and groceries retailing within 5 hours delivery
service arising for the outbreak of Coronavirus disease (COVID-19) pandemic
around the world. We propose CameNN, which is a multi-task model with three
tasks including Image to Text Alignment (ITA) task, Text to Image Alignment
(TIA) task and CVR prediction task. We use pre-trained BERT to generate the
text embedding and pre-trained InceptionV4 to generate image patch embedding
(each image is split into small patches with the same pixels and treat each
patch as an image token). Softmax gating networks follow to learn the weight of
each transformer expert output and choose only a subset of experts conditioned
on the input. Then transformer encoder is applied as the share-bottom layer to
learn all input features' shared interaction. Next, mixture of transformer
experts (MoE) layer is implemented to model different aspects of tasks. At top
of the MoE layer, we deploy a transformer layer for each task as task tower to
learn task-specific information. On the real word intra-city dataset,
experiments demonstrate CameNN outperform baselines and achieve significant
improvements on the image and text representation. In practice, we applied
CameNN on CVR prediction in our intra-city recommender system which is one of
the leading intra-city platforms operated in China.",arxiv
http://arxiv.org/abs/2108.13475v1,2021-08-18T13:39:50Z,2021-08-18T13:39:50Z,"An Analysis Of Entire Space Multi-Task Models For Post-Click Conversion
  Prediction","Industrial recommender systems are frequently tasked with approximating
probabilities for multiple, often closely related, user actions. For example,
predicting if a user will click on an advertisement and if they will then
purchase the advertised product. The conceptual similarity between these tasks
has promoted the use of multi-task learning: a class of algorithms that aim to
bring positive inductive transfer from related tasks. Here, we empirically
evaluate multi-task learning approaches with neural networks for an online
advertising task. Specifically, we consider approximating the probability of
post-click conversion events (installs) (CVR) for mobile app advertising on a
large-scale advertising platform, using the related click events (CTR) as an
auxiliary task. We use an ablation approach to systematically study recent
approaches that incorporate both multitask learning and ""entire space modeling""
which train the CVR on all logged examples rather than learning a conditional
likelihood of conversion given clicked. Based on these results we show that
several different approaches result in similar levels of positive transfer from
the data-abundant CTR task to the CVR task and offer some insight into how the
multi-task design choices address the two primary problems affecting the CVR
task: data sparsity and data bias. Our findings add to the growing body of
evidence suggesting that standard multi-task learning is a sensible approach to
modelling related events in real-world large-scale applications and suggest the
specific multitask approach can be guided by ease of implementation in an
existing system.",arxiv
http://arxiv.org/abs/1904.03987v1,2019-04-08T12:16:06Z,2019-04-08T12:16:06Z,"Early warning in egg production curves from commercial hens: A SVM
  approach","Artificial Intelligence allows the improvement of our daily life, for
instance, speech and handwritten text recognition, real time translation and
weather forecasting are common used applications. In the livestock sector,
machine learning algorithms have the potential for early detection and warning
of problems, which represents a significant milestone in the poultry industry.
Production problems generate economic loss that could be avoided by acting in a
timely manner. In the current study, training and testing of support vector
machines are addressed, for an early detection of problems in the production
curve of commercial eggs, using farm's egg production data of 478,919 laying
hens grouped in 24 flocks. Experiments using support vector machines with a 5
k-fold cross-validation were performed at different previous time intervals, to
alert with up to 5 days of forecasting interval, whether a flock will
experience a problem in production curve. Performance metrics such as accuracy,
specificity, sensitivity, and positive predictive value were evaluated,
reaching 0-day values of 0.9874, 0.9876, 0.9783 and 0.6518 respectively on
unseen data (test-set). The optimal forecasting interval was from zero to three
days, performance metrics decreases as the forecasting interval is increased.
It should be emphasized that this technique was able to issue an alert a day in
advance, achieving an accuracy of 0.9854, a specificity of 0.9865, a
sensitivity of 0.9333 and a positive predictive value of 0.6135. This novel
application embedded in a computer system of poultry management is able to
provide significant improvements in early detection and warning of problems
related to the production curve.",arxiv
http://arxiv.org/abs/2004.05722v1,2020-04-12T23:56:06Z,2020-04-12T23:56:06Z,Complaint-driven Training Data Debugging for Query 2.0,"As the need for machine learning (ML) increases rapidly across all industry
sectors, there is a significant interest among commercial database providers to
support ""Query 2.0"", which integrates model inference into SQL queries.
Debugging Query 2.0 is very challenging since an unexpected query result may be
caused by the bugs in training data (e.g., wrong labels, corrupted features).
In response, we propose Rain, a complaint-driven training data debugging
system. Rain allows users to specify complaints over the query's intermediate
or final output, and aims to return a minimum set of training examples so that
if they were removed, the complaints would be resolved. To the best of our
knowledge, we are the first to study this problem. A naive solution requires
retraining an exponential number of ML models. We propose two novel heuristic
approaches based on influence functions which both require linear retraining
steps. We provide an in-depth analytical and empirical analysis of the two
approaches and conduct extensive experiments to evaluate their effectiveness
using four real-world datasets. Results show that Rain achieves the highest
recall@k among all the baselines while still returns results interactively.",arxiv
http://arxiv.org/abs/2001.03025v1,2020-01-08T10:33:23Z,2020-01-08T10:33:23Z,"Deep Time-Stream Framework for Click-Through Rate Prediction by Tracking
  Interest Evolution","Click-through rate (CTR) prediction is an essential task in industrial
applications such as video recommendation. Recently, deep learning models have
been proposed to learn the representation of users' overall interests, while
ignoring the fact that interests may dynamically change over time. We argue
that it is necessary to consider the continuous-time information in CTR models
to track user interest trend from rich historical behaviors. In this paper, we
propose a novel Deep Time-Stream framework (DTS) which introduces the time
information by an ordinary differential equations (ODE). DTS continuously
models the evolution of interests using a neural network, and thus is able to
tackle the challenge of dynamically representing users' interests based on
their historical behaviors. In addition, our framework can be seamlessly
applied to any existing deep CTR models by leveraging the additional
Time-Stream Module, while no changes are made to the original CTR models.
Experiments on public dataset as well as real industry dataset with billions of
samples demonstrate the effectiveness of proposed approaches, which achieve
superior performance compared with existing methods.",arxiv
http://arxiv.org/abs/2104.08178v1,2021-04-14T20:32:39Z,2021-04-14T20:32:39Z,"Design of an Efficient, Ease-of-use and Affordable Artificial
  Intelligence based Nucleic Acid Amplification Diagnosis Technology for
  Tuberculosis and Multi-drug Resistant Tuberculosis","Current technologies that facilitate diagnosis for simultaneous detection of
Mycobacterium tuberculosis and its resistance to first-line anti-tuberculosis
drugs (Isoniazid and Rifampicim) are designed for lab-based settings and are
unaffordable for large scale testing implementations. The suitability of a TB
diagnosis instrument, generally required in low-resource settings, to be
implementable in point-of-care last mile public health centres depends on
manufacturing cost, ease-of-use, automation and portability. This paper
discusses a portable, low-cost, machine learning automated Nucleic acid
amplification testing (NAAT) device that employs the use of a smartphone-based
fluorescence detection using novel image processing and chromaticity detection
algorithms. To test the instrument, real time polymerase chain reaction (qPCR)
experiment on cDNA dilution spanning over two concentrations (40 ng/uL and 200
ng/uL) was performed and sensitive detection of multiplexed positive control
assay was verified.",arxiv
http://arxiv.org/abs/1903.09477v4,2020-12-30T10:34:47Z,2019-03-22T12:46:34Z,"Facilitating Rapid Prototyping in the OODIDA Data Analytics Platform via
  Active-Code Replacement","OODIDA (On-board/Off-board Distributed Data Analytics) is a platform for
distributed real-time analytics, targeting fleets of reference vehicles in the
automotive industry. Its users are data analysts. The bulk of the data
analytics tasks are performed by clients (on-board), while a central cloud
server performs supplementary tasks (off-board). OODIDA can be automatically
packaged and deployed, which necessitates restarting parts of the system, or
all of it. As this is potentially disruptive, we added the ability to execute
user-defined Python modules on clients as well as the server. These modules can
be replaced without restarting any part of the system; they can even be
replaced between iterations of an ongoing assignment. This feature is referred
to as active-code replacement. It facilitates use cases such as iterative A/B
testing of machine learning algorithms or modifying experimental algorithms
on-the-fly. Consistency of results is achieved by majority vote, which prevents
tainted state. Active-code replacement can be done in less than a second in an
idealized setting whereas a standard deployment takes many orders of magnitude
more time. The main contribution of this paper is the description of a
relatively straightforward approach to active-code replacement that is very
user-friendly. It enables a data analyst to quickly execute custom code on the
cloud server as well as on client devices. Sensible safeguards and design
decisions ensure that this feature can be used by non-specialists who are not
familiar with the implementation of OODIDA in general or this feature in
particular. As a consequence of adding the active-code replacement feature,
OODIDA is now very well-suited for rapid prototyping.",arxiv
http://arxiv.org/abs/2106.06150v1,2021-06-11T03:30:25Z,2021-06-11T03:30:25Z,Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs,"Graph neural networks (GNNs) are powerful tools for learning from graph data
and are widely used in various applications such as social network
recommendation, fraud detection, and graph search. The graphs in these
applications are typically large, usually containing hundreds of millions of
nodes. Training GNN models on such large graphs efficiently remains a big
challenge. Despite a number of sampling-based methods have been proposed to
enable mini-batch training on large graphs, these methods have not been proved
to work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU
training. The state-of-the-art sampling-based methods are usually not optimized
for these real-world hardware setups, in which data movement between CPUs and
GPUs is a bottleneck. To address this issue, we propose Global Neighborhood
Sampling that aims at training GNNs on giant graphs specifically for
mixed-CPU-GPU training. The algorithm samples a global cache of nodes
periodically for all mini-batches and stores them in GPUs. This global cache
allows in-GPU importance sampling of mini-batches, which drastically reduces
the number of nodes in a mini-batch, especially in the input layer, to reduce
data copy between CPU and GPU and mini-batch computation without compromising
the training convergence rate or model accuracy. We provide a highly efficient
implementation of this method and show that our implementation outperforms an
efficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant
graphs. It outperforms an efficient implementation of LADIES with small layers
by a factor of 2X-14X while achieving much higher accuracy than LADIES.We also
theoretically analyze the proposed algorithm and show that with cached node
data of a proper size, it enjoys a comparable convergence rate as the
underlying node-wise sampling method.",arxiv
http://arxiv.org/abs/1707.06959v1,2017-07-21T16:15:31Z,2017-07-21T16:15:31Z,"A Framework for Easing the Development of Applications Embedding Answer
  Set Programming","Answer Set Programming (ASP) is a well-established declarative problem
solving paradigm which became widely used in AI and recognized as a powerful
tool for knowledge representation and reasoning (KRR), especially for its high
expressiveness and the ability to deal also with incomplete knowledge.
  Recently, thanks to the availability of a number of robust and efficient
implementations, ASP has been increasingly employed in a number of different
domains, and used for the development of industrial-level and enterprise
applications. This made clear the need for proper development tools and
interoperability mechanisms for easing interaction and integration with
external systems in the widest range of real-world scenarios, including mobile
applications and educational contexts.
  In this work we present a framework for integrating the KRR capabilities of
ASP into generic applications. We show the use of the framework by illustrating
proper specializations for some relevant ASP systems over different platforms,
including the mobile setting; furthermore, the potential of the framework for
educational purposes is illustrated by means of the development of several
ASP-based applications.",arxiv
http://arxiv.org/abs/1912.03618v2,2020-06-06T03:56:29Z,2019-12-08T05:12:17Z,Efficient Black-box Assessment of Autonomous Vehicle Safety,"While autonomous vehicle (AV) technology has shown substantial progress, we
still lack tools for rigorous and scalable testing. Real-world testing, the
$\textit{de-facto}$ evaluation method, is dangerous to the public. Moreover,
due to the rare nature of failures, billions of miles of driving are needed to
statistically validate performance claims. Thus, the industry has largely
turned to simulation to evaluate AV systems. However, having a simulation stack
alone is not a solution. A simulation testing framework needs to prioritize
which scenarios to run, learn how the chosen scenarios provide coverage of
failure modes, and rank failure scenarios in order of importance. We implement
a simulation testing framework that evaluates an entire modern AV system as a
black box. This framework estimates the probability of accidents under a base
distribution governing standard traffic behavior. In order to accelerate
rare-event probability evaluation, we efficiently learn to identify and rank
failure scenarios via adaptive importance-sampling methods. Using this
framework, we conduct the first independent evaluation of a full-stack
commercial AV system, Comma AI's OpenPilot.",arxiv
http://arxiv.org/abs/2110.00086v1,2021-09-30T20:56:37Z,2021-09-30T20:56:37Z,On the Trustworthiness of Tree Ensemble Explainability Methods,"The recent increase in the deployment of machine learning models in critical
domains such as healthcare, criminal justice, and finance has highlighted the
need for trustworthy methods that can explain these models to stakeholders.
Feature importance methods (e.g. gain and SHAP) are among the most popular
explainability methods used to address this need. For any explainability
technique to be trustworthy and meaningful, it has to provide an explanation
that is accurate and stable. Although the stability of local feature importance
methods (explaining individual predictions) has been studied before, there is
yet a knowledge gap about the stability of global features importance methods
(explanations for the whole model). Additionally, there is no study that
evaluates and compares the accuracy of global feature importance methods with
respect to feature ordering. In this paper, we evaluate the accuracy and
stability of global feature importance methods through comprehensive
experiments done on simulations as well as four real-world datasets. We focus
on tree-based ensemble methods as they are used widely in industry and measure
the accuracy and stability of explanations under two scenarios: 1) when inputs
are perturbed 2) when models are perturbed. Our findings provide a comparison
of these methods under a variety of settings and shed light on the limitations
of global feature importance methods by indicating their lack of accuracy with
and without noisy inputs, as well as their lack of stability with respect to:
1) increase in input dimension or noise in the data; 2) perturbations in models
initialized by different random seeds or hyperparameter settings.",arxiv
http://arxiv.org/abs/2003.07739v2,2020-07-12T15:00:40Z,2020-03-17T14:17:52Z,"Formal Scenario-Based Testing of Autonomous Vehicles: From Simulation to
  the Real World","We present a new approach to automated scenario-based testing of the safety
of autonomous vehicles, especially those using advanced artificial
intelligence-based components, spanning both simulation-based evaluation as
well as testing in the real world. Our approach is based on formal methods,
combining formal specification of scenarios and safety properties, algorithmic
test case generation using formal simulation, test case selection for track
testing, executing test cases on the track, and analyzing the resulting data.
Experiments with a real autonomous vehicle at an industrial testing facility
support our hypotheses that (i) formal simulation can be effective at
identifying test cases to run on the track, and (ii) the gap between simulated
and real worlds can be systematically evaluated and bridged.",arxiv
http://arxiv.org/abs/2105.13420v1,2021-05-27T19:48:23Z,2021-05-27T19:48:23Z,Model Selection for Production System via Automated Online Experiments,"A challenge that machine learning practitioners in the industry face is the
task of selecting the best model to deploy in production. As a model is often
an intermediate component of a production system, online controlled experiments
such as A/B tests yield the most reliable estimation of the effectiveness of
the whole system, but can only compare two or a few models due to budget
constraints. We propose an automated online experimentation mechanism that can
efficiently perform model selection from a large pool of models with a small
number of online experiments. We derive the probability distribution of the
metric of interest that contains the model uncertainty from our Bayesian
surrogate model trained using historical logs. Our method efficiently
identifies the best model by sequentially selecting and deploying a list of
models from the candidate set that balance exploration-exploitation. Using
simulations based on real data, we demonstrate the effectiveness of our method
on two different tasks.",arxiv
http://arxiv.org/abs/1706.08420v1,2017-06-26T14:54:18Z,2017-06-26T14:54:18Z,"StreamLearner: Distributed Incremental Machine Learning on Event
  Streams: Grand Challenge","Today, massive amounts of streaming data from smart devices need to be
analyzed automatically to realize the Internet of Things. The Complex Event
Processing (CEP) paradigm promises low-latency pattern detection on event
streams. However, CEP systems need to be extended with Machine Learning (ML)
capabilities such as online training and inference in order to be able to
detect fuzzy patterns (e.g., outliers) and to improve pattern recognition
accuracy during runtime using incremental model training. In this paper, we
propose a distributed CEP system denoted as StreamLearner for ML-enabled
complex event detection. The proposed programming model and data-parallel
system architecture enable a wide range of real-world applications and allow
for dynamically scaling up and out system resources for low-latency,
high-throughput event processing. We show that the DEBS Grand Challenge 2017
case study (i.e., anomaly detection in smart factories) integrates seamlessly
into the StreamLearner API. Our experiments verify scalability and high event
throughput of StreamLearner.",arxiv
http://arxiv.org/abs/1901.07370v1,2019-01-18T00:53:39Z,2019-01-18T00:53:39Z,"SAML-QC: a Stochastic Assessment and Machine Learning based QC technique
  for Industrial Printing","Recently, the advancement in industrial automation and high-speed printing
has raised numerous challenges related to the printing quality inspection of
final products. This paper proposes a machine vision based technique to assess
the printing quality of text on industrial objects. The assessment is based on
three quality defects such as text misalignment, varying printing shades, and
misprinted text. The proposed scheme performs the quality inspection through
stochastic assessment technique based on the second-order statistics of
printing. First: the text-containing area on printed product is identified
through image processing techniques. Second: the alignment testing of the
identified text-containing area is performed. Third: optical character
recognition is performed to divide the text into different small boxes and only
the intensity value of each text-containing box is taken as a random variable
and second-order statistics are estimated to determine the varying printing
defects in the text under one, two and three sigma thresholds. Fourth: the
K-Nearest Neighbors based supervised machine learning is performed to provide
the stochastic process for misprinted text detection. Finally, the technique is
deployed on an industrial image for the printing quality assessment with
varying values of n and m. The results have shown that the proposed SAML-QC
technique can perform real-time automated inspection for industrial printing.",arxiv
http://arxiv.org/abs/2001.05375v1,2020-01-15T15:30:29Z,2020-01-15T15:30:29Z,"AAAI FSS-19: Human-Centered AI: Trustworthiness of AI Models and Data
  Proceedings","To facilitate the widespread acceptance of AI systems guiding decision-making
in real-world applications, it is key that solutions comprise trustworthy,
integrated human-AI systems. Not only in safety-critical applications such as
autonomous driving or medicine, but also in dynamic open world systems in
industry and government it is crucial for predictive models to be
uncertainty-aware and yield trustworthy predictions. Another key requirement
for deployment of AI at enterprise scale is to realize the importance of
integrating human-centered design into AI systems such that humans are able to
use systems effectively, understand results and output, and explain findings to
oversight committees.
  While the focus of this symposium was on AI systems to improve data quality
and technical robustness and safety, we welcomed submissions from broadly
defined areas also discussing approaches addressing requirements such as
explainable models, human trust and ethical aspects of AI.",arxiv
http://arxiv.org/abs/1802.08974v1,2018-02-25T09:28:52Z,2018-02-25T09:28:52Z,"A Framework in CRM Customer Lifecycle: Identify Downward Trend and
  Potential Issues Detection","Customer retention is one of the primary goals in the area of customer
relationship management. A mass of work exists in which machine learning models
or business rules are established to predict churn. However, targeting users at
an early stage when they start to show a downward trend is a better strategy.
In downward trend prediction, the reasons why customers show a downward trend
is of great interest in the industry as it helps the business to understand the
pain points that customers suffer and to take early action to prevent them from
churning. A commonly used method is to collect feedback from customers by
either aggressively reaching out to them or by passively hearing from them.
However, it is believed that there are a large number of customers who have
unpleasant experiences and never speak out. In the literature, there is limited
research work that provides a comprehensive and scientific approach to identify
these ""silent suffers"". In this study, we propose a novel two-part framework:
developing the downward prediction process and establishing the methodology to
identify the reasons why customers are in the downward trend. In the first
prediction part, we focus on predicting the downward trend, which is an earlier
stage of the customer lifecycle compared to churn. In the second part, we
propose an approach to figuring out the cause (of the downward trend) based on
a causal inference method and semi-supervised learning. The proposed approach
is capable of identifying potential silent sufferers. We take bad shopping
experiences as inputs to develop the framework and validate it via a marketing
A/B test in the real world. The test readout demonstrates the effectiveness of
the framework by driving 88.5% incremental lift in purchase volume.",arxiv
http://arxiv.org/abs/1611.05902v2,2017-11-13T22:28:26Z,2016-11-17T21:21:52Z,"Practical heteroskedastic Gaussian process modeling for large simulation
  experiments","We present a unified view of likelihood based Gaussian progress regression
for simulation experiments exhibiting input-dependent noise. Replication plays
an important role in that context, however previous methods leveraging
replicates have either ignored the computational savings that come from such
design, or have short-cut full likelihood-based inference to remain tractable.
Starting with homoskedastic processes, we show how multiple applications of a
well-known Woodbury identity facilitate inference for all parameters under the
likelihood (without approximation), bypassing the typical full-data sized
calculations. We then borrow a latent-variable idea from machine learning to
address heteroskedasticity, adapting it to work within the same thrifty
inferential framework, thereby simultaneously leveraging the computational and
statistical efficiency of designs with replication. The result is an
inferential scheme that can be characterized as single objective function,
complete with closed form derivatives, for rapid library-based optimization.
Illustrations are provided, including real-world simulation experiments from
manufacturing and the management of epidemics.",arxiv
http://arxiv.org/abs/1705.08409v1,2017-05-23T16:59:29Z,2017-05-23T16:59:29Z,Ridesourcing Car Detection by Transfer Learning,"Ridesourcing platforms like Uber and Didi are getting more and more popular
around the world. However, unauthorized ridesourcing activities taking
advantages of the sharing economy can greatly impair the healthy development of
this emerging industry. As the first step to regulate on-demand ride services
and eliminate black market, we design a method to detect ridesourcing cars from
a pool of cars based on their trajectories. Since licensed ridesourcing car
traces are not openly available and may be completely missing in some cities
due to legal issues, we turn to transferring knowledge from public transport
open data, i.e, taxis and buses, to ridesourcing detection among ordinary
vehicles. We propose a two-stage transfer learning framework. In Stage 1, we
take taxi and bus data as input to learn a random forest (RF) classifier using
trajectory features shared by taxis/buses and ridesourcing/other cars. Then, we
use the RF to label all the candidate cars. In Stage 2, leveraging the subset
of high confident labels from the previous stage as input, we further learn a
convolutional neural network (CNN) classifier for ridesourcing detection, and
iteratively refine RF and CNN, as well as the feature set, via a co-training
process. Finally, we use the resulting ensemble of RF and CNN to identify the
ridesourcing cars in the candidate pool. Experiments on real car, taxi and bus
traces show that our transfer learning framework, with no need of a pre-labeled
ridesourcing dataset, can achieve similar accuracy as the supervised learning
methods.",arxiv
http://arxiv.org/abs/2002.02741v1,2020-02-07T12:41:28Z,2020-02-07T12:41:28Z,"Can't Boil This Frog: Robustness of Online-Trained Autoencoder-Based
  Anomaly Detectors to Adversarial Poisoning Attacks","In recent years, a variety of effective neural network-based methods for
anomaly and cyber attack detection in industrial control systems (ICSs) have
been demonstrated in the literature. Given their successful implementation and
widespread use, there is a need to study adversarial attacks on such detection
methods to better protect the systems that depend upon them. The extensive
research performed on adversarial attacks on image and malware classification
has little relevance to the physical system state prediction domain, which most
of the ICS attack detection systems belong to. Moreover, such detection systems
are typically retrained using new data collected from the monitored system,
thus the threat of adversarial data poisoning is significant, however this
threat has not yet been addressed by the research community. In this paper, we
present the first study focused on poisoning attacks on online-trained
autoencoder-based attack detectors. We propose two algorithms for generating
poison samples, an interpolation-based algorithm and a back-gradient
optimization-based algorithm, which we evaluate on both synthetic and
real-world ICS data. We demonstrate that the proposed algorithms can generate
poison samples that cause the target attack to go undetected by the autoencoder
detector, however the ability to poison the detector is limited to a small set
of attack types and magnitudes. When the poison-generating algorithms are
applied to the popular SWaT dataset, we show that the autoencoder detector
trained on the physical system state data is resilient to poisoning in the face
of all ten of the relevant attacks in the dataset. This finding suggests that
neural network-based attack detectors used in the cyber-physical domain are
more robust to poisoning than in other problem domains, such as malware
detection and image processing.",arxiv
http://arxiv.org/abs/1907.10554v2,2020-03-26T16:11:35Z,2019-07-24T16:47:47Z,"Development of a Real-time Indoor Location System using Bluetooth Low
  Energy Technology and Deep Learning to Facilitate Clinical Applications","An indoor, real-time location system (RTLS) can benefit both hospitals and
patients by improving clinical efficiency through data-driven optimization of
procedures. Bluetooth-based RTLS systems are cost-effective but lack accuracy
and robustness because Bluetooth signal strength is subject to fluctuation. We
developed a machine learning-based solution using a Long Short-Term Memory
(LSTM) network followed by a Multilayer Perceptron classifier and a posterior
constraint algorithm to improve RTLS performance. Training and validation
datasets showed that most machine learning models perform well in classifying
individual location zones, although LSTM was most reliable. However, when faced
with data indicating cross-zone trajectories, all models showed erratic zone
switching. Thus, we implemented a history-based posterior constraint algorithm
to reduce the variability in exchange for a slight decrease in responsiveness.
This network increases robustness at the expense of latency. When latency is
less of a concern, we computed the latency-corrected accuracy which is 100% for
our testing data, significantly improved from LSTM without constraint which is
96.2%. The balance between robustness and responsiveness can be considered and
adjusted on a case-by-case basis, according to the specific needs of downstream
clinical applications. This system was deployed and validated in an academic
medical center. Industry best practices enabled system scaling without
substantial compromises to performance or cost.",arxiv
http://arxiv.org/abs/2005.00936v1,2020-05-02T22:49:34Z,2020-05-02T22:49:34Z,"An Ensemble Deep Learning-based Cyber-Attack Detection in Industrial
  Control System","The integration of communication networks and the Internet of Things (IoT) in
Industrial Control Systems (ICSs) increases their vulnerability towards
cyber-attacks, causing devastating outcomes. Traditional Intrusion Detection
Systems (IDSs), which are mainly developed to support Information Technology
(IT) systems, count vastly on predefined models and are trained mostly on
specific cyber-attacks. Besides, most IDSs do not consider the imbalanced
nature of ICS datasets, thereby suffering from low accuracy and high false
positive on real datasets. In this paper, we propose a deep representation
learning model to construct new balanced representations of the imbalanced
dataset. The new representations are fed into an ensemble deep learning attack
detection model specifically designed for an ICS environment. The proposed
attack detection model leverages Deep Neural Network (DNN) and Decision Tree
(DT) classifiers to detect cyber-attacks from the new representations. The
performance of the proposed model is evaluated based on 10-fold
cross-validation on two real ICS datasets. The results show that the proposed
method outperforms conventional classifiers, including Random Forest (RF), DNN,
and AdaBoost, as well as recent existing models in the literature. The proposed
approach is a generalized technique, which can be implemented in existing ICS
infrastructures with minimum changes.",arxiv
http://arxiv.org/abs/2003.00838v1,2020-02-11T12:02:47Z,2020-02-11T12:02:47Z,A Machine Learning Framework for Data Ingestion in Document Images,"Paper documents are widely used as an irreplaceable channel of information in
many fields, especially in financial industry, fostering a great amount of
demand for systems which can convert document images into structured data
representations. In this paper, we present a machine learning framework for
data ingestion in document images, which processes the images uploaded by users
and return fine-grained data in JSON format. Details of model architectures,
design strategies, distinctions with existing solutions and lessons learned
during development are elaborated. We conduct abundant experiments on both
synthetic and real-world data in State Street. The experimental results
indicate the effectiveness and efficiency of our methods.",arxiv
http://arxiv.org/abs/1204.1653v1,2012-04-07T16:34:20Z,2012-04-07T16:34:20Z,Machine Cognition Models: EPAM and GPS,"Through history, the human being tried to relay its daily tasks to other
creatures, which was the main reason behind the rise of civilizations. It
started with deploying animals to automate tasks in the field of
agriculture(bulls), transportation (e.g. horses and donkeys), and even
communication (pigeons). Millenniums after, come the Golden age with
""Al-jazari"" and other Muslim inventors, which were the pioneers of automation,
this has given birth to industrial revolution in Europe, centuries after. At
the end of the nineteenth century, a new era was to begin, the computational
era, the most advanced technological and scientific development that is driving
the mankind and the reason behind all the evolutions of science; such as
medicine, communication, education, and physics. At this edge of technology
engineers and scientists are trying to model a machine that behaves the same as
they do, which pushed us to think about designing and implementing ""Things
that-Thinks"", then artificial intelligence was. In this work we will cover each
of the major discoveries and studies in the field of machine cognition, which
are the ""Elementary Perceiver and Memorizer""(EPAM) and ""The General Problem
Solver""(GPS). The First one focus mainly on implementing the human-verbal
learning behavior, while the second one tries to model an architecture that is
able to solve problems generally (e.g. theorem proving, chess playing, and
arithmetic). We will cover the major goals and the main ideas of each model, as
well as comparing their strengths and weaknesses, and finally giving their
fields of applications. And Finally, we will suggest a real life implementation
of a cognitive machine.",arxiv
http://arxiv.org/abs/1808.06352v1,2018-08-20T09:06:21Z,2018-08-20T09:06:21Z,"Navigating the Landscape for Real-time Localisation and Mapping for
  Robotics and Virtual and Augmented Reality","Visual understanding of 3D environments in real-time, at low power, is a huge
computational challenge. Often referred to as SLAM (Simultaneous Localisation
and Mapping), it is central to applications spanning domestic and industrial
robotics, autonomous vehicles, virtual and augmented reality. This paper
describes the results of a major research effort to assemble the algorithms,
architectures, tools, and systems software needed to enable delivery of SLAM,
by supporting applications specialists in selecting and configuring the
appropriate algorithm and the appropriate hardware, and compilation pathway, to
meet their performance, accuracy, and energy consumption goals. The major
contributions we present are (1) tools and methodology for systematic
quantitative evaluation of SLAM algorithms, (2) automated,
machine-learning-guided exploration of the algorithmic and implementation
design space with respect to multiple objectives, (3) end-to-end simulation
tools to enable optimisation of heterogeneous, accelerated architectures for
the specific algorithmic requirements of the various SLAM algorithmic
approaches, and (4) tools for delivering, where appropriate, accelerated,
adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",arxiv
http://arxiv.org/abs/2109.06668v2,2021-09-15T17:25:20Z,2021-09-14T13:16:33Z,Exploration in Deep Reinforcement Learning: A Comprehensive Survey,"Deep Reinforcement Learning (DRL) and Deep Multi-agent Reinforcement Learning
(MARL) have achieved significant success across a wide range of domains, such
as game AI, autonomous vehicles, robotics and finance. However, DRL and deep
MARL agents are widely known to be sample-inefficient and millions of
interactions are usually needed even for relatively simple game settings, thus
preventing the wide application in real-industry scenarios. One bottleneck
challenge behind is the well-known exploration problem, i.e., how to
efficiently explore the unknown environments and collect informative
experiences that could benefit the policy learning most.
  In this paper, we conduct a comprehensive survey on existing exploration
methods in DRL and deep MARL for the purpose of providing understandings and
insights on the critical problems and solutions. We first identify several key
challenges to achieve efficient exploration, which most of the exploration
methods aim at addressing. Then we provide a systematic survey of existing
approaches by classifying them into two major categories: uncertainty-oriented
exploration and intrinsic motivation-oriented exploration. The essence of
uncertainty-oriented exploration is to leverage the quantification of the
epistemic and aleatoric uncertainty to derive efficient exploration. By
contrast, intrinsic motivation-oriented exploration methods usually incorporate
different reward agnostic information for intrinsic exploration guidance.
Beyond the above two main branches, we also conclude other exploration methods
which adopt sophisticated techniques but are difficult to be classified into
the above two categories. In addition, we provide a comprehensive empirical
comparison of exploration methods for DRL on a set of commonly used benchmarks.
Finally, we summarize the open problems of exploration in DRL and deep MARL and
point out a few future directions.",arxiv
http://arxiv.org/abs/2007.09881v1,2020-07-20T04:17:30Z,2020-07-20T04:17:30Z,"DeepCO: Offline Combinatorial Optimization Framework Utilizing Deep
  Learning","Combinatorial optimization serves as an essential part in many modern
industrial applications. A great number of the problems are offline setting due
to safety and/or cost issues. While simulation-based approaches appear
difficult to realise for complicated systems, in this research, we propose
DeepCO, an offline combinatorial optimization framework utilizing deep
learning. We also design an offline variation of Travelling Salesman Problem
(TSP) to model warehouse operation sequence optimization problem for
evaluation. With only limited historical data, novel proposed distribution
regularized optimization method outperforms existing baseline method in offline
TSP experiment reducing route length by 5.7% averagely and shows great
potential in real world problems.",arxiv
http://arxiv.org/abs/1812.03201v2,2018-12-18T23:40:20Z,2018-12-07T20:10:23Z,Residual Reinforcement Learning for Robot Control,"Conventional feedback control methods can solve various types of robot
control problems very efficiently by capturing the structure with explicit
models, such as rigid body equations of motion. However, many control problems
in modern manufacturing deal with contacts and friction, which are difficult to
capture with first-order physical modeling. Hence, applying control design
methodologies to these kinds of problems often results in brittle and
inaccurate controllers, which have to be manually tuned for deployment.
Reinforcement learning (RL) methods have been demonstrated to be capable of
learning continuous robot controllers from interactions with the environment,
even for problems that include friction and contacts. In this paper, we study
how we can solve difficult control problems in the real world by decomposing
them into a part that is solved efficiently by conventional feedback control
methods, and the residual which is solved with RL. The final control policy is
a superposition of both control signals. We demonstrate our approach by
training an agent to successfully perform a real-world block assembly task
involving contacts and unstable objects.",arxiv
http://arxiv.org/abs/2101.02494v2,2021-03-12T01:05:19Z,2021-01-07T11:26:20Z,Corner case data description and detection,"As the major factors affecting the safety of deep learning models, corner
cases and related detection are crucial in AI quality assurance for
constructing safety- and security-critical systems. The generic corner case
researches involve two interesting topics. One is to enhance DL models
robustness to corner case data via the adjustment on parameters/structure. The
other is to generate new corner cases for model retraining and improvement.
However, the complex architecture and the huge amount of parameters make the
robust adjustment of DL models not easy, meanwhile it is not possible to
generate all real-world corner cases for DL training. Therefore, this paper
proposes to a simple and novel study aiming at corner case data detection via a
specific metric. This metric is developed on surprise adequacy (SA) which has
advantages on capture data behaviors. Furthermore, targeting at characteristics
of corner case data, three modifications on distanced-based SA are developed
for classification applications in this paper. Consequently, through the
experiment analysis on MNIST data and industrial data, the feasibility and
usefulness of the proposed method on corner case data detection are verified.",arxiv
http://arxiv.org/abs/1712.06107v1,2017-12-17T13:00:25Z,2017-12-17T13:00:25Z,Railway Track Specific Traffic Signal Selection Using Deep Learning,"With the railway transportation Industry moving actively towards automation,
accurate location and inventory of wayside track assets like traffic signals,
crossings, switches, mileposts, etc. is of extreme importance. With the new
Positive Train Control (PTC) regulation coming into effect, many railway safety
rules will be tied directly to location of assets like mileposts and signals.
Newer speed regulations will be enforced based on location of the Train with
respect to a wayside asset. Hence it is essential for the railroads to have an
accurate database of the types and locations of these assets. This paper talks
about a real-world use-case of detecting railway signals from a camera mounted
on a moving locomotive and tracking their locations. The camera is engineered
to withstand the environment factors on a moving train and provide a consistent
steady image at around 30 frames per second. Using advanced image analysis and
deep learning techniques, signals are detected in these camera images and a
database of their locations is created. Railway signals differ a lot from road
signals in terms of shapes and rules for placement with respect to track. Due
to space constraint and traffic densities in urban areas signals are not placed
on the same side of the track and multiple lines can run in parallel. Hence
there is need to associate signal detected with the track on which the train
runs. We present a method to associate the signals to the specific track they
belong to using a video feed from the front facing camera mounted on the lead
locomotive. A pipeline of track detection, region of interest selection, signal
detection has been implemented which gives an overall accuracy of 94.7% on a
route covering 150km with 247 signals.",arxiv
http://arxiv.org/abs/1808.05347v1,2018-08-16T04:45:15Z,2018-08-16T04:45:15Z,Tool Breakage Detection using Deep Learning,"In manufacture, steel and other metals are mainly cut and shaped during the
fabrication process by computer numerical control (CNC) machines. To keep high
productivity and efficiency of the fabrication process, engineers need to
monitor the real-time process of CNC machines, and the lifetime management of
machine tools. In a real manufacturing process, breakage of machine tools
usually happens without any indication, this problem seriously affects the
fabrication process for many years. Previous studies suggested many different
approaches for monitoring and detecting the breakage of machine tools. However,
there still exists a big gap between academic experiments and the complex real
fabrication processes such as the high demands of real-time detections, the
difficulty in data acquisition and transmission. In this work, we use the
spindle current approach to detect the breakage of machine tools, which has the
high performance of real-time monitoring, low cost, and easy to install. We
analyze the features of the current of a milling machine spindle through tools
wearing processes, and then we predict the status of tool breakage by a
convolutional neural network(CNN). In addition, we use a BP neural network to
understand the reliability of the CNN. The results show that our CNN approach
can detect tool breakage with an accuracy of 93%, while the best performance of
BP is 80%.",arxiv
http://arxiv.org/abs/2106.04525v1,2021-06-08T17:07:20Z,2021-06-08T17:07:20Z,A critical look at the current train/test split in machine learning,"The randomized or cross-validated split of training and testing sets has been
adopted as the gold standard of machine learning for decades. The establishment
of these split protocols are based on two assumptions: (i)-fixing the dataset
to be eternally static so we could evaluate different machine learning
algorithms or models; (ii)-there is a complete set of annotated data available
to researchers or industrial practitioners. However, in this article, we intend
to take a closer and critical look at the split protocol itself and point out
its weakness and limitation, especially for industrial applications. In many
real-world problems, we must acknowledge that there are numerous situations
where assumption (ii) does not hold. For instance, for interdisciplinary
applications like drug discovery, it often requires real lab experiments to
annotate data which poses huge costs in both time and financial considerations.
In other words, it can be very difficult or even impossible to satisfy
assumption (ii). In this article, we intend to access this problem and
reiterate the paradigm of active learning, and investigate its potential on
solving problems under unconventional train/test split protocols. We further
propose a new adaptive active learning architecture (AAL) which involves an
adaptation policy, in comparison with the traditional active learning that only
unidirectionally adds data points to the training pool. We primarily justify
our points by extensively investigating an interdisciplinary drug-protein
binding problem. We additionally evaluate AAL on more conventional machine
learning benchmarking datasets like CIFAR-10 to demonstrate the
generalizability and efficacy of the new framework.",arxiv
http://arxiv.org/abs/1706.01977v1,2017-06-06T21:02:46Z,2017-06-06T21:02:46Z,"From the Lab to the Desert: Fast Prototyping and Learning of Robot
  Locomotion","We present a methodology for fast prototyping of morphologies and controllers
for robot locomotion. Going beyond simulation-based approaches, we argue that
the form and function of a robot, as well as their interplay with real-world
environmental conditions are critical. Hence, fast design and learning cycles
are necessary to adapt robot shape and behavior to their environment. To this
end, we present a combination of laminate robot manufacturing and
sample-efficient reinforcement learning. We leverage this methodology to
conduct an extensive robot learning experiment. Inspired by locomotion in sea
turtles, we design a low-cost crawling robot with variable, interchangeable
fins. Learning is performed using both bio-inspired and original fin designs in
an artificial indoor environment as well as a natural environment in the
Arizona desert. The findings of this study show that static policies developed
in the laboratory do not translate to effective locomotion strategies in
natural environments. In contrast to that, sample-efficient reinforcement
learning can help to rapidly accommodate changes in the environment or the
robot.",arxiv
http://arxiv.org/abs/2110.12422v1,2021-10-24T12:19:41Z,2021-10-24T12:19:41Z,A Differentiable Newton-Euler Algorithm for Real-World Robotics,"Obtaining dynamics models is essential for robotics to achieve accurate
model-based controllers and simulators for planning. The dynamics models are
typically obtained using model specification of the manufacturer or simple
numerical methods such as linear regression. However, this approach does not
guarantee physically plausible parameters and can only be applied to kinematic
chains consisting of rigid bodies. In this article, we describe a
differentiable simulator that can be used to identify the system parameters of
real-world mechanical systems with complex friction models, holonomic as well
as non-holonomic constraints. To guarantee physically consistent parameters, we
utilize virtual parameters and gradient-based optimization. The described
Differentiable Newton-Euler Algorithm (DiffNEA) can be applied to a class of
dynamical systems and guarantees physically plausible predictions. The
extensive experimental evaluation shows, that the proposed model learning
approach learns accurate dynamics models of systems with complex friction and
non-holonomic constraints. Especially in the offline reinforcement learning
experiments, the identified DiffNEA models excel. For the challenging ball in a
cup task, these models solve the task using model-based offline reinforcement
learning on the physical system. The black-box baselines fail on this task in
simulation and on the physical system despite using more data for learning the
model.",arxiv
http://arxiv.org/abs/1803.11254v3,2019-04-28T16:28:15Z,2018-03-29T21:00:56Z,"Detection, localisation and tracking of pallets using machine learning
  techniques and 2D range data","The problem of autonomous transportation in industrial scenarios is receiving
a renewed interest due to the way it can revolutionise internal logistics,
especially in unstructured environments. This paper presents a novel
architecture allowing a robot to detect, localise, and track (possibly
multiple) pallets using machine learning techniques based on an on-board 2D
laser rangefinder only. The architecture is composed of two main components:
the first stage is a pallet detector employing a Faster Region-based
Convolutional Neural Network (Faster R-CNN) detector cascaded with a CNN-based
classifier; the second stage is a Kalman filter for localising and tracking
detected pallets, which we also use to defer commitment to a pallet detected in
the first stage until sufficient confidence has been acquired via a sequential
data acquisition process. For fine-tuning the CNNs, the architecture has been
systematically evaluated using a real-world dataset containing 340 labeled 2D
scans, which have been made freely available in an online repository. Detection
performance has been assessed on the basis of the average accuracy over k-fold
cross-validation, and it scored 99.58% in our tests. Concerning pallet
localisation and tracking, experiments have been performed in a scenario where
the robot is approaching the pallet to fork. Although data have been originally
acquired by considering only one pallet as per specification of the use case we
consider, artificial data have been generated as well to mimic the presence of
multiple pallets in the robot workspace. Our experimental results confirm that
the system is capable of identifying, localising and tracking pallets with a
high success rate while being robust to false positives.",arxiv
http://arxiv.org/abs/2106.10056v1,2021-06-18T11:10:11Z,2021-06-18T11:10:11Z,"A Vertical Federated Learning Framework for Horizontally Partitioned
  Labels","Vertical federated learning is a collaborative machine learning framework to
train deep leaning models on vertically partitioned data with
privacy-preservation. It attracts much attention both from academia and
industry. Unfortunately, applying most existing vertical federated learning
methods in real-world applications still faces two daunting challenges. First,
most existing vertical federated learning methods have a strong assumption that
at least one party holds the complete set of labels of all data samples, while
this assumption is not satisfied in many practical scenarios, where labels are
horizontally partitioned and the parties only hold partial labels. Existing
vertical federated learning methods can only utilize partial labels, which may
lead to inadequate model update in end-to-end backpropagation. Second,
computational and communication resources vary in parties. Some parties with
limited computational and communication resources will become the stragglers
and slow down the convergence of training. Such straggler problem will be
exaggerated in the scenarios of horizontally partitioned labels in vertical
federated learning. To address these challenges, we propose a novel vertical
federated learning framework named Cascade Vertical Federated Learning (CVFL)
to fully utilize all horizontally partitioned labels to train neural networks
with privacy-preservation. To mitigate the straggler problem, we design a novel
optimization objective which can increase straggler's contribution to the
trained models. We conduct a series of qualitative experiments to rigorously
verify the effectiveness of CVFL. It is demonstrated that CVFL can achieve
comparable performance (e.g., accuracy for classification tasks) with
centralized training. The new optimization objective can further mitigate the
straggler problem comparing with only using the asynchronous aggregation
mechanism during training.",arxiv
http://arxiv.org/abs/2109.07827v1,2021-09-16T09:36:53Z,2021-09-16T09:36:53Z,"Enabling risk-aware Reinforcement Learning for medical interventions
  through uncertainty decomposition","Reinforcement Learning (RL) is emerging as tool for tackling complex control
and decision-making problems. However, in high-risk environments such as
healthcare, manufacturing, automotive or aerospace, it is often challenging to
bridge the gap between an apparently optimal policy learnt by an agent and its
real-world deployment, due to the uncertainties and risk associated with it.
Broadly speaking RL agents face two kinds of uncertainty, 1. aleatoric
uncertainty, which reflects randomness or noise in the dynamics of the world,
and 2. epistemic uncertainty, which reflects the bounded knowledge of the agent
due to model limitations and finite amount of information/data the agent has
acquired about the world. These two types of uncertainty carry fundamentally
different implications for the evaluation of performance and the level of risk
or trust. Yet these aleatoric and epistemic uncertainties are generally
confounded as standard and even distributional RL is agnostic to this
difference. Here we propose how a distributional approach (UA-DQN) can be
recast to render uncertainties by decomposing the net effects of each
uncertainty. We demonstrate the operation of this method in grid world examples
to build intuition and then show a proof of concept application for an RL agent
operating as a clinical decision support system in critical care",arxiv
http://arxiv.org/abs/2007.09712v1,2020-07-19T16:47:26Z,2020-07-19T16:47:26Z,"Deep Anomaly Detection for Time-series Data in Industrial IoT: A
  Communication-Efficient On-device Federated Learning Approach","Since edge device failures (i.e., anomalies) seriously affect the production
of industrial products in Industrial IoT (IIoT), accurately and timely
detecting anomalies is becoming increasingly important. Furthermore, data
collected by the edge device may contain the user's private data, which is
challenging the current detection approaches as user privacy is calling for the
public concern in recent years. With this focus, this paper proposes a new
communication-efficient on-device federated learning (FL)-based deep anomaly
detection framework for sensing time-series data in IIoT. Specifically, we
first introduce a FL framework to enable decentralized edge devices to
collaboratively train an anomaly detection model, which can improve its
generalization ability. Second, we propose an Attention Mechanism-based
Convolutional Neural Network-Long Short Term Memory (AMCNN-LSTM) model to
accurately detect anomalies. The AMCNN-LSTM model uses attention
mechanism-based CNN units to capture important fine-grained features, thereby
preventing memory loss and gradient dispersion problems. Furthermore, this
model retains the advantages of LSTM unit in predicting time series data.
Third, to adapt the proposed framework to the timeliness of industrial anomaly
detection, we propose a gradient compression mechanism based on Top-\textit{k}
selection to improve communication efficiency. Extensive experiment studies on
four real-world datasets demonstrate that the proposed framework can accurately
and timely detect anomalies and also reduce the communication overhead by 50\%
compared to the federated learning framework that does not use a gradient
compression scheme.",arxiv
http://arxiv.org/abs/2007.15215v1,2020-07-30T03:54:32Z,2020-07-30T03:54:32Z,"Learner's Dilemma: IoT Devices Training Strategies in Collaborative Deep
  Learning","With the growth of Internet of Things (IoT) and mo-bile edge computing,
billions of smart devices are interconnected to develop applications used in
various domains including smart homes, healthcare and smart manufacturing. Deep
learning has been extensively utilized in various IoT applications which
require huge amount of data for model training. Due to privacy requirements,
smart IoT devices do not release data to a remote third party for their use. To
overcome this problem, collaborative approach to deep learning, also known as
Collaborative DeepLearning (CDL) has been largely employed in data-driven
applications. This approach enables multiple edge IoT devices to train their
models locally on mobile edge devices. In this paper,we address IoT device
training problem in CDL by analyzing the behavior of mobile edge devices using
a game-theoretic model,where each mobile edge device aims at maximizing the
accuracy of its local model at the same time limiting the overhead of
participating in CDL. We analyze the Nash Equilibrium in anN-player static game
model. We further present a novel cluster-based fair strategy to approximately
solve the CDL game to enforce mobile edge devices for cooperation. Our
experimental results and evaluation analysis in a real-world smart home
deployment show that 80% mobile edge devices are ready to cooperate in CDL,
while 20% of them do not train their local models collaboratively.",arxiv
http://arxiv.org/abs/2010.03868v3,2021-04-02T19:58:31Z,2020-10-08T09:49:49Z,"CSTNet: A Dual-Branch Convolutional Network for Imaging of Reactive
  Flows using Chemical Species Tomography","Chemical Species Tomography (CST) has been widely used for in situ imaging of
critical parameters, e.g. species concentration and temperature, in reactive
flows. However, even with state-of-the-art computational algorithms the method
is limited due to the inherently ill-posed and rank-deficient tomographic data
inversion, and by high computational cost. These issues hinder its application
for real-time flow diagnosis. To address them, we present here a novel
CST-based convolutional neural Network (CSTNet) for high-fidelity, rapid, and
simultaneous imaging of species concentration and temperature. CSTNet
introduces a shared feature extractor that incorporates the CST measurement and
sensor layout into the learning network. In addition, a dual-branch
architecture is proposed for image reconstruction with crosstalk decoders that
automatically learn the naturally correlated distributions of species
concentration and temperature. The proposed CSTNet is validated both with
simulated datasets, and with measured data from real flames in experiments
using an industry-oriented sensor. Superior performance is found relative to
previous approaches, in terms of robustness to measurement noise and
millisecond-level computing time. This is the first time, to the best of our
knowledge, that a deep learning-based algorithm for CST has been experimentally
validated for simultaneous imaging of multiple critical parameters in reactive
flows using a low-complexity optical sensor with severely limited number of
laser beams.",arxiv
http://arxiv.org/abs/2106.13955v1,2021-06-26T06:47:41Z,2021-06-26T06:47:41Z,Autonomous Deep Quality Monitoring in Streaming Environments,"The common practice of quality monitoring in industry relies on manual
inspection well-known to be slow, error-prone and operator-dependent. This
issue raises strong demand for automated real-time quality monitoring developed
from data-driven approaches thus alleviating from operator dependence and
adapting to various process uncertainties. Nonetheless, current approaches do
not take into account the streaming nature of sensory information while relying
heavily on hand-crafted features making them application-specific. This paper
proposes the online quality monitoring methodology developed from recently
developed deep learning algorithms for data streams, Neural Networks with
Dynamically Evolved Capacity (NADINE), namely NADINE++. It features the
integration of 1-D and 2-D convolutional layers to extract natural features of
time-series and visual data streams captured from sensors and cameras of the
injection molding machines from our own project. Real-time experiments have
been conducted where the online quality monitoring task is simulated on the fly
under the prequential test-then-train fashion - the prominent data stream
evaluation protocol. Comparison with the state-of-the-art techniques clearly
exhibits the advantage of NADINE++ with 4.68\% improvement on average for the
quality monitoring task in streaming environments. To support the reproducible
research initiative, codes, results of NADINE++ along with supplementary
materials and injection molding dataset are made available in
\url{https://github.com/ContinualAL/NADINE-IJCNN2021}.",arxiv
http://arxiv.org/abs/2005.03297v2,2020-09-23T09:14:20Z,2020-05-07T07:42:17Z,Knowledge Enhanced Neural Fashion Trend Forecasting,"Fashion trend forecasting is a crucial task for both academia and industry.
Although some efforts have been devoted to tackling this challenging task, they
only studied limited fashion elements with highly seasonal or simple patterns,
which could hardly reveal the real fashion trends. Towards insightful fashion
trend forecasting, this work focuses on investigating fine-grained fashion
element trends for specific user groups. We first contribute a large-scale
fashion trend dataset (FIT) collected from Instagram with extracted time series
fashion element records and user information. Further-more, to effectively
model the time series data of fashion elements with rather complex patterns, we
propose a Knowledge EnhancedRecurrent Network model (KERN) which takes
advantage of the capability of deep recurrent neural networks in modeling
time-series data. Moreover, it leverages internal and external knowledge in
fashion domain that affects the time-series patterns of fashion element trends.
Such incorporation of domain knowledge further enhances the deep learning model
in capturing the patterns of specific fashion elements and predicting the
future trends. Extensive experiments demonstrate that the proposed KERN model
can effectively capture the complicated patterns of objective fashion elements,
therefore making preferable fashion trend forecast.",arxiv
http://arxiv.org/abs/1706.06978v4,2018-09-13T04:37:06Z,2017-06-21T16:05:17Z,Deep Interest Network for Click-Through Rate Prediction,"Click-through rate prediction is an essential task in industrial
applications, such as online advertising. Recently deep learning based models
have been proposed, which follow a similar Embedding\&MLP paradigm. In these
methods large scale sparse input features are first mapped into low dimensional
embedding vectors, and then transformed into fixed-length vectors in a
group-wise manner, finally concatenated together to fed into a multilayer
perceptron (MLP) to learn the nonlinear relations among features. In this way,
user features are compressed into a fixed-length representation vector, in
regardless of what candidate ads are. The use of fixed-length vector will be a
bottleneck, which brings difficulty for Embedding\&MLP methods to capture
user's diverse interests effectively from rich historical behaviors. In this
paper, we propose a novel model: Deep Interest Network (DIN) which tackles this
challenge by designing a local activation unit to adaptively learn the
representation of user interests from historical behaviors with respect to a
certain ad. This representation vector varies over different ads, improving the
expressive ability of model greatly. Besides, we develop two techniques:
mini-batch aware regularization and data adaptive activation function which can
help training industrial deep networks with hundreds of millions of parameters.
Experiments on two public datasets as well as an Alibaba real production
dataset with over 2 billion samples demonstrate the effectiveness of proposed
approaches, which achieve superior performance compared with state-of-the-art
methods. DIN now has been successfully deployed in the online display
advertising system in Alibaba, serving the main traffic.",arxiv
http://arxiv.org/abs/2001.07523v3,2021-02-15T23:42:03Z,2020-01-16T20:08:56Z,"The gap between theory and practice in function approximation with deep
  neural networks","Deep learning (DL) is transforming industry as decision-making processes are
being automated by deep neural networks (DNNs) trained on real-world data.
Driven partly by rapidly-expanding literature on DNN approximation theory
showing they can approximate a rich variety of functions, such tools are
increasingly being considered for problems in scientific computing. Yet, unlike
traditional algorithms in this field, little is known about DNNs from the
principles of numerical analysis, e.g., stability, accuracy, computational
efficiency and sample complexity. In this paper we introduce a computational
framework for examining DNNs in practice, and use it to study empirical
performance with regard to these issues. We study performance of DNNs of
different widths & depths on test functions in various dimensions, including
smooth and piecewise smooth functions. We also compare DL against best-in-class
methods for smooth function approx. based on compressed sensing (CS). Our main
conclusion from these experiments is that there is a crucial gap between the
approximation theory of DNNs and their practical performance, with trained DNNs
performing relatively poorly on functions for which there are strong
approximation results (e.g. smooth functions), yet performing well in
comparison to best-in-class methods for other functions. To analyze this gap
further, we provide some theoretical insights. We establish a practical
existence theorem, asserting existence of a DNN architecture and training
procedure that offers the same performance as CS. This establishes a key
theoretical benchmark, showing the gap can be closed, albeit via a strategy
guaranteed to perform as well as, but no better than, current best-in-class
schemes. Nevertheless, it demonstrates the promise of practical DNN approx., by
highlighting potential for better schemes through careful design of DNN
architectures and training strategies.",arxiv
http://arxiv.org/abs/2011.09747v2,2021-09-29T15:26:04Z,2020-11-19T09:53:27Z,"Energy Aware Deep Reinforcement Learning Scheduling for Sensors
  Correlated in Time and Space","Millions of battery-powered sensors deployed for monitoring purposes in a
multitude of scenarios, e.g., agriculture, smart cities, industry, etc.,
require energy-efficient solutions to prolong their lifetime. When these
sensors observe a phenomenon distributed in space and evolving in time, it is
expected that collected observations will be correlated in time and space. In
this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling
mechanism capable of taking advantage of correlated information. We design our
solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The
proposed mechanism is capable of determining the frequency with which sensors
should transmit their updates, to ensure accurate collection of observations,
while simultaneously considering the energy available. To evaluate our
scheduling mechanism, we use multiple datasets containing environmental
observations obtained in multiple real deployments. The real observations
enable us to model the environment with which the mechanism interacts as
realistically as possible. We show that our solution can significantly extend
the sensors' lifetime. We compare our mechanism to an idealized, all-knowing
scheduler to demonstrate that its performance is near-optimal. Additionally, we
highlight the unique feature of our design, energy-awareness, by displaying the
impact of sensors' energy levels on the frequency of updates.",arxiv
http://arxiv.org/abs/2104.08619v2,2021-05-09T17:20:17Z,2021-04-17T18:51:50Z,Optimal Counterfactual Explanations for Scorecard modelling,"Counterfactual explanations is one of the post-hoc methods used to provide
explainability to machine learning models that have been attracting attention
in recent years. Most examples in the literature, address the problem of
generating post-hoc explanations for black-box machine learning models after
the rejection of a loan application. In contrast, in this work, we investigate
mathematical programming formulations for scorecard models, a type of
interpretable model predominant within the banking industry for lending. The
proposed mixed-integer programming formulations combine objective functions to
ensure close, realistic and sparse counterfactuals using multi-objective
optimization techniques for a binary, probability or continuous outcome.
Moreover, we extend these formulations to generate multiple optimal
counterfactuals simultaneously while guaranteeing diversity. Experiments on two
real-world datasets confirm that the presented approach can generate optimal
diverse counterfactuals addressing desired properties with assumable CPU times
for practice use.",arxiv
http://arxiv.org/abs/2012.04861v2,2021-02-23T03:41:10Z,2020-12-09T04:47:07Z,"Multi Agent Team Learning in Disaggregated Virtualized Open Radio Access
  Networks (O-RAN)","Starting from the Cloud Radio Access Network (C-RAN), continuing with the
virtual Radio Access Network (vRAN) and most recently with Open RAN (O-RAN)
initiative, Radio Access Network (RAN) architectures have significantly evolved
in the past decade. In the last few years, the wireless industry has witnessed
a strong trend towards disaggregated, virtualized and open RANs, with numerous
tests and deployments world wide. One unique aspect that motivates this paper
is the availability of new opportunities that arise from using machine learning
to optimize the RAN in closed-loop, i.e. without human intervention, where the
complexity of disaggregation and virtualization makes well-known Self-Organized
Networking (SON) solutions inadequate. In our view, Multi-Agent Systems (MASs)
with team learning, can play an essential role in the control and coordination
of controllers of O-RAN, i.e. near-real-time and non-real-time RAN Intelligent
Controller (RIC). In this article, we first present the state-of-the-art
research in multi-agent systems and team learning, then we provide an overview
of the landscape in RAN disaggregation and virtualization, as well as O-RAN
which emphasizes the open interfaces introduced by the O-RAN Alliance. We
present a case study for agent placement and the AI feedback required in O-RAN,
and finally, we identify challenges and open issues to provide a roadmap for
researchers.",arxiv
http://arxiv.org/abs/2004.13094v1,2020-04-27T18:54:36Z,2020-04-27T18:54:36Z,Compact retail shelf segmentation for mobile deployment,"The recent surge of automation in the retail industries has rapidly increased
demand for applying deep learning models on mobile devices. To make the deep
learning models real-time on-device, a compact efficient network becomes
inevitable. In this paper, we work on one such common problem in the retail
industries - Shelf segmentation. Shelf segmentation can be interpreted as a
pixel-wise classification problem, i.e., each pixel is classified as to whether
they belong to visible shelf edges or not. The aim is not just to segment shelf
edges, but also to deploy the model on mobile devices. As there is no standard
solution for such dense classification problem on mobile devices, we look at
semantic segmentation architectures which can be deployed on edge. We modify
low-footprint semantic segmentation architectures to perform shelf
segmentation. In addressing this issue, we modified the famous U-net
architecture in certain aspects to make it fit for on-devices without impacting
significant drop in accuracy and also with 15X fewer parameters. In this paper,
we proposed Light Weight Segmentation Network (LWSNet), a small compact model
able to run fast on devices with limited memory and can train with less amount
(~ 100 images) of labeled data.",arxiv
http://arxiv.org/abs/1912.02059v2,2020-06-28T20:20:33Z,2019-12-04T15:32:53Z,"Learning to Dynamically Coordinate Multi-Robot Teams in Graph Attention
  Networks","Increasing interest in integrating advanced robotics within manufacturing has
spurred a renewed concentration in developing real-time scheduling solutions to
coordinate human-robot collaboration in this environment. Traditionally, the
problem of scheduling agents to complete tasks with temporal and spatial
constraints has been approached either with exact algorithms, which are
computationally intractable for large-scale, dynamic coordination, or
approximate methods that require domain experts to craft heuristics for each
application. We seek to overcome the limitations of these conventional methods
by developing a novel graph attention network formulation to automatically
learn features of scheduling problems to allow their deployment. To learn
effective policies for combinatorial optimization problems via machine
learning, we combine imitation learning on smaller problems with deep
Q-learning on larger problems, in a non-parametric framework, to allow for
fast, near-optimal scheduling of robot teams. We show that our network-based
policy finds at least twice as many solutions over prior state-of-the-art
methods in all testing scenarios.",arxiv
http://arxiv.org/abs/1610.04494v1,2016-02-07T19:28:08Z,2016-02-07T19:28:08Z,Localization for Wireless Sensor Networks: A Neural Network Approach,"As Wireless Sensor Networks are penetrating into the industrial domain, many
research opportunities are emerging. One such essential and challenging
application is that of node localization. A feed-forward neural network based
methodology is adopted in this paper. The Received Signal Strength Indicator
(RSSI) values of the anchor node beacons are used. The number of anchor nodes
and their configurations has an impact on the accuracy of the localization
system, which is also addressed in this paper. Five different training
algorithms are evaluated to find the training algorithm that gives the best
result. The multi-layer Perceptron (MLP) neural network model was trained using
Matlab. In order to evaluate the performance of the proposed method in real
time, the model obtained was then implemented on the Arduino microcontroller.
With four anchor nodes, an average 2D localization error of 0.2953 m has been
achieved with a 12-12-2 neural network structure. The proposed method can also
be implemented on any other embedded microcontroller system.",arxiv
http://arxiv.org/abs/1611.05356v2,2017-03-24T22:15:17Z,2016-11-16T16:50:57Z,"Towards Interconnected Virtual Reality: Opportunities, Challenges and
  Enablers","Just recently, the concept of augmented and virtual reality (AR/VR) over
wireless has taken the entire 5G ecosystem by storm spurring an unprecedented
interest from both academia, industry and others. Yet, the success of an
immersive VR experience hinges on solving a plethora of grand challenges
cutting across multiple disciplines. This article underscores the importance of
VR technology as a disruptive use case of 5G (and beyond) harnessing the latest
development of storage/memory, fog/edge computing, computer vision, artificial
intelligence and others. In particular, the main requirements of wireless
interconnected VR are described followed by a selection of key enablers, then,
research avenues and their underlying grand challenges are presented.
Furthermore, we examine three VR case studies and provide numerical results
under various storage, computing and network configurations. Finally, this
article exposes the limitations of current networks and makes the case for more
theory, and innovations to spearhead VR for the masses.",arxiv
http://arxiv.org/abs/1807.07506v2,2018-11-19T07:11:52Z,2018-07-19T15:58:14Z,Improving Simple Models with Confidence Profiles,"In this paper, we propose a new method called ProfWeight for transferring
information from a pre-trained deep neural network that has a high test
accuracy to a simpler interpretable model or a very shallow network of low
complexity and a priori low test accuracy. We are motivated by applications in
interpretability and model deployment in severely memory constrained
environments (like sensors). Our method uses linear probes to generate
confidence scores through flattened intermediate representations. Our transfer
method involves a theoretically justified weighting of samples during the
training of the simple model using confidence scores of these intermediate
layers. The value of our method is first demonstrated on CIFAR-10, where our
weighting method significantly improves (3-4%) networks with only a fraction of
the number of Resnet blocks of a complex Resnet model. We further demonstrate
operationally significant results on a real manufacturing problem, where we
dramatically increase the test accuracy of a CART model (the domain standard)
by roughly 13%.",arxiv
http://arxiv.org/abs/2107.00127v1,2021-06-30T22:18:49Z,2021-06-30T22:18:49Z,"SQRP: Sensing Quality-aware Robot Programming System for Non-expert
  Programmers","Robot programming typically makes use of a set of mechanical skills that is
acquired by machine learning. Because there is in general no guarantee that
machine learning produces robot programs that are free of surprising behavior,
the safe execution of a robot program must utilize monitoring modules that take
sensor data as inputs in real time to ensure the correctness of the skill
execution. Owing to the fact that sensors and monitoring algorithms are usually
subject to physical restrictions and that effective robot programming is
sensitive to the selection of skill parameters, these considerations may lead
to different sensor input qualities such as the view coverage of a vision
system that determines whether a skill can be successfully deployed in
performing a task. Choosing improper skill parameters may cause the monitoring
modules to delay or miss the detection of important events such as a mechanical
failure. These failures may reduce the throughput in robotic manufacturing and
could even cause a destructive system crash. To address above issues, we
propose a sensing quality-aware robot programming system that automatically
computes the sensing qualities as a function of the robot's environment and
uses the information to guide non-expert users to select proper skill
parameters in the programming phase. We demonstrate our system framework on a
6DOF robot arm for an object pick-up task.",arxiv
http://arxiv.org/abs/1912.11066v1,2019-12-23T19:11:50Z,2019-12-23T19:11:50Z,"FisheyeMultiNet: Real-time Multi-task Learning Architecture for
  Surround-view Automated Parking System","Automated Parking is a low speed manoeuvring scenario which is quite
unstructured and complex, requiring full 360{\deg} near-field sensing around
the vehicle. In this paper, we discuss the design and implementation of an
automated parking system from the perspective of camera based deep learning
algorithms. We provide a holistic overview of an industrial system covering the
embedded system, use cases and the deep learning architecture. We demonstrate a
real-time multi-task deep learning network called FisheyeMultiNet, which
detects all the necessary objects for parking on a low-power embedded system.
FisheyeMultiNet runs at 15 fps for 4 cameras and it has three tasks namely
object detection, semantic segmentation and soiling detection. To encourage
further research, we release a partial dataset of 5,000 images containing
semantic segmentation and bounding box detection ground truth via WoodScape
project \cite{yogamani2019woodscape}.",arxiv
http://arxiv.org/abs/1911.06832v1,2019-11-15T19:01:21Z,2019-11-15T19:01:21Z,"Data-efficient Co-Adaptation of Morphology and Behaviour with Deep
  Reinforcement Learning","Humans and animals are capable of quickly learning new behaviours to solve
new tasks. Yet, we often forget that they also rely on a highly specialized
morphology that co-adapted with motor control throughout thousands of years.
Although compelling, the idea of co-adapting morphology and behaviours in
robots is often unfeasible because of the long manufacturing times, and the
need to re-design an appropriate controller for each morphology. In this paper,
we propose a novel approach to automatically and efficiently co-adapt a robot
morphology and its controller. Our approach is based on recent advances in deep
reinforcement learning, and specifically the soft actor critic algorithm. Key
to our approach is the possibility of leveraging previously tested morphologies
and behaviors to estimate the performance of new candidate morphologies. As
such, we can make full use of the information available for making more
informed decisions, with the ultimate goal of achieving a more data-efficient
co-adaptation (i.e., reducing the number of morphologies and behaviors tested).
Simulated experiments show that our approach requires drastically less design
prototypes to find good morphology-behaviour combinations, making this method
particularly suitable for future co-adaptation of robot designs in the real
world.",arxiv
http://arxiv.org/abs/2009.02560v1,2020-09-05T16:20:47Z,2020-09-05T16:20:47Z,"Particle Swarm Optimized Federated Learning For Industrial IoT and Smart
  City Services","Most of the research on Federated Learning (FL) has focused on analyzing
global optimization, privacy, and communication, with limited attention
focusing on analyzing the critical matter of performing efficient local
training and inference at the edge devices. One of the main challenges for
successful and efficient training and inference on edge devices is the careful
selection of parameters to build local Machine Learning (ML) models. To this
aim, we propose a Particle Swarm Optimization (PSO)-based technique to optimize
the hyperparameter settings for the local ML models in an FL environment. We
evaluate the performance of our proposed technique using two case studies.
First, we consider smart city services and use an experimental transportation
dataset for traffic prediction as a proxy for this setting. Second, we consider
Industrial IoT (IIoT) services and use the real-time telemetry dataset to
predict the probability that a machine will fail shortly due to component
failures. Our experiments indicate that PSO provides an efficient approach for
tuning the hyperparameters of deep Long short-term memory (LSTM) models when
compared to the grid search method. Our experiments illustrate that the number
of clients-server communication rounds to explore the landscape of
configurations to find the near-optimal parameters are greatly reduced (roughly
by two orders of magnitude needing only 2%--4% of the rounds compared to state
of the art non-PSO-based approaches). We also demonstrate that utilizing the
proposed PSO-based technique to find the near-optimal configurations for FL and
centralized learning models does not adversely affect the accuracy of the
models.",arxiv
http://arxiv.org/abs/1208.5333v2,2012-09-16T13:12:03Z,2012-08-27T08:51:23Z,A hybrid ACO approach to the Matrix Bandwidth Minimization Problem,"The evolution of the human society raises more and more difficult endeavors.
For some of the real-life problems, the computing time-restriction enhances
their complexity. The Matrix Bandwidth Minimization Problem (MBMP) seeks for a
simultaneous permutation of the rows and the columns of a square matrix in
order to keep its nonzero entries close to the main diagonal. The MBMP is a
highly investigated P-complete problem, as it has broad applications in
industry, logistics, artificial intelligence or information recovery. This
paper describes a new attempt to use the Ant Colony Optimization framework in
tackling MBMP. The introduced model is based on the hybridization of the Ant
Colony System technique with new local search mechanisms. Computational
experiments confirm a good performance of the proposed algorithm for the
considered set of MBMP instances.",arxiv
http://arxiv.org/abs/2105.04261v1,2021-05-10T10:59:38Z,2021-05-10T10:59:38Z,"Neuroscience-inspired perception-action in robotics: applying active
  inference for state estimation, control and self-perception","Unlike robots, humans learn, adapt and perceive their bodies by interacting
with the world. Discovering how the brain represents the body and generates
actions is of major importance for robotics and artificial intelligence. Here
we discuss how neuroscience findings open up opportunities to improve current
estimation and control algorithms in robotics. In particular, how active
inference, a mathematical formulation of how the brain resists a natural
tendency to disorder, provides a unified recipe to potentially solve some of
the major challenges in robotics, such as adaptation, robustness, flexibility,
generalization and safe interaction. This paper summarizes some experiments and
lessons learned from developing such a computational model on real embodied
platforms, i.e., humanoid and industrial robots. Finally, we showcase the
limitations and challenges that we are still facing to give robots human-like
perception",arxiv
http://arxiv.org/abs/2001.08855v1,2020-01-24T01:06:25Z,2020-01-24T01:06:25Z,"Privacy for All: Demystify Vulnerability Disparity of Differential
  Privacy against Membership Inference Attack","Machine learning algorithms, when applied to sensitive data, pose a potential
threat to privacy. A growing body of prior work has demonstrated that
membership inference attack (MIA) can disclose specific private information in
the training data to an attacker. Meanwhile, the algorithmic fairness of
machine learning has increasingly caught attention from both academia and
industry. Algorithmic fairness ensures that the machine learning models do not
discriminate a particular demographic group of individuals (e.g., black and
female people). Given that MIA is indeed a learning model, it raises a serious
concern if MIA ``fairly'' treats all groups of individuals equally. In other
words, whether a particular group is more vulnerable against MIA than the other
groups. This paper examines the algorithmic fairness issue in the context of
MIA and its defenses. First, for fairness evaluation, it formalizes the
notation of vulnerability disparity (VD) to quantify the difference of MIA
treatment on different demographic groups. Second, it evaluates VD on four
real-world datasets, and shows that VD indeed exists in these datasets. Third,
it examines the impacts of differential privacy, as a defense mechanism of MIA,
on VD. The results show that although DP brings significant change on VD, it
cannot eliminate VD completely. Therefore, fourth, it designs a new mitigation
algorithm named FAIRPICK to reduce VD. An extensive set of experimental results
demonstrate that FAIRPICK can effectively reduce VD for both with and without
the DP deployment.",arxiv
http://arxiv.org/abs/1711.05253v3,2018-03-30T17:37:00Z,2017-11-14T18:56:12Z,"Learning Image-Conditioned Dynamics Models for Control of Under-actuated
  Legged Millirobots","Millirobots are a promising robotic platform for many applications due to
their small size and low manufacturing costs. Legged millirobots, in
particular, can provide increased mobility in complex environments and improved
scaling of obstacles. However, controlling these small, highly dynamic, and
underactuated legged systems is difficult. Hand-engineered controllers can
sometimes control these legged millirobots, but they have difficulties with
dynamic maneuvers and complex terrains. We present an approach for controlling
a real-world legged millirobot that is based on learned neural network models.
Using less than 17 minutes of data, our method can learn a predictive model of
the robot's dynamics that can enable effective gaits to be synthesized on the
fly for following user-specified waypoints on a given terrain. Furthermore, by
leveraging expressive, high-capacity neural network models, our approach allows
for these predictions to be directly conditioned on camera images, endowing the
robot with the ability to predict how different terrains might affect its
dynamics. This enables sample-efficient and effective learning for locomotion
of a dynamic legged millirobot on various terrains, including gravel, turf,
carpet, and styrofoam. Experiment videos can be found at
https://sites.google.com/view/imageconddyn",arxiv
http://arxiv.org/abs/1608.01733v2,2016-10-04T03:49:19Z,2016-08-05T01:12:55Z,"The IPAC Image Subtraction and Discovery Pipeline for the intermediate
  Palomar Transient Factory","We describe the near real-time transient-source discovery engine for the
intermediate Palomar Transient Factory (iPTF), currently in operations at the
Infrared Processing and Analysis Center (IPAC), Caltech. We coin this system
the IPAC/iPTF Discovery Engine (or IDE). We review the algorithms used for
PSF-matching, image subtraction, detection, photometry, and machine-learned
(ML) vetting of extracted transient candidates. We also review the performance
of our ML classifier. For a limiting signal-to-noise ratio of 4 in relatively
unconfused regions, ""bogus"" candidates from processing artifacts and imperfect
image subtractions outnumber real transients by ~ 10:1. This can be
considerably higher for image data with inaccurate astrometric and/or
PSF-matching solutions. Despite this occasionally high contamination rate, the
ML classifier is able to identify real transients with an efficiency (or
completeness) of ~ 97% for a maximum tolerable false-positive rate of 1% when
classifying raw candidates. All subtraction-image metrics, source features, ML
probability-based real-bogus scores, contextual metadata from other surveys,
and possible associations with known Solar System objects are stored in a
relational database for retrieval by the various science working groups. We
review our efforts in mitigating false-positives and our experience in
optimizing the overall system in response to the multitude of science projects
underway with iPTF.",arxiv
http://arxiv.org/abs/1209.3775v1,2012-09-17T20:00:02Z,2012-09-17T20:00:02Z,Using Machine Learning for Discovery in Synoptic Survey Imaging,"Modern time-domain surveys continuously monitor large swaths of the sky to
look for astronomical variability. Astrophysical discovery in such data sets is
complicated by the fact that detections of real transient and variable sources
are highly outnumbered by bogus detections caused by imperfect subtractions,
atmospheric effects and detector artefacts. In this work we present a machine
learning (ML) framework for discovery of variability in time-domain imaging
surveys. Our ML methods provide probabilistic statements, in near real time,
about the degree to which each newly observed source is astrophysically
relevant source of variable brightness. We provide details about each of the
analysis steps involved, including compilation of the training and testing
sets, construction of descriptive image-based and contextual features, and
optimization of the feature subset and model tuning parameters. Using a
validation set of nearly 30,000 objects from the Palomar Transient Factory, we
demonstrate a missed detection rate of at most 7.7% at our chosen
false-positive rate of 1% for an optimized ML classifier of 23 features,
selected to avoid feature correlation and over-fitting from an initial library
of 42 attributes. Importantly, we show that our classification methodology is
insensitive to mis-labelled training data up to a contamination of nearly 10%,
making it easier to compile sufficient training sets for accurate performance
in future surveys. This ML framework, if so adopted, should enable the
maximization of scientific gain from future synoptic survey and enable fast
follow-up decisions on the vast amounts of streaming data produced by such
experiments.",arxiv
http://arxiv.org/abs/2012.03631v1,2020-12-07T12:24:32Z,2020-12-07T12:24:32Z,"Exploitation of Channel-Learning for Enhancing 5G Blind Beam Index
  Detection","Proliferation of 5G devices and services has driven the demand for wide-scale
enhancements ranging from data rate, reliability, and compatibility to sustain
the ever increasing growth of the telecommunication industry. In this regard,
this work investigates how machine learning technology can improve the
performance of 5G cell and beam index search in practice. The cell search is an
essential function for a User Equipment (UE) to be initially associated with a
base station, and is also important to further maintain the wireless
connection. Unlike the former generation cellular systems, the 5G UE faces with
an additional challenge to detect suitable beams as well as the cell identities
in the cell search procedures. Herein, we propose and implement new
channel-learning schemes to enhance the performance of 5G beam index detection.
The salient point lies in the use of machine learning models and softwarization
for practical implementations in a system level. We develop the proposed
channel-learning scheme including algorithmic procedures and corroborative
system structure for efficient beam index detection. We also implement a
real-time operating 5G testbed based on the off-the-shelf Software Defined
Radio (SDR) platform and conduct intensive experiments with commercial 5G base
stations. The experimental results indicate that the proposed channel-learning
schemes outperform the conventional correlation-based scheme in real 5G channel
environments.",arxiv
http://arxiv.org/abs/1512.07454v1,2015-12-23T12:44:09Z,2015-12-23T12:44:09Z,Evaluation-as-a-Service: Overview and Outlook,"Evaluation in empirical computer science is essential to show progress and
assess technologies developed. Several research domains such as information
retrieval have long relied on systematic evaluation to measure progress: here,
the Cranfield paradigm of creating shared test collections, defining search
tasks, and collecting ground truth for these tasks has persisted up until now.
In recent years, however, several new challenges have emerged that do not fit
this paradigm very well: extremely large data sets, confidential data sets as
found in the medical domain, and rapidly changing data sets as often
encountered in industry. Also, crowdsourcing has changed the way that industry
approaches problem-solving with companies now organizing challenges and handing
out monetary awards to incentivize people to work on their challenges,
particularly in the field of machine learning.
  This white paper is based on discussions at a workshop on
Evaluation-as-a-Service (EaaS). EaaS is the paradigm of not providing data sets
to participants and have them work on the data locally, but keeping the data
central and allowing access via Application Programming Interfaces (API),
Virtual Machines (VM) or other possibilities to ship executables. The objective
of this white paper are to summarize and compare the current approaches and
consolidate the experiences of these approaches to outline the next steps of
EaaS, particularly towards sustainable research infrastructures.
  This white paper summarizes several existing approaches to EaaS and analyzes
their usage scenarios and also the advantages and disadvantages. The many
factors influencing EaaS are overviewed, and the environment in terms of
motivations for the various stakeholders, from funding agencies to challenge
organizers, researchers and participants, to industry interested in supplying
real-world problems for which they require solutions.",arxiv
http://arxiv.org/abs/1712.07452v2,2019-08-13T20:41:40Z,2017-12-20T12:47:39Z,"Self-Supervised Damage-Avoiding Manipulation Strategy Optimization via
  Mental Simulation","Everyday robotics are challenged to deal with autonomous product handling in
applications like logistics or retail, possibly causing damage on the items
during manipulation. Traditionally, most approaches try to minimize physical
interaction with goods. However, this paper proposes to take into account any
unintended object motion and to learn damage-minimizing manipulation strategies
in a self-supervised way. The presented approach consists of a simulation-based
planning method for an optimal manipulation sequence with respect to possible
damage. The planned manipulation sequences are generalized to new, unseen
scenes in the same application scenario using machine learning. This learned
manipulation strategy is continuously refined in a self-supervised,
simulation-in-the-loop optimization cycle during load-free times of the system,
commonly known as mental simulation. In parallel, the generated manipulation
strategies can be deployed in near-real time in an anytime fashion. The
approach is validated on an industrial container-unloading scenario and on a
retail shelf-replenishment scenario.",arxiv
http://arxiv.org/abs/1903.04958v1,2019-03-07T04:25:36Z,2019-03-07T04:25:36Z,Real-Time Boiler Control Optimization with Machine Learning,"In coal-fired power plants, it is critical to improve the operational
efficiency of boilers for sustainability. In this work, we formulate real-time
boiler control as an optimization problem that looks for the best distribution
of temperature in different zones and oxygen content from the flue to improve
the boiler's stability and energy efficiency. We employ an efficient algorithm
by integrating appropriate machine learning and optimization techniques. We
obtain a large dataset collected from a real boiler for more than two months
from our industry partner, and conduct extensive experiments to demonstrate the
effectiveness and efficiency of the proposed algorithm.",arxiv
http://arxiv.org/abs/2010.01169v2,2021-02-01T22:32:51Z,2020-10-02T19:06:29Z,"DocuBot : Generating financial reports using natural language
  interactions","The financial services industry perpetually processes an overwhelming amount
of complex data. Digital reports are often created based on tedious manual
analysis as well as visualization of the underlying trends and characteristics
of data. Often, the accruing costs of human computation errors in creating
these reports are very high. We present DocuBot, a novel AI-powered virtual
assistant for creating and modifying content in digital documents by modeling
natural language interactions as ""skills"" and using them to transform
underlying data. DocuBot has the ability to agglomerate saved skills for reuse,
enabling humans to automatically generate recurrent reports. DocuBot also has
the capability to continuously learn domain-specific and user-specific
vocabulary by interacting with the user. We present evidence that DocuBot adds
value to the financial industry and demonstrate its impact with experiments
involving real and simulated users tasked with creating PowerPoint
presentations.",arxiv
http://arxiv.org/abs/1705.03451v2,2017-05-26T13:17:17Z,2017-05-09T17:55:15Z,Proceedings of the Workshop on Data Mining for Oil and Gas,"The process of exploring and exploiting Oil and Gas (O&G) generates a lot of
data that can bring more efficiency to the industry. The opportunities for
using data mining techniques in the ""digital oil-field"" remain largely
unexplored or uncharted. With the high rate of data expansion, companies are
scrambling to develop ways to develop near-real-time predictive analytics, data
mining and machine learning capabilities, and are expanding their data storage
infrastructure and resources. With these new goals, come the challenges of
managing data growth, integrating intelligence tools, and analyzing the data to
glean useful insights. Oil and Gas companies need data solutions to
economically extract value from very large volumes of a wide variety of data
generated from exploration, well drilling and production devices and sensors.
  Data mining for oil and gas industry throughout the lifecycle of the
reservoir includes the following roles: locating hydrocarbons, managing
geological data, drilling and formation evaluation, well construction, well
completion, and optimizing production through the life of the oil field. For
each of these phases during the lifecycle of oil field, data mining play a
significant role. Based on which phase were talking about, knowledge creation
through scientific models, data analytics and machine learning, a effective,
productive, and on demand data insight is critical for decision making within
the organization.
  The significant challenges posed by this complex and economically vital field
justify a meeting of data scientists that are willing to share their experience
and knowledge. Thus, the Worskhop on Data Mining for Oil and Gas (DM4OG) aims
to provide a quality forum for researchers that work on the significant
challenges arising from the synergy between data science, machine learning, and
the modeling and optimization problems in the O&G industry.",arxiv
http://arxiv.org/abs/2008.06933v1,2020-08-16T15:10:39Z,2020-08-16T15:10:39Z,"The reinforcement learning-based multi-agent cooperative approach for
  the adaptive speed regulation on a metallurgical pickling line","We present a holistic data-driven approach to the problem of productivity
increase on the example of a metallurgical pickling line. The proposed approach
combines mathematical modeling as a base algorithm and a cooperative
Multi-Agent Reinforcement Learning (MARL) system implemented such as to enhance
the performance by multiple criteria while also meeting safety and reliability
requirements and taking into account the unexpected volatility of certain
technological processes. We demonstrate how Deep Q-Learning can be applied to a
real-life task in a heavy industry, resulting in significant improvement of
previously existing automation systems.The problem of input data scarcity is
solved by a two-step combination of LSTM and CGAN, which helps to embrace both
the tabular representation of the data and its sequential properties. Offline
RL training, a necessity in this setting, has become possible through the
sophisticated probabilistic kinematic environment.",arxiv
http://arxiv.org/abs/1805.03994v2,2018-05-29T16:28:18Z,2018-05-10T14:19:50Z,"Multi-View Semantic Labeling of 3D Point Clouds for Automated Plant
  Phenotyping","Semantic labeling of 3D point clouds is important for the derivation of 3D
models from real world scenarios in several economic fields such as building
industry, facility management, town planning or heritage conservation. In
contrast to these most common applications, we describe in this study the
semantic labeling of 3D point clouds derived from plant organs by
high-precision scanning. Our approach is optimized for the task of plant
phenotyping with its very specific challenges and is employing a deep learning
framework. Thereby, we report important experiences concerning detailed
parameter initialization and optimization techniques. By evaluating our
approach with challenging datasets we achieve state-of-the-art results without
difficult and time consuming feature engineering as being necessary in
traditional approaches to semantic labeling.",arxiv
http://arxiv.org/abs/2010.01388v1,2020-10-03T16:55:59Z,2020-10-03T16:55:59Z,Online Neural Networks for Change-Point Detection,"Moments when a time series changes its behaviour are called change points.
Detection of such points is a well-known problem, which can be found in many
applications: quality monitoring of industrial processes, failure detection in
complex systems, health monitoring, speech recognition and video analysis.
Occurrence of change point implies that the state of the system is altered and
its timely detection might help to prevent unwanted consequences. In this
paper, we present two online change-point detection approaches based on neural
networks. These algorithms demonstrate linear computational complexity and are
suitable for change-point detection in large time series. We compare them with
the best known algorithms on various synthetic and real world data sets.
Experiments show that the proposed methods outperform known approaches.",arxiv
http://arxiv.org/abs/2106.11593v1,2021-06-22T07:57:46Z,2021-06-22T07:57:46Z,A Vertical Federated Learning Framework for Graph Convolutional Network,"Recently, Graph Neural Network (GNN) has achieved remarkable success in
various real-world problems on graph data. However in most industries, data
exists in the form of isolated islands and the data privacy and security is
also an important issue. In this paper, we propose FedVGCN, a federated GCN
learning paradigm for privacy-preserving node classification task under data
vertically partitioned setting, which can be generalized to existing GCN
models. Specifically, we split the computation graph data into two parts. For
each iteration of the training process, the two parties transfer intermediate
results to each other under homomorphic encryption. We conduct experiments on
benchmark data and the results demonstrate the effectiveness of FedVGCN in the
case of GraphSage.",arxiv
http://arxiv.org/abs/2001.10249v1,2020-01-28T10:30:20Z,2020-01-28T10:30:20Z,"Online LiDAR-SLAM for Legged Robots with Robust Registration and
  Deep-Learned Loop Closure","In this paper, we present a factor-graph LiDAR-SLAM system which incorporates
a state-of-the-art deeply learned feature-based loop closure detector to enable
a legged robot to localize and map in industrial environments. These facilities
can be badly lit and comprised of indistinct metallic structures, thus our
system uses only LiDAR sensing and was developed to run on the quadruped
robot's navigation PC. Point clouds are accumulated using an inertial-kinematic
state estimator before being aligned using ICP registration. To close loops we
use a loop proposal mechanism which matches individual segments between clouds.
We trained a descriptor offline to match these segments. The efficiency of our
method comes from carefully designing the network architecture to minimize the
number of parameters such that this deep learning method can be deployed in
real-time using only the CPU of a legged robot, a major contribution of this
work. The set of odometry and loop closure factors are updated using pose graph
optimization. Finally we present an efficient risk alignment prediction method
which verifies the reliability of the registrations. Experimental results at an
industrial facility demonstrated the robustness and flexibility of our system,
including autonomous following paths derived from the SLAM map.",arxiv
http://arxiv.org/abs/1208.6310v1,2012-08-16T12:14:46Z,2012-08-16T12:14:46Z,"Automated Marble Plate Classification System Based On Different Neural
  Network Input Training Sets and PLC Implementation","The process of sorting marble plates according to their surface texture is an
important task in the automated marble plate production. Nowadays some
inspection systems in marble industry that automate the classification tasks
are too expensive and are compatible only with specific technological equipment
in the plant. In this paper a new approach to the design of an Automated Marble
Plate Classification System (AMPCS),based on different neural network input
training sets is proposed, aiming at high classification accuracy using simple
processing and application of only standard devices. It is based on training a
classification MLP neural network with three different input training sets:
extracted texture histograms, Discrete Cosine and Wavelet Transform over the
histograms. The algorithm is implemented in a PLC for real-time operation. The
performance of the system is assessed with each one of the input training sets.
The experimental test results regarding classification accuracy and quick
operation are represented and discussed.",arxiv
http://arxiv.org/abs/1905.02616v2,2019-06-16T23:00:46Z,2019-05-07T14:40:32Z,"An Integrated Multi-Time-Scale Modeling for Solar Irradiance Forecasting
  Using Deep Learning","For short-term solar irradiance forecasting, the traditional point
forecasting methods are rendered less useful due to the non-stationary
characteristic of solar power. The amount of operating reserves required to
maintain reliable operation of the electric grid rises due to the variability
of solar energy. The higher the uncertainty in the generation, the greater the
operating-reserve requirements, which translates to an increased cost of
operation. In this research work, we propose a unified architecture for
multi-time-scale predictions for intra-day solar irradiance forecasting using
recurrent neural networks (RNN) and long-short-term memory networks (LSTMs).
This paper also lays out a framework for extending this modeling approach to
intra-hour forecasting horizons thus, making it a multi-time-horizon
forecasting approach, capable of predicting intra-hour as well as intra-day
solar irradiance. We develop an end-to-end pipeline to effectuate the proposed
architecture. The performance of the prediction model is tested and validated
by the methodical implementation. The robustness of the approach is
demonstrated with case studies conducted for geographically scattered sites
across the United States. The predictions demonstrate that our proposed unified
architecture-based approach is effective for multi-time-scale solar forecasts
and achieves a lower root-mean-square prediction error when benchmarked against
the best-performing methods documented in the literature that use separate
models for each time-scale during the day. Our proposed method results in a
71.5% reduction in the mean RMSE averaged across all the test sites compared to
the ML-based best-performing method reported in the literature. Additionally,
the proposed method enables multi-time-horizon forecasts with real-time inputs,
which have a significant potential for practical industry applications in the
evolving grid.",arxiv
http://arxiv.org/abs/1910.03060v1,2019-10-07T20:06:38Z,2019-10-07T20:06:38Z,Impact of Inference Accelerators on hardware selection,"As opportunities for AI-assisted healthcare grow steadily, model deployment
faces challenges due to the specific characteristics of the industry. The
configuration choice for a production device can impact model performance while
influencing operational costs. Moreover, in healthcare some situations might
require fast, but not real time, inference. We study different configurations
and conduct a cost-performance analysis to determine the optimized hardware for
the deployment of a model subject to healthcare domain constraints. We observe
that a naive performance comparison may not lead to an optimal configuration
selection. In fact, given realistic domain constraints, CPU execution might be
preferable to GPU accelerators. Hence, defining beforehand precise expectations
for model deployment is crucial.",arxiv
http://arxiv.org/abs/2009.13437v2,2021-08-17T10:53:45Z,2020-09-28T16:04:07Z,"A Human-in-the-Loop Approach based on Explainability to Improve NTL
  Detection","Implementing systems based on Machine Learning to detect fraud and other
Non-Technical Losses (NTL) is challenging: the data available is biased, and
the algorithms currently used are black-boxes that cannot be either easily
trusted or understood by stakeholders. This work explains our human-in-the-loop
approach to mitigate these problems in a real system that uses a supervised
model to detect Non-Technical Losses (NTL) for an international utility company
from Spain. This approach exploits human knowledge (e.g. from the data
scientists or the company's stakeholders) and the information provided by
explanatory methods to guide the system during the training process. This
simple, efficient method that can be easily implemented in other industrial
projects is tested in a real dataset and the results show that the derived
prediction model is better in terms of accuracy, interpretability, robustness
and flexibility.",arxiv
http://arxiv.org/abs/2103.04263v2,2021-03-11T01:08:38Z,2021-03-07T04:40:15Z,Deepfake Videos in the Wild: Analysis and Detection,"AI-manipulated videos, commonly known as deepfakes, are an emerging problem.
Recently, researchers in academia and industry have contributed several
(self-created) benchmark deepfake datasets, and deepfake detection algorithms.
However, little effort has gone towards understanding deepfake videos in the
wild, leading to a limited understanding of the real-world applicability of
research contributions in this space. Even if detection schemes are shown to
perform well on existing datasets, it is unclear how well the methods
generalize to real-world deepfakes. To bridge this gap in knowledge, we make
the following contributions: First, we collect and present the largest dataset
of deepfake videos in the wild, containing 1,869 videos from YouTube and
Bilibili, and extract over 4.8M frames of content. Second, we present a
comprehensive analysis of the growth patterns, popularity, creators,
manipulation strategies, and production methods of deepfake content in the
real-world. Third, we systematically evaluate existing defenses using our new
dataset, and observe that they are not ready for deployment in the real-world.
Fourth, we explore the potential for transfer learning schemes and
competition-winning techniques to improve defenses.",arxiv
http://arxiv.org/abs/2101.03742v1,2021-01-11T08:03:57Z,2021-01-11T08:03:57Z,"Hierarchical Clustering using Auto-encoded Compact Representation for
  Time-series Analysis","Getting a robust time-series clustering with best choice of distance measure
and appropriate representation is always a challenge. We propose a novel
mechanism to identify the clusters combining learned compact representation of
time-series, Auto Encoded Compact Sequence (AECS) and hierarchical clustering
approach. Proposed algorithm aims to address the large computing time issue of
hierarchical clustering as learned latent representation AECS has a length much
less than the original length of time-series and at the same time want to
enhance its performance.Our algorithm exploits Recurrent Neural Network (RNN)
based under complete Sequence to Sequence(seq2seq) autoencoder and
agglomerative hierarchical clustering with a choice of best distance measure to
recommend the best clustering. Our scheme selects the best distance measure and
corresponding clustering for both univariate and multivariate time-series. We
have experimented with real-world time-series from UCR and UCI archive taken
from diverse application domains like health, smart-city, manufacturing etc.
Experimental results show that proposed method not only produce close to
benchmark results but also in some cases outperform the benchmark.",arxiv
http://arxiv.org/abs/2008.13419v1,2020-08-31T08:24:06Z,2020-08-31T08:24:06Z,"Integrative Object and Pose to Task Detection for an
  Augmented-Reality-based Human Assistance System using Neural Networks","As a result of an increasingly automatized and digitized industry, processes
are becoming more complex. Augmented Reality has shown considerable potential
in assisting workers with complex tasks by enhancing user understanding and
experience with spatial information. However, the acceptance and integration of
AR into industrial processes is still limited due to the lack of established
methods and tedious integration efforts. Meanwhile, deep neural networks have
achieved remarkable results in computer vision tasks and bear great prospects
to enrich Augmented Reality applications . In this paper, we propose an
Augmented-Reality-based human assistance system to assist workers in complex
manual tasks where we incorporate deep neural networks for computer vision
tasks. More specifically, we combine Augmented Reality with object and action
detectors to make workflows more intuitive and flexible. To evaluate our system
in terms of user acceptance and efficiency, we conducted several user studies.
We found a significant reduction in time to task completion in untrained
workers and a decrease in error rate. Furthermore, we investigated the users
learning curve with our assistance system.",arxiv
http://arxiv.org/abs/2003.05198v2,2020-03-12T05:42:35Z,2020-03-11T10:15:37Z,Industrial Scale Privacy Preserving Deep Neural Network,"Deep Neural Network (DNN) has been showing great potential in kinds of
real-world applications such as fraud detection and distress prediction.
Meanwhile, data isolation has become a serious problem currently, i.e.,
different parties cannot share data with each other. To solve this issue, most
research leverages cryptographic techniques to train secure DNN models for
multi-parties without compromising their private data. Although such methods
have strong security guarantee, they are difficult to scale to deep networks
and large datasets due to its high communication and computation complexities.
To solve the scalability of the existing secure Deep Neural Network (DNN) in
data isolation scenarios, in this paper, we propose an industrial scale privacy
preserving neural network learning paradigm, which is secure against
semi-honest adversaries. Our main idea is to split the computation graph of DNN
into two parts, i.e., the computations related to private data are performed by
each party using cryptographic techniques, and the rest computations are done
by a neutral server with high computation ability. We also present a defender
mechanism for further privacy protection. We conduct experiments on real-world
fraud detection dataset and financial distress prediction dataset, the
encouraging results demonstrate the practicalness of our proposal.",arxiv
http://arxiv.org/abs/2106.13802v1,2021-06-25T17:57:04Z,2021-06-25T17:57:04Z,"Efficient Document Image Classification Using Region-Based Graph Neural
  Network","Document image classification remains a popular research area because it can
be commercialized in many enterprise applications across different industries.
Recent advancements in large pre-trained computer vision and language models
and graph neural networks has lent document image classification many tools.
However using large pre-trained models usually requires substantial computing
resources which could defeat the cost-saving advantages of automatic document
image classification. In the paper we propose an efficient document image
classification framework that uses graph convolution neural networks and
incorporates textual, visual and layout information of the document. We have
rigorously benchmarked our proposed algorithm against several state-of-art
vision and language models on both publicly available dataset and a real-life
insurance document classification dataset. Empirical results on both publicly
available and real-world data show that our methods achieve near SOTA
performance yet require much less computing resources and time for model
training and inference. This results in solutions than offer better cost
advantages, especially in scalable deployment for enterprise applications. The
results showed that our algorithm can achieve classification performance quite
close to SOTA. We also provide comprehensive comparisons of computing
resources, model sizes, train and inference time between our proposed methods
and baselines. In addition we delineate the cost per image using our method and
other baselines.",arxiv
http://arxiv.org/abs/2001.11610v1,2020-01-30T23:49:15Z,2020-01-30T23:49:15Z,"UAV Autonomous Localization using Macro-Features Matching with a CAD
  Model","Research in the field of autonomous Unmanned Aerial Vehicles (UAVs) has
significantly advanced in recent years, mainly due to their relevance in a
large variety of commercial, industrial, and military applications. However,
UAV navigation in GPS-denied environments continues to be a challenging problem
that has been tackled in recent research through sensor-based approaches. This
paper presents a novel offline, portable, real-time in-door UAV localization
technique that relies on macro-feature detection and matching. The proposed
system leverages the support of machine learning, traditional computer vision
techniques, and pre-existing knowledge of the environment. The main
contribution of this work is the real-time creation of a macro-feature
description vector from the UAV captured images which are simultaneously
matched with an offline pre-existing vector from a Computer-Aided Design (CAD)
model. This results in a quick UAV localization within the CAD model. The
effectiveness and accuracy of the proposed system were evaluated through
simulations and experimental prototype implementation. Final results reveal the
algorithm's low computational burden as well as its ease of deployment in
GPS-denied environments.",arxiv
http://arxiv.org/abs/1904.09035v2,2019-04-22T04:07:17Z,2019-03-21T02:55:14Z,"Evolving Deep Neural Networks by Multi-objective Particle Swarm
  Optimization for Image Classification","In recent years, convolutional neural networks (CNNs) have become deeper in
order to achieve better classification accuracy in image classification.
However, it is difficult to deploy the state-of-the-art deep CNNs for
industrial use due to the difficulty of manually fine-tuning the
hyperparameters and the trade-off between classification accuracy and
computational cost. This paper proposes a novel multi-objective optimization
method for evolving state-of-the-art deep CNNs in real-life applications, which
automatically evolves the non-dominant solutions at the Pareto front. Three
major contributions are made: Firstly, a new encoding strategy is designed to
encode one of the best state-of-the-art CNNs; With the classification accuracy
and the number of floating point operations as the two objectives, a
multi-objective particle swarm optimization method is developed to evolve the
non-dominant solutions; Last but not least, a new infrastructure is designed to
boost the experiments by concurrently running the experiments on multiple GPUs
across multiple machines, and a Python library is developed and released to
manage the infrastructure. The experimental results demonstrate that the
non-dominant solutions found by the proposed algorithm form a clear Pareto
front, and the proposed infrastructure is able to almost linearly reduce the
running time.",arxiv
http://arxiv.org/abs/1801.05643v1,2018-01-17T12:51:01Z,2018-01-17T12:51:01Z,"The Case for Automatic Database Administration using Deep Reinforcement
  Learning","Like any large software system, a full-fledged DBMS offers an overwhelming
amount of configuration knobs. These range from static initialisation
parameters like buffer sizes, degree of concurrency, or level of replication to
complex runtime decisions like creating a secondary index on a particular
column or reorganising the physical layout of the store. To simplify the
configuration, industry grade DBMSs are usually shipped with various advisory
tools, that provide recommendations for given workloads and machines. However,
reality shows that the actual configuration, tuning, and maintenance is usually
still done by a human administrator, relying on intuition and experience.
Recent work on deep reinforcement learning has shown very promising results in
solving problems, that require such a sense of intuition. For instance, it has
been applied very successfully in learning how to play complicated games with
enormous search spaces. Motivated by these achievements, in this work we
explore how deep reinforcement learning can be used to administer a DBMS.
First, we will describe how deep reinforcement learning can be used to
automatically tune an arbitrary software system like a DBMS by defining a
problem environment. Second, we showcase our concept of NoDBA at the concrete
example of index selection and evaluate how well it recommends indexes for
given workloads.",arxiv
http://arxiv.org/abs/1812.01029v1,2018-12-03T19:05:25Z,2018-12-03T19:05:25Z,Sensitivity based Neural Networks Explanations,"Although neural networks can achieve very high predictive performance on
various different tasks such as image recognition or natural language
processing, they are often considered as opaque ""black boxes"". The difficulty
of interpreting the predictions of a neural network often prevents its use in
fields where explainability is important, such as the financial industry where
regulators and auditors often insist on this aspect. In this paper, we present
a way to assess the relative input features importance of a neural network
based on the sensitivity of the model output with respect to its input. This
method has the advantage of being fast to compute, it can provide both global
and local levels of explanations and is applicable for many types of neural
network architectures. We illustrate the performance of this method on both
synthetic and real data and compare it with other interpretation techniques.
This method is implemented into an open-source Python package that allows its
users to easily generate and visualize explanations for their neural networks.",arxiv
http://arxiv.org/abs/1809.03149v2,2019-09-03T01:55:56Z,2018-09-10T06:15:42Z,Learning Adaptive Display Exposure for Real-Time Advertising,"In E-commerce advertising, where product recommendations and product ads are
presented to users simultaneously, the traditional setting is to display ads at
fixed positions. However, under such a setting, the advertising system loses
the flexibility to control the number and positions of ads, resulting in
sub-optimal platform revenue and user experience. Consequently, major
e-commerce platforms (e.g., Taobao.com) have begun to consider more flexible
ways to display ads. In this paper, we investigate the problem of advertising
with adaptive exposure: can we dynamically determine the number and positions
of ads for each user visit under certain business constraints so that the
platform revenue can be increased? More specifically, we consider two types of
constraints: request-level constraint ensures user experience for each user
visit, and platform-level constraint controls the overall platform monetization
rate. We model this problem as a Constrained Markov Decision Process with
per-state constraint (psCMDP) and propose a constrained two-level reinforcement
learning approach to decompose the original problem into two relatively
independent sub-problems. To accelerate policy learning, we also devise a
constrained hindsight experience replay mechanism. Experimental evaluations on
industry-scale real-world datasets demonstrate the merits of our approach in
both obtaining higher revenue under the constraints and the effectiveness of
the constrained hindsight experience replay mechanism.",arxiv
http://arxiv.org/abs/1812.08273v1,2018-12-19T22:25:52Z,2018-12-19T22:25:52Z,Analog Signal Processing Using Stochastic Magnets,"We present a low barrier magnet based compact hardware unit for analog
stochastic neurons and demonstrate its use as a building-block for neuromorphic
hardware. By coupling circular magnetic tunnel junctions (MTJs) with a CMOS
based analog buffer, we show that these units can act as leaky-integrate-and
fire (LIF) neurons, a model of biological neural networks particularly suited
for temporal inferencing and pattern recognition. We demonstrate examples of
temporal sequence learning, processing, and prediction tasks in real time, as a
proof of concept demonstration of scalable and adaptive signal-processors.
Efficient non von-Neumann hardware implementation of such processors can open
up a pathway for integration of hardware based cognition in a wide variety of
emerging systems such as IoT, industrial controls, bio- and photo-sensors, and
Unmanned Autonomous Vehicles.",arxiv
http://arxiv.org/abs/2109.09530v1,2021-09-20T13:30:11Z,2021-09-20T13:30:11Z,A Novel Online Incremental Learning Intrusion Prevention System,"Attack vectors are continuously evolving in order to evade Intrusion
Detection systems. Internet of Things (IoT) environments, while beneficial for
the IT ecosystem, suffer from inherent hardware limitations, which restrict
their ability to implement comprehensive security measures and increase their
exposure to vulnerability attacks. This paper proposes a novel Network
Intrusion Prevention System that utilises a SelfOrganizing Incremental Neural
Network along with a Support Vector Machine. Due to its structure, the proposed
system provides a security solution that does not rely on signatures or rules
and is capable to mitigate known and unknown attacks in real-time with high
accuracy. Based on our experimental results with the NSL KDD dataset, the
proposed framework can achieve on-line updated incremental learning, making it
suitable for efficient and scalable industrial applications.",arxiv
http://arxiv.org/abs/2106.03374v2,2021-10-08T05:35:19Z,2021-06-07T07:01:39Z,"MixRL: Data Mixing Augmentation for Regression using Reinforcement
  Learning","Data augmentation is becoming essential for improving regression accuracy in
critical applications including manufacturing and finance. Existing techniques
for data augmentation largely focus on classification tasks and do not readily
apply to regression tasks. In particular, the recent Mixup techniques for
classification rely on the key assumption that linearity holds among training
examples, which is reasonable if the label space is discrete, but has
limitations when the label space is continuous as in regression. We show that
mixing examples that either have a large data or label distance may have an
increasingly-negative effect on model performance. Hence, we use the stricter
assumption that linearity only holds within certain data or label distances for
regression where the degree may vary by each example. We then propose MixRL, a
data augmentation meta learning framework for regression that learns for each
example how many nearest neighbors it should be mixed with for the best model
performance using a small validation set. MixRL achieves these objectives using
Monte Carlo policy gradient reinforcement learning. Our experiments conducted
both on synthetic and real datasets show that MixRL significantly outperforms
state-of-the-art data augmentation baselines. MixRL can also be integrated with
other classification Mixup techniques for better results.",arxiv
http://arxiv.org/abs/1808.10134v1,2018-08-30T06:29:10Z,2018-08-30T06:29:10Z,"Baidu Apollo Auto-Calibration System - An Industry-Level Data-Driven and
  Learning based Vehicle Longitude Dynamic Calibrating Algorithm","For any autonomous driving vehicle, control module determines its road
performance and safety, i.e. its precision and stability should stay within a
carefully-designed range. Nonetheless, control algorithms require vehicle
dynamics (such as longitudinal dynamics) as inputs, which, unfortunately, are
obscure to calibrate in real time. As a result, to achieve reasonable
performance, most, if not all, research-oriented autonomous vehicles do manual
calibrations in a one-by-one fashion. Since manual calibration is not
sustainable once entering into mass production stage for industrial purposes,
we here introduce a machine-learning based auto-calibration system for
autonomous driving vehicles. In this paper, we will show how we build a
data-driven longitudinal calibration procedure using machine learning
techniques. We first generated offline calibration tables from human driving
data. The offline table serves as an initial guess for later uses and it only
needs twenty-minutes data collection and process. We then used an
online-learning algorithm to appropriately update the initial table (the
offline table) based on real-time performance analysis. This longitudinal
auto-calibration system has been deployed to more than one hundred Baidu Apollo
self-driving vehicles (including hybrid family vehicles and electronic
delivery-only vehicles) since April 2018. By August 27, 2018, it had been
tested for more than two thousands hours, ten thousands kilometers (6,213
miles) and yet proven to be effective.",arxiv
http://arxiv.org/abs/1806.09057v1,2018-06-24T00:28:12Z,2018-06-24T00:28:12Z,In-situ Stochastic Training of MTJ Crossbar based Neural Networks,"Owing to high device density, scalability and non-volatility, Magnetic Tunnel
Junction-based crossbars have garnered significant interest for implementing
the weights of an artificial neural network. The existence of only two stable
states in MTJs implies a high overhead of obtaining optimal binary weights in
software. We illustrate that the inherent parallelism in the crossbar structure
makes it highly appropriate for in-situ training, wherein the network is taught
directly on the hardware. It leads to significantly smaller training overhead
as the training time is independent of the size of the network, while also
circumventing the effects of alternate current paths in the crossbar and
accounting for manufacturing variations in the device. We show how the
stochastic switching characteristics of MTJs can be leveraged to perform
probabilistic weight updates using the gradient descent algorithm. We describe
how the update operations can be performed on crossbars both with and without
access transistors and perform simulations on them to demonstrate the
effectiveness of our techniques. The results reveal that stochastically trained
MTJ-crossbar NNs achieve a classification accuracy nearly same as that of
real-valued-weight networks trained in software and exhibit immunity to device
variations.",arxiv
http://arxiv.org/abs/2105.11056v1,2021-05-24T01:41:12Z,2021-05-24T01:41:12Z,"User-oriented Natural Human-Robot Control with Thin-Plate Splines and
  LRCN","We propose a real-time vision-based teleoperation approach for robotic arms
that employs a single depth-based camera, exempting the user from the need for
any wearable devices. By employing a natural user interface, this novel
approach leverages the conventional fine-tuning control, turning it into a
direct body pose capture process. The proposed approach is comprised of two
main parts. The first is a nonlinear customizable pose mapping based on
Thin-Plate Splines (TPS), to directly transfer human body motion to robotic arm
motion in a nonlinear fashion, thus allowing matching dissimilar bodies with
different workspace shapes and kinematic constraints. The second is a Deep
Neural Network hand-state classifier based on Long-term Recurrent Convolutional
Networks (LRCN) that exploits the temporal coherence of the acquired depth
data. We validate, evaluate and compare our approach through both classical
cross-validation experiments of the proposed hand state classifier; and user
studies over a set of practical experiments involving variants of
pick-and-place and manufacturing tasks. Results revealed that LRCN networks
outperform single image Convolutional Neural Networks; and that users' learning
curves were steep, thus allowing the successful completion of the proposed
tasks. When compared to a previous approach, the TPS approach revealed no
increase in task complexity and similar times of completion, while providing
more precise operation in regions closer to workspace boundaries.",arxiv
http://arxiv.org/abs/1802.09788v1,2018-02-27T09:19:23Z,2018-02-27T09:19:23Z,Time-sensitive Customer Churn Prediction based on PU Learning,"With the fast development of Internet companies throughout the world,
customer churn has become a serious concern. To better help the companies
retain their customers, it is important to build a customer churn prediction
model to identify the customers who are most likely to churn ahead of time. In
this paper, we propose a Time-sensitive Customer Churn Prediction (TCCP)
framework based on Positive and Unlabeled (PU) learning technique.
Specifically, we obtain the recent data by shortening the observation period,
and start to train model as long as enough positive samples are collected,
ignoring the absence of the negative examples. We conduct thoroughly
experiments on real industry data from Alipay.com. The experimental results
demonstrate that TCCP outperforms the rule-based models and the traditional
supervised learning models.",arxiv
http://arxiv.org/abs/1710.01476v2,2018-01-28T23:33:00Z,2017-10-04T06:35:02Z,A Comparative Taxonomy and Survey of Public Cloud Infrastructure Vendors,"An increasing number of technology enterprises are adopting cloud-native
architectures to offer their web-based products, by moving away from
privately-owned data-centers and relying exclusively on cloud service
providers. As a result, cloud vendors have lately increased, along with the
estimated annual revenue they share. However, in the process of selecting a
provider's cloud service over the competition, we observe a lack of universal
common ground in terms of terminology, functionality of services and billing
models. This is an important gap especially under the new reality of the
industry where each cloud provider has moved towards his own service taxonomy,
while the number of specialized services has grown exponentially. This work
discusses cloud services offered by four dominant, in terms of their current
market share, cloud vendors. We provide a taxonomy of their services and
sub-services that designates major service families namely computing, storage,
databases, analytics, data pipelines, machine learning, and networking. The aim
of such clustering is to indicate similarities, common design approaches and
functional differences of the offered services. The outcomes are essential both
for individual researchers, and bigger enterprises in their attempt to identify
the set of cloud services that will utterly meet their needs without
compromises. While we acknowledge the fact that this is a dynamic industry,
where new services arise constantly, and old ones experience important updates,
this study paints a solid image of the current offerings and gives prominence
to the directions that cloud service providers are following.",arxiv
http://arxiv.org/abs/2102.12967v2,2021-11-11T08:44:36Z,2021-02-25T16:14:47Z,"A statistical framework for efficient out of distribution detection in
  deep neural networks","Background. Commonly, Deep Neural Networks (DNNs) generalize well on samples
drawn from a distribution similar to that of the training set. However, DNNs'
predictions are brittle and unreliable when the test samples are drawn from a
dissimilar distribution. This is a major concern for deployment in real-world
applications, where such behavior may come at a considerable cost, such as
industrial production lines, autonomous vehicles, or healthcare applications.
Contributions. We frame Out Of Distribution (OOD) detection in DNNs as a
statistical hypothesis testing problem. Tests generated within our proposed
framework combine evidence from the entire network. Unlike previous OOD
detection heuristics, this framework returns a $p$-value for each test sample.
It is guaranteed to maintain the Type I Error (T1E - mistakenly identifying OOD
samples as ID) for test data. Moreover, this allows combining several detectors
while maintaining the T1E. Building on this framework, we suggest a novel OOD
procedure based on low-order statistics. Our method achieves comparable or
better results than state-of-the-art methods on well-accepted OOD benchmarks,
without retraining the network parameters or assuming prior knowledge on the
test distribution -- and at a fraction of the computational cost.",arxiv
http://arxiv.org/abs/1807.00139v1,2018-06-30T08:31:54Z,2018-06-30T08:31:54Z,Harnessing constrained resources in service industry via video analytics,"Service industries contribute significantly to many developed and developing
- economies. As their business activities expand rapidly, many service
companies struggle to maintain customer's satisfaction due to sluggish service
response caused by resource shortages. Anticipating resource shortages and
proffering solutions before they happen is an effective way of reducing the
adverse effect on operations. However, this proactive approach is very
expensive in terms of capacity and labor costs. Many companies fall into
productivity conundrum as they fail to find sufficient strong arguments to
justify the cost of a new technology yet cannot afford not to invest in new
technologies to match up with competitors. The question is whether there is an
innovative solution to maximally utilize available resources and drastically
reduce the effect that the shortages of resources may cause yet achieving high
level of service quality at a low cost. This work demonstrates with a practical
analysis of a trolley tracking system we designed and deployed at Hong Kong
International Airport (HKIA) on how video analytics helps achieve management's
goal of satisfying customer's needs via real-time detection and prevention of
problems they may encounter during the service consumption process using
existing video technology rather than adopting new technologies. This paper
presents the integration of commercial video surveillance system with deep
learning algorithms for video analytics. We show that our system can provide
accurate decision when faced with total or partial occlusion with high accuracy
and it significantly improves daily operation. It is envisioned that this work
will heighten the appreciation of integrative technologies for resource
management within the service industries and as a measure for real-time
customer assistance.",arxiv
http://arxiv.org/abs/2101.06175v1,2021-01-15T15:36:22Z,2021-01-15T15:36:22Z,PaddleSeg: A High-Efficient Development Toolkit for Image Segmentation,"Image Segmentation plays an essential role in computer vision and image
processing with various applications from medical diagnosis to autonomous car
driving. A lot of segmentation algorithms have been proposed for addressing
specific problems. In recent years, the success of deep learning techniques has
tremendously influenced a wide range of computer vision areas, and the modern
approaches of image segmentation based on deep learning are becoming prevalent.
In this article, we introduce a high-efficient development toolkit for image
segmentation, named PaddleSeg. The toolkit aims to help both developers and
researchers in the whole process of designing segmentation models, training
models, optimizing performance and inference speed, and deploying models.
Currently, PaddleSeg supports around 20 popular segmentation models and more
than 50 pre-trained models from real-time and high-accuracy levels. With
modular components and backbone networks, users can easily build over one
hundred models for different requirements. Furthermore, we provide
comprehensive benchmarks and evaluations to show that these segmentation
algorithms trained on our toolkit have more competitive accuracy. Also, we
provide various real industrial applications and practical cases based on
PaddleSeg. All codes and examples of PaddleSeg are available at
https://github.com/PaddlePaddle/PaddleSeg.",arxiv
http://arxiv.org/abs/2106.01674v1,2021-06-03T08:23:24Z,2021-06-03T08:23:24Z,"JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale
  Online Inference at Baidu","In modern internet industries, deep learning based recommender systems have
became an indispensable building block for a wide spectrum of applications,
such as search engine, news feed, and short video clips. However, it remains
challenging to carry the well-trained deep models for online real-time
inference serving, with respect to the time-varying web-scale traffics from
billions of users, in a cost-effective manner. In this work, we present JIZHI -
a Model-as-a-Service system - that per second handles hundreds of millions of
online inference requests to huge deep models with more than trillions of
sparse parameters, for over twenty real-time recommendation services at Baidu,
Inc. In JIZHI, the inference workflow of every recommendation request is
transformed to a Staged Event-Driven Pipeline (SEDP), where each node in the
pipeline refers to a staged computation or I/O intensive task processor. With
traffics of real-time inference requests arrived, each modularized processor
can be run in a fully asynchronized way and managed separately. Besides, JIZHI
introduces heterogeneous and hierarchical storage to further accelerate the
online inference process by reducing unnecessary computations and potential
data access latency induced by ultra-sparse model parameters. Moreover, an
intelligent resource manager has been deployed to maximize the throughput of
JIZHI over the shared infrastructure by searching the optimal resource
allocation plan from historical logs and fine-tuning the load shedding policies
over intermediate system feedback. Extensive experiments have been done to
demonstrate the advantages of JIZHI from the perspectives of end-to-end service
latency, system-wide throughput, and resource consumption. JIZHI has helped
Baidu saved more than ten million US dollars in hardware and utility costs
while handling 200% more traffics without sacrificing inference efficiency.",arxiv
http://arxiv.org/abs/1909.06727v1,2019-09-15T04:01:39Z,2019-09-15T04:01:39Z,"An Empirical Study towards Characterizing Deep Learning Development and
  Deployment across Different Frameworks and Platforms","Deep Learning (DL) has recently achieved tremendous success. A variety of DL
frameworks and platforms play a key role to catalyze such progress. However,
the differences in architecture designs and implementations of existing
frameworks and platforms bring new challenges for DL software development and
deployment. Till now, there is no study on how various mainstream frameworks
and platforms influence both DL software development and deployment in
practice. To fill this gap, we take the first step towards understanding how
the most widely-used DL frameworks and platforms support the DL software
development and deployment. We conduct a systematic study on these frameworks
and platforms by using two types of DNN architectures and three popular
datasets. (1) For development process, we investigate the prediction accuracy
under the same runtime training configuration or same model weights/biases. We
also study the adversarial robustness of trained models by leveraging the
existing adversarial attack techniques. The experimental results show that the
computing differences across frameworks could result in an obvious prediction
accuracy decline, which should draw the attention of DL developers. (2) For
deployment process, we investigate the prediction accuracy and performance
(refers to time cost and memory consumption) when the trained models are
migrated/quantized from PC to real mobile devices and web browsers. The DL
platform study unveils that the migration and quantization still suffer from
compatibility and reliability issues. Meanwhile, we find several DL software
bugs by using the results as a benchmark. We further validate the results
through bug confirmation from stakeholders and industrial positive feedback to
highlight the implications of our study. Through our study, we summarize
practical guidelines, identify challenges and pinpoint new research directions.",arxiv
http://arxiv.org/abs/2101.05325v2,2021-06-18T16:33:59Z,2021-01-13T20:00:44Z,"Learning Kinematic Feasibility for Mobile Manipulation through Deep
  Reinforcement Learning","Mobile manipulation tasks remain one of the critical challenges for the
widespread adoption of autonomous robots in both service and industrial
scenarios. While planning approaches are good at generating feasible whole-body
robot trajectories, they struggle with dynamic environments as well as the
incorporation of constraints given by the task and the environment. On the
other hand, dynamic motion models in the action space struggle with generating
kinematically feasible trajectories for mobile manipulation actions. We propose
a deep reinforcement learning approach to learn feasible dynamic motions for a
mobile base while the end-effector follows a trajectory in task space generated
by an arbitrary system to fulfill the task at hand. This modular formulation
has several benefits: it enables us to readily transform a broad range of
end-effector motions into mobile applications, it allows us to use the
kinematic feasibility of the end-effector trajectory as a dense reward signal
and its modular formulation allows it to generalise to unseen end-effector
motions at test time. We demonstrate the capabilities of our approach on
multiple mobile robot platforms with different kinematic abilities and
different types of wheeled platforms in extensive simulated as well as
real-world experiments.",arxiv
http://arxiv.org/abs/1811.07674v1,2018-11-19T13:33:07Z,2018-11-19T13:33:07Z,"An Adaptive Oversampling Learning Method for Class-Imbalanced Fault
  Diagnostics and Prognostics","Data-driven fault diagnostics and prognostics suffers from class-imbalance
problem in industrial systems and it raises challenges to common machine
learning algorithms as it becomes difficult to learn the features of the
minority class samples. Synthetic oversampling methods are commonly used to
tackle these problems by generating the minority class samples to balance the
distributions between majority and minority classes. However, many of
oversampling methods are inappropriate that they cannot generate effective and
useful minority class samples according to different distributions of data,
which further complicate the process of learning samples. Thus, this paper
proposes a novel adaptive oversampling technique: EM-based Weighted Minority
Oversampling TEchnique (EWMOTE) for industrial fault diagnostics and
prognostics. The methods comprises a weighted minority sampling strategy to
identify hard-to-learn informative minority fault samples and Expectation
Maximization (EM) based imputation algorithm to generate fault samples. To
validate the performance of the proposed methods, experiments are conducted in
two real datasets. The results show that the method could achieve better
performance on not only binary class, but multi-class imbalance learning task
in different imbalance ratios than other oversampling-based baseline models.",arxiv
http://arxiv.org/abs/2007.05303v1,2020-07-10T11:07:32Z,2020-07-10T11:07:32Z,Multi-future Merchant Transaction Prediction,"The multivariate time series generated from merchant transaction history can
provide critical insights for payment processing companies. The capability of
predicting merchants' future is crucial for fraud detection and recommendation
systems. Conventionally, this problem is formulated to predict one multivariate
time series under the multi-horizon setting. However, real-world applications
often require more than one future trend prediction considering the
uncertainties, where more than one multivariate time series needs to be
predicted. This problem is called multi-future prediction. In this work, we
combine the two research directions and propose to study this new problem:
multi-future, multi-horizon and multivariate time series prediction. This
problem is crucial as it has broad use cases in the financial industry to
reduce the risk while improving user experience by providing alternative
futures. This problem is also challenging as now we not only need to capture
the patterns and insights from the past but also train a model that has a
strong inference capability to project multiple possible outcomes. To solve
this problem, we propose a new model using convolutional neural networks and a
simple yet effective encoder-decoder structure to learn the time series pattern
from multiple perspectives. We use experiments on real-world merchant
transaction data to demonstrate the effectiveness of our proposed model. We
also provide extensive discussions on different model design choices in our
experimental section.",arxiv
http://arxiv.org/abs/2101.01652v1,2021-01-05T17:06:03Z,2021-01-05T17:06:03Z,"Interpersonal distance in VR: reactions of older adults to the presence
  of a virtual agent","The rapid development of virtual reality technology has increased its
availability and, consequently, increased the number of its possible
applications. The interest in the new medium has grown due to the entertainment
industry (games, VR experiences and movies). The number of freely available
training and therapeutic applications is also increasing. Contrary to popular
opinion, new technologies are also adopted by older adults. Creating virtual
environments tailored to the needs and capabilities of older adults requires
intense research on the behaviour of these participants in the most common
situations, towards commonly used elements of the virtual environment, in
typical sceneries. Comfortable immersion in a virtual environment is key to
achieving the impression of presence. Presence is, in turn, necessary to obtain
appropriate training, persuasive and therapeutic effects. A virtual agent (a
humanoid representation of an algorithm or artificial intelligence) is often an
element of the virtual environment interface. Maintaining an appropriate
distance to the agent is, therefore, a key parameter for the creator of the VR
experience. Older (65+) participants maintain greater distance towards an agent
(a young white male) than younger ones (25-35). It may be caused by differences
in the level of arousal, but also cultural norms. As a consequence, VR
developers are advised to use algorithms that maintain the agent at the
appropriate distance, depending on the user's age.",arxiv
http://arxiv.org/abs/1711.05098v1,2017-11-14T14:20:56Z,2017-11-14T14:20:56Z,Web Robot Detection in Academic Publishing,"Recent industry reports assure the rise of web robots which comprise more
than half of the total web traffic. They not only threaten the security,
privacy and efficiency of the web but they also distort analytics and metrics,
doubting the veracity of the information being promoted. In the academic
publishing domain, this can cause articles to be faulty presented as prominent
and influential. In this paper, we present our approach on detecting web robots
in academic publishing websites. We use different supervised learning
algorithms with a variety of characteristics deriving from both the log files
of the server and the content served by the website. Our approach relies on the
assumption that human users will be interested in specific domains or articles,
while web robots crawl a web library incoherently. We experiment with features
adopted in previous studies with the addition of novel semantic characteristics
which derive after performing a semantic analysis using the Latent Dirichlet
Allocation (LDA) algorithm. Our real-world case study shows promising results,
pinpointing the significance of semantic features in the web robot detection
problem.",arxiv
http://arxiv.org/abs/2012.14770v1,2020-12-29T14:33:01Z,2020-12-29T14:33:01Z,Hybrid Interest Modeling for Long-tailed Users,"User behavior modeling is a key technique for recommender systems. However,
most methods focus on head users with large-scale interactions and hence suffer
from data sparsity issues. Several solutions integrate side information such as
demographic features and product reviews, another is to transfer knowledge from
other rich data sources. We argue that current methods are limited by the
strict privacy policy and have low scalability in real-world applications and
few works consider the behavioral characteristics behind long-tailed users. In
this work, we propose the Hybrid Interest Modeling (HIM) network to hybrid both
personalized interest and semi-personalized interest in learning long-tailed
users' preferences in the recommendation. To achieve this, we first design the
User Behavior Pyramid (UBP) module to capture the fine-grained personalized
interest of high confidence from sparse even noisy positive feedbacks.
Moreover, the individual interaction is too sparse and not enough for modeling
user interest adequately, we design the User Behavior Clustering (UBC) module
to learn latent user interest groups with self-supervised learning mechanism
novelly, which capture coarse-grained semi-personalized interest from
group-item interaction data. Extensive experiments on both public and
industrial datasets verify the superiority of HIM compared with the
state-of-the-art baselines.",arxiv
http://arxiv.org/abs/2106.04174v1,2021-06-08T08:27:31Z,2021-06-08T08:27:31Z,"Interpretable and Low-Resource Entity Matching via Decoupling Feature
  Learning from Decision Making","Entity Matching (EM) aims at recognizing entity records that denote the same
real-world object. Neural EM models learn vector representation of entity
descriptions and match entities end-to-end. Though robust, these methods
require many resources for training, and lack of interpretability. In this
paper, we propose a novel EM framework that consists of Heterogeneous
Information Fusion (HIF) and Key Attribute Tree (KAT) Induction to decouple
feature representation from matching decision. Using self-supervised learning
and mask mechanism in pre-trained language modeling, HIF learns the embeddings
of noisy attribute values by inter-attribute attention with unlabeled data.
Using a set of comparison features and a limited amount of annotated data, KAT
Induction learns an efficient decision tree that can be interpreted by
generating entity matching rules whose structure is advocated by domain
experts. Experiments on 6 public datasets and 3 industrial datasets show that
our method is highly efficient and outperforms SOTA EM models in most cases.
Our codes and datasets can be obtained from https://github.com/THU-KEG/HIF-KAT.",arxiv
http://arxiv.org/abs/2004.11573v1,2020-04-24T07:29:47Z,2020-04-24T07:29:47Z,"Towards Characterizing Adversarial Defects of Deep Learning Software
  from the Lens of Uncertainty","Over the past decade, deep learning (DL) has been successfully applied to
many industrial domain-specific tasks. However, the current state-of-the-art DL
software still suffers from quality issues, which raises great concern
especially in the context of safety- and security-critical scenarios.
Adversarial examples (AEs) represent a typical and important type of defects
needed to be urgently addressed, on which a DL software makes incorrect
decisions. Such defects occur through either intentional attack or
physical-world noise perceived by input sensors, potentially hindering further
industry deployment. The intrinsic uncertainty nature of deep learning
decisions can be a fundamental reason for its incorrect behavior. Although some
testing, adversarial attack and defense techniques have been recently proposed,
it still lacks a systematic study to uncover the relationship between AEs and
DL uncertainty. In this paper, we conduct a large-scale study towards bridging
this gap. We first investigate the capability of multiple uncertainty metrics
in differentiating benign examples (BEs) and AEs, which enables to characterize
the uncertainty patterns of input data. Then, we identify and categorize the
uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated
by existing methods do follow common uncertainty patterns, some other
uncertainty patterns are largely missed. Based on this, we propose an automated
testing technique to generate multiple types of uncommon AEs and BEs that are
largely missed by existing techniques. Our further evaluation reveals that the
uncommon data generated by our method is hard to be defended by the existing
defense techniques with the average defense success rate reduced by 35\%. Our
results call for attention and necessity to generate more diverse data for
evaluating quality assurance solutions of DL software.",arxiv
http://arxiv.org/abs/1907.12650v2,2020-01-18T19:33:53Z,2019-07-30T15:41:01Z,"Beyond Safety Drivers: Staffing a Teleoperations System for Autonomous
  Vehicles","Driverless vehicles promise a host of societal benefits including
dramatically improved safety, increased accessibility, greater productivity,
and higher quality of life. As this new technology approaches widespread
deployment, both industry and government are making provisions for
teleoperations systems, in which remote human agents provide assistance to
driverless vehicles. This assistance can involve real-time remote operation and
even ahead-of-time input via human-in-the-loop artificial intelligence systems.
In this paper, we address the problem of staffing such a remote support center.
Our analysis focuses on the tradeoffs between the total number of remote
agents, the reliability of the remote support system, and the resulting safety
of the driverless vehicles. By establishing a novel connection between queues
with large batch arrivals and storage processes, we determine the probability
of the system exceeding its service capacity. This connection drives our
staffing methodology. We also develop a numerical method to compute the exact
staffing level needed to achieve various performance measures. This moment
generating function based technique may be of independent interest, and our
overall staffing analysis may be of use in other applications that combine
human expertise and automated systems.",arxiv
http://arxiv.org/abs/1908.11863v1,2019-08-30T17:48:05Z,2019-08-30T17:48:05Z,Systematic Analysis of Image Generation using GANs,"Generative Adversarial Networks have been crucial in the developments made in
unsupervised learning in recent times. Exemplars of image synthesis from text
or other images, these networks have shown remarkable improvements over
conventional methods in terms of performance. Trained on the adversarial
training philosophy, these networks aim to estimate the potential distribution
from the real data and then use this as input to generate the synthetic data.
Based on this fundamental principle, several frameworks can be generated that
are paragon implementations in several real-life applications such as art
synthesis, generation of high resolution outputs and synthesis of images from
human drawn sketches, to name a few. While theoretically GANs present better
results and prove to be an improvement over conventional methods in many
factors, the implementation of these frameworks for dedicated applications
remains a challenge. This study explores and presents a taxonomy of these
frameworks and their use in various image to image synthesis and text to image
synthesis applications. The basic GANs, as well as a variety of different niche
frameworks, are critically analyzed. The advantages of GANs for image
generation over conventional methods as well their disadvantages amongst other
frameworks are presented. The future applications of GANs in industries such as
healthcare, art and entertainment are also discussed.",arxiv
http://arxiv.org/abs/1907.00400v1,2019-06-30T15:42:53Z,2019-06-30T15:42:53Z,"Prediction is very hard, especially about conversion. Predicting user
  purchases from clickstream data in fashion e-commerce","Knowing if a user is a buyer vs window shopper solely based on clickstream
data is of crucial importance for ecommerce platforms seeking to implement
real-time accurate NBA (next best action) policies. However, due to the low
frequency of conversion events and the noisiness of browsing data, classifying
user sessions is very challenging. In this paper, we address the clickstream
classification problem in the fashion industry and present three major
contributions to the burgeoning field of AI in fashion: first, we collected,
normalized and prepared a novel dataset of live shopping sessions from a major
European e-commerce fashion website; second, we use the dataset to test in a
controlled environment strong baselines and SOTA models from the literature;
finally, we propose a new discriminative neural model that outperforms neural
architectures recently proposed at Rakuten labs.",arxiv
http://arxiv.org/abs/1907.02797v2,2020-03-14T14:25:49Z,2019-07-03T16:37:48Z,"Predicting e-commerce customer conversion from minimal temporal patterns
  on symbolized clickstream trajectories","Knowing if a user is a buyer or window shopper solely based on clickstream
data is of crucial importance for e-commerce platforms seeking to implement
real-time accurate NBA (next best action) policies. However, due to the low
frequency of conversion events and the noisiness of browsing data, classifying
user sessions is very challenging. In this paper, we address the clickstream
classification problem in the eCommerce industry and present three major
contributions to the burgeoning field of AI-for-retail: first, we collected,
normalized and prepared a novel dataset of live shopping sessions from a major
European e-commerce website; second, we use the dataset to test in a controlled
environment strong baselines and SOTA models from the literature; finally, we
propose a new discriminative neural model that outperforms neural architectures
recently proposed at Rakuten labs.",arxiv
http://arxiv.org/abs/2011.04106v1,2020-11-08T23:37:58Z,2020-11-08T23:37:58Z,Ensembled CTR Prediction via Knowledge Distillation,"Recently, deep learning-based models have been widely studied for
click-through rate (CTR) prediction and lead to improved prediction accuracy in
many industrial applications. However, current research focuses primarily on
building complex network architectures to better capture sophisticated feature
interactions and dynamic user behaviors. The increased model complexity may
slow down online inference and hinder its adoption in real-time applications.
Instead, our work targets at a new model training strategy based on knowledge
distillation (KD). KD is a teacher-student learning framework to transfer
knowledge learned from a teacher model to a student model. The KD strategy not
only allows us to simplify the student model as a vanilla DNN model but also
achieves significant accuracy improvements over the state-of-the-art teacher
models. The benefits thus motivate us to further explore the use of a powerful
ensemble of teachers for more accurate student model training. We also propose
some novel techniques to facilitate ensembled CTR prediction, including teacher
gating and early stopping by distillation loss. We conduct comprehensive
experiments against 12 existing models and across three industrial datasets.
Both offline and online A/B testing results show the effectiveness of our
KD-based training strategy.",arxiv
http://arxiv.org/abs/2106.03415v1,2021-06-07T08:32:19Z,2021-06-07T08:32:19Z,"Leveraging Tripartite Interaction Information from Live Stream
  E-Commerce for Improving Product Recommendation","Recently, a new form of online shopping becomes more and more popular, which
combines live streaming with E-Commerce activity. The streamers introduce
products and interact with their audiences, and hence greatly improve the
performance of selling products. Despite of the successful applications in
industries, the live stream E-commerce has not been well studied in the data
science community. To fill this gap, we investigate this brand-new scenario and
collect a real-world Live Stream E-Commerce (LSEC) dataset. Different from
conventional E-commerce activities, the streamers play a pivotal role in the
LSEC events. Hence, the key is to make full use of rich interaction information
among streamers, users, and products. We first conduct data analysis on the
tripartite interaction data and quantify the streamer's influence on users'
purchase behavior. Based on the analysis results, we model the tripartite
information as a heterogeneous graph, which can be decomposed to multiple
bipartite graphs in order to better capture the influence. We propose a novel
Live Stream E-Commerce Graph Neural Network framework (LSEC-GNN) to learn the
node representations of each bipartite graph, and further design a multi-task
learning approach to improve product recommendation. Extensive experiments on
two real-world datasets with different scales show that our method can
significantly outperform various baseline approaches.",arxiv
http://arxiv.org/abs/1902.10666v1,2019-02-27T18:01:06Z,2019-02-27T18:01:06Z,Improving Missing Data Imputation with Deep Generative Models,"Datasets with missing values are very common on industry applications, and
they can have a negative impact on machine learning models. Recent studies
introduced solutions to the problem of imputing missing values based on deep
generative models. Previous experiments with Generative Adversarial Networks
and Variational Autoencoders showed interesting results in this domain, but it
is not clear which method is preferable for different use cases. The goal of
this work is twofold: we present a comparison between missing data imputation
solutions based on deep generative models, and we propose improvements over
those methodologies. We run our experiments using known real life datasets with
different characteristics, removing values at random and reconstructing them
with several imputation techniques. Our results show that the presence or
absence of categorical variables can alter the selection of the best model, and
that some models are more stable than others after similar runs with different
random number generator seeds.",arxiv
http://arxiv.org/abs/1909.10233v1,2019-09-23T09:09:12Z,2019-09-23T09:09:12Z,Machine Learning Optimization Algorithms & Portfolio Allocation,"Portfolio optimization emerged with the seminal paper of Markowitz (1952).
The original mean-variance framework is appealing because it is very efficient
from a computational point of view. However, it also has one well-established
failing since it can lead to portfolios that are not optimal from a financial
point of view. Nevertheless, very few models have succeeded in providing a real
alternative solution to the Markowitz model. The main reason lies in the fact
that most academic portfolio optimization models are intractable in real life
although they present solid theoretical properties. By intractable we mean that
they can be implemented for an investment universe with a small number of
assets using a lot of computational resources and skills, but they are unable
to manage a universe with dozens or hundreds of assets. However, the emergence
and the rapid development of robo-advisors means that we need to rethink
portfolio optimization and go beyond the traditional mean-variance optimization
approach. Another industry has faced similar issues concerning large-scale
optimization problems. Machine learning has long been associated with linear
and logistic regression models. Again, the reason was the inability of
optimization algorithms to solve high-dimensional industrial problems.
Nevertheless, the end of the 1990s marked an important turning point with the
development and the rediscovery of several methods that have since produced
impressive results. The goal of this paper is to show how portfolio allocation
can benefit from the development of these large-scale optimization algorithms.
Not all of these algorithms are useful in our case, but four of them are
essential when solving complex portfolio optimization problems. These four
algorithms are the coordinate descent, the alternating direction method of
multipliers, the proximal gradient method and the Dykstra's algorithm.",arxiv
http://arxiv.org/abs/2106.01272v1,2021-06-02T16:33:43Z,2021-06-02T16:33:43Z,Grasp stability prediction with time series data based on STFT and LSTM,"With an increasing demand for robots, robotic grasping will has a more
important role in future applications. This paper takes grasp stability
prediction as the key technology for grasping and tries to solve the problem
with time series data inputs including the force and pressure data. Widely
applied to more fields to predict unstable grasping with time series data,
algorithms can significantly promote the application of artificial intelligence
in traditional industries. This research investigates models that combine
short-time Fourier transform (STFT) and long short-term memory (LSTM) and then
tested generalizability with dexterous hand and suction cup gripper. The
experiments suggest good results for grasp stability prediction with the force
data and the generalized results in the pressure data. Among the 4 models,
(Data + STFT) & LSTM delivers the best performance. We plan to perform more
work on grasp stability prediction, generalize the findings to different types
of sensors, and apply the grasp stability prediction in more grasping use cases
in real life.",arxiv
http://arxiv.org/abs/1706.02952v3,2018-10-29T15:49:37Z,2017-06-09T13:55:18Z,TIP: Typifying the Interpretability of Procedures,"We provide a novel notion of what it means to be interpretable, looking past
the usual association with human understanding. Our key insight is that
interpretability is not an absolute concept and so we define it relative to a
target model, which may or may not be a human. We define a framework that
allows for comparing interpretable procedures by linking them to important
practical aspects such as accuracy and robustness. We characterize many of the
current state-of-the-art interpretable methods in our framework portraying its
general applicability. Finally, principled interpretable strategies are
proposed and empirically evaluated on synthetic data, as well as on the largest
public olfaction dataset that was made recently available \cite{olfs}. We also
experiment on MNIST with a simple target model and different oracle models of
varying complexity. This leads to the insight that the improvement in the
target model is not only a function of the oracle model's performance, but also
its relative complexity with respect to the target model. Further experiments
on CIFAR-10, a real manufacturing dataset and FICO dataset showcase the benefit
of our methods over Knowledge Distillation when the target models are simple
and the complex model is a neural network.",arxiv
http://arxiv.org/abs/2102.06838v1,2021-02-13T01:07:26Z,2021-02-13T01:07:26Z,"Learning Variable Impedance Control via Inverse Reinforcement Learning
  for Force-Related Tasks","Many manipulation tasks require robots to interact with unknown environments.
In such applications, the ability to adapt the impedance according to different
task phases and environment constraints is crucial for safety and performance.
Although many approaches based on deep reinforcement learning (RL) and learning
from demonstration (LfD) have been proposed to obtain variable impedance skills
on contact-rich manipulation tasks, these skills are typically task-specific
and could be sensitive to changes in task settings. This paper proposes an
inverse reinforcement learning (IRL) based approach to recover both the
variable impedance policy and reward function from expert demonstrations. We
explore different action space of the reward functions to achieve a more
general representation of expert variable impedance skills. Experiments on two
variable impedance tasks (Peg-in-Hole and Cup-on-Plate) were conducted in both
simulations and on a real FANUC LR Mate 200iD/7L industrial robot. The
comparison results with behavior cloning and force-based IRL proved that the
learned reward function in the gain action space has better transferability
than in the force space. Experiment videos are available at
https://msc.berkeley.edu/research/impedance-irl.html.",arxiv
http://arxiv.org/abs/1811.07112v2,2019-04-10T09:59:47Z,2018-11-17T07:09:13Z,Augmented LiDAR Simulator for Autonomous Driving,"In Autonomous Driving (AD), detection and tracking of obstacles on the roads
is a critical task. Deep-learning based methods using annotated LiDAR data have
been the most widely adopted approach for this. Unfortunately, annotating 3D
point cloud is a very challenging, time- and money-consuming task. In this
paper, we propose a novel LiDAR simulator that augments real point cloud with
synthetic obstacles (e.g., cars, pedestrians, and other movable objects).
Unlike previous simulators that entirely rely on CG models and game engines,
our augmented simulator bypasses the requirement to create high-fidelity
background CAD models. Instead, we can simply deploy a vehicle with a LiDAR
scanner to sweep the street of interests to obtain the background point cloud,
based on which annotated point cloud can be automatically generated. This
unique ""scan-and-simulate"" capability makes our approach scalable and
practical, ready for large-scale industrial applications. In this paper, we
describe our simulator in detail, in particular the placement of obstacles that
is critical for performance enhancement. We show that detectors with our
simulated LiDAR point cloud alone can perform comparably (within two percentage
points) with these trained with real data. Mixing real and simulated data can
achieve over 95% accuracy.",arxiv
http://arxiv.org/abs/1312.0317v1,2013-12-02T03:21:28Z,2013-12-02T03:21:28Z,Evolutionary Dynamics of Information Diffusion over Social Networks,"Current social networks are of extremely large-scale generating tremendous
information flows at every moment. How information diffuse over social networks
has attracted much attention from both industry and academics. Most of the
existing works on information diffusion analysis are based on machine learning
methods focusing on social network structure analysis and empirical data
mining. However, the dynamics of information diffusion, which are heavily
influenced by network users' decisions, actions and their socio-economic
interactions, is generally ignored by most of existing works. In this paper, we
propose an evolutionary game theoretic framework to model the dynamic
information diffusion process in social networks. Specifically, we derive the
information diffusion dynamics in complete networks, uniform degree and
non-uniform degree networks, with the highlight of two special networks,
Erd\H{o}s-R\'enyi random network and the Barab\'asi-Albert scale-free network.
We find that the dynamics of information diffusion over these three kinds of
networks are scale-free and the same with each other when the network scale is
sufficiently large. To verify our theoretical analysis, we perform simulations
for the information diffusion over synthetic networks and real-world Facebook
networks. Moreover, we also conduct experiment on Twitter hashtags dataset,
which shows that the proposed game theoretic model can well fit and predict the
information diffusion over real social networks.",arxiv
http://arxiv.org/abs/1905.06004v1,2019-05-15T07:48:21Z,2019-05-15T07:48:21Z,Domain Adaptive Transfer Learning for Fault Diagnosis,"Thanks to digitization of industrial assets in fleets, the ambitious goal of
transferring fault diagnosis models fromone machine to the other has raised
great interest. Solving these domain adaptive transfer learning tasks has the
potential to save large efforts on manually labeling data and modifying models
for new machines in the same fleet. Although data-driven methods have shown
great potential in fault diagnosis applications, their ability to generalize on
new machines and new working conditions are limited because of their tendency
to overfit to the training set in reality. One promising solution to this
problem is to use domain adaptation techniques. It aims to improve model
performance on the target new machine. Inspired by its successful
implementation in computer vision, we introduced Domain-Adversarial Neural
Networks (DANN) to our context, along with two other popular methods existing
in previous fault diagnosis research. We then carefully justify the
applicability of these methods in realistic fault diagnosis settings, and offer
a unified experimental protocol for a fair comparison between domain adaptation
methods for fault diagnosis problems.",arxiv
http://arxiv.org/abs/2104.01036v1,2021-04-02T13:17:11Z,2021-04-02T13:17:11Z,"Hybrid Policy Learning for Energy-Latency Tradeoff in MEC-Assisted VR
  Video Service","Virtual reality (VR) is promising to fundamentally transform a broad spectrum
of industry sectors and the way humans interact with virtual content. However,
despite unprecedented progress, current networking and computing
infrastructures are incompetent to unlock VR's full potential. In this paper,
we consider delivering the wireless multi-tile VR video service over a mobile
edge computing (MEC) network. The primary goal is to minimize the system
latency/energy consumption and to arrive at a tradeoff thereof. To this end, we
first cast the time-varying view popularity as a model-free Markov chain to
effectively capture its dynamic characteristics. After jointly assessing the
caching and computing capacities on both the MEC server and the VR playback
device, a hybrid policy is then implemented to coordinate the dynamic caching
replacement and the deterministic offloading, so as to fully utilize the system
resources. The underlying multi-objective problem is reformulated as a
partially observable Markov decision process, and a deep deterministic policy
gradient algorithm is proposed to iteratively learn its solution, where a long
short-term memory neural network is embedded to continuously predict the
dynamics of the unobservable popularity. Simulation results demonstrate the
superiority of the proposed scheme in achieving a trade-off between the energy
efficiency and the latency reduction over the baseline methods.",arxiv
http://arxiv.org/abs/1710.08135v1,2017-10-23T08:12:45Z,2017-10-23T08:12:45Z,"An iterative closest point method for measuring the level of similarity
  of 3d log scans in wood industry","In the Canadian's lumber industry, simulators are used to predict the lumbers
resulting from the sawing of a log at a given sawmill. Giving a log or several
logs' 3D scans as input, simulators perform a real-time job to predict the
lumbers. These simulators, however, tend to be slow at processing large volume
of wood. We thus explore an alternative approximation techniques based on the
Iterative Closest Point (ICP) algorithm to identify the already processed log
to which an unseen log resembles the most. The main benefit of the ICP approach
is that it can easily handle 3D scans with a variable number of points. We
compare this ICP-based nearest neighbor predictor, to predictors built using
machine learning algorithms such as the K-nearest-neighbor (kNN) and Random
Forest (RF). The implemented ICP-based predictor enabled us to identify key
points in using the 3D scans directly for distance calculation. The long-term
goal of this ongoing research is to integrated ICP distance calculations and
machine learning.",arxiv
http://arxiv.org/abs/2104.08542v2,2021-05-11T14:11:46Z,2021-04-17T13:36:19Z,"ScaleFreeCTR: MixCache-based Distributed Training System for CTR Models
  with Huge Embedding Table","Because of the superior feature representation ability of deep learning,
various deep Click-Through Rate (CTR) models are deployed in the commercial
systems by industrial companies. To achieve better performance, it is necessary
to train the deep CTR models on huge volume of training data efficiently, which
makes speeding up the training process an essential problem. Different from the
models with dense training data, the training data for CTR models is usually
high-dimensional and sparse. To transform the high-dimensional sparse input
into low-dimensional dense real-value vectors, almost all deep CTR models adopt
the embedding layer, which easily reaches hundreds of GB or even TB. Since a
single GPU cannot afford to accommodate all the embedding parameters, when
performing distributed training, it is not reasonable to conduct the
data-parallelism only. Therefore, existing distributed training platforms for
recommendation adopt model-parallelism. Specifically, they use CPU (Host)
memory of servers to maintain and update the embedding parameters and utilize
GPU worker to conduct forward and backward computations. Unfortunately, these
platforms suffer from two bottlenecks: (1) the latency of pull \& push
operations between Host and GPU; (2) parameters update and synchronization in
the CPU servers. To address such bottlenecks, in this paper, we propose the
ScaleFreeCTR: a MixCache-based distributed training system for CTR models.
Specifically, in SFCTR, we also store huge embedding table in CPU but utilize
GPU instead of CPU to conduct embedding synchronization efficiently. To reduce
the latency of data transfer between both GPU-Host and GPU-GPU, the MixCache
mechanism and Virtual Sparse Id operation are proposed. Comprehensive
experiments and ablation studies are conducted to demonstrate the effectiveness
and efficiency of SFCTR.",arxiv
http://arxiv.org/abs/2003.13948v3,2020-08-02T03:32:48Z,2020-03-31T04:44:31Z,Segmenting Transparent Objects in the Wild,"Transparent objects such as windows and bottles made by glass widely exist in
the real world. Segmenting transparent objects is challenging because these
objects have diverse appearance inherited from the image background, making
them had similar appearance with their surroundings. Besides the technical
difficulty of this task, only a few previous datasets were specially designed
and collected to explore this task and most of the existing datasets have major
drawbacks. They either possess limited sample size such as merely a thousand of
images without manual annotations, or they generate all images by using
computer graphics method (i.e. not real image). To address this important
problem, this work proposes a large-scale dataset for transparent object
segmentation, named Trans10K, consisting of 10,428 images of real scenarios
with carefully manual annotations, which are 10 times larger than the existing
datasets. The transparent objects in Trans10K are extremely challenging due to
high diversity in scale, viewpoint and occlusion as shown in Fig. 1. To
evaluate the effectiveness of Trans10K, we propose a novel boundary-aware
segmentation method, termed TransLab, which exploits boundary as the clue to
improve segmentation of transparent objects. Extensive experiments and ablation
studies demonstrate the effectiveness of Trans10K and validate the practicality
of learning object boundary in TransLab. For example, TransLab significantly
outperforms 20 recent object segmentation methods based on deep learning,
showing that this task is largely unsolved. We believe that both Trans10K and
TransLab have important contributions to both the academia and industry,
facilitating future researches and applications.",arxiv
http://arxiv.org/abs/1909.02182v1,2019-09-05T02:02:26Z,2019-09-05T02:02:26Z,"Machine Learning in Least-Squares Monte Carlo Proxy Modeling of Life
  Insurance Companies","Under the Solvency II regime, life insurance companies are asked to derive
their solvency capital requirements from the full loss distributions over the
coming year. Since the industry is currently far from being endowed with
sufficient computational capacities to fully simulate these distributions, the
insurers have to rely on suitable approximation techniques such as the
least-squares Monte Carlo (LSMC) method. The key idea of LSMC is to run only a
few wisely selected simulations and to process their output further to obtain a
risk-dependent proxy function of the loss. In this paper, we present and
analyze various adaptive machine learning approaches that can take over the
proxy modeling task. The studied approaches range from ordinary and generalized
least-squares regression variants over GLM and GAM methods to MARS and kernel
regression routines. We justify the combinability of their regression
ingredients in a theoretical discourse. Further, we illustrate the approaches
in slightly disguised real-world experiments and perform comprehensive
out-of-sample tests.",arxiv
http://arxiv.org/abs/2006.06983v4,2021-03-12T08:45:11Z,2020-06-12T07:49:21Z,"Characterizing Impacts of Heterogeneity in Federated Learning upon
  Large-Scale Smartphone Data","Federated learning (FL) is an emerging, privacy-preserving machine learning
paradigm, drawing tremendous attention in both academia and industry. A unique
characteristic of FL is heterogeneity, which resides in the various hardware
specifications and dynamic states across the participating devices.
Theoretically, heterogeneity can exert a huge influence on the FL training
process, e.g., causing a device unavailable for training or unable to upload
its model updates. Unfortunately, these impacts have never been systematically
studied and quantified in existing FL literature.
  In this paper, we carry out the first empirical study to characterize the
impacts of heterogeneity in FL. We collect large-scale data from 136k
smartphones that can faithfully reflect heterogeneity in real-world settings.
We also build a heterogeneity-aware FL platform that complies with the standard
FL protocol but with heterogeneity in consideration. Based on the data and the
platform, we conduct extensive experiments to compare the performance of
state-of-the-art FL algorithms under heterogeneity-aware and
heterogeneity-unaware settings. Results show that heterogeneity causes
non-trivial performance degradation in FL, including up to 9.2% accuracy drop,
2.32x lengthened training time, and undermined fairness. Furthermore, we
analyze potential impact factors and find that device failure and participant
bias are two potential factors for performance degradation. Our study provides
insightful implications for FL practitioners. On the one hand, our findings
suggest that FL algorithm designers consider necessary heterogeneity during the
evaluation. On the other hand, our findings urge system providers to design
specific mechanisms to mitigate the impacts of heterogeneity.",arxiv
http://arxiv.org/abs/2009.09945v4,2021-05-22T06:08:04Z,2020-09-21T15:08:10Z,"Clicks can be Cheating: Counterfactual Recommendation for Mitigating
  Clickbait Issue","Recommendation is a prevalent and critical service in information systems. To
provide personalized suggestions to users, industry players embrace machine
learning, more specifically, building predictive models based on the click
behavior data. This is known as the Click-Through Rate (CTR) prediction, which
has become the gold standard for building personalized recommendation service.
However, we argue that there is a significant gap between clicks and user
satisfaction -- it is common that a user is ""cheated"" to click an item by the
attractive title/cover of the item. This will severely hurt user's trust on the
system if the user finds the actual content of the clicked item disappointing.
What's even worse, optimizing CTR models on such flawed data will result in the
Matthew Effect, making the seemingly attractive but actually low-quality items
be more frequently recommended.
  In this paper, we formulate the recommendation models as a causal graph that
reflects the cause-effect factors in recommendation, and address the clickbait
issue by performing counterfactual inference on the causal graph. We imagine a
counterfactual world where each item has only exposure features (i.e., the
features that the user can see before making a click decision). By estimating
the click likelihood of a user in the counterfactual world, we are able to
reduce the direct effect of exposure features and eliminate the clickbait
issue. Experiments on real-world datasets demonstrate that our method
significantly improves the post-click satisfaction of CTR models.",arxiv
http://arxiv.org/abs/2110.01659v2,2021-10-06T17:21:46Z,2021-10-04T18:49:51Z,Cross-Modal Virtual Sensing for Combustion Instability Monitoring,"In many cyber-physical systems, imaging can be an important but expensive or
'difficult to deploy' sensing modality. One such example is detecting
combustion instability using flame images, where deep learning frameworks have
demonstrated state-of-the-art performance. The proposed frameworks are also
shown to be quite trustworthy such that domain experts can have sufficient
confidence to use these models in real systems to prevent unwanted incidents.
However, flame imaging is not a common sensing modality in engine combustors
today. Therefore, the current roadblock exists on the hardware side regarding
the acquisition and processing of high-volume flame images. On the other hand,
the acoustic pressure time series is a more feasible modality for data
collection in real combustors. To utilize acoustic time series as a sensing
modality, we propose a novel cross-modal encoder-decoder architecture that can
reconstruct cross-modal visual features from acoustic pressure time series in
combustion systems. With the ""distillation"" of cross-modal features, the
results demonstrate that the detection accuracy can be enhanced using the
virtual visual sensing modality. By providing the benefit of cross-modal
reconstruction, our framework can prove to be useful in different domains well
beyond the power generation and transportation industries.",arxiv
http://arxiv.org/abs/1602.02339v1,2016-02-07T04:01:55Z,2016-02-07T04:01:55Z,"Dynamic Selection of Virtual Machines for Application Servers in Cloud
  Environments","Autoscaling is a hallmark of cloud computing as it allows flexible
just-in-time allocation and release of computational resources in response to
dynamic and often unpredictable workloads. This is especially important for web
applications whose workload is time dependent and prone to flash crowds. Most
of them follow the 3-tier architectural pattern, and are divided into
presentation, application/domain and data layers. In this work we focus on the
application layer. Reactive autoscaling policies of the type ""Instantiate a new
Virtual Machine (VM) when the average server CPU utilisation reaches X%"" have
been used successfully since the dawn of cloud computing. But which VM type is
the most suitable for the specific application at the moment remains an open
question. In this work, we propose an approach for dynamic VM type selection.
It uses a combination of online machine learning techniques, works in real time
and adapts to changes in the users' workload patterns, application changes as
well as middleware upgrades and reconfigurations. We have developed a
prototype, which we tested with the CloudStone benchmark deployed on AWS EC2.
Results show that our method quickly adapts to workload changes and reduces the
total cost compared to the industry standard approach.",arxiv
http://arxiv.org/abs/1710.07709v1,2017-10-20T21:34:49Z,2017-10-20T21:34:49Z,"Solving the ""false positives"" problem in fraud prediction","In this paper, we present an automated feature engineering based approach to
dramatically reduce false positives in fraud prediction. False positives plague
the fraud prediction industry. It is estimated that only 1 in 5 declared as
fraud are actually fraud and roughly 1 in every 6 customers have had a valid
transaction declined in the past year. To address this problem, we use the Deep
Feature Synthesis algorithm to automatically derive behavioral features based
on the historical data of the card associated with a transaction. We generate
237 features (>100 behavioral patterns) for each transaction, and use a random
forest to learn a classifier. We tested our machine learning model on data from
a large multinational bank and compared it to their existing solution. On an
unseen data of 1.852 million transactions, we were able to reduce the false
positives by 54% and provide a savings of 190K euros. We also assess how to
deploy this solution, and whether it necessitates streaming computation for
real time scoring. We found that our solution can maintain similar benefits
even when historical features are computed once every 7 days.",arxiv
http://arxiv.org/abs/2103.03512v1,2021-03-05T07:47:54Z,2021-03-05T07:47:54Z,"Amino acid frequency and domain features serve well for random forest
  based classification of thermophilic and mesophilic protein; a case study on
  serine proteases","Thermostability is an important prerequisite for enzymes employed for
industrial applications. Several machine learning based models have thus been
formulated for protein classification based on this particular trait. These
models have employed features derived from sequences, structures or both
resulting in a >93% accuracy based on a 10-fold cross-validation. Besides using
various proteins from a wide range of organisms, such studies also rely on
hundreds of features. In the present study, an enzyme specific classification
model was created using significantly less number of features that provides a
similar accuracy of classification for thermophilic and non-thermophilic enzyme
serine proteases. For building the classifier, 219 thermophilic and 200
mesophilic bacterial genomes were mined for their respective serine protease
sequences. Features were extracted for 800 sequences followed by feature
selection. We deployed a random forest based classifier that identified
thermophilic and non-thermophilic serine proteases with an accuracy of 95.71%.
Knowledge of thermostability along with amino acid positional shifts can be
vital for downstream protein engineering techniques. Thus, to emphasize the
real time application of the enzyme specific classification model, a web
platform has been designed. Combining the sequence data and the classification
model, this prototype can allow users to align their query serine protease
sequence against the custom database and identify its thermophilic nature.",arxiv
http://arxiv.org/abs/2103.10997v3,2021-09-16T10:22:17Z,2021-03-19T19:38:00Z,"MVGrasp: Real-Time Multi-View 3D Object Grasping in Highly Cluttered
  Environments","Nowadays service robots are entering more and more in our daily life. In such
a dynamic environment, a robot frequently faces pile, packed, or isolated
objects. Therefore, it is necessary for the robot to know how to grasp and
manipulate various objects in different situations to help humans in everyday
tasks. Most state-of-the-art grasping approaches addressed four
degrees-of-freedom (DoF) object grasping, where the robot is forced to grasp
objects from above based on grasp synthesis of a given top-down scene. Although
such approaches showed a very good performance in predefined industrial
settings, they are not suitable for human-centric environments as the robot
will not able to grasp a range of household objects robustly, for example,
grasping a bottle from above is not stable. In this work, we propose a
multi-view deep learning approach to handle robust object grasping in
human-centric domains. In particular, our approach takes a partial point cloud
of a scene as an input, and then, generates multi-views of existing objects.
The obtained views of each object are used to estimate pixel-wise grasp
synthesis for each object. To evaluate the performance of the proposed
approach, we performed extensive experiments in both simulation and real-world
environments within the pile, packed, and isolated objects scenarios.
Experimental results showed that our approach can estimate appropriate grasp
configurations in only 22ms without the need for explicit collision checking.
Therefore, the proposed approach can be used in real-time robotic applications
that need closed-loop grasp planning.",arxiv
http://arxiv.org/abs/1710.11319v2,2018-06-07T21:16:06Z,2017-10-31T04:19:20Z,"Learning Motion Predictors for Smart Wheelchair using Autoregressive
  Sparse Gaussian Process","Constructing a smart wheelchair on a commercially available powered
wheelchair (PWC) platform avoids a host of seating, mechanical design and
reliability issues but requires methods of predicting and controlling the
motion of a device never intended for robotics. Analog joystick inputs are
subject to black-box transformations which may produce intuitive and adaptable
motion control for human operators, but complicate robotic control approaches;
furthermore, installation of standard axle mounted odometers on a commercial
PWC is difficult. In this work, we present an integrated hardware and software
system for predicting the motion of a commercial PWC platform that does not
require any physical or electronic modification of the chair beyond plugging
into an industry standard auxiliary input port. This system uses an RGB-D
camera and an Arduino interface board to capture motion data, including visual
odometry and joystick signals, via ROS communication. Future motion is
predicted using an autoregressive sparse Gaussian process model. We evaluate
the proposed system on real-world short-term path prediction experiments.
Experimental results demonstrate the system's efficacy when compared to a
baseline neural network model.",arxiv
http://arxiv.org/abs/2006.04380v1,2020-06-08T06:57:18Z,2020-06-08T06:57:18Z,"Learning the Compositional Visual Coherence for Complementary
  Recommendations","Complementary recommendations, which aim at providing users product
suggestions that are supplementary and compatible with their obtained items,
have become a hot topic in both academia and industry in recent years.
%However, it is challenging due to its complexity and subjectivity. Existing
work mainly focused on modeling the co-purchased relations between two items,
but the compositional associations of item collections are largely unexplored.
Actually, when a user chooses the complementary items for the purchased
products, it is intuitive that she will consider the visual semantic coherence
(such as color collocations, texture compatibilities) in addition to global
impressions. Towards this end, in this paper, we propose a novel Content
Attentive Neural Network (CANN) to model the comprehensive compositional
coherence on both global contents and semantic contents. Specifically, we first
propose a \textit{Global Coherence Learning} (GCL) module based on multi-heads
attention to model the global compositional coherence. Then, we generate the
semantic-focal representations from different semantic regions and design a
\textit{Focal Coherence Learning} (FCL) module to learn the focal compositional
coherence from different semantic-focal representations. Finally, we optimize
the CANN in a novel compositional optimization strategy. Extensive experiments
on the large-scale real-world data clearly demonstrate the effectiveness of
CANN compared with several state-of-the-art methods.",arxiv
http://arxiv.org/abs/2008.00610v2,2021-03-18T07:35:45Z,2020-08-03T02:17:42Z,"Robust Collaborative Learning of Patch-level and Image-level Annotations
  for Diabetic Retinopathy Grading from Fundus Image","Diabetic retinopathy (DR) grading from fundus images has attracted increasing
interest in both academic and industrial communities. Most convolutional neural
network (CNN) based algorithms treat DR grading as a classification task via
image-level annotations. However, these algorithms have not fully explored the
valuable information in the DR-related lesions. In this paper, we present a
robust framework, which collaboratively utilizes patch-level and image-level
annotations, for DR severity grading. By an end-to-end optimization, this
framework can bi-directionally exchange the fine-grained lesion and image-level
grade information. As a result, it exploits more discriminative features for DR
grading. The proposed framework shows better performance than the recent
state-of-the-art algorithms and three clinical ophthalmologists with over nine
years of experience. By testing on datasets of different distributions (such as
label and camera), we prove that our algorithm is robust when facing image
quality and distribution variations that commonly exist in real-world practice.
We inspect the proposed framework through extensive ablation studies to
indicate the effectiveness and necessity of each motivation. The code and some
valuable annotations are now publicly available.",arxiv
http://arxiv.org/abs/2101.09645v1,2021-01-24T04:25:08Z,2021-01-24T04:25:08Z,Multi-Task Time Series Forecasting With Shared Attention,"Time series forecasting is a key component in many industrial and business
decision processes and recurrent neural network (RNN) based models have
achieved impressive progress on various time series forecasting tasks. However,
most of the existing methods focus on single-task forecasting problems by
learning separately based on limited supervised objectives, which often suffer
from insufficient training instances. As the Transformer architecture and other
attention-based models have demonstrated its great capability of capturing long
term dependency, we propose two self-attention based sharing schemes for
multi-task time series forecasting which can train jointly across multiple
tasks. We augment a sequence of paralleled Transformer encoders with an
external public multi-head attention function, which is updated by all data of
all tasks. Experiments on a number of real-world multi-task time series
forecasting tasks show that our proposed architectures can not only outperform
the state-of-the-art single-task forecasting baselines but also outperform the
RNN-based multi-task forecasting method.",arxiv
http://arxiv.org/abs/2107.14033v1,2021-07-22T02:16:09Z,2021-07-22T02:16:09Z,"Temporal-Relational Hypergraph Tri-Attention Networks for Stock Trend
  Prediction","Predicting the future price trends of stocks is a challenging yet intriguing
problem given its critical role to help investors make profitable decisions. In
this paper, we present a collaborative temporal-relational modeling framework
for end-to-end stock trend prediction. The temporal dynamics of stocks is
firstly captured with an attention-based recurrent neural network. Then,
different from existing studies relying on the pairwise correlations between
stocks, we argue that stocks are naturally connected as a collective group, and
introduce the hypergraph structures to jointly characterize the stock
group-wise relationships of industry-belonging and fund-holding. A novel
hypergraph tri-attention network (HGTAN) is proposed to augment the hypergraph
convolutional networks with a hierarchical organization of intra-hyperedge,
inter-hyperedge, and inter-hypergraph attention modules. In this manner, HGTAN
adaptively determines the importance of nodes, hyperedges, and hypergraphs
during the information propagation among stocks, so that the potential
synergies between stock movements can be fully exploited. Extensive experiments
on real-world data demonstrate the effectiveness of our approach. Also, the
results of investment simulation show that our approach can achieve a more
desirable risk-adjusted return. The data and codes of our work have been
released at https://github.com/lixiaojieff/HGTAN.",arxiv
http://arxiv.org/abs/1911.06633v1,2019-11-15T13:50:27Z,2019-11-15T13:50:27Z,"HealthFog: An Ensemble Deep Learning based Smart Healthcare System for
  Automatic Diagnosis of Heart Diseases in Integrated IoT and Fog Computing
  Environments","Cloud computing provides resources over the Internet and allows a plethora of
applications to be deployed to provide services for different industries. The
major bottleneck being faced currently in these cloud frameworks is their
limited scalability and hence inability to cater to the requirements of
centralized Internet of Things (IoT) based compute environments. The main
reason for this is that latency-sensitive applications like health monitoring
and surveillance systems now require computation over large amounts of data
(Big Data) transferred to centralized database and from database to cloud data
centers which leads to drop in performance of such systems. The new paradigms
of fog and edge computing provide innovative solutions by bringing resources
closer to the user and provide low latency and energy-efficient solutions for
data processing compared to cloud domains. Still, the current fog models have
many limitations and focus from a limited perspective on either accuracy of
results or reduced response time but not both. We proposed a novel framework
called HealthFog for integrating ensemble deep learning in Edge computing
devices and deployed it for a real-life application of automatic Heart Disease
analysis. HealthFog delivers healthcare as a fog service using IoT devices and
efficiently manages the data of heart patients, which comes as user requests.
Fog-enabled cloud framework, FogBus is used to deploy and test the performance
of the proposed model in terms of power consumption, network bandwidth,
latency, jitter, accuracy and execution time. HealthFog is configurable to
various operation modes that provide the best Quality of Service or prediction
accuracy, as required, in diverse fog computation scenarios and for different
user requirements.",arxiv
http://arxiv.org/abs/2011.13493v1,2020-11-26T23:13:42Z,2020-11-26T23:13:42Z,"FIST: A Feature-Importance Sampling and Tree-Based Method for Automatic
  Design Flow Parameter Tuning","Design flow parameters are of utmost importance to chip design quality and
require a painfully long time to evaluate their effects. In reality, flow
parameter tuning is usually performed manually based on designers' experience
in an ad hoc manner. In this work, we introduce a machine learning-based
automatic parameter tuning methodology that aims to find the best design
quality with a limited number of trials. Instead of merely plugging in machine
learning engines, we develop clustering and approximate sampling techniques for
improving tuning efficiency. The feature extraction in this method can reuse
knowledge from prior designs. Furthermore, we leverage a state-of-the-art
XGBoost model and propose a novel dynamic tree technique to overcome
overfitting. Experimental results on benchmark circuits show that our approach
achieves 25% improvement in design quality or 37% reduction in sampling cost
compared to random forest method, which is the kernel of a highly cited
previous work. Our approach is further validated on two industrial designs. By
sampling less than 0.02% of possible parameter sets, it reduces area by 1.83%
and 1.43% compared to the best solutions hand-tuned by experienced designers.",arxiv
http://arxiv.org/abs/2101.02518v4,2021-05-19T13:38:09Z,2021-01-07T12:35:16Z,Towards Automating Code Review Activities,"Code reviews are popular in both industrial and open source projects. The
benefits of code reviews are widely recognized and include better code quality
and lower likelihood of introducing bugs. However, since code review is a
manual activity it comes at the cost of spending developers' time on reviewing
their teammates' code.
  Our goal is to make the first step towards partially automating the code
review process, thus, possibly reducing the manual costs associated with it. We
focus on both the contributor and the reviewer sides of the process, by
training two different Deep Learning architectures. The first one learns code
changes performed by developers during real code review activities, thus
providing the contributor with a revised version of her code implementing code
transformations usually recommended during code review before the code is even
submitted for review. The second one automatically provides the reviewer
commenting on a submitted code with the revised code implementing her comments
expressed in natural language.
  The empirical evaluation of the two models shows that, on the contributor
side, the trained model succeeds in replicating the code transformations
applied during code reviews in up to 16% of cases. On the reviewer side, the
model can correctly implement a comment provided in natural language in up to
31% of cases. While these results are encouraging, more research is needed to
make these models usable by developers.",arxiv
http://arxiv.org/abs/2106.10251v2,2021-09-09T14:16:43Z,2021-06-18T17:33:13Z,Active Offline Policy Selection,"This paper addresses the problem of policy selection in domains with abundant
logged data, but with a very restricted interaction budget. Solving this
problem would enable safe evaluation and deployment of offline reinforcement
learning policies in industry, robotics, and recommendation domains among
others. Several off-policy evaluation (OPE) techniques have been proposed to
assess the value of policies using only logged data. However, there is still a
big gap between the evaluation by OPE and the full online evaluation in the
real environment. At the same time, large amount of online interactions is
often not feasible in practice. To overcome this problem, we introduce
\emph{active offline policy selection} -- a novel sequential decision approach
that combines logged data with online interaction to identify the best policy.
This approach uses OPE estimates to warm start the online evaluation. Then, in
order to utilize the limited environment interactions wisely, it relies on a
Bayesian optimization method, with a kernel function that represents policy
similarity, to decide which policy to evaluate next. We use multiple benchmarks
with a large number of candidate policies to show that the proposed approach
improves upon state-of-the-art OPE estimates and pure online policy evaluation.",arxiv
http://arxiv.org/abs/2003.04987v1,2020-02-17T18:48:55Z,2020-02-17T18:48:55Z,A Financial Service Chatbot based on Deep Bidirectional Transformers,"We develop a chatbot using Deep Bidirectional Transformer models (BERT) to
handle client questions in financial investment customer service. The bot can
recognize 381 intents, and decides when to say ""I don't know"" and escalates
irrelevant/uncertain questions to human operators. Our main novel contribution
is the discussion about uncertainty measure for BERT, where three different
approaches are systematically compared on real problems. We investigated two
uncertainty metrics, information entropy and variance of dropout sampling in
BERT, followed by mixed-integer programming to optimize decision thresholds.
Another novel contribution is the usage of BERT as a language model in
automatic spelling correction. Inputs with accidental spelling errors can
significantly decrease intent classification performance. The proposed approach
combines probabilities from masked language model and word edit distances to
find the best corrections for misspelled words. The chatbot and the entire
conversational AI system are developed using open-source tools, and deployed
within our company's intranet. The proposed approach can be useful for
industries seeking similar in-house solutions in their specific business
domains. We share all our code and a sample chatbot built on a public dataset
on Github.",arxiv
http://arxiv.org/abs/1610.04872v1,2016-10-16T15:14:36Z,2016-10-16T15:14:36Z,"Fault Detection Engine in Intelligent Predictive Analytics Platform for
  DCIM","With the advancement of huge data generation and data handling capability,
Machine Learning and Probabilistic modelling enables an immense opportunity to
employ predictive analytics platform in high security critical industries
namely data centers, electricity grids, utilities, airport etc. where downtime
minimization is one of the primary objectives. This paper proposes a novel,
complete architecture of an intelligent predictive analytics platform, Fault
Engine, for huge device network connected with electrical/information flow.
Three unique modules, here proposed, seamlessly integrate with available
technology stack of data handling and connect with middleware to produce online
intelligent prediction in critical failure scenarios. The Markov Failure module
predicts the severity of a failure along with survival probability of a device
at any given instances. The Root Cause Analysis model indicates probable
devices as potential root cause employing Bayesian probability assignment and
topological sort. Finally, a community detection algorithm produces correlated
clusters of device in terms of failure probability which will further narrow
down the search space of finding route cause. The whole Engine has been tested
with different size of network with simulated failure environments and shows
its potential to be scalable in real-time implementation.",arxiv
http://arxiv.org/abs/1901.05147v1,2019-01-16T06:10:45Z,2019-01-16T06:10:45Z,The Winning Solution to the IEEE CIG 2017 Game Data Mining Competition,"Machine learning competitions such as those organized by Kaggle or KDD
represent a useful benchmark for data science research. In this work, we
present our winning solution to the Game Data Mining competition hosted at the
2017 IEEE Conference on Computational Intelligence and Games (CIG 2017). The
contest consisted of two tracks, and participants (more than 250, belonging to
both industry and academia) were to predict which players would stop playing
the game, as well as their remaining lifetime. The data were provided by a
major worldwide video game company, NCSoft, and came from their successful
massively multiplayer online game Blade and Soul. Here, we describe the long
short-term memory approach and conditional inference survival ensemble model
that made us win both tracks of the contest, as well as the validation
procedure that we followed in order to prevent overfitting. In particular,
choosing a survival method able to deal with censored data was crucial to
accurately predict the moment in which each player would leave the game, as
censoring is inherent in churn. The selected models proved to be robust against
evolving conditions---since there was a change in the business model of the
game (from subscription-based to free-to-play) between the two sample datasets
provided---and efficient in terms of time cost. Thanks to these features and
also to their a ability to scale to large datasets, our models could be readily
implemented in real business settings.",arxiv
http://arxiv.org/abs/2110.01864v1,2021-10-05T08:00:46Z,2021-10-05T08:00:46Z,"Mobile authentication of copy detection patterns: how critical is to
  know fakes?","Protection of physical objects against counterfeiting is an important task
for the modern economies. In recent years, the high-quality counterfeits appear
to be closer to originals thanks to the rapid advancement of digital
technologies. To combat these counterfeits, an anti-counterfeiting technology
based on hand-crafted randomness implemented in a form of copy detection
patterns (CDP) is proposed enabling a link between the physical and digital
worlds and being used in various brand protection applications. The modern
mobile phone technologies make the verification process of CDP easier and
available to the end customers. Besides a big interest and attractiveness, the
CDP authentication based on the mobile phone imaging remains insufficiently
studied. In this respect, in this paper we aim at investigating the CDP
authentication under the real-life conditions with the codes printed on an
industrial printer and enrolled via a modern mobile phone under the regular
light conditions. The authentication aspects of the obtained CDP are
investigated with respect to the four types of copy fakes. The impact of fakes'
type used for training of authentication classifier is studied in two
scenarios: (i) supervised binary classification under various assumptions about
the fakes and (ii) one-class classification under unknown fakes. The obtained
results show that the modern machine-learning approaches and the technical
capacity of modern mobile phones allow to make the CDP authentication under
unknown fakes feasible with respect to the considered types of fakes and code
design.",arxiv
http://arxiv.org/abs/2108.10205v1,2021-08-02T13:09:53Z,2021-08-02T13:09:53Z,"Power transformer faults diagnosis using undestructive methods (Roger
  and IEC) and artificial neural network for dissolved gas analysis applied on
  the functional transformer in the Algerian north-eastern: a comparative study","Nowadays, power transformer aging and failures are viewed with great
attention in power transmission industry. Dissolved gas analysis (DGA) is
classified among the biggest widely used methods used within the context of
asset management policy to detect the incipient faults in their earlier stage
in power transformers. Up to now, several procedures have been employed for the
lecture of DGA results. Among these useful means, we find Key Gases, Rogers
Ratios, IEC Ratios, the historical technique less used today Doernenburg
Ratios, the two types of Duval Pentagons methods, several versions of the Duval
Triangles method and Logarithmic Nomograph. Problem. DGA data extracted from
different units in service served to verify the ability and reliability of
these methods in assessing the state of health of the power transformer. Aim.
An improving the quality of diagnostics of electrical power transformer by
artificial neural network tools based on two conventional methods in the case
of a functional power transformer at S\'etif province in East North of Algeria.
Methodology. Design an inelegant tool for power transformer diagnosis using
neural networks based on traditional methods IEC and Rogers, which allows to
early detection faults, to increase the reliability, of the entire electrical
energy system from transport to consumers and improve a continuity and quality
of service. Results. The solution of the problem was carried out by using
feed-forward back-propagation neural networks implemented in MATLAB-Simulink
environment. Four real power transformers working under different environment
and climate conditions such as: desert, humid, cold were taken into account.
The practical results of the diagnosis of these power transformers by the DGA
are presented. Practical value.....",arxiv
http://arxiv.org/abs/1710.08299v1,2017-09-26T05:20:12Z,2017-09-26T05:20:12Z,An In-field Automatic Wheat Disease Diagnosis System,"Crop diseases are responsible for the major production reduction and economic
losses in agricultural industry world- wide. Monitoring for health status of
crops is critical to control the spread of diseases and implement effective
management. This paper presents an in-field automatic wheat disease diagnosis
system based on a weakly super- vised deep learning framework, i.e. deep
multiple instance learning, which achieves an integration of identification for
wheat diseases and localization for disease areas with only image-level
annotation for training images in wild conditions. Furthermore, a new in-field
image dataset for wheat disease, Wheat Disease Database 2017 (WDD2017), is
collected to verify the effectiveness of our system. Under two different
architectures, i.e. VGG-FCN-VD16 and VGG-FCN-S, our system achieves the mean
recognition accuracies of 97.95% and 95.12% respectively over 5-fold
cross-validation on WDD2017, exceeding the results of 93.27% and 73.00% by two
conventional CNN frameworks, i.e. VGG-CNN-VD16 and VGG-CNN-S. Experimental
results demonstrate that the proposed system outperforms conventional CNN
architectures on recognition accuracy under the same amount of parameters,
meanwhile main- taining accurate localization for corresponding disease areas.
Moreover, the proposed system has been packed into a real-time mobile app to
provide support for agricultural disease diagnosis.",arxiv
http://arxiv.org/abs/1810.12950v2,2019-03-17T22:56:50Z,2018-10-30T18:22:23Z,"Learning to serve: an experimental study for a new learning from
  demonstrations framework","Learning from demonstrations is an easy and intuitive way to show examples of
successful behavior to a robot. However, the fact that humans optimize or take
advantage of their body and not of the robot, usually called the embodiment
problem in robotics, often prevents industrial robots from executing the task
in a straightforward way. The shown movements often do not or cannot utilize
the degrees of freedom of the robot efficiently, and moreover suffer from
excessive execution errors. In this paper, we explore a variety of solutions
that address these shortcomings. In particular, we learn sparse movement
primitive parameters from several demonstrations of a successful table tennis
serve. The number of parameters learned using our procedure is independent of
the degrees of freedom of the robot. Moreover, they can be ranked according to
their importance in the regression task. Learning few parameters that are
ranked is a desirable feature to combat the curse of dimensionality in
Reinforcement Learning. Preliminary real robot experiments on the Barrett WAM
for a table tennis serve using the learned movement primitives show that the
representation can capture successfully the style of the movement with few
parameters.",arxiv
http://arxiv.org/abs/2101.08828v1,2021-01-21T19:53:03Z,2021-01-21T19:53:03Z,E-commerce warehousing: learning a storage policy,"E-commerce with major online retailers is changing the way people consume.
The goal of increasing delivery speed while remaining cost-effective poses
significant new challenges for supply chains as they race to satisfy the
growing and fast-changing demand. In this paper, we consider a warehouse with a
Robotic Mobile Fulfillment System (RMFS), in which a fleet of robots stores and
retrieves shelves of items and brings them to human pickers. To adapt to
changing demand, uncertainty, and differentiated service (e.g., prime vs.
regular), one can dynamically modify the storage allocation of a shelf. The
objective is to define a dynamic storage policy to minimise the average cycle
time used by the robots to fulfil requests. We propose formulating this system
as a Partially Observable Markov Decision Process, and using a Deep Q-learning
agent from Reinforcement Learning, to learn an efficient real-time storage
policy that leverages repeated experiences and insightful forecasts using
simulations. Additionally, we develop a rollout strategy to enhance our method
by leveraging more information available at a given time step. Using
simulations to compare our method to traditional storage rules used in the
industry showed preliminary results up to 14\% better in terms of travelling
times.",arxiv
http://arxiv.org/abs/1802.02312v2,2018-06-05T03:12:06Z,2018-02-07T05:32:59Z,"Machine Learning-Based Prototyping of Graphical User Interfaces for
  Mobile Apps","It is common practice for developers of user-facing software to transform a
mock-up of a graphical user interface (GUI) into code. This process takes place
both at an application's inception and in an evolutionary context as GUI
changes keep pace with evolving features. Unfortunately, this practice is
challenging and time-consuming. In this paper, we present an approach that
automates this process by enabling accurate prototyping of GUIs via three
tasks: detection, classification, and assembly. First, logical components of a
GUI are detected from a mock-up artifact using either computer vision
techniques or mock-up metadata. Then, software repository mining, automated
dynamic analysis, and deep convolutional neural networks are utilized to
accurately classify GUI-components into domain-specific types (e.g.,
toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates
a suitable hierarchical GUI structure from which a prototype application can be
automatically assembled. We implemented this approach for Android in a system
called ReDraw. Our evaluation illustrates that ReDraw achieves an average
GUI-component classification accuracy of 91% and assembles prototype
applications that closely mirror target mock-ups in terms of visual affinity
while exhibiting reasonable code structure. Interviews with industrial
practitioners illustrate ReDraw's potential to improve real development
workflows.",arxiv
http://arxiv.org/abs/2102.09200v1,2021-02-18T07:47:43Z,2021-02-18T07:47:43Z,"Unsupervised Clustering of Time Series Signals using Neuromorphic
  Energy-Efficient Temporal Neural Networks","Unsupervised time series clustering is a challenging problem with diverse
industrial applications such as anomaly detection, bio-wearables, etc. These
applications typically involve small, low-power devices on the edge that
collect and process real-time sensory signals. State-of-the-art time-series
clustering methods perform some form of loss minimization that is extremely
computationally intensive from the perspective of edge devices. In this work,
we propose a neuromorphic approach to unsupervised time series clustering based
on Temporal Neural Networks that is capable of ultra low-power, continuous
online learning. We demonstrate its clustering performance on a subset of UCR
Time Series Archive datasets. Our results show that the proposed approach
either outperforms or performs similarly to most of the existing algorithms
while being far more amenable for efficient hardware implementation. Our
hardware assessment analysis shows that in 7 nm CMOS the proposed architecture,
on average, consumes only about 0.005 mm^2 die area and 22 uW power and can
process each signal with about 5 ns latency.",arxiv
http://arxiv.org/abs/2107.12024v1,2021-07-26T08:29:18Z,2021-07-26T08:29:18Z,"Leaf-FM: A Learnable Feature Generation Factorization Machine for
  Click-Through Rate Prediction","Click-through rate (CTR) prediction plays important role in personalized
advertising and recommender systems. Though many models have been proposed such
as FM, FFM and DeepFM in recent years, feature engineering is still a very
important way to improve the model performance in many applications because
using raw features can rarely lead to optimal results. For example, the
continuous features are usually transformed to the power forms by adding a new
feature to allow it to easily form non-linear functions of the feature.
However, this kind of feature engineering heavily relies on peoples experience
and it is both time consuming and labor consuming. On the other side, concise
CTR model with both fast online serving speed and good model performance is
critical for many real life applications. In this paper, we propose LeafFM
model based on FM to generate new features from the original feature embedding
by learning the transformation functions automatically. We also design three
concrete Leaf-FM models according to the different strategies of combing the
original and the generated features. Extensive experiments are conducted on
three real-world datasets and the results show Leaf-FM model outperforms
standard FMs by a large margin. Compared with FFMs, Leaf-FM can achieve
significantly better performance with much less parameters. In Avazu and
Malware dataset, add version Leaf-FM achieves comparable performance with some
deep learning based models such as DNN and AutoInt. As an improved FM model,
Leaf-FM has the same computation complexity with FM in online serving phase and
it means Leaf-FM is applicable in many industry applications because of its
better performance and high computation efficiency.",arxiv
http://arxiv.org/abs/2103.03544v2,2021-03-10T21:27:51Z,2021-03-05T08:52:31Z,Challenges of engineering safe and secure highly automated vehicles,"After more than a decade of intense focus on automated vehicles, we are still
facing huge challenges for the vision of fully autonomous driving to become a
reality. The same ""disillusionment"" is true in many other domains, in which
autonomous Cyber-Physical Systems (CPS) could considerably help to overcome
societal challenges and be highly beneficial to society and individuals. Taking
the automotive domain, i.e. highly automated vehicles (HAV), as an example,
this paper sets out to summarize the major challenges that are still to
overcome for achieving safe, secure, reliable and trustworthy highly automated
resp. autonomous CPS. We constrain ourselves to technical challenges,
acknowledging the importance of (legal) regulations, certification,
standardization, ethics, and societal acceptance, to name but a few, without
delving deeper into them as this is beyond the scope of this paper. Four
challenges have been identified as being the main obstacles to realizing HAV:
Realization of continuous, post-deployment systems improvement, handling of
uncertainties and incomplete information, verification of HAV with machine
learning components, and prediction. Each of these challenges is described in
detail, including sub-challenges and, where appropriate, possible approaches to
overcome them. By working together in a common effort between industry and
academy and focusing on these challenges, the authors hope to contribute to
overcome the ""disillusionment"" for realizing HAV.",arxiv
http://arxiv.org/abs/2009.00945v1,2020-09-02T10:57:28Z,2020-09-02T10:57:28Z,"LAVARNET: Neural Network Modeling of Causal Variable Relationships for
  Multivariate Time Series Forecasting","Multivariate time series forecasting is of great importance to many
scientific disciplines and industrial sectors. The evolution of a multivariate
time series depends on the dynamics of its variables and the connectivity
network of causal interrelationships among them. Most of the existing time
series models do not account for the causal effects among the system's
variables and even if they do they rely just on determining the
between-variables causality network. Knowing the structure of such a complex
network and even more specifically knowing the exact lagged variables that
contribute to the underlying process is crucial for the task of multivariate
time series forecasting. The latter is a rather unexplored source of
information to leverage. In this direction, here a novel neural network-based
architecture is proposed, termed LAgged VAriable Representation NETwork
(LAVARNET), which intrinsically estimates the importance of lagged variables
and combines high dimensional latent representations of them to predict future
values of time series. Our model is compared with other baseline and state of
the art neural network architectures on one simulated data set and four real
data sets from meteorology, music, solar activity, and finance areas. The
proposed architecture outperforms the competitive architectures in most of the
experiments.",arxiv
http://arxiv.org/abs/2006.08312v1,2020-06-15T12:04:47Z,2020-06-15T12:04:47Z,"Algebraic Ground Truth Inference: Non-Parametric Estimation of Sample
  Errors by AI Algorithms","Binary classification is widely used in ML production systems. Monitoring
classifiers in a constrained event space is well known. However, real world
production systems often lack the ground truth these methods require. Privacy
concerns may also require that the ground truth needed to evaluate the
classifiers cannot be made available. In these autonomous settings,
non-parametric estimators of performance are an attractive solution. They do
not require theoretical models about how the classifiers made errors in any
given sample. They just estimate how many errors there are in a sample of an
industrial or robotic datastream. We construct one such non-parametric
estimator of the sample errors for an ensemble of weak binary classifiers. Our
approach uses algebraic geometry to reformulate the self-assessment problem for
ensembles of binary classifiers as an exact polynomial system. The polynomial
formulation can then be used to prove - as an algebraic geometry algorithm -
that no general solution to the self-assessment problem is possible. However,
specific solutions are possible in settings where the engineering context puts
the classifiers close to independent errors. The practical utility of the
method is illustrated on a real-world dataset from an online advertising
campaign and a sample of common classification benchmarks. The accuracy
estimators in the experiments where we have ground truth are better than one
part in a hundred. The online advertising campaign data, where we do not have
ground truth data, is verified by an internal consistency approach whose
validity we conjecture as an algebraic geometry theorem. We call this approach
- algebraic ground truth inference.",arxiv
http://arxiv.org/abs/2106.00314v2,2021-06-08T09:02:40Z,2021-06-01T08:43:31Z,Dual Graph enhanced Embedding Neural Network for CTR Prediction,"CTR prediction, which aims to estimate the probability that a user will click
an item, plays a crucial role in online advertising and recommender system.
Feature interaction modeling based and user interest mining based methods are
the two kinds of most popular techniques that have been extensively explored
for many years and have made great progress for CTR prediction. However, (1)
feature interaction based methods which rely heavily on the co-occurrence of
different features, may suffer from the feature sparsity problem (i.e., many
features appear few times); (2) user interest mining based methods which need
rich user behaviors to obtain user's diverse interests, are easy to encounter
the behavior sparsity problem (i.e., many users have very short behavior
sequences). To solve these problems, we propose a novel module named Dual Graph
enhanced Embedding, which is compatible with various CTR prediction models to
alleviate these two problems. We further propose a Dual Graph enhanced
Embedding Neural Network (DG-ENN) for CTR prediction. Dual Graph enhanced
Embedding exploits the strengths of graph representation with two carefully
designed learning strategies (divide-and-conquer, curriculum-learning-inspired
organized learning) to refine the embedding. We conduct comprehensive
experiments on three real-world industrial datasets. The experimental results
show that our proposed DG-ENN significantly outperforms state-of-the-art CTR
prediction models. Moreover, when applying to state-of-the-art CTR prediction
models, Dual graph enhanced embedding always obtains better performance.
Further case studies prove that our proposed dual graph enhanced embedding
could alleviate the feature sparsity and behavior sparsity problems. Our
framework will be open-source based on MindSpore in the near future.",arxiv
http://arxiv.org/abs/2109.02629v1,2021-09-01T08:06:23Z,2021-09-01T08:06:23Z,"An Efficient Deep Learning Approach Using Improved Generative
  Adversarial Networks for Incomplete Information Completion of Self-driving","Autonomous driving is the key technology of intelligent logistics in
Industrial Internet of Things (IIoT). In autonomous driving, the appearance of
incomplete point clouds losing geometric and semantic information is inevitable
owing to limitations of occlusion, sensor resolution, and viewing angle when
the Light Detection And Ranging (LiDAR) is applied. The emergence of incomplete
point clouds, especially incomplete vehicle point clouds, would lead to the
reduction of the accuracy of autonomous driving vehicles in object detection,
traffic alert, and collision avoidance. Existing point cloud completion
networks, such as Point Fractal Network (PF-Net), focus on the accuracy of
point cloud completion, without considering the efficiency of inference
process, which makes it difficult for them to be deployed for vehicle point
cloud repair in autonomous driving. To address the above problem, in this
paper, we propose an efficient deep learning approach to repair incomplete
vehicle point cloud accurately and efficiently in autonomous driving. In the
proposed method, an efficient downsampling algorithm combining incremental
sampling and one-time sampling is presented to improves the inference speed of
the PF-Net based on Generative Adversarial Network (GAN). To evaluate the
performance of the proposed method, a real dataset is used, and an autonomous
driving scene is created, where three incomplete vehicle point clouds with 5
different sizes are set for three autonomous driving situations. The improved
PF-Net can achieve the speedups of over 19x with almost the same accuracy when
compared to the original PF-Net. Experimental results demonstrate that the
improved PF-Net can be applied to efficiently complete vehicle point clouds in
autonomous driving.",arxiv
http://arxiv.org/abs/1903.12110v1,2019-03-28T16:51:17Z,2019-03-28T16:51:17Z,Building Automated Survey Coders via Interactive Machine Learning,"Software systems trained via machine learning to automatically classify
open-ended answers (a.k.a. verbatims) are by now a reality. Still, their
adoption in the survey coding industry has been less widespread than it might
have been. Among the factors that have hindered a more massive takeup of this
technology are the effort involved in manually coding a sufficient amount of
training data, the fact that small studies do not seem to justify this effort,
and the fact that the process needs to be repeated anew when brand new coding
tasks arise. In this paper we will argue for an approach to building verbatim
classifiers that we will call ""Interactive Learning"", and that addresses all
the above problems. We will show that, for the same amount of training effort,
interactive learning delivers much better coding accuracy than standard
""non-interactive"" learning. This is especially true when the amount of data we
are willing to manually code is small, which makes this approach attractive
also for small-scale studies. Interactive learning also lends itself to reusing
previously trained classifiers for dealing with new (albeit related) coding
tasks. Interactive learning also integrates better in the daily workflow of the
survey specialist, and delivers a better user experience overall.",arxiv
http://arxiv.org/abs/2005.03077v1,2020-05-06T19:06:51Z,2020-05-06T19:06:51Z,"AVAC: A Machine Learning based Adaptive RRAM Variability-Aware
  Controller for Edge Devices","Recently, the Edge Computing paradigm has gained significant popularity both
in industry and academia. Researchers now increasingly target to improve
performance and reduce energy consumption of such devices. Some recent efforts
focus on using emerging RRAM technologies for improving energy efficiency,
thanks to their no leakage property and high integration density. As the
complexity and dynamism of applications supported by such devices escalate, it
has become difficult to maintain ideal performance by static RRAM controllers.
Machine Learning provides a promising solution for this, and hence, this work
focuses on extending such controllers to allow dynamic parameter updates. In
this work we propose an Adaptive RRAM Variability-Aware Controller, AVAC, which
periodically updates Wait Buffer and batch sizes using on-the-fly learning
models and gradient ascent. AVAC allows Edge devices to adapt to different
applications and their stages, to improve computation performance and reduce
energy consumption. Simulations demonstrate that the proposed model can provide
up to 29% increase in performance and 19% decrease in energy, compared to
static controllers, using traces of real-life healthcare applications on a
Raspberry-Pi based Edge deployment.",arxiv
http://arxiv.org/abs/1912.04444v1,2019-12-10T01:36:21Z,2019-12-10T01:36:21Z,Practice of Efficient Data Collection via Crowdsourcing at Large-Scale,"Modern machine learning algorithms need large datasets to be trained.
Crowdsourcing has become a popular approach to label large datasets in a
shorter time as well as at a lower cost comparing to that needed for a limited
number of experts. However, as crowdsourcing performers are non-professional
and vary in levels of expertise, such labels are much noisier than those
obtained from experts. For this reason, in order to collect good quality data
within a limited budget special techniques such as incremental relabelling,
aggregation and pricing need to be used. We make an introduction to data
labeling via public crowdsourcing marketplaces and present key components of
efficient label collection. We show how to choose one of real label collection
tasks, experiment with selecting settings for the labelling process, and launch
label collection project at Yandex.Toloka, one of the largest crowdsourcing
marketplace. The projects will be run on real crowds. We also present main
algorithms for aggregation, incremental relabelling, and pricing in
crowdsourcing. In particular, we, first, discuss how to connect these three
components to build an efficient label collection process; and, second, share
rich industrial experiences of applying these algorithms and constructing
large-scale label collection pipelines (emphasizing best practices and common
pitfalls).",arxiv
http://arxiv.org/abs/2101.07831v1,2021-01-19T19:29:38Z,2021-01-19T19:29:38Z,"Multi-Task Network Pruning and Embedded Optimization for Real-time
  Deployment in ADAS","Camera-based Deep Learning algorithms are increasingly needed for perception
in Automated Driving systems. However, constraints from the automotive industry
challenge the deployment of CNNs by imposing embedded systems with limited
computational resources. In this paper, we propose an approach to embed a
multi-task CNN network under such conditions on a commercial prototype
platform, i.e. a low power System on Chip (SoC) processing four surround-view
fisheye cameras at 10 FPS.
  The first focus is on designing an efficient and compact multi-task network
architecture. Secondly, a pruning method is applied to compress the CNN,
helping to reduce the runtime and memory usage by a factor of 2 without
lowering the performances significantly. Finally, several embedded optimization
techniques such as mixed-quantization format usage and efficient data transfers
between different memory areas are proposed to ensure real-time execution and
avoid bandwidth bottlenecks. The approach is evaluated on the hardware
platform, considering embedded detection performances, runtime and memory
bandwidth. Unlike most works from the literature that focus on classification
task, we aim here to study the effect of pruning and quantization on a compact
multi-task network with object detection, semantic segmentation and soiling
detection tasks.",arxiv
http://arxiv.org/abs/2110.13465v1,2021-10-26T08:00:03Z,2021-10-26T08:00:03Z,"CS-Rep: Making Speaker Verification Networks Embracing
  Re-parameterization","Automatic speaker verification (ASV) systems, which determine whether two
speeches are from the same speaker, mainly focus on verification accuracy while
ignoring inference speed. However, in real applications, both inference speed
and verification accuracy are essential. This study proposes cross-sequential
re-parameterization (CS-Rep), a novel topology re-parameterization strategy for
multi-type networks, to increase the inference speed and verification accuracy
of models. CS-Rep solves the problem that existing re-parameterization methods
are unsuitable for typical ASV backbones. When a model applies CS-Rep, the
training-period network utilizes a multi-branch topology to capture speaker
information, whereas the inference-period model converts to a time-delay neural
network (TDNN)-like plain backbone with stacked TDNN layers to achieve the fast
inference speed. Based on CS-Rep, an improved TDNN with friendly test and
deployment called Rep-TDNN is proposed. Compared with the state-of-the-art
model ECAPA-TDNN, which is highly recognized in the industry, Rep-TDNN
increases the actual inference speed by about 50% and reduces the EER by 10%.
The code will be released.",arxiv
http://arxiv.org/abs/2009.03094v1,2020-09-07T13:27:06Z,2020-09-07T13:27:06Z,"Capturing dynamics of post-earnings-announcement drift using genetic
  algorithm-optimised supervised learning","While Post-Earnings-Announcement Drift (PEAD) is one of the most studied
stock market anomalies, the current literature is often limited in explaining
this phenomenon by a small number of factors using simpler regression methods.
In this paper, we use a machine learning based approach instead, and aim to
capture the PEAD dynamics using data from a large group of stocks and a wide
range of both fundamental and technical factors. Our model is built around the
Extreme Gradient Boosting (XGBoost) and uses a long list of engineered input
features based on quarterly financial announcement data from 1,106 companies in
the Russell 1000 index between 1997 and 2018. We perform numerous experiments
on PEAD predictions and analysis and have the following contributions to the
literature. First, we show how Post-Earnings-Announcement Drift can be analysed
using machine learning methods and demonstrate such methods' prowess in
producing credible forecasting on the drift direction. It is the first time
PEAD dynamics are studied using XGBoost. We show that the drift direction is in
fact driven by different factors for stocks from different industrial sectors
and in different quarters and XGBoost is effective in understanding the
changing drivers. Second, we show that an XGBoost well optimised by a Genetic
Algorithm can help allocate out-of-sample stocks to form portfolios with higher
positive returns to long and portfolios with lower negative returns to short, a
finding that could be adopted in the process of developing market neutral
strategies. Third, we show how theoretical event-driven stock strategies have
to grapple with ever changing market prices in reality, reducing their
effectiveness. We present a tactic to remedy the difficulty of buying into a
moving market when dealing with PEAD signals.",arxiv
http://arxiv.org/abs/1211.0906v2,2013-10-26T09:00:50Z,2012-11-05T16:15:16Z,Algorithm Runtime Prediction: Methods & Evaluation,"Perhaps surprisingly, it is possible to predict how long an algorithm will
take to run on a previously unseen input, using machine learning techniques to
build a model of the algorithm's runtime as a function of problem-specific
instance features. Such models have important applications to algorithm
analysis, portfolio-based algorithm selection, and the automatic configuration
of parameterized algorithms. Over the past decade, a wide variety of techniques
have been studied for building such models. Here, we describe extensions and
improvements of existing models, new families of models, and -- perhaps most
importantly -- a much more thorough treatment of algorithm parameters as model
inputs. We also comprehensively describe new and existing features for
predicting algorithm runtime for propositional satisfiability (SAT), travelling
salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate
these innovations through the largest empirical analysis of its kind, comparing
to a wide range of runtime modelling techniques from the literature. Our
experiments consider 11 algorithms and 35 instance distributions; they also
span a very wide range of SAT, MIP, and TSP instances, with the least
structured having been generated uniformly at random and the most structured
having emerged from real industrial applications. Overall, we demonstrate that
our new models yield substantially better runtime predictions than previous
approaches in terms of their generalization to new problem instances, to new
algorithms from a parameterized space, and to both simultaneously.",arxiv
http://arxiv.org/abs/1804.11207v1,2018-04-30T14:03:22Z,2018-04-30T14:03:22Z,An Anti-fraud System for Car Insurance Claim Based on Visual Evidence,"Automatically scene understanding using machine learning algorithms has been
widely applied to different industries to reduce the cost of manual labor.
Nowadays, insurance companies launch express vehicle insurance claim and
settlement by allowing customers uploading pictures taken by mobile devices.
This kind of insurance claim is treated as small claim and can be processed
either manually or automatically in a quick fashion. However, due to the
increasing amount of claims every day, system or people are likely to be fooled
by repeated claims for identical case leading to big lost to insurance
companies.Thus, an anti-fraud checking before processing the claim is
necessary. We create the first data set of car damage images collected from
internet and local parking lots. In addition, we proposed an approach to
generate robust deep features by locating the damages accurately and
efficiently in the images. The state-of-the-art real-time object detector YOLO
\cite{redmon2016you}is modified to train and discover damage region as an
important part of the pipeline. Both local and global deep features are
extracted using VGG model\cite{Simonyan14c}, which are fused later for more
robust system performance. Experiments show our approach is effective in
preventing fraud claims as well as meet the requirement to speed up the
insurance claim prepossessing.",arxiv
http://arxiv.org/abs/2107.08662v3,2021-08-11T08:15:43Z,2021-07-19T07:51:31Z,"A Queueing-Theoretic Framework for Vehicle Dispatching in Dynamic
  Car-Hailing [technical report]","With the rapid development of smart mobile devices, the car-hailing platforms
(e.g., Uber or Lyft) have attracted much attention from both the academia and
the industry. In this paper, we consider an important dynamic car-hailing
problem, namely \textit{maximum revenue vehicle dispatching} (MRVD), in which
rider requests dynamically arrive and drivers need to serve as many riders as
possible such that the entire revenue of the platform is maximized. We prove
that the MRVD problem is NP-hard and intractable. In addition, the dynamic
car-hailing platforms have no information of the future riders, which makes the
problem even harder. To handle the MRVD problem, we propose a queueing-based
vehicle dispatching framework, which first uses existing machine learning
algorithms to predict the future vehicle demand of each region, then estimates
the idle time periods of drivers through a queueing model for each region. With
the information of the predicted vehicle demands and estimated idle time
periods of drivers, we propose two batch-based vehicle dispatching algorithms
to efficiently assign suitable drivers to riders such that the expected overall
revenue of the platform is maximized during each batch processing. Through
extensive experiments, we demonstrate the efficiency and effectiveness of our
proposed approaches over both real and synthetic datasets.",arxiv
http://arxiv.org/abs/1809.07053v1,2018-09-19T08:17:54Z,2018-09-19T08:17:54Z,NAIS: Neural Attentive Item Similarity Model for Recommendation,"Item-to-item collaborative filtering (aka. item-based CF) has been long used
for building recommender systems in industrial settings, owing to its
interpretability and efficiency in real-time personalization. It builds a
user's profile as her historically interacted items, recommending new items
that are similar to the user's profile. As such, the key to an item-based CF
method is in the estimation of item similarities. Early approaches use
statistical measures such as cosine similarity and Pearson coefficient to
estimate item similarities, which are less accurate since they lack tailored
optimization for the recommendation task. In recent years, several works
attempt to learn item similarities from data, by expressing the similarity as
an underlying model and estimating model parameters by optimizing a
recommendation-aware objective function. While extensive efforts have been made
to use shallow linear models for learning item similarities, there has been
relatively less work exploring nonlinear neural network models for item-based
CF.
  In this work, we propose a neural network model named Neural Attentive Item
Similarity model (NAIS) for item-based CF. The key to our design of NAIS is an
attention network, which is capable of distinguishing which historical items in
a user profile are more important for a prediction. Compared to the
state-of-the-art item-based CF method Factored Item Similarity Model (FISM),
our NAIS has stronger representation power with only a few additional
parameters brought by the attention network. Extensive experiments on two
public benchmarks demonstrate the effectiveness of NAIS. This work is the first
attempt that designs neural network models for item-based CF, opening up new
research possibilities for future developments of neural recommender systems.",arxiv
http://arxiv.org/abs/2101.07594v1,2021-01-19T12:42:58Z,2021-01-19T12:42:58Z,"Real-Time Limited-View CT Inpainting and Reconstruction with Dual Domain
  Based on Spatial Information","Low-dose Computed Tomography is a common issue in reality. Current reduction,
sparse sampling and limited-view scanning can all cause it. Between them,
limited-view CT is general in the industry due to inevitable mechanical and
physical limitation. However, limited-view CT can cause serious imaging problem
on account of its massive information loss. Thus, we should effectively utilize
the scant prior information to perform completion. It is an undeniable fact
that CT imaging slices are extremely dense, which leads to high continuity
between successive images. We realized that fully exploit the spatial
correlation between consecutive frames can significantly improve restoration
results in video inpainting. Inspired by this, we propose a deep learning-based
three-stage algorithm that hoist limited-view CT imaging quality based on
spatial information. In stage one, to better utilize prior information in the
Radon domain, we design an adversarial autoencoder to complement the Radon
data. In the second stage, a model is built to perform inpainting based on
spatial continuity in the image domain. At this point, we have roughly restored
the imaging, while its texture still needs to be finely repaired. Hence, we
propose a model to accurately restore the image in stage three, and finally
achieve an ideal inpainting result. In addition, we adopt FBP instead of
SART-TV to make our algorithm more suitable for real-time use. In the
experiment, we restore and reconstruct the Radon data that has been cut the
rear one-third part, they achieve PSNR of 40.209, SSIM of 0.943, while
precisely present the texture.",arxiv
http://arxiv.org/abs/1708.04106v3,2018-03-15T03:35:52Z,2017-08-14T13:06:15Z,"Rocket Launching: A Universal and Efficient Framework for Training
  Well-performing Light Net","Models applied on real time response task, like click-through rate (CTR)
prediction model, require high accuracy and rigorous response time. Therefore,
top-performing deep models of high depth and complexity are not well suited for
these applications with the limitations on the inference time. In order to
further improve the neural networks' performance given the time and
computational limitations, we propose an approach that exploits a cumbersome
net to help train the lightweight net for prediction. We dub the whole process
rocket launching, where the cumbersome booster net is used to guide the
learning of the target light net throughout the whole training process. We
analyze different loss functions aiming at pushing the light net to behave
similarly to the booster net, and adopt the loss with best performance in our
experiments. We use one technique called gradient block to improve the
performance of the light net and booster net further. Experiments on benchmark
datasets and real-life industrial advertisement data present that our light
model can get performance only previously achievable with more complex models.",arxiv
http://arxiv.org/abs/2007.09868v1,2020-07-20T03:40:51Z,2020-07-20T03:40:51Z,"Attention Sequence to Sequence Model for Machine Remaining Useful Life
  Prediction","Accurate estimation of remaining useful life (RUL) of industrial equipment
can enable advanced maintenance schedules, increase equipment availability and
reduce operational costs. However, existing deep learning methods for RUL
prediction are not completely successful due to the following two reasons.
First, relying on a single objective function to estimate the RUL will limit
the learned representations and thus affect the prediction accuracy. Second,
while longer sequences are more informative for modelling the sensor dynamics
of equipment, existing methods are less effective to deal with very long
sequences, as they mainly focus on the latest information. To address these two
problems, we develop a novel attention-based sequence to sequence with
auxiliary task (ATS2S) model. In particular, our model jointly optimizes both
reconstruction loss to empower our model with predictive capabilities (by
predicting next input sequence given current input sequence) and RUL prediction
loss to minimize the difference between the predicted RUL and actual RUL.
Furthermore, to better handle longer sequence, we employ the attention
mechanism to focus on all the important input information during training
process. Finally, we propose a new dual-latent feature representation to
integrate the encoder features and decoder hidden states, to capture rich
semantic information in data. We conduct extensive experiments on four real
datasets to evaluate the efficacy of the proposed method. Experimental results
show that our proposed method can achieve superior performance over 13
state-of-the-art methods consistently.",arxiv
http://arxiv.org/abs/2110.08633v2,2021-10-23T18:04:29Z,2021-10-16T18:13:57Z,Hydra: A System for Large Multi-Model Deep Learning,"Training deep learning (DL) models that do not fit into the memory of a
single GPU is a vexed process, forcing users to procure multiple GPUs to adopt
model-parallel execution. Unfortunately, sequential dependencies in neural
architectures often block efficient multi-device training, leading to
suboptimal performance. We present 'model spilling', a technique aimed at
models such as Transformers and CNNs to move groups of layers, or shards,
between DRAM and GPU memory, thus enabling arbitrarily large models to be
trained even on just one GPU. We then present a set of novel techniques
leveraging spilling to raise efficiency for multi-model training workloads such
as model selection: a new hybrid of task- and model-parallelism, a new shard
scheduling heuristic, and 'double buffering' to hide latency. We prototype our
ideas into a system we call HYDRA to support seamless single-model and
multi-model training of large DL models. Experiments with real benchmark
workloads show that HYDRA is over 7x faster than regular model parallelism and
over 50% faster than state-of-the-art industrial tools for pipeline
parallelism.",arxiv
http://arxiv.org/abs/2101.04285v1,2021-01-12T04:12:18Z,2021-01-12T04:12:18Z,"Explainable Deep Behavioral Sequence Clustering for Transaction Fraud
  Detection","In e-commerce industry, user behavior sequence data has been widely used in
many business units such as search and merchandising to improve their products.
However, it is rarely used in financial services not only due to its 3V
characteristics - i.e. Volume, Velocity and Variety - but also due to its
unstructured nature. In this paper, we propose a Financial Service scenario
Deep learning based Behavior data representation method for Clustering
(FinDeepBehaviorCluster) to detect fraudulent transactions. To utilize the
behavior sequence data, we treat click stream data as event sequence, use time
attention based Bi-LSTM to learn the sequence embedding in an unsupervised
fashion, and combine them with intuitive features generated by risk experts to
form a hybrid feature representation. We also propose a GPU powered HDBSCAN
(pHDBSCAN) algorithm, which is an engineering optimization for the original
HDBSCAN algorithm based on FAISS project, so that clustering can be carried out
on hundreds of millions of transactions within a few minutes. The computation
efficiency of the algorithm has increased 500 times compared with the original
implementation, which makes flash fraud pattern detection feasible. Our
experimental results show that the proposed FinDeepBehaviorCluster framework is
able to catch missed fraudulent transactions with considerable business values.
In addition, rule extraction method is applied to extract patterns from risky
clusters using intuitive features, so that narrative descriptions can be
attached to the risky clusters for case investigation, and unknown risk
patterns can be mined for real-time fraud detection. In summary,
FinDeepBehaviorCluster as a complementary risk management strategy to the
existing real-time fraud detection engine, can further increase our fraud
detection and proactive risk defense capabilities.",arxiv
http://arxiv.org/abs/2108.04058v1,2021-08-05T12:59:38Z,2021-08-05T12:59:38Z,"An Interpretable Probabilistic Model for Short-Term Solar Power
  Forecasting Using Natural Gradient Boosting","The stochastic nature of photovoltaic (PV) power has led both academia and
industry to a large amount of research work aiming at the development of
accurate PV power forecasting models. However, most of those models are based
on machine learning algorithms and are considered as black boxes which do not
provide any insight or explanation about their predictions. Therefore, their
direct implementation in environments, where transparency is required, and the
trust associated with their predictions may be questioned. To this end, we
propose a two stage probabilistic forecasting framework able to generate highly
accurate, reliable, and sharp forecasts yet offering full transparency on both
the point forecasts and the prediction intervals (PIs). In the first stage, we
exploit natural gradient boosting (NGBoost) for yielding probabilistic
forecasts while in the second stage, we calculate the Shapley additive
explanation (SHAP) values in order to fully understand why a prediction was
made. To highlight the performance and the applicability of the proposed
framework, real data from two PV parks located in Southern Germany are
employed. Initially, the natural gradient boosting is thoroughly compared with
two state-of-the-art algorithms, namely Gaussian process and lower upper bound
estimation, in a wide range of forecasting metrics. Secondly, a detailed
analysis of the model's complex nonlinear relationships and interaction effects
between the various features is presented. The latter allows us to interpret
the model, identify some learned physical properties, explain individual
predictions, reduce the computational requirements for the training without
jeopardizing the model accuracy, detect possible bugs, and gain trust in the
model. Finally, we conclude that the model was able to develop nonlinear
relationships following human logic and intuition based on learned physical
properties.",arxiv
http://arxiv.org/abs/2001.09550v1,2020-01-27T01:07:15Z,2020-01-27T01:07:15Z,"Experimental Evaluation of Human Motion Prediction: Toward Safe and
  Efficient Human Robot Collaboration","Human motion prediction is non-trivial in modern industrial settings.
Accurate prediction of human motion can not only improve efficiency in human
robot collaboration, but also enhance human safety in close proximity to
robots. Among existing prediction models, the parameterization and
identification methods of those models vary. It remains unclear what is the
necessary parameterization of a prediction model, whether online adaptation of
the model is necessary, and whether prediction can help improve safety and
efficiency during human robot collaboration. These problems result from the
difficulty to quantitatively evaluate various prediction models in a
closed-loop fashion in real human-robot interaction settings. This paper
develops a method to evaluate the closed-loop performance of different
prediction models. In particular, we compare models with different
parameterizations and models with or without online parameter adaptation.
Extensive experiments were conducted on a human robot collaboration platform.
The experimental results demonstrated that human motion prediction
significantly enhanced the collaboration efficiency and human safety. Adaptable
prediction models that were parameterized by neural networks achieved the best
performance.",arxiv
http://arxiv.org/abs/2006.06820v2,2020-07-17T18:48:24Z,2020-06-11T21:05:38Z,"Calendar Graph Neural Networks for Modeling Time Structures in
  Spatiotemporal User Behaviors","User behavior modeling is important for industrial applications such as
demographic attribute prediction, content recommendation, and target
advertising. Existing methods represent behavior log as a sequence of adopted
items and find sequential patterns; however, concrete location and time
information in the behavior log, reflecting dynamic and periodic patterns,
joint with the spatial dimension, can be useful for modeling users and
predicting their characteristics. In this work, we propose a novel model based
on graph neural networks for learning user representations from spatiotemporal
behavior data. A behavior log comprises a sequence of sessions; and a session
has a location, start time, end time, and a sequence of adopted items. Our
model's architecture incorporates two networked structures. One is a tripartite
network of items, sessions, and locations. The other is a hierarchical calendar
network of hour, week, and weekday nodes. It first aggregates embeddings of
location and items into session embeddings via the tripartite network, and then
generates user embeddings from the session embeddings via the calendar
structure. The user embeddings preserve spatial patterns and temporal patterns
of a variety of periodicity (e.g., hourly, weekly, and weekday patterns). It
adopts the attention mechanism to model complex interactions among the multiple
patterns in user behaviors. Experiments on real datasets (i.e., clicks on news
articles in a mobile app) show our approach outperforms strong baselines for
predicting missing demographic attributes.",arxiv
http://arxiv.org/abs/2009.03565v1,2020-09-08T07:54:44Z,2020-09-08T07:54:44Z,"A Deep Learning-Based Autonomous RobotManipulator for Sorting
  Application","Robot manipulation and grasping mechanisms have received considerable
attention in the recent past, leading to the development of wide range of
industrial applications. This paper proposes the development of an autonomous
robotic grasping system for object sorting application. RGB-D data is used by
the robot for performing object detection, pose estimation, trajectory
generation, and object sorting tasks. The proposed approach can also handle
grasping certain objects chosen by users. Trained convolutional neural networks
are used to perform object detection and determine the corresponding point
cloud cluster of the object to be grasped. From the selected point cloud data,
a grasp generator algorithm outputs potential grasps. A grasp filter then
scores these potential grasps, and the highest-scored grasp is chosen to
execute on a real robot. A motion planner generates collision-free trajectories
to execute the chosen grasp. The experiments on AUBO robotic manipulator show
the potentials of the proposed approach in the context of autonomous object
sorting with robust and fast sorting performance.",arxiv
http://arxiv.org/abs/2110.03865v1,2021-10-08T02:45:47Z,2021-10-08T02:45:47Z,Stable Prediction on Graphs with Agnostic Distribution Shift,"Graph is a flexible and effective tool to represent complex structures in
practice and graph neural networks (GNNs) have been shown to be effective on
various graph tasks with randomly separated training and testing data. In real
applications, however, the distribution of training graph might be different
from that of the test one (e.g., users' interactions on the user-item training
graph and their actual preference on items, i.e., testing environment, are
known to have inconsistencies in recommender systems). Moreover, the
distribution of test data is always agnostic when GNNs are trained. Hence, we
are facing the agnostic distribution shift between training and testing on
graph learning, which would lead to unstable inference of traditional GNNs
across different test environments. To address this problem, we propose a novel
stable prediction framework for GNNs, which permits both locally and globally
stable learning and prediction on graphs. In particular, since each node is
partially represented by its neighbors in GNNs, we propose to capture the
stable properties for each node (locally stable) by re-weighting the
information propagation/aggregation processes. For global stability, we propose
a stable regularizer that reduces the training losses on heterogeneous
environments and thus warping the GNNs to generalize well. We conduct extensive
experiments on several graph benchmarks and a noisy industrial recommendation
dataset that is collected from 5 consecutive days during a product promotion
festival. The results demonstrate that our method outperforms various SOTA GNNs
for stable prediction on graphs with agnostic distribution shift, including
shift caused by node labels and attributes.",arxiv
http://arxiv.org/abs/2006.11751v2,2020-06-23T00:41:58Z,2020-06-21T10:00:23Z,"Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with
  Asynchronous Reinforcement Learning","Increasing the scale of reinforcement learning experiments has allowed
researchers to achieve unprecedented results in both training sophisticated
agents for video games, and in sim-to-real transfer for robotics. Typically
such experiments rely on large distributed systems and require expensive
hardware setups, limiting wider access to this exciting area of research. In
this work we aim to solve this problem by optimizing the efficiency and
resource utilization of reinforcement learning algorithms instead of relying on
distributed computation. We present the ""Sample Factory"", a high-throughput
training system optimized for a single-machine setting. Our architecture
combines a highly efficient, asynchronous, GPU-based sampler with off-policy
correction techniques, allowing us to achieve throughput higher than $10^5$
environment frames/second on non-trivial control problems in 3D without
sacrificing sample efficiency. We extend Sample Factory to support self-play
and population-based training and apply these techniques to train highly
capable agents for a multiplayer first-person shooter game. The source code is
available at https://github.com/alex-petrenko/sample-factory",arxiv
http://arxiv.org/abs/2006.04381v1,2020-06-08T07:01:38Z,2020-06-08T07:01:38Z,Balance-Subsampled Stable Prediction,"In machine learning, it is commonly assumed that training and test data share
the same population distribution. However, this assumption is often violated in
practice because the sample selection bias may induce the distribution shift
from training data to test data. Such a model-agnostic distribution shift
usually leads to prediction instability across unknown test data. In this
paper, we propose a novel balance-subsampled stable prediction (BSSP) algorithm
based on the theory of fractional factorial design. It isolates the clear
effect of each predictor from the confounding variables. A design-theoretic
analysis shows that the proposed method can reduce the confounding effects
among predictors induced by the distribution shift, hence improve both the
accuracy of parameter estimation and prediction stability. Numerical
experiments on both synthetic and real-world data sets demonstrate that our
BSSP algorithm significantly outperforms the baseline methods for stable
prediction across unknown test data.",arxiv
http://arxiv.org/abs/2108.00620v2,2021-10-14T07:08:59Z,2021-08-02T03:54:39Z,Investigating Attention Mechanism in 3D Point Cloud Object Detection,"Object detection in three-dimensional (3D) space attracts much interest from
academia and industry since it is an essential task in AI-driven applications
such as robotics, autonomous driving, and augmented reality. As the basic
format of 3D data, the point cloud can provide detailed geometric information
about the objects in the original 3D space. However, due to 3D data's sparsity
and unorderedness, specially designed networks and modules are needed to
process this type of data. Attention mechanism has achieved impressive
performance in diverse computer vision tasks; however, it is unclear how
attention modules would affect the performance of 3D point cloud object
detection and what sort of attention modules could fit with the inherent
properties of 3D data. This work investigates the role of the attention
mechanism in 3D point cloud object detection and provides insights into the
potential of different attention modules. To achieve that, we comprehensively
investigate classical 2D attentions, novel 3D attentions, including the latest
point cloud transformers on SUN RGB-D and ScanNetV2 datasets. Based on the
detailed experiments and analysis, we conclude the effects of different
attention modules. This paper is expected to serve as a reference source for
benefiting attention-embedded 3D point cloud object detection. The code and
trained models are available at:
https://github.com/ShiQiu0419/attentions_in_3D_detection.",arxiv
http://arxiv.org/abs/2012.09242v1,2020-12-16T20:14:41Z,2020-12-16T20:14:41Z,"S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point
  Clouds","With the increasing reliance of self-driving and similar robotic systems on
robust 3D vision, the processing of LiDAR scans with deep convolutional neural
networks has become a trend in academia and industry alike. Prior attempts on
the challenging Semantic Scene Completion task - which entails the inference of
dense 3D structure and associated semantic labels from ""sparse"" representations
- have been, to a degree, successful in small indoor scenes when provided with
dense point clouds or dense depth maps often fused with semantic segmentation
maps from RGB images. However, the performance of these systems drop
drastically when applied to large outdoor scenes characterized by dynamic and
exponentially sparser conditions. Likewise, processing of the entire sparse
volume becomes infeasible due to memory limitations and workarounds introduce
computational inefficiency as practitioners are forced to divide the overall
volume into multiple equal segments and infer on each individually, rendering
real-time performance impossible. In this work, we formulate a method that
subsumes the sparsity of large-scale environments and present S3CNet, a sparse
convolution based neural network that predicts the semantically completed scene
from a single, unified LiDAR point cloud. We show that our proposed method
outperforms all counterparts on the 3D task, achieving state-of-the art results
on the SemanticKITTI benchmark. Furthermore, we propose a 2D variant of S3CNet
with a multi-view fusion strategy to complement our 3D network, providing
robustness to occlusions and extreme sparsity in distant regions. We conduct
experiments for the 2D semantic scene completion task and compare the results
of our sparse 2D network against several leading LiDAR segmentation models
adapted for bird's eye view segmentation on two open-source datasets.",arxiv
http://arxiv.org/abs/1811.03934v1,2018-11-09T14:46:40Z,2018-11-09T14:46:40Z,"RadIoT: Radio Communications Intrusion Detection for IoT - A Protocol
  Independent Approach","Internet-of-Things (IoT) devices are nowadays massively integrated in daily
life: homes, factories, or public places. This technology offers attractive
services to improve the quality of life as well as new economic markets through
the exploitation of the collected data. However, these connected objects have
also become attractive targets for attackers because their current security
design is often weak or flawed, as illustrated by several vulnerabilities such
as Mirai, Blueborne, etc. This paper presents a novel approach for detecting
intrusions in smart spaces such as smarthomes, or smartfactories, that is based
on the monitoring and profiling of radio communications at the physical layer
using machine learning techniques. The approach is designed to be independent
of the large and heterogeneous set of wireless communication protocols
typically implemented by connected objects such as WiFi, Bluetooth, Zigbee,
Bluetooth-Low-Energy (BLE) or proprietary communication protocols. The main
concepts of the proposed approach are presented together with an experimental
case study illustrating its feasibility based on data collected during the
deployment of the intrusion detection approach in a smart home under real-life
conditions.",arxiv
http://arxiv.org/abs/2012.15006v3,2021-09-29T22:12:30Z,2020-12-30T02:25:07Z,Dynamic Graph-Based Anomaly Detection in the Electrical Grid,"Given sensor readings over time from a power grid, how can we accurately
detect when an anomaly occurs? A key part of achieving this goal is to use the
network of power grid sensors to quickly detect, in real-time, when any unusual
events, whether natural faults or malicious, occur on the power grid. Existing
bad-data detectors in the industry lack the sophistication to robustly detect
broad types of anomalies, especially those due to emerging cyber-attacks, since
they operate on a single measurement snapshot of the grid at a time. New ML
methods are more widely applicable, but generally do not consider the impact of
topology change on sensor measurements and thus cannot accommodate regular
topology adjustments in historical data. Hence, we propose DYNWATCH, a domain
knowledge based and topology-aware algorithm for anomaly detection using
sensors placed on a dynamic grid. Our approach is accurate, outperforming
existing approaches by 20% or more (F-measure) in experiments; and fast,
running in less than 1.7ms on average per time tick per sensor on a 60K+ branch
case using a laptop computer, and scaling linearly in the size of the graph.",arxiv
http://arxiv.org/abs/2104.07541v1,2021-04-15T15:53:31Z,2021-04-15T15:53:31Z,Reward Optimization for Neural Machine Translation with Learned Metrics,"Neural machine translation (NMT) models are conventionally trained with
token-level negative log-likelihood (NLL), which does not guarantee that the
generated translations will be optimized for a selected sequence-level
evaluation metric. Multiple approaches are proposed to train NMT with BLEU as
the reward, in order to directly improve the metric. However, it was reported
that the gain in BLEU does not translate to real quality improvement, limiting
the application in industry. Recently, it became clear to the community that
BLEU has a low correlation with human judgment when dealing with
state-of-the-art models. This leads to the emerging of model-based evaluation
metrics. These new metrics are shown to have a much higher human correlation.
In this paper, we investigate whether it is beneficial to optimize NMT models
with the state-of-the-art model-based metric, BLEURT. We propose a
contrastive-margin loss for fast and stable reward optimization suitable for
large NMT models. In experiments, we perform automatic and human evaluations to
compare models trained with smoothed BLEU and BLEURT to the baseline models.
Results show that the reward optimization with BLEURT is able to increase the
metric scores by a large margin, in contrast to limited gain when training with
smoothed BLEU. The human evaluation shows that models trained with BLEURT
improve adequacy and coverage of translations. Code is available via
https://github.com/naver-ai/MetricMT.",arxiv
http://arxiv.org/abs/1704.04979v1,2017-03-30T09:28:55Z,2017-03-30T09:28:55Z,"Urban Data Streams and Machine Learning: A Case of Swiss Real Estate
  Market","In this paper, we show how using publicly available data streams and machine
learning algorithms one can develop practical data driven services with no
input from domain experts as a form of prior knowledge. We report the initial
steps toward development of a real estate portal in Switzerland. Based on
continuous web crawling of publicly available real estate advertisements and
using building data from Open Street Map, we developed a system, where we
roughly estimate the rental and sale price indexes of 1.7 million buildings
across the country. In addition to these rough estimates, we developed a web
based API for accurate automated valuation of rental prices of individual
properties and spatial sensitivity analysis of rental market. We tested several
established function approximation methods against the test data to check the
quality of the rental price estimations and based on our experiments, Random
Forest gives very reasonable results with the median absolute relative error of
6.57 percent, which is comparable with the state of the art in the industry. We
argue that while recently there have been successful cases of real estate
portals, which are based on Big Data, majority of the existing solutions are
expensive, limited to certain users and mostly with non-transparent underlying
systems. As an alternative we discuss, how using the crawled data sets and
other open data sets provided from different institutes it is easily possible
to develop data driven services for spatial and temporal sensitivity analysis
in the real estate market to be used for different stakeholders. We believe
that this kind of digital literacy can disrupt many other existing business
concepts across many domains.",arxiv
http://arxiv.org/abs/1701.07681v1,2017-01-26T13:09:48Z,2017-01-26T13:09:48Z,Fast and Accurate Time Series Classification with WEASEL,"Time series (TS) occur in many scientific and commercial applications,
ranging from earth surveillance to industry automation to the smart grids. An
important type of TS analysis is classification, which can, for instance,
improve energy load forecasting in smart grids by detecting the types of
electronic devices based on their energy consumption profiles recorded by
automatic sensors. Such sensor-driven applications are very often characterized
by (a) very long TS and (b) very large TS datasets needing classification.
However, current methods to time series classification (TSC) cannot cope with
such data volumes at acceptable accuracy; they are either scalable but offer
only inferior classification quality, or they achieve state-of-the-art
classification quality but cannot scale to large data volumes.
  In this paper, we present WEASEL (Word ExtrAction for time SEries
cLassification), a novel TSC method which is both scalable and accurate. Like
other state-of-the-art TSC methods, WEASEL transforms time series into feature
vectors, using a sliding-window approach, which are then analyzed through a
machine learning classifier. The novelty of WEASEL lies in its specific method
for deriving features, resulting in a much smaller yet much more discriminative
feature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more
accurate than the best current non-ensemble algorithms at orders-of-magnitude
lower classification and training times, and it is almost as accurate as
ensemble classifiers, whose computational complexity makes them inapplicable
even for mid-size datasets. The outstanding robustness of WEASEL is also
confirmed by experiments on two real smart grid datasets, where it
out-of-the-box achieves almost the same accuracy as highly tuned,
domain-specific methods.",arxiv
http://arxiv.org/abs/2110.00468v1,2021-10-01T15:03:03Z,2021-10-01T15:03:03Z,"New Evolutionary Computation Models and their Applications to Machine
  Learning","Automatic Programming is one of the most important areas of computer science
research today. Hardware speed and capability have increased exponentially, but
the software is years behind. The demand for software has also increased
significantly, but it is still written in old fashion: by using humans.
  There are multiple problems when the work is done by humans: cost, time,
quality. It is costly to pay humans, it is hard to keep them satisfied for a
long time, it takes a lot of time to teach and train them and the quality of
their output is in most cases low (in software, mostly due to bugs).
  The real advances in human civilization appeared during the industrial
revolutions. Before the first revolution, most people worked in agriculture.
Today, very few percent of people work in this field.
  A similar revolution must appear in the computer programming field.
Otherwise, we will have so many people working in this field as we had in the
past working in agriculture.
  How do people know how to write computer programs? Very simple: by learning.
Can we do the same for software? Can we put the software to learn how to write
software?
  It seems that is possible (to some degree) and the term is called Machine
Learning. It was first coined in 1959 by the first person who made a computer
perform a serious learning task, namely, Arthur Samuel.
  However, things are not so easy as in humans (well, truth to be said - for
some humans it is impossible to learn how to write software). So far we do not
have software that can learn perfectly to write software. We have some
particular cases where some programs do better than humans, but the examples
are sporadic at best. Learning from experience is difficult for computer
programs. Instead of trying to simulate how humans teach humans how to write
computer programs, we can simulate nature.",arxiv
http://arxiv.org/abs/2104.04837v1,2021-04-10T18:46:48Z,2021-04-10T18:46:48Z,"CalQNet -- Detection of Calibration Quality for Life-Long Stereo Camera
  Setups","Many mobile robotic platforms rely on an accurate knowledge of the extrinsic
calibration parameters, especially systems performing visual stereo matching.
Although a number of accurate stereo camera calibration methods have been
developed, which provide good initial ""factory"" calibrations, the determined
parameters can lose their validity over time as the sensors are exposed to
environmental conditions and external effects. Thus, on autonomous platforms
on-board diagnostic methods for an early detection of the need to repeat
calibration procedures have the potential to prevent critical failures of
crucial systems, such as state estimation or obstacle detection. In this work,
we present a novel data-driven method to estimate the calibration quality and
detect discrepancies between the original calibration and the current system
state for stereo camera systems. The framework consists of a novel dataset
generation pipeline to train CalQNet, a deep convolutional neural network.
CalQNet can estimate the calibration quality using a new metric that
approximates the degree of miscalibration in stereo setups. We show the
framework's ability to predict from a single stereo frame if a state-of-the-art
stereo-visual odometry system will diverge due to a degraded calibration in two
real-world experiments.",arxiv
http://arxiv.org/abs/1609.08018v1,2016-09-26T15:15:09Z,2016-09-26T15:15:09Z,"Small near-Earth asteroids in the Palomar Transient Factory survey: A
  real-time streak-detection system","Near-Earth asteroids (NEAs) in the 1-100 meter size range are estimated to be
$\sim$1,000 times more numerous than the $\sim$15,000 currently-catalogued
NEAs, most of which are in the 0.5-10 kilometer size range. Impacts from 10-100
meter size NEAs are not statistically life-threatening but may cause
significant regional damage, while 1-10 meter size NEAs with low velocities
relative to Earth are compelling targets for space missions. We describe the
implementation and initial results of a real-time NEA-discovery system
specialized for the detection of small, high angular rate (visually-streaked)
NEAs in Palomar Transient Factory (PTF) images. PTF is a 1.2-m aperture,
7.3-deg$^2$ field-of-view optical survey designed primarily for the discovery
of extragalactic transients (e.g., supernovae) in 60-second exposures reaching
$\sim$20.5 visual magnitude. Our real-time NEA discovery pipeline uses a
machine-learned classifier to filter a large number of false-positive streak
detections, permitting a human scanner to efficiently and remotely identify
real asteroid streaks during the night. Upon recognition of a streaked NEA
detection (typically within an hour of the discovery exposure), the scanner
triggers follow-up with the same telescope and posts the observations to the
Minor Planet Center for worldwide confirmation. We describe our ten initial
confirmed discoveries, all small NEAs that passed 0.3-15 lunar distances from
Earth. Lastly, we derive useful scaling laws for comparing
streaked-NEA-detection capabilities of different surveys as a function of their
hardware and survey-pattern characteristics. This work most directly informs
estimates of the streak-detection capabilities of the Zwicky Transient Facility
(ZTF, planned to succeed PTF in 2017), which will apply PTF's current
resolution and sensitivity over a 47-deg$^2$ field-of-view.",arxiv
http://arxiv.org/abs/1801.05671v1,2018-01-17T14:14:22Z,2018-01-17T14:14:22Z,"Compact Real-time avoidance on a Humanoid Robot for Human-robot
  Interaction","With robots leaving factories and entering less controlled domains, possibly
sharing the space with humans, safety is paramount and multimodal awareness of
the body surface and the surrounding environment is fundamental. Taking
inspiration from peripersonal space representations in humans, we present a
framework on a humanoid robot that dynamically maintains such a protective
safety zone, composed of the following main components: (i) a human 2D
keypoints estimation pipeline employing a deep learning based algorithm,
extended here into 3D using disparity; (ii) a distributed peripersonal space
representation around the robot's body parts; (iii) a reaching controller that
incorporates all obstacles entering the robot's safety zone on the fly into the
task. Pilot experiments demonstrate that an effective safety margin between the
robot's and the human's body parts is kept. The proposed solution is flexible
and versatile since the safety zone around individual robot and human body
parts can be selectively modulated---here we demonstrate stronger avoidance of
the human head compared to rest of the body. Our system works in real time and
is self-contained, with no external sensory equipment and use of onboard
cameras only.",arxiv
