id,type,publication,publisher,publication_date,database,title,url,abstract,domain
10.1016/j.resconrec.2021.106022,Journal,"Resources, Conservation and Recycling",scopus,2022-03-01,sciencedirect,Using computer vision to recognize composition of construction waste mixtures: A semantic segmentation approach,https://api.elsevier.com/content/abstract/scopus_id/85118570774,"Timely and accurate recognition of construction waste (CW) composition can provide yardstick information for its subsequent management (e.g., segregation, determining proper disposal destination). Increasingly, smart technologies such as computer vision (CV), robotics, and artificial intelligence (AI) are deployed to automate waste composition recognition. Existing studies focus on individual waste objects in well-controlled environments, but do not consider the complexity of the real-life scenarios. This research takes the challenges of the mixture and clutter nature of CW as a departure point and attempts to automate CW composition recognition by using CV technologies. Firstly, meticulous data collection, cleansing, and annotation efforts are made to create a high-quality CW dataset comprising 5,366 images. Then, a state-of-the-art CV semantic segmentation technique, DeepLabv3+, is introduced to develop a CW segmentation model. Finally, several training hyperparameters are tested via orthogonal experiments to calibrate the model performance. The proposed approach achieved a mean Intersection over Union (mIoU) of 0.56 in segmenting nine types of materials/objects with a time performance of 0.51 s per image. The approach was found to be robust to variation of illumination and vehicle types. The study contributes to the important problem of material composition recognition, formalizing a deep learning-based semantic segmentation approach for CW composition recognition in complex environments. It paves the way for better CW management, particularly in engaging robotics, in the future. The trained models are hosted on GitHub, based on which researchers can further finetune for their specific applications.",robotics
10.1016/j.ymssp.2021.108372,Journal,Mechanical Systems and Signal Processing,scopus,2022-02-15,sciencedirect,Robotic seam tracking system combining convolution filter and deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85113754502,"To perform automatic, real-time seam tracking tasks effectively, a robust and accurate seam tracking system must be designed. In this paper, we solve the seam tracking issue using a six-axis welding robot, a line laser sensor and an industrial computer. The processing of welding images is the core of the seam tracking system, which aims to determine the weld feature point in each image. We propose a two-stage weld feature point localization method that combines convolution filter and deep reinforcement learning (CF-DRL) to localize the weld feature point in each welding image robustly and accurately. In the first stage, the weld feature point is roughly tracked using a convolution filter tracker. But the position given by the convolution tracker is sometimes not accurate enough due to the natural gap between visual tracking and seam tracking. Consequently, in the second stage, the weld feature point should be further refined using our trained policy network. Using our two-stage weld feature point localization method, the weld feature points can be determined from noisy images in real time during the welding process. The 3D coordinate values of these points are obtained according to the structured light measurement principle to control the movement of the robot and the torch in real time. A robotic seam tracking system is established based on the equipment and methods mentioned above. Experimental results show that the welding torch runs smoothly with a strong arc light and splash interference. The mean tracking error of our experiments reaches 0.189 mm, which can fulfill actual welding requirements. Several comparison tests have been performed to illustrate the robustness and accuracy of our seam tracking system using our welding image dataset.",robotics
10.1016/j.automatica.2021.110007,Journal,Automatica,scopus,2022-01-01,sciencedirect,An analytic layer-wise deep learning framework with applications to robotics,https://api.elsevier.com/content/abstract/scopus_id/85118989490,"Deep learning (DL) has achieved great success in many applications, but it has been less well analyzed from the theoretical perspective. The unexplainable success of black-box DL models has raised questions among scientists and promoted the emergence of the field of explainable artificial intelligence (XAI). In robotics, it is particularly important to deploy DL algorithms in a predictable and stable manner as robots are active agents that need to interact safely with the physical world. This paper presents an analytic deep learning framework for fully connected neural networks, which can be applied for both regression problems and classification problems. Examples for regression and classification problems include online robot control and robot vision. We present two layer-wise learning algorithms such that the convergence of the learning systems can be analyzed. Firstly, an inverse layer-wise learning algorithm for multilayer networks with convergence analysis for each layer is presented to understand the problems of layer-wise deep learning. Secondly, a forward progressive learning algorithm where the deep networks are built progressively by using single hidden layer networks is developed to achieve better accuracy. It is shown that the progressive learning method can be used for fine-tuning of weights from convergence point of view. The effectiveness of the proposed framework is illustrated based on classical benchmark recognition tasks using the MNIST and CIFAR-10 datasets and the results show a good balance between performance and explainability. The proposed method is subsequently applied for online learning of robot kinematics and experimental results on kinematic control of UR5e robot with unknown model are presented.",robotics
10.1016/j.neunet.2021.10.002,Journal,Neural Networks,scopus,2022-01-01,sciencedirect,Probabilistic generative modeling and reinforcement learning extract the intrinsic features of animal behavior,https://api.elsevier.com/content/abstract/scopus_id/85118333957,"It is one of the ultimate goals of ethology to understand the generative process of animal behavior, and the ability to reproduce and control behavior is an important step in this field. However, it is not easy to achieve this goal in systems with complex and stochastic dynamics such as animal behavior. In this study, we have shown that MDN–RNN,a type of probabilistic deep generative model, is able to reproduce stochastic animal behavior with high accuracy by modeling the behavior of C. elegans. Furthermore, we found that the model learns different dynamics in a disentangled representation as a time-evolving Gaussian mixture. Finally, by combining the model and reinforcement learning, we were able to extract a behavioral policy of goal-directed behavior in silico, and showed that it can be used for regulating the behavior of real animals. This set of methods will be applicable not only to animal behavior but also to broader areas such as neuroscience and robotics.",robotics
10.1016/j.engappai.2021.104514,Journal,Engineering Applications of Artificial Intelligence,scopus,2022-01-01,sciencedirect,Instance-based defense against adversarial attacks in Deep Reinforcement Learning,https://api.elsevier.com/content/abstract/scopus_id/85118104144,"Deep Reinforcement Learning systems are now a hot topic in Machine Learning for their effectiveness in many complex tasks, but their application in safety-critical domains (e.g., robot control or self-autonomous driving) remains dangerous without mechanism to detect and prevent risk situations. In Deep RL, such risk is mostly in the form of adversarial attacks, which introduce small perturbations to sensor inputs with the aim of changing the network-based decisions and thus cause catastrophic situations. In the light of these dangers, a promising line of research is that of providing these Deep RL algorithms with suitable defenses, especially when deploying in real environments. This paper suggests that this line of research could be greatly improved by the concepts from the existing research field of Safe Reinforcement Learning, which has been postulated as a family of RL algorithms capable of providing defenses against many forms of risks. However, the connections between Safe RL and the design of defenses against adversarial attacks in Deep RL remain largely unexplored. This paper seeks to explore precisely some of these connections. In particular, this paper proposes to reuse some of the concepts from existing Safe RL algorithms to create a novel and effective instance-based defense for the deployment stage of Deep RL policies. The proposed algorithm uses a risk function based on how far a state is from the state space known by the agent, that allows identifying and preventing adversarial situations. The success of the proposed defense has been evaluated in 4 Atari games.",robotics
10.1016/j.compag.2021.106503,Journal,Computers and Electronics in Agriculture,scopus,2021-12-01,sciencedirect,Fast and accurate green pepper detection in complex backgrounds via an improved Yolov4-tiny model,https://api.elsevier.com/content/abstract/scopus_id/85117612390,"In agricultural production, the branches and leaves of green peppers are severely blocked due to the dense plant distribution. This makes the identification of green peppers difficult. Traditional green pepper detection methods entail the problems of low accuracy and poor robustness. This paper introduces a deep learning target detection algorithm based on Yolov4_tiny for green pepper detection. The backbone network in the classic target detection algorithm model is used to ensure classification accuracy. This paper introduces an adaptive spatial feature pyramid method that combines an attention mechanism and the idea of multi-scale prediction to improve the recognition effect of occluded and small-target green peppers. Finally, the method was applied to a test set of 145 images (the target number of green peppers was 602). The AP value of green peppers reached 95.11%; the precision rate was 96.91%, and the recall rate was 93.85%. In order to verify the effectiveness of the module in improving detection performance, we conducted independent combined experiments on the improved module and compared the results with five classic target detection algorithms: SSD, Faster-RCNN, Yolov3, Yolov3_tiny, and Yolov4_tiny. The comparisons verified that the detection rate of the model reached that of the current state-of-the-art technology (SOTA) green pepper detection models. In addition, this green pepper detection model is suitable for real-time detection and embedded development needs of agricultural robots. Such new methods are key components of the technology for predicting green pepper parameters and intelligent picking in unmanned farms.",robotics
10.1016/j.heares.2021.108371,Journal,Hearing Research,scopus,2021-12-01,sciencedirect,Electrical impedance guides electrode array in cochlear implantation using machine learning and robotic feeder,https://api.elsevier.com/content/abstract/scopus_id/85117569953,"Cochlear Implant provides an electronic substitute for hearing to severely or profoundly deaf patients. However, postoperative hearing outcomes significantly depend on the proper placement of electrode array (EA) into scala tympani (ST) during cochlear implant surgery. Due to limited intra-operative methods to access array placement, the objective of the current study was to evaluate the relationship between EA complex impedance and different insertion trajectories in a plastic ST model. A prototype system was designed to measure bipolar complex impedance (magnitude and phase) and its resistive and reactive components of electrodes. A 3-DoF actuation system was used as an insertion feeder. 137 insertions were performed from 3 different directions at a speed of 0.08 mm/s. Complex impedance data of 8 electrode pairs were sequentially recorded in each experiment. Machine learning algorithms were employed to classify both the full and partial insertion lengths. Support Vector Machine (SVM) gave the highest 97.1% accuracy for full insertion. When a real-time prediction was tested, Shallow Neural Network (SNN) model performed better than other algorithms using partial insertion data. The highest accuracy was found at 86.1% when 4 time samples and 2 apical electrode pairs were used. Direction prediction using partial data has the potential of online control of the insertion feeder for better EA placement. Accessing the position of the electrode array during the insertion has the potential to optimize its intraoperative placement that will result in improved hearing outcomes.",robotics
10.1016/j.robot.2021.103891,Journal,Robotics and Autonomous Systems,scopus,2021-12-01,sciencedirect,Hybrid autonomous controller for bipedal robot balance with deep reinforcement learning and pattern generators[Formula presented],https://api.elsevier.com/content/abstract/scopus_id/85116222985,"Recovering after an abrupt push is essential for bipedal robots in real-world applications within environments where humans must collaborate closely with robots. There are several balancing algorithms for bipedal robots in the literature, however most of them either rely on hard coding or power-hungry algorithms. We propose a hybrid autonomous controller that hierarchically combines two separate, efficient systems, to address this problem. The lower-level system is a reliable, high-speed, full state controller that was hardcoded on a microcontroller to be power efficient. The higher-level system is a low-speed reinforcement learning controller implemented on a low-power onboard computer. While one controller offers speed, the other provides trainability and adaptability. An efficient control is then formed without sacrificing adaptability to new dynamic environments. Additionally, as the higher-level system is trained via deep reinforcement learning, the robot could learn after deployment, which is ideal for real-world applications. The system’s performance is validated with a real robot recovering after a random push in less than 5 s, with minimal steps from its initial positions. The training was conducted using simulated data.",robotics
10.1016/j.robot.2021.103861,Journal,Robotics and Autonomous Systems,scopus,2021-12-01,sciencedirect,Multi-Spectral Image Synthesis for Crop/Weed Segmentation in Precision Farming,https://api.elsevier.com/content/abstract/scopus_id/85114121190,"An effective perception system is a fundamental component for farming robots, as it enables them to properly perceive the surrounding environment and to carry out targeted operations. The most recent methods make use of state-of-the-art machine learning techniques to learn a valid model for the target task. However, those techniques need a large amount of labeled data for training. A recent approach to deal with this issue is data augmentation through Generative Adversarial Networks (GANs), where entire synthetic scenes are added to the training data, thus enlarging and diversifying their informative content. In this work, we propose an alternative solution with respect to the common data augmentation methods, applying it to the fundamental problem of crop/weed segmentation in precision farming. Starting from real images, we create semi-artificial samples by replacing the most relevant object classes (i.e., crop and weeds) with their synthesized counterparts. To do that, we employ a conditional GAN (cGAN), where the generative model is trained by conditioning the shape of the generated object. Moreover, in addition to RGB data, we take into account also near-infrared (NIR) information, generating four channel multi-spectral synthetic images. Quantitative experiments, carried out on three publicly available datasets, show that (i) our model is capable of generating realistic multi-spectral images of plants and (ii) the usage of such synthetic images in the training process improves the segmentation performance of state-of-the-art semantic segmentation convolutional networks.",robotics
10.1016/j.rcim.2021.102183,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2021-12-01,sciencedirect,Autonomous nondestructive evaluation of resistance spot welded joints,https://api.elsevier.com/content/abstract/scopus_id/85110261037,"The application of non-destructive evaluation approaches has attracted strong interests in modern automotive industries. This study presents an autonomous deep-computing framework to analyze raw videos from infrared systems and to predict weld nugget shape and size with unprecedented accuracy and speed. In a comprehensive training and testing experiment with 90 videos (seven sets of welding material stack-ups), a new method was developed to assemble sufficient datasets for neural network training. Our framework successfully predicts all the nugget shapes with F1 scores that range from 0.84 to 0.92. The total training time on Nvidia DGX station takes less than 10 min for each set of welding material stack-up. The real inference time of an individual dataset (with 30 video frames) takes about 0.005 s. The procedure and methods developed in the study can be applied to other image-based weld property prediction, as well as other manufacturing processes. Furthermore, our well-trained neural networks take limited memory resources (2.3 MB) and are suitable for embedded microprocessors for in-situ welding quality control as edge computing within an intelligent welding framework.",robotics
10.1016/j.eswa.2021.115498,Journal,Expert Systems with Applications,scopus,2021-12-01,sciencedirect,Real-time human pose estimation on a smart walker using convolutional neural networks,https://api.elsevier.com/content/abstract/scopus_id/85109217957,"Rehabilitation is important to improve quality of life for mobility-impaired patients. Smart walkers are a commonly used solution that should embed automatic and objective tools for data-driven human-in-the-loop control and monitoring. However, present solutions focus on extracting few specific metrics from dedicated sensors with no unified full-body approach. We investigate a general, real-time, full-body pose estimation framework based on two RGB+D camera streams with non-overlapping views mounted on a smart walker equipment used in rehabilitation. Human keypoint estimation is performed using a two-stage neural network framework. The 2D-Stage implements a detection module that locates body keypoints in the 2D image frames. The 3D-Stage implements a regression module that lifts and relates the detected keypoints in both cameras to the 3D space relative to the walker. Model predictions are low-pass filtered to improve temporal consistency. A custom acquisition method was used to obtain a dataset, with 14 healthy subjects, used for training and evaluating the proposed framework offline, which was then deployed on the real walker equipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage and 44.05 mm for the 3D-Stage were reported, with an inference time of 26.6 ms when deployed on the constrained hardware of the walker. We present a novel approach to patient monitoring and data-driven human-in-the-loop control in the context of smart walkers. It is able to extract a complete and compact body representation in real-time and from inexpensive sensors, serving as a common base for downstream metrics extraction solutions, and Human-Robot interaction applications. Despite promising results, more data should be collected on users with impairments, to assess its performance as a rehabilitation tool in real-world scenarios.",robotics
10.1016/j.neucom.2021.08.115,Journal,Neurocomputing,scopus,2021-11-20,sciencedirect,HuRAI: A brain-inspired computational model for human-robot auditory interface,https://api.elsevier.com/content/abstract/scopus_id/85114916023,"The deep learning era endows immense opportunities for ubiquitous robotic applications by leveraging big data generated from widespread sensors and ever-growing computing capability. While the growing demands for natural human-robot interaction (HRI) as well as concerns for energy efficiency, real-time performance, and data security motive novel solutions. In this paper, we present a brain-inspired spiking neural network (SNN) based Human-Robot Auditory Interface, namely HuRAI. The HuRAI integrates the voice activity detection, speaker localization and voice command recognition systems into a unified framework that can be implemented on the emerging low-power neuromorphic computing (NC) devices. Our experimental results demonstrate superior modeling capabilities of SNNs, achieving accurate and rapid prediction for each task. Moreover, the energy efficiency analysis reveals a compelling prospect, with up to three orders of magnitude energy savings, over the equivalent artificial neural networks that running on the state-of-the-art Nvidia graphics processing unit (GPU). Therefore, integrating the algorithmic power of large-scale SNN models and the energy efficiency of NC devices offers an attractive solution for real-time, low-power robotic applications.",robotics
10.1016/j.cmpb.2021.106460,Journal,Computer Methods and Programs in Biomedicine,scopus,2021-11-01,sciencedirect,Multi-Modality guidance based surgical navigation for percutaneous endoscopic transforaminal discectomy,https://api.elsevier.com/content/abstract/scopus_id/85118353932,"Objective
                  Fluoroscopic guidance is a critical step for the puncture procedure in percutaneous endoscopic transforaminal discectomy (PETD). However, two-dimensional observations of the three-dimensional anatomic structure suffer from the effects of projective simplification. To accurately assess the spatial relations between the patient vertebra tissues and puncture needle, a considerable number of fluoroscopic images from different orientations need to be acquired by the surgeons. This process significantly increases the radiation risk for both the patient and surgeons.
               
                  Methods
                  In this paper, we propose an augmented reality (AR) surgical navigation system for PETD based on multi-modality information, which contains fluoroscopy, optical tracking, and depth camera. To register the fluoroscopic image with the intraoperative video, we design a lightweight non-invasive fiducial with markers and detect the markers based on the deep learning method. It can display the intraoperative video fused with the registered fluoroscopic images. We also present a self-adaptive calibration and transformation method between a 6-DOF optical tracking device and a depth camera, which are in different coordinate systems.
               
                  Results
                  With the substantially reduced frequency of fluoroscopy imaging, the system can accurately track and superimpose the virtual puncture needle on fluoroscopy images in real-time. From operating theatre in vivo animal experiments, the results illustrate that the system average positioning accuracy can reach 1.98mm and the orientation accuracy can reach 1.19
                        
                           
                           ∘
                        
                     . From the clinical validation results, the system significantly lower the frequency of fluoroscopy imaging (42.7%) and reduce the radiation risk for both the patient and surgeons.
               
                  Conclusion
                  Coupled with the user study, both the quantitative and qualitative results indicate that our navigation system has the potential to be highly useful in clinical practice. Compared with the existing navigation systems, which are usually equipped with a variety of large and high-cost medical equipments, such as O-arm, cone-beam CT, and robots, our navigation system does not need special equipment and can be implemented with common equipment in the operating room, such as C-arm, desktop, etc., even in small hospitals.",robotics
10.1016/j.engappai.2021.104471,Journal,Engineering Applications of Artificial Intelligence,scopus,2021-11-01,sciencedirect,Human gaze-aware attentive object detection for ambient intelligence,https://api.elsevier.com/content/abstract/scopus_id/85116125642,"Understanding human behavior and the surrounding environment is essential for realizing ambient intelligence (AmI), for which eye gaze and object information are reliable cues. In this study, the authors propose a novel human gaze-aware attentive object detection framework as an elemental technology for AmI. The proposed framework detects users’ attentive objects and shows more precise and robust performance against object-scale variations. A novel Adaptive-3D-Region-of-Interest (Ada-3D-RoI) scheme is designed as a front-end module, and scalable detection network structures are proposed to maximize cost-efficiency. The experiments show that the detection rate is improved up to 97.6% on small objects (14.1% on average), and it is selectively tunable with a tradeoff between accuracy and computational complexity. In addition, the qualitative results demonstrate that the proposed framework detects a user’s single object-of-interest only, even when the target object is occluded or extremely small. Complementary matters for follow-up study are presented as suggestions to extend the results of the proposed framework to further practical AmI applications. This study will help develop advanced AmI applications that demand a higher-level understanding of scene context and human behavior such as human–robot symbiosis, remote-/autonomous control, and augmented/mixed reality.",robotics
10.1016/j.compag.2021.106450,Journal,Computers and Electronics in Agriculture,scopus,2021-11-01,sciencedirect,Detecting ripe fruits under natural occlusion and illumination conditions,https://api.elsevier.com/content/abstract/scopus_id/85114907175,"For an accurate detection of ripe fruits under uneven illumination and ubiquitous occlusion conditions, this paper proposes a method to detect and locate ripe fruits based on machine vision. There are four key steps in this method including image graying and background removal, binary image optimization, true contour fragments extraction, and fruit fitting. For testing the proposed method, field experiments were conducted with tomato and citrus, and the ripe fruits in complex environments were successfully detected and located. From the detection experiments, it showed that the recognition rate for ripe fruits in the near zone of the proposed method was higher than 97.44%, the average time consumption was 0.2966 s, and the positioning error was less than 4.41%. In addition, it can be concluded from the comparative experiment that the proposed method is superior to conventional Hough transform, random Hough transform, and other methods based on deep learning in terms of detection rate, time performance and positioning accuracy. Therefore, it can be applied to picking robots for real-time detecting and locating ripe fruits.",robotics
10.1016/j.neunet.2021.07.021,Journal,Neural Networks,scopus,2021-11-01,sciencedirect,Continual learning for recurrent neural networks: An empirical evaluation,https://api.elsevier.com/content/abstract/scopus_id/85111476234,"Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications.
                  We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.",robotics
10.1016/j.orgel.2021.106290,Journal,Organic Electronics,scopus,2021-11-01,sciencedirect,A wearable and highly sensitive capacitive pressure sensor integrated a dual-layer dielectric layer of PDMS microcylinder array and PVDF electrospun fiber,https://api.elsevier.com/content/abstract/scopus_id/85111240125,"Recently, high-performance flexible pressure sensors have received considerable attention because of their potential application in fitness tracking, human–machine interfaces, and artificial intelligence. Sensitivity is a key parameter that directly affects a sensor's performance; therefore, improving the sensitivity of sensors is a vital research topic. This study developed a dual-layer dielectric structure comprising a layer of electrospun fiber and an array of microcylinders and used it to fabricate a novel high-sensitivity capacitive pressure sensor. A simple, rapid, low-cost, and controllable microstructured method that did not require complex and expensive equipment was adopted. The proposed sensor can efficiently detect capacitance changes by analyzing changes in the fiber and microcylinder structure when compressed. It has high sensitivity of 0.6 kPa−1, rapid response time of 25 ms, ultralow limit of detection of 0.065 Pa, and high durability and high reliability without any signal attenuation up to 10,000 load/unload cycles and up to 5000 bending/unbending cycles. Moreover, it yielded favorable results in real-time tests, such as pulse monitoring, acoustic tests, breathe monitoring, and body motion monitoring. Furthermore, experiments were conducted using a robotic arm, and the obtained results verify that the sensor has different capacitance responses to objects with different shapes, which is crucial for its future applications in smart machinery. Finally, the sensors were arranged as a 6 × 6 matrix, and they successfully displayed the pressure distribution in a plane. Thus, the contributions of the capacitance pressure sensor with a dual-layer dielectric structure in the field of high-performance pressure sensors were verified.",robotics
10.1016/j.actaastro.2021.07.012,Journal,Acta Astronautica,scopus,2021-11-01,sciencedirect,"A review of space surgery - What have we achieved, current challenges, and future prospects",https://api.elsevier.com/content/abstract/scopus_id/85110745640,"Major surgical events/incidents onboard are rare but can be catastrophic to any mission. National Aeronautics and Space Administration (NASA) uses the Integrated Medical Model (IMM) to develop an integrated, quantified, evidence-based decision support tool useful for crew health and mission planners to assess risk and design medical systems. In 2017, the IMM of the NASA Human Research Program included a list of 100 medical conditions that could be anticipated during space flight. Of those conditions, 27 are expected to need surgical treatment. Consequently, there has been a continuing interest in surgical capabilities for exploration space flight. The surgical system capabilities aboard all space stations and analogue flights have been designed and implemented with an emphasis on stabilisation, medical evacuation, and ATLS capabilities. However, with future missions to the Moon and Mars, evacuation is not a possibility and astronauts will need to troubleshoot, adapt, and self-administer complex surgical care autonomously.
                  This narrative review aims to examine the published work on surgical care in space, discuss the inherent challenges, and identify scope for future studies. The review evaluates and analyses results from several landmark experiments covering important technical aspects such as basic surgical skills, laparoscopic surgery, robotic surgery, and tele surgery. Relevant studies for the review were identified from the MEDLINE, PubMed, and EMBASE databases. Eligible studies were published between 1960 and June 2021 and were identified using the terms “space surgery”, “microgravity”, “zero gravity”, “weightlessness”, “parabolic flight”, “neutral buoyancy”, and “spaceflight”. Only articles in English were selected and references cited in the selected publications were followed up and included where appropriate. Documents available in the public domain and/or archives of National Space agencies were also included. The search yielded a total of 86 hits including review articles, commentaries, studies, meeting summaries and technical reports submitted to National Space agencies. Results were then filtered for eligible papers relevant to this narrative review. Challenges on a long-duration mission will be unique, unlike anything we have faced so far in the last 60 years of space travel. Despite the progress in space surgery in the last 40 years, there are several challenges to achieving a fully functional surgical care system on any mission outside Low Earth Orbit. The microgravity environment presents unique challenges related to altered physiology as well as mechanics and techniques pertinent to surgical care. Some of the challenges include but are not limited to crew selection, role of prophylactic surgery, adaptation to zero gravity, lack of ground support, training and maintenance of surgical skills and limitation of weight and volume for hardware. Ultrasound imaging, 3D printing and AI-based surgical assistance coupled with robotic surgery have shown promise, but their real efficacy and functionality remains to be tested.",robotics
10.1016/j.jhlste.2020.100275,Journal,"Journal of Hospitality, Leisure, Sport and Tourism Education",scopus,2021-11-01,sciencedirect,Industry 4.0 technologies in tourism education: Nurturing students to think with technology,https://api.elsevier.com/content/abstract/scopus_id/85092173436,"The Industry 4.0 revolution is bringing major transformations in the tourism systems design suitable for technologically oriented consumers. Indeed, methods and technologies introduced by Big Data, Automation, Virtual and augmented reality, Robotics and ICT well fit with the Tourism 4.0 paradigm. However, tourism students are not yet trained on techniques, issues and methods related to the Industry 4.0 framework.
                  Hence, relying on a careful examination of the literature on tourism market trends linked to the offer of innovative technological services, we identified conceptual, methodological, technological and practical skills to be developed in an academic curriculum for Tourism Science students. Learning path were focused on: i) processes of data acquisition from social media, ii) data analysis using Machine Learning techniques and iii) data design into significant elements useful to implement communication systems in the tourism field.
               
                  Results
                  showed that the most of participants achieved a medium-high evaluation for the implementation of the communication systems, applying appropriately techniques and tools learned along the course. Furthermore, the high percentage of students satisfaction registered in relation to the course, revealed that students enjoyed this experience. Outcomes reflects the acquisition and the awareness of those skills that will enable students to be conscious protagonists of their role in tourism 4.0.",robotics
10.1016/j.neucom.2021.08.023,Journal,Neurocomputing,scopus,2021-10-28,sciencedirect,A data-efficient goal-directed deep reinforcement learning method for robot visuomotor skill,https://api.elsevier.com/content/abstract/scopus_id/85113175261,"Deep reinforcement learning (DRL) has provided an effective end to end approach for autonomous robot skill learning. However, the task goal in most DRL approaches is always single and fixed which restricts the flexibility of policy. In addition, the data inefficiency also makes it impractical to applied on real-world robots. To address these problems, this paper proposes a data-efficient goal-directed DRL method for robotic skill learning. First, an asymmetric deep deterministic policy gradients algorithm is constructed as the basic framework of the method by taking advantage of the low-dimensional physical state accessible in simulations. Then, a Siamese representation learning network is designed to embed the RGB observation and the human intention at the same feature space to realize human-robot intention transfer. And, an auxiliary similarity evaluation network is added to the DRL algorithm to accelerate representation learning. Finally, a domain randomization method is employed to transfer the learned policies from simulation to reality. In experiments, two typical robotic tasks are set up to evaluate the proposed method. The experimental results validate the effectiveness of the proposed method. The trained robot can switch among multiple goals automatically according to human intentions. And, the Siamese representation learning network and auxiliary similarity evaluation network can improve data efficiency effectively.",robotics
10.1016/j.imavis.2021.104266,Journal,Image and Vision Computing,scopus,2021-10-01,sciencedirect,A bioinspired retinal neural network for accurately extracting small-target motion information in cluttered backgrounds,https://api.elsevier.com/content/abstract/scopus_id/85112563915,"Robust and accurate detection of small moving targets in cluttered moving backgrounds is a significant and challenging problem for robotic visual systems to perform search and tracking tasks. Inspired by the neural circuitry of elementary motion vision in the mammalian retina, this paper proposes a bioinspired retinal neural network based on a new neurodynamics-based temporal filtering and multiform 2-D spatial Gabor filtering. This model can estimate motion direction accurately via only two perpendicular spatiotemporal filtering signals, and respond to small targets of different sizes and velocities through adjusting the dendrite field size of spatial filter. Meanwhile, an algorithm of directionally selective inhibition is proposed to suppress the target-like features in the moving background, which can reduce the influence of background motion effectively. Extensive synthetic and real-data experiments show that the proposed model works stably for small targets of a wider size and velocity range, and has better detection performance than other bioinspired models. Additionally, it can also extract the information of motion direction and motion energy accurately and rapidly.",robotics
10.1016/j.neunet.2021.05.010,Journal,Neural Networks,scopus,2021-10-01,sciencedirect,Robot navigation as hierarchical active inference,https://api.elsevier.com/content/abstract/scopus_id/85109109629,"Localization and mapping has been a long standing area of research, both in neuroscience, to understand how mammals navigate their environment, as well as in robotics, to enable autonomous mobile robots. In this paper, we treat navigation as inferring actions that minimize (expected) variational free energy under a hierarchical generative model. We find that familiar concepts like perception, path integration, localization and mapping naturally emerge from this active inference formulation. Moreover, we show that this model is consistent with models of hippocampal functions, and can be implemented in silico on a real-world robot. Our experiments illustrate that a robot equipped with our hierarchical model is able to generate topologically consistent maps, and correct navigation behaviour is inferred when a goal location is provided to the system.",robotics
10.1016/j.asoc.2021.107601,Journal,Applied Soft Computing,scopus,2021-10-01,sciencedirect,Hierarchical deep reinforcement learning to drag heavy objects by adult-sized humanoid robot,https://api.elsevier.com/content/abstract/scopus_id/85107951694,"Most research on robot manipulation focuses on objects that are light enough for the robot to pick them up. However, in our daily life, some objects are too big or too heavy to be picked up or carried, so that dragging them is necessary. Although bipedal humanoid robots have nowadays good mobility on level ground, dragging unfamiliar objects including large and heavy objects on various surfaces is an interesting research area with many applications, which will provide insights into human manipulation and will encourage the development of novel algorithms for robot motion planning and control. This is a challenging problem, not only because of the unknown and potentially variable friction of the foot, but also because the feet of the robot may slip during unbalanced poses. In this paper, we propose a novel hierarchical deep learning algorithm that learns how to drag heavy objects with an adult-sized humanoid robot for the first time. First, we present a Three-layered Convolution Volumetric Network (TCVN) for 3D object classification with point clouds volumetric occupancy grid integration. Second, we propose a lightweight real-time instance segmentation method named Tiny-YOLACT for the detection and classification of the floor surface. Third, we propose a deep Q-learning algorithm to learn the policy control of the Center of Mass of the robot (DQL-COM). The DQL-COM algorithm learning is bootstrapped using the ROS Gazebo simulator. After initial training, we complete training on the THORMANG-Wolf, a 1.4 m tall adult-sized humanoid robot with 27 degrees of freedom and weighing 48 kg, on three distinct types of surfaces. We evaluate the performance of our approach by dragging eight different types of objects (e.g., a small suitcase, a large suitcase, a chair). The extensive experiments (480 times on the real robot) included dragging a heavy object with a mass of 84.6 kg (two times greater than the robot’s weight) and showed remarkable success rates of 92.92% when combined with the force–torque sensors, and 83.75% without force–torque sensors.",robotics
10.1016/j.rcim.2021.102176,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2021-10-01,sciencedirect,Robotic grasping: from wrench space heuristics to deep learning policies,https://api.elsevier.com/content/abstract/scopus_id/85104603575,"The robotic grasping task persists as a modern industry problem that seeks autonomous, fast implementation, and efficient techniques. Domestic robots are also a reality demanding a delicate and accurate human–machine interaction, with precise robotic grasping and handling. From decades ago, with analytical heuristics, to recent days, with the new deep learning policies, grasping in complex scenarios is still the aim of several works’ that propose distinctive approaches. In this context, this paper aims to cover recent methodologies’ development and discuss them, showing state-of-the-art challenges and the gap to industrial applications deployment. Given the complexity of the related issue associated with the elaborated proposed methods, this paper formulates some fair and transparent definitions for results’ assessment to provide researchers with a clear and standardised idea of the comparison between the new proposals.",robotics
10.1016/j.jsams.2020.10.007,Journal,Journal of Science and Medicine in Sport,scopus,2021-10-01,sciencedirect,The implications of emerging technology on military human performance research priorities,https://api.elsevier.com/content/abstract/scopus_id/85094939116,"Objectives
                  To demonstrate the need for the military human performance research community to anticipate and evolve with the emergence of new and disruptive battlefield technologies that are changing the fundamental role of the human combatant.
               
                  Methods
                  An international team of military performance researchers drew on relevant literature and their individual national perspectives and experiences to provide an integrated forecast of research priorities and needs based on current trends.
               
                  Results
                  Rapid advances and convergence in fields such as robotics, information technology and artificial intelligence will continue to have a revolutionary impact on the battlefield of the future. The disruption associated with these technologies will most acutely be experienced by the human combatant at the tactical level, with increasing cognitive demands associated with the employment and use of new capabilities. New research priorities may include augmented performance of humans-machine teams, enhanced cognitive and immunological resilience based on exercise neurobiology findings, and psychophysiological stress tolerance developed in realistic but safe synthetic training environments. Solving these challenges will require interdisciplinary research teams that have the capacity to work across the physical, digital and biological boundaries whilst collaborating seamlessly with end-users, human combatants. New research methodologies taking full advantage of sensing technologies will be needed to provide rigorous, evidence-based data in real and near-real world environments. Longer term research goals involving biological manipulation will be shaped by moral, legal and ethical considerations and evolving concepts of what it means to be human.
               
                  Conclusion
                  This paper outlines key recommendations to assist military human performance researchers to adapt their practice in order to match the increasing pace of military modernisation. By anticipating technological change and forecasting possible emerging technologies the military human performance research community can manoeuvre to prioritise research activities today in line with future needs and requirements.",robotics
10.1016/j.compag.2021.106350,Journal,Computers and Electronics in Agriculture,scopus,2021-09-01,sciencedirect,Collision-free path planning for a guava-harvesting robot based on recurrent deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85111285188,"In unstructured orchard environments, picking a target fruit without colliding with neighboring branches is a significant challenge for guava-harvesting robots. This paper introduces a fast and robust collision-free path-planning method based on deep reinforcement learning. A recurrent neural network is first adopted to remember and exploit the past states observed by the robot, then a deep deterministic policy gradient algorithm (DDPG) predicts a collision-free path from the states. A simulation environment is developed and its parameters are randomized during the training phase to enable recurrent DDPG to generalize to real-world scenarios. We also introduce an image processing method that uses a deep neural network to detect obstacles and uses many three-dimensional line segments to approximate the obstacles. Simulations show that recurrent DDPG only needs 29 ms to plan a collision-free path with a success rate of 90.90%. Field tests show that recurrent DDPG can increase grasp, detachment, and harvest success rates by 19.43%, 9.11%, and 10.97%, respectively, compared to cases where no collision-free path-planning algorithm is implemented. Recurrent DDPG strikes a strong balance between efficiency and robustness and may be suitable for other fruits.",robotics
10.1016/j.engappai.2021.104382,Journal,Engineering Applications of Artificial Intelligence,scopus,2021-09-01,sciencedirect,Partially Observable Monte Carlo Planning with state variable constraints for mobile robot navigation,https://api.elsevier.com/content/abstract/scopus_id/85111045874,"Autonomous mobile robots employed in industrial applications often operate in complex and uncertain environments. In this paper we propose an approach based on an extension of Partially Observable Monte Carlo Planning (POMCP) for robot velocity regulation in industrial-like environments characterized by uncertain motion difficulties. The velocity selected by POMCP is used by a standard engine controller which deals with path planning. This two-layer approach allows POMCP to exploit prior knowledge on the relationships between task similarities to improve performance in terms of time spent to traverse a path with obstacles. We also propose three measures to support human-understanding of the strategy used by POMCP to improve the performance. The overall architecture is tested on a Turtlebot3 in two environments, a rectangular path and a realistic production line in a research lab. Tests performed on a C++ simulator confirm the capability of the proposed approach to profitably use prior knowledge, achieving a performance improvement from 0.7% to 3.1% depending on the complexity of the path. Experiments on a Unity simulator show that the proposed two-layer approach outperforms also single-layer approaches based only on the engine controller (i.e., without the POMCP layer). In this case the performance improvement is up to 37% comparing to a state-of-the-art deep reinforcement learning engine controller, and up to 51% comparing to the standard ROS engine controller. Finally, experiments in a real-world testing arena confirm the possibility to run the approach on real robots.",robotics
10.1016/j.robot.2021.103830,Journal,Robotics and Autonomous Systems,scopus,2021-09-01,sciencedirect,Visual recognition of gymnastic exercise sequences. Application to supervision and robot learning by demonstration,https://api.elsevier.com/content/abstract/scopus_id/85109177424,"This work presents a novel software architecture to autonomously identify and evaluate the gymnastic activity that people are carrying out. It is composed of three different interconnected layers. The first corresponds to a Multilayer Perceptron (MLP) trained from a set of angular magnitudes derived from the information provided by the OpenPose library. This library works frame by frame, so some postures may be incorrectly detected due to eventual occlusions. The MLP layer makes it possible to accurately identify the posture a person is performing. A second layer, based on a Hidden Markov Model (HMM) and the Viterbi algorithm, filters the incorrect spurious postures. Thus, the accuracy of the algorithm is improved, leading to a precise sequence of postures. A third layer identifies the current exercise and evaluates whether the person is doing it at a correct speed. This layer uses an innovative Modified Levenshtein Distance (MLD), which considers not only the number of operations to transform a given sequence, but also the nature of the elements participating in the comparison. The system works in real time with little delay, thus recognizing sequences of arbitrary length and providing continuous feedback on the exercises being performed. An experiment carried out consisted in reproducing the output of the second layer on an autonomous Pepper robot that can be used in environments where physical exercise is performed, such as a residence for the elderly or others. It has reproduced different exercises previously executed by an instructor so that people can copy the robot. The article analyzes the current situation of the automated gymnastic activities recognition, presents the architecture, the different experiments carried out and the results obtained. The integration of the three components (MLP, HMM and MLD) results in a robust system that has allowed us to improve the results of previous works.",robotics
10.1016/j.robot.2021.103813,Journal,Robotics and Autonomous Systems,scopus,2021-09-01,sciencedirect,Learning whole-image descriptors for real-time loop detection and kidnap recovery under large viewpoint difference,https://api.elsevier.com/content/abstract/scopus_id/85107523220,"We present a real-time stereo visual-inertial-SLAM system which is able to recover from complicated kidnap scenarios and failures online in realtime. We propose to learn the whole-image-descriptor in a weakly supervised manner based on NetVLAD and decoupled convolutions. We analyze the training difficulties in using standard loss formulations and propose an allpairloss and show its effect through extensive experiments. Compared to standard NetVLAD, our network takes an order of magnitude fewer computations and model parameters, as a result runs about three times faster. We evaluate the representation power of our descriptor on standard datasets with precision–recall. Unlike previous loop detection methods which have been evaluated only on fronto-parallel revisits, we evaluate the performance of our method with competing methods on scenarios involving large viewpoint difference. Finally, we present the fully functional system with relative computation and handling of multiple world co-ordinate system which is able to reduce odometry drift, recover from complicated kidnap scenarios and random odometry failures. We open source our fully functional system as an add-on for the popular VINS-Fusion.",robotics
10.1016/j.sysarc.2021.102183,Journal,Journal of Systems Architecture,scopus,2021-09-01,sciencedirect,Memory-efficient deep learning inference with incremental weight loading and data layout reorganization on edge systems,https://api.elsevier.com/content/abstract/scopus_id/85107073021,"Pattern recognition applications such as face recognition and agricultural product detection have drawn a rapid interest on Cyber–Physical–Social-Systems (CPSS). These CPSS applications rely on the deep neural networks (DNN) to conduct the image classification. However, traditional DNN inference models in the cloud could suffer from network delay fluctuations and privacy leakage problems. In this regard, current real-time CPSS applications are preferred to be deployed on edge-end embedded devices. Constrained by the computing power and memory limitations of edge devices, improving the memory management efficacy is the key to improving the quality of service for model inference. First, this study explored the incremental loading strategy of model weights for the model inference. Second, the memory space at runtime is optimized through data layout reorganization from the spatial dimension. In particular, the proposed schemes are orthogonal to existing models. Experimental results demonstrate that the proposed approach reduced the memory consumption by 61.05% without additional inference time overhead.",robotics
10.1016/j.asoc.2021.107465,Journal,Applied Soft Computing,scopus,2021-09-01,sciencedirect,Click-event sound detection in automotive industry using machine/deep learning,https://api.elsevier.com/content/abstract/scopus_id/85105315919,"In the automotive industry, despite the robotic systems on the production lines, factories continue employing workers in several custom tasks getting for semi-automatic assembly operations. Specifically, the assembly of electrical harnesses of engines comprises a set of connections between electrical components. Despite the task is easy to perform, employees tend not to notice that a few components are not being connected properly due to physical fatigue provoked by repetitive tasks. This yields a low quality of the assembly production line and possible hazards. In this work, we propose a sound detection system based on machine/deep learning (ML/DL) approaches to identify click sounds produced when electrical harnesses are connected. The purpose of this system is to count the number of connections properly made and to feedback to the employees. We collect and release a public dataset of 25,000 click sounds of 25 ms length at 22 kHz during three months of assembly operations in an automotive production line located in Mexico. Then, we design an ML/DL-based methodology for click sound detection of assembled harnesses under real conditions of a noisy environment (noise level ranging from 
                        
                           −
                           16
                           .
                           67
                        
                      dB to 
                        
                           −
                           12
                           .
                           87
                        
                      dB) including other machinery sounds. Our best ML/DL model (i.e., a combination between five acoustic features and an optimized convolutional neural network) is able to detect click sounds in a real assembly production line with an accuracy of 
                        
                           94
                           .
                           55
                           ±
                           0
                           .
                           83
                        
                      %. To the best of our knowledge, this is the first time a click sounds detection system in assembling electrical harnesses of engines for giving feedback to the workers is proposed and implemented in a real-world automotive production line. We consider this work valuable for the automotive industry on how to apply ML/DL approaches for improving the quality of semi-automatic assembly operations.",robotics
10.1016/j.asr.2021.04.023,Journal,Advances in Space Research,scopus,2021-08-15,sciencedirect,Interactive imitation learning for spacecraft path-planning in binary asteroid systems,https://api.elsevier.com/content/abstract/scopus_id/85107153834,"Exploration of small body systems poses the problem of designing path planning strategies for possibly uncharted environments. Traditional methods aimed at developing rigorous trajectory baselines may suffer inefficiencies, or turn infeasible when confronted with unknown dynamics. In strongly non-linear dynamics, mapping point design solutions from one dynamical regime to another may be hindered by underlying chaotic behavior. Rather than relying on baseline driven approaches, more generalized strategies may be found by observing human pilots controlling spacecraft motion within varying dynamical environments; the resultant data can then be utilized to initialize machine learning agents to provide more autonomous solutions. A previous numerical experiment resulted in a technical dataset comprising of human-based path planning strategies across a range of binary asteroid systems. This dataset is now used to train various imitation learning agents, and initiate the creation of a framework that integrates human–machine cooperation into the early training phases of artificial intelligent agents; the current application is for spacecraft guidance in binary asteroid systems, as a prototype of complex, potentially unknown, orbit dynamics. An interactive training architecture, based on the DAgger algorithm, is designed and employed to train both original and interactively coached agents, the latter stemming from both corrective and evaluative feedback by a real time human interactor. All agents were interactively trained for a predefined time period. The results from this investigation may provide the first, empirical observations of behavioral cloning within multi-body dynamics with largely randomized parameters, with some notable contributions including early characterization of training time, initial evidence of an autonomous agent learning meaningful policy features via imitation, and early identification of challenges in training fully autonomous agents for a multi-body dynamics path planning problem of this complexity and high dimensional state space.",robotics
10.1016/j.jvcir.2021.103220,Journal,Journal of Visual Communication and Image Representation,scopus,2021-08-01,sciencedirect,S&amp;CNet: A lightweight network for fast and accurate depth completion,https://api.elsevier.com/content/abstract/scopus_id/85111559179,"Dense depth completion is essential for autonomous driving and robotic navigation. Existing methods focused on attaining higher accuracy of the estimated depth, which comes at the price of increasing complexity and cannot be well applied in a real-time system. In this paper, a coarse-to-fine and lightweight network (S&CNet) is proposed for dense depth completion to reduce the computational complexity with negligible sacrifice on accuracy. A dual-stream attention module (S&C enhancer) is proposed according to a new finding of deep neural network-based depth completion, which can capture both the spatial-wise and channel-wise global-range information of extracted features efficiently. Then it is plugged between the encoder and decoder of the coarse estimation network so as to improve the performance. The experiments on KITTI dataset demonstrate that the proposed approach achieves competitive result with respect to state-of-the-art works but via an almost four times faster speed. The S&C enhancer can also be easily plugged into other existing works to boost their performances significantly with negligible additional computations.",robotics
10.1016/j.comcom.2021.03.015,Journal,Computer Communications,scopus,2021-08-01,sciencedirect,Precise grabbing of overlapping objects system based on end-to-end deep neural network,https://api.elsevier.com/content/abstract/scopus_id/85108697361,"In recent years, robotic arm technology is in dire need of reform because of the remarkable advances in artificial intelligence and computer vision. The traditional robotic arm techniques, e.g., template matching algorithm and iterative closest point algorithm, suffer from the low precision issue, especially when the target objects overlap with each other, resulting in inaccurate estimation of overlapping objects. This paper proposes a precise grabbing of overlapping objects system based on an end-to-end deep neural network. The successful grabbing is realized in the case of overlapping objects. First, the datasets needed for network training were established, utilizing structured light to obtain the point cloud information of the arbitrarily placed target objects. Furthermore, we collect the corresponding postures as data labels via the teaching device of the robotic arm, and train the network models using the datasets and labels. Finally, we can predict the postures of the target objects in real time and transmit the results to a robotic arm to complete the grabbing work. The experiment results indicate that the proposed grabbing system can grab small irregular objects accurately, only using the point cloud information, estimating the posture of multiple target objects in the scene simultaneously, and estimating the posture of overlapping small objects in the scene.",robotics
10.1016/j.pmcj.2021.101437,Journal,Pervasive and Mobile Computing,scopus,2021-08-01,sciencedirect,Porting deep neural networks on the edge via dynamic K-means compression: A case study of plant disease detection,https://api.elsevier.com/content/abstract/scopus_id/85108633572,"Cyber Physical Systems (CPS) totally revolutionized the way we interact with the world providing useful services that can support the human being in many aspects of his life. Artificial Intelligence (AI) is another important player for bringing intelligence to CPS and allows the realization of Intelligent Cyber Physical Systems where smart applications can run. However, the constrained hardware of these devices in terms of memory and computing power makes challenging the deployment and execution of powerful algorithms (e.g., deep neural networks). To address this problem, modern solutions involve the use of compression techniques to reduce the memory footprint of deep learning models while saving the accuracy performance. The proposed work focuses on plant disease detection which represents one of the biggest challenges in smart agriculture; in such a context, the possibility to perform a timely diagnosis on crops suspected to be infected can avoid the spread of diseases, thus saving a lot of time and money during the plantation works. In this paper, we realized an intelligent CPS on top of which we implemented an AI application, called Deep Leaf that exploits Convolutional Neural Networks to detect the main biotic stresses affecting crops. To meet the hardware requirements of the Edge device running our application, we propose a novel dynamic compression algorithm based on K-Means for the reduction of models footprint. Experimental results show that our detector is able to correctly classify the plant health condition with an accuracy of 95% and demonstrate the effectiveness of the proposed compression algorithm which is able to maintain the same accuracy of the original 32 bit float model, with an overall memory size reduction of about 85.2%.",robotics
10.1016/j.mechatronics.2021.102576,Journal,Mechatronics,scopus,2021-08-01,sciencedirect,Active learning in robotics: A review of control principles,https://api.elsevier.com/content/abstract/scopus_id/85107977387,"Active learning is a decision-making process. In both abstract and physical settings, active learning demands both analysis and action. This is a review of active learning in robotics, focusing on methods amenable to the demands of embodied learning systems. Robots must be able to learn efficiently and flexibly through continuous online deployment. This poses a distinct set of control-oriented challenges—one must choose suitable measures as objectives, synthesize real-time control, and produce analyses that guarantee performance and safety with limited knowledge of the environment or robot itself. In this work, we survey the fundamental components of robotic active learning systems. We discuss classes of learning tasks that robots typically encounter, measures with which they gauge the information content of observations, and algorithms for generating action plans. Moreover, we provide a variety of examples – from environmental mapping to nonparametric shape estimation – that highlight the qualitative differences between learning tasks, information measures, and control techniques. We conclude with a discussion of control-oriented open challenges, including safety-constrained learning and distributed learning.",robotics
10.1016/j.engappai.2021.104296,Journal,Engineering Applications of Artificial Intelligence,scopus,2021-08-01,sciencedirect,Adaptable automation with modular deep reinforcement learning and policy transfer,https://api.elsevier.com/content/abstract/scopus_id/85107671197,"Future industrial automation systems are anticipated to be shaped by intelligent technologies that allow for the adaptability of machines to the variations and uncertainties in processes and work environments. This paper is motivated by the need for devising new intelligent methods that enable efficient and scalable training of collaborative robots on a variety of tasks that foster their adaptability to new tasks and environments. Recent advances in deep Reinforcement Learning (RL) provide new possibilities to realize this vision. The state-of-the-art in deep RL offers proven algorithms that enable autonomous learning and mastery of a variety of robotic manipulation tasks with minimal human intervention. However, current deep RL algorithms predominantly specialize in a narrow range of tasks, are sample inefficient, and lack sufficient stability, which hinders their adoption in real-life, industrial settings. This paper develops and tests a Hyper-Actor Soft Actor–Critic (HASAC) deep RL framework based on the notions of task modularization and transfer learning to tackle this limitation. The goal of the proposed HASAC is to enhance an agent’s adaptability to new tasks by transferring the learned policies of former tasks to the new task through a ”hyper-actor”. The HASAC framework is tested on the virtual robotic manipulation benchmark, Meta-World. Numerical experiments indicate superior performance by HASAC over state-of-the-art deep RL algorithms in terms of reward value, success rate, and task completion time.",robotics
10.1016/j.renene.2021.04.040,Journal,Renewable Energy,scopus,2021-08-01,sciencedirect,Damage identification of wind turbine blades with deep convolutional neural networks,https://api.elsevier.com/content/abstract/scopus_id/85105896900,"Online early detection of surface damages on blades is critical for the safety of wind turbines, which could avoid catastrophic failures, minimize downtime, and enhance the reliability of the system. Monitoring the health status of blades is attracting more and more attention including on-site cameras and mobile cameras by drones and crawling robots. To deploy fast and efficient damage detection methods from image data, this work presents a hierarchical identification framework for wind turbine blades, which consists of a Haar-AdaBoost step for region proposal and a convolutional neural network (CNN) classifier for damage detection and fault diagnosis. Case studies are carried out on real data set collected from an eastern China wind farm. Results show that (i) the proposed framework can detect and identify the blade damages and outperforms other schemes include SVM and VGG16 models, (ii) sensitive analysis is conducted to validate the robustness of proposed method under limited data conditions, (iii) the proposed scheme is faster than one-step CNN method that directly classifying raw data.",robotics
10.1016/j.jterra.2020.12.002,Journal,Journal of Terramechanics,scopus,2021-08-01,sciencedirect,Recurrent and convolutional neural networks for deep terrain classification by autonomous robots,https://api.elsevier.com/content/abstract/scopus_id/85099601850,"The future challenge for field robots is to increase the level of autonomy towards long distance (>1 km) and duration (>1h) applications. One of the key technologies is the ability to accurately estimate the properties of the traversed terrain to optimize onboard control strategies and energy efficient path-planning, ensuring safety and avoiding possible immobilization conditions that would lead to mission failure. Two main hypotheses are put forward in this research. The first hypothesis is that terrain can be effectively detected by relying exclusively on the measurement of quantities that pertain to the robot-ground interaction, i.e., on proprioceptive signals. Therefore, no visual or depth information is required. Then, artificial deep neural networks can provide an accurate and robust solution to the classification problem of different terrain types. Under these hypotheses, sensory signals are classified as time series directly by a Recurrent Neural Network or by a Convolutional Neural Network in the form of higher-level features or spectrograms resulting from additional processing. In both cases, results obtained from real experiments show comparable or better performance when contrasted with standard Support Vector Machine with the additional advantage of not requiring an a priori definition of the feature space.",robotics
10.1016/j.bdr.2021.100241,Journal,Big Data Research,scopus,2021-07-15,sciencedirect,“Brains” for Robots: Application of the Mivar Expert Systems for Implementation of Autonomous Intelligent Robots,https://api.elsevier.com/content/abstract/scopus_id/85107957013,"Recently the contemporary robotic systems can manipulate different objects and make decisions in a range of situations due to significant advances in innovation technologies and artificial intelligence. The new expert technologies can handle millions of instructions on computers and smartphones, which allow them to be used as a tool to create “decision-making systems” for autonomous robots. The goal of this paper was to create a dynamic algorithm of robot actions that can be used in the decision module has been considered. It is proposed to use Mivar expert systems of a new generation for high-level control. The experiment results showed that Mivar decision-making systems can control groups of small robots and even an unmanned autonomous car in real time. The algorithms created in the Mivar environment can be very flexible, and their build-up depends only on engineering approaches. In addition to traditional low-level robot control systems, a Mivar decision-making system has been implemented, which can be considered as universal “Brains” for autonomous intelligent robots and now knowledge bases can be created and various robots can be trained for practical tasks.",robotics
10.1016/j.enbuild.2021.110967,Journal,Energy and Buildings,scopus,2021-07-15,sciencedirect,"A non-intrusive approach for fault detection and diagnosis of water distribution systems based on image sensors, audio sensors and an inspection robot",https://api.elsevier.com/content/abstract/scopus_id/85104453372,"Fault diagnosis is important to maintain the normal operation of air-conditioning systems, reduce the energy consumption in buildings, and increase the service life of air-conditioning system equipment. We present a novel approach for fault detection and diagnosis system that relies on image and audio sensors and relevant algorithms.
                  This paper proposes a fault diagnosis algorithm based on a robot that can automatically capture audio and image signals from microphone arrays and cameras during inspection in a chiller room. It includes audio- and image-based fault diagnosis algorithms. The validity of the algorithm combined with sensors is verified using data from actual equipment in a chiller room.
                  The audio-based algorithm, which can monitor the abnormal sound of pumps to detect faults, utilizes Fourier transform, a finite impulse response digital filter, and an autoregressive integrated moving average model. We analyze the frequency domain of the pump signal and set the appropriate threshold to monitor abnormal signals based on the fitted model. Meanwhile, the image-based algorithms are divided into three sections to achieve three functions: 1) an AlexNet convolutional neural network is modified to classify the images of the chiller room equipment obtained by the visible light camera; 2) image morphology methods and trigonometric functions are used to read the dials’ indicators acquired by the visible light camera; and 3) optical character recognition is used to obtain the highest temperature value in the infrared image of the pump captured by the infrared camera, which helps maintenance staff verify the operation of the pump and detect faults as soon as possible.
                  These diagnostic algorithms are non-intrusive, low cost, and easy to deploy. Combined with real-time data collection from the sensors on the robot, the algorithms can effectively improve the intelligence of the equipment room and allocate human resources more reasonably.",robotics
10.1016/j.knosys.2021.107025,Journal,Knowledge-Based Systems,scopus,2021-07-08,sciencedirect,Layered Relative Entropy Policy Search,https://api.elsevier.com/content/abstract/scopus_id/85104310246,"Many reinforcement learning problems are hierarchical in nature. Exploiting this property can ease the learning process. In this paper, a hierarchical policy search method based on clustering is presented. We use a hierarchical policy which is composed of a high-level gating policy and a set of low-level sub-policies. Depending on the observed state, the gating policy chooses one of the sub-policies for action selection. The gating policy and each sub-policy is provided with a learning method to update their parameters. If a task is truly hierarchical, its state–action space must have meaningful clusters. We cluster the observed samples and use each cluster to update one sub-policy. This way, each sub-policy is adapted to some portion of the state–action space. Using the updated sub-policy probability density functions and observed samples, the gating policy is updated. We evaluate our method on three multimodal tasks as well as a simulated and real robotic manipulation task. Our experiments show that our method can discover versatile sub-policies for multimodal tasks and the manipulation task. Moreover, we point out the errors in some of the equations of Hierarchical Relative Entropy Policy Search paper and provide the necessary corrections.",robotics
10.1016/j.comnet.2021.108078,Journal,Computer Networks,scopus,2021-07-05,sciencedirect,Transfer reinforcement learning-based road object detection in next generation IoT domain,https://api.elsevier.com/content/abstract/scopus_id/85104603461,"The landscape of fifth generation (5G) and beyond 5G (B5G)-enabled Internet of Things(IoT) is expected to seamlessly and ubiquitously connect everything, which includes 5G, cloud computing, artificial intelligence and other cutting-edge technologies to realize truly intelligent applications in smart cities. In this paper, we present an important key technology for smart city, which is a road target recognition algorithm for smart city applications and designs a set of corresponding programs to assist automatic drivers, pedestrians and visually impaired people in road safety, or to manage city infrastructure. The system can connect robots in cars, wearable devices and body area network in pedestrians or blind people. A target recognition algorithm based on scene fusion is designed to recognize the specific target in the road environment, and transfer reinforcement learning method is used to improve the accuracy and real-time performance of target recognition. The system provides them with travel assistance, identify dangerous or useful objects for them through high-performance target recognition services. It can collect the road visual scene data by road cameras and transmit it to edge devices for training model. The model is collaborated trained in the edge devices and aggregated by the cloud. Based on the transfer reinforcement learning method, the vision-based road target recognition has been implemented, and the accurate and reliable target recognition can be realized. Many details of experiments verify the effectiveness of our technology.",robotics
10.1016/j.jstrokecerebrovasdis.2021.105826,Journal,Journal of Stroke and Cerebrovascular Diseases,scopus,2021-07-01,sciencedirect,Automatic Acute Stroke Symptom Detection and Emergency Medical Systems Alerting by Mobile Health Technologies: A Review,https://api.elsevier.com/content/abstract/scopus_id/85107711467,"Objectives
                  To survey recent advances in acute stroke symptom automatic detection and Emergency Medical Systems (EMS) alerting by mobile health technologies.
               
                  Materials and methods
                  Narrative review
               
                  Results
                  Delayed activation of EMS for stroke symptoms by patients and witnesses deprives patients of rapid access to brain-saving therapies and occurs due to public unawareness of stroke features, cognitive and motor deficits produced by the stroke itself, and sleep onset. A promising emerging approach to overcoming the inherent biologic constraints of patient capacity to self-detect and respond to stroke symptoms is continuous monitoring by mobile health technologies with wireless sensors and artificial intelligence recognition systems. This review surveys 11 sensing technologies - accelerometers, gyroscopes, magnetometers, pressure sensors, touch screen and keyboard input detectors, artificial vision, and artificial hearing; and 10 consumer device form factors in which they are increasingly implemented: smartphones, smart speakers, smart watches and fitness bands, smart speakers/voice assistants, home health robots, smart clothing, smart beds, closed circuit television, smart rings, and desktop/laptop/tablet computers.
               
                  Conclusions
                  The increase in computing power, wearable sensors, and mobile connectivity have ushered in an array of mobile health technologies that can transform stroke detection and EMS activation. By continuously monitoring a diverse range of biometric parameters, commercially available devices provide the technologic capability to detect cardinal language, motor, gait, and sensory signs of stroke onset. Intensified translational research to convert the promise of these technologies to validated, accurate real-world deployments are an important next priority for stroke investigation.",robotics
10.1016/j.media.2021.102058,Journal,Medical Image Analysis,scopus,2021-07-01,sciencedirect,EndoSLAM dataset and an unsupervised monocular visual odometry and depth estimation approach for endoscopic videos,https://api.elsevier.com/content/abstract/scopus_id/85105694449,"Deep learning techniques hold promise to develop dense topography reconstruction and pose estimation methods for endoscopic videos. However, currently available datasets do not support effective quantitative benchmarking. In this paper, we introduce a comprehensive endoscopic SLAM dataset consisting of 3D point cloud data for six porcine organs, capsule and standard endoscopy recordings, synthetically generated data as well as clinically in use conventional endoscope recording of the phantom colon with computed tomography(CT) scan ground truth. A Panda robotic arm, two commercially available capsule endoscopes, three conventional endoscopes with different camera properties, two high precision 3D scanners, and a CT scanner were employed to collect data from eight ex-vivo porcine gastrointestinal (GI)-tract organs and a silicone colon phantom model. In total, 35 sub-datasets are provided with 6D pose ground truth for the ex-vivo part: 18 sub-datasets for colon, 12 sub-datasets for stomach, and 5 sub-datasets for small intestine, while four of these contain polyp-mimicking elevations carried out by an expert gastroenterologist. To verify the applicability of this data for use with real clinical systems, we recorded a video sequence with a state-of-the-art colonoscope from a full representation silicon colon phantom. Synthetic capsule endoscopy frames from stomach, colon, and small intestine with both depth and pose annotations are included to facilitate the study of simulation-to-real transfer learning algorithms. Additionally, we propound Endo-SfMLearner, an unsupervised monocular depth and pose estimation method that combines residual networks with a spatial attention module in order to dictate the network to focus on distinguishable and highly textured tissue regions. The proposed approach makes use of a brightness-aware photometric loss to improve the robustness under fast frame-to-frame illumination changes that are commonly seen in endoscopic videos. To exemplify the use-case of the EndoSLAM dataset, the performance of Endo-SfMLearner is extensively compared with the state-of-the-art: SC-SfMLearner, Monodepth2, and SfMLearner. The codes and the link for the dataset are publicly available at https://github.com/CapsuleEndoscope/EndoSLAM. A video demonstrating the experimental setup and procedure is accessible as Supplementary Video 1.",robotics
10.1016/j.bspc.2021.102629,Journal,Biomedical Signal Processing and Control,scopus,2021-07-01,sciencedirect,Hand gestures recognition from surface electromyogram signal based on self-organizing mapping and radial basis function network,https://api.elsevier.com/content/abstract/scopus_id/85104067433,"Predicting the intention of human hand movements is a practical problem in prosthetic control. In recent years, surface electromyography (sEMG) has been widely used as a signal source in the field of wearable exoskeleton motion recognition and human-computer interaction. However, how to extract the information from sEMG signals and evaluate the intention of human hand movement effectively is still difficult. In order to achieve this goal, this work proposed a processing algorithm based on self-organizing mapping network (SOM) and radial basis neural network (RBF) for feature selection and classification recognition, then the principal component analysis (PCA) to reduce the size of feature vectors was used, finally used for pattern classification from sEMG signals to hand motion. In this research, the classification method mainly used the SOM method to find the hidden nodes centers of the RBF network, the Euclidean distance between the data centers was used to calculate the variance of the node and find the optimal center and radius of the radial basis function, so as to improve the learning performance of RBF network. In the experiment, the MYO armband sensor was used to sample the real sEMG signal data of 6 volunteers under 8 gestures. The experiment result show that the proposed algorithm as a classifier achieves a maximum recognition rate of 100 %, an average recognition accuracy of 96.875 ± 2.7296 %, and a response time of 0.437 s. Meanwhile, the effects of the proposed method on hand motion recognition with different classifiers (RBF with k-means, K-Nearest Neighbor, Multi-Layer Perceptron with Scaled Conjugate Gradient) were compared. The corresponding average accuracy rates were 95.833 ± 3.3244 % (RBF with k-means), 94.583 ± 2.243 % (KNN) and 88.89 ± 1.1324 % (MLP with SCG). Compared with existed methods, the advantages of the method proposed in this research are as follows: 1) This research selects the PCA method and threshold value method based on the short-term average energy (STAE) used to detect the active segment of sEMG signal, so as to select the appropriate feature vector; 2) The proposed algorithm of SOM combined with RBF has higher identification accuracy and efficiency than that of RBF with k-means, which is more conducive to distinguish different actions; 3) While ensuring real-time performance, it can accurately classify gestures that are easy to be confused, indicating that this classification method has a good application prospect in prosthetic control and other fields.",robotics
10.1016/j.robot.2021.103775,Journal,Robotics and Autonomous Systems,scopus,2021-07-01,sciencedirect,6D pose estimation with combined deep learning and 3D vision techniques for a fast and accurate object grasping,https://api.elsevier.com/content/abstract/scopus_id/85102869529,"Real-time robotic grasping, supporting a subsequent precise object-in-hand operation task, is a priority target towards highly advanced autonomous systems. However, such an algorithm which can perform sufficiently-accurate grasping with time efficiency is yet to be found. This paper proposes a novel method with a 2-stage approach that combines a fast 2D object recognition using a deep neural network and a subsequent accurate and fast 6D pose estimation based on Point Pair Feature framework to form a real-time 3D object recognition and grasping solution capable of multi-object class scenes. The proposed solution has a potential to perform robustly on real-time applications, requiring both efficiency and accuracy. In order to validate our method, we conducted extensive and thorough experiments involving laborious preparation of our own dataset. The experiment results show that the proposed method scores 97.37% accuracy in 5cm5deg metric and 99.37% in Average Distance metric. Experiment results have shown an overall 62% relative improvement (5cm5deg metric) and 52.48% (Average Distance metric) by using the proposed method. Moreover, the pose estimation execution also showed an average improvement of 47.6% in running time. Finally, to illustrate the overall efficiency of the system in real-time operations, a pick-and-place robotic experiment is conducted and has shown a convincing success rate with 90% of accuracy. This experiment video is available at https://sites.google.com/view/dl-ppf6dpose/.",robotics
10.1016/j.neunet.2021.01.028,Journal,Neural Networks,scopus,2021-07-01,sciencedirect,Biomimetic FPGA-based spatial navigation model with grid cells and place cells,https://api.elsevier.com/content/abstract/scopus_id/85101901812,"The mammalian spatial navigation system is characterized by an initial divergence of internal representations, with disparate classes of neurons responding to distinct features including location, speed, borders and head direction; an ensuing convergence finally enables navigation and path integration. Here, we report the algorithmic and hardware implementation of biomimetic neural structures encompassing a feed-forward trimodular, multi-layer architecture representing grid-cell, place-cell and decoding modules for navigation. The grid-cell module comprised of neurons that fired in a grid-like pattern, and was built of distinct layers that constituted the dorsoventral span of the medial entorhinal cortex. Each layer was built as an independent continuous attractor network with distinct grid-field spatial scales. The place-cell module comprised of neurons that fired at one or few spatial locations, organized into different clusters based on convergent modular inputs from different grid-cell layers, replicating the gradient in place-field size along the hippocampal dorso-ventral axis. The decoding module, a two-layer neural network that constitutes the convergence of the divergent representations in preceding modules, received inputs from the place-cell module and provided specific coordinates of the navigating object. After vital design optimizations involving all modules, we implemented the tri-modular structure on Zynq Ultrascale+ field-programmable gate array silicon chip, and demonstrated its capacity in precisely estimating the navigational trajectory with minimal overall resource consumption involving a mere 2.92% Look Up Table utilization. Our implementation of a biomimetic, digital spatial navigation system is stable, reliable, reconfigurable, real-time with execution time of about 32 s for 100k input samples (in contrast to 40 minutes on Intel Core i7-7700 CPU with 8 cores clocking at 3.60 GHz) and thus can be deployed for autonomous-robotic navigation without requiring additional sensors.",robotics
10.1016/S2589-7500(21)00005-4,Journal,The Lancet Digital Health,scopus,2021-06-01,sciencedirect,Health information technology and digital innovation for national learning health and care systems,https://api.elsevier.com/content/abstract/scopus_id/85106359380,"Health information technology can support the development of national learning health and care systems, which can be defined as health and care systems that continuously use data-enabled infrastructure to support policy and planning, public health, and personalisation of care. The COVID-19 pandemic has offered an opportunity to assess how well equipped the UK is to leverage health information technology and apply the principles of a national learning health and care system in response to a major public health shock. With the experience acquired during the pandemic, each country within the UK should now re-evaluate their digital health and care strategies. After leaving the EU, UK countries now need to decide to what extent they wish to engage with European efforts to promote interoperability between electronic health records. Major priorities for strengthening health information technology in the UK include achieving the optimal balance between top-down and bottom-up implementation, improving usability and interoperability, developing capacity for handling, processing, and analysing data, addressing privacy and security concerns, and encouraging digital inclusivity. Current and future opportunities include integrating electronic health records across health and care providers, investing in health data science research, generating real-world data, developing artificial intelligence and robotics, and facilitating public–private partnerships. Many ethical challenges and unintended consequences of implementation of health information technology exist. To address these, there is a need to develop regulatory frameworks for the development, management, and procurement of artificial intelligence and health information technology systems, create public–private partnerships, and ethically and safely apply artificial intelligence in the National Health Service.",robotics
10.1016/j.isprsjprs.2021.04.012,Journal,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2021-06-01,sciencedirect,Adversarial unsupervised domain adaptation for 3D semantic segmentation with multi-modal learning,https://api.elsevier.com/content/abstract/scopus_id/85105538507,"Semantic segmentation in 3D point-clouds plays an essential role in various applications, such as autonomous driving, robot control, and mapping. In general, a segmentation model trained on one source domain suffers a severe decline in performance when applied to a different target domain due to the cross-domain discrepancy. Various Unsupervised Domain Adaptation (UDA) approaches have been proposed to tackle this issue. However, most are only for uni-modal data and do not explore how to learn from the multi-modality data containing 2D images and 3D point clouds. We propose an Adversarial Unsupervised Domain Adaptation (AUDA) based 3D semantic segmentation framework for achieving this goal. The proposed AUDA can leverage the complementary information between 2D images and 3D point clouds by cross-modal learning and adversarial learning. On the other hand, there is a highly imbalanced data distribution in real scenarios. We further develop a simple and effective threshold-moving technique during the final inference stage to mitigate this issue. Finally, we conduct experiments on three unsupervised domain adaptation scenarios, ie., Country-to-Country (USA →Singapore), Day-to-Night, and Dataset-to-Dataset (A2D2 →SemanticKITTI). The experimental results demonstrate the effectiveness of proposed method that can significantly improve segmentation performance for rare classes. Code and trained models are available at https://github.com/weiliu-ai/auda.",robotics
10.1016/j.inpa.2020.05.001,Journal,Information Processing in Agriculture,scopus,2021-06-01,sciencedirect,Recognition of cotton growth period for precise spraying based on convolution neural network,https://api.elsevier.com/content/abstract/scopus_id/85084432675,"Dynamic acquisition of crop morphology is beneficial to real-time variable decision of precise spraying operations in fields. However, the existing spraying quantity regulation has high tolerance on the statistical characteristics of regional morphology, so expensive LiDAR and ultrasonic radar can't make full use of their high accuracy, and can reduce decision speed because of too much detail of branches and leaves. Therefore, designing a novel recognition system embedded machine learning with low-cost monocular vision is more feasible, especially in China, where the agricultural implements are medium sizes and cost-sensitive. In addition, we found that the growth period of crops is an important reference index for guiding spraying. So, taking cotton as a case study, a cotton morphology acquisition by a single camera is established, and a cotton growth period recognition algorithm based on Convolution Neural Network (CNN) is proposed in this paper. Through the optimization process based on confusion matrix and recognition efficiency, an optimized CNN model structure is determined from 9 different model structures, and its reliability was verified by changing training sets and test sets many times based on the idea of k-fold test. The accuracy, precision, recall, F1-score and recognition speed of this CNN model are 93.27%, 95.39%, 94.31%, 94.76% and 71.46 ms per image, respectively. In addition, compared with the performance of VGG16 and AlexNet, the convolution neural network model proposed in this paper has better performance. Finally, in order to verify the reliability of the designed recognition system and the feasibility of the spray decision-making algorithm based on CNN, spraying deposition experiments were carried out with 3 different growth-periods of cotton. The experiments’ results validate that after the optimal spray parameters were applied at different growth periods respectively, the average optimum index in 3 growth periods was 42.29%, which was increased up to 62.24% than the operations without distinguishing growth periods.",robotics
10.1016/j.knosys.2021.106918,Journal,Knowledge-Based Systems,scopus,2021-05-23,sciencedirect,Intelligent human action recognition using an ensemble model of evolving deep networks with swarm-based optimization,https://api.elsevier.com/content/abstract/scopus_id/85102146279,"Automatic interpretation of human actions from realistic videos attracts increasing research attention owing to its growing demand in real-world deployments such as biometrics, intelligent robotics, and surveillance. In this research, we propose an ensemble model of evolving deep networks comprising Convolutional Neural Networks (CNNs) and bidirectional Long Short-Term Memory (BLSTM) networks for human action recognition. A swarm intelligence (SI)-based algorithm is also proposed for identifying the optimal hyper-parameters of the deep networks. The SI algorithm plays a crucial role for determining the BLSTM network and learning configurations such as the learning and dropout rates and the number of hidden neurons, in order to establish effective deep features that accurately represent the temporal dynamics of human actions. The proposed SI algorithm incorporates hybrid crossover operators implemented by sine, cosine, and tanh functions for multiple elite offspring signal generation, as well as geometric search coefficients extracted from a three-dimensional super-ellipse surface. Moreover, it employs a versatile search process led by the yielded promising offspring solutions to overcome stagnation. Diverse CNN–BLSTM networks with distinctive hyper-parameter settings are devised. An ensemble model is subsequently constructed by aggregating a set of three optimized CNN–BLSTM​ networks based on the average prediction probabilities. Evaluated using several publicly available human action data sets, our evolving ensemble deep networks illustrate statistically significant superiority over those with default and optimal settings identified by other search methods. The proposed SI algorithm also shows great superiority over several other methods for solving diverse high-dimensional unimodal and multimodal optimization functions with artificial landscapes.",robotics
10.1016/j.eswa.2020.114519,Journal,Expert Systems with Applications,scopus,2021-05-15,sciencedirect,Towards optimal hydro-blasting in reconfigurable climbing system for corroded ship hull cleaning and maintenance,https://api.elsevier.com/content/abstract/scopus_id/85100189762,"The operation of a ship in the ocean depends crucially on the quality of routine offshore dry dock maintenance. Automation by robotics is an efficient solution to address the issues of saving water, energy, time, and easing the labour workload when conducting hydro-blasting hulls in the dry dock ship maintenance industry. In this paper, the automated hydro-blasting in corroded ship hull cleaning by a novel robot platform with reconfigurable manipulators named Hornbill is proposed. The robot is able to maneuver smoothly on a vertical surface by permanent magnetic force, to carry the heavy load, to clean the corroded ship hull by hydro-blasting, and to self-evaluate hydro-blasting task by leveraging the Deep Convolutional Neural Network (DCNN) to synthesis the corrosion level map of the blasted workspace. We also propose an optimal complete waypoint path planning (CWPP) framework to help the robot re-blast the benchmarked workspace. The optimal CWPP problem, including objective functions of the shortest travel distance, the least upward moving direction to reduce water, energy spent while ensuring the visiting of the robot to all uncleaned waypoints defined by benchmarking output, is modeled as the classic Travel Salesman Problem (TSP). The evolutionary-based optimization techniques, including Genetic Algorithm (GA) and Ant Colony Optimization (ACO), are explored to derive the Pareto-optima solution for given TSP. The experimental results show that the magnetic force and motors torque are synchronized to enable the proposed system to navigate smoothly on the vertical surfaces tested with different corrosion levels. The proposed corrosion level benchmarking achieves a mean accuracy of 0.956 with an execution time of 30 fps. Besides, the proposed CWPP enables the proposed robot to yield about 15%, 26%, and 5% the energy, water, and time, respectively, less than the conventional methods when the experiments are conducted in various workspaces on the real ship hull.",robotics
10.1016/j.jvcir.2021.103136,Journal,Journal of Visual Communication and Image Representation,scopus,2021-05-01,sciencedirect,Underwater image super-resolution using multi-stage information distillation networks,https://api.elsevier.com/content/abstract/scopus_id/85105694523,"Recently, single image super-resolution (SISR) has been widely applied in the fields of underwater robot vision and obtained remarkable performance. However, most current methods generally suffered from the problem of a heavy burden on computational resources with large model sizes, which limited their real-world underwater robotic applications. In this paper, we introduce and tackle the super resolution (SR) problem for underwater robot vision and provide an efficient solution for near real-time applications. We present a novel lightweight multi-stage information distillation network, named MSIDN, for better balancing performance against applicability, which aggregates the local distilled features from different stages for more powerful feature representation. Moreover, a novel recursive residual feature distillation (RRFD) module is constructed to progressively extract useful features with a modest number of parameters in each stage. We also propose a channel interaction & distillation (CI&D) module that employs channel split operation on the preceding features to produce two-part features and utilizes the inter channel-wise interaction information between them to generate the distilled features, which can effectively extract the useful information of current stage without extra parameters. Besides, we present USR-2K dataset, a collection of over 1.6K samples for large-scale underwater image SR training, and a testset with an additional 400 samples for benchmark evaluation. Extensive experiments on several standard benchmark datasets show that the proposed MSIDN can provide state-of-the-art or even better performance in both quantitative and qualitative measurements.",robotics
10.1016/j.jbi.2021.103787,Journal,Journal of Biomedical Informatics,scopus,2021-05-01,sciencedirect,Can technological advancements help to alleviate COVID-19 pandemic? a review,https://api.elsevier.com/content/abstract/scopus_id/85104674391,"The COVID-19 pandemic is continuing, and the innovative and efficient contributions of the emerging modern technologies to the pandemic responses are too early and cannot be completely quantified at this moment. Digital technologies are not a final solution but are the tools that facilitate a quick and effective pandemic response. In accordance, mobile applications, robots and drones, social media platforms (such as search engines, Twitter, and Facebook), television, and associated technologies deployed in tackling the COVID-19 (SARS-CoV-2) outbreak are discussed adequately, emphasizing the current-state-of-art. A collective discussion on reported literature, press releases, and organizational claims are reviewed. This review addresses and highlights how these effective modern technological solutions can aid in healthcare (involving contact tracing, real-time isolation monitoring/screening, disinfection, quarantine enforcement, syndromic surveillance, and mental health), communication (involving remote assistance, information sharing, and communication support), logistics, tourism, and hospitality. The study discusses the benefits of these digital technologies in curtailing the pandemic and ‘how’ the different sectors adapted to these in a shorter period. Social media and television’s role in ensuring global connectivity and serving as a common platform to share authentic information among the general public were summarized. The World Health Organization and Governments’ role globally in-line with the prevention of propagation of false news, spreading awareness, and diminishing the severity of the COVID-19 was discussed. Furthermore, this collective review is helpful to investigators, health departments, Government organizations, and policymakers alike to facilitate a quick and effective pandemic response.",robotics
10.1016/j.compag.2021.106091,Journal,Computers and Electronics in Agriculture,scopus,2021-05-01,sciencedirect,DeepWay: A Deep Learning waypoint estimator for global path generation,https://api.elsevier.com/content/abstract/scopus_id/85103275872,"Agriculture 3.0 and 4.0 have gradually introduced service robotics and automation into several agricultural processes, mostly improving crops quality and seasonal yield. Row-based crops are the perfect settings to test and deploy smart machines capable of monitoring and manage the harvest. In this context, global path generation is essential either for ground or aerial vehicles, and it is the starting point for every type of mission plan. Nevertheless, little attention has been currently given to this problem by the research community and global path generation automation is still far to be solved. In order to generate a viable path for an autonomous machine, the presented research proposes a feature learning fully convolutional model capable of estimating waypoints given an occupancy grid map. In particular, we apply the proposed data-driven methodology to the specific case of row-based crops with the general objective to generate a global path able to cover the extension of the crop completely. Extensive experimentation with a custom made synthetic dataset and real satellite-derived images of different scenarios have proved the effectiveness of our methodology and demonstrated the feasibility of an end-to-end and completely autonomous global path planner.",robotics
10.1016/j.autcon.2021.103569,Journal,Automation in Construction,scopus,2021-05-01,sciencedirect,Robotic assembly of timber joints using reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85101596404,"In architectural construction, automated robotic assembly is challenging due to occurring tolerances, small series production and complex contact situations, especially in assembly of elements with form-closure such as timber structures with integral joints. This paper proposes to apply Reinforcement Learning to control robot movements in contact-rich and tolerance-prone assembly tasks and presents the first successful demonstration of this approach in the context of architectural construction. Exemplified by assembly of lap joints for custom timber frames, robot movements are guided by force/torque and pose data to insert a timber element in its mating counterpart(s). Using an adapted Ape-X DDPG algorithm, the control policy is trained entirely in simulation and successfully deployed in reality. The experiments show the policy can also generalize to situations in real world not seen in training, such as tolerances and shape variations. This caters to uncertainties occurring in construction processes and facilitates fabrication of differentiated, customized designs.",robotics
10.1016/j.robot.2021.103735,Journal,Robotics and Autonomous Systems,scopus,2021-05-01,sciencedirect,Accurate and real-time human-joint-position estimation for a patient-transfer robot using a two-level convolutional neutral network,https://api.elsevier.com/content/abstract/scopus_id/85101411793,"Human-joint-position estimation is crucial for patient-transfer robots. However, high accuracy and real-time property are difficult to achieve simultaneously. To tackle the problem, we develop a new convolutional neural network (CNN), containing two levels of subnetworks, to fuse the information in color and depth images. The first-level subnetwork generates two-dimensional (2D) human joint positions from a color image by the part-affinity-fields method. The second-level subnetwork estimates 3D human-joint positions from 2D ones and corresponding depth images. Here, strong feature-extraction function of the CNN may suppress the negative effect caused by invalid information in depth images. Meanwhile, all the estimations are implemented with the 2D CNNs, which may cause higher time-efficiency than 3D ones (mostly used in previous studies). To assess the validity, first we employed the CNN to estimate human joint positions, and obtained the accuracy and speed of respectively 90.3% and 210 ms (implemented with an affordable processing unit). Then we applied the CNN to a dual-arm nursing-care robot and found that the accuracy and processing speed satisfied the requirements in practical usage; these validated the effectiveness of our proposal and provided a new approach to generate 3D-human-joint positions through information fusion of color and depth images.",robotics
10.1016/j.patrec.2021.02.001,Journal,Pattern Recognition Letters,scopus,2021-05-01,sciencedirect,ACDnet: An action detection network for real-time edge computing based on flow-guided feature approximation and memory aggregation,https://api.elsevier.com/content/abstract/scopus_id/85101180148,"Interpreting human actions requires understanding the spatial and temporal context of the scenes. State-of-the-art action detectors based on Convolutional Neural Network (CNN) have demonstrated remarkable results by adopting two-stream or 3D CNN architectures. However, these methods typically operate in a non-real-time, ofline fashion due to system complexity to reason spatio-temporal information. Consequently, their high computational cost is not compliant with emerging real-world scenarios such as service robots or public surveillance where detection needs to take place at resource-limited edge devices. In this paper, we propose ACDnet, a compact action detection network targeting real-time edge computing which addresses both efficiency and accuracy. It intelligently exploits the temporal coherence between successive video frames to approximate their CNN features rather than naively extracting them. It also integrates memory feature aggregation from past video frames to enhance current detection stability, implicitly modeling long temporal cues over time. Experiments conducted on the public benchmark datasets UCF-24 and JHMDB-21 demonstrate that ACDnet, when integrated with the SSD detector, can robustly achieve detection well above real-time (75 FPS). At the same time, it retains reasonable accuracy (70.92 and 49.53 frame mAP) compared to other top-performing methods using far heavier configurations. Codes will be available at https://github.com/dginhac/ACDnet.",robotics
10.1016/j.atmosres.2021.105490,Journal,Atmospheric Research,scopus,2021-05-01,sciencedirect,Changes of ammonia concentrations in wintertime on the North China Plain from 2018 to 2020,https://api.elsevier.com/content/abstract/scopus_id/85100690328,"The reduced economic and social activities during the Chinese Spring Festival provide a unique experiment to evaluate reductions in anthropogenic NH3 emissions in China. However, quantifying this unique scenario is challenging as meteorology may mask the real changes in observed NH3 concentrations. Here, we applied a machine learning technique to decouple the effects of meteorology and confirmed that the real (deweathered) NH3 concentration dropped to a minimum during the Spring Festival in 2019 and 2020 at both urban (Beijing) and rural (Xianghe) sites on the North China Plain. Compared with the scenario without the Spring Festival effect, we predicted that NH3 concentrations in 2020 were 39.8% and 24.6% higher than the observed values at the urban and rural sites, respectively. The significant difference between the two sites indicates a larger reduction in anthropogenic NH3 emissions in urban areas than in rural areas due to the Spring Festival and lockdown measures of COVID-19. Future control strategies should consider the emissions of NH3 from the transportation, industrial and residential sectors, considering that agricultural emissions are minor in cold seasons.",robotics
10.1016/j.future.2020.11.007,Journal,Future Generation Computer Systems,scopus,2021-05-01,sciencedirect,Learning hierarchical face representation to enhance HCI among medical robots,https://api.elsevier.com/content/abstract/scopus_id/85100215322,"In this paper, we propose a hierarchical framework for face recognition by learning deep representation. In order to exploit key patches for face recognition, we separate the entire image into several patches including eyes, nose, and mouth. A binary facegrid is generated to indicate the accurate position of the key patches in face image. The patches are fed into the hierarchical framework to learn the deep representation of the image. We leverage the PCA and SVM method for face recognition. Our face representation can enhance many medical robot applications. Comprehensive experiments have demonstrated that our proposed method can effectively recognize real human faces from fake samples.",robotics
10.1016/j.neucom.2020.12.113,Journal,Neurocomputing,scopus,2021-04-28,sciencedirect,High speed long-term visual object tracking algorithm for real robot systems,https://api.elsevier.com/content/abstract/scopus_id/85099790072,"Although many visual tracking algorithms have made many achievements in video sequences, they have not been confirmed to work well on the real robot systems with the unpredictable changes and limited computing capabilities. In order to face the complex practical conditions, including huge scale variation, occlusion and long-term task, this paper develops a CF-based long-term tracking algorithm. The main strategies are as follows. A novel confidence score is proposed to judge tracking reliability, and the tracking drift is corrected to keep the target’s long-term appearance. Furthermore, once the target is lost, it can be relocated by the multi-scale search. Our tracker performs favorably against other CF-based trackers with strong engineering applicability. Finally, experiments on the datasets and an UAV are carried out to verify the effectiveness for real robot systems.",robotics
10.1016/j.knosys.2021.106839,Journal,Knowledge-Based Systems,scopus,2021-04-22,sciencedirect,Graph neural network for 6D object pose estimation[Formula presented],https://api.elsevier.com/content/abstract/scopus_id/85101389096,"6D object pose estimation plays an important role in various applications such as robot manipulation and virtual reality. In this paper, we introduce a graph convolution neural network based method to addresses the problem of estimating the 6D pose of objects from a single RGB-D image. The proposed method fuses the appearance feature of the RGB image with the geometry feature of point clouds to predict pixel-level pose and the network also predicts pixel-level confidences to prune outlier predictions. The inner structure information of point cloud is learned by a graph convolution neural network. Specially, we adopt a residual graph convolution module to learn a discriminative feature. Our network enables end-to-end training and fast inference. The extensive experiments verify the method and the model achieves state-of-the-art for the LINEMOD and LINEMOD-OCCLUSION dataset (ADD-S: 88.68 and 65.38 respectively).",robotics
10.1016/j.biosystemseng.2021.01.014,Journal,Biosystems Engineering,scopus,2021-04-01,sciencedirect,Combining generative adversarial networks and agricultural transfer learning for weeds identification,https://api.elsevier.com/content/abstract/scopus_id/85100388257,"In recent years, automatic weed control has emerged as a promising alternative for reducing the amount of herbicide applied to the field, instead of conventional spraying. The use of artificial intelligence through the implementation of deep learning for early weeds identification has been one of the engines to boost this progress. However, these techniques usually need very large datasets coping with real-world conditions, which are scarce in the agricultural domain. To address the lack of such datasets, this paper proposes a methodology that combines the use of agricultural transfer learning and the creation of artificial images by generative adversarial networks (GANs). Several architectures and configurations have been evaluated on a dataset containing images of tomato and black nightshade. The best configuration was a combination of GANs creating plausible synthetic images and the Xception network, with a performance of 99.07% on the test set and 93.23% on a noisy version of the same set. Other architectures, such as Inception or DenseNet have also been evaluated, and they obtained promising results by using GANs. According to the results, the combination of advanced transfer learning and data augmentation techniques through GANs should be deeply studied in the future with more complex datasets.",robotics
10.1016/j.robot.2021.103730,Journal,Robotics and Autonomous Systems,scopus,2021-04-01,sciencedirect,Learning image-based Receding Horizon Planning for manipulation in clutter,https://api.elsevier.com/content/abstract/scopus_id/85100235606,"The manipulation of an object into a desired location in a cluttered and restricted environment requires reasoning over the long-term consequences of an action while reacting locally to the multiple physics-based interactions. We present Visual Receding Horizon Planning (VisualRHP) in a framework which interleaves real-world execution with look-ahead planning to efficiently solve a short-horizon approximation to a multi-step sequential decision making problem. VisualRHP is guided by a learned heuristic that acts on an abstract colour-labelled image-based representation of the state. With this representation, the robot can generalize its behaviours to different environment setups, that is, different number and shape of objects, while also having transferable manipulation skills that can be applied to a multitude of real-world objects. We train the heuristic with imitation and reinforcement learning in discrete and continuous actions spaces. We detail our heuristic learning process for environments with sparse rewards, and non-linear, non-continuous, dynamics. In particular, we introduce necessary changes for improving the stability of existing reinforcement learning algorithms that use neural networks with shared parameters. In a series of simulation and real-world experiments, we show the robot performing prehensile and non-prehensile actions in synergy to successfully manipulate a variety of real-world objects in real-time.",robotics
10.1016/j.neucom.2020.10.097,Journal,Neurocomputing,scopus,2021-03-21,sciencedirect,3D-RVP: A method for 3D object reconstruction from a single depth view using voxel and point,https://api.elsevier.com/content/abstract/scopus_id/85097471582,"Three-dimensional object reconstruction technology has a wide range of applications such as augment reality, virtual reality, industrial manufacturing and intelligent robotics. Although deep learning-based 3D object reconstruction technology has developed rapidly in recent years, there remain important problems to be solved. One of them is that the resolution of reconstructed 3D models is hard to improve because of the limitation of memory and computational efficiency when deployed on resource-limited devices. In this paper, we propose 3D-RVP to reconstruct a complete and accurate 3D geometry from a single depth view, where R, V and P represent Reconstruction, Voxel and Point, respectively. It is a novel two-stage method that combines a 3D encoder-decoder network with a point prediction network. In the first stage, we propose a 3D encoder-decoder network with residual learning to output coarse prediction results. In the second stage, we propose an iterative subdivision algorithm to predict the labels of adaptively selected points. The proposed method can output high-resolution 3D models by increasing a small number of parameters. Experiments are conducted on widely used benchmarks of a ShapeNet dataset in which four categories of models are selected to test the performance of neural networks. Experimental results show that our proposed method outperforms the state-of-the-arts, and achieves about 
                        
                           2.7
                           %
                        
                      improvement in terms of the intersection-over-union metric.",robotics
10.1016/j.neunet.2020.12.001,Journal,Neural Networks,scopus,2021-03-01,sciencedirect,Modular deep reinforcement learning from reward and punishment for robot navigation,https://api.elsevier.com/content/abstract/scopus_id/85098173477,"Modular Reinforcement Learning decomposes a monolithic task into several tasks with sub-goals and learns each one in parallel to solve the original problem. Such learning patterns can be traced in the brains of animals. Recent evidence in neuroscience shows that animals utilize separate systems for processing rewards and punishments, illuminating a different perspective for modularizing Reinforcement Learning tasks. MaxPain and its deep variant, Deep MaxPain, showed the advances of such dichotomy-based decomposing architecture over conventional Q-learning in terms of safety and learning efficiency. These two methods differ in policy derivation. MaxPain linearly unified the reward and punishment value functions and generated a joint policy based on unified values; Deep MaxPain tackled scaling problems in high-dimensional cases by linearly forming a joint policy from two sub-policies obtained from their value functions. However, the mixing weights in both methods were determined manually, causing inadequate use of the learned modules. In this work, we discuss the signal scaling of reward and punishment related to discounting factor 
                        γ
                     , and propose a weak constraint for signaling design. To further exploit the learning models, we propose a state-value dependent weighting scheme that automatically tunes the mixing weights: hard-max and softmax based on a case analysis of Boltzmann distribution. We focus on maze-solving navigation tasks and investigate how two metrics (pain-avoiding and goal-reaching) influence each other’s behaviors during learning. We propose a sensor fusion network structure that utilizes lidar and images captured by a monocular camera instead of lidar-only and image-only sensing. Our results, both in the simulation of three types of mazes with different complexities and a real robot experiment of an L-maze on Turtlebot3 Waffle Pi, showed the improvements of our methods.",robotics
10.1016/j.cogsys.2020.10.021,Journal,Cognitive Systems Research,scopus,2021-03-01,sciencedirect,Causal cognitive architecture 1: Integration of connectionist elements into a navigation-based framework,https://api.elsevier.com/content/abstract/scopus_id/85097355403,"The brain-inspired Causal Cognitive Architecture 1 (CCA1) tightly integrates the sensory processing capabilities found in neural networks with many of the causal abilities found in human cognition. Causality emerges not from a central controlling stored program but directly from the architecture. Sensory input vectors are processed by robust association circuitry and then propagated to a navigational temporary map. Instinctive and learned objects and procedures are applied to the same temporary map, with a resultant navigation signal obtained. Navigation can similarly be for the physical world as well as for a landscape of higher cognitive concepts. There is good explainability for causal decisions. A simulation of the CCA1 controlling a search and rescue robot is presented with the goal of finding and rescuing a lost hiker within a grid world. A simulation of the CCA1 controlling a repair robot is presented that can predict the movement of a series of gears.",robotics
10.1016/j.compag.2020.105967,Journal,Computers and Electronics in Agriculture,scopus,2021-02-01,sciencedirect,A robot-based intelligent management design for agricultural cyber-physical systems,https://api.elsevier.com/content/abstract/scopus_id/85100144946,"This work proposes a robot-based intelligent management design that can intelligently manage the opening growth environments of crops. Different from the most existing agricultural robots, the FPGA device is used as the computing architecture of the proposed robot. In this robot design, a binarized neural network (BNN) hardware module provides the real-time and accurate detection of target crops, while cryptographic hardware functions ensure the security of sensor data transfers in an unsupervised environment. Based on a layered and virtualizable design method, the robot can not only control the actuators to maintain the ideal growth environment of crops but also dynamically adapt its cryptographic hardware functions to meet different requirements. Experiments show that, compared to the conventional microprocessor-based architecture, in this proposed robot, the processing time of cryptographic functions can be reduced by 
                        
                           99.46
                           %
                        
                      to 
                        
                           99.62
                           %
                        
                     , while the BNN inference can accelerate by a factor of 13,359.6 on average. To support all the AES, DESand 3DES functions, through system adaptivity, the robot design can reduce 
                        
                           37.4
                           %
                        
                      of slice LUTs and 
                        
                           29.81
                           %
                        
                      of slice registers in a Xilinx Zynq XC7Z020 device, while it can result in a power reduction of 
                        
                           13.6
                           %
                        
                     .",robotics
10.1016/j.robot.2020.103701,Journal,Robotics and Autonomous Systems,scopus,2021-02-01,sciencedirect,On deep learning techniques to boost monocular depth estimation for autonomous navigation,https://api.elsevier.com/content/abstract/scopus_id/85098871371,"Inferring the depth of images is a fundamental inverse problem within the field of Computer Vision since depth information is obtained through 2D images, which can be generated from infinite possibilities of observed real scenes. Benefiting from the progress of Convolutional Neural Networks (CNNs) to explore structural features and spatial image information, Single Image Depth Estimation (SIDE) is often highlighted in scopes of scientific and technological innovation, as this concept provides advantages related to its low implementation cost and robustness to environmental conditions. In the context of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by producing high-quality depth maps, which are essential during the autonomous navigation process in different locations. However, such networks are usually supervised by sparse and noisy depth data, from Light Detection and Ranging (LiDAR) laser scans, and are carried out at high computational cost, requiring high-performance Graphic Processing Units (GPUs). Therefore, we propose a new lightweight and fast supervised CNN architecture combined with novel feature extraction models which are designed for real-world autonomous navigation. We also introduce an efficient surface normals module, jointly with a simple geometric 2.5D loss function, to solve SIDE problems. We also innovate by incorporating multiple Deep Learning techniques, such as the use of densification algorithms and additional semantic, surface normals and depth information to train our framework. The method introduced in this work focuses on robotic applications in indoor and outdoor environments and its results are evaluated on the competitive and publicly available NYU Depth V2 and KITTI Depth datasets.",robotics
10.1016/j.micpro.2020.103301,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,IoT enabled cancer prediction system to enhance the authentication and security using cloud computing,https://api.elsevier.com/content/abstract/scopus_id/85094168107,"In recent days, Internet of Things, Cloud Computing, Deep learning, Machine learning and Artificial Intelligence are considered to be an emerging technologies to solve variety of real world problems. These techniques are importantly applied in various fields such as healthcare systems, transportation systems, agriculture and smart cities to produce fruitful results for number of issues in today's environment. This research work focuses on one such application in the field of IoT together with cloud computing. More number of sensors that are deployed in human body is used to collect patient related data such as deviation in body temperature and others which leads to variation in blood cells that turned to be cancerous cells. Main intention of this work is design a cancer prediction system using Internet of Things upon extracting the details of blood results to test whether it is normal or abnormal. In addition to this, encryption is done on the blood results of cancer affected patient and store it in cloud for quick reference through Internet for the doctor or healthcare nurse to handle the patient data secretly. This research work concentrates on enhancing the health care computations and processing. It provides a framework to enhance the performance of the existing health care industry across the globe. As the entire medical data has to be saved in cloud, the traditional medical treatment limitations can be overcome. Encryption and decryption is done using AES algorithm in order to provide authentication and security in handling cancer patients. The main focus is to handle healthcare data effectively for the patient when they are away from the home town since the needed cancer treatment details are stored in cloud. The task completion time is greatly reduce from 400 to 160  by using VMs. CloudSim gives an adaptable simulation structure that empowers displaying and reproduced results.",robotics
10.1016/j.rcim.2020.102029,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2021-02-01,sciencedirect,Towards manufacturing robotics accuracy degradation assessment: A vision-based data-driven implementation,https://api.elsevier.com/content/abstract/scopus_id/85088120602,"In this manuscript we report on a vision-based data-driven methodology for industrial robot health assessment. We provide an experimental evidence of the usefulness of our methodology on a system comprised of a 6-axis industrial robot, two monocular cameras and five binary squared fiducial markers. The fiducial marker system permits to accurately track the deviation of the end-effector along a fixed non-trivial trajectory. Moreover, we monitor the trajectory deflection using three gradually increasing weights attached to the end-effector. When the robot is loaded with the maximum allowed payload, a deviation of 0.77mm is identified in the Z-coordinate of the end-effector. Tracing trajectory information, we train five supervised learning regression models. Such models are afterwards used to predict the deviation of the end-effector, using the pose estimation provided by the visual tracking system. As a result of this study, we show that this procedure is a stable, robust, rigorous and reliable tool for robot trajectory deviation estimation and it even allows to identify the mechanical element producing non-kinematic errors.",robotics
10.1016/j.ejso.2020.04.010,Journal,European Journal of Surgical Oncology,scopus,2021-02-01,sciencedirect,Peroperative personalised decision support and analytics for colon cancer surgery- Short report,https://api.elsevier.com/content/abstract/scopus_id/85083856433,"Advanced instrumentation whether robotic or non-robotic- hasn't itself made for better surgery as all critical measures of operative success depend still on intraoperative surgeon judgement and decision-making. Computer assisted surgery, or digital surgery, refers to the combination of technology with real-time data during an operation and is often assumed to need new hardware platforms to become a reality. However, methods to support personalised surgical endeavour exist now and can be deployed today within standard laparoscopic paradigms. Here we describe in detail the rationale for the deployment of such assistance for surgical step-advancement in our current practice evolution from traditional proximal colon cancer resection to complete mesocolic excision focussing on personalised 3d anatomical display, intraoperative, quantificative fluorescence assessment of intracorporeal anastomoses and postoperative digital feedback to enable reflection and identify areas of technical improvement.",robotics
10.1016/j.paid.2020.109969,Journal,Personality and Individual Differences,scopus,2021-02-01,sciencedirect,"Evolution and revolution: Personality research for the coming world of robots, artificial intelligence, and autonomous systems",https://api.elsevier.com/content/abstract/scopus_id/85081938220,"In forty years, human existence will be radically transformed by advances in information technology, including Artificial Intelligence, robots capable of social agency, and other autonomous physical and virtual systems. Future personality research must assess, understand, and apply individual differences in adaptation to these novel challenges. This review article discusses directions for future personality research. Cross-cultural research provides a model, in that both universal traits and those specific to future society are needed. Evolution of major “etic” trait models of today will maintain their relevance. There is also scope for defining a range of new “emic” dimensions for constructs such as trust in autonomy, mental models for robots, anthropomorphism of technology, and preferences for communication with machines. A more revolutionary perspective is that availability of big data on the individual will revive idiographic perspectives. Both nomothetic and idiographic accounts of personality may support applications such as design of intelligent systems and products that adapt to the individual.",robotics
10.1016/j.procs.2021.09.173,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Safe learning for control using control lyapunov functions and control barrier functions: A review,https://api.elsevier.com/content/abstract/scopus_id/85116855071,"Real-world autonomous systems are often controlled using conventional model-based control methods. But if accurate models of a system are not available, these methods may be unsuitable. For many safety-critical systems, such as robotic systems, a model of the system and a control strategy may be learned using data. When applying learning to safety-critical systems, guaranteeing safety during learning as well as testing/deployment is paramount. A variety of different approaches for ensuring safety exists, but the published works are cluttered and there are few reviews that compare the latest approaches. This paper reviews two promising approaches on guaranteeing safety for learning-based robust control of uncertain dynamical systems, which are based on control barrier functions and control Lyapunov functions. While control barrier functions provide an option to incorporate safety in terms of constraint satisfaction, control Lyapunov functions are used to define safety in terms of stability. This review categorises learning-based methods that use control barrier functions and control Lyapunov functions into three groups, namely reinforcement learning, online and offline supervised learning. Finally, the paper presents a discussion of the suitability of the different methods for different applications.",robotics
10.1016/j.ejrs.2021.08.007,Journal,Egyptian Journal of Remote Sensing and Space Science,scopus,2021-01-01,sciencedirect,Smart farming for improving agricultural management,https://api.elsevier.com/content/abstract/scopus_id/85114414365,"The food shortage and the population growth are the most challenges facing sustainable development worldwide. Advanced technologies such as artificial intelligence (AI), the Internet of Things (IoT), and the mobile internet can provide realistic solutions to the challenges that are facing the world. Therefore, this work focuses on the new approaches regarding smart farming (SF) from 2019 to 2021, where the work illustrates the data gathering, transmission, storage, analysis, and also, suitable solutions. IoT is one of the essential pillars in smart systems, as it connects sensor devices to perform various basic tasks. The smart irrigation system included those sensors for monitoring water level, irrigation efficiency, climate, etc. Smart irrigation is based on smart controllers and sensors as well as some mathematical relations. In addition, this work illustrated the application of unmanned aerial vehicles (UAV) and robots, where they can be achieved several functions such as harvesting, seedling, weed detection, irrigation, spraying of agricultural pests, livestock applications, etc. real-time using IoT, artificial intelligence (AI), deep learning (DL), machine learning (ML) and wireless communications. Moreover, this work demonstrates the importance of using a 5G mobile network in developing smart systems, as it leads to high-speed data transfer, up to 20 Gbps, and can link a large number of devices per square kilometer. Although the applications of smart farming in developing countries are facing several challenges, this work highlighted some approaches the smart farming. In addition, the implementation of Smart Decision Support Systems (SDSS) in developing countries supports the real-time analysis, mapping of soil characteristics and also helps to make proper decision management. Finally, smart agriculture in developing countries needs more support from governments at the small farms and the private sector.",robotics
10.1016/j.procs.2021.04.159,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Transfer Learning with Demonstration Forgetting for Robotic Manipulator,https://api.elsevier.com/content/abstract/scopus_id/85112691815,Deep learning and especially deep reinforcement learning usually require huge amount of data for training and simulators using is perspective approach to provide this data. Model trained in simulator can be transferred on real robot without wasting a lot of time to collect data. Training in simulator also allows using of different techniques to speed up convergence and increase resulting performance. One of them is to train feature-based model with access to the whole information about the environment and use it as expert for main image-based model. This reduces earning time and the computational costs that are necessary to obtain quality results with image-based model. In this work we improve idea of behaviour cloning feature agent and make it more flexible with using of expert demonstrations forgetting. We conducted experiments on transfer learning for a robotic manipulator that interacts with complex objects and compared them with classic off-policy approaches.,robotics
10.1016/B978-0-323-88506-5.50194-7,Book Series,Computer Aided Chemical Engineering,scopus,2021-01-01,sciencedirect,Attack Detection Using Unsupervised Learning Algorithms in Cyber-Physical Systems,https://api.elsevier.com/content/abstract/scopus_id/85110277992,"Cyber-Physical Systems (CPS) are collections of physical and computer components that are integrated with each other to operate a process safely and efficiently. Examples of CPS include industrial control systems, water systems, robotics systems, smart grid, etc. However, the security aspect of CPS is still a concern that makes them vulnerable to cyber attacks on the control elements, network or physical systems. The work reported here is an attempt towards detecting cyber attacks and improving process monitoring in CPS; using unsupervised machine learning anomaly detection algorithms such as one-class SVM, isolation forest, elliptic envelope. These algorithms are evaluated using the dataset of a real Water Distribution Plant (WADI) built at the iTrust centre at Singapore University of Technology and Design for cyber security research. For modelling purposes, process 1 and 2 of the aforementioned plant were taken into consideration because the implemented attacks were closely related to only these sub-processes. The result of the experiment shows that one-class SVM is found to be the most effective algorithm in determining anomalies for this particular dataset.",robotics
10.1016/j.isatra.2021.06.017,Journal,ISA Transactions,scopus,2021-01-01,sciencedirect,Multi-objective optimization technique for trajectory planning of multi-humanoid robots in cluttered terrain,https://api.elsevier.com/content/abstract/scopus_id/85108514869,"Humanoid robots hold a decent advantage over wheeled robots because of their ability to mimic human exile. The presented paper proposes a novel strategy for trajectory planning in a cluttered terrain using the hybridized controller modeled on the basis of modified MANFIS (multiple adaptive neuro-fuzzy inference system) and MOSFO (multi-objective sunflower optimization) techniques. The controller works in a two-step mechanism. The input parameters, i.e., obstacle distances and target direction, are first fed to the MANFIS controller, which generates a steering angle in both directions of an obstacle to dodge it. The intermediate steering angles are obtained based on the training model. The final steering angle to avoid obstacles is selected based on the direction of the target and additional obstacles in the path. It is further works as input for the MOSFO technique, which provides the ultimate steering angle. Using the proposed technique, various simulations are carried out in the WEBOT simulator, which shows a deviation under 5% when the results are validated in real-time experiments, revealing the technique to be robust. To resolve the complication of providing preference to the robot during deadlock condition in multi-humanoids system, the dining philosopher controller is implemented. The efficiency of the proposed technique is examined through the comparisons with the default controller of NAO based on toques produces at various joints that present an average improvement of 6.12%, 7.05% and 15.04% in ankle, knee and hip, respectively. It is further compared against the existed navigational strategy in multiple robot systems that also displays an acceptable improvement in travel length. In comparison in reference to the existing controller, the proposed technique emerges to be a clear winner by portraying its superiority.",robotics
10.1016/j.isatra.2021.06.010,Journal,ISA Transactions,scopus,2021-01-01,sciencedirect,A real-world application of Markov chain Monte Carlo method for Bayesian trajectory control of a robotic manipulator,https://api.elsevier.com/content/abstract/scopus_id/85108508566,"Reinforcement learning methods are being applied to control problems in robotics domain. These algorithms are well suited for dealing with the continuous large scale state spaces in robotics field. Even though policy search methods related to stochastic gradient optimization algorithms have become a successful candidate for coping with challenging robotics and control problems in recent years, they may become unstable when abrupt variations occur in gradient computations. Moreover, they may end up with a locally optimal solution. To avoid these disadvantages, a Markov chain Monte Carlo (MCMC) algorithm for policy learning under the RL configuration is proposed. The policy space is explored in a non-contiguous manner such that higher reward regions have a higher probability of being visited. The proposed algorithm is applied in a risk-sensitive setting where the reward structure is multiplicative. Our method has the advantages of being model-free and gradient-free, as well as being suitable for real-world implementation. The merits of the proposed algorithm are shown with experimental evaluations on a 2-Degree of Freedom robot arm. The experiments demonstrate that it can perform a thorough policy space search while maintaining adequate control performance and can learn a complex trajectory control task within a small finite number of iteration steps.",robotics
10.1016/j.procir.2021.01.010,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,Analysis of Barriers to Industry 4.0 adoption in Manufacturing Organizations: An ISM Approach,https://api.elsevier.com/content/abstract/scopus_id/85102622489,"Industry 4.0 has enabled technological integration of cyber physical systems and internet based communication in manufacturing value creation processes. As of now, many people use it as a collective term for advanced technologies, i.e. advanced robotics, artificial intelligence, machine learning, big data analytics, cloud computing, smart sensors, internet of things, augmented reality, etc. This substantially improves flexibility, quality, productivity, cost, and customer satisfaction by transforming existing centralized manufacturing systems towards digital and decentralized one. Despite having potential benefits of industry 4.0, the organizations are facing typical obstacles and challenges in adopting new technologies and successful implementation in their business models. This paper aims to identify potential barriers which may hinder the implementation of industry 4.0 in manufacturing organizations. The identified barriers, through comprehensive literature review and on the basis of opinions collected from industry experts, are: poor value-chain integration, cyber-security challenges, uncertainty about economic benefits, lack of adequate skills in workforce, high investment requirements, lack of infrastructure, jobs disruptions, challenges in data management and data quality, lack of secure standards and norms, and resistance to change. Interpretive Structural Modeling (ISM) is used to establish relationships among these barriers to develop a hierarchical model and MICMAC analysis for further classification of identified barriers for better understanding. An analysis of driving and dependence of the barriers may help in clear understanding of these for successful implementation of Industry 4.0 practices in the organizations.",robotics
10.1016/j.aei.2021.101246,Journal,Advanced Engineering Informatics,scopus,2021-01-01,sciencedirect,"A systematic literature review on intelligent automation: Aligning concepts from theory, practice, and future perspectives",https://api.elsevier.com/content/abstract/scopus_id/85099458674,"With the recent developments in robotic process automation (RPA) and artificial intelligence (AI), academics and industrial practitioners are now pursuing robust and adaptive decision making (DM) in real-life engineering applications and automated business workflows and processes to accommodate context awareness, adaptation to environment and customisation. The emerging research via RPA, AI and soft computing offers sophisticated decision analysis methods, data-driven DM and scenario analysis with regard to the consideration of decision choices and provides benefits in numerous engineering applications. The emerging intelligent automation (IA) – the combination of RPA, AI and soft computing – can further transcend traditional DM to achieve unprecedented levels of operational efficiency, decision quality and system reliability. RPA allows an intelligent agent to eliminate operational errors and mimic manual routine decisions, including rule-based, well-structured and repetitive decisions involving enormous data, in a digital system, while AI has the cognitive capabilities to emulate the actions of human behaviour and process unstructured data via machine learning, natural language processing and image processing. Insights from IA drive new opportunities in providing automated DM processes, fault diagnosis, knowledge elicitation and solutions under complex decision environments with the presence of context-aware data, uncertainty and customer preferences. This sophisticated review attempts to deliver the relevant research directions and applications from the selected literature to the readers and address the key contributions of the selected literature, IA’s benefits, implementation considerations, challenges and potential IA applications to foster the relevant research development in the domain.",robotics
10.1016/j.compag.2020.105908,Journal,Computers and Electronics in Agriculture,scopus,2021-01-01,sciencedirect,Advance research in agricultural text-to-speech: the word segmentation of analytic language and the deep learning-based end-to-end system,https://api.elsevier.com/content/abstract/scopus_id/85097635082,"Agricultural Text-to-Speech (TTS) has attracted increasingly more attention. The application of agricultural TTS and its problems are analyzed in this paper, and the traditional framework of the TTS system and its key technologies, i.e., text analysis, rhythm generation and speech synthesis are discussed. Furthermore, two advancements in agricultural TTS, the word segmentation of analytic language and the deep learning-based end-to-end TTS system, are detailed summarized. Based on the characteristics of agriculture, some appealing research directions are pointed out: how to improve the training speed and synthesis speed of the deep learning models is still the focus; the study on the approaches of weakly-supervised learning in TTS is in fancy; and research on the real-time and high-quality speech synthesis that can be deployed in mobile devices is a key point of agricultural TTS research.",robotics
10.1016/j.engappai.2020.104052,Journal,Engineering Applications of Artificial Intelligence,scopus,2021-01-01,sciencedirect,Deep softmax collaborative representation for robust degraded face recognition,https://api.elsevier.com/content/abstract/scopus_id/85096643925,"Deep convolutional neural networks (DCNN) have attracted much attention in the field of face recognition because they have achieved high performance than other approaches in the so-called in-the-wild datasets. However, in many real-world applications of face recognition, the performance of CNN-based algorithms is significantly decreased when images contain various kinds of degradations caused by random noise, motion blur, compression artifacts, uncontrolled illumination, and occlusion. Moreover, this is because the main weakness of existing DCNN models is the overfitting problem. To boost the recognition performance of state-of-the-art deep learning networks, we propose a deep softmax collaborative representation-based network, which can be used as a divide-and-conquer algorithm to help multiple DCCNs work together more effectively to solve multiple sub-problems of face reconstruction and classification. We demonstrated several experiments with challenging face recognition datasets. Our extensive experiments demonstrate that our proposed method is more robust and efficient in dealing with the challenging real-world problems in face recognition compared to related state-of-the-art methods.",robotics
10.1016/j.asoc.2020.106811,Journal,Applied Soft Computing,scopus,2021-01-01,sciencedirect,Hybrid prediction strategy to predict agricultural information,https://api.elsevier.com/content/abstract/scopus_id/85095944181,"The crop yield prediction (CYP) has a high significance in agriculture. Early crop yield predictions assist the farmers, decision-makers in making timely decisions during the actual growing season. In many developing countries such as India, the process of crop yield prediction is done manually, based on surveys and field visits which are time-consuming, expensive and prone to human error. To overcome these drawbacks, we propose a hybrid prediction strategy which can be applied to predict agricultural information such as crop yield and air temperature with a critical focus on crop yield prediction. The weighted principal component analysis (w-PCA) is used as the feature extraction strategy to extract the relevant features. A hybrid prediction strategy integrating artificial neural network (ANN) with modified-particle swarm optimization (m-PSO) is proposed. Initial parameters of ANN are selected using m-PSO with modified inertia weight and velocity update equations. This hybridized ANN then performs prediction on the selected features. This proposed prediction model which we call as hybrid-ANN (H-ANN) comprises of w-PCA as feature extractor, m-PSO for selecting initial weights and biases of ANN. Experiments were performed on eight real world and two benchmark agriculture data sets for crop yield and air temperature prediction. Results show that the proposed prediction model (H-ANN) performed with improvements in the range of 2 to 30%, 0.2 to 4% and 0.12 to 3% with respect to R-squared (
                        
                           
                              R
                           
                           
                              2
                           
                        
                     ), root mean square error (
                        
                           R
                           M
                           S
                           E
                        
                     ) and mean absolute error (
                        
                           M
                           A
                           E
                        
                     ) respectively when compared to other prediction models such as ANN, ANN trained using GA (GA-ANN), ANN trained using standard PSO (SPSO-ANN), multiple linear regression (MLR), support vector regression (SVR) and ensemble of bagged regression trees (ET) on benchmark and real-world agricultural datasets.",robotics
10.1016/j.cogsys.2020.09.006,Journal,Cognitive Systems Research,scopus,2021-01-01,sciencedirect,Learning data-driven decision-making policies in multi-agent environments for autonomous systems,https://api.elsevier.com/content/abstract/scopus_id/85092486365,"Autonomous systems such as Connected Autonomous Vehicles (CAVs), assistive robots are set improve the way we live. Autonomous systems need to be equipped with capabilities to Reinforcement Learning (RL) is a type of machine learning where an agent learns by interacting with its environment through trial and error, which has gained significant interest from research community for its promise to efficiently learn decision making through abstraction of experiences. However, most of the control algorithms used today in current autonomous systems such as driverless vehicle prototypes or mobile robots are controlled through supervised learning methods or manually designed rule-based policies. Additionally, many emerging autonomous systems such as driverless cars, are set in a multi-agent environment, often with partial observability. Learning decision making policies in multi-agent environments is a challenging problem, because the environment is not stationary from the perspective of a learning agent, and hence the Markov properties assumed in single agent RL does not hold. This paper focuses on learning decision-making policies in multi-agent environments, both in cooperative settings with full observability and dynamic environments with partial observability. We present experiments in simple, yet effective, new multi-agent environments to simulate policy learning in scenarios that could be encountered by an autonomous navigating agent such as a CAV. The results illustrate how agents learn to cooperate in order to achieve their objectives successfully. Also, it was shown that in a partially observable setting, an agent was capable of learning to roam around its environment without colliding in the presence of obstacles and other moving agents. Finally, the paper discusses how data-driven multi-agent policy learning can be extended to real-world environments by augmenting the intelligence of autonomous vehicles.",robotics
10.1016/j.jmsy.2020.06.012,Journal,Journal of Manufacturing Systems,scopus,2021-01-01,sciencedirect,"A digital twin to train deep reinforcement learning agent for smart manufacturing plants: Environment, interfaces and intelligence",https://api.elsevier.com/content/abstract/scopus_id/85087690907,"Filling the gaps between virtual and physical systems will open new doors in Smart Manufacturing. This work proposes a data-driven approach to utilize digital transformation methods to automate smart manufacturing systems. This is fundamentally enabled by using a digital twin to represent manufacturing cells, simulate system behaviors, predict process faults, and adaptively control manipulated variables. First, the manufacturing cell is accommodated to environments such as computer-aided applications, industrial Product Lifecycle Management solutions, and control platforms for automation systems. Second, a network of interfaces between the environments is designed and implemented to enable communication between the digital world and physical manufacturing plant, so that near-synchronous controls can be achieved. Third, capabilities of some members in the family of Deep Reinforcement Learning (DRL) are discussed with manufacturing features within the context of Smart Manufacturing. Trained results for Deep Q Learning algorithms are finally presented in this work as a case study to incorporate DRL-based artificial intelligence to the industrial control process. As a result, developed control methodology, named Digital Engine, is expected to acquire process knowledges, schedule manufacturing tasks, identify optimal actions, and demonstrate control robustness. The authors show that integrating a smart agent into the industrial platforms further expands the usage of the system-level digital twin, where intelligent control algorithms are trained and verified upfront before deployed to the physical world for implementation. Moreover, DRL approach to automated manufacturing control problems under facile optimization environments will be a novel combination between data science and manufacturing industries.",robotics
10.1016/j.ymssp.2020.107061,Journal,Mechanical Systems and Signal Processing,scopus,2021-01-01,sciencedirect,Recovering compressed images for automatic crack segmentation using generative models,https://api.elsevier.com/content/abstract/scopus_id/85086994715,"In a structural health monitoring (SHM) system that uses digital cameras to monitor cracks of structural surfaces, techniques for reliable and effective data compression are essential to ensure a stable and energy-efficient crack images transmission in wireless devices, e.g., drones and robots with high definition cameras installed. Compressive sensing (CS) is a signal processing technique that allows accurate recovery of a signal from a sampling rate much smaller than the limitation of the Nyquist sampling theorem. Different from the popular approach of simultaneously training encoder and decoder using neural network models, the CS theory ensures a high probability of accurate signal reconstruction based on random measurements that is shorter than the length of the original signal under a sparsity constraint. Such method is particularly useful when measurements are expensive, such as wireless sensing of civil structures, because its hardware implementation allows down sampling of signals during the sensing process. Hence, CS methods can achieve significant energy saving for the sensing devices. However, the strong assumption of the signals being highly sparse in an invertible space is relatively hard to guarantee for many real images, such as image of cracks. In this paper, we present a new approach of CS that replaces the sparsity regularization with a generative model that is able to effectively capture a low dimension representation of targeted images. We develop a recovery framework for automatic crack segmentation of compressed crack images based on this new CS method. We demonstrate the remarkable performance of our method that takes advantage of the strong capability of generative models to capture the necessary features required in the crack segmentation task even the backgrounds of the generated images are not well reconstructed. The superior performance of our recovery framework is illustrated by comparisons to three existing CS algorithms. Furthermore, we show that our framework is potentially extensible to other common problems in automatic crack segmentation, such as defect recovery from motion blurring and occlusion.",robotics
10.1016/j.neucom.2020.08.026,Journal,Neurocomputing,scopus,2020-12-22,sciencedirect,Improved recurrent neural networks for solving Moore-Penrose inverse of real-time full-rank matrix,https://api.elsevier.com/content/abstract/scopus_id/85090835863,"Recently, motivated by Zhang neural network (ZNN) models, Lv et al. presented two novel neural network (NNN) models for solving Moore-Penrose inverse of a time-invariant full-rank matrix. The NNN models were established by introducing two new matrix factors in the ZNN models, which results in their higher convergence rates than those of the ZNN models. In this paper we extend the NNN models to the more general cases through introducing a “regularization” parameter and a power parameter in these two matrix factors. The new proposed models are named here as the improved recurrent neural networks (IMRNN) since their convergence performance can be much better than the NNN models by appropriate choices of the introduced parameters. Such convergence property is theoretically analyzed in detail. Some numerical experiments are also performed to validate the theoretical results, including the numerical comparisons with the existing gradient neural network (GNN), ZNN and NNN models. In particular, the proposed IMRNN models are successfully applied to the inverse kinematic control of a three-link redundant robot manipulator where the superiority of the IMRNN models to the GNN, ZNN and NNN models is also indicated.",robotics
10.1016/j.enggeo.2020.105817,Journal,Engineering Geology,scopus,2020-12-05,sciencedirect,"Successful implementations of a real-time and intelligent early warning system for loess landslides on the Heifangtai terrace, China",https://api.elsevier.com/content/abstract/scopus_id/85090411791,"Real-time monitoring and intelligent early warning system are crucial and significant to take mitigation measures and reduce casualties and property losses related to landslides. It is difficult to obtain entire monitoring data in the accelerated deformation phase in a landslide event, and hard to issue early warning information using a traditional monitoring approach with fixed and low sampling frequency. Displacement increments of loess landslides induced by agriculture irrigation on the Heifangtai terrace could be sudden and extremely rapid. Typical landslide types include loess flowslides and loess falls. It is of practical significance to develop a self-adaptive data acquisition monitoring technique and establish a real-time landslide early warning system (LEWS) to meet the needs for risk mitigation of rapid sliding slopes on the Heifangtai terrace. The monitoring technique can wirelessly transmit displacement data and the LEWS was devised using the new artificial intelligence. The LEWS could automatically release the warning information in advance of the event once the early warning parameters exceed default thresholds. In this study, the early warning procedures, real-time monitoring approach, intelligent LEWS, a multiple criteria warning model, warning release and emergency mitigation measures, and performance are introduced in detail. Six loess landslides at Heifangtai and eight landslides in other regions of China have been successfully warned since its implementation in 2012. This study proposed an effective and practical solution for the early warning of loess landslides at Heifangtai. Two typical loess landslides that had successful early warnings at Heifangtai were presented. The successful implementation could serve as a reference for global rapid slope failure cases, considering the complex nature of landslide behaviors and failure mechanisms.",robotics
10.1016/j.asoc.2020.106783,Journal,Applied Soft Computing Journal,scopus,2020-12-01,sciencedirect,Re-training and parameter sharing with the Hash trick for compressing convolutional neural networks,https://api.elsevier.com/content/abstract/scopus_id/85093986069,"As an ubiquitous technology for improving machine intelligence, deep learning has largely taken the dominant position among nowadays most advanced computer vision systems. To achieve superior performance on large-scale datasets, convolutional neural networks (CNNs) are often designed as complex models with millions of parameters. This limits the deployment of CNNs in embedded intelligent computer vision systems, such as intelligent robots that are resource-constrained with real-time computing requirement. This paper proposes a simple and effective model compression scheme to improve the real-time sensing of the surrounding objects. In the proposed framework, the Hash trick is first applied to a modified convolutional layer, and the compression of the convolutional layer is realized via weight sharing. Subsequently, the Hash index matrix is introduced to represent the Hash function, and its relaxation regularization is introduced into the fine-tuned loss function. Through the dynamic retraining of the index matrix, the Hash function can be updated. We evaluate our method using several state-of-the-art CNNs. Experimental results showed that the proposed method can reduce the number of parameters in AlexNet by 24
                        ×
                      with no accuracy loss. In addition, the compressed VGG16 and ResNet50 can achieve a more than 60
                        ×
                      increased speed, which is significant.",robotics
10.1016/j.robot.2020.103652,Journal,Robotics and Autonomous Systems,scopus,2020-12-01,sciencedirect,Self-awareness in intelligent vehicles: Feature based dynamic Bayesian models for abnormality detection,https://api.elsevier.com/content/abstract/scopus_id/85092022930,"The evolution of Intelligent Transportation Systems in recent times necessitates the development of self-awareness in agents. Before the intensive use of Machine Learning, the detection of abnormalities was manually programmed by checking every variable and creating huge nested conditions that are very difficult to track. This paper aims to introduce a novel method to develop self-awareness in autonomous vehicles that mainly focuses on detecting abnormal situations around the considered agents. Multi-sensory time-series data from the vehicles are used to develop the data-driven Dynamic Bayesian Network (DBN) models used for future state prediction and the detection of dynamic abnormalities. Moreover, an initial level collective awareness model that can perform joint anomaly detection in co-operative tasks is proposed.
                  The GNG algorithm learns the DBN models’ discrete node variables; probabilistic transition links connect the node variables. A Markov Jump Particle Filter (MJPF) is applied to predict future states and detect when the vehicle is potentially misbehaving using learned DBNs as filter parameters.
                  In this paper, datasets from real experiments of autonomous vehicles performing various tasks used to learn and test a set of switching DBN models.",robotics
10.1016/j.asoc.2020.106729,Journal,Applied Soft Computing,scopus,2020-12-01,sciencedirect,Two-stage grasp strategy combining CNN-based classification and adaptive detection on a flexible hand,https://api.elsevier.com/content/abstract/scopus_id/85091509038,"Robotic autonomous grasping of food-related objects requires a nondestructive and safe grasp system for picking up various objects. A novel underactuated flexible hand consisting of a variable palm and four soft fingers is designed and manufactured to enhance the grasp space and deformability during interaction with unknown objects. A position deviation formulation is fitted to estimate the free bend deformation of soft fingers approximately through finite element analysis. A modular convolutional neural network is presented to identify the grasp directions, shape features and anticipated input pressure levels of novel objects for achieving multitarget classification. A vision-based adaptive detection method is proposed to obtain an accurate wrist orientation and the best grasp candidate by using two means of grasp planning (i.e. cross grasp planning and equidistant optimal grasp planning). A two-stage grasp strategy combining the classification and detection methods is developed as an effective solution to estimate the grasp configuration accurately. Results show that our flexible hand achieves 91.1% success rate in a physical grasp experiment on a UR robot, thereby demonstrating the reliability and adaptability of our grasp approach. The target object can be identified and detected within 0.263 s, which indicates the suitability of our approach in real-time applications.",robotics
10.1016/j.cogsys.2020.08.010,Journal,Cognitive Systems Research,scopus,2020-12-01,sciencedirect,Toward ethical cognitive architectures for the development of artificial moral agents,https://api.elsevier.com/content/abstract/scopus_id/85090423654,"New technologies based on artificial agents promise to change the next generation of autonomous systems and therefore our interaction with them. Systems based on artificial agents such as self-driving cars and social robots are examples of this technology that is seeking to improve the quality of people’s life. Cognitive architectures aim to create some of the most challenging artificial agents commonly known as bio-inspired cognitive agents. This type of artificial agent seeks to embody human-like intelligence in order to operate and solve problems in the real world as humans do. Moreover, some cognitive architectures such as Soar, LIDA, ACT-R, and iCub try to be fundamental architectures for the Artificial General Intelligence model of human cognition. Therefore, researchers in the machine ethics field face ethical questions related to what mechanisms an artificial agent must have for making moral decisions in order to ensure that their actions are always ethically right. This paper aims to identify some challenges that researchers need to solve in order to create ethical cognitive architectures. These cognitive architectures are characterized by the capacity to endow artificial agents with appropriate mechanisms to exhibit explicit ethical behavior. Additionally, we offer some reasons to develop ethical cognitive architectures. We hope that this study can be useful to guide future research on ethical cognitive architectures.",robotics
10.1016/j.jns.2020.117081,Journal,Journal of the Neurological Sciences,scopus,2020-11-15,sciencedirect,New technologies and Amyotrophic Lateral Sclerosis – Which step forward rushed by the COVID-19 pandemic?,https://api.elsevier.com/content/abstract/scopus_id/85090005531,"Amyotrophic Lateral Sclerosis (ALS) is a fast-progressive neurodegenerative disease leading to progressive physical immobility with usually normal or mild cognitive and/or behavioural involvement. Many patients are relatively young, instructed, sensitive to new technologies, and professionally active when developing the first symptoms. Older patients usually require more time, encouragement, reinforcement and a closer support but, nevertheless, selecting user-friendly devices, provided earlier in the course of the disease, and engaging motivated carers may overcome many technological barriers. ALS may be considered a model for neurodegenerative diseases to further develop and test new technologies. From multidisciplinary teleconsults to telemonitoring of the respiratory function, telemedicine has the potentiality to embrace other fields, including nutrition, physical mobility, and the interaction with the environment. Brain-computer interfaces and eye tracking expanded the field of augmentative and alternative communication in ALS but their potentialities go beyond communication, to cognition and robotics. Virtual reality and different forms of artificial intelligence present further interesting possibilities that deserve to be investigated. COVID-19 pandemic is an unprecedented opportunity to speed up the development and implementation of new technologies in clinical practice, improving the daily living of both ALS patients and carers.
                  The present work reviews the current technologies for ALS patients already in place or being under evaluation with published publications, prompted by the COVID-19 pandemic.",robotics
10.1016/j.asoc.2020.106682,Journal,Applied Soft Computing Journal,scopus,2020-11-01,sciencedirect,AGLNet: Towards real-time semantic segmentation of self-driving images via attention-guided lightweight network,https://api.elsevier.com/content/abstract/scopus_id/85090219040,"The extensive computational burden limits the usage of convolutional neural networks (CNNs) in edge devices for image semantic segmentation, which plays a significant role in many real-world applications, such as augmented reality, robotics, and self-driving. To address this problem, this paper presents an attention-guided lightweight network, namely AGLNet, which employs an encoder–decoder architecture for real-time semantic segmentation. Specifically, the encoder adopts a novel residual module to abstract feature representations, where two new operations, channel split and shuffle, are utilized to greatly reduce computation cost while maintaining higher segmentation accuracy. On the other hand, instead of using complicated dilated convolution and artificially designed architecture, two types of attention mechanism are subsequently employed in the decoder to upsample features to match input resolution. Specifically, a factorized attention pyramid module (FAPM) is used to explore hierarchical spatial attention from high-level output, still remaining fewer model parameters. To delineate object shapes and boundaries, a global attention upsample module (GAUM) is adopted as global guidance for high-level features. The comprehensive experiments demonstrate that our approach achieves state-of-the-art results in terms of speed and accuracy on three self-driving datasets: CityScapes, CamVid, and Mapillary Vistas. AGLNet achieves 71.3%, 69.4%, and 30.7% mean IoU on these datasets with only 1.12M model parameters. Our method also achieves 52 FPS, 90 FPS, and 53 FPS inference speed, respectively, using a single GTX 1080Ti GPU. Our code is open-source and available at https://github.com/xiaoyufenfei/Efficient-Segmentation-Networks.",robotics
10.1016/j.neucom.2020.06.004,Journal,Neurocomputing,scopus,2020-10-07,sciencedirect,Building and optimization of 3D semantic map based on Lidar and camera fusion,https://api.elsevier.com/content/abstract/scopus_id/85087333210,"When considering the robot application of the complex scenarios, the traditional geometric maps are insufficient because of the lack of interactions with the environment. In this paper, a three-dimensional (3D) semantic map with large-scale and accurate integrating Lidar and camera information is presented to achieve real-time road scenes. Firstly, simultaneous localization and mapping (SLAM) is performed to locate the robot position with the multi-sensor fusion of the Lidar and inertial measurement unit (IMU), and the map of the surrounding scenes is constructed while the robot is moving. Moreover, a convolutional neural networks (CNNs)-based semantic segmentation of images is employed to develop the semantic map of the environment. Following the synchronization of the time and space, the sensor fusion of Lidar and camera are used to generate the semantic labeled frame of point clouds and then create a semantic map in term of the posture. Besides, improving the capacity of classification, a higher-order 3D full connection conditional random fields (CRFs) method is utilized to optimize the semantic map. Finally, extensive experiment results evaluated on the KITTI dataset have illustrated the effectiveness of the proposed method.",robotics
10.1016/j.image.2020.115969,Journal,Signal Processing: Image Communication,scopus,2020-10-01,sciencedirect,Re-identification framework for long term visual object tracking based on object detection and classification,https://api.elsevier.com/content/abstract/scopus_id/85089267326,"In this paper, we address the problem of long-term visual object tracking and we present an efficient real-time single object tracking system suitable for integration in autonomous platforms that need to encompass intelligent capabilities. We propose a novel long-term tracking framework for classification based re-detection and tracking, that incorporates state estimation, object re-identification and automated management of tracking and detection results. Our method integrates a novel object re-identification technique which efficiently filters a number of detection candidates and systematically corrects the tracking results. Through extensive experimental validation on the UAV123, UAV20L and TLP datasets, we demonstrate the effectiveness of the proposed system and its advantage over several state-of-the art trackers. The results furthermore highlight the proposed tracker’s ability to handle challenges arising from real-world and long-term scenarios, such as variations in pose, scale, occlusions and out-of-view situations. Furthermore, we propose a variant that is suitable for deployment on autonomous robots, such as Unmanned Aerial Vehicles.",robotics
10.1016/j.asoc.2020.106539,Journal,Applied Soft Computing Journal,scopus,2020-10-01,sciencedirect,Cognitive visual anomaly detection with constrained latent representations for industrial inspection robot,https://api.elsevier.com/content/abstract/scopus_id/85088661065,"With the fast growth of intelligent manufacturing industry, developing advanced industrial inspection robots is becoming a research and application hotspot in the fields of both computer vision and robotics. This kind of industrial inspection robots is expected to automatically detect anomalous structures (e.g., defects, damages, rejects, etc.) from the images of the manufactured products. Generally, the existing visual anomaly detection (VAD) methods mainly focus on modeling the complex and high-dimensional distribution of normal data, while neglecting the specific visual properties of abnormal data since their frequency of occurrence is much less than that of the normal data. In this paper, inspired by the human cognition on extracting abstractly visual properties and to distinguish the anomaly patterns from the observed data, we propose a novel cognitive VAD method for industrial inspection robot. Specifically, we introduce a constrained latent space to mimic the cognitive ability of humans, where the abstraction learned from the observed normal and anomaly data are represented. We build our method based on a convolutional generative adversarial network and a denoising auto-encoder, where the adversarial learning mechanism is adopted to establish the boundary between the normal and anomaly data. In the experiment, we evaluate our method on a real-world dataset where the images are captured for the manufactured products. The comprehensive results comparing with several recent VAD methods show that the proposed method is effective to detect the anomaly images of different categories with a high accuracy.",robotics
10.1016/j.asoc.2020.106559,Journal,Applied Soft Computing Journal,scopus,2020-10-01,sciencedirect,Use of stochastic nature-inspired population-based algorithms within an online adaptive controller for mechatronic devices,https://api.elsevier.com/content/abstract/scopus_id/85088633066,"Stochastic nature-inspired population-based algorithms are very powerful tools for solving stationary and deterministic, NP-hard optimization problems. These algorithms have rarely been applied to real-world dynamic and uncertain optimization due to their complexity. In this paper, this kind of algorithms were ported onto real hardware (i.e., the velocity controller of a one degree of freedom robot mechanism), where they were used to control the behavior of a non-linear system online. This means that the feedback response from the system must be less than 5 ms. Due to the complexity of the fitness function evaluation, a surrogate linear model was used, implemented as a single-layer artificial neural network, consisting of two phases: learning and simulation. In the first phase, the model of the nonlinear plant is learned during online operation, while in the second, the value of the fitness function needed by the optimization algorithms is predicted. Six algorithms were compared with the PI-controller in our experimental work. This were: classical evolution strategies, contemporary evolution strategies, differential evolution, self-adaptive differential evolution, particle swarm optimization, and the bat algorithm. The results showed that the algorithms outperformed PI-controller in the sense of stability, flexibility and adaptability.",robotics
10.1016/j.compeleceng.2020.106766,Journal,Computers and Electrical Engineering,scopus,2020-10-01,sciencedirect,Imputation of Missing Values Affecting the Software Performance of Component-based Robots,https://api.elsevier.com/content/abstract/scopus_id/85088022516,"Intelligent robots are foreseen as a technology that would be soon present in most public and private environments. In order to increase the trust of humans, robotic systems must be reliable while both response and down times are minimized. In keeping with this idea, present paper proposes the application of machine learning (regression models more precisely) to preprocess data in order to improve the detection of failures. Such failures deeply affect the performance of the software components embedded in human-interacting robots. To address one of the most common problems of real-life datasets (missing values), some traditional (such as linear regression) as well as innovative (decision tree and neural network) models are applied. The aim is to impute missing values with minimum error in order to improve the quality of data and consequently maximize the failure-detection rate. Experiments are run on a public and up-to-date dataset and the obtained results support the viability of the proposed models.",robotics
10.1016/j.cmpb.2020.105643,Journal,Computer Methods and Programs in Biomedicine,scopus,2020-10-01,sciencedirect,High accurate lightweight deep learning method for gesture recognition based on surface electromyography,https://api.elsevier.com/content/abstract/scopus_id/85087496227,"Background and objectives
                  Surface Electromyography (sEMG) is used mostly for neuromuscular diagnosis, assistive technology, physical rehabilitation, and human-computer interactions. Achieving a precise and lightweight method along with low latency for gesture recognition is still a real-life challenge, especially for rehabilitation and assistive robots. This work aims to introduce a highly accurate and lightweight deep learning method for gesture recognition.
               
                  Methods
                  High-density sEMG, unlike sparse sEMG, does not require accurate electrode placement and provides more physiological information. Then we apply high-density sEMG, which, according to previous studies, leads to sEMG images. In this study, we introduce the Sensor-Wise method, which has a higher capability to extract features compared to the sEMG image method due to its high compatibility with the nature of sEMG signals and the structure of convolutional networks.
               
                  Results
                  The proposed method, because of its optimal structure with only two hidden layers and its high compatibility, has shown no sign of overfitting and was able to reach an accuracy of almost 100% (99.99%) when it was evaluated by CapgMyo DB-a database through 96 electrodes. Using this method, even with 16 electrodes, we were able to reach an accuracy of 99.8%, which was higher than the accuracies reported in the previous studies. Additionally, the method was evaluated by the CSL-HDEMG database, where the accuracy reached 99.55%. Previous studies either introduced expensive computational methods with overfitting or reported lower accuracies compared to this study.
               
                  Conclusions
                  The Sensor- Wise method has high compatibility with the nature of sEMG signals and the structure of convolutional networks. The high accuracy and lightweight structure of this method with only two hidden layers make it a proper option for hardware implementation.",robotics
10.1016/j.fusengdes.2020.111736,Journal,Fusion Engineering and Design,scopus,2020-10-01,sciencedirect,Combining object detection with generative adversarial networks for in-component anomaly detection,https://api.elsevier.com/content/abstract/scopus_id/85086034398,"Present inspection techniques in place at the Joint European Torus (JET), as well as some of those planned for ITER make use of robotically deployed inspection systems, which typically collect data for offline analysis. This can be a slow, laborious process with subjective or error-prone results. There are significant benefits to be gained through automation or user assistance, for example through prioritisation of samples for analysis.
                  Automated visual anomaly detection is a highly challenging problem due to high dimensionality of the input data, meaning that the normal statistical distribution cannot be directly modelled. We provide a robotic and algorithmic framework that utilizes Generative Adversarial Ngenerative adversarial networks (GANs) to indirectly model this distribution, and hence provide a mechanism to quantify the anomalousness of given image data samples from a tokamak environment.
                  This paper presents an approach to visual anomaly detection that combines multiple deep neural network architectures in order to extract individual components and then classify anomalies. An overview of the architecture and algorithms employed as well as quantitative and qualitative assessments of the performance against data from both a benchmark dataset, and real data gathered from JET components is provided.",robotics
10.1016/j.dt.2019.12.006,Journal,Defence Technology,scopus,2020-10-01,sciencedirect,A novel facial emotion recognition scheme based on graph mining,https://api.elsevier.com/content/abstract/scopus_id/85077698126,"Recent years have seen an explosion in graph data from a variety of scientific, social and technological fields. From these fields, emotion recognition is an interesting research area because it finds many applications in real life such as in effective social robotics to increase the interactivity of the robot with human, driver safety during driving, pain monitoring during surgery etc. A novel facial emotion recognition based on graph mining has been proposed in this paper to make a paradigm shift in the way of representing the face region, where the face region is represented as a graph of nodes and edges and the gSpan frequent sub-graphs mining algorithm is used to find the frequent sub-structures in the graph database of each emotion. To reduce the number of generated sub-graphs, overlap ratio metric is utilized for this purpose. After encoding the final selected sub-graphs, binary classification is then applied to classify the emotion of the queried input facial image using six levels of classification. Binary cat swarm intelligence is applied within each level of classification to select proper sub-graphs that give the highest accuracy in that level. Different experiments have been conducted using Surrey Audio-Visual Expressed Emotion (SAVEE) database and the final system accuracy was 90.00%. The results show significant accuracy improvements (about 2%) by the proposed system in comparison to current published works in SAVEE database.",robotics
10.1016/j.jocn.2020.04.125,Journal,Journal of Clinical Neuroscience,scopus,2020-09-01,sciencedirect,Tele-robotics and artificial-intelligence in stroke care,https://api.elsevier.com/content/abstract/scopus_id/85088965193,"In the last forty years, the field of medicine has experienced dramatic shifts in technology-enhanced surgical procedures – from its initial use in 1985 for neurosurgical biopsies to current implementation of systems such as magnetic-guided catheters for endovascular procedures. Systems such as the Niobe Magnetic Navigation system and CorPath GRX have allowed for utilization of a fully integrated surgical robotic systems for perioperative manipulation, as well as tele-controlled manipulation systems for telemedicine. These robotic systems hold tremendous potential for future implementation in cerebrovascular procedures, but lack of relevant clinical experience and uncharted ethical and legal territory for real-life tele-robotics have stalled their adoption for neurovascular surgery, and might present significant challenges for future development and widespread implementation. Yet, the promise that these technologies hold for dramatically improving the quality and accessibility of cerebrovascular procedures such as thrombectomy for acute stroke, drives the research and development of surgical robotics. These technologies, coupled with artificial intelligence (AI) capabilities such as machine learning, deep-learning, and outcome-based analyses and modifications, have the capability to uncover new dimensions within the realm of cerebrovascular surgery.",robotics
10.1016/j.engappai.2020.103799,Journal,Engineering Applications of Artificial Intelligence,scopus,2020-09-01,sciencedirect,Trajectory based lateral control: A Reinforcement Learning case study,https://api.elsevier.com/content/abstract/scopus_id/85087950678,"Reinforcement Learning (RL) has been employed in many applications of robotics and has steadily been gaining traction in the field of Autonomous Driving (AD). This paper proposes a Deep Reinforcement Learning based approach for lateral Vehicle Motion Control (VMC), and explores the generalization capabilities of the approach. The proposed methodology uses a sequence of waypoints generated from a planning module of an AD stack as the input. The network has been trained to predict accurate steering commands to follow the given trajectory. In this paper we detail our implementation and share our learning experience on real-vehicle deployment of the RL based controller. Our experiments yield promising results with an agent trained on less than 4 h of simulated driving experience without any real-world data. The trained agent is able to successfully complete unseen and more complex tracks using different unseen vehicle models. The agent safely reached up to 150km/h in simulation and up to 60km/h in a real-life Sport Utility Vehicle (SUV) weighing more than 2000kg.",robotics
10.1016/j.robot.2020.103578,Journal,Robotics and Autonomous Systems,scopus,2020-09-01,sciencedirect,Real-time topological localization using structured-view ConvNet with expectation rules and training renewal,https://api.elsevier.com/content/abstract/scopus_id/85086575996,"Mobile service robots possess high potential of providing numerous assistances in the working areas. In an attempt to develop a mobile service robot which is dynamically balanced for faster movement and taller manipulation capability, we designed and prototyped J4.alpha, which is intended for swift navigation and nimble manipulation. Previously, we devised a pure visual method based on a supervised deep learning model for real-time recognition of nodal locations. Four low-resolution RGB cameras are installed around J4.alpha to capture the surrounding visual features for training and detection. As the method is developed for ease of implementation, fast real-time application, accurate detection, and low cost, we further improve the accuracy and the practicality of the method in this study. Specifically, a set of expectation rules are introduced to reject outlier detections, and a scheme of training renewal is devised to effectively react to environmental modifications. In our previous tests, precision and recall rates of the location coordinate detection by the ConvNet models were generally between 0.78 and 0.91; by introducing the expectation rules, precision and recall are improved by approximately 10%. A large scale field test is also carried out here for both corridor and factory scenarios; the performance of the proposed method was tested for detection accuracy and verified for 2 m and 0.5 m nodal intervals. The scheme of training renewal designed for capturing and reflecting environmental modifications was also proved to be effective.",robotics
10.1016/j.eswa.2020.113402,Journal,Expert Systems with Applications,scopus,2020-09-01,sciencedirect,Optimisation of phonetic aware speech recognition through multi-objective evolutionary algorithms,https://api.elsevier.com/content/abstract/scopus_id/85083000399,"Recent advances in the availability of computational resources allow for more sophisticated approaches to speech recognition than ever before. This study considers Artificial Neural Network and Hidden Markov Model methods of classification for Human Speech Recognition through Diphthong Vowel sounds in the English Phonetic Alphabet rather than the classical approach of the classification of whole words and phrases, with a specific focus on both single and multi-objective evolutionary optimisation of bioinspired classification methods. A set of audio clips are recorded by subjects from the United Kingdom and Mexico and the recordings are transformed into a static dataset of statistics by way of their Mel-Frequency Cepstral Coefficients (MFCC) at sliding window length of 200ms as well as a reshaped MFCC timeseries format for forecast-based models. An deep neural network with evolutionary optimised topology achieves 90.77% phoneme classification accuracy in comparison to the best HMM that achieves 86.23% accuracy with 150 hidden units, when only accuracy is considered in a single-objective optimisation approach. The obtained solutions are far more complex than the HMM taking around 248 seconds to train on powerful hardware versus 160 for the HMM. A multi-objective approach is explored due to this. In the multi-objective approaches of scalarisation presented, within which real-time resource usage is also considered towards solution fitness, far more optimal solutions are produced which train far quicker than the forecast approach (69 seconds) with classification ability retained (86.73%). Weightings towards either maximising accuracy or reducing resource usage from 0.1 to 0.9 are suggested depending on the resources available, since many future IoT devices and autonomous robots may have limited access to cloud resources at a premium in comparison to the GPU used in this experiment.",robotics
10.1016/j.neucom.2020.02.109,Journal,Neurocomputing,scopus,2020-08-04,sciencedirect,Tracking control of redundant mobile manipulator: An RNN based metaheuristic approach,https://api.elsevier.com/content/abstract/scopus_id/85082490397,"In this paper, we propose a topology of Recurrent Neural Network (RNN) based on a metaheuristic optimization algorithm for the tracking control of mobile-manipulator while enforcing nonholonomic constraints. Traditional approaches for tracking control of mobile robots usually require the computation of Jacobian-inverse or linearization of its mathematical model. The proposed algorithm uses a nature-inspired optimization approach to directly solve the nonlinear optimization problem without any further transformation. First, we formulate the tracking control as a constrained optimization problem. The optimization problem is formulated on position-level to avoid the computationally expensive Jacobian-inversion. The nonholonomic limitation is ensured by adding equality constraints to the formulated optimization problem. We then present the Beetle Antennae Olfactory Recurrent Neural Network (BAORNN) algorithm to solve the optimization problem efficiently using very few mathematical operations. We present a theoretical analysis of the proposed algorithm and show that its computational cost is linear with respect to the degree of freedoms (DOFs), i.e., O(m). Additionally, we also prove its stability and convergence. Extensive simulation results are prepared using a simulated model of IIWA14, a 7-DOF industrial-manipulator, mounted on a differentially driven cart. Comparison results with particle swarm optimization (PSO) algorithm are also presented to prove the accuracy and numerical efficiency of the proposed controller. The results demonstrate that the proposed algorithm is several times (around 75 in the worst case) faster in execution as compared to PSO, and suitable for real-time implementation. The tracking results for three different trajectories; circular, rectangular, and rhodonea paths are presented.",robotics
10.1016/j.compbiomed.2020.103867,Journal,Computers in Biology and Medicine,scopus,2020-08-01,sciencedirect,MASSD: Multi-scale attention single shot detector for surgical instruments,https://api.elsevier.com/content/abstract/scopus_id/85087591678,"Surgical instrument detection is a significant task in computer-aided minimal invasive surgery for providing real-time feedback to physicians, evaluating surgical skills, and developing a training plan for surgeons. In this study, a multi-scale attention single detector is designed for surgical instruments. In the field of object detection, accurate detection of small objects is always a challenging task. We propose an innovative feature fusion technique aimed at small surgical instrument detection. First, the attention map is created from high-level features to act on the low-level features and enrich the semantic information of the low-level features. The original and processed features are then fused by skip connection. Finally, multi-scale feature maps are created to predict fusion features. The experiments on the ATLAS Dione dataset yielded results with a detection time of 0.066 s per frame and a mean average precision of 90.08%. Our proposed feature fusion module can obtain more semantic information for low-level features and significantly enhance the performance of small surgical instrument detection.",robotics
10.1016/j.jbi.2020.103483,Journal,Journal of Biomedical Informatics,scopus,2020-08-01,sciencedirect,Agents and robots for collaborating and supporting physicians in healthcare scenarios,https://api.elsevier.com/content/abstract/scopus_id/85087202709,"Monitoring patients through robotics telehealth systems is an interesting scenario where patients’ conditions, and their environment, are dynamic and unknown variables. We propose to improve telehealth systems’ features to include the ability to serve patients with their needs, operating as human caregivers. The objective is to support the independent living of patients at home without losing the opportunity to monitor their health status. Application scenarios are several, and they spread from simple clinical assisting scenarios to an emergency one. For instance, in the case of a nursing home, the system would support in continuously monitoring the elderly patients. In contrast, in the case of an epidemic diffusion, such as COVID-19 pandemic, the system may help in all the early triage phases, significantly reducing the risk of contagion. However, the system has to let medical assistants perform actions remotely such as changing therapies or interacting with patients that need support. The paper proposes and describes a multi-agent architecture for intelligent medical care. We propose to use the beliefs-desires-intentions agent architecture, part of it is devised to be deployed in a robot. The result is an intelligent system that may allow robots the ability to select the most useful plan for unhandled situations and to communicate the choice to the physician for his validation and permission.",robotics
10.1016/j.cag.2020.06.002,Journal,Computers and Graphics (Pergamon),scopus,2020-08-01,sciencedirect,Predicting ready-made garment dressing fit for individuals based on highly reliable examples,https://api.elsevier.com/content/abstract/scopus_id/85086821320,"Predicting the dressing fit of a ready-made garment for different individuals is a challenging problem in online garment sales. At present, the main solution involves physics-based virtual garment simulation, which has limitations in terms of a lack of sufficient reality and high computational costs. In this study, we developed a novel example-based method to guarantee the high reality and efficiency of garment dressing fit prediction using two methods. First, highly reliable examples were captured with the assistance of a robotic mannequin, thereby ensuring the authenticity of the sample data. Second, a mapping was established between the body-garment ease allowance and garment deformations using a convolutional neural network-based network in order to address the problem of dressing fit prediction for ready-made garments on different individuals. This method can also be extended to predicting the dressing fit for ready-made garments with similar styles. Experiments showed our virtual clothes try-on system obtained acceptable precision with a good sense of reality, and it can potentially be used in many applications such as three-dimensional garment design and online clothes retail.",robotics
10.1016/j.compag.2020.105535,Journal,Computers and Electronics in Agriculture,scopus,2020-08-01,sciencedirect,Vineyard trunk detection using deep learning – An experimental device benchmark,https://api.elsevier.com/content/abstract/scopus_id/85086590127,"Research and development in mobile robotics are continuously growing. The ability of a human-made machine to navigate safely in a given environment is a challenging task. In agricultural environments, robot navigation can achieve high levels of complexity due to the harsh conditions that they present. Thus, the presence of a reliable map where the robot can localize itself is crucial, and feature extraction becomes a vital step of the navigation process. In this work, the feature extraction issue in the vineyard context is solved using Deep Learning to detect high-level features – the vine trunks. An experimental performance benchmark between two devices is performed: NVIDIA’s Jetson Nano and Google’s USB Accelerator. Several models were retrained and deployed on both devices, using a Transfer Learning approach. Specifically, MobileNets, Inception, and lite version of You Only Look Once are used to detect vine trunks in real-time. The models were retrained in a built in–house dataset, that is publicly available. The training dataset contains approximately 1600 annotated vine trunks in 336 different images. Results show that NVIDIA’s Jetson Nano provides compatibility with a wider variety of Deep Learning architectures, while Google’s USB Accelerator is limited to a unique family of architectures to perform object detection. On the other hand, the Google device showed an overall Average precision higher than Jetson Nano, with a better runtime performance. The best result obtained in this work was an average precision of 52.98% with a runtime performance of 23.14 ms per image, for MobileNet-V2. Recent experiments showed that the detectors are suitable for the use in the Localization and Mapping context.",robotics
10.1016/j.psep.2020.05.017,Journal,Process Safety and Environmental Protection,scopus,2020-08-01,sciencedirect,Adsorption removal and reuse of phosphate from wastewater using a novel adsorbent of lanthanum-modified platanus biochar,https://api.elsevier.com/content/abstract/scopus_id/85085017552,"In this study, a novel adsorbent of lanthanum-modified platanus ball fiber biochar (La-TC) was developed for efficient adsorption and reuse of phosphate from actual wastewater. La-TC adsorbing phosphate could be used as agricultural fertilizer. The saturated adsorption capacity of phosphate on La-TC was 148.11 mg/g at dosage of 0.4 g/L, initial solution pH of 6.0, contact time of 20 min, and temperature of 35 ℃, which was much higher than most phosphate biochar adsorbents. Simultaneously, La-TC had a wide pH (3–9) adsorption stability, a strong ability to resist interference of anti-competitive anion (SO2−
                     4 and NO− 
                     3), and an outstanding regeneration ability. Fourier transform infrared spectroscopy (FTIR), and X-ray photoelectron spectroscopy (XPS) were used to reveal the adsorption mechanism of La-TC to phosphate, including electrostatic adsorption, ligand exchange, and complexation mechanisms. In the fixed bed column experiment of phosphorus removal from real wastewater, the maximum treated bed volume was 420, 440 and 480 (BV) under the bed flow of 1 mL/min and La-TC of 0.5, 0.75 and 1.0 g, respectively. Outcomes suggested that La-TC had a broad prospect of engineering application for removal and reuse of phosphate from wastewater, also realizing the resource utilization of wasted platanus ball.",robotics
10.1016/j.neucom.2020.02.088,Journal,Neurocomputing,scopus,2020-07-25,sciencedirect,Robust adaptive neuronal controller for exoskeletons with sliding-mode,https://api.elsevier.com/content/abstract/scopus_id/85081893724,"A robust neural adaptive integral sliding mode control approach is proposed in the present paper for nonlinear exoskeleton systems. The proposed control technique is composed of two parts: an adaptive neural network controller and an adaptive integral terminal sliding mode controller. The adaptive laws are developed to estimate unknown parameters and ensure asymptotic stability of the closed-loop system. Only classical system’s properties are supposed to be known, such as the bounds on some parameters. The unknown dynamics of the system are estimated on line by the neural network control part. The proposed adaptive control strategy is designed to ensure the reaching of the sliding surface with enhanced tracking performance. The singularity problem of the terminal sliding mode approach is overcomed without adding any constraint. The closed-loop stability of the system in the sense of Lyapunov is demonstrated. The effectiveness of the proposed approach is tested in real time application with healthy human subjects by performing passive arm movements using a 2-DOF upper limb exoskeleton.",robotics
10.1016/j.neucom.2018.09.104,Journal,Neurocomputing,scopus,2020-07-05,sciencedirect,A data-efficient deep learning approach for deployable multimodal social robots,https://api.elsevier.com/content/abstract/scopus_id/85065221778,"The deep supervised and reinforcement learning paradigms (among others) have the potential to endow interactive multimodal social robots with the ability of acquiring skills autonomously. But it is still not very clear yet how they can be best deployed in real world applications. As a step in this direction, we propose a deep learning-based approach for efficiently training a humanoid robot to play multimodal games—and use the game of ‘Noughts and Crosses’ with two variants as a case study. Its minimum requirements for learning to perceive and interact are based on a few hundred example images, a few example multimodal dialogues and physical demonstrations of robot manipulation, and automatic simulations. In addition, we propose novel algorithms for robust visual game tracking and for competitive policy learning with high winning rates, which substantially outperform DQN-based baselines. While an automatic evaluation shows evidence that the proposed approach can be easily extended to new games with competitive robot behaviours, a human evaluation with 130 humans playing with the Pepper robot confirms that highly accurate visual perception is required for successful game play.",robotics
10.1016/j.jmsy.2020.06.001,Journal,Journal of Manufacturing Systems,scopus,2020-07-01,sciencedirect,Deep reinforcement learning for a color-batching resequencing problem,https://api.elsevier.com/content/abstract/scopus_id/85086638517,"In automotive paint shops, changes of colors between consecutive production orders cause costs for cleaning the painting robots. It is a significant task to re-sequence orders and group orders with identical color as a color batch to minimize the color changeover costs. In this paper, a Color-batching Resequencing Problem (CRP) with mix bank buffer systems is considered. We propose a Color-Histogram (CH) model to describe the CRP as a Markov decision process and a Deep Q-Network (DQN) algorithm to solve the CRP integrated with the virtual car resequencing technique. The CH model significantly reduces the number of possible actions of the DQN agent, so that the DQN algorithm can be applied to the CRP at a practical scale. A DQN agent is trained in a deep reinforcement learning environment to minimize the costs of color changeovers for the CRP. Two experiments with different assumptions on the order attribute distributions and cost metrics were conducted and evaluated. Experimental results show that the proposed approach outperformed conventional algorithms under both conditions. The proposed agent can run in real time on a regular personal computer with a GPU. Hence, the proposed approach can be readily applied in the production control of automotive paint shops to resolve order-resequencing problems.",robotics
10.1016/j.jvcir.2020.102790,Journal,Journal of Visual Communication and Image Representation,scopus,2020-07-01,sciencedirect,Learning latent geometric consistency for 6D object pose estimation in heavily cluttered scenes,https://api.elsevier.com/content/abstract/scopus_id/85084953749,"6D object pose (3D rotation and translation) estimation from RGB-D image is an important and challenging task in computer vision and has been widely applied in a variety of applications such as robotic manipulation, autonomous driving, augmented reality etc. Prior works extract global feature or reason about local appearance from an individual frame, which neglect the spatial geometric relevance between two frames, limiting their performance for occluded or truncated objects in heavily cluttered scenes. In this paper, we present a dual-stream network for estimating 6D pose of a set of known objects from RGB-D images. Our novelty stands in contrast to prior work that learns latent geometric consistency in pairwise dense feature representations from multiple observations of the same objects in a self-supervised manner. We show in experiments that our method outperforms state-of-the-art approaches on 6D object pose estimation in two challenging datasets, YCB-Video and LineMOD.",robotics
10.1016/j.conengprac.2020.104460,Journal,Control Engineering Practice,scopus,2020-07-01,sciencedirect,Learning control for transmission and navigation with a mobile robot under unknown communication rates,https://api.elsevier.com/content/abstract/scopus_id/85084846470,"In tasks such as surveying or monitoring remote regions, an autonomous robot must move while transmitting data over a wireless network with unknown, position-dependent transmission rates. For such a robot, this paper considers the problem of transmitting a data buffer in minimum time, while possibly also navigating towards a goal position. Two approaches are proposed, each consisting of a machine-learning component that estimates the rate function from samples; and of an optimal-control component that moves the robot given the current rate function estimate. Simple obstacle avoidance is performed for the case without a goal position. In extensive simulations, these methods achieve competitive performance compared to known-rate and unknown-rate baselines. A real indoor experiment is provided in which a Parrot AR.Drone 2 successfully learns to transmit the buffer.",robotics
10.1016/j.compag.2020.105499,Journal,Computers and Electronics in Agriculture,scopus,2020-07-01,sciencedirect,Implementation of deep-learning algorithm for obstacle detection and collision avoidance for robotic harvester,https://api.elsevier.com/content/abstract/scopus_id/85084730841,"Convolutional neural networks (CNNs) are the current state of the art systems in image semantic segmentation (SS). However, because it requires a large computational cost, it is not suitable for running on embedded devices, such as on rice combine harvesters. In order to detect and identify the surrounding environment for a rice combine harvester in real time, a neural network using Network Slimming to reduce the network model size, which takes wide neural networks as the input model, yielding a compact model (hereafter referred to as “pruned model”) with comparable accuracy, was applied based on an image cascade network (ICNet). Network Slimming performs channel-level sparsity of convolutional layers in the ICNet by imposing L1 regularization on channel scaling factors with the corresponding batch normalization layer, which removes less informative feature channels in the convolutional layers to obtain a more compact model. Then each of the pruned models were evaluated by mean intersection over union (IoU) on the test set. When the compaction ratio is 80%, it gives a 97.4% reduction of model volume size, running 1.33 times faster with comparable accuracy as the original model. The results showed that when the compaction ratio is less than 80%, a more efficient (less computational cost) model with a slightly reduced accuracy in comparison to the original model was achieved. Field tests were conducted with the pruned model (80% compaction ratio) to verify the performance of obstacle detection. Results showed that the average success rate of collision avoidance was 96.6% at an average processing speed of 32.2 FPS (31.1 ms per frame) with an image size of 640 × 480 pixels on a Jetson Xavier. It shows that the pruned model can be used for obstacle detection and collision avoidance in robotic harvesters.",robotics
10.1016/j.tins.2020.03.013,Journal,Trends in Neurosciences,scopus,2020-06-01,sciencedirect,Social Cognition in the Age of Human–Robot Interaction,https://api.elsevier.com/content/abstract/scopus_id/85084225891,"Artificial intelligence advances have led to robots endowed with increasingly sophisticated social abilities. These machines speak to our innate desire to perceive social cues in the environment, as well as the promise of robots enhancing our daily lives. However, a strong mismatch still exists between our expectations and the reality of social robots. We argue that careful delineation of the neurocognitive mechanisms supporting human–robot interaction will enable us to gather insights critical for optimising social encounters between humans and robots. To achieve this, the field must incorporate human neuroscience tools including mobile neuroimaging to explore long-term, embodied human–robot interaction in situ. New analytical neuroimaging approaches will enable characterisation of social cognition representations on a finer scale using sensitive and appropriate categorical comparisons (human, animal, tool, or object). The future of social robotics is undeniably exciting, and insights from human neuroscience research will bring us closer to interacting and collaborating with socially sophisticated robots.",robotics
10.1016/j.engappai.2020.103670,Journal,Engineering Applications of Artificial Intelligence,scopus,2020-06-01,sciencedirect,"Detecting, locating and recognising human touches in social robots with contact microphones",https://api.elsevier.com/content/abstract/scopus_id/85083676413,"There are many situations in our daily life where touch gestures during natural human–human interaction take place: meeting people (shaking hands), personal relationships (caresses), moments of celebration or sadness (hugs), etc. Considering that robots are expected to form part of our daily life in the future, they should be endowed with the capacity of recognising these touch gestures and the part of its body that has been touched since the gesture’s meaning may differ. Therefore, this work presents a learning system for both purposes: detect and recognise the type of touch gesture (stroke, tickle, tap and slap) and its localisation. The interpretation of the meaning of the gesture is out of the scope of this paper.
                  Different technologies have been applied to perceive touch by a social robot, commonly using a large number of sensors. Instead, our approach uses 3 contact microphones installed inside some parts of the robot. The audio signals generated when the user touches the robot are sensed by the contact microphones and processed using Machine Learning techniques. We acquired information from sensors installed in two social robots, Maggie and Mini (both developed by the RoboticsLab at the Carlos III University of Madrid), and a real-time version of the whole system has been deployed in the robot Mini. The system allows the robot to sense if it has been touched or not, to recognise the kind of touch gesture, and its approximate location. The main advantage of using contact microphones as touch sensors is that by using just one, it is possible to “cover” a whole solid part of the robot. Besides, the sensors are unaffected by ambient noises, such as human voice, TV, music etc. Nevertheless, the fact of using several contact microphones makes possible that a touch gesture is detected by all of them, and each may recognise a different gesture at the same time. The results show that this system is robust against this phenomenon. Moreover, the accuracy obtained for both robots is about 86%.",robotics
10.1016/j.compag.2020.105378,Journal,Computers and Electronics in Agriculture,scopus,2020-06-01,sciencedirect,Optimising realism of synthetic images using cycle generative adversarial networks for improved part segmentation,https://api.elsevier.com/content/abstract/scopus_id/85083496168,"In this paper we report on improving part segmentation performance for robotic vision using convolutional neural networks by optimising the visual realism of synthetic agricultural images. In Part I, a cycle consistent generative adversarial network was applied to synthetic and empirical images with the objective to generate more realistic synthetic images by translating them to the empirical domain. We hypothesise that plant part image features (e.g. color, texture) become more similar to the empirical domain after translation of the synthetic images. Results confirm this with an improved mean color distribution correlation with the empirical data prior of 0.62 and post translation of 0.90. Furthermore, the mean image features of contrast, homogeneity, energy and entropy moved closer to the empirical mean, post translation. In Part II, 7 experiments were performed using convolutional neural networks with different combinations of synthetic, synthetic translated to empirical and empirical images. We hypothesise that the translated images can be used for (i) improved learning of empirical images, and (ii) that learning without any fine-tuning with empirical images is improved by bootstrapping with translated images over bootstrapping with synthetic images.
                  Results confirm our hypotheses in Part II. First a maximum intersection-over-union performance was achieved of 0.52 when bootstrapping with translated images and fine-tuning with empirical images; an 8% increase compared to only using synthetic images. Second, training without any empirical fine-tuning resulted in an average IOU of 0.31; a 55% performance increase over previous methods that only used synthetic images. The key contribution of this paper to robotic vision is to provide supporting evidence that domain adaptation can be successfully used to translate and improve synthetic data to the real empirical domain that results in improved segmentation learning whilst lowering the dependency on manually annotated data.",robotics
10.1016/j.compag.2020.105434,Journal,Computers and Electronics in Agriculture,scopus,2020-06-01,sciencedirect,Dynamic Simulation Tool of fertigation in drip irrigation subunits,https://api.elsevier.com/content/abstract/scopus_id/85083338592,"Agriculture consumes approximately 95 million tonnes of fertilizers and 97,000 tonnes of active ingredients of pesticides and herbicides. Reducing external input systems can result in significant economic, social and environmental impact. Therefore, establishing an optimal fertigation schedule is essential to achieve efficient drip irrigation management and, therefore, achieve an optimal irrigated agriculture management system that ensures productive, environmental and economic viability. The objective of this study was to develop a decision support system (DSS) to facilitate farmers’ decision-making process and optimize the design and management of farm fertigation scheduling. Implemented in MATLAB®, FERTI-DRIP, was tested in a regular irrigation subunit and in an irregular irrigation subunit of a real water user association. Both irrigation subunits were tested with two irrigation emitter types: pressure-compensating emitters and non-pressure-compensating emitters. Thus, FERTI-DRIP was applied to four scenarios to analyse the effect of the size and shape of the irrigation subunit on the fertigation process. The effect of the pressure head in the irrigation subunit and the effect of the fertilizer dynamics during the fertigation event were also analysed. FERTI-DRIP allows users to compute fertilizer quality parameters to determine how to implement fertigation. FERTI-DRIP also allows users to accurately select pre-fertigation and post-fertigation processes to optimize hydraulic stability at the beginning of the fertigation event and ensure that there is no fertilizer remaining in the irrigation system after the fertigation event. FERTI-DRIP can be very helpful for start-time irrigation events, such in the case of sandy soils where pulse drip irrigation should be performed.",robotics
10.1016/j.infrared.2020.103296,Journal,Infrared Physics and Technology,scopus,2020-06-01,sciencedirect,Hyperspectral image classification using CNN with spectral and spatial features integration,https://api.elsevier.com/content/abstract/scopus_id/85083076268,"Hyperspectral image (HSI) classification is very important task having numerous applications in the remote sensing field. Many methods have been proposed in the recent years. Among them Convolutional Neural Network (CNN) based algorithms have shown higher performance. But these algorithms need high computational power and storage capacity. This Paper presents an approach for remote sensing hyper spectral image classification based on data normalization and CNN. HSI data is first normalized by reducing its scalar values by retaining complete information. Then, spectral and spatial information is extracted using Probabilistic Principal Component Analysis (PPCA) and Gabor filtering respectively. Further, the spectral and spatial information is integrated to form fused features. Finally classification task is done using simply designed CNN framework. Experiments are performed on three benchmark hyperspectral datasets (Indian Pines, Pavia University and Salinas). The proposed approach has achieved significant performance over the state-of-art methods. This can be useful in real world applications like agriculture, forestry and food processing.",robotics
10.1016/j.isatra.2020.02.012,Journal,ISA Transactions,scopus,2020-06-01,sciencedirect,Adaptive tracking control of an unmanned aerial system based on a dynamic neural-fuzzy disturbance estimator,https://api.elsevier.com/content/abstract/scopus_id/85080895180,"The main goal of this study is developing an adaptive controller which can solve the trajectory tracking for a class of quadcopter unmanned aerial system (UAS), namely a quadrotor. The control design introduces a new paradigm for adaptive controllers based on the implementation of a set of differential neural networks (DNNs) in the consequence section of a Takagi–Sugeno (T–S) fuzzy inference system. This dynamic fuzzy inference structure was used to approximate the UAS description. The particular form of interaction between neural networks and fuzzy inference systems proposed in the present work received the name of dynamic neural fuzzy system (DNFS). An adaptive controller based on this DNFS form was the main solution attained in this study. This DNFS controller was focused on the estimation and compensation of the uncertain section of the Quadrotor dynamics and then, forced the UAS to perform a hover flight while the tracking of desired angular positions succeeded, which results in tracking a desired trajectory in the X-Y plane. The control design methodology supported on the Lyapunov stability theory guaranteed ultimate boundedness of the estimation and tracking errors simultaneously. Several experimental tests in an outdoor environment by using a real Quadrotor platform was performed by using an RTK-GPS (Real Time Kinematic) system to determine the position of the vehicle in the X-Y plane. The experimental results confirmed the superior performance of the proposed algorithm based on the combination of DNNs and T–S techniques with respect to classical robust controllers.",robotics
10.1016/j.inffus.2019.12.004,Journal,Information Fusion,scopus,2020-06-01,sciencedirect,"Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges",https://api.elsevier.com/content/abstract/scopus_id/85077515262,"Continual learning (CL) is a particular machine learning paradigm where the data distribution and learning objective change through time, or where all the training data and objective criteria are never available at once. The evolution of the learning process is modeled by a sequence of learning experiences where the goal is to be able to learn new skills all along the sequence without forgetting what has been previously learned. CL can be seen as an online learning where knowledge fusion needs to take place in order to learn from streams of data presented sequentially in time. Continual learning also aims at the same time at optimizing the memory, the computation power and the speed during the learning process. An important challenge for machine learning is not necessarily finding solutions that work in the real world but rather finding stable algorithms that can learn in real world. Hence, the ideal approach would be tackling the real world in a embodied platform: an autonomous agent. Continual learning would then be effective in an autonomous agent or robot, which would learn autonomously through time about the external world, and incrementally develop a set of complex skills and knowledge.Robotic agents have to learn to adapt and interact with their environment using a continuous stream of observations. Some recent approaches aim at tackling continual learning for robotics, but most recent papers on continual learning only experiment approaches in simulation or with static datasets. Unfortunately, the evaluation of those algorithms does not provide insights on whether their solutions may help continual learning in the context of robotics. This paper aims at reviewing the existing state of the art of continual learning, summarizing existing benchmarks and metrics, and proposing a framework for presenting and evaluating both robotics and non robotics approaches in a way that makes transfer between both fields easier. We put light on continual learning in the context of robotics to create connections between fields and normalize approaches.",robotics
10.1016/j.neucom.2019.07.104,Journal,Neurocomputing,scopus,2020-05-21,sciencedirect,Estimation of human impedance and motion intention for constrained human–robot interaction,https://api.elsevier.com/content/abstract/scopus_id/85075464952,"In this paper, a complete framework for safe and efficient physical human-robot interaction (pHRI) is developed for robot by considering both issues of adaptation to the human partner and ensuring the motion constraints during the interaction. We consider the robot's learning of not only human motion intention, but also the human impedance. We employ radial basis function neural networks (RBFNNs) to estimate human motion intention in real time, and least square method is utilized in robot learning of human impedance. When robot has learned the impedance information about human, it can adjust its desired impedance parameters by a simple tuning law for operative compliance. An adaptive impedance control integrated with RBFNNs and full-state constraints is also proposed in our work. We employ RBFNNs to compensate for uncertainties in the dynamics model of robot and barrier Lyapunov functions are chosen to ensure that full-state constraints are not violated in pHRI. Results in simulations and experiments show the better performance of our proposed framework compared with traditional methods.",robotics
10.1016/j.neucom.2019.02.066,Journal,Neurocomputing,scopus,2020-05-21,sciencedirect,Robust real-time hand detection and localization for space human–robot interaction based on deep learning,https://api.elsevier.com/content/abstract/scopus_id/85075445505,"Hand gestures are quite suitable for space human–robot interaction (SHRI) because of their natural and convenient features. While the detection and localization of hands are the premise and foundation for SHRI based on hand gestures. But hand gestures are very complicated and hand sizes are very small in some images. These problems make the robust real-time hand detection and localization very difficult. In this paper, a feature-map-fused single shot multibox detector (FF-SSD) which is a deep learning network is designed to deal with the problems of hand detection and localization in SHRI. First, the background of the method is introduced in this paper, including an astronaut assistant robot platform, the difficulties of hand detection and localization, and introduction of the state-of-the-art deep learning networks for object detection and localization. Then, the FF-SSD is proposed for detecting and localizing hands especially pony-size hands. This network takes into consideration both accuracy and speed with balanced performance. And in the experiment part, the FF-SSD is trained and tested on hand databases which include a homemade database and two public databases. At last, the superiority of the proposed method is demonstrated compared with the state-of-the-art methods.",robotics
10.1016/j.cviu.2020.102947,Journal,Computer Vision and Image Understanding,scopus,2020-05-01,sciencedirect,ALCN: Adaptive Local Contrast Normalization,https://api.elsevier.com/content/abstract/scopus_id/85081126758,"To make Robotics and Augmented Reality applications robust to illumination changes, the current trend is to train a Deep Network with training images captured under many different lighting conditions. Unfortunately, creating such a training set is a very unwieldy and complex task. We therefore propose a novel illumination normalization method that can easily be used for different problems with challenging illumination conditions. Our preliminary experiments show that among current normalization methods, the Difference-of-Gaussians method remains a very good baseline, and we introduce a novel illumination normalization model that generalizes it. Our key insight is then that the normalization parameters should depend on the input image, and we aim to train a Convolutional Neural Network to predict these parameters from the input image. This, however, cannot be done in a supervised manner, as the optimal parameters are not known a priori. We thus designed a method to train this network jointly with another network that aims to recognize objects under different illuminations: The latter network performs well when the former network predicts good values for the normalization parameters. We show that our method significantly outperforms standard normalization methods and would also be appear to be universal since it does not have to be re-trained for each new application. Our method improves the robustness to light changes of state-of-the-art 3D object detection and face recognition methods.",robotics
10.1016/j.asoc.2020.106156,Journal,Applied Soft Computing Journal,scopus,2020-05-01,sciencedirect,A novel foraging algorithm for swarm robotics based on virtual pheromones and neural network,https://api.elsevier.com/content/abstract/scopus_id/85079878611,"Swarm robotics is an emerging interdisciplinary field that has many potential real-world applications. Swarm robotics aims to produce robust, scalable, and flexible self-organizing behaviors through local interactions from a large number of simple robots. In this paper, a novel pheromone model of swarm foraging behavior is developed based on a neural network. The output of a single neuron corresponds to the density of a pheromone, which diffuses to neighboring neurons through their local connections. A neural network is updated based on the proposed evaporation model. Neural networks can often mimic the dynamics and features of pheromones. Therefore, in this work, we develop an optimization method to determine the key parameters of cooperative foraging based on mathematical modeling. The differential equation variables represent the number of foraging robots assigned different tasks. The solutions of the differential equations represent the dynamics of the foraging behavior. The key parameters that affect task allocation are determined to make optimal decision rules. Simulation experiments are conducted under different foraging scenarios. The experimental results demonstrate the effectiveness of the proposed pheromone model.",robotics
10.1016/j.jnca.2020.102577,Journal,Journal of Network and Computer Applications,scopus,2020-05-01,sciencedirect,Bot recognition in a Web store: An approach based on unsupervised learning,https://api.elsevier.com/content/abstract/scopus_id/85079622929,"Web traffic on e-business sites is increasingly dominated by artificial agents (Web bots) which pose a threat to the website security, privacy, and performance. To develop efficient bot detection methods and discover reliable e-customer behavioural patterns, the accurate separation of traffic generated by legitimate users and Web bots is necessary. This paper proposes a machine learning solution to the problem of bot and human session classification, with a specific application to e-commerce. The approach studied in this work explores the use of unsupervised learning (k-means and Graded Possibilistic c-Means), followed by supervised labelling of clusters, a generative learning strategy that decouples modelling the data from labelling them. Its efficiency is evaluated through experiments on real e-commerce data, in realistic conditions, and compared to that of supervised learning classifiers (a multi-layer perceptron neural network and a support vector machine). Results demonstrate that the classification based on unsupervised learning is very efficient, achieving a similar performance level as the fully supervised classification. This is an experimental indication that the bot recognition problem can be successfully dealt with using methods that are less sensitive to mislabelled data or missing labels. A very small fraction of sessions remain misclassified in both cases, so an in-depth analysis of misclassified samples was also performed. This analysis exposed the superiority of the proposed approach which was able to correctly recognize more bots, in fact, and identified more camouflaged agents, that had been erroneously labelled as humans.",robotics
10.1016/j.simpat.2019.102015,Journal,Simulation Modelling Practice and Theory,scopus,2020-05-01,sciencedirect,A fog computing model for implementing motion guide to visually impaired,https://api.elsevier.com/content/abstract/scopus_id/85074521783,"A guide dog robot system for visually impaired often needs to process many kinds of information, such as image, voice and other sensor information. Information processing methods based on deep neural network can achieve better results. However, it requires expensive computing and communication resources to meet the real-time requirement. Fog computing has emerged as a promising solution for applications that are data-intensive and delay-sensitive. We propose a fog computing framework named PEN (Phone + Embedded board + Neural compute stick) for the guide dog robot system. The robot’s functions in PEN are wrapped as services and deployed on the appropriate devices. Services are combined as an application in a visual programming language environment. Neural compute stick accelerates image processing speed at low power consumption. A simulation environment and a prototype are built on the framework. The simulated guide dog system is developed for operating in a miniature environment, including a small robot dog, a small wheelchair, model cars, traffic lights, and traffic blockage. The prototype is a full-sized portable guide system that can be used by a visually impaired person in a real environment. Simulation and experiments show that the framework can meet the functional and performance requirements for implementing the guide systems for visually impaired.",robotics
10.1016/j.robot.2020.103472,Journal,Robotics and Autonomous Systems,scopus,2020-04-01,sciencedirect,Deploying MAVs for autonomous navigation in dark underground mine environments,https://api.elsevier.com/content/abstract/scopus_id/85079573394,"Operating Micro Aerial Vehicles (MAVs) in subterranean environments is becoming more and more relevant in the field of aerial robotics. Despite the large spectrum of technological advances in the field, flying in such challenging environments is still an ongoing quest that requires the combination of multiple sensor modalities like visual/thermal cameras as well as 3D and 2D lidars. Nevertheless, there exist cases in subterranean environments where the aim is to deploy fast and lightweight aerial robots for area reckoning purposes after an event (e.g. blasting in production areas). This work proposes a novel baseline approach for the navigation of resource constrained robots, introducing the aerial underground scout, with the main goal to rapidly explore unknown areas and provide a feedback to the operator. The main proposed framework focuses on the navigation, control and vision capabilities of the aerial platforms with low-cost sensor suites, contributing significantly towards real-life applications. The merit of the proposed control architecture is that it considers the flying platform as a floating object, composing a velocity controller on the 
                        x
                     , 
                        y
                      axes and altitude control to navigate along the tunnel. Two novel approaches make up the cornerstone of the proposed contributions for the task of navigation: (1) a vector geometry method based on 2D lidar, and (2) a Deep Learning (DL) method through a classification process based on an on-board image stream, where both methods correct the heading towards the center of the mine tunnel. Finally, the framework has been evaluated in multiple field trials in an underground mine in Sweden.",robotics
10.1016/j.aei.2020.101052,Journal,Advanced Engineering Informatics,scopus,2020-04-01,sciencedirect,Deep learning-based method for vision-guided robotic grasping of unknown objects,https://api.elsevier.com/content/abstract/scopus_id/85079340469,"Nowadays, robots are heavily used in factories for different tasks, most of them including grasping and manipulation of generic objects in unstructured scenarios. In order to better mimic a human operator involved in a grasping action, where he/she needs to identify the object and detect an optimal grasp by means of visual information, a widely adopted sensing solution is Artificial Vision. Nonetheless, state-of-art applications need long training and fine-tuning for manually build the object’s model that is used at run-time during the normal operations, which reduce the overall operational throughput of the robotic system. To overcome such limits, the paper presents a framework based on Deep Convolutional Neural Networks (DCNN) to predict both single and multiple grasp poses for multiple objects all at once, using a single RGB image as input. Thanks to a novel loss function, our framework is trained in an end-to-end fashion and matches state-of-art accuracy with a substantially smaller architecture, which gives unprecedented real-time performances during experimental tests, and makes the application reliable for working on real robots. The system has been implemented using the ROS framework and tested on a Baxter collaborative robot.",robotics
10.1016/j.autcon.2020.103078,Journal,Automation in Construction,scopus,2020-04-01,sciencedirect,Complete coverage path planning using reinforcement learning for Tetromino based cleaning and maintenance robot,https://api.elsevier.com/content/abstract/scopus_id/85077924455,"Tiling robotics have been deployed in autonomous complete area coverage tasks such as floor cleaning, building inspection, and maintenance, surface painting. One class of tiling robotics, polyomino-based reconfigurable robots, overcome the limitation of fixed-form robots in achieving high-efficiency area coverage by adopting different morphologies to suit the needs of the current environment. Since the reconfigurable actions of these robots are produced by real-time intelligent decisions during operations, an optimal path planning algorithm is paramount to maximize the area coverage while minimizing the energy consumed by these robots. This paper proposes a complete coverage path planning (CCPP) model trained using deep blackreinforcement learning (RL) for the tetromino based reconfigurable robot platform called hTetro to simultaneously generate the optimal set of shapes for any pretrained arbitrary environment shape with a trajectory that has the least overall cost. To this end, a Convolutional Neural Network (CNN) with Long Short Term Memory (LSTM) layers is trained using Actor Critic Experience Replay (ACER) reinforcement learning algorithm. The results are compared with existing approaches which are based on the traditional tiling theory model, including zigzag, spiral, and greedy search schemes. The model is also compared with the Travelling salesman problem (TSP) based Genetic Algorithm (GA) and Ant Colony Optimization (ACO) schemes. The proposed scheme generates a path with lower cost while also requiring lesser time to generate it. The model is also highly robust and can generate a path in any pretrained arbitrary environments.",robotics
10.1016/j.bspc.2019.101844,Journal,Biomedical Signal Processing and Control,scopus,2020-04-01,sciencedirect,Evolving control of human-exoskeleton system using Gaussian process with local model,https://api.elsevier.com/content/abstract/scopus_id/85077492393,"Significant application breakthrough has not been achieved in human-exoskeleton systems due to the lack of transparent human-exoskeleton interaction. An evolving learning control strategy based on Gaussian process with local model (GPLM) is proposed to realize transparent control of a human-exoskeleton system. As a data driven technique, Gaussian process (GP) serves as a mathematical foundation to learn the dynamics of human lower limbs. Local model is incorporated into the GP framework to improve the prediction results and reduce the computation cost. The GPLM integrates the white-box model and block-box strategy and make use of their benefits. A knowledge base is developed to achieve evolving of the system structure and hyper-parameters. The proposed method can figure out the unspecific kinematics and dynamics of human-exoskeleton systems and cope with the emerging dynamics in new operation regions by system evolving. Surface electromyography (sEMG) signals are processed by a novel method, and are treated as one of the inputs of the model. The proposed sEMG signals processing method is expected to grasp the human muscle activation levels and improve the computational efficiency which can meet the requirements of real-time control. Experimental results show that the interactive force, human effort and interaction damping are reduced greatly compared with traditional force control. The proposed algorithm makes huge improvements on the transparent human-exoskeleton interaction by system evolving, while ensuring the safety of users by risk-based control.",robotics
10.1016/j.neucom.2019.09.098,Journal,Neurocomputing,scopus,2020-03-14,sciencedirect,A neuromorphic SLAM architecture using gated-memristive synapses,https://api.elsevier.com/content/abstract/scopus_id/85075974125,"Navigation in GPS-denied environments is a critical challenge for autonomous mobile platforms such as drones. The concept of simultaneous localization and mapping (SLAM) addresses this challenge through real-time mapping of the platform's surroundings as it explores its environment. The computational resources required for traditional SLAM implementations (e.g. graphical processing units) require large size, weight, and power overheads; making it infeasible to employ them in resource-constrained applications. This work proposes a self-learning hardware architecture utilizing a novel gated-memristive device to address the implementation of SLAM in an energy-efficient manner. The gated-memristive devices are implemented as electronic synapses in tandem with novel low-energy spiking neurons to create a spiking neural network (SNN). This work shows how the SNN allows for navigation through an environment via landmark association without needing GPS. In the simple environment in which the network exists, it can successfully determine a direction in which to navigate while only consuming 36 µW of power and only needing to be exposed to each landmark within the environment for 1-2ms in order to remember that location.",robotics
10.1016/j.neucom.2019.11.007,Journal,Neurocomputing,scopus,2020-03-14,sciencedirect,Neuropod: A real-time neuromorphic spiking CPG applied to robotics,https://api.elsevier.com/content/abstract/scopus_id/85075928379,"Initially, robots were developed with the aim of making our life easier, carrying out repetitive or dangerous tasks for humans. Although they were able to perform these tasks, the latest generation of robots are being designed to take a step further, by performing more complex tasks that have been carried out by smart animals or humans up to date. To this end, inspiration needs to be taken from biological examples. For instance, insects are able to optimally solve complex environment navigation problems, and many researchers have started to mimic how these insects behave. Recent interest in neuromorphic engineering has motivated us to present a real-time, neuromorphic, spike-based Central Pattern Generator of application in neurorobotics, using an arthropod-like robot. A Spiking Neural Network was designed and implemented on SpiNNaker. The network models a complex, online-change capable Central Pattern Generator which generates three gaits for a hexapod robot locomotion in real-time. Reconfigurable hardware was used to manage both the motors of the robot and the low-latency communication interface with the Spiking Neural Networks. Real-time measurements confirm the simulation results, and locomotion tests show that NeuroPod can perform the gaits without any balance loss or added delay.",robotics
10.1016/j.robot.2019.103410,Journal,Robotics and Autonomous Systems,scopus,2020-03-01,sciencedirect,Multi-agent sensitivity enhanced iterative best response: A real-time game theoretic planner for drone racing in 3D environments,https://api.elsevier.com/content/abstract/scopus_id/85078148856,"This paper presents a real-time game theoretic motion planning approach that enables an autonomous drone to race competitively against an arbitrary number of opponent drones along a 2D or 3D racecourse. Our method computes an approximate Nash equilibrium in the space of robot trajectories to maximally advance the ego robot while taking into account the opponents’ intentions and responses. The core of our solution is a “sensitivity enhanced” iterative best response algorithm that the ego robot uses to repeatedly plan its own trajectory and infer opponents’ trajectories, ultimately seeking a Nash equilibrium in the joint space of trajectories for all the drones. The algorithm includes a term that allows the ego vehicle to gain advantage by exploiting the influence of the ego drone’s trajectory on the adversaries’ objectives through the shared collision avoidance constraints among the vehicles. We also propose two methods for accelerating this computationally intensive iterative algorithm using (i) parallel computing with multiple CPU cores, and (ii) a neural network model that learns to predict trajectories close to the Nash equilibrium through offline training examples. Extensive simulation studies are conducted to benchmark the performance of our game theoretic planner and the statistical results show that our approach largely outperforms a baseline model predictive control algorithm that does not account for the opponents’ reactions. Hardware experiments with 4 quadrotor robots on a 3D racecourse are performed to show the applicability of our method in real-time robotic systems.",robotics
10.1016/j.artint.2019.103228,Journal,Artificial Intelligence,scopus,2020-03-01,sciencedirect,Reasoning about uncertain parameters and agent behaviors through encoded experiences and belief planning,https://api.elsevier.com/content/abstract/scopus_id/85077739377,Robots are expected to handle increasingly complex tasks. Such tasks often include interaction with objects or collaboration with other agents. One of the key challenges for reasoning in such situations is the lack of accurate models that hinders the effectiveness of planners. We present a system for online model adaptation that continuously validates and improves models while solving tasks with a belief space planner. We employ the well known online belief planner POMCP. Particles are used to represent hypotheses about the current state and about models of the world. They are sufficient to configure a simulator to provide transition and observation models. We propose an enhanced particle reinvigoration process that leverages prior experiences encoded in a recurrent neural network (RNN). The network is trained through interaction with a large variety of object and agent parametrizations. The RNN is combined with a mixture density network (MDN) to process the current history of observations in order to propose suitable particles and models parametrizations. The proposed method also ensures that newly generated particles are consistent with the current history. These enhancements to the particle reinvigoration process help alleviate problems arising from poor sampling quality in large state spaces and enable handling of dynamics with discontinuities. The proposed approach can be applied to a variety of domains depending on what uncertainty the decision maker needs to reason about. We evaluate the approach with experiments in several domains and compare against other state-of-the-art methods. Experiments are done in a collaborative multi-agent and a single agent object manipulation domain. The experiments are performed both in simulation and on a real robot. The framework handles reasoning with uncertain agent behaviors and with unknown object and environment parametrizations well. The results show good performance and indicate that the proposed approach can improve existing state-of-the-art methods.,robotics
10.1016/j.engappai.2019.103447,Journal,Engineering Applications of Artificial Intelligence,scopus,2020-03-01,sciencedirect,Emotional neural networks with universal approximation property for stable direct adaptive nonlinear control systems,https://api.elsevier.com/content/abstract/scopus_id/85077512941,"Universal approximation, continuity, and differentiability are desirable properties of any computational framework, including those that rise from human cognition and/or are inspired by nature. Emotional machines constitute one such framework, but few studies have addressed their mathematical properties. Here, we propose a Continuous Radial Basis Emotional Neural Network (CRBENN) that benefits from the universal approximation property, continuous output, and simple structure of RBF; while keeping the fast response properties of emotion-based approaches. As such, CRBENN is amenable to a wide array of challenging problems in systems engineering and artificial intelligence. Here, we propose a CRBENN-based direct adaptive robust emotional neuro-control approach (DARENC) for a class of uncertain nonlinear systems. Stability is theoretically established using Lyapunov analysis of the closed-loop system. DARENC is then applied to control two nonlinear systems, and the performance of the controller is numerically compared with several competing fuzzy, neural, and emotional controllers. The simulation results indicate improved tracking performance, better disturbance rejection, and less control effort. Finally, DARENC is implemented on a real-world 3-PSP (spherical–prismatic–spherical) parallel robot in our laboratory. The experimental results show the satisfactory performance of the robot in tracking the desired trajectory with low control effort.",robotics
10.1016/j.ast.2019.105657,Journal,Aerospace Science and Technology,scopus,2020-03-01,sciencedirect,Reinforcement learning in dual-arm trajectory planning for a free-floating space robot,https://api.elsevier.com/content/abstract/scopus_id/85077502803,"A free-floating space robot exhibits strong dynamic coupling between the arm and the base, and the resulting position of the end of the arm depends not only on the joint angles but also on the state of the base. Dynamic modeling is complicated for multiple degree of freedom (DOF) manipulators, especially for a space robot with two arms. Therefore, the trajectories are typically planned offline and tracked online. However, this approach is not suitable if the target has relative motion with respect to the servicing space robot. To handle this issue, a model-free reinforcement learning strategy is proposed for training a policy for online trajectory planning without establishing the dynamic and kinematic models of the space robot. The model-free learning algorithm learns a policy that maps states to actions via trial and error in a simulation environment. With the learned policy, which is represented by a feedforward neural network with 2 hidden layers, the space robot can schedule and perform actions quickly and can be implemented for real-time applications. The feasibility of the trained policy is demonstrated for both fixed and moving targets.",robotics
10.1016/j.engappai.2019.103427,Journal,Engineering Applications of Artificial Intelligence,scopus,2020-03-01,sciencedirect,Stochastic parallel extreme artificial hydrocarbon networks: An implementation for fast and robust supervised machine learning in high-dimensional data,https://api.elsevier.com/content/abstract/scopus_id/85076620125,"Artificial hydrocarbon networks (AHN) – a supervised learning method inspired on organic chemical structures and mechanisms – have shown improvements in predictive power and interpretability in comparison with other well-known machine learning models. However, AHN are very time-consuming that are not able to deal with large data until now. In this paper, we introduce the stochastic parallel extreme artificial hydrocarbon networks (SPE-AHN), an algorithm for fast and robust training of supervised AHN models in high-dimensional data. This training method comprises a population-based meta-heuristic optimization with defined individual encoding and objective function related to the AHN-model, an implementation in parallel-computing, and a stochastic learning approach for consuming large data. We conducted three experiments with synthetic and real data sets to validate the training execution time and performance of the proposed algorithm. Experimental results demonstrated that the proposed SPE-AHN outperforms the original-AHN method, increasing the speed of training more than 
                        
                           10
                           ,
                           000
                           x
                        
                      times in the worst case scenario. Additionally, we present two case studies in real data sets for solar-panel deployment prediction (regression problem), and human falls and daily activities classification in healthcare monitoring systems (classification problem). These case studies showed that SPE-AHN improves the state-of-the-art machine learning models in both engineering problems. We anticipate our new training algorithm to be useful in many applications of AHN like robotics, finance, medical engineering, aerospace, and others, in which large amounts of data (e.g. big data) is essential.",robotics
10.1016/j.ress.2019.106700,Journal,Reliability Engineering and System Safety,scopus,2020-03-01,sciencedirect,Optimizing inspection routes in pipeline networks,https://api.elsevier.com/content/abstract/scopus_id/85073997788,"Maintaining an aging network is a challenge for many water utilities due to limited budgets and uncertainty surrounding the physical condition of buried pipeline assets. The deployment of robotic inspections provides high quality data, but these platforms have limited use due to cost and operational constraints. To facilitate cost-efficient inspections, operators need to identify high-risk assets while accounting for the effectiveness of the tools at hand. This paper addresses inspection planning with the goal of finding an optimal route while considering tool limitations. An exact integer programming formulation is presented where only three factors are used to characterize tool constraints. Two classes of solution methods are explored: 1) tree based searches, and 2) integer programming. This paper demonstrates how each method can be used to identify optimal paths within a real water distribution system. Empirical trials suggest that tree-based search methods are the most efficient when the path limit is short, but do not scale well when the path length increases. In contrast, integer-programming methods are more effective for longer path lengths but have scalability issues for large network sizes. Data preprocessing, where the input network size is reduced, can provide large computation time reductions while returning near-optimal solutions.",robotics
10.1016/j.eng.2019.11.003,Journal,Engineering,scopus,2020-02-01,sciencedirect,Grasp Planning and Visual Servoing for an Outdoors Aerial Dual Manipulator,https://api.elsevier.com/content/abstract/scopus_id/85076562878,"This paper describes a system for grasping known objects with unmanned aerial vehicles (UAVs) provided with dual manipulators using an RGB-D camera. Aerial manipulation remains a very challenging task. This paper covers three principal aspects for this task: object detection and pose estimation, grasp planning, and in-flight grasp execution. First, an artificial neural network (ANN) is used to obtain clues regarding the object’s position. Next, an alignment algorithm is used to obtain the object’s six-dimensional (6D) pose, which is filtered with an extended Kalman filter. A three-dimensional (3D) model of the object is then used to estimate an arranged list of good grasps for the aerial manipulator. The results from the detection algorithm—that is, the object’s pose—are used to update the trajectories of the arms toward the object. If the target poses are not reachable due to the UAV’s oscillations, the algorithm switches to the next feasible grasp. This paper introduces the overall methodology, and provides the experimental results of both simulation and real experiments for each module, in addition to a video showing the results.",robotics
10.1016/j.robot.2019.103386,Journal,Robotics and Autonomous Systems,scopus,2020-02-01,sciencedirect,Learning inverse kinematics and dynamics of a robotic manipulator using generative adversarial networks,https://api.elsevier.com/content/abstract/scopus_id/85076459973,"Obtaining inverse kinematics and dynamics of a robotic manipulator is often crucial for robot control. Analytical models are typically used to approximate real robot systems, and various controllers have been designed on top of the analytical model to compensate for the approximation error. Recently, machine learning techniques have been developed for error compensation, resulting in better performance. Unfortunately, combining a learned compensator with an analytical model makes the designed controller redundant and computationally expensive. Also, general machine learning techniques require a lot of data to perform the training process and approximation, especially in solving high dimensional problems. As a result, state-of-the-art machine learning applications are either expensive in terms of computation and data collection, or limited to a local approximation for a specific task or routine. In order to address the high dimensionality problem in learning inverse kinematics and dynamics, as well as to make the training process more data efficient, this paper presents a novel approach using a series of modified Generative Adversarial Networks (GANs). Namely, we use Conditional GANs (CGANs), Least Squares GANs (LSGANs), Bidirectional GANs (BiGANs) and Dual GANs(DualGANs). We trained and tested the proposed methods using real-world data collected from two types of robotic manipulators, a MICO robotic manipulator and a Fetch robotic manipulator. The data input to the GANs was obtained using a sampling method applied to the real data. The proposed approach enables approximating the real model using limited data without compromising the performance and accuracy. The proposed methods were tested in real-world experiments using unseen trajectories to validate the “learned” approximate inverse kinematics and inverse dynamics as well as to demonstrate the capability and effectiveness of the proposed algorithm over existing analytical models.",robotics
10.1016/j.neucom.2019.09.004,Journal,Neurocomputing,scopus,2020-01-02,sciencedirect,Digital neuromorphic real-time platform,https://api.elsevier.com/content/abstract/scopus_id/85072526243,"Hardware implementations of spiking neural networks in portable devices can improve many applications of robotics, neurorobotics or prosthetic fields in terms of power consumption, high-speed processing and learning mechanisms. Analog and digital platforms have been previously proposed to run these networks. Analog designs are closer to biology since they implement the original mathematical model. However, digital platforms are, to some extent, abstractions of this model so far. In this paper, a full digital platform to design, implement and run real-time analog-like spiking neural networks is presented. Specifically, we present the design and implementation of digital circuits to run real-time biologically plausible spiking neural networks on a Field Programmable Gate Array (FPGA). The circuit designed for the neuron implements the Leaky Integrate and Fire (LIF) model. The synapsis implemented is a bi-exponential current-based one. The synaptic circuit design consists of one static memory with the baseline current and a dynamic memory which stores the updated contribution over time of each pre-synaptic connection. All the parameters of both the neuron and the synapse are configurable. The results of the circuits are validated by running the same experiments on the Brian simulator. The circuits, which are totally original and independent of the technology, use only 136 slice registers of hardware resources. Thus, these designs allow the scale of the network. These circuits aim to be the basis of the spiking neural networks on digital devices. This platform allows the user to first simulate their network within the Brian simulator and then, confidently, move to the hardware platform replicating the same performance or even replace their analog platform with the digital one.",robotics
10.1016/j.ifacol.2020.12.2628,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Deep-learning-based relocalization in large-scale outdoor environment,https://api.elsevier.com/content/abstract/scopus_id/85108025670,"For the issue of relocalization, this paper proposed a deep-learning-based method for outdoor large-scale environment. In the first step, we projected a 3D Light Detection and Ranging(LiDAR) scan onto three 2D images from top to bottom. Then a densenet-based neural network structure was designed to regress a 4-DOF robot pose. These images are then stacked together, fed into the proposed DCNN architecture, and the output is the predicted robot pose. Extensive experiments have been conducted in practice with a real mobile robot, verifying the effectiveness of the proposed strategy. Our network can obtain approximately 3.5m and 4◦ accuracy outdoors.",robotics
10.1016/j.ifacol.2020.12.2729,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,A novel multilateral control design for delayed nonlinear teleoperation system with RBFNN-based environments,https://api.elsevier.com/content/abstract/scopus_id/85107782234,"In this paper, a novel multilateral control design for nonlinear teleoperation system is proposed to improve the capability of multiple robots to coordinate efficiently and precisely in the remote environments under time-varying delays and various uncertainties. The environment is modeled with a general form of force under radial basis function neural network(RBFNN)-based identification and reconstruction to avoid the passivity issue in the traditional teleoperation control and provide the human operators with good sensing of environments. The desired trajectory producers and RBFNN-based sliding mode controllers are designed separately to achieve the good tracking of master/slave robots, and the coordinated distribution algorithm is designed to obtain the control input u
                        S,i
                      for each slave robot. Therefore, the global stability, good transparency performance with both position tracking and force feedback, and good cooperative performance can be achieved simultaneously for delayed nonlinear teleoperation system. The real platform experiment is carried out on a 2-master-2-slave teleoperation system to verify the effectiveness of proposed control design.",robotics
10.1016/j.ifacol.2020.12.2306,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Path-following control of fish-like robots: A deep reinforcement learning approach,https://api.elsevier.com/content/abstract/scopus_id/85102821107,"In this paper, we propose a deep reinforcement learning (DRL) approach for path-following control of a fish-like robot. The desired path may be a randomly generated Bézier curve. First, to implement the locomotion control of the fish-like robot, we design a modified Central Pattern Generated (CPG) model, using which the fish achieves varied swimming behaviors just by adjusting a single control input. To reduce the reality gap between simulation and the physical system, using the experimental data of the real fish-like robot, we build a surrogate simulation environment, which also well balances the accuracy and the speed of training. Second, for the path-following control, we select the advantage actor-critic (A2C) approach and train the control policy in the surrogate simulation environment with a straight line as the desired path. Then the trained control policy is directly deployed on a physical fish-like robot to follow a randomly generated Bézier curve. The experimental results show that our proposed approach has good practical applicability in view of its efficiency and feasibility in controlling the physical fishlike robot. This work shows a novel and promising way to control biomimetic underwater robots in the real world.",robotics
10.1016/j.promfg.2020.11.049,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Near Sensors Computation based on Embedded Machine Learning for Electronic Skin,https://api.elsevier.com/content/abstract/scopus_id/85100744258,"The electronic skin system is usually made of distributed tactile sensors integrated with an embedded electronic system for tactile data decoding. Meaningful information e.g. texture classification and pattern recognition can be decoded from tactile data by employing machine learning methods. Near sensors computation using embedded machine learning algorithms may enable the electronic skin system to be used in various application domains such as wearable Internet of Things devices, prosthetics, and robotics. However, embedding machine-learning algorithms is constrained by the high computational complexity of Machine Learning methods. This poses relevant challenges on 1) real-time operation and 2) very low (e.g. pJ/op) power/energy consumption due to the limited energy budget available on wearable/portable systems. In this perspective, the paper presents our recent achievements in the implementation of embedded machine learning methods for near sensors tactile data processing. The paper provides an overview about the implementation on dedicated hardware platforms. Finally, efficient techniques for embedded machine learning highlighting the challenges and perspectives are discussed with major emphasis on energy-efficient intelligent electronic skin systems.",robotics
10.1016/j.procir.2020.04.135,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Application of Artificial Intelligence to an Electrical Rewinding Factory Shop,https://api.elsevier.com/content/abstract/scopus_id/85091693237,"The evolution of artificial intelligence (AI) and big data resulted in the full potential realization of technologies through convergence. Tremendous acceptance, adoption and implementation of the United Nations Sustainable Development Goals (SDG) Agenda 2030, has resulted in original equipment manufacturers (OEM) developing various designs of rotary machines in a bid to improve energy efficiency, with more improvements expected in the coming decade. An effective technique to manage energy efficiency in the smart grid is through integration of demand side management, inclusive of optimization of rewinding of an electric motor in a machine shop. This paper aims to conceptualize application of AI and augmented reality (AR) towards process visibility of remanufacturing rotary machine stators by robotic vision. SLT is the triangulation methodology used in laser scanning for 3D modelling, and instantaneous condition assessment of the core. A pre-defined robotic path is used towards identification of features for range image acquisition. Therefore, the potential of industry 4.0 in resuscitation of end-of-life products through service remanufacturers by RE in a rewinding shop are presented.",robotics
10.1016/j.matpr.2019.11.222,Conference Proceeding,Materials Today: Proceedings,scopus,2020-01-01,sciencedirect,Design of a robotic walking stick with mobility assistance control technology (MAVI) for visually impaired people,https://api.elsevier.com/content/abstract/scopus_id/85088581669,"The investigation of the present research is a new mechatronic system for a robotic unicycle staff that thanks to a meticulous analysis of the new technologies applied to assistance of technological mobility of people with visual disability as an orthopedic walking stick or a walker. We proceed to contribute with the design of a stick-type mechatronic system coupled to a differential traction platform that facilitates the mobilization of people with motor deficiency by giving them an assistance system to support in the march. It was verified that the design supports the maximum weight established in the parameters of 14[kg] It was shown that the walking stick can be used with people up to a weight of 93[kg]. The performance of the load cells with an experimental work cycle by incorporating an application in Android so the user has a feeling of immersion and to provide the possibility of interacting with the device, being able to establish between the user and the robotic staff a bidirectional transfer in real time of information. The findings of the research, multi-axis load cells will be implemented in the future with a control was applying neural networks to minimize the displacement error.",robotics
10.1016/j.promfg.2020.02.258,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Combining Mobile Robotics and Packing for Optimal deliveries,https://api.elsevier.com/content/abstract/scopus_id/85088044464,"The problem addressed is taken from warehousing and distribution. It concerns the order preparation in e-commerce. An order is a set of lines. Each line describes the ordered product (box) with its corresponding quantity. The problem consists of packing boxes subject to orientation, fragility, stackability, weight and volume constraints of various sizes into available containers (trays, cartons) in a way which optimizes the total number of containers to integrate in a number of delivery trips in e-commerce. The article addresses the problem where the packing is performed by a mobile robot. The aim of the work carried out in the context of the project INCASE[0] which is a plat-form of the INTERREG V [1] is to conduct, among other experiments[2] a set of experiments where the packing activity is performed by a mobile robot in order to pick up items from shelves and then pack items in boxes. To keep the problem simple, we assume that positions of items to pick up and to place them in the box are known; they are computed by efficient bin packing optimization algorithms (2D, 3D) of Optim Suite of KLS OPTIM in production since many years. There are many ways of using bin packing algorithms. The first possible use is the online packing of items based on a fixed order defined by the host system. This approach has several drawbacks, among them the inefficient handling of stability constraint and sub-use of available box volumes. The second approach is to allow packing algorithms to compute optimal or sub-optimal solutions whilst respecting all constraints; this approach is used in production. One drawback of this approach is such solutions are not always feasible by a mobile robot. The aim of our work is that the mobile robot must be able to take into account optimization constraints in order to place objects on the pallets. The objective is to find a trade off between optimality and feasibility. The work is to conduct a set of real-life experiments and simulations by mobile robots to identify failure cases like falling, unsteady, damaged items due to robot arm forces, instability or space. The aim is to learn from these simulations, introduction of machine learning, and to derive a set of rules to integrate in the bin packing optimization algorithms as strategies to build robust packing solution for autonomous mobile robots.",robotics
10.1016/j.procs.2020.04.220,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Design and Fabrication of SHRALA: Social Humanoid Robot Based on Autonomous Learning Algorithm,https://api.elsevier.com/content/abstract/scopus_id/85086626621,"This paper presents the preliminary research work in the Design, Fabrication of a Social Humanoid Robot based on Autonomous Learning Algorithm (SHRALA). Virtual Model of the humanoid robot was developed using Solidworks environment. This model is then fabricated using Creality Ender-3 3D printer. The electronic control circuit was designed and interfaced to computer using ATMEGA 2650 controller board, based on 8-bit AVR microcontroller. In order to easily and efficiently control the SHRALA a Graphical User Interface (GUI) was created using Unity3D editor, where a simple USB joystick was used to actuate the motions of the SHRALA in the virtual environment. The fabricated SHRALA was controlled in real time using a serial communication interface created between the GUI and Arduino Mega 2650 board. The humanoid robot was successfully controlled using the GUI environment and the preliminary results are satisfactory as it is performing the task as per the desired instructions. This research work is a part of the real time humanoid robot development project “SHRALA”, In near future autonomous learning algorithm will also be implemented in the robot and the same will be published as research article in a modular approach.",robotics
10.1016/j.compag.2019.105108,Journal,Computers and Electronics in Agriculture,scopus,2020-01-01,sciencedirect,Fast implementation of real-time fruit detection in apple orchards using deep learning,https://api.elsevier.com/content/abstract/scopus_id/85076234578,"To perform robust and efficient fruit detection in orchards is challenging since there are a number of variances in the working environments. Recently, deep-learning have shown a promising performance in many visual-guided agriculture applications. However, deep-learning based approaches requires labelling on training data, which is a labour-intensive and time-consuming task. In this study, a fast implementation framework of a deep-learning based fruit detector for apple harvesting is developed. The developed framework comprises an auto label generation module and a deep-learning-based fruit detector ‘LedNet’. The Label Generation algorithm utilises the multi-scale pyramid and clustering classifier to assist fast labelling of training data. LedNet adopts feature pyramid network and atrous spatial pyramid pooling to improve the detection performance of the model. A light-weight backbone is also developed and utilised to improve computational efficiency. From the experimental results, LedNet achieves 0.821 and 0.853 on recall and accuracy on apple detection in orchards, and its weights size and inference time are 7.4 M and 28 ms, respectively. The experimental results show that LedNet can perform real-time apple detection in orchards robustly and efficiently.",robotics
10.1016/j.asoc.2019.105929,Journal,Applied Soft Computing Journal,scopus,2020-01-01,sciencedirect,Multi robot distance based formation using Parallel Genetic Algorithm,https://api.elsevier.com/content/abstract/scopus_id/85075464885,"In this paper an alternative method to achieve distance based formation is presented. The method uses Genetic Algorithms to find a suitable solution based on angle and distance, and an appropriate constant velocity to avoid collisions. The designed algorithm is extended to a parallel scheme to improve its performance and achieve Artificial Distributed Intelligence, in which the robots share, through solution migration, the best ways to converge to desired distances while avoiding collisions, finally reaching consensus on the solution. The algorithm is tested using simulations and real robots experiments.",robotics
10.1016/j.ins.2019.09.005,Journal,Information Sciences,scopus,2020-01-01,sciencedirect,Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot interaction,https://api.elsevier.com/content/abstract/scopus_id/85071968961,"The two-layer fuzzy multiple random forest (TLFMRF) is proposed for speech emotion recognition. When recognizing speech emotion, there are usually some problems. One is that feature extraction relies on personalized features. The other is that emotion recognition doesn’t consider the differences among different categories of people. In the proposal, personalized and non-personalized features are fused for speech emotion recognition. High dimensional emotional features are divided into different subclasses by adopting the fuzzy C-means clustering algorithm, and multiple random forest is used to recognize different emotional states. Finally, a TLFMRF is established. Moreover, a separate classification of certain emotions which are difficult to recognize to some extent is conducted. The results show that the TLFMRF can identify emotions in a stable manner. To demonstrate the effectiveness of the proposal, experiments on CASIA corpus and Berlin EmoDB are conducted. Experimental results show the recognition accuracies of the proposal are 1.39%–7.64% and 4.06%–4.30% higher than that of back propagation neural network and random forest respectively. Meanwhile, preliminary application experiments are also conducted to investigate the emotional social robot system, and application results indicate that mobile robot can real-time track six basic emotions, including angry, fear, happy, neutral, sad, and surprise.",robotics
10.1016/j.eswa.2019.06.066,Journal,Expert Systems with Applications,scopus,2019-12-15,sciencedirect,Double Q-PID algorithm for mobile robot control,https://api.elsevier.com/content/abstract/scopus_id/85068505390,"Many expert systems have been developed for self-adaptive PID controllers of mobile robots. However, the high computational requirements of the expert systems layers, developed for the tuning of the PID controllers, still require previous expert knowledge and high efficiency in algorithmic and software execution for real-time applications. To address these problems, in this paper we propose an expert agent-based system, based on a reinforcement learning agent, for self-adapting multiple low-level PID controllers in mobile robots. For the formulation of the artificial expert agent, we develop an incremental model-free algorithm version of the double Q-Learning algorithm for fast on-line adaptation of multiple low-level PID controllers. Fast learning and high on-line adaptability of the artificial expert agent is achieved by means of a proposed incremental active-learning exploration-exploitation procedure, for a non-uniform state space exploration, along with an experience replay mechanism for multiple value functions updates in the double Q-learning algorithm. A comprehensive comparative simulation study and experiments in a real mobile robot demonstrate the high performance of the proposed algorithm for a real-time simultaneous tuning of multiple adaptive low-level PID controllers of mobile robots in real world conditions.",robotics
10.1016/j.asoc.2019.105812,Journal,Applied Soft Computing Journal,scopus,2019-12-01,sciencedirect,A new feature selection method based on task environments for controlling robots,https://api.elsevier.com/content/abstract/scopus_id/85072837166,"This paper proposes a task-based feature selection as a part of the Supervised Deep Learning (SDL) framework. The main challenge of control tasks is choosing appropriate inputs for a controller, especially when a camera prepares the sensory data. Since the captured images have high dimensions, it is important to extract proper task-based features to reduce the dimension of controller input while preserving its performance. In this paper, deep learning is utilized to do this and the appropriate features representing the depth images are acquired. These features besides some expert and memory-based features form a candidate feature set. A feature selection method based on state transition probability is proposed. This method exploits the clustering technique and genetic algorithm. It chooses a subset of candidate feature set to maximize the certainty of transition from the current state of the task environment to the next one. The suggested method named Transition Certainty based Feature Selection (TCFS) is examined for a wheeled robot navigation task in uneven terrains with many obstacles. Wall following and obstacle avoidance tasks are considered as robot missions. Depth images captured by a vision system (Kinect camera) are the only sensory data of the robot controller. The proposed architecture is evaluated in WEBOTS© simulator and on a real robot. The experiments demonstrate that TCFS improves robot performance in comparison to SDL architecture according to defined indexes.",robotics
10.1016/j.compeleceng.2019.106465,Journal,Computers and Electrical Engineering,scopus,2019-12-01,sciencedirect,A novel backstepping adaptive impedance control for an upper limb rehabilitation robot,https://api.elsevier.com/content/abstract/scopus_id/85072382915,"Stroke contributes to hemiplegia, which severely reduces people's ability to perform activities of daily living. Due to the insufficiency of medical resources, there is an urgent need for home-based rehabilitation robot. In this paper, we design a home-based upper limb rehabilitation robot, based on the principle that three axes intersect at one point. A three-dimensional force sensor is equipped at the end of the manipulator to measure the interaction forces between the affected upper limb and the robot during rehabilitation training. The virtual rehabilitation training environment is designed to improve the enthusiasm of patients. A backstepping adaptive fuzzy based impedance control method is proposed for the home-based upper limb rehabilitation robot to prevent secondary injury of the affected limb. The adaptive law is introduced, and the backstepping adaptive fuzzy based impedance controller is proved in details. Experiments results demonstrate the effectiveness of the proposed control method.",robotics
10.1016/j.rcim.2019.05.008,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2019-12-01,sciencedirect,A real-time human-robot interaction framework with robust background invariant hand gesture detection,https://api.elsevier.com/content/abstract/scopus_id/85066259834,"In the light of factories of the future, to ensure productive and safe interaction between robot and human coworkers, it is imperative that the robot extracts the essential information of the coworker. We address this by designing a reliable framework for real-time safe human-robot collaboration, using static hand gestures and 3D skeleton extraction. OpenPose library is integrated with Microsoft Kinect V2, to obtain a 3D estimation of the human skeleton. With the help of 10 volunteers, we recorded an image dataset of alpha-numeric static hand gestures, taken from the American Sign Language. We named our dataset OpenSign and released it to the community for benchmarking. Inception V3 convolutional neural network is adapted and trained to detect the hand gestures. To augment the data for training the hand gesture detector, we use OpenPose to localize the hands in the dataset images and segment the backgrounds of hand images, by exploiting the Kinect V2 depth map. Then, the backgrounds are substituted with random patterns and indoor architecture templates. Fine-tuning of Inception V3 is performed in three phases, to achieve validation accuracy of 99.1% and test accuracy of 98.9%. An asynchronous integration of image acquisition and hand gesture detection is performed to ensure real-time detection of hand gestures. Finally, the proposed framework is integrated in our physical human-robot interaction library OpenPHRI. This integration complements OpenPHRI by providing successful implementation of the ISO/TS 15066 safety standards for “safety rated monitored stop” and “speed and separation monitoring” collaborative modes. We validate the performance of the proposed framework through a complete teaching by demonstration experiment with a robotic manipulator.",robotics
10.1016/j.patrec.2018.04.009,Journal,Pattern Recognition Letters,scopus,2019-12-01,sciencedirect,A real-time and unsupervised face re-identification system for human-robot interaction,https://api.elsevier.com/content/abstract/scopus_id/85046146958,"In the context of Human-Robot Interaction (HRI), face Re-Identification (face Re-ID) aims to verify if certain detected faces have already been observed by robots. The ability of distinguishing between different users is crucial in social robots as it will enable the robot to tailor the interaction strategy toward the users’ individual preferences. So far face recognition research has achieved great success, however little attention has been paid to the realistic applications of Face Re-ID in social robots. In this paper, we present an effective and unsupervised face Re-ID system which simultaneously re-identifies multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural Networks to extract features, and an online clustering algorithm to determine the face's ID. Its performance is evaluated on two datasets: the TERESA video dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF Dataset). We demonstrate that the optimised combination of techniques achieves an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on YTF dataset. We have implemented the proposed method into a software module in the HCI^2 Framework [1] for it to be further integrated into the TERESA robot [2], and has achieved real-time performance at 10–26 Frames per second.",robotics
10.1016/j.eswa.2019.05.035,Journal,Expert Systems with Applications,scopus,2019-11-15,sciencedirect,Social mimic optimization algorithm and engineering applications,https://api.elsevier.com/content/abstract/scopus_id/85066806914,"Increase in complexity of real world problems has provided an area to explore efficient methods to solve computer science problems. Meta-heuristic methods based on evolutionary computations and swarm intelligence are instances of techniques inspired by nature. This paper presents a novel social mimic optimization (SMO) algorithm inspired by mimicking behavior to solve optimization problems. The proposed algorithm is evaluated using 23 test functions. Obtained results are compared with 14 known optimization algorithms including Whale optimization algorithm (WOA), Grasshopper optimization algorithm (GOA), Particle Swarm Optimization (PSO), Stochastic fractal search (SFS), Grey Wolf Optimizer (GWO), Optics Inspired Optimization (OIO), League Championship Algorithm (LCA), Wind Driven Optimization (WDO), Harmony search (HS), Firefly Algorithm (FA), Artificial Bee Colony (ABC), Biogeography Based Optimization (BBO), Bat Algorithm (BA), and Teaching Learning Based Optimization (TLBO). Obtained results indicate higher capability of the SMO algorithm in solving high-dimensional decision variables. Furthermore, SMO is used to solve two classic engineering design problems. Three important features of SMO are simple implementation, solving optimization problems with minimum population size and not requiring control parameters. Results of various evaluations show superiority of the proposed method in finding the optimal solution with minimum function evaluations. This superiority is achieved based on reducing number of initial population. The proposed method can be applied to applications like automatic evolution of robotics, automatic control of machines and innovation of machines in finding better solutions with less cost.",robotics
10.1016/j.neucom.2019.07.044,Journal,Neurocomputing,scopus,2019-10-21,sciencedirect,Design and analysis of new complex zeroing neural network for a set of dynamic complex linear equations,https://api.elsevier.com/content/abstract/scopus_id/85069679375,"This paper proposes a new complex zeroing neural network (NCZNN) to solve a set of dynamic complex linear equations, which is an extension from the design idea of the real-valued zeroing neural network. Different from the previous complex ZNN (CZNN) model, which cannot process nonlinear activation functions and thus only converge in infinite time, a nonlinear sign-bi-power (SBP) activation function is explored to enable the proposed NCZNN model to converge within finite time in complex domain by using two different ways. One is to simultaneously activate the real part and the imaginary part of a complex number and the other is to activate the modulus of a complex number. In addition, the detailed theoretical analyses of the NCZNN model are provided according to these two processing ways, and the corresponding convergence upper bounds are analytically calculated. Two numerical experiments are conducted by using the NCZNN model and the CZNN model to solve a set of dynamic complex linear equations. Comparative results further prove that the NCZNN model has better convergence performance than the CZNN model. At last, the proposed method is applied to the motion tracking of a mobile manipulator, and simulative results verify the feasibility of our method in robotic applications.",robotics
10.1016/j.eswa.2019.05.001,Journal,Expert Systems with Applications,scopus,2019-10-15,sciencedirect,Canadian Traveler Problem with Neutralizations,https://api.elsevier.com/content/abstract/scopus_id/85065488474,"The Canadian Traveler Problem (CTP) and the Obstacle Neutralization Problem (ONP) are two well-studied graph-theoretic path planning problems in the literature and both problems have been shown to be computationally intractable. In CTP, certain edges in a graph are blocked by a known probability and their status is revealed only when the traversing agent is at either end of these edges using the agent’s limited disambiguation capability. The goal is to minimize the expected length of the traversal between a starting and a termination vertex by devising a policy that dictates in real-time which edge to disambiguate. In ONP, an agent needs to safely and swiftly navigate from a given source location to a destination through an arrangement of obstacles in the plane. The agent has a limited neutralization capability and uses it to safely pass through an obstacle at a cost of increased traversal length. The agent’s goal is to find the sequence of obstacles to be neutralized en route which minimizes the overall traversal length subject to the agent’s limited neutralization capability. Both of these problems have important and practical applications within the context of expert and intelligent systems. These include: autonomous robot navigation, adaptive transportation systems, naval and land minefield countermeasures, and navigation inside disaster areas for emergency relief operations. In this study, we consider a new path planning problem in the simultaneous presence of disambiguation and neutralization capabilities. This appears to be the first of its kind in the literature despite the close and inherent relationship between CTP and ONP. We call this problem the Canadian Traveler Problem with Neutralizations (CTPN). We present a Markov decision process formulation of CTPN and propose an optimal algorithm. This is based on an extension of the well-known AO* search algorithm. We provide computational experiments on Delaunay graphs to assess the relative performance of this algorithm in comparison to the well-known value iteration and AO* algorithms. We then investigate the relative utility and importance of the disambiguation and neutralization capabilities in order to assist decision-makers with financial constraints as well as navigation performance decisions.",robotics
10.1016/j.compag.2019.104973,Journal,Computers and Electronics in Agriculture,scopus,2019-10-01,sciencedirect,Deep learning-based visual recognition of rumex for robotic precision farming,https://api.elsevier.com/content/abstract/scopus_id/85071398904,"In this paper we address the problem of recognising the Broad-leaved dock (Rumex obtusifolius L.) in grasslands from high-resolution 2D images. We discuss and present the determining factors for developing and implementing weed visual recognition algorithms using deep learning. This analysis, leads to the formulation of the proposed algorithm. Our implementation exploits Transfer Learning techniques for deep learning-based feature extraction, in combination with a classifier for weed recognition. A prototype robotic platform has been used to make available an image dataset from a dairy farm containing broad-leaved docks. The evaluation of the proposed algorithm on this dataset shows that it outperforms competing weed/plant recognition methods in recognition accuracy, while producing low false-positive rates under real-world operation conditions.",robotics
10.1016/j.asoc.2019.105628,Journal,Applied Soft Computing Journal,scopus,2019-10-01,sciencedirect,A real-time decentralized algorithm for task scheduling in multi-agent system with continuous damage,https://api.elsevier.com/content/abstract/scopus_id/85069715361,"In this paper, a common model of task scheduling problems in agent rescue scenario is proposed, in which tasks with continuous dynamic damage are introduced to capture the emerging applications of using rescue robots and other resources to enhance human disaster rescue capability. Beyond this, we mainly focus on finding the optimal task scheduling strategy. We design a heuristic algorithm based on greedy strategy to obtain the optimal dynamic scheduling strategy of agents. Compared with solving global integer programming directly, the computational time is greatly reduced. The proof of the greedy strategy’s validity is also demonstrated under some specific damage functions. By comparing with the two strategies commonly used in real life, it is proved that our strategy is optimal. For practical application, we design an automatic negotiation framework, which realizes the real-time decentralized automated negotiation of agents. Then, using Game Description Language (GDL) as a tool, an automated negotiation algorithm is implemented, which enables agents to adjust the plan dispersedly. Experiments show that the algorithm is more efficient than the centralized algorithm in the case of limited communication.",robotics
10.1016/j.neucom.2019.05.062,Journal,Neurocomputing,scopus,2019-09-24,sciencedirect,Multi-agent behavioral control system using deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85066428189,"Deep reinforcement learning (DRL) has emerged as the dominant approach to achieving successive advancements in the creation of human-wise agents. By leveraging neural networks as decision-making controllers, DRL supplements traditional reinforcement methods to address the curse of dimensionality in complicated tasks. However, agents in complicated environments are likely to get stuck in sub-optimal solutions. In such cases, the agent inadvertently turns into a “zombie” owing to its short-term vision and harmful behaviors. In this study, we use human learning strategies to adjust agent behaviors in high-dimensional environments. Therefore, the agent behaves predictably and succeeds in attaining its designated goal. In summary, the contribution of this study is two-fold. First, we introduce a lightweight workflow that enables a nonexpert to preserve a certain level of safety in AI systems. Specifically, the workflow involves a novel concept of a target map and a multi-agent behavioral control system named Multi-Policy Control System (MPCS). MPCS successfully controls agent behaviors in real time without involving the burden of human feedback. Second, we develop a multi-agent game named Tank Battle that provides a configurable environment to examine agent behaviors and human-agent interactions in DRL. Finally, simulation results show that agents guided by MPCS outperform agents that do not use MPCS with respect to the mean of total rewards and human-like behaviors in complicated environments such as Seaquest and Tank Battle.",robotics
10.1016/j.neucom.2019.05.064,Journal,Neurocomputing,scopus,2019-09-17,sciencedirect,Improving novelty detection with generative adversarial networks on hand gesture data,https://api.elsevier.com/content/abstract/scopus_id/85066314268,"We propose a novel way of solving the issue of classification of out-of-vocabulary gestures using Artificial Neural Networks (ANNs) trained in the Generative Adversarial Network (GAN) framework. A generative model augments the data set in an online fashion with new samples and stochastic target vectors, while a discriminative model determines the class of the samples. The approach was evaluated on the UC2017 SG and UC2018 DualMyo data sets. The generative models’ performance was measured with a distance metric between generated and real samples. The discriminative models were evaluated by their accuracy on trained and novel classes. In terms of sample generation quality, the GAN is significantly better than a random distribution (noise) in mean distance, for all classes. In the classification tests, the baseline neural network was not capable of identifying untrained gestures. When the proposed methodology was implemented, we found that there is a trade-off between the detection of trained and untrained gestures, with some trained samples being mistaken as novelty. Nevertheless, a novelty detection accuracy of 95.4% or 90.2% (depending on the data set) was achieved with just 5% loss of accuracy on trained classes.",robotics
10.1016/j.compag.2019.104890,Journal,Computers and Electronics in Agriculture,scopus,2019-09-01,sciencedirect,Spectral filter design based on in-field hyperspectral imaging and machine learning for mango ripeness estimation,https://api.elsevier.com/content/abstract/scopus_id/85069712788,"Hyperspectral imaging (HSI) is a powerful technology already used for many objectives in agriculture. Applications include disease monitoring, plant phenotyping, yield estimation or fruit composition and ripeness. However, the cost of hyperspectral sensors is typically an order of magnitude higher than simpler RGB cameras, which can be prohibitive. Given that in HSI processing the spectral data often contains redundancies, the full spectra are not always required for a specific application and there is an opportunity to design a lower cost multi-spectral sensing system by dimensionality reduction. In past work, HSI dimensionality reduction has been applied in the form of band selection to achieve faster computation times. If, however, the objective is to design a lower cost multi-spectral camera system, band selection is poorly suited because real-world sensor and optical filter responses do not typically replicate the individual bands of a hyperspectral sensor. The objective of this paper is to develop a new methodology for filter selection by simulating several imaging devices with different real-world optical filters, to use a high cost HSI device to design a lower cost multi-spectral solution for a specific application. In this paper, we apply the technique to the specific task of mango fruit maturity estimation (dry matter), which was recently shown to be possible using HSI. Mango HSI acquired under field conditions from an UGV was used as input for the experiments. These involved the simulation of imaging devices, using support vector machines for modelling, and testing several filter combinations by brute force or optimisation with genetic algorithms. The mango prediction performance of the simulations was compared to the best performance obtained with full HSI data, which had an R2 of 0.74. The best values came from the simulation of a four-sensor device with four distinct filters, achieving R2 up to 0.69 for mango dry matter estimation. The results showed that genetic algorithms, when compared to brute force approaches, were able to obtain the best solution in an efficient way, and that a good performance for mango ripeness estimation can be achieved from the combination of four spectral filters that would allow to implement them into a low-cost, custom-made multi-spectral sensor. The methods exposed in this paper are more broadly applicable to applications beyond mango maturity estimation.",robotics
10.1016/j.robot.2019.05.005,Journal,Robotics and Autonomous Systems,scopus,2019-09-01,sciencedirect,Context-based affordance segmentation from 2D images for robot actions,https://api.elsevier.com/content/abstract/scopus_id/85068232177,"Affordances play a crucial role in robotics since they allow developing truly autonomous robots, which can freely explore and interact with the environment. Most of the existing approaches for analyzing affordances in a scene consider only one or few types of affordance, e.g., grasping points, object manipulation or locomotion. In many cases only whole objects are considered. In our study we include in total 12 affordances of object-related, manipulation and locomotion affordances, considering affordances of both objects and/or their parts. We design a system that can densely predict affordances given only a single 2D RGB image. For this, we propose a method that transfers object class labels to affordances. This enables us to train convolutional neural networks, a PSPNet-based network and a U-Net-style network, to directly predict affordances from an image using a selective binary cross entropy loss function. The method is able to handle (potentially multiple) affordances of objects and their parts in a pixel-wise manner even in the case of incomplete data. We perform qualitative as well as quantitative evaluations with simulated and real data including robot experiments. In general, we find that frequent affordances are recognized with a substantial fraction of correctly assigned pixels, while this is harder for infrequent affordances and small objects. In addition, we demonstrate that our method performs better than a recent competitive approach. As the proposed method operates on 2D images, it is easier to implement than competing 3D methods and it could therefore more easily provide useful affordance estimates for robotic actions as demonstrated experimentally.",robotics
10.1016/j.neucom.2019.04.037,Journal,Neurocomputing,scopus,2019-08-04,sciencedirect,LEGION-based image segmentation by means of spiking neural networks using normalized synaptic weights implemented on a compact scalable neuromorphic architecture,https://api.elsevier.com/content/abstract/scopus_id/85065418438,"LEGION (Locally Excitatory, Globally Inhibitory Oscillator Network) topology has demonstrated good capabilities in scene segmentation applications. However, the implementation of LEGION algorithm requires machines with high performance to process a set of complex differential equations limiting its use in practical real-time applications. Recently, several authors have proposed alternative methods based on spiking neural networks (SNN) to create oscillatory neural networks with low computational complexity and highly feasible to be implemented on digital hardware to perform adaptive segmentation of images. Nevertheless, existing SNN with LEGION configuration focus on the membrane model leaving aside the behavior of the synapses although they play an important role in the synchronization of several segments by self-adapting their weights. In this work, we propose a SNN-LEGION configuration along with normalized weight of the synapses to self-adapt the SNN network to synchronize several segments of any size and shape at the same time. The proposed SNN-LEGION method involves a global inhibitor, which is in charge of performing the segmentation process between different objects with different sizes and shapes on time. To validate the proposal, the SNN-LEGION method is implemented on an optimized scalable neuromorphic architecture. Our preliminary results demonstrate that the proposed normalization process of the synaptic weights along with the SNN-LEGION configuration keep the capacity of the LEGION network to separate the segments on time, which can be useful in video processing applications such as vision processing systems for mobile robots, offering lower computational complexity and area consumption compared with previously reported solutions.",robotics
10.1016/j.compind.2019.05.001,Journal,Computers in Industry,scopus,2019-08-01,sciencedirect,Industrial robot control and operator training using virtual reality interfaces,https://api.elsevier.com/content/abstract/scopus_id/85065132267,"Nowadays, we are involved in the fourth industrial revolution, commonly referred to as “Industry 4.0,” where cyber-physical systems and intelligent automation, including robotics, are the keys. Traditionally, the use of robots has been limited by safety and, in addition, some manufacturing tasks are too complex to be fully automated. Thus, human-robot collaborative applications, where robots are not isolated, are necessary in order to increase the productivity ensuring the safety of the operators with new perception systems for the robot and new interaction interfaces for the human. Moreover, virtual reality has been extended to the industry in the last years, but most of its applications are not related to robots. In this context, this paper works on the synergies between virtual reality and robotics, presenting the use of commercial gaming technologies to create a totally immersive environment based on virtual reality. This environment includes an interface connected to the robot controller, where the necessary mathematical models have been implemented for the control of the virtual robot. The proposed system can be used for training, simulation, and what is more innovative, for robot controlling in an integrated, non-expensive and unique application. Results show that the immersive experience increments the efficiency of the training and simulation processes, offering a cost-effective solution.",robotics
10.1016/j.rcim.2019.01.013,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2019-08-01,sciencedirect,A selective muscle fatigue management approach to ergonomic human-robot co-manipulation,https://api.elsevier.com/content/abstract/scopus_id/85061633513,"In this paper, we propose a method for selective monitoring and management of human muscle fatigue in human-robot co-manipulation scenarios. The proposed approach uses a machine learning technique to learn the complex relationship between individual human muscle forces, arm configuration and arm endpoint force that are provided by a sophisticated offline musculoskeletal model. The estimated muscle forces are used in the fatigue model to estimate the individual muscle fatigue levels online. Two fatigue management protocols are proposed that enable the robot to handle and reduce the human fatigue by altering the configuration of task execution. The first protocol uses optimisation technique to find the optimal position for task execution, where the fatigue-related endurance time can be maximised. The second protocol divides the arm muscles into groups and then alters the direction of endpoint force so that the fatigued muscle group can relax and the relaxed muscle group becomes active. The proposed method has a potential to enable the robot to facilitate safer and more ergonomic working conditions for the human coworker. The main advantage of this approach is that it can operate online, and that all the measurements can be performed by the robot sensory system, which can significantly increase the applicability in real world scenarios. To validate the proposed method, we performed multiple experiments with two collaborative tasks (polishing and drilling) under different conditions.",robotics
10.1016/j.jcin.2019.04.048,Journal,JACC: Cardiovascular Interventions,scopus,2019-07-22,sciencedirect,Impact of Artificial Intelligence on Interventional Cardiology: From Decision-Making Aid to Advanced Interventional Procedure Assistance,https://api.elsevier.com/content/abstract/scopus_id/85068485437,"Access to big data analyzed by supercomputers using advanced mathematical algorithms (i.e., deep machine learning) has allowed for enhancement of cognitive output (i.e., visual imaging interpretation) to previously unseen levels and promises to fundamentally change the practice of medicine. This field, known as “artificial intelligence” (AI), is making significant progress in areas such as automated clinical decision making, medical imaging analysis, and interventional procedures, and has the potential to dramatically influence the practice of interventional cardiology. The unique nature of interventional cardiology makes it an ideal target for the development of AI-based technologies designed to improve real-time clinical decision making, streamline workflow in the catheterization laboratory, and standardize catheter-based procedures through advanced robotics. This review provides an introduction to AI by highlighting its scope, potential applications, and limitations in interventional cardiology.",robotics
10.1016/j.robot.2019.03.012,Journal,Robotics and Autonomous Systems,scopus,2019-07-01,sciencedirect,Dynamic-SLAM: Semantic monocular visual localization and mapping based on deep learning in dynamic environment,https://api.elsevier.com/content/abstract/scopus_id/85064149772,"When working in dynamic environment, traditional SLAM framework performs poorly due to interference from dynamic objects. By taking advantages of deep learning in object detection, a semantic simultaneous localization and mapping framework named Dynamic-SLAM is proposed, in order to solve the problem of SLAM in dynamic environment. First, based on the convolutional neural network, an SSD object detector which combines prior knowledge is constructed to detect dynamic objects in the newly detection thread at semantic level. Then, in view of low recall rate of the existing SSD object detection network, a missed detection compensation algorithm based on the speed invariance in adjacent frames is proposed, which greatly improves the recall rate of detection. Finally, a feature-based visual SLAM system is constructed, which processes the feature points of dynamic objects through a selective tracking algorithm in the tracking thread, to significantly reduce the error of pose estimation caused by incorrect matching. The recall rate of the system is increased from 82.3% to 99.8% compared with the original SSD network. Several experiments show that the localization accuracy of Dynamic-SLAM is higher than the state-of-the-art systems. The system successfully localizes and constructs an accurate environmental map in real-world dynamic environment by using a mobile robot. In sum, our experimental demonstrations verify that Dynamic-SLAM shows improved accuracy and robustness in robot localization and mapping comparing to the state-of-the-art SLAM system in dynamic environment.",robotics
10.1016/j.neucom.2019.01.087,Journal,Neurocomputing,scopus,2019-06-14,sciencedirect,Robot skill acquisition in assembly process using deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85063108149,"Uncertain factors in environments restrict the intelligence level of industrial robots. Based on deep reinforcement learning, a skill-acquisition method is used to solve the posed problems of uncertainty in a complex assembly process. Under the frame of the Markov decision process, a quaternion sequence of the assembly process is represented. The reward function uses a trained classification model, which mainly recognizes whether the assembly is successful. The proposed skill-acquisition method is designed to make robots acquire assembly skills. The input of the model is the contact state of the assembly process, and the output is the robot action. The robot can complete the assembly by self-learning with little prior knowledge. To evaluate the performance of the proposed skill-acquisition method, simulations and real-world experiments were performed in a low-voltage apparatus assembly. The assembly success rate increases with the learning time. In the case of a random initial position and orientation, the assembly success rate was greater than 80% with little prior knowledge. The results show that the robot has a capability to complex assembly through skill acquisition.",robotics
10.1016/j.rcim.2018.12.014,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2019-06-01,sciencedirect,Novel decoupling algorithm based on parallel voltage extreme learning machine (PV-ELM) for six-axis F/M sensors,https://api.elsevier.com/content/abstract/scopus_id/85059099701,"Accurate, time-effective calibration and decoupling procedures of multi-axis Force/Moment (F/M) sensors are critical to the implementation of these sensors. Recently, many decoupling methods have been proposed by researchers, but the inherent coupling relationship among components of multi-axis F/M sensors has not been taken into account. In this paper, we thus proposed a novel Sparse Voltage Maximum Inter-class Variance (SVMIV) algorithm, which took advantage of the inherent relationship nature. Furthermore, a novel nonlinear decoupling method based on the Parallel Voltage-Extreme Learning Machine (PV-ELM) was also presented. To demonstrate the utility of the proposed approach, extensive comparisons were made between the proposed PV-ELM decoupling method and several conventional decoupling approaches such as Least Square (LS), Support Vector Regression (SVR), Back Propagation Neural Network (BPNN), and Extreme Learning Machine (ELM). Results of real decoupling experiments demonstrated that the proposed the PV-ELM decoupling algorithm outperforms linear decoupling algorithms for six-axis F/M sensors. In addition, it was also proved that the PV-ELM decoupling method can decouple the outputs of six-axis F/M sensors with higher precision, faster speed, improved robustness, and faster convergence than the state-of-the-art nonlinear decoupling algorithms such as SVR, BP and ELM. Overall, this paper proposed a novel way to deal with the inherent coupling relationship of six-axis F/M sensors, and the experimental results demonstrated that the maximum I-type and II-type errors were below 0.356% and 0.270%, respectively, of full scale in all measured variables.",robotics
10.1016/j.amsu.2019.04.001,Journal,Annals of Medicine and Surgery,scopus,2019-05-01,sciencedirect,"Artificial intelligence, regenerative surgery, robotics? What is realistic for the future of surgery?",https://api.elsevier.com/content/abstract/scopus_id/85064430299,"The potential of surgery lies in the technological advances that would complement it. The landscape of the field will differ depending on the time period being looked at and would no doubt include conjecture. Initial breakthroughs will need to pave the way for future medical technology and apply to the surgical sciences. Within the next 10 years we would expect to see the emergence of big data analysis, cuttingedge image processing techniques for surgical planning and better implementation of virtual and augmented reality in operating theatres for both patient care and teaching purposes. Over the next 50 to 100 years, the use of quantum computing should lead to increased automation in our healthcare systems. The inception of novel biomaterial invention and advanced genetic engineering will usher in the new age of regenerative medicine in the clinical setting. The future of surgery includes many predictions and promises, but it is apparent that the development will lead to bettering outcome and focus on patient care.",robotics
10.1016/j.robot.2018.11.017,Journal,Robotics and Autonomous Systems,scopus,2019-05-01,sciencedirect,A real-time framework for kinodynamic planning in dynamic environments with application to quadrotor obstacle avoidance,https://api.elsevier.com/content/abstract/scopus_id/85062619374,"The objective of this paper is to present a full-stack, real-time motion planning framework for kinodynamic robots and then show how it is applied and demonstrated on a physical quadrotor system operating in a laboratory environment. The proposed framework utilizes an offline–online computation paradigm, neighborhood classification through machine learning, sampling-based motion planning with an optimal cost distance metric, and trajectory smoothing to achieve real-time planning for aerial vehicles. This framework accounts for dynamic obstacles with an event-based replanning structure and a locally reactive control layer that minimizes replanning events. The approach is demonstrated on a quadrotor navigating moving obstacles in an indoor space and stands as, arguably, one of the first demonstrations of full-online kinodynamic motion planning, with execution cycles of 3 Hz to 5 Hz. For the quadrotor, a simplified dynamics model is used during the planning phase to accelerate online computation. A trajectory smoothing phase, which leverages the differentially flat nature of quadrotor dynamics, is then implemented to guarantee a dynamically feasible trajectory.",robotics
10.1016/j.robot.2019.02.013,Journal,Robotics and Autonomous Systems,scopus,2019-05-01,sciencedirect,Solving the optimal path planning of a mobile robot using improved Q-learning,https://api.elsevier.com/content/abstract/scopus_id/85062269308,"Q-learning, a type of reinforcement learning, has gained increasing popularity in autonomous mobile robot path planning recently, due to its self-learning ability without requiring a priori model of the environment. Yet, despite such advantage, Q-learning exhibits slow convergence to the optimal solution. In order to address this limitation, the concept of partially guided Q-learning is introduced wherein, the flower pollination algorithm (FPA) is utilized to improve the initialization of Q-learning. Experimental evaluation of the proposed improved Q-learning under the challenging environment with a different layout of obstacles shows that the convergence of Q-learning can be accelerated when Q-values are initialized appropriately using the FPA. Additionally, the effectiveness of the proposed algorithm is validated in a real-world experiment using a three-wheeled mobile robot.",robotics
10.1016/j.cogsys.2018.10.028,Journal,Cognitive Systems Research,scopus,2019-05-01,sciencedirect,Bioinspired decision-making for a socially interactive robot,https://api.elsevier.com/content/abstract/scopus_id/85057001839,"Nowadays, robots and humans coexist in real settings where robots need to interact autonomously making their own decisions. Many applications require that robots adapt their behavior to different users and remember each user’s preferences to engage them in the interaction. To this end, we propose a decision making system for social robots that drives their actions taking into account the user and the robot’s state. This system is based on bio-inspired concepts, such as motivations, drives and wellbeing, that facilitate the rise of natural behaviors to ease the acceptance of the robot by the users. The system has been designed to promote the human-robot interaction by using drives and motivations related with social aspects, such as the users’ satisfaction or the need of social interaction. Furthermore, the changes of state produced by the users’ exogenous actions have been modeled as transitional states that are considered when the next robot’s action has to be selected. Our system has been evaluated considering two different user profiles. In the proposed system, user’s preferences are considered and alter the homeostatic process that controls the decision making system. As a result, using reinforcement learning algorithms and considering the robot’s wellbeing as the reward function, the social robot Mini has learned from scratch two different policies of action, one for each user, that fit the users’ preferences. The robot learned behaviors that maximize its wellbeing as well as keep the users engaged in the interactions.",robotics
10.1016/j.robot.2018.11.018,Journal,Robotics and Autonomous Systems,scopus,2019-04-01,sciencedirect,"Cognition-enabled robotic wiping: Representation, planning, execution, and interpretation",https://api.elsevier.com/content/abstract/scopus_id/85061748854,"Advanced cognitive capabilities enable humans to solve even complex tasks by representing and processing internal models of manipulation actions and their effects. Consequently, humans are able to plan the effect of their motions before execution and validate the performance afterwards. In this work, we derive an analog approach for robotic wiping actions which are fundamental for some of the most frequent household chores including vacuuming the floor, sweeping dust, and cleaning windows. We describe wiping actions and their effects based on a qualitative particle distribution model. This representation enables a robot to plan goal-oriented wiping motions for the prototypical wiping actions of absorbing, collecting and skimming. The particle representation is utilized to simulate the task outcome before execution and infer the real performance afterwards based on haptic perception. This way, the robot is able to estimate the task performance and schedule additional motions if necessary. We evaluate our methods in simulated scenarios, as well as in real experiments with the humanoid service robot Rollin’ Justin.",robotics
10.1016/j.encep.2018.08.002,Journal,Encephale,scopus,2019-04-01,sciencedirect,Toward a motor signature in autism: Studies from human-machine interaction,https://api.elsevier.com/content/abstract/scopus_id/85057386595,"Background
                  Autism spectrum disorder (ASD) is a heterogeneous group of neurodevelopmental disorders which core symptoms are impairments in socio-communication and repetitive symptoms and stereotypies. Although not cardinal symptoms per se, motor impairments are fundamental aspects of ASD. These impairments are associated with postural and motor control disabilities that we investigated using computational modeling and developmental robotics through human-machine interaction paradigms.
               
                  Method
                  First, in a set of studies involving a human–robot posture imitation, we explored the impact of 3 different groups of partners (including a group of children with ASD) on robot learning by imitation. Second, using an ecological task, i.e. a real-time motor imitation with a tightrope walker (TW) avatar, we investigated interpersonal synchronization, motor coordination and motor control during the task in children with ASD (n
                     =29), TD children (n
                     =39) and children with developmental coordination disorder (n
                     =17, DCD).
               
                  Results
                  From the human–robot experiments, we evidenced that motor signature at both groups’ and individuals’ levels had a key influence on imitation learning, posture recognition and identity recognition. From the more dynamic motor imitation paradigm with a TW avatar, we found that interpersonal synchronization, motor coordination and motor control were more impaired in children with ASD compared to both TD children and children with DCD. Taken together these results confirm the motor peculiarities of children with ASD despite imitation tasks were adequately performed.
               
                  Discussion
                  Studies from human-machine interaction support the idea of a behavioral signature in children with ASD. However, several issues need to be addressed. Is this behavioral signature motoric in essence? Is it possible to ascertain that these peculiarities occur during all motor tasks (e.g. posture, voluntary movement)? Could this motor signature be considered as specific to autism, notably in comparison to DCD that also display poor motor coordination skills? We suggest that more work comparing the two conditions should be implemented, including analysis of kinematics and movement smoothness with sufficient measurement quality to allow spectral analysis.",robotics
10.1016/j.jvcir.2019.01.005,Journal,Journal of Visual Communication and Image Representation,scopus,2019-02-01,sciencedirect,Object detection and localization in 3D environment by fusing raw fisheye image and attitude data,https://api.elsevier.com/content/abstract/scopus_id/85059748912,"In robotic systems, the fisheye camera can provide a large field of view (FOV). Usually, the traditional restoring algorithms are needed, which are computational heavy and will introduce noise into original data, since the fisheye images are distorted. In this paper, we propose a framework to detect objects from the raw fisheye images without restoration, then locate objects in the real world coordinate by fusing attitude information. A deep neural network architecture based on the MobileNet and feature pyramid structure is designed to detect targets directly on the fisheye raw images. Then, the target can be located based on the fisheye visual model and the attitude of the camera. Compared to traditional approaches, this approach has advantages in computational efficiency and accuracy. This approach is validated by experiments with a fisheye camera and an onboard computer on a micro-aerial vehicle (MAV).",robotics
10.1016/j.robot.2018.11.006,Journal,Robotics and Autonomous Systems,scopus,2019-02-01,sciencedirect,An improved scheme for eliminating the coupled motion of surgical instruments used in laparoscopic surgical robots,https://api.elsevier.com/content/abstract/scopus_id/85056947027,"Considering the nonlinear characteristics such as backlash hysteresis and coupled motion commonly exist in cable-driven mechanism of laparoscopic surgical robot end-effector, it is a great challenge to control the motion of robotic end-effector precisely during the surgical procedure. Due to the effects of coupled motion, the surgical end-effector will not move accurately as surgeons expected. Previous studies mostly focused on the design of special compensation mechanisms and software compensation algorithms to solve coupled motion problem. However, these approaches are limited because the backlash hysteresis is ignored and the mechanism of end-effector is restricted. This paper shows an improved scheme to eliminate the coupled motion of end-effector and reduce the position tracking error. The proposed decoupling scheme is conducted in three stages. Firstly, the time and frequency domain information of the driving motor current and the motion information of surgical instrument are extracted in real-time. Thereafter, a feedforward neural network is designed to identify the movement stage of end-effector. Finally, a prediction model is designed to predict the coupling error, after that the coupling error can be eliminated by using feedforward compensation control. An experimental platform was set up to verify the effectiveness of the proposed control scheme, and the results of corresponding comparative experiments revealed that the proposed strategy can substantially improve the tracking accuracy.",robotics
10.1016/j.robot.2018.10.015,Journal,Robotics and Autonomous Systems,scopus,2019-02-01,sciencedirect,Time-dependent genetic algorithm and its application to quadruped's locomotion,https://api.elsevier.com/content/abstract/scopus_id/85056925953,"Genetic algorithms (GAs) are widely used in machine learning and optimization. This paper proposes a time-dependent genetic algorithm (TDGA) based on real-coded genetic algorithm (RCGA) to improve the convergence performance of functions over time such as a foot trajectory. TDGA has several distinguishing features when compared with traditional RCGA. First, individuals are arranged over time, and then the individuals are optimized in sequence. Second, search spaces of design variables are newly comprised of processes of reductions for search spaces. Third, the search space for crossover operations is expanded to avoid local minima traps that can occur in new search spaces up to the previous search space before performing any reduction of search space, and boundary mutation operation is performed to the new search spaces. Computer simulations are implemented to verify the convergence performance of the robot locomotion optimized by TDGA. Then, TDGA optimizes the desired feet trajectories of quadruped robots that climb up a slope and the impedance parameters of admittance control so that quadruped robots can trot stably over irregular terrains. Simulation results clearly represent that the convergence performance is improved by TDGA, which also shows that TDGA could be broadly used in robot locomotion research.",robotics
10.1016/j.measurement.2018.10.043,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2019-02-01,sciencedirect,3D reconstruction of GMAW pool surface using composite sensor technology,https://api.elsevier.com/content/abstract/scopus_id/85055282411,"Weld pool is the most direct signal to reflect welding quality, that means measurement and reconstruction of the pool surface is one of the most urgent task in industrial robotic welding. A lot of researches have been done in this area, but the strong welding arc and the violent oscillation of weld pool affect their effectiveness. To better resolve this problem, a composite vision sensing system was proposed in our previous study, which consists of an active vision part and a passive vision part. The geometric parameters of the weld cross section and the weld pool have been successfully extracted by using this system. Based on the obtained information, the dynamic relationship between the weld speed and the bead geometric parameters is established by multistep predictive model based on neural network. By using the established model and iterative algorithm, the pool tail height is estimated in this paper. A spatial vision calibration is also proposed to calculate the real geometric parameters of the weld pool. Then the 3D surface of the weld pool is also reconstructed by the means of space curved surface fitting. Finally, the verification experiment is also conducted to verify the feasibility of this method.",robotics
10.1016/j.image.2018.09.013,Journal,Signal Processing: Image Communication,scopus,2019-02-01,sciencedirect,Object instance detection with pruned Alexnet and extended training data,https://api.elsevier.com/content/abstract/scopus_id/85054588872,"Object instance detection has garnered much concern in many practical applications, especially in the field of intelligent service robot. Imagine robots working in real scenes, one may expect the instance detection system to be light-weighted to enable mobile or embedded system deployment. Focusing on reconstructing a smaller learning network from a noted deep model,we have pruned Alexnet to a compressed model with fewer parameters but equivalent accuracy, denoted as BING-Pruned Alexnet(B-PA). Our method first utilizes BING(Binarized Normed Gradient) to compute bounding boxes, then builds a pruned network for recognition by reducing neurons and cutting fully connected layers on the classic architecture Alexnet. Since the training samples for instance detection are limited and of small variation, we extend the training data by combining data augmentation with synthetic generation. In the end, our B-PA network occupies only 5MB, which is 50 times smaller than the original Alexnet, but can still achieve Alexnet-level accuracy when recognizing on GMU Kitchen dataset. Numerical experiments are conducted to compare our algorithm with the state-of-art instance detection algorithms on a self-made BHID database and two public database i.e.,WRGB-D dataset and GMU Kitchen dataset, which demonstrate that B-PA reduces the storage requirements of neural networks substantially while preserving generalization performance on object instance detection.",robotics
10.1016/B978-0-12-819178-1.00035-6,Book,"Precision Medicine for Investigators, Practitioners and Providers",scopus,2019-01-01,sciencedirect,Precision medicine in ophthalmology: An evolving revolution in diagnostic and therapeutic tools,https://api.elsevier.com/content/abstract/scopus_id/85093483812,"Precision medicine refers to a stratification of patients using a wide array of individual-specific data to enable precise targeting of disease subgroups with the best available diagnostic and therapeutic approaches. Within ophthalmology, this strategy is being applied successfully and is most evident in the management of the inherited diseases. This paradigm shift in provision of care is accelerated by the emergence of novel imaging technologies, robotics, and artificial intelligence, as well as emerging technologies that integrate bioinformatics data into clinically relevant knowledge. This knowledge is used in turn to develop a system capable of supporting clinical decision-making and utilization of high-precision therapeutic options in both a personalized and cost-effective way. Examples of the diverse areas making rapid progress toward full implementation of precision medicine include, but are not limited to, ocular genetic diseases, robotic surgery, virtual reality simulations, modern imaging techniques, and the role of healthcare providers.",robotics
10.1016/j.promfg.2020.01.136,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Transferring human manipulation knowledge to industrial robots using reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85083532554,"Nowadays in the context of Industry 4.0, manufacturing companies are faced by increasing global competition and challenges, which requires them to become more flexible and able to adapt fast to rapid market changes. Advanced robot system is an enabler for achieving greater flexibility and adaptability, however, programming such systems also become increasingly more complex. Thus, new methods for programming robot systems and enabling self-learning capabilities to accommodate the natural variation exhibited in real-world tasks are needed. In this paper, we propose a Reinforcement Learning (RL) enabled robot system, which learns task trajectories from human workers. The presented work demonstrates that with minimal human effort, we can transfer manual manipulation tasks in certain domains to a robot system without the requirement for a complicated hardware system model or tedious and complex programming. Furthermore, the robot is able to build upon the learned concepts from the human expert and improve its performance over time. Initially, Q-learning is applied, which has shown very promising results. Preliminary experiments, from a use case in slaughterhouses, demonstrate the viability of the proposed approach. We conclude that the feasibility and applicability of RL for industrial robots and industrial processes, holds and unseen potential, especially for tasks where natural variation is exhibited in either the product or process.",robotics
10.1016/B978-0-12-816176-0.00045-4,Book,Handbook of Medical Image Computing and Computer Assisted Intervention,scopus,2019-01-01,sciencedirect,Challenges in computer assisted interventions,https://api.elsevier.com/content/abstract/scopus_id/85082596227,"Challenges in design, implementation, clinical evaluation, and deployment of computer assisted intervention solutions are manifold. Some of these challenges will be discussed in this chapter.
               Computer assistance in both surgical procedures and radiology interventions aim at augmenting the clinicians with the overall objective of providing better clinical outcome. Multimodal imaging, robotics, artificial intelligence, and augmented reality play a major role in computer assisted interventions. After a brief analysis of the state-of-the-art and practice in this field, we discuss the challenges in design and development, as well as translation and deployment of the technology, from research projects motivated by clinical needs to solutions routinely used within clinical setups. We also consider the required training of surgeons and the surgical team as a major component for smooth and successful translation. We present simulation as an important tool not only for the design and development of computer assisted intervention solutions but also in their fast and smooth translation into daily practice.",robotics
10.1016/j.ifacol.2019.12.685,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,Humanoid Receptionist Connected to IoT Subsystems and Smart Infrastructure is Smarter than Expected,https://api.elsevier.com/content/abstract/scopus_id/85081595081,"The authors of this paper have the unique opportunity to design the AI engine of a humanoid front desk office (receptionist) employee. The designed AI engine conception is strongly influenced by the Smart Infrastructure approach, making the robot be just an interface to a complex system cooperating with various databases, subsystems and IoT modules and sensors. The WeegreOne robot has been given the access to a wide variety of preferred conversation topics, additional facts, databases and sensors, making it become a valuable interlocutor. The prototype has been installed, and the AI engine have adapted to most of the real-case uses. Surprisingly, human interlocutors neither do not use nor discover the full functionality and versatility of the robot. The research team extended the usability and connectivity of the robot while analyzing the interactions with the employees and visitors of the company. The article presents the concept, design and final thoughts on the results of the experiment - that people seem not to need robotic employees to be really smart or versatile -or - people seem to underestimate the versatility of robotic employees.",robotics
10.1016/j.ifacol.2019.12.517,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,The development of autonomous navigation and obstacle avoidance for a robotic mower using machine vision technique,https://api.elsevier.com/content/abstract/scopus_id/85081065786,"The autonomous driving of agricultural machinery using information from global navigation satellite system (GNSS) information has developed rapidly because it is considered as a labor-saving measure in agriculture. The agricultural machinery is able to locate its position using a GNSS signal allowing it to move in an area autonomously. However, if machinery uses the GNSS signal only to self-locate it may run the risk of colliding with obstacles as it may not accurately sense the surrounding environment. Furthermore, sensors such as radars or lasers cannot distinguish between grass and obstacles; hence they cannot be used for sensing an agricultural environment including the detection of obstacles that are likely to be encountered by the machinery. Autonomous driving cannot be performed in environments such as orchards where the satellite positioning accuracy is low. This paper presents an autonomous driving system that we developed that is able to avoid obstacles and drive without the aid of a GNSS signal. The system uses an object detection system that is based on a stereo camera and deep learning technique i.e. convolutional neural networks as they can be used to recognize an environment and avoid obstacles. The autonomous driving ability of the vehicle was evaluated using real-time kinematic-GNSS to measure the true values through experiments that were conducted in the Tanashi Forest of the University of Tokyo.",robotics
10.1016/j.ifacol.2019.12.501,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,A Study on the Detection of Visible Parts of Cordons Using Deep Learning Networks for Automated Green Shoot Thinning in Vineyards,https://api.elsevier.com/content/abstract/scopus_id/85081061819,"Green shoot thinning operation in vineyards helps to reduce the crop load in favor of optimal quality wines. Mechanical green shoot thinning exists, but it causes cluster removal efficiencies to vary widely between 10-85 % because of difficulty in controlling the thinning end-effector position precisely to cordon trajectories. Automatically positioning the thinning end-effector to cordon trajectories will help to precisely remove the green shoots and to increase the efficiency and performance of the mechanical green shoot thinning operation. However, heavy occlusion of cordons due to shoots/leaves during thinning season makes it challenging to accurately determine the trajectories of cordons. Successfully detecting the visible parts of the cordons during the thinning season will help to estimate the trajectories of cordons for automated/robotics operation. In this study, a total of 390 wine grape vines were selected, and color images of these wine grapes were captured from a fixed distance and height for three weeks during the thinning season in real-time field conditions. Faster R-CNN (Faster regions-convolutional neural network) was deployed through transfer learning and fine tuning using the pre-trained networks (AlexNet, VGG16, VGG19 and ResNet18) to detect the visible parts of the cordons. Results showed that, Faster-RCNN model trained with ResNet18 networks provides higher accuracy in detecting visible parts of cordons compared to other tested networks with faster detection speed. Moreover, the detection accuracy with week 2 dataset was higher compared to that with week 3 and week 4 datasets because of the higher visibility of cordons. These results show the potential of Faster-RCNN model in detecting the visible parts of cordons, which will be used in the future to estimate the trajectories of the cordons for the automated green shoot thinning operation in vineyards.",robotics
10.1016/j.ifacol.2019.12.529,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,Autonomous Canal Following by a Micro-Aerial Vehicle Using Deep CNN,https://api.elsevier.com/content/abstract/scopus_id/85081052924,"Globally, large-scale irrigation canal networks serve as the backbone of agriculture in many important river basins. However, these water channels are in a constant threat of erosion, silt accumulation and structural damages over time which significantly reduces the water carrying capacity. Therefore, periodic inspections of the canals are required for critical operations and maintenance tasks. Due to the vast lengths of the channels and time-critical operations, automation has become a necessity. In this paper, we have proposed an aerial autonomous canal traversal system using ResNet50 inspired deep convolutional neural network. Given the uniqueness of our problem, we have generated our dataset for supervised learning and validation and later evaluated the proposed approach on a real canal. We have implemented our approach on a COTS micro-aerial vehicle. We have designed our system in such a way that it takes 200ms from perception to action thereby making the system real-time. We compare the superior performance of our Res Net 50 inspired network with other state-of-the-art CNNs trained on canal datasets.",robotics
10.1016/j.promfg.2018.12.026,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Hybrid artificial intelligence system for the design of highly-automated production systems,https://api.elsevier.com/content/abstract/scopus_id/85072561400,"The automated design of production systems is a young field of research which has not been widely explored by industry nor research in recent decades. Currently, the effort spent in production system design is increasing significantly in automotive industry due to the number of product variants and product complexity. Intelligent methods can support engineers in repetitive tasks and give them more opportunity to focus on work which requires their core competencies. This paper presents a novel artificial intelligence methodology that automatically generates initial production system configurations based on real industrial scenarios in the automotive field of body-in-white production. The hybrid methodology reacts flexibly against data sets of different content and has been implemented in a software prototype.",robotics
10.1016/j.promfg.2019.03.047,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,A Practical Approach of Teaching Digitalization and Safety Strategies in Cyber-Physical Production Systems,https://api.elsevier.com/content/abstract/scopus_id/85065658005,"Digitalization strategies in cyber-physical production systems (CPPS) are one of the key factors of Industry 4.0. The topic not only addresses data preparation, real-time data processing, big data analytics, visualization and machine interface design but also cyber security and safety. Especially, unauthorized access to protected (personal or enterprise) data or unauthorized control of production facilities imply risks when it comes to digitalization. Because of the increased complexity of state-of-the-art technologies, educational institutions need to provide practice-oriented teaching methods in learning factories to help engineers of today understand the impact of those developments.
                  In the light of this fact, this paper presents a practical approach of teaching digitalization strategies in CPPS. Planning, implementing and impacts of digitalization strategies are taught on a use-case with human-robot-collaboration. The objective of the use-case is to realize a real-time obstacle avoidance approach for a collaborative application based on a local positioning system. Here, students not only learn how to model the kinematics of a robot and program a robot but also how to design machine interfaces for real-time data transfer and processing as well as impacts of digitalization on safety and security.
                  The implementation of the use-case is part of the TU Wien teaching portfolio and thus part of its learning factory, where students and apprentices have the possibility to experiment and gain experiences by deliberate error simulations.",robotics
10.1016/j.cirpj.2018.12.002,Journal,CIRP Journal of Manufacturing Science and Technology,scopus,2019-01-01,sciencedirect,"From factory floor to process models: A data gathering approach to generate, transform, and visualize manufacturing processes",https://api.elsevier.com/content/abstract/scopus_id/85058703955,"The need for tools to help guide decision making is growing within the manufacturing industry. The analysis performed by these tools will help operators and engineers to understand the behaviour of the manufacturing stations better and thereby take data-driven decisions to improve them. The tools use techniques borrowed from fields such as Data Analytics, BigData, Predictive Modelling, and Machine Learning. However, to be able to use these tools efficiently, data from the factory floor is required as input. This data needs to be extracted from two sources, the PLCs, and the robots. In practice, methods to extract usable data from robots are rather scarce. The present work describes an approach to capture data from robots, which can be applied to both legacy and current state-of-the-art manufacturing systems. The described approach is developed using Sequence Planner – a tool for modelling and analyzing production systems – and is currently implemented at an automotive company as a pilot project to visualize and examine the ongoing process. By exploiting the robot code structure, robot actions are converted to event streams that are abstracted into operations. We then demonstrate the applicability of the resulting operations, by visualizing the ongoing process in real-time as Gantt charts, that support the operators performing maintenance. And, the data is also analyzed off-line using process mining techniques to create a general model that describes the underlying behaviour existing in the manufacturing station. Such models are used to derive insights about relationships between different operations, and also between resources.",robotics
10.1016/j.imavis.2018.12.001,Journal,Image and Vision Computing,scopus,2019-01-01,sciencedirect,Large-scale multiview 3D hand pose dataset,https://api.elsevier.com/content/abstract/scopus_id/85058661735,"Accurate visual hand pose estimation at joint level has several applications for human-robot interaction, natural user interfaces and virtual/augmented reality applications. However, it is still an open problem being addressed by the computer vision community. Recent novel deep learning techniques may help circumvent the limitations of standard approaches. However, they require large amounts of accurate annotated data.
                  Hand pose datasets that have been released so far present issues such as limited number of samples, inaccurate data or high-level annotations. Moreover, most of them are focused on depth-based approaches, providing only depth information (missing RGB data).
                  In this work, we present a novel multiview hand pose dataset in which we provide hand color images and different kind of annotations for each sample, i.e. the bounding box and the 2D and 3D location on the joints in the hand. Furthermore, we introduce a simple yet accurate deep learning architecture for real-time robust 2D hand pose estimation. Then, we conduct experiments that show how the use of the proposed dataset in the training stage produces accurate results for 2D hand pose estimation using a single color camera.",robotics
10.1016/j.compag.2018.10.031,Journal,Computers and Electronics in Agriculture,scopus,2019-01-01,sciencedirect,RRT-based path planning for an intelligent litchi-picking manipulator,https://api.elsevier.com/content/abstract/scopus_id/85056808664,"Aiming to realize the obstacle avoidance of the litchi-picking robot in dynamic and unstructured environments, an improved rapidly exploring random tree (RRT) algorithm is presented in this study. By establishing the collision detection model of the manipulator and obstacles, the obstacle avoidance path planning is carried out. The idea of target gravity was adopted to accelerate the path search speed. To optimize the path generated by RRT, a genetic algorithm (GA) and smooth processing were proposed. Performance tests in a virtual environment showed that the optimal combination of step sizes in the path search were 1.6 degrees for step size 1 and 2 degrees for step size 2. The time to achieve the path was 0.47 s, and the success rate was 100%. The path length was shortened by 20% after optimization. Experiments in the real environment were carried out for the 6 degrees of freedom (DOF) picking manipulator to pick litchi, and the results showed that the collision-free path planned by the proposed algorithm could successfully drive the manipulator from its initial position to the goal position without any collision.",robotics
10.1016/j.infsof.2018.08.003,Journal,Information and Software Technology,scopus,2019-01-01,sciencedirect,An extensible framework for software configuration optimization on heterogeneous computing systems: Time and energy case study,https://api.elsevier.com/content/abstract/scopus_id/85051630181,"Context: Application of component based software engineering methods to heterogeneous computing (HC) enables different software configurations to realize the same function with different non–functional properties (NFP). Finding the best software configuration with respect to multiple NFPs is a non–trivial task.
                  
                     Objective: We propose a Software Component Allocation Framework (SCAF) with the goal to acquire a (sub–) optimal software configuration with respect to multiple NFPs, thus providing performance prediction of a software configuration in its early design phase. We focus on the software configuration optimization for the average energy consumption and average execution time.
                  
                     Method: We validated SCAF through its instantiation on a real–world demonstrator and a simulation. Firstly, we verified the correctness of our model through comparing the performance prediction of six software configurations to the actual performance, obtained through extensive measurements with a confidence interval of 95%. Secondly, to demonstrate how SCAF scales up, we performed software configuration optimization on 55 generated use–cases (with solution spaces ranging from 1030 to 3070) and benchmark the results against best performing random configurations.
                  
                     Results: The performance of a configuration as predicted by our framework matched the configuration implemented and measured on a real–world platform. Furthermore, by applying the genetic algorithm and simulated annealing to the weight function given in SCAF, we obtain sub–optimal software configurations differing in performance at most 7% and 13% from the optimal configuration (respectfully).
                  
                     Conclusion: SCAF is capable of correctly describing a HC platform and reliably predict the performance of software configuration in the early design phase. Automated in the form of an Eclipse plugin, SCAF allows software architects to model architectural constraints and preferences, acting as a multi–criterion software architecture decision support system. In addition to said, we also point out several interesting research directions, to further investigate and improve our approach.",robotics
10.1016/j.robot.2018.08.013,Journal,Robotics and Autonomous Systems,scopus,2018-12-01,sciencedirect,Neural network for black-box fusion of underwater robot localization under unmodeled noise,https://api.elsevier.com/content/abstract/scopus_id/85054170379,"The research on autonomous robotics has focused on the aspect of information fusion from redundant estimates. Choosing a convenient fusion policy, that reduces the impact of unmodeled noise, and is computationally efficient, is an open research issue. The objective of this work is to study the problem of underwater localization which is a challenging field of research, given the dynamic aspect of the environment. For this, we explore navigation task scenarios based on inertial and geophysical sensory. We propose a neural network framework named B-PR-F which heuristically performs adaptable fusion of information, based on the principle of contextual anticipation of the localization signal within an ordered processing neighborhood. In the framework black-box unimodal estimations are related to the task context, and the confidence on individual estimates is evaluated before fusing information. A study conducted in a virtual environment illustrates the relevance of the model in fusing information under multiple task scenarios. A real experiment shows that our model outperforms the Kalman Filter and the Augmented Monte Carlo Localization algorithms in the task. We believe that the principle proposed can be relevant to related application fields, involving the problem of state estimation from the fusion of redundant information.",robotics
10.1016/j.neunet.2018.07.006,Journal,Neural Networks,scopus,2018-12-01,sciencedirect,State representation learning for control: An overview,https://api.elsevier.com/content/abstract/scopus_id/85053829041,"Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. The representation is learned to capture the variation in the environment generated by the agent’s actions; this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension characteristic of the representation helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning.
                  This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.",robotics
10.1016/j.mechatronics.2018.08.011,Journal,Mechatronics,scopus,2018-11-01,sciencedirect,An interactive and intuitive control interface for a tele-operated robot (AVATAR) system,https://api.elsevier.com/content/abstract/scopus_id/85053070319,"Robotic systems, which are controlled by artificial intelligent or tele-operation control interfaces, have been developed to be deployed instead of the human in extreme environments. However, insufficient artificial intelligence performance in unknown and unpredictable environments, and non-intuitive control interfaces with low immersive feedback have prevented wide spread of such robotic systems. In this paper, an intuitive and interactive control interface with inertial measurement units (IMUs), haptic gloves and a head mounted display (HMD) was developed to control a tele-operated robot in remote environments, which was abbreviated as AVATAR system. The tele-operated robot can be operated by a user’s motions which are measured by the wearable interface. Through a kinematic analysis of the user and the tele-operated robot, desired robot joint angles are calculated to follow the user’s motions in real time. Also, dual cameras on the robot head provide 3D visual information around the robot to the user. A grasping force of the robot hands, measured by motor current, is transmitted to the user as vibration feedback to fingertips of the haptic gloves. A long term evolution (LTE) was used as wireless communication between the user and the robot. The performance of the proposed AVATAR system has been verified by experiments.",robotics
10.1016/j.mechmachtheory.2018.06.019,Journal,Mechanism and Machine Theory,scopus,2018-11-01,sciencedirect,Grasp configuration planning for a low-cost and easy-operation underactuated three-fingered robot hand,https://api.elsevier.com/content/abstract/scopus_id/85050550185,"This paper proposes a method for modeling and planning the grasping configuration of a robotic hand with underactuated finger mechanisms. The proposed modeling algorithm is based on analysis and mimicking of human grasping experience. Results of the analysis is preprocessed and stored in a database. The grasp configuration planning algorithm can be used within a real time online grasp control as based on artificial neural networks. Namely, shapes and sizes of task objects are described by taxonomy data, which are used to generate grasp configurations. Then, a robot hand grasp control system is designed as based on the proposed grasp planning with close-loop position and force feedback. Simulations and experiments are carried out to show the basic features of the proposed formulation for identifying the grasp configurations while dealing with target objects of different shapes and sizes. It is hoped that the well-trained underactuated robot hand can solve most of grasping tasks in our life. The research approach is aimed to research low-cost easy-operation solution for feasible and practical implementation.",robotics
10.1016/j.ssci.2018.06.012,Journal,Safety Science,scopus,2018-11-01,sciencedirect,Occupational health and safety in the industry 4.0 era: A cause for major concern?,https://api.elsevier.com/content/abstract/scopus_id/85049323662,"Real-time communication, Big Data, human–machine cooperation, remote sensing, monitoring and process control, autonomous equipment and interconnectivity are becoming major assets in modern industry. As the fourth industrial revolution or Industry 4.0 becomes the predominant reality, it will bring new paradigm shifts, which will have an impact on the management of occupational health and safety (OHS).
                  In the midst of this new and accelerating industrial trend, are we giving due consideration to changes in OHS imperatives? Are the OHS consequences of Industry 4.0 being evaluated properly? Do we stand to lose any of the gains made through proactive approaches? Are there rational grounds for major concerns? In this article, we examine these questions in order to raise consciousness with regard to the integration of OHS into Industry4.0.
                  It is clear that if the technologies driving Industry 4.0 develop in silos and manufacturers’ initiatives are isolated and fragmented, the dangers will multiply and the net impact on OHS will be negative. As major changes are implemented, previous gains in preventive management of workplace health and safety will be at risk. If we are to avoid putting technological progress and OHS on a collision course, researchers, field experts and industrialists will have to collaborate on a smooth transition towards Industry 4.0.",robotics
10.1016/j.neunet.2018.03.014,Journal,Neural Networks,scopus,2018-11-01,sciencedirect,Intrinsically motivated reinforcement learning for human–robot interaction in the real-world,https://api.elsevier.com/content/abstract/scopus_id/85044999508,"For a natural social human–robot interaction, it is essential for a robot to learn the human-like social skills. However, learning such skills is notoriously hard due to the limited availability of direct instructions from people to teach a robot. In this paper, we propose an intrinsically motivated reinforcement learning framework in which an agent gets the intrinsic motivation-based rewards through the action-conditional predictive model. By using the proposed method, the robot learned the social skills from the human–robot interaction experiences gathered in the real uncontrolled environments. The results indicate that the robot not only acquired human-like social skills but also took more human-like decisions, on a test dataset, than a robot which received direct rewards for the task achievement.",robotics
10.1016/j.neucom.2018.05.069,Journal,Neurocomputing,scopus,2018-10-15,sciencedirect,A real-time spike-timing classifier of spatio-temporal patterns,https://api.elsevier.com/content/abstract/scopus_id/85048119736,"Considering the problem of recognizing non-verbal cues in Human–Robot Interaction applications, this paper proposes a novel real-time unsupervised spike timing neural network for recognition and early detection of spatio-temporal human gestures. Two spiking network classifiers one based on Izhikevich neuron model, and the other one based on Integrate-and-Fire-or-Burst neuron model have been implemented in CUDA, and allow the classification to be performed in real-time. To evaluate the performance of this proposal, we test the case of a physical robot observing air-handwritings of human gesture. The proposed approaches run in real-time, thus they are suitable for human–robot applications; they allow real-time early classifying human gestures and actions while they require a very small number of training samples. In comparing to other prominent techniques, our approaches demonstrate superior accuracy and are suitable for early classification of different types of human actions in time-sensitive mobile applications such as robotics.",robotics
10.1016/j.cmpb.2018.08.013,Journal,Computer Methods and Programs in Biomedicine,scopus,2018-10-01,sciencedirect,A facial expression controlled wheelchair for people with disabilities,https://api.elsevier.com/content/abstract/scopus_id/85052210734,"Background and Objectives
                  In order to improve assistive technologies for people with reduced mobility, this paper develops a new intelligent real-time emotion detection system to control equipment, such as electric wheelchairs (EWC) or robotic assistance vehicles. Every year, degenerative diseases and traumas prohibit thousands of people to easily control the joystick of their wheelchairs with their hands. Most current technologies are considered invasive and uncomfortable such as those requiring the user to wear some body sensor to control the wheelchair.
               
                  Methods
                  In this work, the proposed Human Machine Interface (HMI) provides an efficient hands-free option that does not require sensors or objects attached to the user's body. It allows the user to drive the wheelchair using its facial expressions which can be flexibly updated. This intelligent solution is based on a combination of neural networks (NN) and specific image preprocessing steps. First, the Viola-Jones combination is used to detect the face of the disability from a video. Subsequently, a neural network is used to classify the emotions displayed on the face. This solution called ""The Mathematics Behind Emotion"" is capable of classifying many facial expressions in real time, such as smiles and raised eyebrows, which are translated into signals for wheelchair control. On the hardware side, this solution only requires a smartphone and a Raspberry Pi card that can be easily mounted on the wheelchair.
               
                  Results
                  Many experiments have been conducted to evaluate the efficiency of the control acquisition process and the user experience in driving a wheelchair through facial expressions. The classification accuracy can expect 98.6% and it can offer an average recall rate of 97.1%. Thus, all these experiments have proven that the proposed system is able of accurately recognizing user commands in real time. Indeed, the obtained results indicate that the suggested system is more comfortable and better adapted to severely disabled people in their daily lives, than conventional methods. Among the advantages of this system, we cite its real time ability to identify facial emotions from different angles.
               
                  Conclusions
                  The proposed system takes into account the patient's pathology. It is intuitive, modern, doesn't require physical effort and can be integrated into a smartphone or tablet. The results obtained highlight the efficiency and reliability of this system, which ensures safe navigation for the disabled patient.",robotics
10.1016/j.robot.2018.08.001,Journal,Robotics and Autonomous Systems,scopus,2018-10-01,sciencedirect,A learning framework for semantic reach-to-grasp tasks integrating machine learning and optimization,https://api.elsevier.com/content/abstract/scopus_id/85051398189,"The ability to implement semantic Reach-to-grasp (RTG) tasks successfully is a crucial skill for robots. Given unknown objects in an unstructured environment, finding an feasible grasp configuration and generating a constraint-satisfied trajectory to reach it are challenging. In this paper, a learning framework which combines semantic grasp planning with trajectory generation is presented to implement semantic RTG tasks. Firstly, the object of interest is detected by using an object detection model trained by deep learning. A Bayesian-based search algorithm is proposed to find the grasp configuration with highest probability of success from the segmented image of the object using a trained quality network. Secondly, for robotic reaching movements, a model-based trajectory generation method inspired by the human internal model theory is designed to generate a constraint-satisfied trajectory. Finally, the presented framework is validated both in comparative analysis and on real-world experiments. Experimental results demonstrated that the proposed learning framework enables the robots to implement semantic RTG tasks in unstructured environments.",robotics
10.1016/j.neunet.2018.04.001,Journal,Neural Networks,scopus,2018-10-01,sciencedirect,Shaping the collision selectivity in a looming sensitive neuron model with parallel ON and OFF pathways and spike frequency adaptation,https://api.elsevier.com/content/abstract/scopus_id/85050469923,"Shaping the collision selectivity in vision-based artificial collision-detecting systems is still an open challenge. This paper presents a novel neuron model of a locust looming detector, i.e. the lobula giant movement detector (LGMD1), in order to provide effective solutions to enhance the collision selectivity of looming objects over other visual challenges. We propose an approach to model the biologically plausible mechanisms of ON and OFF pathways and a biophysical mechanism of spike frequency adaptation (SFA) in the proposed LGMD1 visual neural network. The ON and OFF pathways can separate both dark and light looming features for parallel spatiotemporal computations. This works effectively on perceiving a potential collision from dark or light objects that approach; such a bio-plausible structure can also separate LGMD1’s collision selectivity to its neighbouring looming detector — the LGMD2. The SFA mechanism can enhance the LGMD1’s collision selectivity to approaching objects rather than receding and translating stimuli, which is a significant improvement compared with similar LGMD1 neuron models. The proposed framework has been tested using off-line tests of synthetic and real-world stimuli, as well as on-line bio-robotic tests. The enhanced collision selectivity of the proposed model has been validated in systematic experiments. The computational simplicity and robustness of this work have also been verified by the bio-robotic tests, which demonstrates potential in building neuromorphic sensors for collision detection in both a fast and reliable manner.",robotics
10.1016/j.robot.2018.05.016,Journal,Robotics and Autonomous Systems,scopus,2018-09-01,sciencedirect,Adaptive low-level control of autonomous underwater vehicles using deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85048259874,"Low-level control of autonomous underwater vehicles (AUVs) has been extensively addressed by classical control techniques. However, the variable operating conditions and hostile environments faced by AUVs have driven researchers towards the formulation of adaptive control approaches. The reinforcement learning (RL) paradigm is a powerful framework which has been applied in different formulations of adaptive control strategies for AUVs. However, the limitations of RL approaches have lead towards the emergence of deep reinforcement learning which has become an attractive and promising framework for developing real adaptive control strategies to solve complex control problems for autonomous systems. However, most of the existing applications of deep RL use video images to train the decision making artificial agent but obtaining camera images only for an AUV control purpose could be costly in terms of energy consumption. Moreover, the rewards are not easily obtained directly from the video frames. In this work we develop a deep RL framework for adaptive control applications of AUVs based on an actor-critic goal-oriented deep RL architecture, which takes the available raw sensory information as input and as output the continuous control actions which are the low-level commands for the AUV’s thrusters. Experiments on a real AUV demonstrate the applicability of the stated deep RL approach for an autonomous robot control problem.",robotics
10.1016/j.eswa.2018.03.051,Journal,Expert Systems with Applications,scopus,2018-09-01,sciencedirect,Multimodal sensor-based semantic 3D mapping for a large-scale environment,https://api.elsevier.com/content/abstract/scopus_id/85044743836,"Semantic 3D mapping is one of the most important fields in robotics, and has been used in many applications, such as robot navigation, surveillance, and virtual reality. In general, semantic 3D mapping is mainly composed of 3D reconstruction and semantic segmentation. As these technologies evolve, there has been great progress in semantic 3D mapping in recent years. Furthermore, the number of robotic applications requiring semantic information in 3D mapping to perform high-level tasks has increased, and many studies on semantic 3D mapping have been published. Existing methods use a camera for both 3D reconstruction and semantic segmentation. However, this is not suitable for large-scale environments and has the disadvantage of high computational complexity. To address this problem, we propose a multimodal sensor-based semantic 3D mapping system using a 3D Lidar combined with a camera. In this study, the odometry is obtained by high-precision global positioning system (GPS) and inertial measurement unit (IMU), and it is estimated by iterative closest point (ICP) when a GPS signal is weak. Then, we use the latest 2D convolutional neural network (CNN) for semantic segmentation. To build a semantic 3D map, we integrate the 3D map with semantic information by using coordinate transformation and Bayes’ update scheme. In order to improve the semantic 3D map, we propose a 3D refinement process to correct wrongly segmented voxels and remove traces of moving vehicles in the 3D map. Through experiments on challenging sequences, we demonstrate that our method outperforms state-of-the-art methods in terms of accuracy and intersection over union (IoU). Thus, our method can be used for various applications that require semantic information in 3D map.",robotics
10.1016/j.bica.2018.07.016,Journal,Biologically Inspired Cognitive Architectures,scopus,2018-08-01,sciencedirect,Knowledge acquisition through introspection in Human-Robot Cooperation,https://api.elsevier.com/content/abstract/scopus_id/85051216556,"When cooperating with a team including humans, robots have to understand and update semantic information concerning the state of the environment. The run-time evaluation and acquisition of new concepts fall in the critical mass learning. It is a cognitive skill that enables the robot to show environmental awareness to complete its tasks successfully. A kind of self-consciousness emerges: the robot activates the introspective mental processes inferring if it owns a domain concept or not, and correctly blends the conceptual meaning of new entities. Many works attempt to simulate human brain functions leading to neural network implementation of consciousness; regrettably, some of these produce accurate model that however do not provide means for creating virtual agents able to interact with a human in a teamwork in a human-like fashion, hence including aspects such as self-conscious abilities, trust, emotions and motivations. We propose a method that, based on a cognitive architecture for human-robot teaming interaction, endows a robot with the ability to model its knowledge about the environment it is interacting with and to acquire new knowledge when it occurs.",robotics
10.1016/j.mechatronics.2018.05.014,Journal,Mechatronics,scopus,2018-08-01,sciencedirect,Development of an RBFN-based neural-fuzzy adaptive control strategy for an upper limb rehabilitation exoskeleton,https://api.elsevier.com/content/abstract/scopus_id/85048723595,"The patients of paralysis with motion impairment problems require extensive rehabilitation programs to regain motor functions. The great labor intensity and limited therapeutic effect of traditional human-based manual treatment have recently boosted the development of robot-assisted rehabilitation therapy. In the present work, a neural-fuzzy adaptive controller (NFAC) based on radial basis function network (RBFN) is developed for a rehabilitation exoskeleton to provide human arm movement assistance. A comprehensive overview is presented to describe the mechanical structure and electrical real-time control system of the therapeutic robot, which provides seven actuated degrees of freedom (DOFs) and achieves natural ranges of upper extremity movement. For the purpose of supporting the disable patients to perform repetitive passive rehabilitation training, the RBFN-based NFAC algorithm is proposed to guarantee trajectory tracking accuracy with parametric uncertainties and environmental disturbances. The stability of the proposed control scheme is demonstrated through Lyapunov stability theory. Further experimental investigation, involving the position tracking experiment and the frequency response experiment, are conducted to compare the control performance of the proposed method to those of cascaded proportional-integral-derivative controller (CPID) and fuzzy sliding mode controller (FSMC). The comparison results indicate that the proposed RBFN-based NFAC algorithm is capable of obtaining lower position tracking error and better frequency response characteristic.",robotics
10.1016/j.micpro.2018.04.002,Journal,Microprocessors and Microsystems,scopus,2018-07-01,sciencedirect,Modular design of a factor-graph-based inference engine on a System-On-Chip (SoC),https://api.elsevier.com/content/abstract/scopus_id/85045660948,"Factor graphs are probabilistic graphical frameworks for modeling complex and dynamic systems. They can be used in a broad range of application domains, from machine learning and robotics, to signal processing and digital communications. One important aspect that makes a factor graph very useful and very promising to be applied widely is its inference mechanism that is suitable for performing a complex model-based reasoning. However, its features have not fully explored and factor graphs are still used mainly as modeling tools that run on standard computers. Whereas in real applications such as robotics, one needs a practical implementation of such a framework. In this paper, we describe the development of a factor-graph-based inference engine that runs on a System-on-Chip (SoC). Running natively on a low level hardware, our factor graph engine delivers highest performance for real-time applications. We designed the embedded architecture so that it conveys important aspects such as modularity, scalability, flexibility and platform-friendly framework. The proposed architecture has customizable levels of parallelism as well as re-configurable modules that are extensible to accommodate large networks. We optimized the design to achieve high efficiency in terms of clock latency and resources consumption. We have tested our design on Xilinx Zynq-7000 SoCs and the implementation result demonstrates that the proposed framework can potentially be extended into a massively distributed probabilistic computing engine.",robotics
10.1016/j.eswa.2017.11.011,Journal,Expert Systems with Applications,scopus,2018-06-15,sciencedirect,Towards a common implementation of reinforcement learning for multiple robotic tasks,https://api.elsevier.com/content/abstract/scopus_id/85035079318,"Mobile robots are increasingly being employed for performing complex tasks in dynamic environments. Those tasks can be either explicitly programmed by an engineer or learned by means of some automatic learning method, which improves the adaptability of the robot and reduces the effort of setting it up. In this sense, reinforcement learning (RL) methods are recognized as a promising tool for a machine to learn autonomously how to do tasks that are specified in a relatively simple manner. However, the dependency between these methods and the particular task to learn is a well-known problem that has strongly restricted practical implementations in robotics so far. Breaking this barrier would have a significant impact on these and other intelligent systems; in particular, having a core method that requires little tuning effort for being applicable to diverse tasks would boost their autonomy in learning and self-adaptation capabilities. In this paper we present such a practical core implementation of RL, which enables the learning process for multiple robotic tasks with minimal per-task tuning or none. Based on value iteration methods, we introduce a novel approach for action selection, called Q-biased softmax regression (QBIASSR), that takes advantage of the structure of the state space by attending the physical variables involved (e.g., distances to obstacles, robot pose, etc.), thus experienced sets of states accelerate the decision-making process of unexplored or rarely-explored states. Intensive experiments with both real and simulated robots, carried out with the software framework also introduced here, show that our implementation is able to learn different robotic tasks without tuning the learning method. They also suggest that the combination of true online SARSA(λ) (TOSL) with QBIASSR can outperform the existing RL core algorithms in low-dimensional robotic tasks. All of these are promising results towards the possibility of learning much more complex tasks autonomously by a robotic agent.",robotics
10.1016/j.compind.2018.03.014,Journal,Computers in Industry,scopus,2018-06-01,sciencedirect,Real-time object detection in agricultural/remote environments using the multiple-expert colour feature extreme learning machine (MEC-ELM),https://api.elsevier.com/content/abstract/scopus_id/85044151304,"It is necessary for autonomous robotics in agriculture to provide real time feedback, but due to a diverse array of objects and lack of landscape uniformity this objective is inherently complex. The current study presents two implementations of the multiple-expert colour feature extreme learning machine (MEC-ELM). The MEC-ELM is a cascading algorithm that has been implemented along side a summed area table (SAT) for fast feature extraction and object classification, for a fully functioning object detection algorithm. The MEC-ELM is an implementation of the colour feature extreme learning machine (CF-ELM), which is an extreme learning machine (ELM) with a partially connected hidden layer; taking three colour bands as inputs. The colour implementation used with the SAT enable the MEC-ELM to find and classify objects quickly, with 84% precision and 91% recall in weed detection in the Y’UV colour space and in 0.5 s per frame. The colour implementation is however limited to low resolution images and for this reason a colour level co-occurrence matrix (CLCM) variant of the MEC-ELM is proposed. This variant uses the SAT to produce a CLCM and texture analyses, with texture values processed as an input to the MEC-ELM. This enabled the MEC-ELM to achieve 78–85% precision and 81–93% recall in cattle, weed and quad bike detection and in times between 1 and 2 s per frame. Both implementations were benchmarked on a standard i7 mobile processor. Thus the results presented in this paper demonstrated that the MEC-ELM with SAT grid and CLCM makes an ideal candidate for fast object detection in complex and/or agricultural landscapes.",robotics
10.1016/j.robot.2018.02.010,Journal,Robotics and Autonomous Systems,scopus,2018-06-01,sciencedirect,Visual attention and object naming in humanoid robots using a bio-inspired spiking neural network,https://api.elsevier.com/content/abstract/scopus_id/85044145526,"Recent advances in behavioural and computational neuroscience, cognitive robotics, and in the hardware implementation of large-scale neural networks, provide the opportunity for an accelerated understanding of brain functions and for the design of interactive robotic systems based on brain-inspired control systems. This is especially the case in the domain of action and language learning, given the significant scientific and technological developments in this field. In this work we describe how a neuroanatomically grounded spiking neural network for visual attention has been extended with a word learning capability and integrated with the iCub humanoid robot to demonstrate attention-led object naming. Experiments were carried out with both a simulated and a real iCub robot platform with successful results. The iCub robot is capable of associating a label to an object with a ‘preferred’ orientation when visual and word stimuli are presented concurrently in the scene, as well as attending to said object, thus naming it. After learning is complete, the name of the object can be recalled successfully when only the visual input is present, even when the object has been moved from its original position or when other objects are present as distractors.",robotics
10.1016/j.compind.2018.02.015,Journal,Computers in Industry,scopus,2018-06-01,sciencedirect,Comprehensive evaluation method for performance of unmanned robot applied to automotive test using fuzzy logic and evidence theory and FNN,https://api.elsevier.com/content/abstract/scopus_id/85043482449,"In order to obtain reliable and exact evaluation, a new comprehensive evaluation method for performance of an unmanned robot applied to automotive test (URAT) using fuzzy logic, evidence theory and fuzzy neural network (FNN) is presented in this paper. Throttle repeatability, speed tracking accuracy, speed repeatability, driving shock degree are used as the system evaluation index. The subjective evaluation results with various expressions are quantified using fuzzy logic. The group decision making with quantified subjective evaluation results from various drivers is combined through evidence theory. The objective evaluation indexes measured by instrumentation and the corresponding combined subjective evaluation are self-learned and trained with FNN. The comprehensive performance evaluation system of the URAT is established. Finally, real vehicle experiments are conducted. The effectiveness of the presented method for the URAT is experimentally verified.",robotics
10.1016/j.neucom.2018.01.002,Journal,Neurocomputing,scopus,2018-04-12,sciencedirect,Robot manipulator control using neural networks: A survey,https://api.elsevier.com/content/abstract/scopus_id/85041636063,"Robot manipulators are playing increasingly significant roles in scientific researches and engineering applications in recent years. Using manipulators to save labors and increase accuracies are becoming common practices in industry. Neural networks, which feature high-speed parallel distributed processing, and can be readily implemented by hardware, have been recognized as a powerful tool for real-time processing and successfully applied widely in various control systems. Particularly, using neural networks for the control of robot manipulators have attracted much attention and various related schemes and methods have been proposed and investigated. In this paper, we make a review of research progress about controlling manipulators by means of neural networks. The problem foundation of manipulator control and the theoretical ideas on using neural network to solve this problem are first analyzed and then the latest progresses on this topic in recent years are described and reviewed in detail. Finally, toward practical applications, some potential directions possibly deserving investigation in controlling manipulators by neural networks are pointed out and discussed.",robotics
10.1016/j.mechatronics.2018.02.005,Journal,Mechatronics,scopus,2018-04-01,sciencedirect,Experimental study on the kinematic control of a cable suspended parallel robot for object tracking purpose,https://api.elsevier.com/content/abstract/scopus_id/85042311625,"In this paper, two novel methods are proposed and compared which allows computation of the position of the end-effector in under-constrained cable-driven parallel robots. In the first method, from the data collected from an inertial measurement unit attached on the end-effector, the forward kinematic problem is reduced to a linear system of equations in which the position of the end-effector can be readily obtained real-time which is a definite asset for control purpose. In what concern the second method, a LoLiMoT neural network is trained with data collected from the simulation environment to solve the forward kinematic problem. Based on the obtained position feedback, an experimental closed loop kinematic control is performed and the proposed method is validated based on the observation of a camera which is mounted on the end-effector. A simple low cost mechanical structure is designed and constructed to measure the cable tensions by use of bending Loadcells. The sensors were calibrated and their data were used to guarantee the existence of minimum force in cables during the end-effector’s motion. At last, an algorithm for tracking a moving object is proposed and implemented. The experimental results verify the efficiency of the proposed methods.",robotics
10.1016/j.compeleceng.2017.04.030,Journal,Computers and Electrical Engineering,scopus,2018-04-01,sciencedirect,Evaluation of a new virtual reality micro-robotic cell injection training system,https://api.elsevier.com/content/abstract/scopus_id/85019686507,"This study considers a virtual reality (VR) micro-robotic cell injection training system developed to reduce the time and cost required for a trainee to become proficient in cell injection. The VR environment replicates a micro-robotic cell injection setup to be interacted with and controlled using either a keyboard or haptic device. Using these two input control methods, user training evaluation experiments were designed and conducted to evaluate trainee performance. The performance improvement of 13 participants after undergoing training was analyzed. Results demonstrate that the participants attained higher accuracy and success rates when utilizing the haptic device control method than when applying the keyboard control method. All participants successfully performed the required task when employing the haptic device control method with haptic guidance enabled.",robotics
10.1016/j.neucom.2017.12.016,Journal,Neurocomputing,scopus,2018-03-22,sciencedirect,Use of human gestures for controlling a mobile robot via adaptive CMAC network and fuzzy logic controller,https://api.elsevier.com/content/abstract/scopus_id/85038844344,"Mobile robots with manipulators have been more and more commonly applied in extreme and hostile environments to assist or even replace human operators for complex tasks. In addition to autonomous abilities, mobile robots need to facilitate the human–robot interaction control mode that enables human users to easily control or collaborate with robots. This paper proposes a system which uses human gestures to control an autonomous mobile robot integrating a manipulator and a video surveillance platform. A human user can control the mobile robot just as one drives an actual vehicle in the vehicle’s driving cab. The proposed system obtains human’s skeleton joints information using a motion sensing input device, which is then recognized and interpreted into a set of control commands. This is implemented, based on the availability of training data set and requirement of in-time performance, by an adaptive cerebellar model articulation controller neural network, a finite state machine, a fuzzy controller and purposely designed gesture recognition and control command generation systems. These algorithms work together implement the steering and velocity control of the mobile robot in real-time. The experimental results demonstrate that the proposed approach is able to conveniently control a mobile robot using virtual driving method, with smooth manoeuvring trajectories in various speeds.",robotics
10.1016/j.physa.2017.11.155,Journal,Physica A: Statistical Mechanics and its Applications,scopus,2018-03-15,sciencedirect,Efficient digital implementation of a conductance-based globus pallidus neuron and the dynamics analysis,https://api.elsevier.com/content/abstract/scopus_id/85042234061,"Balance between biological plausibility of dynamical activities and computational efficiency is one of challenging problems in computational neuroscience and neural system engineering. This paper proposes a set of efficient methods for the hardware realization of the conductance-based neuron model with relevant dynamics, targeting reproducing the biological behaviors with low-cost implementation on digital programmable platform, which can be applied in wide range of conductance-based neuron models. Modified GP neuron models for efficient hardware implementation are presented to reproduce reliable pallidal dynamics, which decode the information of basal ganglia and regulate the movement disorder related voluntary activities. Implementation results on a field-programmable gate array (FPGA) demonstrate that the proposed techniques and models can reduce the resource cost significantly and reproduce the biological dynamics accurately. Besides, the biological behaviors with weak network coupling are explored on the proposed platform, and theoretical analysis is also made for the investigation of biological characteristics of the structured pallidal oscillator and network. The implementation techniques provide an essential step towards the large-scale neural network to explore the dynamical mechanisms in real time. Furthermore, the proposed methodology enables the FPGA-based system a powerful platform for the investigation on neurodegenerative diseases and real-time control of bio-inspired neuro-robotics.",robotics
10.1016/j.petlm.2017.11.003,Journal,Petroleum,scopus,2018-03-01,sciencedirect,Application of artificial intelligence to forecast hydrocarbon production from shales,https://api.elsevier.com/content/abstract/scopus_id/85044139353,"Artificial intelligence (AI) methods and applications have recently gained a great deal of attention in many areas, including fields of mathematics, neuroscience, economics, engineering, linguistics, gaming, and many others. This is due to the surge of innovative and sophisticated AI techniques applications to highly complex problems as well as the powerful new developments in high speed computing. Various applications of AI in everyday life include machine learning, pattern recognition, robotics, data processing and analysis, etc. The oil and gas industry is not behind either, in fact, AI techniques have recently been applied to estimate PVT properties, optimize production, predict recoverable hydrocarbons, optimize well placement using pattern recognition, optimize hydraulic fracture design, and to aid in reservoir characterization efforts. In this study, three different AI models are trained and used to forecast hydrocarbon production from hydraulically fractured wells. Two vastly used artificial intelligence methods, namely the Least Square Support Vector Machine (LSSVM) and the Artificial Neural Networks (ANN), are compared to a traditional curve fitting method known as Response Surface Model (RSM) using second order polynomial equations to determine production from shales. The objective of this work is to further explore the potential of AI in the oil and gas industry. Eight parameters are considered as input factors to build the model: reservoir permeability, initial dissolved gas-oil ratio, rock compressibility, gas relative permeability, slope of gas oil ratio, initial reservoir pressure, flowing bottom hole pressure, and hydraulic fracture spacing. The range of values used for these parameters resemble real field scenarios from prolific shale plays such as the Eagle Ford, Bakken, and the Niobrara in the United States. Production data consists of oil recovery factor and produced gas-oil ratio (GOR) generated from a generic hydraulically fractured reservoir model using a commercial simulator. The Box-Behnken experiment design was used to minimize the number of simulations for this study. Five time-based models (for production periods of 90 days, 1 year, 5 years, 10 years, and 15 years) and one rate-based model (when oil rate drops to 5 bbl/day/fracture) were considered. Particle Swarm Optimization (PSO) routine is used in all three surrogate models to obtain the associated model parameters. Models were trained using 80% of all data generated through simulation while 20% was used for testing of the models. All models were evaluated by measuring the goodness of fit through the coefficient of determination (R2) and the Normalized Root Mean Square Error (NRMSE). Results show that RSM and LSSVM have very accurate oil recovery forecasting capabilities while LSSVM shows the best performance for complex GOR behavior. Furthermore, all surrogate models are shown to serve as reliable proxy reservoir models useful for fast fluid recovery forecasts and sensitivity analyses.",robotics
10.1016/j.cmpb.2017.12.018,Journal,Computer Methods and Programs in Biomedicine,scopus,2018-03-01,sciencedirect,Passive magnetic-based localization for precise untethered medical instrument tracking,https://api.elsevier.com/content/abstract/scopus_id/85043295115,"Background and objective
                  Motion tracking and navigation systems are paramount for both safety and efficacy in a variety of surgical insertions, interventions and procedures. Among the state-of-art tracking technology, passive magnetic tracking using permanent magnets or passive magnetic sources for localization is an effective technology to provide untethered medical instrument tracking without cumbersome wires needed for signal or power transmission. Motivated by practical needs in two medical insertion procedures: Nasogastric intubation and Ventriculostomy, we propose a unified method based on passive magnetic-field localization, for enhanced efficacy and safety.
               
                  Methods
                  Traditional approaches to passive magnetic tracking involve solving the inverse localization problem. Limited by the idealistic magnetic field dipole model and computationally intense nonlinear optimization algorithm, the overall accuracy and computational cost are greatly compromised. The method introduced here features direct localization with artificial neural network (ANN) models that bypasses the need to resolve the inverse problem and is adaptable for a variety of real-time localization and tracking applications.
               
                  Results
                  The efficiency of the two methods, the inverse optimization method and the direct ANN method are experimentally evaluated by comparing the estimated position of reference trajectories for typical nasogastric and ventriculostomy insertion paths performed by a dexterous robotic arm which provides ground truth measurement. It was found that within the region of interest (ROI), the direct ANN technique could significantly improve the localization accuracy, with an average experimental localization error of less than 2 mm, while that of the traditional inverse optimization method using a dipole-based mathematical model at greater than 5 mm. Ex-vivo experiments were performed to validate the localization methods in clinical settings.
               
                  Conclusions
                  While the proposed method for passive magnetic tracking requires a procedure-specific pre-procedural calibration, it is able to provide real-time tracking with high accuracy, robustness and diversity. It could be the missing piece to the puzzle to bring passive magnetic tracking technology into practice, therefore leading to untethered medical instrument tracking.",robotics
10.1016/j.artint.2017.12.001,Journal,Artificial Intelligence,scopus,2018-03-01,sciencedirect,Decentralized Reinforcement Learning of robot behaviors,https://api.elsevier.com/content/abstract/scopus_id/85038868982,"A multi-agent methodology is proposed for Decentralized Reinforcement Learning (DRL) of individual behaviors in problems where multi-dimensional action spaces are involved. When using this methodology, sub-tasks are learned in parallel by individual agents working toward a common goal. In addition to proposing this methodology, three specific multi agent DRL approaches are considered: DRL-Independent, DRL Cooperative-Adaptive (CA), and DRL-Lenient. These approaches are validated and analyzed with an extensive empirical study using four different problems: 3D Mountain Car, SCARA Real-Time Trajectory Generation, Ball-Dribbling in humanoid soccer robotics, and Ball-Pushing using differential drive robots. The experimental validation provides evidence that DRL implementations show better performances and faster learning times than their centralized counterparts, while using less computational resources. DRL-Lenient and DRL-CA algorithms achieve the best final performances for the four tested problems, outperforming their DRL-Independent counterparts. Furthermore, the benefits of the DRL-Lenient and DRL-CA are more noticeable when the problem complexity increases and the centralized scheme becomes intractable given the available computational resources and training time.",robotics
10.1016/j.knosys.2017.11.033,Journal,Knowledge-Based Systems,scopus,2018-02-15,sciencedirect,Response selection from unstructured documents for human-computer conversation systems,https://api.elsevier.com/content/abstract/scopus_id/85037568062,"This paper studies response selection for human-computer conversation systems. Existing retrieval-based human-computer conversation systems are intended to reply to user utterances based on existing utterance-response pairs. However, collecting sufficient utterance-response pairs is intractable in practical situations, especially for many specific domains. We introduce DocChat a novel information retrieval approach for human-computer conversation systems that can use unstructured documents rather than semi-structured utterance-response pairs, to react to user utterances. The key of DocChat is a learning to rank model with features designed at various levels of granularity which is proposed to quantify the relevance between utterances and responses directly. We conduct comprehensive experiments on both sentence selection and real human-computer conversation scenarios. Empirical studies of sentence selection datasets shows reasonable improvements and the strong adaptability of our model. We compare DocChat with Xiaoice, a famous open domain chitchat engine in China. Side-by-side evaluation shows that DocChat is a good complement for human-computer conversation systems using utterance-response pairs as the primary source of responses. Furthermore, we release a large scale open-domain dataset for sentence selection which contains 304,413 query-sentence pairs.",robotics
10.1016/j.robot.2017.11.010,Journal,Robotics and Autonomous Systems,scopus,2018-02-01,sciencedirect,Reset-free Trial-and-Error Learning for Robot Damage Recovery,https://api.elsevier.com/content/abstract/scopus_id/85041300580,"The high probability of hardware failures prevents many advanced robots (e.g., legged robots) from being confidently deployed in real-world situations (e.g., post-disaster rescue). Instead of attempting to diagnose the failures, robots could adapt by trial-and-error in order to be able to complete their tasks. In this situation, damage recovery can be seen as a Reinforcement Learning (RL) problem. However, the best RL algorithms for robotics require the robot and the environment to be reset to an initial state after each episode, that is, the robot is not learning autonomously. In addition, most of the RL methods for robotics do not scale well with complex robots (e.g., walking robots) and either cannot be used at all or take too long to converge to a solution (e.g., hours of learning). In this paper, we introduce a novel learning algorithm called “Reset-free Trial-and-Error” (RTE) that (1) breaks the complexity by pre-generating hundreds of possible behaviors with a dynamics simulator of the intact robot, and (2) allows complex robots to quickly recover from damage while completing their tasks and taking the environment into account. We evaluate our algorithm on a simulated wheeled robot, a simulated six-legged robot, and a real six-legged walking robot that are damaged in several ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and whose objective is to reach a sequence of targets in an arena. Our experiments show that the robots can recover most of their locomotion abilities in an environment with obstacles, and without any human intervention.",robotics
10.1016/j.mechatronics.2017.12.002,Journal,Mechatronics,scopus,2018-02-01,sciencedirect,SLAM in dynamic environments via ML-RANSAC,https://api.elsevier.com/content/abstract/scopus_id/85038213100,"Simultaneous localization and mapping (SLAM) in dynamic environments is an important problem in robotics navigation, yet it is less studied. In this paper, we present a novel approach to segment and track multiple moving objects in real dynamic environments. Detected objects are classified into stationary and moving objects using a state-of-the-art method referred to as multilevel-RANSAC (ML-RANSAC) algorithm. The algorithm is designed to track moving objects in conflict situations while running SLAM. The ML-RANSAC algorithm is developed to robustly estimate velocity and position of the multiple moving objects in an unknown environment whereas the state of the objects (static or dynamic) is not known a priori. The main characteristic of the algorithm is its ability to address both static and dynamic objects in SLAM and to detect and track moving objects (DATMO) without dividing the problem into two separate parts (SLAM and DATMO). We apply the proposed algorithm on two sets of simulated data to validate its performance in situations where the objects are either occluded or placed in dense dynamic scenario. We have compared our method with the true data via simulation studies. Furthermore, we have implemented the algorithm on a Pioneer P3-DX mobile robot navigating a real dynamic environment. Simulation studies as well as real-time experiments suggest that the algorithm is able to track and classify objects accurately while performing SLAM in dynamic environments.",robotics
10.1016/j.patcog.2017.09.042,Journal,Pattern Recognition,scopus,2018-02-01,sciencedirect,Active garment recognition and target grasping point detection using deep learning,https://api.elsevier.com/content/abstract/scopus_id/85030791138,"Identification and bi-manual handling of deformable objects, like textiles, is one of the most challenging tasks in the field of industrial and service robotics. Their unpredictable shape and pose makes it very difficult to identify the type of garment and locate the most relevant parts that can be used for grasping. In this paper, we propose an algorithm that first, identifies the type of garment and second, performs a search of the two grasping points that allow a robot to bring the garment to a known pose. We show that using an active search strategy it is possible to grasp a garment directly from predefined grasping points, as opposed to the usual approach based on multiple re-graspings of the lowest hanging parts. Our approach uses a hierarchy of three Convolutional Neural Networks (CNNs) with different levels of specialization, trained both with synthetic and real images. The results obtained in the three steps (recognition, first grasping point, second grasping point) are promising. Experiments with real robots show that most of the errors are due to unsuccessful grasps and not to the localization of the grasping points, thus a more robust grasping strategy is required.",robotics
10.1016/j.neucom.2017.11.008,Journal,Neurocomputing,scopus,2018-01-31,sciencedirect,A dynamic colour perception system for autonomous robot navigation on unmarked roads,https://api.elsevier.com/content/abstract/scopus_id/85033771926,"Navigation on unmarked and possible poorly delineated roads where the boundaries between the road and the non-road surfaces are not clearly indicated is a particularly challenging task for autonomous vehicles. The results of this study show that fairly robust navigation strategies can be generated by a robot equipped with a dynamic active-vision based control system represented by an artificial neural network synthesized using evolutionary computation techniques. In the experiments described in this paper, a simulated Pioneer robot is required to visually navigate multiple poorly delineated roads that differ in terms of variations in luminance and/or chrominance between the road and the adjacent non-road areas. Low resolution camera images are processed by a mechanism that continuously adjusts the contribution of each component of a three dimensional colour model (e.g., R, G and B) to the generation of the robot perceptual experience. We show that the best controller can successfully drive a simulated Pioneer robot in environments with colour characteristics never encountered during the design phase, and operate with colour models never used during training. We show that the dynamic differential weighting of the colour components is underpinned by a complex pattern of neural activity that allows the robot to successfully adapt its perceptual system to the colour characteristics of different visual scenes. We also show that the controller can be easily ported onto real hardware, by showing the results of a series of tests with a physical Pioneer robot required to navigate various poorly delineated pedestrian roads.",robotics
10.1016/j.neucom.2017.01.110,Journal,Neurocomputing,scopus,2018-01-03,sciencedirect,Making physical proofs of concept of reinforcement learning control in single robot hose transport task complete,https://api.elsevier.com/content/abstract/scopus_id/85023642304,"This paper deals with the realization of physical proof of concept experiments in the paradigm of Linked Multi-Component Robotic Systems (LMCRS). The main objective is to demonstrate that the controllers learned through Reinforcement Learning (RL) algorithms with different state space formalizations and different spatial discretizations in a simulator are reliable in a real world configuration of the task of transporting a hose by a single robot. This one is a prototypical example of LMCRS task (extendable to much more complex tasks). We describe how the complete system has been designed and implemented. Two different previously learned RL controllers have been tested solving two different LMCRS control problems, using different state space modeling and discretization step in each case. The physical realizations validate previously published simulation based results, giving a strong argument in favor of the suitability of RL techniques to deal with LMCRS systems.",robotics
10.1016/j.neucom.2017.02.098,Journal,Neurocomputing,scopus,2018-01-03,sciencedirect,Real time direct kinematic problem computation of the 3PRS robot using neural networks,https://api.elsevier.com/content/abstract/scopus_id/85022211920,"The reliable calculation of the Direct Kinematic Problem (DKP) is one of the main challenges for the implementation of Real-Time (RT) controllers in Parallel Robots. The DKP estimates the pose of the end effector of the robot in terms of the sensors placed on the actuators. However, this calculation requires the use of time-consuming numerical iterative procedures.
                  Artificial Neural Networks have been proposed to implement the complex DKP equation mapping due to their universal approximator property. However, the proposals in this area do not consider the Real Time implementation of the ANN based solution, and no approximation error vs computational time analysis is carried out.
                  In this work, a methodology that uses Artificial Neural Networks (ANNs) to approximate the DKP is proposed. Based on the 3PRS parallel robot, a comprehensive study is carried out in which several network configurations are proposed to approximate the DKP. Moreover, to demonstrate the effectiveness of the approach, the proposed networks are evaluated considering not only their approximation capabilities, but also their Real Time performance in comparison with the traditional iterative procedures used in robotics.",robotics
10.1016/j.procir.2018.01.036,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,"Intuitive robot programming through environment perception, augmented reality simulation and automated program verification",https://api.elsevier.com/content/abstract/scopus_id/85061975291,"The increasing complexity of products and machines as well as short production cycles with small lot sizes present great challenges to production industry. Both, the programming of industrial robots in online mode using hand-held control devices or in offline mode using text-based programming requires specific knowledge of robotics and manufacturer-dependent robot control systems. In particular for small and medium-sized enterprises the machine control software needs to be easy, intuitive and usable without time-consuming learning steps, even for employees with no in-depth knowledge of information technology. To simplify the programming of application programs for industrial robots, we extended a cloud-based, task-oriented robot control system with environment perception and plausibility check functions. For the environment perception a depth camera and pointcloud processing hardware were installed. We detect objects located in the robot’s workspace by pointcloud processing with ROS and the PCL and add them to the augmented reality user interface of the robot control. The combination of process knowledge from task-oriented application programming and information about available workpieces from automated image processing enables a plausibility check and verification of the robot program before execution. After a robot program has been approved by the plausibility check, it is tested in an augmented reality simulation for collisions with the detected objects before deployment to the physical robot hardware. Experiments were carried out to evaluate the effectiveness of the developed extensions and confirmed their functionality.",robotics
10.1016/j.procs.2018.11.110,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Compositional models for VQA: Can neural module networks really count?,https://api.elsevier.com/content/abstract/scopus_id/85059483400,"Large neural networks trained in an end-to-end fashion usually fail to generalize over novel inputs which were not included in the training data. In contrast, biologically-inspired compositional models offer a more robust solution due to adaptive chaining of logical operations performed by specialized modules. In this paper, we present an implementation of a cognitive architecture based on the End-to-End Module Networks (N2NMNs) model [9] in the humanoid robot Pepper. The architecture is focused on the Visual Question Answering task (VQA), in which the robot answers questions regarding the seen image in natural language. We trained the system on the synthetic CLEVR dataset [10] and tested it on both synthetic images and real-world situations with CLEVR-like objects. We compare between the results and discuss the decrease of accuracy in real-world situations. Furthermore, we propose a new evaluation method, in which we test whether the model’s results for counting objects in each category is consistent with the overall number of seen objects. In summary, our results show that the current visual reasoning models are still far from being applicable in everyday life.",robotics
10.1016/j.ifacol.2018.11.271,Conference Proceeding,IFAC-PapersOnLine,scopus,2018-01-01,sciencedirect,Transportation of small objects by robotic throwing and catching: applying genetic programming for trajectory estimation,https://api.elsevier.com/content/abstract/scopus_id/85057037476,"Robotic catching of thrown objects is one of the common robotic tasks, which is explored in several works. This task includes subtask of tracking and forecasting the trajectory of the thrown object. Here we propose an algorithm for estimating future trajectory based on video signal from two cameras. Most of existing implementations use deterministic trajectory prediction and several are based on machine learning. We propose a combined forecasting algorithm where the deterministic motion model for each trajectory is generated via the genetic programming algorithm. Genetic programming is implemented on C++ with use of CUDA library and executed in parallel way on the graphical processing unit. Parallel execution allow genetic programming in real time. Numerical experiments with real trajectories of the thrown tennis ball show that the algorithm can forecast the trajectory accurately.",robotics
10.1016/j.procs.2018.07.108,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Ambience Inhaling: Speech Noise Inhaler in Mobile Robots using Deep Learning,https://api.elsevier.com/content/abstract/scopus_id/85051344062,"Audio based, machine learning human-computer interface with speech recognition systems performs sensibly well with the human voice under clean ambience, but become frail in applied technological implementation involving real-life interface. In mobile robotic systems, the speech machines are normally retrained with new changing acoustic ambience conditions are to be met. To inhale, classify, and track the real-world ambience noise with the new changing acoustic condition, we introduce an Ambience Inhaling (AI) framework in this article. This framework of an AI is to seek out complete noise information from speech data, in contrast with noise-nature discovery. Our proposed framework uses a deep convolutional neural network (CNN) based learning for classification with speech spectrogram patch segments, including a hybrid Harold Hotelling's T-square algorithm with Bayesian statistics for segmentation analysis. We use a symposium presentation-ambience as a test platform. In the symposium presentation-ambience, noise modeling is done with n-gram language having the parameter of n = 2. The impulsive or short-term noise which is superimposed with long-term noise caused degradation in classification. This degradation caused the classification errors. The provision of decision was made. The Gaussian mixture model and hidden Markova model are used with noise-only and noisy speech respectively. Time and frequency pooling are used with spectrogram also. The classification scores of 62.26%, 65.89%, and 69.12% are achieved with 5, 10 and 15 CNN filters respectively. As a significance, an AI is efficient and innovative.",robotics
10.1016/j.procir.2018.03.022,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,Fostering Robust Human-Robot Collaboration through AI Task Planning,https://api.elsevier.com/content/abstract/scopus_id/85049587790,"Recent advances in Artificial Intelligence (AI) are facilitating the deployment of intelligent systems in manufacturing. In Human-Robot Collaboration (HRC), industrial robots offer accuracy and efficiency while humans guarantee both experience and specialized and not replaceable skills. The seamless coordination of such different abilities constitutes one of the current challenges. This paper presents a dynamic task sequencing system for robust HRC developed within a EU-funded project. The proposed solution uses AI techniques to deal with the temporal variance entailed by the active presence of humans as well as to dynamically adapt task plans according to actual behavior of the pair human-worker/robot. The tool has been deployed in a real pilot plant.",robotics
10.1016/j.ifacol.2018.06.088,Conference Proceeding,IFAC-PapersOnLine,scopus,2018-01-01,sciencedirect,Design and Implementation of a Real-Time Autonomous Navigation System Applied to Lego Robots,https://api.elsevier.com/content/abstract/scopus_id/85048935639,"Teaching theoretical concepts of a real-time autonomous robot system may be a challenging task without real hardware support. The paper discusses the application of the Lego Robot for teaching multi interdisciplinary subjects to Mechatronics students. A real-time mobile robot system with perception using sensors, path planning algorithm, PID controller is used as the case to demonstrate the teaching methodology. The novelties are introduced compared to classical robotic classes: (i) the adoption of a project-based learning approach as teaching methodology; (ii) an effective real-time autonomous navigation approach for the mobile robot. However, the extendibility and applicability of the presented approach are not limited to only the educational purpose.",robotics
10.1016/j.plrev.2018.04.005,Journal,Physics of Life Reviews,scopus,2018-01-01,sciencedirect,Muscleless motor synergies and actions without movements: From motor neuroscience to cognitive robotics,https://api.elsevier.com/content/abstract/scopus_id/85048263877,"Emerging trends in neurosciences are providing converging evidence that cortical networks in predominantly motor areas are activated in several contexts related to ‘action’ that 
                        do not cause any overt movement
                     . Indeed for any complex body, human or embodied robot inhabiting unstructured environments, the dual processes of shaping motor output during action execution and providing the self with information related to feasibility, consequence and understanding of potential actions (of oneself/others) must seamlessly alternate during goal-oriented behaviors, social interactions. While prominent approaches like Optimal Control, Active Inference converge on the role of forward models, they diverge on the underlying computational basis. In this context, revisiting older ideas from motor control like the Equilibrium Point Hypothesis and synergy formation, this article offers an alternative perspective emphasizing the functional role of a ‘plastic, configurable’ internal representation of the body (body-schema) as a critical link enabling the seamless continuum between motor control and imagery. With the central proposition that both “real and imagined” actions are consequences of an internal simulation process achieved though passive goal-oriented animation of the body schema, the computational/neural basis of 
                        muscleless motor synergies
                      (and 
                        ensuing simulated actions without movements
                     ) is explored. The rationale behind this perspective is articulated in the context of several interdisciplinary studies in motor neurosciences (for example, intracranial depth recordings from the parietal cortex, FMRI studies highlighting a shared cortical basis for action ‘execution, imagination and understanding’), animal cognition (in particular, tool-use and neuro-rehabilitation experiments, revealing how coordinated tools are incorporated as an extension to the body schema) and pertinent challenges towards building cognitive robots that can seamlessly “act, interact, anticipate and understand” in unstructured natural living spaces.",robotics
10.1016/j.cogsys.2017.08.002,Journal,Cognitive Systems Research,scopus,2018-01-01,sciencedirect,A computational cognitive framework of spatial memory in brains and robots,https://api.elsevier.com/content/abstract/scopus_id/85034106426,"Computational cognitive models of spatial memory often neglect difficulties posed by the real world, such as sensory noise, uncertainty, and high spatial complexity. On the other hand, robotics is unconcerned with understanding biological cognition. Here, we describe a computational framework for robotic architectures aiming to function in realistic environments, as well as to be cognitively plausible.
                  We motivate and describe several mechanisms towards achieving this despite the sensory noise and spatial complexity inherent in the physical world. We tackle error accumulation during path integration by means of Bayesian localization, and loop closing with sequential gradient descent. Finally, we outline a method for structuring spatial representations using metric learning and clustering. Crucially, unlike the algorithms of traditional robotics, we show that these mechanisms can be implemented in neuronal or cognitive models.
                  We briefly outline a concrete implementation of the proposed framework as part of the LIDA cognitive architecture, and argue that this kind of probabilistic framework is well-suited for use in cognitive robotic architectures aiming to combine spatial functionality and psychological plausibility.",robotics
10.1016/j.biosystems.2017.10.001,Journal,BioSystems,scopus,2017-12-01,sciencedirect,Towards a first implementation of the WLIMES approach in living system studies advancing the diagnostics and therapy in personalized medicine,https://api.elsevier.com/content/abstract/scopus_id/85033459793,"The goal of this paper is to advance an extensible theory of living systems using an approach to biomathematics and biocomputation that suitably addresses self-organized, self-referential and anticipatory systems with multi-temporal multi-agents. Our first step is to provide foundations for modelling of emergent and evolving dynamic multi-level organic complexes and their sustentative processes in artificial and natural life systems. Main applications are in life sciences, medicine, ecology and astrobiology, as well as robotics, industrial automation, man-machine interface and creative design. Since 2011 over 100 scientists from a number of disciplines have been exploring a substantial set of theoretical frameworks for a comprehensive theory of life known as Integral Biomathics. That effort identified the need for a robust core model of organisms as dynamic wholes, using advanced and adequately computable mathematics. The work described here for that core combines the advantages of a situation and context aware multivalent computational logic for active self-organizing networks, Wandering Logic Intelligence (WLI), and a multi-scale dynamic category theory, Memory Evolutive Systems (MES), hence WLIMES. This is presented to the modeller via a formal augmented reality language as a first step towards practical modelling and simulation of multi-level living systems. Initial work focuses on the design and implementation of this visual language and calculus (VLC) and its graphical user interface. The results will be integrated within the current methodology and practices of theoretical biology and (personalized) medicine to deepen and to enhance the holistic understanding of life.",robotics
10.1016/j.jmatprotec.2017.07.005,Journal,Journal of Materials Processing Technology,scopus,2017-12-01,sciencedirect,Automated control of welding penetration based on audio sensing technology,https://api.elsevier.com/content/abstract/scopus_id/85023614883,"This paper presents a technology for welding quality control in pulse gas tungsten arc welding (GTAW). An automated welding penetration control system and effective controller are designed to achieve real-time collection and analysis of the welding acoustic signal. A special preprocessing method called “auditory attention” is proposed to optimize the extraction of the arc sound signal, which includes region of interest (ROI) extraction and denoising. The penetration feature extraction is implemented in the preprocessed signal. A sound channel feature based on linear prediction cepstrum coefficient (LPCC) is proposed for inclusion in the feature extraction method. Using these penetration features, a typical back propagation artificial neural network (BPANN) prediction model is introduced for identification of the penetration state during the welding process. Through training using a large number of data, the prediction rate reached 80–90%. The BPANN-piecewise (BPANN-PW) controller is used to achieve online control of welding penetration via arc sound signal for pulse GTAW welding using work-piece of different shapes. The results showed that this controller could adjust the welding current accurately and promptly depending on the variation of the arc sound signal. The controlling effect was good for the online monitoring of automated robotic GTAW welding.",robotics
10.1016/j.cogsys.2017.02.003,Journal,Cognitive Systems Research,scopus,2017-12-01,sciencedirect,Using agent transparency to support situation awareness of the Autonomous Squad Member,https://api.elsevier.com/content/abstract/scopus_id/85017417829,"Agent transparency has been proposed as a solution to the problem of facilitating operators’ situation awareness in human-robot teams. Sixty participants performed a dual monitoring task, monitoring both an intelligent, autonomous robot teammate and performing threat detection in a virtual environment. The robot displayed four different interfaces, corresponding to information from the Situation awareness-based Agent Transparency (SAT) model. Participants’ situation awareness of the robot, confidence in their situation awareness, trust in the robot, workload, cognitive processing, and perceived usability of the robot displays were assessed. Results indicate that participants using interfaces corresponding to higher SAT level had greater situation awareness, cognitive processing, and trust in the robot than when they viewed lower level SAT interfaces. No differences in workload or perceived usability of the display were detected. Based on these findings, we observed that transparency has a significant effect on situation awareness, trust, and cognitive processing.",robotics
10.1016/j.chb.2017.02.064,Journal,Computers in Human Behavior,scopus,2017-12-01,sciencedirect,Shopping with a robotic companion,https://api.elsevier.com/content/abstract/scopus_id/85015734591,"In this paper, we present a robotic shopping assistant, designed with a cognitive architecture, grounded in machine learning systems, in order to study how the human-robot interaction (HRI) is changing the shopping behavior in smart technological stores. In the software environment of the NAO robot, connected to the Internet with cloud services, we designed a social-like interaction where the robot carries out actions with the customer. In particular, we focused our design on two main skills the robot has to learn: the first is the ability to acquire social input communicated by relevant clues that humans provide about their emotional state (emotions, emotional speech), or collected in the Social Media (such as, information on the customer's tastes, cultural background, etc.). The second is the skill to express in turn its own emotional state, so that it can affect the customer buying decision, refining in the user the sense of interacting with a human-like companion. By combining social robotics and machine learning systems the potential of robotics to assist people in real life situations will increase, providing a gentle customers' acceptance of advanced technologies.",robotics
10.1016/j.engappai.2017.08.013,Journal,Engineering Applications of Artificial Intelligence,scopus,2017-11-01,sciencedirect,Gesture recognition system for real-time mobile robot control based on inertial sensors and motion strings,https://api.elsevier.com/content/abstract/scopus_id/85029641125,"Navigating and controlling a mobile robot in an indoor or outdoor environment by using a range of body-worn sensors is becoming an increasingly interesting research area in the robotics community. In such scenarios, hand gestures offer some unique capabilities for human–robot interaction inherent to nonverbal communication with features and application scenarios not possible with the currently predominant vision-based systems. Therefore, in this paper, we propose and develop an effective inertial-sensor-based system, worn by the user, along with a microprocessor and wireless module for communication with the robot at distances of up to 250 m. Possible features describing hand-gesture dynamics are introduced and their feasibility is demonstrated in an off-line scenario by using several classification methods (e.g., random forests and artificial neural networks). Refined motion features are then used in K-means unsupervised clustering for motion primitive extraction, which forms the motion strings used for real-time classification. The system demonstrated an 
                        F
                        1
                      score of 
                        90
                        .
                        05
                        %
                      with the possibility of gesture spotting and null class classification (e.g., undefined gestures were discarded from the analysis). Finally, to demonstrate the feasibility of the proposed algorithm, it was implemented in an Arduino-based 
                        8
                     -bit ATmega2560 microcontroller for control of a mobile, tracked robot platform.",robotics
10.1016/j.actaastro.2017.07.038,Journal,Acta Astronautica,scopus,2017-11-01,sciencedirect,Self-supervised learning as an enabling technology for future space exploration robots: ISS experiments on monocular distance learning,https://api.elsevier.com/content/abstract/scopus_id/85026883328,"Although machine learning holds an enormous promise for autonomous space robots, it is currently not employed because of the inherent uncertain outcome of learning processes. In this article we investigate a learning mechanism, Self-Supervised Learning (SSL), which is very reliable and hence an important candidate for real-world deployment even on safety-critical systems such as space robots. To demonstrate this reliability, we introduce a novel SSL setup that allows a stereo vision equipped robot to cope with the failure of one of its cameras. The setup learns to estimate average depth using a monocular image, by using the stereo vision depths from the past as trusted ground truth. We present preliminary results from an experiment on the International Space Station (ISS) performed with the MIT/NASA SPHERES VERTIGO satellite. The presented experiments were performed on October 8th, 2015 on board the ISS. The main goals were (1) data gathering, and (2) navigation based on stereo vision. First the astronaut Kimiya Yui moved the satellite around the Japanese Experiment Module to gather stereo vision data for learning. Subsequently, the satellite freely explored the space in the module based on its (trusted) stereo vision system and a pre-programmed exploration behavior, while simultaneously performing the self-supervised learning of monocular depth estimation on board. The two main goals were successfully achieved, representing the first online learning robotic experiments in space. These results lay the groundwork for a follow-up experiment in which the satellite will use the learned single-camera depth estimation for autonomous exploration in the ISS, and are an advancement towards future space robots that continuously improve their navigation capabilities over time, even in harsh and completely unknown space environments.",robotics
10.1016/j.eswa.2017.05.006,Journal,Expert Systems with Applications,scopus,2017-11-01,sciencedirect,Real-time gait subphase detection using an EMG signal graph matching (ESGM) algorithm based on EMG signals,https://api.elsevier.com/content/abstract/scopus_id/85019592914,"This study presents a gait subphase recognition method using an electromyogram (EMG) with a signal graph matching (ESGM) algorithm. Existing pattern recognition and machine learning using EMG signals has several innate problems in gait subphase detection. With respect to time domain features, their feature values may be analogous because two different gait steps may have similar muscle activation. In addition, the current gait subphase might not be recognized until the next gait subphase passes because the window size needed for feature extraction is larger than the period of the gait subphase. The ESGM algorithm is a new approach that compares reference EMG signals and input EMG signals according to time variance to solve these problems and considers variations of physiological muscle activity. We also determined all the elements of the ESGM algorithm using kinematic gait analysis and optimized the algorithm using experiments. Therefore, the ESGM algorithm reflects better timing characteristics of EMG signals than the time domain feature extraction algorithm. In addition, it can provide real-time and user-adaptive recognition of the gait subphase by using only EMG signals. Experimental results show that the average accuracy of the proposed method is 13% better than existing methods and the average detection latency of the proposed method was 5.5 times lower than existing methods.",robotics
10.1016/j.patrec.2017.09.007,Journal,Pattern Recognition Letters,scopus,2017-10-15,sciencedirect,Let the robot tell: Describe car image with natural language via LSTM,https://api.elsevier.com/content/abstract/scopus_id/85029030417,"Image-based car detection and classification has remained as a research hub in self-driving for decades. However, natural language description of car images is still a virgin territory even though it is a simple task for human to describe it by sentences within a glimpse at the image. In this paper, we present an end-to-end trainable and spatial-temporal deep recurrent neural network: LSTM (Long-Short Term Memory) to automatically convert car images to human understandable natural language descriptions. Our model builds on state of the art progress in computer vision and machine translation: we extract car region proposals with Region Convolutional Neural Networks(R-CNN) and embed them into fixed-sized vectors. Each word in a sentence is also embedded into real-valued vectors of the same size of images through a local global context aware neural network. The LSTM, feeding by image-sentence pairs sequentially in the training stage, is trained to maximize the joint probability of target word in each time step. In the test stage, the pre-trained LSTM receives a car image and predicts natural language description word by word. Finally, we evaluate our model regarding car’s static/dynamic attribute description on both 30,000 CompCar dataset [21] and 1000 video dataset collected on street scenario by our self-driving car, with quantitative BLEU score and subjective human-rating system evaluation metrics. We test our model’s generalization ability, its transfer ability to address car property classification issue and various image feature extractors’ impact on our model. Experiment results show the superiority and robustness of our model (refer to www.carlib.net/carimg2text.html for more experiment results).",robotics
10.1016/j.neucom.2017.01.077,Journal,Neurocomputing,scopus,2017-10-11,sciencedirect,Teaching robots to do object assembly using multi-modal 3D vision,https://api.elsevier.com/content/abstract/scopus_id/85012868996,"The motivation of this paper is to develop an intelligent robot assembly system using multi-modal vision for next-generation industrial assembly. The system includes two phases where in the first phase human beings demonstrate assembly to robots and in the second phase robots detect objects, plan grasps, and assemble objects following human demonstration using AI searching. A notorious difficulty to implement such a system is the bad precision of 3D visual detection. This paper presents multi-modal approaches to overcome the difficulty: It uses AR markers in the teaching phase to detect human operation, and uses point clouds and geometric constraints in the robot execution phase to avoid unexpected occlusion and noises. The paper presents several experiments to examine the precision and correctness of the approaches. It demonstrates the applicability of the approaches by integrating them with graph model-based motion planning, and by executing the results on industrial robots in real-world scenarios.",robotics
10.1016/j.eswa.2017.03.002,Journal,Expert Systems with Applications,scopus,2017-09-01,sciencedirect,Incremental Q-learning strategy for adaptive PID control of mobile robots,https://api.elsevier.com/content/abstract/scopus_id/85015894497,"Expert and intelligent systems are being developed to control many technological systems including mobile robots. However, the PID (Proportional-Integral-Derivative) controller is a fast low-level control strategy widely used in many control engineering tasks. Classic control theory has contributed with different tuning methods to obtain the gains of PID controllers for specific operation conditions. Nevertheless, when the system is not fully known and the operative conditions are variable and not previously known, classical techniques are not entirely suitable for the PID tuning. To overcome these drawbacks many adaptive approaches have been arisen, mainly from the field of artificial intelligent. In this work, we propose an incremental Q-learning strategy for adaptive PID control. In order to improve the learning efficiency we define a temporal memory into the learning process. While the memory remains invariant, a non-uniform specialization process is carried out generating new limited subspaces of learning. An implementation on a real mobile robot demonstrates the applicability of the proposed approach for a real-time simultaneous tuning of multiples adaptive PID controllers for a real system operating under variable conditions in a real environment.",robotics
10.1016/j.asoc.2016.07.049,Journal,Applied Soft Computing Journal,scopus,2017-08-01,sciencedirect,Distributed and resilient localization algorithm for Swarm Robotic Systems,https://api.elsevier.com/content/abstract/scopus_id/84995687162,"Many applications of Swarm Robotic Systems (SRSs) require each robot to be able to discover its own position. To provide such capability, some localization methods have been proposed, in which the positions of the robots are estimated based on a set of reference nodes in the swarm. In this paper, a distributed and resilient localization algorithm is proposed based on the BSA–MMA algorithm, which uses the Backtracking Search Algorithm (BSA) and the Min–Max Area (MMA) confidence factor. It is designed in a novel four-stage approach, where a new method, called Multi-hop Collaborative Min–Max Localization (MCMM), is included to improve the resilience in case of failures during the recognition of the reference nodes. The results, obtained with real Kilobot robots, show 28–36% of performance improvement obtained by the MCMM. Also, it is shown that the final result of the localization process is better when the MCMM is executed than if it is not executed. The experiments outcomes demonstrate that the novel four-stage approach and the use of the MCMM algorithm represents a progress in the design of distributed localization algorithms for SRS, especially with regard to its resilience.",robotics
10.1016/j.ifacol.2017.08.1219,Conference Proceeding,IFAC-PapersOnLine,scopus,2017-07-01,sciencedirect,Implementation of Brain Emotional Learning-Based Intelligent Controller for Flocking of Multi-Agent Systems,https://api.elsevier.com/content/abstract/scopus_id/85031823232,"The Brain Emotional Learning Based Intelligent Controller (BELBIC) is a neurobiologically-motivated intelligent controller based on a computational model of emotional learning in mammalian limbic system. The learning capabilities, multi-objective properties, and low computational complexity of BELBIC make it a very promising tool for implementation in real-time applications.
                  Our research combines, in an original way, the BELBIC methodology with a flocking control strategy, in order to perform real-time coordination of multiple Unmanned Aircraft Systems (UAS). The characteristics of BELBIC fit well in this scenario, since almost always the dynamics of the autonomous agents are not fully known, and furthermore, since they operate in close proximity, they are subjected to aggressive external disturbances. Numerical and experimental results based on the coordination of multiple quad rotorcraft UAS platforms demonstrate the applicability and satisfactory performance of the proposed method.",robotics
10.1016/j.micpro.2017.06.004,Journal,Microprocessors and Microsystems,scopus,2017-07-01,sciencedirect,Functional verification based platform for evaluating fault tolerance properties,https://api.elsevier.com/content/abstract/scopus_id/85020644987,"The fundamental topic of this article is the interconnection of simulation-based functional verification, which is standardly used for removing design errors from simulated hardware systems, with fault-tolerant mechanisms that serve for hardening electro-mechanical FPGA SRAM-based systems against faults. For this purpose, an evaluation platform that connects these two approaches was designed and tested for one particular casestudy: a robot that moves through a maze (its electronic part is the robot controller and the mechanical part is the robot itself). However, in order to make the evaluation platform generally applicable for various electro-mechanical systems, several subtopics and sub-problems need to solved. For example, the electronic controller can have several representations (hard-coded, processor based, neural-network based) and for each option, extendability of verification environment must be possible. Furthermore, in order to check complex behavior of verified systems, different verification scenarios must be prepared and this is the role of random generators or effective regression tests scenarios. Also, despite the transfer of the controller to the SRAM-based FPGA which was solved together with an injection of artificial faults, many more experiments must be done in order to create a sufficient fault-tolerant methodology that indicates how a general electronic controller can be hardened against faults by different fault-tolerant mechanisms in order to make it reliable enough in the real environment. All these additional topics are presented in this article together with some side experiments that led to their integration into the evaluation platform.",robotics
10.1016/j.neucom.2017.03.028,Journal,Neurocomputing,scopus,2017-06-28,sciencedirect,A real-time FPGA implementation of a biologically inspired central pattern generator network,https://api.elsevier.com/content/abstract/scopus_id/85016031943,"Central pattern generators (CPGs) functioning as biological neuronal circuits are responsible for generating rhythmic patterns to control locomotion. In this paper, a biologically inspired CPG composed of two reciprocally inhibitory neurons was implemented on a reconfigurable FPGA with real-time computational speed and considerably low hardware cost. High-accuracy neural circuit implementation can be computationally expensive, especially for a high-dimensional conductance-based neuron model. Thus, we aimed to present an efficient multiplier-less hardware implementation method for the investigation of real-time hardware CPG (hCPG) networks. In order to simplify the hardware implementation, a modified neuron model without nonlinear parts was given to decrease the complexity of the original model. A simple CPG network involving two chemical coupled neurons was realized which represented the pyloric dilator (PD) and lateral pyloric (LP) neurons in the crustacean pyloric CPG. The implementation results of the hCPG network showed that rhythmic behaviors were successfully reproduced and the resource consumption was dramatically reduced by using our multiplier-less implementation method. The presented FPGA-based implementation of hCPG network with remarkable performance set a prototype for the realization of other large-scale CPG networks and could be applied in bio-inspired robotics and motion rehabilitation for locomotion control.",robotics
10.1016/j.artint.2014.11.005,Journal,Artificial Intelligence,scopus,2017-06-01,sciencedirect,Model-based contextual policy search for data-efficient generalization of robot skills,https://api.elsevier.com/content/abstract/scopus_id/84919497776,"In robotics, lower-level controllers are typically used to make the robot solve a specific task in a fixed context. For example, the lower-level controller can encode a hitting movement while the context defines the target coordinates to hit. However, in many learning problems the context may change between task executions. To adapt the policy to a new context, we utilize a hierarchical approach by learning an upper-level policy that generalizes the lower-level controllers to new contexts. A common approach to learn such upper-level policies is to use policy search. However, the majority of current contextual policy search approaches are model-free and require a high number of interactions with the robot and its environment. Model-based approaches are known to significantly reduce the amount of robot experiments, however, current model-based techniques cannot be applied straightforwardly to the problem of learning contextual upper-level policies. They rely on specific parametrizations of the policy and the reward function, which are often unrealistic in the contextual policy search formulation. In this paper, we propose a novel model-based contextual policy search algorithm that is able to generalize lower-level controllers, and is data-efficient. Our approach is based on learned probabilistic forward models and information theoretic policy search. Unlike current algorithms, our method does not require any assumption on the parametrization of the policy or the reward function. We show on complex simulated robotic tasks and in a real robot experiment that the proposed learning framework speeds up the learning process by up to two orders of magnitude in comparison to existing methods, while learning high quality policies.",robotics
10.1016/j.eswa.2016.12.007,Journal,Expert Systems with Applications,scopus,2017-04-15,sciencedirect,A novel mobile robot localization approach based on topological maps using classification with reject option in omnidirectional images,https://api.elsevier.com/content/abstract/scopus_id/85002750897,"Mobile robot localization, which allows a robot to identify its position, is one of main challenges in the field of Robotics. In this work, we provide an evaluation of consolidated feature extractions and machine learning techniques from omnidirectional images focusing on topological map and localization tasks. The main contributions of this work are a novel method for localization via classification with reject option using omnidirectional images, as well as two novel omnidirectional image data sets. The localization system was analyzed in both virtual and real environments. Based on the experiments performed, the Minimal Learning Machine with Nearest Neighbors classifier and Local Binary Patterns feature extraction proved to be the best combination for mobile robot localization with accuracy of 96.7% and an Fscore of 96.6%.",robotics
10.1016/j.robot.2016.11.001,Journal,Robotics and Autonomous Systems,scopus,2017-04-01,sciencedirect,"Reprint of “Cognition, cognitics, and team action—Overview, foundations, and five theses for a better world”",https://api.elsevier.com/content/abstract/scopus_id/85011101312,"Consider now a shift of attention onto cognition. Novel definitions and metrics have been made, and it is time to reap the benefits, and to boost the development of intelligent autonomous systems. Mankind has gained a decisive advantage, in the race for survival and in the perspective of enjoyable lives, when cognitive abilities, i.e. cognition, appeared and started to develop in humans. Now cognition appears also as a crucial faculty to harness, i.e. to implement on machines; this is the field of cognitics. What is learnt about cognition for the purpose of machines, by a mirror effect, also affects the way we may recognize the role of cognition for ourselves, as humans. What is cognition? How does it relate to classical concepts, which appear much less well defined than expected? A summary of critical answers to these questions is sketched below. Then five theses about cognition are summarized: cognition to know the real world, to explore and perceive, to model; cognition for defining alternative worlds and possible futures, visions, and anticausality; cognition for effective control; cognitics for a large scale, technical deployment of cognition; and social cognitics, a foundation for team action and increased momentum for change. The five theses can be seen both as paths toward better insights in human and social nature and also as a roadmap for simultaneous and iterative processes capable to freely foster a better future for individuals and society. The paper finally includes as well an overview of MCS cognition theory, with some additional contributions, notably relating to foundations and time derivative aspects.",robotics
10.1016/j.simpat.2016.08.005,Journal,Simulation Modelling Practice and Theory,scopus,2017-04-01,sciencedirect,Computation offloading of a vehicle's continuous intrusion detection workload for energy efficiency and performance,https://api.elsevier.com/content/abstract/scopus_id/84995376610,"Computation offloading has been used and studied extensively in relation to mobile devices. That is because their relatively limited processing power and reliance on a battery render the concept of offloading any processing/energy-hungry tasks to a remote server, cloudlet or cloud infrastructure particularly attractive. However, the mobile device’s tasks that are typically offloaded are not time-critical and tend to be one-off. We argue that the concept can be practical also for continuous tasks run on more powerful cyber-physical systems where timeliness is a priority. As case study, we use the process of real-time intrusion detection on a robotic vehicle. Typically, such detection would employ lightweight statistical learning techniques that can run onboard the vehicle without severely affecting its energy consumption. We show that by offloading this task to a remote server, we can utilse approaches of much greater complexity and detection strength based on deep learning. We show both mathematically and experimentally that this allows not only greater detection accuracy, but also significant energy savings, which improve the operational autonomy of the vehicle. In addition, the overall detection latency is reduced in most of our experiments. This can be very important for vehicles and other cyber-physical systems where cyber attacks can directly affect physical safety. In fact, in some cases, the reduction in detection latency thanks to offloading is not only beneficial but necessary. An example is when detection latency onboard the vehicle would be higher than the detection period, and as a result a detection run cannot complete before the next one is scheduled, increasingly delaying consecutive detection decisions. Offloading to a remote server is an effective and energy-efficient solution to this problem too.",robotics
10.1016/j.isprsjprs.2017.01.002,Journal,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2017-03-01,sciencedirect,Appearance learning for 3D pose detection of a satellite at close-range,https://api.elsevier.com/content/abstract/scopus_id/85009110439,"In this paper we present a learning-based 3D detection of a highly challenging specular object exposed to a direct sunlight at very close-range. An object detection is one of the most important areas of image processing, and can also be used for initialization of local visual tracking methods. While the object detection in 3D space is generally a difficult problem, it poses more difficulties when the object is specular and exposed to the direct sunlight as in a space environment. Our solution to a such problem relies on an appearance learning of a real satellite mock-up based on a vector quantization and the vocabulary tree. Our method, implemented on a standard computer (CPU), exploits a full perspective projection model and provides near real-time 3D pose detection of a satellite for close-range approach and manipulation. The time consuming part of the training (feature description, building the vocabulary tree and indexing, depth buffering and back-projection) are performed offline, while a fast image retrieval and 3D-2D registration are performed on-line. In contrast, the state of the art image-based 3D pose detection methods are slower on CPU or assume a weak perspective camera projection model. In our case the dimension of the satellite is larger than the distance to the camera, hence the assumption of the weak perspective model does not hold. To evaluate the proposed method, the appearance of a full scale mock-up of the rear part of the TerraSAR-X satellite is trained under various illumination and camera views. The training images are captured with a camera mounted on six degrees of freedom robot, which enables to position the camera in a desired view, sampled over a sphere. The views that are not within the workspace of the robot are interpolated using image-based rendering. Moreover, we generate ground truth poses to verify the accuracy of the detection algorithm. The achieved results are robust and accurate even under noise due to specular reflection, and able to initialize a local tracking method.",robotics
10.1016/j.procs.2017.11.321,Conference Proceeding,Procedia Computer Science,scopus,2017-01-01,sciencedirect,Detecting static and dynamic novelties using dynamic neural network,https://api.elsevier.com/content/abstract/scopus_id/85040230399,"This paper presents a dynamic neural network based novelty filter where a mobile robot explored in an environment and built a dynamic model of the robot normal sensory-motor values perceived from the environment. Afterwards, the acquired model of normality was used on the robot to predict expected values of the sensory-motor inputs during the patrolling. Novelties could be detected if the prediction error between model-predicted values and real observed values exceeded a certain novelty threshold. Because of high uncertainty while robot interactions with the environment, the network-prediction errors along the robot route were not normally distributed to define only one novelty threshold. Therefore region-specific novelty thresholds were estimated by the proposed novelty detection system. As a result, the robot is capable of selecting a local novelty threshold depending on where the robot currently occupied.
                  To evaluate the proposed system, a set of real-world robotic experiments were carried out. Experimental results showed that the novelty filter was able to highlight unusual static and dynamic objects in the environment. Furthermore, the filter also produced reliable local novelty thresholds while the robot patrolled in the noisy environment.",robotics
10.1016/j.procs.2017.10.014,Conference Proceeding,Procedia Computer Science,scopus,2017-01-01,sciencedirect,Implementation of Blind Speech Separation for Intelligent Humanoid Robot using DUET Method,https://api.elsevier.com/content/abstract/scopus_id/85040022814,"Nowadays, there are many efforts in building intelligent humanoid robot and adding advanced ability such as Blind Speech Separation (BSS). BSS is a problem of separation of several speech signals in a real world from mono or stereo audio record. In this research, we implement BSP system using DUET algorithm which allow to separate any number of sources by using only stereo (two) mixtures. The DUET (Degenerate Unmixing Estimation Technique) algorithm replaces our previous FastICA (Fast Independent Component Analysis) method only success in simulation but failed in the implementation. The main problem of FastICA is that it assumes instantaneous mixing without time delay in the recording process. To deals with audio record in the presence of inevitable time delays, it has to be replaced with DUET algorithm to separate well in real time. Finally, the DUET algorithm is implemented to humanoid robot which is developed using Raspberry Pi and equipped with RaspPi Cam to detect human face. Furthermore, the Cirrus Logic Audio Card is stacked to Raspberry Pi in order to record stereo audio. In our experiments, there are three controlled variables to evaluate algorithm performance, that is: distance, number of sources, and subject’s name. Robot will record stereo audio for four seconds after face is detected by system. The recording is then separated by DUET algorithm and produce two source estimations with average computation time 1.8 seconds. With Google API, the recognition accuracy of separated speech is varying between 40%-70%.",robotics
10.1016/j.procs.2017.01.188,Conference Proceeding,Procedia Computer Science,scopus,2017-01-01,sciencedirect,Smartphone Based Data Mining for Fall Detection: Analysis and Design,https://api.elsevier.com/content/abstract/scopus_id/85016131722,"Falls can be devastating to the affected individual, yet a common event and hence one of the major causes of injury or disability within the aged population in Malaysia and worldwide. This paper aims to detect human fall utilizing the built inertial measurement unit (IMU) sensors of a smartphone attached to the subject's body with the signals wirelessly transmitted to remote PC for processing. Matlab's mobile and the Smartphone Sensor Support is used to acquire the data from the smartphone which is then analysed to design an algorithm for the detection of fall. Falls in human are usually characterized by large acceleration. However, focusing only on a large value of the acceleration can result in many false positives from fall-like activities such as sitting down quickly and jumping. Thus, in this work, a threshold based fall detection algorithm is implemented while a supervised machine learning algorithm is used to classify activity daily living (ADL). This combination has been found effective in increasing the accuracy of the fall detection. The aim is to develop and verify the high precision detection algorithm using Matlab Simulink, followed by a few real time testing.",robotics
10.1016/j.procs.2017.01.213,Conference Proceeding,Procedia Computer Science,scopus,2017-01-01,sciencedirect,Hybrid Agents Implementation for the Control of the Construction Company,https://api.elsevier.com/content/abstract/scopus_id/85016095509,"Planning the project duration together with separate works is an essential element of managing the construction. The final duration depends on multiple factors, including the funds, customer requests, and capabilities of the construction company. In order to avoid additional costs in penalties or additional expenses, the management needs to estimate the real construction duration in advance, before the contract is signed. Further on, these terms need to be monitored both in whole and for the specific jobs in order to be able to edit further stages with regard of the remaining time, resources and used resources ratio. The development of a decision support system for the construction company is a pressing problem due to the growing demand in decision making persons’ labor automation in planning and monitoring the construction processes. The paper presents the model and the application experience for such a system.",robotics
10.1016/j.patcog.2016.07.026,Journal,Pattern Recognition,scopus,2017-01-01,sciencedirect,Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order,https://api.elsevier.com/content/abstract/scopus_id/84991821737,"Facial expression recognition has been an active research area in the past 10 years, with growing application areas including avatar animation, neuromarketing and sociable robots. The recognition of facial expressions is not an easy problem for machine learning methods, since people can vary significantly in the way they show their expressions. Even images of the same person in the same facial expression can vary in brightness, background and pose, and these variations are emphasized if considering different subjects (because of variations in shape, ethnicity among others). Although facial expression recognition is very studied in the literature, few works perform fair evaluation avoiding mixing subjects while training and testing the proposed algorithms. Hence, facial expression recognition is still a challenging problem in computer vision. In this work, we propose a simple solution for facial expression recognition that uses a combination of Convolutional Neural Network and specific image pre-processing steps. Convolutional Neural Networks achieve better accuracy with big data. However, there are no publicly available datasets with sufficient data for facial expression recognition with deep architectures. Therefore, to tackle the problem, we apply some pre-processing techniques to extract only expression specific features from a face image and explore the presentation order of the samples during training. The experiments employed to evaluate our technique were carried out using three largely used public databases (CK+, JAFFE and BU-3DFE). A study of the impact of each image pre-processing operation in the accuracy rate is presented. The proposed method: achieves competitive results when compared with other facial expression recognition methods – 96.76% of accuracy in the CK+ database – it is fast to train, and it allows for real time facial expression recognition with standard computers.",robotics
10.1016/j.measurement.2016.09.026,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2016-12-01,sciencedirect,Gesture imitation and recognition using Kinect sensor and extreme learning machines,https://api.elsevier.com/content/abstract/scopus_id/84988699181,"This study presents a framework that recognizes and imitates human upper-body motions in real time. The framework consists of two parts. In the first part, a transformation algorithm is applied to 3D human motion data captured by a Kinect. The data are then converted into the robot’s joint angles by the algorithm. The human upper-body motions are successfully imitated by the NAO humanoid robot in real time.
                  In the second part, the human action recognition algorithm is implemented for upper-body gestures. A human action dataset is also created for the upper-body movements. Each action is performed 10 times by twenty-four users. The collected joint angles are divided into six action classes. Extreme Learning Machines (ELMs) are used to classify the human actions. Additionally, the Feed-Forward Neural Networks (FNNs) and K-Nearest Neighbor (K-NN) classifiers are used for comparison. According to the comparative results, ELMs produce a good human action recognition performance.",robotics
10.1016/j.cogsys.2016.03.001,Journal,Cognitive Systems Research,scopus,2016-12-01,sciencedirect,Simulation within simulation for agent decision-making: Theoretical foundations from cognitive science to operational computer model,https://api.elsevier.com/content/abstract/scopus_id/84963705117,"This article deals with artificial intelligence models inspired from cognitive science. The scope of this paper is the simulation of the decision-making process for virtual entities. The theoretical framework consists of concepts from the use of internal behavioral simulation for human decision-making. Inspired from such cognitive concepts, the contribution consists in a computational framework that enables a virtual entity to possess an autonomous world of simulation within the simulation. It can simulate itself (using its own model of behavior) and simulate its environment (using its representation of other entities). The entity has the ability to anticipate using internal simulations, in complex environments where it would be extremely difficult to use formal proof methods. Comparing the prediction and the original simulation, its predictive models are improved through a learning process. Illustrations of this model are provided through two implementations. First illustration is an example showing a shepherd, his herd and dogs. The dog simulates the sheep’s behavior in order to make predictions testing different strategies. Second, an artificial 3D juggler plays in interaction with virtual jugglers, humans and robots. For this application, the juggler predicts the behavior of balls in the air and uses prediction to coordinate its behavior in order to juggle.",robotics
10.1016/j.eswa.2016.06.021,Journal,Expert Systems with Applications,scopus,2016-11-15,sciencedirect,Neural networks based reinforcement learning for mobile robots obstacle avoidance,https://api.elsevier.com/content/abstract/scopus_id/84975032535,"This study proposes a new approach for solving the problem of autonomous movement of robots in environments that contain both static and dynamic obstacles. The purpose of this research is to provide mobile robots a collision-free trajectory within an uncertain workspace which contains both stationary and moving entities. The developed solution uses Q-learning and a neural network planner to solve path planning problems. The algorithm presented proves to be effective in navigation scenarios where global information is available. The speed of the robot can be set prior to the computation of the trajectory, which provides a great advantage in time-constrained applications. The solution is deployed in both Virtual Reality (VR) for easier visualization and safer testing activities, and on a real mobile robot for experimental validation. The algorithm is compared with Powerbot's ARNL proprietary navigation algorithm. Results show that the proposed solution has a good conversion rate computed at a satisfying speed.",robotics
10.1016/j.robot.2016.08.008,Journal,Robotics and Autonomous Systems,scopus,2016-11-01,sciencedirect,"Cognition, cognitics, and team action—Overview, foundations, and five theses for a better world",https://api.elsevier.com/content/abstract/scopus_id/85011032498,"Consider now a shift of attention onto cognition. Novel definitions and metrics have been made, and it is time to reap the benefits, and to boost the development of intelligent autonomous systems. Mankind has gained a decisive advantage, in the race for survival and in the perspective of enjoyable lives, when cognitive abilities, i.e. cognition, appeared and started to develop in humans. Now cognition appears also as a crucial faculty to harness, i.e. to implement on machines; this is the field of cognitics. What is learnt about cognition for the purpose of machines, by a mirror effect, also affects the way we may recognize the role of cognition for ourselves, as humans. What is cognition? How does it relate to classical concepts, which appear much less well defined than expected? A summary of critical answers to these questions is sketched below. Then five theses about cognition are summarized: cognition to know the real world, to explore and perceive, to model; cognition for defining alternative worlds and possible futures, visions, and anticausality; cognition for effective control; cognitics for a large scale, technical deployment of cognition; and social cognitics, a foundation for team action and increased momentum for change. The five theses can be seen both as paths toward better insights in human and social nature and also as a roadmap for simultaneous and iterative processes capable to freely foster a better future for individuals and society. The paper finally includes as well an overview of MCS cognition theory, with some additional contributions, notably relating to foundations and time derivative aspects.",robotics
10.1016/j.neunet.2016.08.003,Journal,Neural Networks,scopus,2016-11-01,sciencedirect,Implementation of Imitation Learning using Natural Learner Central Pattern Generator Neural Networks,https://api.elsevier.com/content/abstract/scopus_id/84984998255,"In this paper a new design of neural networks is introduced, which is able to generate oscillatory patterns. The fundamental building block of the neural network is O-neurons that can generate an oscillation in its transfer functions. Since the natural policy gradient learning has been used in training a central pattern generator paradigm, it is called Natural Learner CPG Neural Networks (NLCPGNN). O-neurons are connected and coupled to each other in order to shape a network and their unknown parameters are found by a natural policy gradient learning algorithm. The main contribution of this paper is design of this learning algorithm which is able to simultaneously search for the weights and topology of the network. This system is capable to obtain any complex motion and rhythmic trajectory via first layer and learn rhythmic trajectories in the second layer and converge towards all these movements. Moreover this two layers system is able to provide various features of a learner model for instance resistance against perturbations, modulation of trajectories amplitude and frequency. Simulation of the learning system in the robot simulator (WEBOTS) that is linked with MATLAB software has been done. Implementation on a real NAO robot demonstrates that the robot has learned desired motion with high accuracy. These results show proposed system produces high convergence rate and low test errors.",robotics
10.1016/j.autcon.2016.03.012,Journal,Automation in Construction,scopus,2016-11-01,sciencedirect,"SmartSite: Intelligent and autonomous environments, machinery, and processes to realize smart road construction projects",https://api.elsevier.com/content/abstract/scopus_id/84969580061,"This article presents an overview of the SmartSite research project that adopts machine learning, decision theory and distributed artificial intelligence to design and test a multi-agent system (MAS) for asphalt road construction. SmartSite puts major emphasis on sensing and communication technologies that integrate real-time automated information exchange in the supply chain of road construction. As part of the larger SmartSite project, this article introduces a novel real-time path planning system for compactors and presents the results of several simulation and field realistic experiments conducted to evaluate the system in a sophisticated simulation and harsh construction environment, respectively. The system operates based on Belief-Desire-Intention (BDI) software agents and real-time sensory inputs. The newly developed integrated and information rich process benefits asphalt compactor operators, as they are now capable to control their machinery and react to changing environmental, material-related and process-related disturbances or changes. This improves the quality of the delivery and laying of asphalt material, prevents compactors from over-compacting certain road segments, increases the road's pavement longevity during the operational life cycle phase; refocuses the work tasks of the site managers, and reduces the construction budget and schedule. The system's ability to maneuver an asphalt roller during real-word operation also makes it an important step towards a fully automated asphalt compactor.",robotics
10.1016/j.future.2016.03.023,Journal,Future Generation Computer Systems,scopus,2016-11-01,sciencedirect,Principles and experimentations of self-organizing embedded agents allowing learning from demonstration in ambient robotics,https://api.elsevier.com/content/abstract/scopus_id/84963850942,"Ambient systems are populated by many heterogeneous devices to provide adequate services to their users. The adaptation of an ambient system to the specific needs of its users is a challenging task. Because human–system interaction has to be as natural as possible, we propose an approach based on Learning from Demonstration (LfD). LfD is an interesting approach to generalize what has been observed during the demonstration to similar situations. However, using LfD in ambient systems needs adaptivity of the learning technique. We present ALEX, a multi-agent system able to dynamically learn and reuse contexts from demonstrations performed by a tutor. The results of the experiments performed on both a real and a virtual robot show interesting properties of our technology for ambient applications.",robotics
10.1016/j.ijleo.2016.06.126,Journal,Optik,scopus,2016-10-01,sciencedirect,Development of a calibrating algorithm for Delta Robot's visual positioning based on artificial neural network,https://api.elsevier.com/content/abstract/scopus_id/84978140749,"Delta robot with vision system can automatically control the end-actuator to accurately grasp moving objects on the conveyor belt. Establishment of the mapping relationship between the image feature space and the robot working space form a closed-loop chain for transformational link between the robot coordinate, camera coordinate and conveyor belt coordinate. The vision system calibration is a basic problem of robot vision research and implementation. The artificial neural networks (ANN) which has learning ability, adaptive ability and nonlinear function approximation ability can establish the nonlinear relationship between space points and pixel points to complete accurate calibration of the vision system. The convergence speed of calibration algorithm affects the real-time visual servo system. The calibration precision, generalization ability and calibration space of algorithm influence the robot grasping accuracy. Therefore, a new calibration technique for delta robot’s vision system was presented in this paper. The algorithm combines ANN with Faugeras vision system calibration technology. The setting of the initial value, network structure and the choice of the activation function is based on the model of Faugeras vision system calibration algorithm, which makes the actual output of the network closer to the target output. Experiments proved that this algorithm has higher calibration accuracy and generalization ability compared with the conventional calibration algorithm, as well as faster convergence speed compared with the conventional artificial neural network structure in the case of high calibration accuracy.",robotics
10.1016/j.neucom.2016.05.010,Journal,Neurocomputing,scopus,2016-09-26,sciencedirect,Enhanced discrete-time Zhang neural network for time-variant matrix inversion in the presence of bias noises,https://api.elsevier.com/content/abstract/scopus_id/84969497717,"Inevitable noises and limited computational time are major issues for time-variant matrix inversion in practice. When designing a time-variant matrix inversion algorithm, it is highly demanded to suppress noises without violating the performance of real-time computation. However, most existing algorithms only consider a nominal system in the absence of noises, and may suffer from a great computational error when noises are taken into account. Some other algorithms assume that denoising has been conducted before computation, which may consume extra time and may not be suitable in practice. By considering the above situation, in this paper, an enhanced discrete-time Zhang neural network (EDTZNN) model is proposed, analyzed and investigated for time-variant matrix inversion. For comparison, an original discrete-time Zhang neural network (ODTZNN) model is presented. Note that the EDTZNN model is superior to ODTZNN model in suppressing various kinds of bias noises. Moreover, theoretical analyses show the convergence of the proposed EDTZNN model in the presence of various kinds of bias noises. In addition, numerical experiments including an application to robot motion planning are provided to substantiate the efficacy and superiority of the proposed EDTZNN model for time-variant matrix inversion.",robotics
10.1016/j.cviu.2016.03.010,Journal,Computer Vision and Image Understanding,scopus,2016-08-01,sciencedirect,Interactive multiple object learning with scanty human supervision,https://api.elsevier.com/content/abstract/scopus_id/84963525756,"We present a fast and online human-robot interaction approach that progressively learns multiple object classifiers using scanty human supervision. Given an input video stream recorded during the human-robot interaction, the user just needs to annotate a small fraction of frames to compute object specific classifiers based on random ferns which share the same features. The resulting methodology is fast (in a few seconds, complex object appearances can be learned), versatile (it can be applied to unconstrained scenarios), scalable (real experiments show we can model up to 30 different object classes), and minimizes the amount of human intervention by leveraging the uncertainty measures associated to each classifier.
                  We thoroughly validate the approach on synthetic data and on real sequences acquired with a mobile platform in indoor and outdoor scenarios containing a multitude of different objects. We show that with little human assistance, we are able to build object classifiers robust to viewpoint changes, partial occlusions, varying lighting and cluttered backgrounds.",robotics
10.1016/j.bica.2016.07.006,Journal,Biologically Inspired Cognitive Architectures,scopus,2016-07-01,sciencedirect,Analyzing and discussing primary creative traits of a robotic artist,https://api.elsevier.com/content/abstract/scopus_id/84994896174,"We present a robot aimed at producing a collage formed by a mix of photomontage and digital collage. The artwork is created after a visual and verbal interaction with a human user. The proposed system, through a cognitive architecture, allows the robot to manage the three different phases of the real-time artwork process: (i) taking inspiration from information captured during the postural and verbal interaction with the human user and from the analysis of his/her social web items; (ii) performing a creative process to obtain a model of the artwork; (iii) executing the creative collage composition and providing a significant title. The paper explains, primarily, how the creativity traits of the robot are implemented in the proposed architecture: how ideas are generated through an elaboration that is modulated by affective influences; how the personality and the artistic behavior are modeled by learning and guided by external evaluations; the motivation and the confidence evolution as a function of successes or failures.",robotics
10.1016/j.robot.2016.03.009,Journal,Robotics and Autonomous Systems,scopus,2016-07-01,sciencedirect,On-line expectation-based novelty detection for mobile robots,https://api.elsevier.com/content/abstract/scopus_id/84981765075,"This paper presents a recurrent neural network based novelty filter where a Scitos G5 mobile robot explored the environment and built dynamic models of observed sensory–motor values, then the acquired models of normality are used to predict the expected future values of sensory–motor inputs during patrol. Novelties could be detected whenever the prediction error between models-predicted values and actual observed values exceeded a local novelty threshold. The network is trained on-line; it grows by inserting new nodes when abnormal observation is perceived from the environment; and also shrinks when the learned information is not necessary anymore. In addition, the network is also capable of learning region-specific novelty thresholds on-line continuously.
                  To evaluate the proposed algorithm, real-world robotic experiments were conducted by fusing sensory perceptions (vision and laser sensors) and the robot motor control outputs (translational and rotational velocities). Experimental results showed that all of the novelty cases were highlighted by the proposed algorithms and it produced reliable local novelty thresholds while the robot patrols in the noisy environment. The statistical analysis showed that there was a strong correlation between the novelty filter responses and the actual novelty status. Furthermore, the filter was also compared with another novelty filter and the results showed that the proposed system performed better novelty detection.",robotics
10.1016/j.biosystems.2016.05.007,Journal,BioSystems,scopus,2016-07-01,sciencedirect,Robotic action acquisition with cognitive biases in coarse-grained state space,https://api.elsevier.com/content/abstract/scopus_id/84973440991,"Some of the authors have previously proposed a cognitively inspired reinforcement learning architecture (LS-Q) that mimics cognitive biases in humans. LS-Q adaptively learns under uniform, coarse-grained state division and performs well without parameter tuning in a giant-swing robot task. However, these results were shown only in simulations. In this study, we test the validity of the LS-Q implemented in a robot in a real environment. In addition, we analyze the learning process to elucidate the mechanism by which the LS-Q adaptively learns under the partially observable environment. We argue that the LS-Q may be a versatile reinforcement learning architecture, which is, despite its simplicity, easily applicable and does not require well-prepared settings.",robotics
10.1016/j.neucom.2016.01.005,Journal,Neurocomputing,scopus,2016-05-26,sciencedirect,Improvements on parsimonious extreme learning machine using recursive orthogonal least squares,https://api.elsevier.com/content/abstract/scopus_id/84971461271,"Recently novel constructive and destructive parsimonious extreme learning machines (CP-ELM and DP-ELM) arose to cope with regression problems. With these foundations, several improvements on CP-ELM and DP-ELM are suggested. CP-ELM can be improved by replacing the Givens rotation with the Householder transformation, yielding the improved CP-ELM (ICP-ELM) which results in the acceleration of the training speed without hampering the generalization performance. Subsequently, a hybrid constructive–destructive ELM (CDP-ELM) is generated integrating elements from CP-ELM and DP-ELM. The goal is to combine the advantages of training speed and parsimony from CP-ELM and DP-ELM. Finally, experiments on regression data sets and a real-world system identification of robot arm example are done to test the feasibility and efficacy of these variants including ICP-ELM and CDP-ELM.",robotics
10.1016/j.neucom.2015.11.085,Journal,Neurocomputing,scopus,2016-03-19,sciencedirect,Adaptive chaotification of robot manipulators via neural networks with experimental evaluations,https://api.elsevier.com/content/abstract/scopus_id/84951774957,"Chaotification is a problem that has been studied in recent years. It consists in injecting a chaotic behavior by means of a control scheme to a system, which in natural form does not present it. This paper explores the chaotification (also denoted anticontrol of chaos) of robot manipulators. Adaptive neural networks have the advantage of compensating the dynamics of a system with practically null information about this. By using a Lyapunov-like framework, chaotification of robot manipulators is assured with an adaptive neural network control law. A two layer neural network is used. Adaptation of the output weights are designed. Real-time experiments in a two degrees-of-freedom robot are presented. The new neural network-based controller is compared theoretically and experimentally with respect to a regressor-based controller.",robotics
10.1016/j.asoc.2015.11.014,Journal,Applied Soft Computing Journal,scopus,2016-03-01,sciencedirect,Wavenet fuzzy PID controller for nonlinear MIMO systems: Experimental validation on a high-end haptic robotic interface,https://api.elsevier.com/content/abstract/scopus_id/84951836601,"A novel global PID control scheme for nonlinear MIMO systems is proposed and implemented for a robot as study case, this scheme is called AWFPID from its adaptive wavelet fuzzy PID control structure. Basically, it identifies inverse error dynamics using a radial basis neural network with daughter RASP1 wavelets activation function; its output is in cascaded with an infinite impulse response (IIR) filter to prune irrelevant signals and nodes as well as to recover a canonical form. Then, online adaptive fuzzy tuning of a discrete PID regulator is proposed, whose closed-loop guarantees global regulation for nonlinear dynamical plants. The wavelet network includes a fuzzy inference system for online tuning of learning rates. A real-time experimental study on a three degrees of freedom haptic interface, the PHANToM Premium 1.0A, highlights the regulation with smooth control effort without using the mathematical model of the robot.",robotics
10.1016/j.neucom.2015.11.026,Journal,Neurocomputing,scopus,2016-02-12,sciencedirect,Digital implementations of thalamocortical neuron models and its application in thalamocortical control using FPGA for Parkinson's disease,https://api.elsevier.com/content/abstract/scopus_id/84959510221,"Due to the relay ability of sensory information and communication between cortical regions, the thalamocortical (TC) relay neuron plays an essential role in the therapy of Parkinson׳s disease. This paper first explores a series of efficient methods for the hardware implementation of TC relay neuron models, aiming to reproduce relevant biological behaviors and present appropriate feedback control in neural dynamics in thalamic systems. In addition, a modified two-dimensional TC neuron model is presented for convenient realization to decrease the complexity of the original model and promote the feasibility of the digital design, which shows significance for the large-scale network simulation of TC-based networks and the establishment of digital thalamus. A system-on-a-chip model-based control system is implemented on an FPGA using the modified TC neuron model, which is aimed at the real-time feedback control of tremor dominant Parkinsonian state. In this paper, the hardware syntheses and theoretical researches are given to illustrate the outstanding performance of the presented hardware implementation. The presented platform can be applied in both the brain-machine interface and the robotic control projects, and the proposed modular hardware framework can be extended to the real-time closed-loop treatments of other dyskinesia diseases.",robotics
10.1016/j.neucom.2015.11.029,Journal,Neurocomputing,scopus,2016-02-12,sciencedirect,Multiple task learning with flexible structure regularization,https://api.elsevier.com/content/abstract/scopus_id/84959482110,"Due to the theoretical advances and empirical successes, Multi-task Learning (MTL) has become a popular design paradigm for training a set of tasks jointly. Through exploring the hidden relationships among multiple tasks, many MTL algorithms have been developed to enhance learning performance. In general, the complicated hidden relationships can be considered as a combination of two key structural elements: task grouping and task outlier. Based on such task relationship, here we propose a generic MTL framework with flexible structure regularization, which aims in relaxing any type of specific structure assumptions. In particular, we directly impose a joint 
                        
                           
                              ℓ
                           
                           
                              11
                           
                        
                        /
                        
                           
                              ℓ
                           
                           
                              21
                           
                        
                     -norm as the regularization term to reveal the underlying task relationship in a flexible way. Such a flexible structure regularization term takes into account any convex combination of grouping and outlier structural characteristics among the multiple tasks. In order to derive efficient solutions for the generic MTL framework, we develop two algorithms, i.e., the Iteratively Reweighted Least Square (IRLS) method and the Accelerated Proximal Gradient (APG) method, with different emphasis and strength. In addition, the theoretical convergence and performance guarantee are analyzed for both algorithms. Finally, extensive experiments over both synthetic and real data, and the comparisons with several state-of-the-art algorithms demonstrate the superior performance of the proposed generic MTL method.",robotics
10.1016/j.procs.2016.07.425,Conference Proceeding,Procedia Computer Science,scopus,2016-01-01,sciencedirect,PlaNeural: Spiking Neural Networks that Plan,https://api.elsevier.com/content/abstract/scopus_id/85006365013,"PlaNeural is a spike-based neural network that has the ability to plan. The network is a spreading activation network implemented with Cell Assemblies; this combination has built a dynamic network of nodes that is able to interact with an environment and respond appropriately. PlaNeural uses Cell Assemblies to make decisions and plan - there is no pre-determined code managing the decision process that leads to planning. PlaNeural is the planning component of a virtual robot in a virtual environment. This paper describes PlaNeural's behaviour in two virtual environments, programmed independently of it; actions are completed in a closed-loop. PlaNeural was programmed in PyNN, executed with Nest and on a neuromorphic platform, SpiNNaker. PlaNeural has been tested on two environments and results show a successful performance; in both cases PlaNeural takes appropriate actions to fulfil user selected goals based on environmental changes.",robotics
10.1016/j.ifacol.2016.07.192,Conference Proceeding,IFAC-PapersOnLine,scopus,2016-01-01,sciencedirect,Proactive Teaching of Mechatronics in Master Courses – Project Case Study,https://api.elsevier.com/content/abstract/scopus_id/84994890825,"University education of young people in very complex branches like robotics is nowadays an issue. Methods like e-learning, distance education etc. are implemented. The authors of the article consider that practice courses play an irreplaceable role in mechatronics education. The paper deals with description of internship concept of French Military Academy of Saint-Cyr for master degree students. A real system of robot arm manipulator playing a Tic Tac Toe game with a human is presented as a case study of a typical issue solved by the students.",robotics
10.1016/j.ifacol.2016.10.527,Conference Proceeding,IFAC-PapersOnLine,scopus,2016-01-01,sciencedirect,Place Concept Learning by hMLDA Based on Position and Vision Information,https://api.elsevier.com/content/abstract/scopus_id/84994888257,"In this study, we propose a novel method that enables robots to autonomously form place concepts by using hierarchical Multimodal Latent Dirichlet Allocation (hMLDA) based on position and vision information. Generally, Simultaneous Localization and Mapping (SLAM) is used to identify the self-position of a robot on a metric map. In contrast, human beings frequently use place concepts that are defined by the name of a place and its spatial extent such as ”kitchen,” ”meeting space,” and ”in front of the TV.” The realization of human-life support by robots would require robots to learn place concepts such as these and to use them to collaborate with human beings. The proposed method enables robots to autonomously form hierarchical place concepts by using hMLDA, which stochastically integrates position information obtained by Monte Carlo Localization (MCL) and vision information obtained by a Convolutional Neural Network (CNN). Evaluation experiments using a robot in a real environment demonstrated the applicability of the hierarchical place concept formed by the proposed method.",robotics
10.1016/j.ifacol.2016.10.485,Conference Proceeding,IFAC-PapersOnLine,scopus,2016-01-01,sciencedirect,"Towards Visual Detection, Mapping and Quantification of Posidonia Oceanica using a Lightweight AUV",https://api.elsevier.com/content/abstract/scopus_id/84994128037,"Posidonia Oceanica (P.O.) is a Mediterranean endemic seagrass strongly related to the health of the coastal ecosystems. Monitoring the presence and state of P.O. is essential not only for safeguarding the shallow-water life diversity, but also as an indicator of the water quality. Nowadays, the control of P.O. is done by divers in successive missions of a duration limited by the capacity of the scuba tanks. This paper proposes the application of robotic and computer vision technologies to upgrade these current methods, namely: 1) employing a lightweight Autonomous Underwater Vehicle (AUV) equipped with cameras to survey and image marine areas, 2) the automatic discrimination of P.O. from the rest of the seafloor, using several techniques based on image texture analysis and machine learning, and, 3) the fast computation of 2D maps (photo-mosaics) of the surveyed areas from all the images included in the grabbed video sequences; these mosaics are extremely useful to measure the real extension of the meadows and some of the descriptors needed for a biological analysis. Experiments conducted with an AUV in several marine areas of Mallorca reveal promising results in the discrimination of different patterns of P.O. and in the construction of highly realistic photo-mosaics of the surveyed areas.",robotics
10.1016/j.procs.2016.05.404,Conference Proceeding,Procedia Computer Science,scopus,2016-01-01,sciencedirect,An evolutionary algorithm for autonomous robot navigation,https://api.elsevier.com/content/abstract/scopus_id/84978471421,"This paper presents an implementation of an evolutionary algorithm to control a robot with autonomous navigation in avoiding obstacles. The paper describes how the evolutionary system controls the sensors and motors in order to complete this task. A simulator was developed to test the algorithm and its configurations. The tests were performed in a simulated environment containing a set of barriers that were observed by means of a set of sensors. The solution obtained in the simulator was embedded in a real robot, which was tested in an arena containing obstacles. The robot was able to navigate and avoid the obstacles in this environment.",robotics
10.1016/j.bbe.2016.01.002,Journal,Biocybernetics and Biomedical Engineering,scopus,2016-01-01,sciencedirect,Human impedance parameter estimation using artificial neural network for modelling physiotherapist motion,https://api.elsevier.com/content/abstract/scopus_id/84964756545,"Physiotherapy (physical therapy) is a form of therapy aimed at regaining patients their bodily limb motor functions. The use of what are called therapeutic exercise robots for such purposes is gradually increasing. Therapeutic exercise robots have been developed for lower and upper limbs. These robots lighten the workload of physiotherapists (PTs) by providing the movements on patients’ relevant limbs. In order to get robots to perform the movements that the PT expects the patient to perform, it is required to determine the mechanical impedance parameters (inertia, stiffness and damping) due to the contact between the PT and patient's limb's, and to ensure that the robot moves according to these parameters. The aim of this study is to estimate these impedance parameters by using artificial neural networks (ANNs). Data from experiments on real subjects were used to train the network, and success was obtained using new data not presented to the network before. Subsequently, the previously acquired output was re-directed to the network with the purpose of developing a network, which can learn more accurately. Results have provided the designed ANN structure can generate necessary impedance parameter value to imitate PT motions.",robotics
10.1016/j.robot.2014.09.028,Journal,Robotics and Autonomous Systems,scopus,2016-01-01,sciencedirect,Leader following: A study on classification and selection,https://api.elsevier.com/content/abstract/scopus_id/84948583085,"This work proposes a different form of robotic navigation in dynamic environments, where the robot takes advantage of the motion of pedestrians, in order to improve its own navigation capabilities. Instead of treating persons as dynamic obstacles that should be avoided, here they are treated as special agents with an expert knowledge on navigating in dynamic scenarios. This work proposes that the robot selects and follows leaders, in order to move along optimal paths, deviate from undetected obstacles, improve navigation in densely populated areas and increase its acceptance by other humans. To accomplish this proposition, two novel approaches are developed in the area of leader selection. In the first, a motion prediction approach is used, to detect candidates that are moving to the same place that the robot is. In the second, a machine learning algorithm is trained with real examples and is used to select the best leader among several candidates. Experiments with a real robot are performed to validate the proposed approaches.",robotics
10.1016/j.isatra.2015.09.003,Journal,ISA Transactions,scopus,2015-11-01,sciencedirect,Neural networks for tracking of unknown SISO discrete-time nonlinear dynamic systems,https://api.elsevier.com/content/abstract/scopus_id/84947584344,"This article presents a Lyapunov function based neural network tracking (LNT) strategy for single-input, single-output (SISO) discrete-time nonlinear dynamic systems. The proposed LNT architecture is composed of two feedforward neural networks operating as controller and estimator. A Lyapunov function based back propagation learning algorithm is used for online adjustment of the controller and estimator parameters. The controller and estimator error convergence and closed-loop system stability analysis is performed by Lyapunov stability theory. Moreover, two simulation examples and one real-time experiment are investigated as case studies. The achieved results successfully validate the controller performance.",robotics
10.1016/j.neunet.2015.07.017,Journal,Neural Networks,scopus,2015-11-01,sciencedirect,Cost-efficient FPGA implementation of basal ganglia and their Parkinsonian analysis,https://api.elsevier.com/content/abstract/scopus_id/84940021490,"The basal ganglia (BG) comprise multiple subcortical nuclei, which are responsible for cognition and other functions. Developing a brain–machine interface (BMI) demands a suitable solution for the real-time implementation of a portable BG. In this study, we used a digital hardware implementation of a BG network containing 256 modified Izhikevich neurons and 2048 synapses to reliably reproduce the biological characteristics of BG on a single field programmable gate array (FPGA) core. We also highlighted the role of Parkinsonian analysis by considering neural dynamics in the design of the hardware-based architecture. Thus, we developed a multi-precision architecture based on a precise analysis using the FPGA-based platform with fixed-point arithmetic. The proposed embedding BG network can be applied to intelligent agents and neurorobotics, as well as in BMI projects with clinical applications. Although we only characterized the BG network with Izhikevich models, the proposed approach can also be extended to more complex neuron models and other types of functional networks.",robotics
10.1016/j.csl.2015.03.007,Journal,Computer Speech and Language,scopus,2015-11-01,sciencedirect,Reinforcement-learning based dialogue system for human-robot interactions with socially-inspired rewards,https://api.elsevier.com/content/abstract/scopus_id/84938086164,"This paper investigates some conditions under which polarized user appraisals gathered throughout the course of a vocal interaction between a machine and a human can be integrated in a reinforcement learning-based dialogue manager. More specifically, we discuss how this information can be cast into socially-inspired rewards for speeding up the policy optimisation for both efficient task completion and user adaptation in an online learning setting. For this purpose a potential-based reward shaping method is combined with a sample efficient reinforcement learning algorithm to offer a principled framework to cope with these potentially noisy interim rewards. The proposed scheme will greatly facilitate the system's development by allowing the designer to teach his system through explicit positive/negative feedbacks given as hints about task progress, in the early stage of training. At a later stage, the approach will be used as a way to ease the adaptation of the dialogue policy to specific user profiles. Experiments carried out using a state-of-the-art goal-oriented dialogue management framework, the Hidden Information State (HIS), support our claims in two configurations: firstly, with a user simulator in the tourist information domain (and thus simulated appraisals), and secondly, in the context of man–robot dialogue with real user trials.",robotics
10.1016/j.csl.2015.03.010,Journal,Computer Speech and Language,scopus,2015-11-01,sciencedirect,The roles and recognition of Haptic-Ostensive actions in collaborative multimodal human-human dialogues,https://api.elsevier.com/content/abstract/scopus_id/84938061943,"The RoboHelper project has the goal of developing assistive robots for the elderly. One crucial component of such a robot is a multimodal dialogue architecture, since collaborative task-oriented human–human dialogue is inherently multimodal. In this paper, we focus on a specific type of interaction, Haptic-Ostensive (H-O) actions, that are pervasive in collaborative dialogue. H-O actions manipulate objects, but they also often perform a referring function.
                  We collected 20 collaborative task-oriented human–human dialogues between a helper and an elderly person in a realistic setting. To collect the haptic signals, we developed an unobtrusive sensory glove with pressure sensors. Multiple annotations were then conducted to build the Find corpus. Supervised machine learning was applied to these annotations in order to develop reference resolution and dialogue act classification modules. Both corpus analysis, and these two modules show that H-O actions play a crucial role in interaction: models that include H-O actions, and other extra-linguistic information such as pointing gestures, perform better.
                  For true human–robot interaction, all communicative intentions must of course be recognized in real time, not on the basis of annotated categories. To demonstrate that our corpus analysis is not an end in itself, but can inform actual human–robot interaction, the last part of our paper presents additional experiments on recognizing H-O actions from the haptic signals measured through the sensory glove. We show that even though pressure sensors are relatively imprecise and the data provided by the glove is noisy, the classification algorithms can successfully identify actions of interest within subjects.",robotics
10.1016/j.neucom.2015.04.014,Journal,Neurocomputing,scopus,2015-10-20,sciencedirect,Data-driven heuristic dynamic programming with virtual reality,https://api.elsevier.com/content/abstract/scopus_id/84931577320,"In this paper, we propose a virtual reality (VR) platform as a case study of machine learning, in this case applied to the goal representation heuristic dynamic programming (GrHDP) approach. In general, a VR platform normally includes a physical module, a control/learning module, and a VR module. It facilitates machine learning research, where scientists and engineers can participate in the simulation process to analyze dynamic experiments. The internal structure of the VR platform can be replaced according to different research targets, so the platform can be extended to other applications. In this paper, we present the detailed VR design strategy, with a number of applications, including a triple-link inverted pendulum balancing problem, a maze navigation problem, and a robot navigation with obstacle avoidance.",robotics
10.1016/j.jneumeth.2015.06.015,Journal,Journal of Neuroscience Methods,scopus,2015-09-01,sciencedirect,Watching from a distance: A robotically controlled laser and real-time subject tracking software for the study of conditioned predator/prey-like interactions,https://api.elsevier.com/content/abstract/scopus_id/84936742109,"Background
                  The physical distance between predator and prey is a primary determinant of behavior, yet few paradigms exist to study this reliably in rodents.
               
                  New method
                  The utility of a robotically controlled laser for use in a predator–prey-like (PPL) paradigm was explored for use in rats. This involved the construction of a robotic two-dimensional gimbal to dynamically position a laser beam in a behavioral test chamber. Custom software was used to control the trajectory and final laser position in response to user input on a console. The software also detected the location of the laser beam and the rodent continuously so that the dynamics of the distance between them could be analyzed. When the animal or laser beam came within a fixed distance the animal would either be rewarded with electrical brain stimulation or shocked subcutaneously.
               
                  Results
                  Animals that received rewarding electrical brain stimulation could learn to chase the laser beam, while animals that received aversive subcutaneous shock learned to actively avoid the laser beam in the PPL paradigm. Mathematical computations are presented which describe the dynamic interaction of the laser and rodent.
               
                  Comparison with existing methods
                  The robotic laser offers a neutral stimulus to train rodents in an open field and is the first device to be versatile enough to assess distance between predator and prey in real time.
               
                  Conclusions
                  With ongoing behavioral testing this tool will permit the neurobiological investigation of predator/prey-like relationships in rodents, and may have future implications for prosthetic limb development through brain–machine interfaces.",robotics
10.1016/j.robot.2015.01.002,Journal,Robotics and Autonomous Systems,scopus,2015-09-01,sciencedirect,Autonomous learning of disparity-vergence behavior through distributed coding and population reward: Basic mechanisms and real-world conditioning on a robot stereo head,https://api.elsevier.com/content/abstract/scopus_id/84930764976,"A robotic system implementation that exhibits autonomous learning capabilities of effective control for vergence eye movements is presented. The system, directly relying on a distributed (i.e. neural) representation of binocular disparity, shows a large tolerance to the inaccuracies of real stereo heads and to the changeable environment. The proposed approach combines early binocular vision mechanisms with basic learning processes, such as synaptic plasticity and reward modulation. The computational substrate consists of a network of modeled V1 complex cells that act as oriented binocular disparity detectors. The resulting population response, besides implicit binocular depth cues about the environment, also provides a global signal (i.e. the overall activity of the population itself) to describe the state of the system and thus its deviation from the desired vergence position. The proposed network, by taking into account the modification of its internal state as a consequence of the action performed, evolves following a differential Hebbian rule. The overall activity of the population is exploited to derive an intrinsic signal that drives the weights update. Exploiting this signal implies a maximization of the population activity itself, thus providing an highly effective reward for the developing of a stable and accurate vergence behavior. The role of the different orientations in the learning process is evaluated separately against the whole population, evidencing that the interplay among the differently oriented channels allows a faster learning capability and a more accurate control. The efficacy of the proposed intrinsic reward signal is thus comparatively assessed against the ground-truth signal (the actual disparity) providing equivalent results, and thus validating the approach. Trained in a simulated environment, the proposed network, is able to cope with vergent geometry and thus to learn effective vergence movements for static and moving visual targets. Experimental tests with real robot stereo pairs demonstrate the capability of the architecture not just to directly learn from the environment, but to adapt the control to the stimulus characteristics.",robotics
10.1016/j.robot.2014.11.006,Journal,Robotics and Autonomous Systems,scopus,2015-09-01,sciencedirect,Self-calibrating smooth pursuit through active efficient coding,https://api.elsevier.com/content/abstract/scopus_id/84930730090,This paper presents a model for the autonomous learning of smooth pursuit eye movements based on an efficient coding criterion for active perception. This model accounts for the joint development of visual encoding and eye control. Sparse coding models encode the incoming data at two different spatial resolutions and capture the statistics of the input in spatio-temporal basis functions. A reinforcement learner controls eye velocity so as to maximize a reward signal based on the efficiency of the encoding. We consider the embodiment of the approach in the iCub simulator and real robot. Motion perception and smooth pursuit control are not explicitly expressed as tasks for the robot to achieve but emerge as the result of the system’s active attempt to efficiently encode its sensory inputs. Experiments demonstrate that the proposed approach is self-calibrating and robust to strong perturbations of the perception–action link.,robotics
10.1016/j.chb.2015.03.062,Journal,Computers in Human Behavior,scopus,2015-09-01,sciencedirect,Comparative study of soft computing techniques for mobile robot navigation in an unknown environment,https://api.elsevier.com/content/abstract/scopus_id/84927168200,"An autonomous mobile robot operating in an unstructured environment must be able to deal with dynamic changes of the environment. Navigation and control of a mobile robot in an unstructured environment are one of the most challenging problems. Fuzzy logic control is a useful tool in the field of navigation of mobile robot. In this research, fuzzy logic controller is optimized by integrating fuzzy logic with other soft computing techniques like genetic algorithm, neural networks, and Particle Swarm Optimization (PSO). Soft computing techniques are used in this work to tune the membership function parameters of fuzzy logic controller to improve the navigation performance. Four methods have been designed and implemented: manually constructed fuzzy logic (M-Fuzzy), fuzzy logic with genetic algorithm (GA-Fuzzy), fuzzy logic with neural network (Neuro-Fuzzy), and fuzzy logic with PSO (PSO-Fuzzy). The performances of these approaches are compared through computer simulations and experiment number of scenarios using Khepera III mobile robot platform. Hybrid fuzzy logic controls with soft computing techniques are found to be most efficient for mobile robot navigation. The GA-Fuzzy technique is found to perform better than the other techniques in most of the test scenarios in terms of travelling time and average speed. The performances of both PSO-Fuzzy and Neuro-Fuzzy are found to be better than the other methods in terms of distance travelled. In terms of bending energy, the PSO-Fuzzy and Neuro-Fuzzy are found to be better in simulation results. Although, the M-Fuzzy is found to be better using real experimental results. Hence, the most important system parameter will dictate which of the four methods to use.",robotics
10.1016/j.asoc.2015.07.009,Journal,Applied Soft Computing,scopus,2015-08-03,sciencedirect,Fuzzy classification of pre-harvest tomatoes for ripeness estimation - An approach based on automatic rule learning using decision tree,https://api.elsevier.com/content/abstract/scopus_id/84938634597,"Tomato (Solanum lycopersicum) ripeness estimation is an important process that affects its quality evaluation and marketing. However, the slow speed, subjectivity, time consumption associated with manual assessment has been forcing the agriculture industry to apply automation through robots. The vision system of harvesting robot is responsible for two-tasks. The first task is the recognition of object (tomato) and second is the classification of recognized objects (tomatoes). In this paper, Fuzzy Rule-Based Classification approach (FRBCS) has been proposed to estimate the ripeness of tomatoes based on color. The two color depictions: red-green color difference and red-green color ratio are derived from extracted RGB color information. These are then compared as a criterion for classification. Fuzzy partitioning of the feature space into linguistic variables is done by means of a learning algorithm. A rule set is automatically generated from the derived feature set using Decision Trees. Mamdani fuzzy inference system is adopted for building the fuzzy rule based classification system that classifies the tomatoes into six maturity stages. Dataset used for experiments has been created using the real images that were collected from a farm. 70% of the total images were used for training and 30% images of the total were used for testing the dataset respectively. Training dataset is divided into six classes representing the six different stages of tomato ripeness. Experimental results showed the system achieved the ripeness classification accuracy of 94.29% using proposed FRBCS.",robotics
10.1016/j.ifacol.2015.08.106,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-07-01,sciencedirect,Path planning and motion coordination for multi-robots system using probabilistic neuro-fuzzy,https://api.elsevier.com/content/abstract/scopus_id/84992476822,"In this paper, a Neuro-fuzzy and fuzzy probabilistic coordination and path planning for multiple mobile robots are presented. The coordination relies on a leader-followers conception which means related to the leader position, the followers will behave. The method consists of two fuzzy level controllers architecture based on a fuzzy probabilistic control and an Adaptive Neuro-Fuzzy Inference System (ANFIS). Each robot has low level probabilistic fuzzy controller to eliminate the stochastic uncertainties as well as to make the multi-robots team navigates from the start point to the target point without any dangerous collision. The first order Sugeno fuzzy inference system is utilized to model the leader robot system and create the high level controller. The approach starts by generating the input/output data. Then, the subtractive clustering algorithm along with least square estimation (LSE) generates the fuzzy rules that describe the relationship between input/output data. A learning algorithm based on neural network is developed to tune parameters of membership function and the fuzzy rules are tuned by ANFIS. The feasibility and effectiveness of the proposed approach is verified by simulation. The simulation results demonstrate the effectiveness of the proposed system. In addition, some parts of the proposed approach verified by experiments on real robot.",robotics
10.1016/j.robot.2015.01.007,Journal,Robotics and Autonomous Systems,scopus,2015-06-01,sciencedirect,Interval type-2 fuzzy logic based multiclass ANFIS algorithm for real-time EEG based movement control of a robot arm,https://api.elsevier.com/content/abstract/scopus_id/84925374323,"Brain–computer interfacing is an emerging field of research where signals extracted from the human brain are used for decision making and generation of control signals. Selection of the right classifier to detect the mental states from electroencephalography (EEG) signal is an open area of research because of the signal’s non-stationary and Ergodic nature. Though neural network based classifiers, like Adaptive Neural Fuzzy Inference System (ANFIS), act efficiently, to deal with the uncertainties involved in EEG signals, we have introduced interval type-2 fuzzy system in the fray to improve its uncertainty handling. Also, real-time scenarios require a classifier to detect more than two mental states. Thus, a multi-class discriminating algorithm based on the fusion of interval type-2 fuzzy logic and ANFIS, is introduced in this paper. Two variants of this algorithm have been developed on the basis of One-Vs-All and One-Vs-One methods. Both the variants have been tested on an experiment involving the real-time control of robot arm, where both the variants of the proposed classifier, produces an average success rate of reaching a target to 65% and 70% respectively. The result shows the competitiveness of our algorithm over other standard ones in the domain of non-stationary and uncertain signal data classification.",robotics
10.1016/j.robot.2015.02.006,Journal,Robotics and Autonomous Systems,scopus,2015-06-01,sciencedirect,Intrinsically motivated learning systems based on biologically-inspired novelty detection,https://api.elsevier.com/content/abstract/scopus_id/84925359697,"Intrinsic motivations play an important role in human learning, particularly in the early stages of childhood development, and ideas from this research field have influenced robotic learning and adaptability. In this paper we investigate one specific type of intrinsic motivation, that of novelty detection and we discuss the reasons that make it a powerful facility for continuous learning. We formulate and present one original type of biologically inspired novelty detection architecture and implement it on a robotic system engaged in a perceptual classification task. The results of real-world robot experiments we conducted show how this original architecture conforms to behavioural observations and demonstrate its effectiveness in terms of focusing the system’s attention in areas that are potential for effective learning.",robotics
10.1016/j.ifacol.2015.06.036,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-05-01,sciencedirect,Dexrov: Dexterous undersea inspection and maintenance in presence of communication latencies,https://api.elsevier.com/content/abstract/scopus_id/84992521813,"Underwater inspection and maintenance (e.g. in the oil & gas industry) are demanding and costly activities for which ROV based setups are often deployed in addition or in substitution to deep divers - contributing to operations risks and costs cutting. However the operation of a ROV requires significant off-shore dedicated manpower to handle and operate the robotic platform. In order to reduce the burden of operations, DexROV proposes to work out more cost effective and time efficient ROV operations, where manned support is in a large extent delocalized onshore (i.e. from a ROV control center), possibly at a large distance from the actual operations, relying on satellite communications. The proposed scheme also makes provision for advanced dexterous manipulation capabilities, exploiting human expertise when deemed useful. The outcomes of the project will be integrated and evaluated in a series of tests and evaluation campaigns, culminating with a realistic deep sea (1,300 meters) trial.",robotics
10.1016/j.neuron.2015.03.036,Journal,Neuron,scopus,2015-04-08,sciencedirect,Brain-machine interfaces beyond neuroprosthetics,https://api.elsevier.com/content/abstract/scopus_id/84930370551,"The field of invasive brain-machine interfaces (BMIs) is typically associated with neuroprosthetic applications aiming to recover loss of motor function. However, BMIs also represent a powerful tool to address fundamental questions in neuroscience. The observed subjects of BMI experiments can also be considered as indirect observers of their own neurophysiological activity, and the relationship between observed neurons and (artificial) behavior can be genuinely causal rather than indirectly correlative. These two characteristics defy the classical object-observer duality, making BMIs particularly appealing for investigating how information is encoded and decoded by neural circuits in real time, how this coding changes with physiological learning and plasticity, and how it is altered in pathological conditions. Within neuroengineering, BMI is like a tree that opens its branches into many traditional engineering fields, but also extends deep roots into basic neuroscience beyond neuroprosthetics.",robotics
10.1016/j.ins.2014.05.001,Journal,Information Sciences,scopus,2015-02-10,sciencedirect,Multiple chaotic central pattern generators with learning for legged locomotion and malfunction compensation,https://api.elsevier.com/content/abstract/scopus_id/84922243705,"An originally chaotic system can be controlled into various periodic dynamics. When it is implemented into a legged robot’s locomotion control as a central pattern generator (CPG), sophisticated gait patterns arise so that the robot can perform various walking behaviors. However, such a single chaotic CPG controller has difficulties dealing with leg malfunction. Specifically, in the scenarios presented here, its movement permanently deviates from the desired trajectory. To address this problem, we extend the single chaotic CPG to multiple CPGs with learning. The learning mechanism is based on a simulated annealing algorithm. In a normal situation, the CPGs synchronize and their dynamics are identical. With leg malfunction or disability, the CPGs lose synchronization leading to independent dynamics. In this case, the learning mechanism is applied to automatically adjust the remaining legs’ oscillation frequencies so that the robot adapts its locomotion to deal with the malfunction. As a consequence, the trajectory produced by the multiple chaotic CPGs resembles the original trajectory far better than the one produced by only a single CPG. The performance of the system is evaluated first in a physical simulation of a quadruped as well as a hexapod robot and finally in a real six-legged walking machine called AMOSII. The experimental results presented here reveal that using multiple CPGs with learning is an effective approach for adaptive locomotion generation where, for instance, different body parts have to perform independent movements for malfunction compensation.",robotics
10.1016/j.neucom.2014.07.061,Journal,Neurocomputing,scopus,2015-02-03,sciencedirect,Control of a direct drive robot using fuzzy spiking neural networks with variable structure systems-based learning algorithm,https://api.elsevier.com/content/abstract/scopus_id/85027934609,"In this work, a sliding mode theory based supervised training algorithm that implements fuzzy reasoning on a spiking neural network has been developed and tested on the trajectory control problem of a two-degrees-of-freedom direct drive robotic manipulator. To describe the generation of a new spike train from the incoming spike trains Spike Response Model has been utilized and the Lyapunov stability method has been adopted in the derivation of the update rules for the neurocontroller parameters. The results of the real-time experiments indicate that stable online tuning and fast learning speed are the prominent characteristics of the proposed algorithm.",robotics
10.1016/j.ifacol.2015.11.171,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-01-01,sciencedirect,Hierarchical Hybrid Control with Classical Planning and Trajectory Optimization,https://api.elsevier.com/content/abstract/scopus_id/84983197460,"One method of control of cyberphysical systems, particularly intelligent mobile robots, involves a hierarchy of high-level classical planning and low-level trajectory optimization. We present a new technique to incorporate trajectory costs in a high-level classical planning problem (CPP). The proposed approach is particularly suited to leverage existing algorithms for solving CPPs and, independently, algorithms for trajectory optimization. To this end, we introduce a family of graphs called lifted planning graphs parametrized by an integer H, and we map paths in these graphs to solutions of the CPP. We show that the overall cost of a high-level plan is a nonincreasing function of H, and that there exists a finite H for which an optimal path in the lifted planning graph is associated with the optimal solution of the CPP. For computational speed and future real-time implementation, we discuss incremental modification of paths in the lifted planning graphs for increasing values of H. We illustrate the proposed ideas with a numerical simulation example involving classical planning for Dubins vehicle routing.",robotics
10.1016/j.bica.2015.04.008,Journal,Biologically Inspired Cognitive Architectures,scopus,2015-01-01,sciencedirect,Automatic navigation of wall following mobile robot using Adaptive Resonance Theory of Type-1,https://api.elsevier.com/content/abstract/scopus_id/84960798237,"The automatic navigation of wall following robot is playing important role in various real world tasks such as underwater exploration, unmanned flight, and in automotive industries based on its computational complexity. In this work, a novel navigation approach based on biologically inspired neural network, known as “Adaptive Resonance Theory-1” which was proposed by Carpenter and Grossberg, has been implemented and investigated for navigation of wall following mobile robots. The proposed navigation algorithm is successfully tested with three sensor reading datasets obtained from clockwise navigation of SCITOS G5 mobile robot. Test decision accuracy (%), and simulation time were used as performance analysis parameters for the proposed algorithm and it has been found that the present work can achieve 99.59% of maximum decision accuracy.",robotics
10.1016/j.neucom.2015.04.066,Journal,Neurocomputing,scopus,2015-01-01,sciencedirect,A general CPG network and its implementation on the microcontroller,https://api.elsevier.com/content/abstract/scopus_id/84952631900,"Over the last few years, it has been confirmed by the biologists that Central Pattern Generator (CPG) is the key mechanism of generating adaptive and versatile locomotion in animals. This gives a new inspiration of locomotion control for robots. Although the design of CPG controller using coupled oscillators has been proposed previously, it cannot comprehensively reproduce different rhythmic motion along with smooth transitions to mimic the versatility of animal locomotion. To tackle this problem, we propose a general CPG model emphasizing on its stability analysis, smooth transition and implementation architecture. Global exponential stability of the model is derived by using strict mathematical analysis. Transitions between different oscillating forms are also smooth, and the implementation architecture has low computational cost, thus suitable for microcontrollers. Moreover, all control parameters not only have clear relationships with the physical outputs, but also can be modified online. Both virtual and real robotic fish are developed to verify the effectiveness of our CPG controller together with the proposed implementation architecture, through the experiments of four locomotion gaits and three transitions among them.",robotics
10.1016/j.ijleo.2015.07.153,Journal,Optik,scopus,2015-01-01,sciencedirect,Neural network based visual servo control for CNC load/unload manipulator,https://api.elsevier.com/content/abstract/scopus_id/84952362277,"A visual servo control strategy based on fuzzy-neural networks is proposed for an eye-in-hand CNC load/unload manipulator in this paper. As visual servo control is an uncertain nonlinear strong coupling system, the real-time computation of feature Jacobian matrix is very complicated, improving its poor real-time performance is a must. By approximating the mapping relationship between changes of target image features and robotic joints’ positions with fuzzy-neural networks, which has the advantages of strong learning capability and fast learning speed, a novel controller is designed to achieve an effective operation for CNC load/unload manipulator. The following experiment result indicates that compared with BP and RBF neural network the proposed visual servo controller is of higher precision and convergence rate, enhancing the robust capability and accelerating the response time of the control system.",robotics
10.1016/j.neunet.2015.10.002,Journal,Neural Networks,scopus,2015-01-01,sciencedirect,Bio-inspired homogeneous multi-scale place recognition,https://api.elsevier.com/content/abstract/scopus_id/84947969392,"Robotic mapping and localization systems typically operate at either one fixed spatial scale, or over two, combining a local metric map and a global topological map. In contrast, recent high profile discoveries in neuroscience have indicated that animals such as rodents navigate the world using multiple parallel maps, with each map encoding the world at a specific spatial scale. While a number of theoretical-only investigations have hypothesized several possible benefits of such a multi-scale mapping system, no one has comprehensively investigated the potential mapping and place recognition performance benefits for navigating robots in large real world environments, especially using more than two homogeneous map scales. In this paper we present a biologically-inspired multi-scale mapping system mimicking the rodent multi-scale map. Unlike hybrid metric-topological multi-scale robot mapping systems, this new system is homogeneous, distinguishable only by scale, like rodent neural maps. We present methods for training each network to learn and recognize places at a specific spatial scale, and techniques for combining the output from each of these parallel networks. This approach differs from traditional probabilistic robotic methods, where place recognition spatial specificity is passively driven by models of sensor uncertainty. Instead we intentionally create parallel learning systems that learn associations between sensory input and the environment at different spatial scales. We also conduct a systematic series of experiments and parameter studies that determine the effect on performance of using different neural map scaling ratios and different numbers of discrete map scales. The results demonstrate that a multi-scale approach universally improves place recognition performance and is capable of producing better than state of the art performance compared to existing robotic navigation algorithms. We analyze the results and discuss the implications with respect to several recent discoveries and theories regarding how multi-scale neural maps are learnt and used in the mammalian brain.",robotics
10.1016/j.cogsys.2015.06.001,Journal,Cognitive Systems Research,scopus,2015-01-01,sciencedirect,A single computational model for many learning phenomena,https://api.elsevier.com/content/abstract/scopus_id/84947868532,"Simplicity is a basic principle of science and this implies that, if we want to explain the behaviour of animals by constructing robots that behave like real animals, one and the same robot should reproduce as many behaviours and as many behavioural phenomena as possible. In this paper we describe robots that both evolve and learn in their “natural” environment and, in addition, learn in the equivalent of an experimental laboratory and reproduce a variety of results of experiments on learning in animals. We introduce a new model of learning in which the weights of the connections that link the units of the robots’ neural network are genetically inherited and do not change during the robots’ life but what changes during life and makes the robots learn new behaviours is the synaptic receptivity of a special set of network units which we call learning units. The robots evolve in a variety of different environments and they learn in a variety of different ways including imprinting and learning by imitating the behaviour of others. Then we test the robots in the controlled conditions of an artificial laboratory and we reproduce a number of experimental results on both operant learning and classical conditioning, including learning and extinction curves, the role of the temporal interval between conditioned and unconditioned stimuli, and the influence of motivation on learning.",robotics
10.1016/j.neunet.2015.07.004,Journal,Neural Networks,scopus,2015-01-01,sciencedirect,Neuromorphic implementations of neurobiological learning algorithms for spiking neural networks,https://api.elsevier.com/content/abstract/scopus_id/84945426263,"The application of biologically inspired methods in design and control has a long tradition in robotics. Unlike previous approaches in this direction, the emerging field of neurorobotics not only mimics biological mechanisms at a relatively high level of abstraction but employs highly realistic simulations of actual biological nervous systems. Even today, carrying out these simulations efficiently at appropriate timescales is challenging. Neuromorphic chip designs specially tailored to this task therefore offer an interesting perspective for neurorobotics. Unlike Von Neumann CPUs, these chips cannot be simply programmed with a standard programming language. Like real brains, their functionality is determined by the structure of neural connectivity and synaptic efficacies. Enabling higher cognitive functions for neurorobotics consequently requires the application of neurobiological learning algorithms to adjust synaptic weights in a biologically plausible way. In this paper, we therefore investigate how to program neuromorphic chips by means of learning. First, we provide an overview over selected neuromorphic chip designs and analyze them in terms of neural computation, communication systems and software infrastructure. On the theoretical side, we review neurobiological learning techniques. Based on this overview, we then examine on-die implementations of these learning algorithms on the considered neuromorphic chips. A final discussion puts the findings of this work into context and highlights how neuromorphic hardware can potentially advance the field of autonomous robot systems. The paper thus gives an in-depth overview of neuromorphic implementations of basic mechanisms of synaptic plasticity which are required to realize advanced cognitive capabilities with spiking neural networks.",robotics
10.1016/B978-0-12-800881-2.00006-2,Book,Household Service Robotics,scopus,2015-01-01,sciencedirect,A Household Service Robot with a Cellphone Interface,https://api.elsevier.com/content/abstract/scopus_id/84944415545,"In this chapter, an efficient and low-cost cellphone-commandable mobile manipulation system is described. Aiming at home use and elderly caring, this system can be easily commanded through a common cellphone network to efficiently grasp objects in a household environment, utilizing several low-cost off-the-shelf devices. Unlike the visual servo technology using a high quality vision system with the associated high cost, the household-service robot would not be able to afford such a high quality vision servo system, and thus it is essential to use some low-cost devices. However, it is extremely challenging to create such a vision system with precise localization, as well as motion control. To tackle this challenge, we developed a real-time vision system with which a reliable grasping algorithm combining machine vision, robotic kinematics and motor control technology is presented. After the target is captured by the arm camera, the arm camera keeps tracking the target while the arm keeps stretching until the end effector reaches the target. However, if the target is not captured by the arm camera, the arm will make a move to help the arm camera capture the target under the guidance of the head camera. This algorithm is implemented on two robot systems: one with a fixed base and another with a mobile base. The results demonstrate the feasibility and efficiency of the algorithm and system we developed, and our study shows the significance of developing a service robot in a modern household environment.",robotics
10.1016/B978-0-12-800881-2.00019-0,Book,Household Service Robotics,scopus,2015-01-01,sciencedirect,Implementation of Cognitive Controls for Robots,https://api.elsevier.com/content/abstract/scopus_id/84944402000,"Engineers have long used control systems utilizing models and feedback loops to control real-world systems. Limitations of model-based controls led to a generation of intelligent control techniques such as adaptive and fuzzy controls. The human brain, on the other hand, is known to process a variety of inputs in parallel, ignoring distractions to focus on the task in hand. This process, known as cognitive control in psychology, is unique to humans and some higher classes of animals. We are interested in implementing such cognitive control functionality in robots. This chapter tries to answer the following question: How could cognitive control functionality be implemented in HAM-inspired robots?",robotics
10.1016/j.bica.2015.06.007,Journal,Biologically Inspired Cognitive Architectures,scopus,2015-01-01,sciencedirect,"NARLE: Neurocognitive architecture for the autonomous task recognition, learning, and execution",https://api.elsevier.com/content/abstract/scopus_id/84941177769,"Robots controlled by the state of the art cognitive architectures are still far behind animals in their capabilities to learn complex skills and autonomously adapt to unexpected circumstances. The neurocognitive architecture proposed in this paper addresses the problem of learning and execution of hierarchical behaviors and complex skills. Learning is addressed both on the level of individual elementary behaviors and goal-directed sequences of actions. The proposed architecture comprises a Dynamic Neural Fields (DNFs) implementation of the low-level elementary behaviors and a Functional System Network (FSN) tying these behaviors in goal-directed sequences. The DNF framework enables a continuous, dynamical representation of perceptual features and motor parameters, which may be directly coupled to the robot’s sensors and motors. Attractor states and instabilities of the DNFs account for segregation of cognitive states and mark behaviorally relevant events in the continuous flow of sensorimotor dynamics. The FSN, in its turn, comprises dynamical elements that can be arranged in a multilayered network by a learning process, in which new layers and elementary behaviors are added on demand. In our architecture, the FSN controls adaptation processes in the already acquired neural-dynamic elementary behaviors, as well as formation of new elementary behaviors. Combination of the DNF and FSN frameworks in a neurocognitive architecture NARLE enables pervasive learning both on the level of individual behaviors and goal-directed sequence, contributing to the progress towards more adaptive intelligent robotic systems, capable to learn new tasks and extend their behavioral repertoire in stochastic real-world environments.",robotics
10.1016/j.procs.2015.07.309,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,WWN-8: Incremental online stereo with shape-from-X using life-long big data from multiple modalities,https://api.elsevier.com/content/abstract/scopus_id/84939200801,"When a child lives in the real world, from infancy to adulthood, his retinae receive a flood of stereo sensory stream. His muscles produce another action stream. How does the child's brain deal with such big data from multiple sensory modalities (left- and right-eye modalities) and multiple effector modalities (location, disparity map, and shape type)? This capability incrementally learns to produce simple-to-complex sensorimotor behaviors — autonomous development. We present a model that incrementally fuses such an open-ended life-long stream and updates the “brain” online so the perceived world is 3D. Traditional methods for shape- from-X use a particular type of cue X (e.g., stereo disparity, shading, etc.) to compute depths or local shapes based on a handcrafted physical model. Such a model likely results in a brit- tle system because of the fluctuation of the availability of the cue. An embodiment of the Developmental Network (DN), called Stereo Where-What Network (WWN-8), learns to per- form simultaneous attention and recognition, while developing invariances in location, disparity, shape, and surface type, so that multiple cues can automatically fill in if a particular type of cue (e.g., texture) is missing locally from the real world. We report some experiments: 1) dynamic synapse retraction and growth as a method of developing receptive fields. 2) training for recognizing 3D objects directly in cluttered natural backgrounds. 3) integration of depth perception with location and type information. The experiments used stereo images and motor actions on the order of 105 frames. Potential applications include driver assistance for road safety, mobile robots, autonomous navigation, and autonomous vision-guided manipulators.",robotics
10.1016/j.procs.2015.05.250,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,Pso-based distributed algorithm for dynamic task allocation in a robotic swarm,https://api.elsevier.com/content/abstract/scopus_id/84939163993,"Dynamic task allocation in a robotic swarm is a necessary process for proper management of the swarm. It allows the distribution of the identified tasks to be performed, among the swarm of robots, in such a way that a pre-defined proportion of execution of those tasks is achieved. In this context, there is no central unit to take care of the task allocation. So any algorithm proposal must be distributed, allowing every, and each robot in the swarm to identify the task it must perform. This paper proposes a distributed control algorithm to implement dynamic task allocation in a swarm robotics environment. The algorithm is inspired by the particle swarm optimization. In this context, each robot that integrates the swarm must run the algorithm periodically in order to control the underlying actions and decisions. The algorithm was implemented on ELISA III real swarm robots and extensively tested. The algorithm is effective and the corresponding performance is promising.",robotics
10.1016/j.procs.2015.05.056,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,Principles and experimentations of self-organizing embedded agents allowing learning from demonstration in ambient robotic,https://api.elsevier.com/content/abstract/scopus_id/84939161188,"Ambient systems are populated by many heterogeneous devices to provide adequate services to its users. The adaptation of an ambient system to the specific needs of its users is a challenging task. Because human-system interaction has to be as natural as possible, we propose an approach based on Learning from Demonstration (LfD). However, using LfD in ambient systems needs adaptivity of the learning technique. We present ALEX, a multi-agent system able to dynamically learn and reuse contexts from demonstrations performed by a tutor. Results of experiments performed on both a real and a virtual robot show interesting properties of our technology for ambient applications.",robotics
10.1016/j.nlm.2014.07.003,Journal,Neurobiology of Learning and Memory,scopus,2015-01-01,sciencedirect,A hierarchical model of goal directed navigation selects trajectories in a visual environment,https://api.elsevier.com/content/abstract/scopus_id/84920946400,"We have developed a Hierarchical Look-Ahead Trajectory Model (HiLAM) that incorporates the firing pattern of medial entorhinal grid cells in a planning circuit that includes interactions with hippocampus and prefrontal cortex. We show the model’s flexibility in representing large real world environments using odometry information obtained from challenging video sequences. We acquire the visual data from a camera mounted on a small tele-operated vehicle. The camera has a panoramic field of view with its focal point approximately 5cm above the ground level, similar to what would be expected from a rat’s point of view. Using established algorithms for calculating perceptual speed from the apparent rate of visual change over time, we generate raw dead reckoning information which loses spatial fidelity over time due to error accumulation. We rectify the loss of fidelity by exploiting the loop-closure detection ability of a biologically inspired, robot navigation model termed RatSLAM. The rectified motion information serves as a velocity input to the HiLAM to encode the environment in the form of grid cell and place cell maps. Finally, we show goal directed path planning results of HiLAM in two different environments, an indoor square maze used in rodent experiments and an outdoor arena more than two orders of magnitude larger than the indoor maze. Together these results bridge for the first time the gap between higher fidelity bio-inspired navigation models (HiLAM) and more abstracted but highly functional bio-inspired robotic mapping systems (RatSLAM), and move from simulated environments into real-world studies in rodent-sized arenas and beyond.",robotics
10.1016/j.neunet.2014.10.001,Journal,Neural Networks,scopus,2015-01-01,sciencedirect,Trends in extreme learning machines: A review,https://api.elsevier.com/content/abstract/scopus_id/84908682236,"Extreme learning machine (ELM) has gained increasing interest from various research fields recently. In this review, we aim to report the current state of the theoretical research and practical advances on this subject. We first give an overview of ELM from the theoretical perspective, including the interpolation theory, universal approximation capability, and generalization ability. Then we focus on the various improvements made to ELM which further improve its stability, sparsity and accuracy under general or specific conditions. Apart from classification and regression, ELM has recently been extended for clustering, feature selection, representational learning and many other learning tasks. These newly emerging algorithms greatly expand the applications of ELM. From implementation aspect, hardware implementation and parallel computation techniques have substantially sped up the training of ELM, making it feasible for big data processing and real-time reasoning. Due to its remarkable efficiency, simplicity, and impressive generalization performance, ELM have been applied in a variety of domains, such as biomedical engineering, computer vision, system identification, and control and robotics. In this review, we try to provide a comprehensive view of these advances in ELM together with its future perspectives.",robotics
10.1016/j.asoc.2014.09.021,Journal,Applied Soft Computing Journal,scopus,2015-01-01,sciencedirect,Learning fuzzy controllers in mobile robotics with embedded preprocessing,https://api.elsevier.com/content/abstract/scopus_id/84908428493,"The automatic design of controllers for mobile robots usually requires two stages. In the first stage, sensorial data are preprocessed or transformed into high level and meaningful values of variables which are usually defined from expert knowledge. In the second stage, a machine learning technique is applied to obtain a controller that maps these high level variables to the control commands that are actually sent to the robot. This paper describes an algorithm that is able to embed the preprocessing stage into the learning stage in order to get controllers directly starting from sensorial raw data with no expert knowledge involved. Due to the high dimensionality of the sensorial data, this approach uses Quantified Fuzzy Rules (QFRs), that are able to transform low-level input variables into high-level input variables, reducing the dimensionality through summarization. The proposed learning algorithm, called Iterative Quantified Fuzzy Rule Learning (IQFRL), is based on genetic programming. IQFRL is able to learn rules with different structures, and can manage linguistic variables with multiple granularities. The algorithm has been tested with the implementation of the wall-following behavior both in several realistic simulated environments with different complexity and on a Pioneer 3-AT robot in two real environments. Results have been compared with several well-known learning algorithms combined with different data preprocessing techniques, showing that IQFRL exhibits a better and statistically significant performance. Moreover, three real world applications for which IQFRL plays a central role are also presented: path and object tracking with static and moving obstacles avoidance.",robotics
10.1016/j.mechatronics.2014.07.001,Journal,Mechatronics,scopus,2014-12-01,sciencedirect,A task oriented haptic gait rehabilitation robot,https://api.elsevier.com/content/abstract/scopus_id/84914164679,"This paper presents the concept, design process, and the prototype of a novel haptics-based lower-extremity rehabilitation robot for bed-ridden stroke patients. This system, named Virtual Gait Rehabilitation Robot (ViGRR), is required to provide the average gait motion training as well as other targeted exercises such as leg press, stair stepping and motivational gaming, in order to facilitate motor learning and enable the training of daily activities such as walking and maintaining balance. The system requirements are laid out and linked to the design of a redundant planar 4DOF robot concept prototype. An iterative design optimization loop was setup to obtain the robot kinematic and dynamic parameters as well as the actuators. The robot’s mechanical design, model, safety features, admittance controllers, and the architecture of the haptic controller are presented. Preliminary experiments were planned and performed to evaluate the capability of the system in delivering task-based virtual-reality exercises and trajectory following scenarios.",robotics
10.1016/j.mechatronics.2014.05.007,Journal,Mechatronics,scopus,2014-12-01,sciencedirect,Learning rate free reinforcement learning for real-time motion control using a value-gradient based policy,https://api.elsevier.com/content/abstract/scopus_id/84914148728,"Reinforcement learning (RL) is a framework that enables a controller to find an optimal control policy for a task in an unknown environment. Although RL has been successfully used to solve optimal control problems, learning is generally slow. The main causes are the inefficient use of information collected during interaction with the system and the inability to use prior knowledge on the system or the control task. In addition, the learning speed heavily depends on the learning rate parameter, which is difficult to tune. In this paper, we present a sample-efficient, learning-rate-free version of the Value-Gradient Based Policy (VGBP) algorithm. The main difference between VGBP and other frequently used algorithms, such as Sarsa, is that in VGBP the learning agent has a direct access to the reward function, rather than just the immediate reward values. Furthermore, the agent learns a process model. This enables the algorithm to select control actions by optimizing over the right-hand side of the Bellman equation. We demonstrate the fast learning convergence in simulations and experiments with the underactuated pendulum swing-up task. In addition, we present experimental results for a more complex 2-DOF robotic manipulator.",robotics
10.1016/j.neucom.2012.12.073,Journal,Neurocomputing,scopus,2014-06-25,sciencedirect,Multi-layer quantum neural network controller trained by real-coded genetic algorithm,https://api.elsevier.com/content/abstract/scopus_id/84896549228,"We investigate a quantum neural network and discuss its application to controlling systems. First, we consider a multi-layer quantum neural network that uses qubit neurons as its information processing unit. Next, we propose a direct neural network controller using the multi-layer quantum neural network. To improve learning performance, instead of applying a back-propagation algorithm for the supervised training of the multi-layer quantum neural network, we apply a real-coded genetic algorithm. To evaluate the capabilities of the direct quantum neural network controller, we conduct computational experiments controlling a discrete-time nonlinear system and a nonholonomic system (a two-wheeled robot). Experimental results confirm the effectiveness of the real-coded genetic algorithm in training a quantum neural network and prove the feasibility and robustness of the direct quantum neural network controller.",robotics
10.1016/j.neucom.2013.11.037,Journal,Neurocomputing,scopus,2014-06-10,sciencedirect,Mimicking visual searching with integrated top down cues and low-level features,https://api.elsevier.com/content/abstract/scopus_id/84894564030,"Visual searching is a perception task involved with visual attention, attention shift and active scan of the visual environment for a particular object or feature. The key idea of our paper is to mimic the human visual searching under the static and dynamic scenes. To build up an artificial vision system that performs the visual searching could be helpful to medical and psychological application development to human machine interaction. Recent state-of-the-art researches focus on the bottom-up and top-down saliency maps. Saliency maps indicate that the saliency likelihood of each pixel, however, understanding the visual searching process can help an artificial vision system exam details in a way similar to human and they will be good for future robots or machine vision systems which is a deeper digest than the saliency map. This paper proposed a computational model trying to mimic human visual searching process and we emphasis the motion cues on the visual processing and searching. Our model analysis the attention shifts by fusing the top-down bias and bottom-up cues. This model also takes account the motion factor into the visual searching processing. The proposed model involves five modules: the pre-learning process; top-down biasing; bottom-up mechanism; multi-layer neural network and attention shifts. Experiment evaluation results via benchmark databases and real-time video showed the model demonstrated high robustness and real-time ability under complex dynamic scenes.",robotics
10.1016/j.neucom.2013.03.060,Journal,Neurocomputing,scopus,2014-05-20,sciencedirect,Autonomous UAV based search operations using constrained sampling evolutionary algorithms,https://api.elsevier.com/content/abstract/scopus_id/84896707806,"This paper introduces and studies the application of Constrained Sampling Evolutionary Algorithms in the framework of an UAV based search and rescue scenario. These algorithms have been developed as a way to harness the power of Evolutionary Algorithms (EA) when operating in complex, noisy, multimodal optimization problems and transfer the advantages of their approach to real time real world problems that can be transformed into search and optimization challenges. These types of problems are denoted as Constrained Sampling problems and are characterized by the fact that the physical limitations of reality do not allow for an instantaneous determination of the fitness of the points present in the population that must be evolved. A general approach to address these problems is presented and a particular implementation using Differential Evolution as an example of CS-EA is created and evaluated using teams of UAVs in search and rescue missions. The results are compared to those of a Swarm Intelligence based strategy in the same type of problem as this approach has been widely used within the UAV path planning field in different variants by many authors.",robotics
10.1016/j.patrec.2013.11.004,Journal,Pattern Recognition Letters,scopus,2014-03-01,sciencedirect,Comparison of different approaches to visual terrain classification for outdoor mobile robots,https://api.elsevier.com/content/abstract/scopus_id/84890280921,"In this paper, we present a comparison of multiple approaches to visual terrain classification for outdoor mobile robots based on different color, texture and local features. We introduce and compare three novel composite descriptors called CEDD, FCTH and JCD, with traditional color and texture descriptors, such as LTP, SCD, EHD and a descriptor called CSD–HTD generated by late fusion method. We also test three BOW models based on SIFT, SURF and ORB, respectively. We used two terrain classification datasets of which the images were captured from outdoor moving robots under different weather and ground conditions. Hence some of the images are blurred or unideally exposed. We utilize ELM, SVM and NN for classification to evaluate the performance of different combinations of image descriptors and classifiers. Experiments demonstrate that JCD can represent different terrain images with significant inter-class discrepancies, and ELM has mild optimization constraints and obtains better generalization performance. Results show that the approach based on JCD descriptor and ELM classifier performs best in term of classification effectiveness and it is suitable for real-time outdoor visual terrain classification.",robotics
10.1016/j.neucom.2013.05.044,Journal,Neurocomputing,scopus,2014-02-27,sciencedirect,Activity recognition with android phone using mixture-of-experts co-trained with labeled and unlabeled data,https://api.elsevier.com/content/abstract/scopus_id/84887606604,"As the number of smartphone users has grown recently, many context-aware services have been studied and launched. Activity recognition becomes one of the important issues for user adaptive services on the mobile phones. Even though many researchers have attempted to recognize a user's activities on a mobile device, it is still difficult to infer human activities from uncertain, incomplete and insufficient mobile sensor data. We present a method to recognize a person's activities from sensors in a mobile phone using mixture-of-experts (ME) model. In order to train the ME model, we have applied global–local co-training (GLCT) algorithm with both labeled and unlabeled data to improve the performance. The GLCT is a variation of co-training that uses a global model and a local model together. To evaluate the usefulness of the proposed method, we have conducted experiments using real datasets collected from Google Android smartphones. This paper is a revised and extended version of a paper that was presented at HAIS 2011.",robotics
10.1016/j.engappai.2013.12.003,Journal,Engineering Applications of Artificial Intelligence,scopus,2014-02-01,sciencedirect,Hardware opposition-based PSO applied to mobile robot controllers,https://api.elsevier.com/content/abstract/scopus_id/84892858663,"Adaptation of mobile robot controllers commonly requires the computation of optimal points of operation. Specifically, for miniature mobile robots with serious computational limitations, that are typical of embedded systems, one of the main challenges is the adaptation of efficient computational methods in order to find solutions of complex optimization problems, which demand large execution times. This drawback compels the design of high-performance parallel optimization algorithms which must run over embedded system platforms. This paper describes how adequate hardware implementations of the Particle Swarm Optimization (PSO) algorithm can be useful for real time adaptation of mobile robot controllers. For achieving this, a new architecture is proposed, which is based on an FPGA implementation of the opposition-based learning (OBL) approach applied to the PSO (for short HPOPSO), and which explores the intrinsic parallelism of this algorithm in order to adjust the weights of a neural robot controller in real time according to desired behaviors. The proposed HPOPSO was applied to the learning-from-demonstration problem in which a teacher performs executions of the desired behavior. Effectiveness of the proposed architecture was demonstrated by numerical simulations and the feasibility of the adaptive behavior of the neural robot controller was confirmed for two obstacle avoidance case studies that were preserved when one or more failures on the distance sensors occur. The HPOPSO, which uses the OBL technique, improves the quality of the solutions in comparison with the standard PSO. Comparisons of the adaptation time between hardware and software approaches have demonstrated the suitability of the FPGA implementation of the proposed HPOPSO for attending specific requirements of embedded system applications.",robotics
10.1016/j.robot.2014.08.002,Journal,Robotics and Autonomous Systems,scopus,2014-01-01,sciencedirect,Integrated neural and robotic simulations. Simulation of cerebellar neurobiological substrate for an object-oriented dynamic model abstraction process,https://api.elsevier.com/content/abstract/scopus_id/84908424049,"Experimental studies of the Central Nervous System (CNS) at multiple organization levels aim at understanding how information is represented and processed by the brain’s neurobiological substrate. The information processed within different neural subsystems is neurocomputed using distributed and dynamic patterns of neural activity. These emerging patterns can be hardly understood by merely taking into account individual cell activities. Studying how these patterns are elicited in the CNS under specific behavioral tasks has become a groundbreaking research topic in system neuroscience. This methodology of synthetic behavioral experimentation is also motivated by the concept of embodied neuroscience, according to which the primary goal of the CNS is to solve/facilitate the body–environment interaction.
                  With the aim to bridge the gap between system neuroscience and biological control, this paper presents how the CNS neural structures can be connected/integrated within a body agent; in particular, an efficient neural simulator based on EDLUT (Ros et al., 2006) has been integrated within a simulated robotic environment to facilitate the implementation of object manipulating closed loop experiments (action–perception loop). This kind of experiment allows the study of the neural abstraction process of dynamic models that occurs within our neural structures when manipulating objects.
                  The neural simulator, communication interfaces, and a robot platform have been efficiently integrated enabling real time simulations. The cerebellum is thought to play a crucial role in human-body interaction with a primary function related to motor control which makes it the perfect candidate to start building an embodied nervous system as illustrated in the simulations performed in this work.",robotics
10.1016/j.robot.2014.05.011,Journal,Robotics and Autonomous Systems,scopus,2014-01-01,sciencedirect,Central pattern generator based reflexive control of quadruped walking robots using a recurrent neural network,https://api.elsevier.com/content/abstract/scopus_id/84906318105,"This paper presents a novel Central Pattern Generator (CPG) model for controlling quadruped walking robots. The improvement of this model focuses on generating any desired waveforms along with accurate online modulation. In detail, a well-analyzed Recurrent Neural Network is used as the oscillators to generate simple harmonic periodic signals that exhibit limit cycle effects. Then, an approximate Fourier series is employed to transform those mentioned simple signals into arbitrary desired outputs under the phase constraints of several primary quadruped gaits. With comprehensive closed-form equations, the model also allows the user to modulate the waveform, the frequency and the phase constraint of the outputs online by directly setting the inner parameters without the need for any manual tuning. In addition, an associated controller is designed using leg coordination Cartesian position as the control state space based on which stiffness control is performed at sub-controller level. In addition, several reflex modules are embedded to transform the feedback of all sensors into the CPG space. This helps the CPG recognize external disturbances and utilize inner limit cycle effect to stabilize the robot motion. Finally, experiments with a real quadruped robot named AiDIN III performing several dynamic trotting tasks on several unknown natural terrains are presented to validate the effectiveness of the proposed CPG model and controller.",robotics
10.1016/j.cmpb.2013.12.019,Journal,Computer Methods and Programs in Biomedicine,scopus,2014-01-01,sciencedirect,Motor adaptation with passive machines: A first study on the effect of real and virtual stiffness,https://api.elsevier.com/content/abstract/scopus_id/84903138076,"Motor adaptation to novel force fields is considered as a key mechanism not only for the understanding of skills learning in healthy subjects but also for rehabilitation of neurological subjects. Several studies conducted over the last two decades used active robotic manipulanda to generate force fields capable of perturbing the baseline trajectories of both healthy and impaired subjects.
                  Recent studies showed how motor adaptation to novel force fields can be induced also via virtual environments, whereas the effects of the force are projected onto a virtual hand, while the real hand remains constrained within a channel. This has great potentials of being translated into passive devices, rather than robotic ones, with clear benefits in terms of costs and availability of the devices. However, passive devices and virtual environments have received much less attention at least with regard to motor adaptation.
                  This paper investigates the effects of both the real and virtual stiffness on motor adaptation. In particular, we tested 20 healthy subjects under two different real stiffness conditions (Stiff Channel vs Compliant Channel) and two different virtual conditions (Viscous vs Springy). Our main finding is that compliance of the channel favours a better adaptation featured with less lateral errors and longer retention of the after-effect. We posit that the physical compliance of the channel induces a proprioceptive feedback which is otherwise absent in a stiff condition.",robotics
10.3182/20140313-3-IN-3024.00251,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2014-01-01,sciencedirect,"Improved approach to area exploration in an unknown environment by mobile robot using genetic algorithm, real time reinforcement learning and co-operation among the controllers",https://api.elsevier.com/content/abstract/scopus_id/84899530547,"This paper explains the methodology applied to make a mobile robot explore an unknown environment accurately, with minimum energy dissipations and more speedily. Essentially it focuses on optimization capability of Genetic Algorithms and their convergence property, and how it can be applied in the domain of path planning. Optimization of path planning by mobile robots in environments known and unknown is a hot area of research. This paper is essentially an improvement over a previous paper on target tracking using Direct Competition in terms of lesser energy utilization, better approach of conducting simulations and interpretation of results. Rigorous generation wise experiments actually make the controllers improve a lot from their sub-minimal competent nature thereby overcoming the Bootstrap Problem. Another key point of the research is the observation of behavior in second set of experiment using the evolved weights after the first experiment, how it affects the fitness and how far proves to be successful in achieving the objective.",robotics
10.1016/j.robot.2012.10.003,Journal,Robotics and Autonomous Systems,scopus,2014-01-01,sciencedirect,Identifying vegetation from laser data in structured outdoor environments,https://api.elsevier.com/content/abstract/scopus_id/84897912640,"The ability to reliably detect vegetation is an important requirement for outdoor navigation with mobile robots as it enables the robot to navigate more efficiently and safely. In this paper, we present an approach to detect flat vegetation, such as grass, which cannot be identified using range measurements. This type of vegetation is typically found in structured outdoor environments such as parks or campus sites. Our approach classifies the terrain in the vicinity of the robot based on laser scans and makes use of the fact that plants exhibit specific reflection properties. It uses a support vector machine to learn a classifier for distinguishing vegetation from streets based on laser reflectivity, measured distance, and the incidence angle. In addition, it employs a vibration-based classifier to acquire training data in a self-supervised way and thus reduces manual work. Our approach has been evaluated extensively in real world experiments using several mobile robots. We furthermore evaluated it with different types of sensors and in the context of mapping, autonomous navigation, and exploration experiments. In addition, we compared it to an approach based on linear discriminant analysis. In our real world experiments, our approach yields a classification accuracy close to 100%.",robotics
10.1016/j.asoc.2013.05.017,Journal,Applied Soft Computing Journal,scopus,2014-01-01,sciencedirect,A hybrid noise suppression filter for accuracy enhancement of commercial speech recognizers in varying noisy conditions,https://api.elsevier.com/content/abstract/scopus_id/84888294149,"Commercial speech recognizers have made possible many speech control applications such as wheelchair, tone-phone, multifunctional robotic arms and remote controls, for the disabled and paraplegic. However, they have a limitation in common in that recognition errors are likely to be produced when background noise surrounds the spoken command, thereby creating potential dangers for the disabled if recognition errors exist in the control systems. In this paper, a hybrid noise suppression filter is proposed to interface with the commercial speech recognizers in order to enhance the recognition accuracy under variant noisy conditions. It intends to decrease the recognition errors when the commercial speech recognizers are working under a noisy environment. It is based on a sigmoid function which can effectively enhance noisy speech using simple computational operations, while a robust estimator based on an adaptive-network-based fuzzy inference system is used to determine the appropriate operational parameters for the sigmoid function in order to produce effective speech enhancement under variant noisy conditions. The proposed hybrid noise suppression filter has the following advantages for commercial speech recognizers: (i) it is not possible to tune the inbuilt parameters on the commercial speech recognizers in order to obtain better accuracy; (ii) existing noise suppression filters are too complicated to be implemented for real-time speech recognition; and (iii) existing sigmoid function based filters can operate only in a single-noisy condition, but not under varying noisy conditions. The performance of the hybrid noise suppression filter was evaluated by interfacing it with a commercial speech recognizer, commonly used in electronic products. Experimental results show that improvement in terms of recognition accuracy and computational time can be achieved by the hybrid noise suppression filter when the commercial recognizer is working under various noisy environments in factories.",robotics
10.1016/j.neunet.2013.01.019,Journal,Neural Networks,scopus,2013-11-01,sciencedirect,Realtime cerebellum: A large-scale spiking network model of the cerebellum that runs in realtime using a graphics processing unit,https://api.elsevier.com/content/abstract/scopus_id/84884150994,"The cerebellum plays an essential role in adaptive motor control. Once we are able to build a cerebellar model that runs in realtime, which means that a computer simulation of 1 s in the simulated world completes within 1 s in the real world, the cerebellar model could be used as a realtime adaptive neural controller for physical hardware such as humanoid robots. In this paper, we introduce “Realtime Cerebellum (RC)”, a new implementation of our large-scale spiking network model of the cerebellum, which was originally built to study cerebellar mechanisms for simultaneous gain and timing control and acted as a general-purpose supervised learning machine of spatiotemporal information known as reservoir computing, on a graphics processing unit (GPU). Owing to the massive parallel computing capability of a GPU, RC runs in realtime, while reproducing qualitatively the same simulation results of the Pavlovian delay eyeblink conditioning with the previous version. RC is adopted as a realtime adaptive controller of a humanoid robot, which is instructed to learn a proper timing to swing a bat to hit a flying ball online. These results suggest that RC provides a means to apply the computational power of the cerebellum as a versatile supervised learning machine towards engineering applications.",robotics
10.1016/j.bica.2013.07.009,Journal,Biologically Inspired Cognitive Architectures,scopus,2013-10-01,sciencedirect,Emotional biologically inspired cognitive architecture,https://api.elsevier.com/content/abstract/scopus_id/84883206490,"Human-like artificial emotional intelligence is vital for integration of future robots into the human society. This work introduces a general framework for representation and processing of emotional contents in a cognitive architecture, called “emotional biologically inspired cognitive architecture” (eBICA). Unlike in previous attempts, in this framework emotional elements are added virtually to all cognitive representations and processes by modifying the main building blocks of the prototype architectures. The key elements are appraisals associated as attributes with schemas and mental states, moral schemas that control patterns of appraisals and represent social emotions, and semantic spaces that give values to appraisals. Proposed principles are tested in an experiment involving human subjects and virtual agents, based on a simple paradigm in imaginary virtual world. It is shown that with moral schemas, but probably not without them, eBICA can account for human behavior in the selected paradigm. The model sheds light on clustering of social emotions and allows for their elegant mathematical description. The new framework will be suitable for implementation of believable emotional intelligence in artifacts, necessary for emotionally informed behavior, collaboration of virtual partners with humans, and self-regulated learning of virtual agents.",robotics
10.1016/j.neunet.2013.04.005,Journal,Neural Networks,scopus,2013-09-01,sciencedirect,FPGA implementation of a configurable neuromorphic CPG-based locomotion controller,https://api.elsevier.com/content/abstract/scopus_id/84880792738,"Neuromorphic engineering is a discipline devoted to the design and development of computational hardware that mimics the characteristics and capabilities of neuro-biological systems. In recent years, neuromorphic hardware systems have been implemented using a hybrid approach incorporating digital hardware so as to provide flexibility and scalability at the cost of power efficiency and some biological realism. This paper proposes an FPGA-based neuromorphic-like embedded system on a chip to generate locomotion patterns of periodic rhythmic movements inspired by Central Pattern Generators (CPGs). The proposed implementation follows a top-down approach where modularity and hierarchy are two desirable features. The locomotion controller is based on CPG models to produce rhythmic locomotion patterns or gaits for legged robots such as quadrupeds and hexapods. The architecture is configurable and scalable for robots with either different morphologies or different degrees of freedom (DOFs). Experiments performed on a real robot are presented and discussed. The obtained results demonstrate that the CPG-based controller provides the necessary flexibility to generate different rhythmic patterns at run-time suitable for adaptable locomotion.",robotics
10.1016/j.engappai.2013.03.011,Journal,Engineering Applications of Artificial Intelligence,scopus,2013-08-01,sciencedirect,Solving the forward kinematics problem in parallel robots using Support Vector Regression,https://api.elsevier.com/content/abstract/scopus_id/84878108083,"The Stewart platform, a representative of the class of parallel manipulators, has been successfully used in a wide variety of fields and industries, from medicine to automotive. Parallel robots have key benefits over serial structures regarding stability and positioning capability. At the same time, they present challenges and open problems which need to be addressed in order to take full advantage of their utility. In this paper, we propose a new approach for solving one of these key aspects: the solution to the forward kinematics in real-time, an under-defined problem with a high-degree nonlinear formulation, using a popular machine learning method for classification and regression, the Support Vector Machines. Instead of solving a numerical problem, the proposed method involves applying Support Vector Regression to model the behavior of a platform in a given region or partition of the pose space. It consists of two phases, an off-line preprocessing step and a fast on-line evaluation phase. The experiments made have yielded a good approximation to the analytical solution, and have shown its suitability for real-time application.",robotics
10.1016/j.suronc.2012.12.003,Journal,Surgical Oncology,scopus,2013-06-01,sciencedirect,"Artificial interfaces (""AI"") in surgery: Historic development, current status and program implementation in the public health sector",https://api.elsevier.com/content/abstract/scopus_id/84878893463,"The past two decades have seen considerable advances in the application of artificial interfaces (AI) in surgery. Several have been developed including AESOP (Automated Endoscopic System for Optimal Positioning), Zeus and the Da Vinci Surgical System (DVSS). Whilst each has advantages DVSS is being used increasingly across multiple surgical specialities. These developments generate many challenges in an era where the emphasis is increasingly on safer and cost-effective surgery. Whilst the role of DVSS is firmly established in urologic and gynaecologic surgery, the role of DVSS in gastrointestinal surgery is evolving. Recent data indicate that it is at least as oncologically effective, whilst providing numerous benefits (e.g. reduced conversion and complication rates) over traditional laparoscopic approaches. The increasing adoption of AI/DVSS worldwide places institutes and health sectors under increasing pressure to adopt and develop such programs. This article provides (1) an update on the current status of AI in surgery in general and in colorectal surgery and (2) an appraisal of the cost implications of the establishment and implementation of AI/DVSS–based provisions in the public health sector. The numerous challenges faced generate many opportunities in the implementation of present and future surgical technologies.",robotics
10.1016/j.robot.2012.12.005,Journal,Robotics and Autonomous Systems,scopus,2013-05-01,sciencedirect,A survey of bio-inspired robotics hands implementation: New directions in dexterous manipulation,https://api.elsevier.com/content/abstract/scopus_id/84875695547,"Recently, significant advances have been made in ROBOTICS, ARTIFICIAL INTELLIGENCE and other COGNITIVE related fields, allowing to make much sophisticated biomimetic robotics systems. In addition, enormous number of robots have been designed and assembled, explicitly realize biological oriented behaviors. Towards much skill behaviors and adequate grasping abilities (i.e. ARTICULATION and DEXTEROUS MANIPULATION), a new phase of dexterous hands have been developed recently with biomimetically oriented and bio-inspired functionalities. In this respect, this manuscript brings a detailed survey of biomimetic based dexterous robotics multi-fingered hands. The aim of this survey, is to find out the state of the art on dexterous robotics end-effectors, known in literature as (ROBOTIC HANDS) or (DEXTEROUS MULTI-FINGERED) robot hands. Hence, this review finds such biomimetic approaches using a framework that permits for a common description of biological and technical based hand manipulation behavior. In particular, the manuscript focuses on a number of developments that have been taking place over the past two decades, and some recent developments related to this biomimetic field of research. In conclusions, the study found that, there are rich research efforts in terms of KINEMATICS, DYNAMICS, MODELING and CONTROL methodologies. The survey is also indicating that, the topic of biomimetic inspired robotics systems make significant contributions to robotics hand design, in four main directions for future research. First, they provide a genuine world test of models of biologically inspired hand designs and dexterous manipulation behaviors. Second, they provide novel manipulation articulations and mechanisms available for industrial and domestic uses, most notably in the field of human like hand design and real world applications. Third, this survey has also indicated that, there are quite large number of attempts to acquire biologically inspired hands. These attempts were almost successful, where they exposed more novel ideas for further developments. Such inspirations were directed towards a number of topics related (HAND MECHANICS AND DESIGN), (HAND TACTILE SENSING), (HAND FORCE SENSING), (HAND SOFT ACTUATION) and (HAND CONFIGURATION AND TOPOLOGY). FOURTH, in terms of employing AI related sciences and cognitive thinking, it was also found that, rare and exceptional research attempts were directed towards the employment of biologically inspired thinking, i.e. (AI, BRAIN AND COGNITIVE SCIENCES) for hand upper control and towards much sophisticated dexterous movements. Throughout the study, it has been found there are number of efforts in terms of mechanics and hand designs, tactical sensing, however, for hand soft actuation, it seems this area of research is still far away from having a realistic muscular type fingers and hand movements.",robotics
10.1016/j.eswa.2012.09.010,Journal,Expert Systems with Applications,scopus,2013-04-01,sciencedirect,Neural network Reinforcement Learning for visual control of robot manipulators,https://api.elsevier.com/content/abstract/scopus_id/84872011415,"It is known that most of the key problems in visual servo control of robots are related to the performance analysis of the system considering measurement and modeling errors. In this paper, the development and performance evaluation of a novel intelligent visual servo controller for a robot manipulator using neural network Reinforcement Learning is presented. By implementing machine learning techniques into the vision based control scheme, the robot is enabled to improve its performance online and to adapt to the changing conditions in the environment. Two different temporal difference algorithms (Q-learning and SARSA) coupled with neural networks are developed and tested through different visual control scenarios. A database of representative learning samples is employed so as to speed up the convergence of the neural network and real-time learning of robot behavior. Moreover, the visual servoing task is divided into two steps in order to ensure the visibility of the features: in the first step centering behavior of the robot is conducted using neural network Reinforcement Learning controller, while the second step involves switching control between the traditional Image Based Visual Servoing and the neural network Reinforcement Learning for enabling approaching behavior of the manipulator. The correction in robot motion is achieved with the definition of the areas of interest for the image features independently in both control steps. Various simulations are developed in order to present the robustness of the developed system regarding calibration error, modeling error, and image noise. In addition, a comparison with the traditional Image Based Visual Servoing is presented. Real world experiments on a robot manipulator with the low cost vision system demonstrate the effectiveness of the proposed approach.",robotics
10.1016/j.neucom.2012.10.003,Journal,Neurocomputing,scopus,2013-03-15,sciencedirect,A general associative memory based on self-organizing incremental neural network,https://api.elsevier.com/content/abstract/scopus_id/84873725454,"This paper proposes a general associative memory (GAM) system that combines the functions of other typical associative memory (AM) systems. The GAM is a network consisting of three layers: an input layer, a memory layer, and an associative layer. The input layer accepts key vectors, response vectors, and the associative relationships between these vectors. The memory layer stores the input vectors incrementally to corresponding classes. The associative layer builds associative relationships between classes. The GAM can store and recall binary or non-binary information, learn key vectors and response vectors incrementally, realize many-to-many associations with no predefined conditions, store and recall both static and temporal sequence information, and recall information from incomplete or noise-polluted inputs. Experiments using binary data, real-value data, and temporal sequences show that GAM is an efficient system. The AM experiments using a humanoid robot demonstrates that GAM can accommodate real tasks and build associations between patterns with different dimensions.",robotics
10.1533/9780857093967.1.208,Book,Joining Textiles: Principles and Applications,scopus,2013-01-01,sciencedirect,Intelligent sewing systems for garment automation and robotics,https://api.elsevier.com/content/abstract/scopus_id/84903011824,"Sewing machine interactions at different speeds have been used to construct qualitative rules mapping fabric properties to optimum sewing machine settings for intelligent sewing machines. the inference procedures of fuzzy logic have been implemented in a neural network to allow for optimisation of output membership functions and, subsequently, self-learning. the technique is successfully applied to develop intelligent sewing machines and further implemented in textile and garment manufacturing. An intelligent manufacturing environment has been put forward in which fabric properties predict the sewability of any fabric, determine the minimum change of fabric properties required, and control in real time the stitching of a garment by using the feedback closed loop of the Neuro-Fuzzy model. the system has been successfully tried in an industrial setting. optimum settings were achieved under static and dynamic machine conditions, including for the properties of difficult fabrics and compensation for mishandling by the operator over the speed range of the sewing machine.",robotics
10.1016/j.procs.2013.05.187,Conference Proceeding,Procedia Computer Science,scopus,2013-01-01,sciencedirect,Comparing support vector machines and artificial neural networks in the recognition of steering angle for driving of mobile robots through paths in plantations,https://api.elsevier.com/content/abstract/scopus_id/84896966222,"The use of mobile robots turns out to be interesting in activities where the action of human specialist is difficult or dangerous. Mobile robots are often used for the exploration in areas of difficult access, such as rescue operations and space missions, to avoid human experts exposition to risky situations. Mobile robots are also used in agriculture for planting tasks as well as for keeping the application of pesticides within minimal amounts to mitigate environmental pollution. In this paper we present the development of a system to control the navigation of an autonomous mobile robot through tracks in plantations. Track images are used to control robot direction by pre-processing them to extract image features. Such features are then submitted to a support vector machine and an artificial neural network in order to find out the most appropriate route. A comparison of the two approaches was performed to ascertain the one presenting the best outcome. The overall goal of the project to which this work is connected is to develop a real time robot control system to be embedded into a hardware platform. In this paper we report the software implementation of a support vector machine and of an artificial neural network, which so far presented respectively around 93% and 90% accuracy in predicting the appropriate route.",robotics
10.1016/j.procs.2013.09.232,Conference Proceeding,Procedia Computer Science,scopus,2013-01-01,sciencedirect,Information-preserving transforms: Two graph metrics for simulated spiking neural networks,https://api.elsevier.com/content/abstract/scopus_id/84896959699,"We are interested in self-organization and adaptation in intelligent systems that are robustly coupled with the real world. Such systems have a variety of sensory inputs that provide access to the richness, complexity, and noise of real-world signals. Specifically, the systems we design and implement are ab initio simulated spiking neural networks (SSNNs) with cellular resolution and complex network topologies that evolve according to spike-timing dependent plasticity (STDP). We desire to understand how external signals (like speech, vision, etc.) are encoded in the dynamics of such SSNNs. In particular, we are interested in identifying and confirming the extent to which various population-level measurements (or transforms) are information-preserving. Such transforms could be used as an unambiguous way of identifying the nature of the input signals, when given only access to the SSNN dynamics. Our primary objective in this paper is to empirically examine the extent to which a couple of graph metrics provide an information-preserving transform between the input signals and the output signals. In particular, we focus on the standard deviation of the time-varying distributions for local influence (weighted out-degree) and local impressionability (weighted in-degree), which provide insight into information encoding at the population-level in the dynamics of SSNNs. We report the encouraging results of an experiment carried out in the Language Acquisition and Robotics Group.",robotics
10.3182/20130828-3-UK-2039.00025,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2013-01-01,sciencedirect,A new extensive source for web-based control education - Contlab.eu,https://api.elsevier.com/content/abstract/scopus_id/84885206555,"Modern technologies allow to create a networked control system with off-the-shelf mobile devices. As such, there is the possibility of having the role of who offers and who uses a “remote” laboratory played by the same people. Extending recently published ideas, the paper presents a first nucleus of functionalities allowing one to create process simulators and controllers which run on a mobile application, and then share them with others. Some words are also spent on some of the possibilities opened by the proposal, sketching out some interesting didactic activities to propose to the students.",robotics
10.1016/j.rimni.2011.10.004,Journal,Revista Internacional de Metodos Numericos para Calculo y Diseno en Ingenieria,scopus,2013-01-01,sciencedirect,Discrete neural compensator algorithm of dynamic in mobile robots using extended Kalman filter,https://api.elsevier.com/content/abstract/scopus_id/84880235473,"Este artículo presenta el diseño de un algoritmo basado en redes neuronales en tiempo discreto para su aplicación en robótica móvil. También se muestran las condiciones de estabilidad y una evaluación de los resultados.
                  El robot móvil en el cual se aplicó el algoritmo neural posee 2controladores en cascada, uno para la cinemática y otro para la dinámica; ambos controladores están basados en la linealización por realimentación. El controlador de la dinámica solo posee la información de la dinámica nominal (parámetros). La red neuronal de compensación se adapta para reducir las perturbaciones ocasionadas por las variaciones en la dinámica y las incertidumbres existentes en el modelo, y esas diferencias en la dinámica entre el modelo nominal y el real son aprendidas por una red neuronal RBF (funciones de base radial) usando el filtro de Kalman extendido para el ajuste de los pesos de salida de las funciones de base radial. El algoritmo de compensación neuronal es eficiente, ya que el costo computacional es menor que el necesario para aprender la totalidad de la dinámica y al mismo tiempo posee la robustez que podría aprender la totalidad de la dinámica en caso de fallo del controlador dinámico. En este trabajo se muestra un análisis de estabilidad del algoritmo neuronal adaptable, y además se comprueba que los errores de control están acotados en función del error de aproximación de la red neuronal RBF. Se muestran resultados de experimentación sobre un robot móvil que prueban la viabilidad práctica y el rendimiento para el control de los mismos.
               
                  This paper presents the design of an algorithm based on neural networks in discrete time for its application in mobile robots. In addition, the system stability is analyzed and an evaluation of the experimental results is shown.
                  The mobile robot has two controllers, one addressed for the kinematics and the other one designed for the dynamics. Both controllers are based on the feedback linearization. The controller of the dynamics only has information of the nominal dynamics (parameters). The neural algorithm of compensation adapts its behaviour to reduce the perturbations caused by the variations in the dynamics and the model uncertainties. Thus, the differences in the dynamics between the nominal model and the real one are learned by a neural network RBF (radial basis functions) where the output weights are set using the extended Kalman filter. The neural compensation algorithm is efficient, since the consumed processing time is lower than the one required to learning the totality of the dynamics. In addition, the proposed algorithm is robust with respect to failures of the dynamic controller. In this work, a stability analysis of the adaptable neural algorithm is shown and it is demonstrated that the control errors are bounded depending on the error of approximation of the neural network RBF. Finally, the results of experiments performed by using a mobile robot are shown to test the viability in practice and the performance for the control of robots.",robotics
10.1016/j.bspc.2012.08.008,Journal,Biomedical Signal Processing and Control,scopus,2013-01-01,sciencedirect,Bispectrum-based features classification for myoelectric control,https://api.elsevier.com/content/abstract/scopus_id/84874533807,"Surface electromyographic signals provide useful information about motion intentionality. Therefore, they are a suitable reference signal for control purposes. A continuous classification scheme of five upper limb movements applied to a myoelectric control of a robotic arm is presented. This classification is based on features extracted from the bispectrum of four EMG signal channels. Among several bispectrum estimators, this paper is focused on arithmetic mean, median, and trimmed mean estimators, and their ensemble average versions. All bispectrum estimators have been evaluated in terms of accuracy, robustness against outliers, and computational time. The median bispectrum estimator shows low variance and high robustness properties. Two feature reduction methods for the complex bispectrum matrix are proposed. The first one estimates the three classic means (arithmetic, harmonic, and geometric means) from the module of the bispectrum matrix, and the second one estimates the same three means from the square of the real part of the bispectrum matrix. A two-layer feedforward network for movement's classification and a dedicated system to achieve the myoelectric control of a robotic arm were used. It was found that the classification performance in real-time is similar to those obtained off-line by other authors, and that all volunteers in the practical application successfully completed the control task.",robotics
10.1016/S1672-6529(13)60198-5,Journal,Journal of Bionic Engineering,scopus,2013-01-01,sciencedirect,Automatic Navigation for Rat-Robots with Modeling of the Human Guidance,https://api.elsevier.com/content/abstract/scopus_id/84872508733,"A bio-robot system refers to an animal equipped with Brain-Computer Interface (BCI), through which the outer stimulation is delivered directly into the animal's brain to control its behaviors. The development of bio-robots suffers from the dependency on real-time guidance by human operators. Because of its inherent difficulties, there is no feasible method for automatic controlling of bio-robots yet. In this paper, we propose a new method to realize the automatic navigation for bio-robots. A General Regression Neural Network (GRNN) is adopted to analyze and model the controlling procedure of human operations. Comparing to the traditional approaches with explicit controlling rules, our algorithm learns the controlling process and imitates the decision-making of human-beings to steer the rat-robot automatically. In real-time navigation experiments, our method successfully controls bio-robots to follow given paths automatically and precisely. This work would be significant for future applications of bio-robots and provide a new way to realize hybrid intelligent systems with artificial intelligence and natural biological intelligence combined together.",robotics
10.1016/B978-0-444-62604-2.00022-8,Book Series,Progress in Brain Research,scopus,2013-01-01,sciencedirect,Medial prefrontal cortex and the adaptive regulation of reinforcement learning parameters,https://api.elsevier.com/content/abstract/scopus_id/84872378856,"Converging evidence suggest that the medial prefrontal cortex (MPFC) is involved in feedback categorization, performance monitoring, and task monitoring, and may contribute to the online regulation of reinforcement learning (RL) parameters that would affect decision-making processes in the lateral prefrontal cortex (LPFC). Previous neurophysiological experiments have shown MPFC activities encoding error likelihood, uncertainty, reward volatility, as well as neural responses categorizing different types of feedback, for instance, distinguishing between choice errors and execution errors. Rushworth and colleagues have proposed that the involvement of MPFC in tracking the volatility of the task could contribute to the regulation of one of RL parameters called the learning rate. We extend this hypothesis by proposing that MPFC could contribute to the regulation of other RL parameters such as the exploration rate and default action values in case of task shifts. Here, we analyze the sensitivity to RL parameters of behavioral performance in two monkey decision-making tasks, one with a deterministic reward schedule and the other with a stochastic one. We show that there exist optimal parameter values specific to each of these tasks, that need to be found for optimal performance and that are usually hand-tuned in computational models. In contrast, automatic online regulation of these parameters using some heuristics can help producing a good, although non-optimal, behavioral performance in each task. We finally describe our computational model of MPFC–LPFC interaction used for online regulation of the exploration rate and its application to a human–robot interaction scenario. There, unexpected uncertainties are produced by the human introducing cued task changes or by cheating. The model enables the robot to autonomously learn to reset exploration in response to such uncertain cues and events. The combined results provide concrete evidence specifying how prefrontal cortical subregions may cooperate to regulate RL parameters. It also shows how such neurophysiologically inspired mechanisms can control advanced robots in the real world. Finally, the model's learning mechanisms that were challenged in the last robotic scenario provide testable predictions on the way monkeys may learn the structure of the task during the pretraining phase of the previous laboratory experiments.",robotics
10.1016/j.robot.2012.07.022,Journal,Robotics and Autonomous Systems,scopus,2012-12-01,sciencedirect,Spreading algorithm for efficient vegetation detection in cluttered outdoor environments,https://api.elsevier.com/content/abstract/scopus_id/84869087494,"This paper investigates the problem of detecting vegetation in unstructured environments for guiding an autonomous robot safely, exploiting its mobility capability in a cluttered outdoor environment. The aim is to create an adaptive learning algorithm which performs a quantitatively accurate detection that is fast enough for a real-time application. Chlorophyll-rich vegetation pixels are selected by thresholding vegetation indices, and then are considered as the seeds of a “spread vegetation”. For each seed pixel, a convex combination of color and texture dissimilarities is used to infer the difference between the pixel and its neighbors. The convex combination, trained via semi-supervised learning, models either the difference of vegetation pixels or the difference between a vegetation pixel and a non-vegetation pixel, and thus allows a greedy decision-making process to expand the spread vegetation, so-called vision-based spreading. To avoid overspreading, especially in the case of noise, a spreading scale is set. On the other hand, another vegetation spreading based on spectral reflectance is carried out in parallel. Finally, the intersection part resulting from both the vision-based and spectral reflectance-based methods is added to the spread vegetation. The approach takes into account both vision and chlorophyll light absorption properties. This enables the algorithm to capture much more detailed vegetation features than does prior art, and also give a much richer experience in the interpretation of vegetation representation, even for scenes with significant overexposure or underexposure as well as with the presence of shadow and sunshine. In all real-world experiments we carried out, our approach yields a detection accuracy of over 90%.",robotics
10.1016/j.asoc.2012.01.014,Journal,Applied Soft Computing Journal,scopus,2012-12-01,sciencedirect,Autonomous real-time landing site selection for Venus and Titan using Evolutionary Fuzzy Cognitive Maps,https://api.elsevier.com/content/abstract/scopus_id/84869086510,"Future science-driven landing missions, conceived to collect in situ data on regions of planetary bodies that have the highest potential to yield important scientific discoveries, will require a higher degree of autonomy. The latter includes the ability of the spacecraft to autonomously select the landing site using real-time data acquired during the descent phase. This paper presents the development of an Evolutionary Fuzzy Cognitive Map (E-FCM) model that implements an artificial intelligence system capable of autonomously selecting a landing site with the highest potential for scientific discoveries constrained by the requirement of soft landing in a region with safe terrains. The proposed E-FCM evolves its internal states and interconnections as a function of real-time data collected during the descent phase, therefore improving the decision process as more accurate information becomes available. The E-FCM is constructed using knowledge accumulated by planetary experts and it is tested on scenarios that simulate the decision process during the descent phase toward the Hyndla Regio on Venus. The E-FCM is shown to quickly reach conclusions that are consistent with what would be the choice of a planetary expert if the scientist were presented with the same information. The proposed methodology is fast and efficient and may be suitable for on-board spacecraft implementation and real-time decision making during the course of robotic exploration of the Solar System.",robotics
10.1016/j.neunet.2012.07.010,Journal,Neural Networks,scopus,2012-11-01,sciencedirect,A bio-inspired kinematic controller for obstacle avoidance during reaching tasks with real robots,https://api.elsevier.com/content/abstract/scopus_id/84865627603,"This paper describes a redundant robot arm that is capable of learning to reach for targets in space in a self-organized fashion while avoiding obstacles. Self-generated movement commands that activate correlated visual, spatial and motor information are used to learn forward and inverse kinematic control models while moving in obstacle-free space using the Direction-to-Rotation Transform (DIRECT). Unlike prior DIRECT models, the learning process in this work was realized using an online Fuzzy ARTMAP learning algorithm. The DIRECT-based kinematic controller is fault tolerant and can handle a wide range of perturbations such as joint locking and the use of tools despite not having experienced them during learning. The DIRECT model was extended based on a novel reactive obstacle avoidance direction (DIRECT-ROAD) model to enable redundant robots to avoid obstacles in environments with simple obstacle configurations. However, certain configurations of obstacles in the environment prevented the robot from reaching the target with purely reactive obstacle avoidance. To address this complexity, a self-organized process of mental rehearsals of movements was modeled, inspired by human and animal experiments on reaching, to generate plans for movement execution using DIRECT-ROAD in complex environments. These mental rehearsals or plans are self-generated by using the Fuzzy ARTMAP algorithm to retrieve multiple solutions for reaching each target while accounting for all the obstacles in its environment. The key aspects of the proposed novel controller were illustrated first using simple examples. Experiments were then performed on real robot platforms to demonstrate successful obstacle avoidance during reaching tasks in real-world environments.",robotics
10.1016/j.artmed.2012.08.007,Journal,Artificial Intelligence in Medicine,scopus,2012-10-01,sciencedirect,Case-based reasoning emulation of persons for wheelchair navigation,https://api.elsevier.com/content/abstract/scopus_id/84869093768,"Objective
                  Testing is a key stage in system development, particularly in systems such as a wheelchair, in which the final user is typically a disabled person. These systems have stringent safety requirements, requiring major testing with many different individuals. The best would be to have the wheelchair tested by many different end users, as each disability affects driving skills in a different way. Unfortunately, from a practical point of view it is difficult to engage end users as beta testers. Hence, testing often relies on simulations. Naturally, these simulations need to be as realistic as possible to make the system robust and safe before real tests can be accomplished. This work presents a tool to automatically test wheelchairs through realistic emulation of different wheelchair users.
               
                  Methods and materials
                  Our approach is based on extracting meaningful data from real users driving a power wheelchair autonomously. This data is then used to train a case-based reasoning (CBR) system that captures the specifics of the driver via learning. The resulting case-base is then used to emulate the driving behavior of that specific person in more complex situations or when a new assistive algorithm needs to be tested. CBR returns user's motion commands appropriate for each specific situation to add the human component to shared control systems.
               
                  Results
                  The proposed system has been used to emulate several power wheelchair users presenting different disabilities. Data to create this emulation was obtained from previous wheelchair navigation experiments with 35 volunteer in-patients presenting different degrees of disability. CBR was trained with a limited number of scenarios for each volunteer. Results proved that: (i) emulated and real users returned similar paths in the same scenario (maximum and mean path deviations are equal to 23 and 10cm, respectively) and similar efficiency; (ii) we established the generality of our approach taking a new path not present in the training traces; (iii) the emulated user is more realistic – path and efficiency are less homogeneous and smooth – than potential field approaches; and (iv) the system adequately emulates in-patients – maximum and mean path deviations are equal to 19 and 8.3cm approximately and efficiencies are similar – with specific disabilities (apraxia and dementia) obtaining different behaviors during emulation for each of the in-patients, as expected.
               
                  Conclusions
                  The proposed system adequately emulates the driving behavior of people with different disabilities in indoor scenarios. This approach is suitable to emulate real users’ driving behaviors for early testing stages of assistive navigation systems.",robotics
10.1016/j.robot.2012.06.005,Journal,Robotics and Autonomous Systems,scopus,2012-10-01,sciencedirect,An automated vision based on-line novel percept detection method for a mobile robot,https://api.elsevier.com/content/abstract/scopus_id/84865684391,"It is generally agreed that learning, either supervised or unsupervised, can provide the best possible specification of known classes and offer inference for outlier detection by a dissimilarity threshold from the nominal feature space. Novel percept detection can take a step further by investigating whether these outliers form new dense clusters in both the feature space and the image space. By defining a novel percept to be a pattern group that has not been seen before in the feature space and the image space, in this paper, a non-conventional approach is proposed for multiple-novel-percept detection problem in robotic applications. Based on a computer vision system inspired loosely by neurobiological evidence, our approach can work in near real time for highly sparse high-dimensional feature vectors extracted from image patches while maintaining robustness to image transformations. Experiments conducted in an indoor environment and an outdoor environment demonstrate the efficacy of our method.",robotics
10.1016/j.jtbi.2012.04.022,Journal,Journal of Theoretical Biology,scopus,2012-07-21,sciencedirect,A model of ant navigation based on visual prediction,https://api.elsevier.com/content/abstract/scopus_id/84860506917,"A model of visual navigation in ants is presented which is based on a simple network predicting the changes of a visual scene under translatory movements. The model contains two behavioral components: the acquisition of multiple snapshots in different orientations during a learning walk, and the selection of a movement direction by a scanning behavior where the ant searches through different headings. Both components fit with observations in experiments with desert ants. The model is in most aspects biologically plausible with respect to the equivalent neural networks, and it produces reliable homing behavior in a simulated environment with a complex random surface texture. The model is closely related to the algorithmic min-warping method for visual robot navigation which shows good homing performance in real-world environments.",robotics
10.1016/j.aei.2012.02.005,Journal,Advanced Engineering Informatics,scopus,2012-04-01,sciencedirect,Enabling control software generation by using mechatronics modeling primitives,https://api.elsevier.com/content/abstract/scopus_id/84859593706,"Mechatronic systems are characterized by the synergetic integration of mechanic, electronic, software and control design aspects. The development of control software requires data and information from all design domains in order to create the required integrated functionality. This paper proposes a method that combines function modeling and multi-domain modeling primitives to generate control software automatically. An architecture model, based on the Function-Behavior-State modeling paradigm, provides the decomposition and flow of both functionality and implementation, which serves as input to a knowledge-based engineering application. The control software is subsequently extracted from a virtual product model composed of instantiated modeling primitives. A case study of a mobile robot shows how for a specific application the modeling are defined and how a high-level function model for an environment mapping mission is translated into directly implementable software code. This approach could be extended to real-life mechatronic products, and will improve consistency and reduce development time and cost.",robotics
10.1016/j.robot.2012.01.002,Journal,Robotics and Autonomous Systems,scopus,2012-04-01,sciencedirect,Automatically composing and parameterizing skills by evolving Finite State Automata,https://api.elsevier.com/content/abstract/scopus_id/84857062242,"We propose a robotics algorithm that is able to simultaneously combine, adapt and create actions to solve a task. The actions are combined in a Finite State Automaton whose structure is determined by a novel evolutionary algorithm. The actions parameters, or new actions, are evolved alongside the FSA topology. Actions can be combined together in a hierarchical fashion. This approach relies on skills that with which the robot is already provided, like grasping or motion planning. Therefore software reuse is an important advantage of our proposed approach. We conducted several experiments both in simulation and on a real mobile manipulator PR2 robot, where skills of increasing complexity are evolved. Our results show that (i) an FSA generated in simulation can be directly applied to a real robot without modifications and (ii) the evolved FSA is robust to the noise and the uncertainty arising from real-world sensors.",robotics
10.1016/j.neucom.2011.05.033,Journal,Neurocomputing,scopus,2012-02-15,sciencedirect,SEMG-based continuous estimation of joint angles of human legs by using BP neural network,https://api.elsevier.com/content/abstract/scopus_id/82655181835,"In this paper, we propose an mth order nonlinear model to describe the relationship between the surface electromyography (sEMG) signals and the joint angles of human legs, in which a simple BP neural network is built for the model estimation. The inputs of the model are sEMG time series that have been processed, and the outputs of the model are the joint angles of hip, knee, and ankle. To validate the effectiveness of the BP neural network, six able-bodied people and four spinal cord injury (SCI) patients participated in the experiment. Two movement modes including the treadmill exercise and the leg extension exercise at different speeds and different loads were respectively conducted by the able-bodied individuals, and only the treadmill exercise was selected for the SCI patients. Seven channels of sEMG from seven human leg muscles were recorded and three joint angles including the hip joint, knee joint and the ankle joint were sampled simultaneously. The results present that this method has a good performance on joint angles estimation by using sEMG for both able-bodied subjects and SCI patients. The average angle estimation root-mean-square (rms) error for leg extension exercise is less than 9°, and the average rms error for treadmill exercise is less than 6° for all the able-bodied subjects. The average angle estimation rms error of the SCI patients is even smaller (less than 5°) than that of the able-bodied people because of a smaller movement range. This method would be used to rehabilitation robot or functional electrical stimulation (FES) for active rehabilitation of SCI patients or stroke patients based on sEMG signals.",robotics
10.1016/j.proeng.2012.07.276,Conference Proceeding,Procedia Engineering,scopus,2012-01-01,sciencedirect,SEMG based ANN for shoulder angle prediction,https://api.elsevier.com/content/abstract/scopus_id/84901016357,"Many kinds of upper limb rehabilitation systems have been developing for physically weak and/or injured patients to assist their daily life activities and promote their quality of life. Among those systems, EMG controlled rehabilitation systems provide the most effective and fastest ways to restore the lost functions due to such weakness and injuries. This paper presents the prediction of shoulder angle based on acquisition of surface electromyogram (sEMG) signals. Backpropagation neural network (BPNN) controller is developed to predict the angle of shoulder flexion/extension and abduction/adduction movements. Virtual Reality (VR) human model is developed to simulate the predicted shoulder angle which results from BPNN controller. Four sEMG signals are collected from user arm muscles and processed to extract their feature with root mean square (RMS) method. Then, the signals features directed to the neural network as input and the network predicts the angle of the shoulder joint as an output. The angles from BPNN drive the shoulder joint of the VR human model in virtual environment. Experiments were carried out to evaluate the effectiveness of the developed system and it was found that the constructed BPNN model and VR model can well represent the relationship between sEMG and shoulder joint angles and rotation. These positive results elaborate a move to design and develop the EMG controlled upper limb rehabilitation robot system to rehabilitate the physically weak person and paralysed patients.",robotics
10.3182/20120905-3-HR-2030.00046,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,Tracing commodities in indoor environments for service robotics,https://api.elsevier.com/content/abstract/scopus_id/84880974712,"Daily life assistance for elderly people is one of the most promising scenarios for service robots in the near future. In particular, the go-and-fetch task will be one of the most demanding tasks in these cases. In this paper, we present an informationally structured room that supports a service robot in the task of daily object fetching. Our environment contains different distributed sensors including a floor sensing system and several intelligent cabinets. Sensor information is sent to a centralized management system which processes the data and makes it available to a service robot which is assisting people in the room. We additionally present the first steps of an intelligent framework used to maintain information about locations of commodities in our informationally structured room. This information will be used by the service robot to find objects under request. One of the main goal of our intelligent environment is to maintain a small number of sensors to avoid interfering with the daily activity of people and to reduce as much as possible the invasion of their privacy. In order to compensate this limited available sensor information our framework aims to exploit knowledge about people's activity and their interaction with objects to infer reliable information about the location of commodities. This paper presents simulated results that demonstrate the suitability of this framework to be applied to a service robotic environment equipped with limited sensors. In addition we discuss some preliminary experiments using our real environment and robot.",robotics
10.3182/20120403-3-DE-3010.00041,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,Embedded system for controlling a mini underwater vehicle in autonomous hover mode,https://api.elsevier.com/content/abstract/scopus_id/84866098847,"This work presents the development of a mini underwater vehicle (Triton-PR), the embedded system, and the experiments in real-time for autonomous hover operation. Artificial vision allows the vehicle to obtain the translational position and velocity. The main characteristic of the embedded system is the implementation of low cost devices and materials, besides the number and location of the thrusters was chosen in order to have enough power and generate the rotation and translation movements. The dynamical model of the (Triton-PR) is described by the classic Euler-Lagrange equations, and a PD controller based on saturation functions is proposed for providing autonomous attitude and position of the robot. Finally, the performance of the vehicle is shown in simulation and real-time experimental results.",robotics
10.1016/j.apergo.2011.09.004,Journal,Applied Ergonomics,scopus,2012-01-01,sciencedirect,Cognitive conflict in human-automation interactions: A psychophysiological study,https://api.elsevier.com/content/abstract/scopus_id/84855890488,"The review of literature in sociology and distributed artificial intelligence reveals that the occurrence of conflict is a remarkable precursor to the disruption of multi-agent systems. The study of this concept could be applied to human factors concerns, as man-system conflict appears to provoke perseveration behavior and to degrade attentional abilities with a trend to excessive focus. Once entangled in such conflicts, the human operator will do anything to succeed in his current goal even if it jeopardizes the mission. In order to confirm these findings, an experimental setup, composed of a real unmanned ground vehicle, a ground station is developed. A scenario involving an authority conflict between the participants and the robot is proposed. Analysis of the effects of the conflict on the participants’ cognition and arousal is assessed through heart-rate measurement (reflecting stress level) and eye-tracking techniques (index of attentional focus). Our results clearly show that the occurrence of the conflict leads to perseveration behavior and can induce higher heart rate as well as excessive attentional focus. These results are discussed in terms of task commitment issues and increased arousal. Moreover, our results suggest that individual differences may predict susceptibility to perseveration behavior.",robotics
10.1016/j.proeng.2011.08.009,Conference Proceeding,Procedia Engineering,scopus,2011-12-26,sciencedirect,Back-stepping and neural network control of a mobile robot for curved weld seam tracking,https://api.elsevier.com/content/abstract/scopus_id/84055193493,"This paper proposes a back-stepping and neural network hybrid control method for mobile platform and slider of mobile robot used in shipbuilding welding. The kinematics model of the robot is built firstly, and then a motion controller is designed based on the model and back-stepping method. Stability of the controller is proved through use of Liapunov theory. For improving the tracking precision and anti-interference performance of the controller, a neural network is designed to identify the kinematical model of the robot and to adjust the control coefficients in real time based on the tracking errors. The simulation and experiments have been done to verify the effectiveness of the proposed controllers.",robotics
10.1016/j.compedu.2011.07.003,Journal,Computers and Education,scopus,2011-12-01,sciencedirect,Hands-on experiences of undergraduate students in Automatics and Robotics using a virtual and remote laboratory,https://api.elsevier.com/content/abstract/scopus_id/79960893084,"Automatics and Robotics subjects are always greatly improved when classroom teaching is supported by adequate laboratory courses and experiments following the ""learning by doing"" paradigm, which provides students a deep understanding of theoretical lessons. However, expensive equipment and limited time prevent teachers having sufficient educational platforms, and several low cost and flexible solutions have been developed to permit an effective teaching in Automatics and Robotics at a reasonable cost. Virtual and remote laboratories are inside this group of solutions as Web-based experimentation tools which have demonstrated the importance and effectiveness of hand-on experiences. This paper presents an experience teaching based on a blended-learning method using as experimentation tool a virtual and remote robotic laboratory called RobUALab, which is also described in the paper, in Automatics and Robotics subjects of the Computer Science degree at the University of Alicante. Students experiment with a set of hand-on exercises about Automatics and Robotics using RobUALab, firstly in face-to-face classes where they experiment in-situ with the real plant and, afterwards, they access to the experimentation environment in order to finish remotely their practical exercises outside the laboratory. The results obtained in the evaluation of the educational methodology proposed attest its efficiency in terms of learning degree and performance of the students.",robotics
10.1016/j.jvir.2011.05.002,Journal,Journal of Vascular and Interventional Radiology,scopus,2011-10-01,sciencedirect,Development of hepatic pseudotumors for image-guided interventional and surgical research in a large animal model,https://api.elsevier.com/content/abstract/scopus_id/80053300687,"Purpose
                  Real-time image guidance and navigation have become increasingly important in an era of minimally invasive interventional and surgical procedures in the liver. To develop, test, and implement tools for real-time image guidance, the authors sought to create an in vivo tumor mimic with realistic imaging and treatment capabilities.
               
                  Materials and Methods
                  Hepatic pseudotumors were created by injecting 1–2 mL of alginate (a hydrocolloid) directly into the liver parenchyma in eight live pigs and two dog cadavers. Tumors were imaged by B-mode ultrasound (US), US elasticity imaging, multi–detector row computed tomography (CT), CT fluoroscopy, and magnetic resonance (MR) imaging to assess imaging capabilities. Procedures performed with the alginate pseudotumors included radiofrequency (RF) ablation and robotic needle guidance.
               
                  Results
                  Twenty-four hepatic pseudotumors were created, ranging in size from 10 mm to 28 mm at an average depth of 6 mm. Average time of preparation and insertion was 3 minutes. All tumors were palpable under the surface of the liver and were easily visible on B-mode US, US elasticity imaging, CT, and MR imaging. Tumors were successfully “treated” with RF ablation, and gross examination of the liver showed good encompassment of the tumor by the zone of thermal coagulation. In addition, the pseudotumors allowed for easy introduction of various types of needles, including RF ablation probes and experimental steerable needles.
               
                  Conclusions
                  Alginate pseudotumors can easily be imaged and allow for different procedures to be performed. This model can be used for various research purposes.",robotics
10.1016/j.eswa.2011.04.207,Journal,Expert Systems with Applications,scopus,2011-10-01,sciencedirect,Intelligent control based on wavelet decomposition and neural network for predicting of human trajectories with a novel vision-based robotic,https://api.elsevier.com/content/abstract/scopus_id/79959924555,"In this paper, an intelligent novel vision-based robotic tracking model is developed to predict the performance of human trajectories with a novel vision-based robotic tracking system. The developed model is based on wavelet packet decomposition, entropy and neural network. We represent an implementation of a novel vision-based robotic tracking system based on wavelet decomposition and artificial neural (WD-ANN) which can track desired human trajectory pattern in real environments. The input–output data set of the novel vision-based robotic tracking system were first stored and than these data sets were used to predict the robotic tracking based on WD-ANN. In simulations, performance measures were obtained to compare the predicted and human–robot trajectories like actual values for model validation. In statistical analysis, the RMS value is 0.0729 and the R
                     2 value is 99.76% for the WD-ANN model. This study shows that the values predicted with the WD-ANN can be used to predict human trajectory by vision-based robotic tracking system quite accurately. All simulations have shown that the proposed method is more effective and controls the systems quite successful.",robotics
10.1016/j.rcim.2011.01.004,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2011-08-01,sciencedirect,Neuro-adaptive motion control with velocity observer in operational space formulation,https://api.elsevier.com/content/abstract/scopus_id/79955667317,"In this paper, a neuro-adaptive motion control with velocity observer is developed and validated in real-time experiment using a 6 DOF PUMA 560 robot. The controller is constructed for the operational space formulation, such that dynamic terms and the generalized force descriptions in this algorithm are expressed in the task space. The proposed strategy assumes no prior knowledge of the robot dynamics, and is formulated without assuming the availability of joint velocity feedback. As such, the controller takes only position feedback. This is an important feature as industrial robots are often fitted only with joint displacement sensors, not joint rate sensors. Stability analysis of the algorithm is analysed and presented in the paper. Real-time experiments on a 6 degrees of freedom (DOF) PUMA 560 manipulator were carried out to evaluate the effectiveness of the proposed NN adaptive control strategy with velocity observer and to compare the performance with the conventional inverse-dynamics control and a similar NN adaptive strategy using filtered velocity feedback, obtained through the backward difference of the displacement feedback. It should be noted that the experimental platform, PUMA 560, provides only joint displacement feedback through its joint mounted encoders. The results show a comparable results between the proposed strategy and the inverse-dynamics control law, without the need to perform dynamics identification procedures.",robotics
10.1016/j.eswa.2011.02.024,Journal,Expert Systems with Applications,scopus,2011-08-01,sciencedirect,Application of Neuro-Fuzzy Controller for Sumo Robot control,https://api.elsevier.com/content/abstract/scopus_id/79953695631,"This paper proposes the application of Neuro-Fuzzy (NF) hybrid system for Sumo Robot (SR) control. This robot is frequently designed by engineering students for robotic competition. As the relation between sensors output signals and motors control pulses is highly nonlinear in SR, soft computing techniques can be used to define this nonlinear relation and control of the robot in a competition ring. Application of intelligent methods for SR control not only simplifies robot control and improves robot responses during competition, but also encourages engineering students to use intelligent methods for solving real world’s problems. Regarding above rationale, a NF controller for SR control is proposed and implemented. Firstly, a Fuzzy Inference System (FIS) for detecting and tracking of the opponent in the competition ring is developed, which relates sensor output signals to motor control pulses. Secondly, Artificial Neural Networks (ANN) based learning algorithm is used for rule extraction and tuning the FIS parameters. The design approach of the proposed controller is presented in detail, and effectiveness of the controller is demonstrated by hardware implementation and experimental results. The results show that the intelligent control methods can be easily applied in various robot competitions by engineering students.",robotics
10.1016/j.neucom.2011.03.006,Journal,Neurocomputing,scopus,2011-07-01,sciencedirect,Robotic path planning in static environment using hierarchical multi-neuron heuristic search and probability based fitness,https://api.elsevier.com/content/abstract/scopus_id/79957968990,"Path Planning is a classical problem in the field of robotics. The problem is to find a path of the robot given the various obstacles. The problem has attracted the attention of numerous researchers due to the associated complexities, uncertainties and real time nature. In this paper we propose a new algorithm for solving the problem of path planning in a static environment. The algorithm makes use of an algorithm developed earlier by the authors called Multi-Neuron Heuristic Search (MNHS). This algorithm is a modified A⁎ algorithm that performs better than normal A⁎ when heuristics are prone to sharp changes. This algorithm has been implemented in a hierarchical manner, where each generation of the algorithm gives a more detailed path that has a higher reaching probability. The map used for this purpose is based on a probabilistic approach where we measure the probability of collision with obstacle while traveling inside the cell. As we decompose the cells, the cell size reduces and the probability starts to touch 0 or 1 depending upon the presence or absence of obstacles in the cell. In this approach, it is not compulsory to run the entire algorithm. We may rather break after a certain degree of certainty has been achieved. We tested the algorithm in numerous situations with varying degrees of complexities. The algorithm was able to give an optimal path in all the situations given. The standard A⁎ algorithm failed to give results within time in most of the situations presented.",robotics
10.1016/j.neunet.2011.02.004,Journal,Neural Networks,scopus,2011-06-01,sciencedirect,Learning parametric dynamic movement primitives from multiple demonstrations,https://api.elsevier.com/content/abstract/scopus_id/79953692970,"Learning from demonstration has shown to be a suitable approach for learning control policies (CPs). However, most previous studies learn CPs from a single demonstration, which results in limited scalability and insufficient generalization toward a wide range of applications in real environments. This paper proposes a novel approach to learn highly scalable CPs of basis movement skills from multiple demonstrations. In contrast to conventional studies with a single demonstration, i.e., dynamic movement primitives (DMPs), our approach efficiently encodes multiple demonstrations by shaping a parametric-attractor landscape in a set of differential equations. Assuming a certain similarity among multiple demonstrations, our approach learns the parametric-attractor landscape by extracting a small number of common factors in multiple demonstrations. The learned CPs allow the synthesis of novel movements with novel motion styles by specifying the linear coefficients of the bases as parameter vectors without losing useful properties of the DMPs, such as stability and robustness against perturbations. For both discrete and rhythmic movement skills, we present a unified learning procedure for learning a parametric-attractor landscape from multiple demonstrations. The feasibility and highly extended scalability of DMPs are demonstrated on an actual dual-arm robot.",robotics
10.1016/j.eswa.2010.12.080,Journal,Expert Systems with Applications,scopus,2011-06-01,sciencedirect,An intelligent framework to manage robotic autonomous agents,https://api.elsevier.com/content/abstract/scopus_id/79951576718,"In this paper a joint application of Artificial Intelligence (AI), robotics and Web services is described. The aim of the work presented here was to create a new integrated framework that keeps advantage on one side of the sensing and exploring capabilities of the robotic systems that work in the real world and, on the other side, of the information available via Web. Robots are conceived like (semi-)autonomous systems able to explore and manipulate a portion of their environment in order to find and collect information and data. On the other hand, the Web, that in a robotic domain is usually considered like a channel of communication (e.g. tele-operation, tele-manipulation), here is conceived also like a source of knowledge. This allows to define a new framework able to manage robotic agents in order to get precise, real-time information from the real world. Besides, software agents may search for and get additional information from the Web logical world. The intelligent administration of these services can be applied in different environments and leads to optimize procedures and solve practical problems. To this end a traffic control application has been defined and a simplified test-case implemented.",robotics
10.1016/j.neucom.2010.06.033,Journal,Neurocomputing,scopus,2011-05-01,sciencedirect,Incremental online sparsification for model learning in real-time robot control,https://api.elsevier.com/content/abstract/scopus_id/79956050250,"For many applications such as compliant, accurate robot tracking control, dynamics models learned from data can help to achieve both compliant control performance as well as high tracking quality. Online learning of these dynamics models allows the robot controller to adapt itself to changes in the dynamics (e.g., due to time-variant nonlinearities or unforeseen loads). However, online learning in real-time applications – as required in control – cannot be realized by straightforward usage of off-the-shelf machine learning methods such as Gaussian process regression or support vector regression. In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for fast real-time model learning. The proposed approach employs a sparsification method based on an independence measure. In combination with an incremental learning approach such as incremental Gaussian process regression, we obtain a model approximation method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real Barrett WAM robot demonstrates the applicability of the approach in real-time online model learning for real world systems.",robotics
10.1016/j.cogsys.2010.06.001,Journal,Cognitive Systems Research,scopus,2011-03-01,sciencedirect,"Objects, spatial compatibility, and affordances: A connectionist study",https://api.elsevier.com/content/abstract/scopus_id/77958510665,"In two Artificial Life simulations we evolved artificial organisms possessing a visual and a motor system, and whose nervous system was simulated with a neural network. Each organism could see four objects, either upright or reversed, with a left or a right handle. In Task 1 they learned to reach the object handle independently of the handle’s position. In Task 2 they learned to reach one of two buttons located below the handle either to decide where the handle was (Simulation 1) or whether the object was upright or reversed (Simulation 2). Task 1 simulated real life experience, Task 2 replicated either a classic spatial compatibility task (Simulation 1) or an experiment by Tucker and Ellis (1998) (Simulation 2). In both simulations learning occurred earlier in the Compatible condition, when the button to reach and the handle were on the same side, than in the Incompatible condition.",robotics
10.1016/j.media.2010.07.004,Journal,Medical Image Analysis,scopus,2011-02-01,sciencedirect,"Task-based performance analysis of FBP, SART and ML for digital breast tomosynthesis using signal CNR and Channelised Hotelling Observers",https://api.elsevier.com/content/abstract/scopus_id/78449232312,"We assess the performance of filtered backprojection (FBP), the simultaneous algebraic reconstruction technique (SART) and the maximum likelihood (ML) algorithm for digital breast tomosynthesis (DBT) under variations in key imaging parameters, including the number of iterations, number of projections, angular range, initial guess, and radiation dose. This is the first study to compare these algorithms for the application of DBT. We present a methodology for the evaluation of DBT reconstructions, and use it to conduct preliminary experiments investigating trade-offs between the selected imaging parameters. This investigation includes trade-offs not previously considered in the DBT literature, such as the use of a stationary detector versus a C-arm imaging geometry. A real breast CT volume serves as a ground truth digital phantom from which to simulate X-ray projections under the various acquisition parameters. The reconstructed image quality is measured using task-based metrics, namely signal CNR and the AUC of a Channelised Hotelling Observer with Laguerre–Gauss basis functions. The task at hand is the detection of a simulated mass inserted into the breast CT volume. We find that the image quality in limited view tomography is highly dependent on the particular acquisition and reconstruction parameters used. In particular, we draw the following conclusions. First, we find that optimising the FBP filter design and SART relaxation parameter yields significant improvements in reconstruction quality from the same projection data. Second, we show that the convergence rate of the maximum likelihood algorithm, optimised with paraboloidal surrogates and conjugate gradient ascent (ML-PSCG), can be greatly accelerated using view-by-view updates. Third, we find that the optimal initial guess is algorithm dependent. In particular, we obtained best results with a zero initial guess for SART, and an FBP initial guess for ML-PSCG. Fourth, when the exposure per view is constant, increasing the total number of views within a given angular range improves the reconstruction quality, albeit with diminishing returns. When the total dose of all views combined is constant, there is a trade-off between increased sampling using a larger number of views and increased levels of quantum noise in each view. Fifth, we do not observe significant differences when testing various access ordering schemes, presumably due to the limited angular range of DBT. Sixth, we find that adjusting the z-resolution of the reconstruction can improve image quality, but that this resolution is best adjusted by using post-reconstruction binning, rather than by declaring lower-resolution voxels. Seventh, we find that the C-arm configuration yields higher image quality than a stationary detector geometry, the difference being most outspoken for the FBP algorithm. Lastly, we find that not all prototype systems found in the literature are currently being run under the best possible system or algorithm configurations. In other words, the present study demonstrates the critical importance (and reward) of using optimisation methodologies such as the one presented here to maximise the DBT reconstruction quality from a single scan of the patient.",robotics
10.3182/20110828-6-IT-1002.01424,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2011-01-01,sciencedirect,Neural Network based system for real-time organ recognition during surgical operation,https://api.elsevier.com/content/abstract/scopus_id/84866753336,"Abstract
                  In this paper we propose (based on testing results) a Neural Network structures that can be used for recognition of presence and absence of internal organ on images received from endoscope. Based on selected NN structure we design two NN-based systems for distinguishing and real-time recognition of internal organs on sequence of endoscopic images during abdominal surgery. First NN-based system proposed in this paper is designed for recognition of several different internal organs on color endoscopic images. Second NN-based system is designed for real-time recognition of presence of a particular internal organ on a sequence of color images (video stream) from endoscope. Restricted connectivity structure of the network makes possible decomposition of the image during the analysis and significantly reduces the number of parameters thus making training easier, faster and more accurate. The algorithms proposed in the paper are implemented in software application and their effectiveness is demonstrated on simulations.",robotics
10.3182/20110828-6-IT-1002.03103,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2011-01-01,sciencedirect,UniBot Remote Laboratory: A scalable web-based set-up for education and experimental activities in robotics,https://api.elsevier.com/content/abstract/scopus_id/84866747278,"Abstract
                  The direct work on a real set-ups is an important experience for students in control theory and robotics. On the other hand, for several reasons (space, costs, complexity, etc.), it is not always possible to give students an individual access to laboratory set-ups, for their practical activities. Therefore, in recent years many tele-laboratories have been implemented by different universities, providing experimental set-ups to each student, while minimizing problems related to costs, spaces, and so on. The UniBot Remote Lab has been implemented to provide remote access via TCP connection, to assign to students different time-slots for their experiences, and to reduce the financial effort required by real set-ups. Moreover, the entire framework has been developed with high modularity both from the hardware and software point of view and, even if the basic set-up has been conceived for mobile robotics, different kind of robots or automatic machines can be easily added and be available for experimental activities.",robotics
10.3182/20110828-6-IT-1002.02080,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2011-01-01,sciencedirect,A LEGO Mindstorms multi-robot setup in the Automatic Control Telelab,https://api.elsevier.com/content/abstract/scopus_id/84866106157,"Abstract
                  This paper presents an experimental setup for multi-robot systems based on the LEGO Mindstorms NXT technology. The team of mobile robots is supervised by a vision system, which allows one to simulate different types of sensors and communication architectures. The whole setup is embedded in the Automatic Control Telelab (http://act.dii.unisi.itact.dii.unisi.it), a remote lab featuring several educational experiences in control. Remote users can design control laws for the multi-agent system in the Matlab environment and test them by performing real experiments in the proposed setup. The paper presents some experiments showing how this remote lab can stimulate students’ interest in mobile robotics.",robotics
10.1016/j.isatra.2010.08.002,Journal,ISA Transactions,scopus,2011-01-01,sciencedirect,Operational space trajectory tracking control of robot manipulators endowed with a primary controller of synthetic joint velocity,https://api.elsevier.com/content/abstract/scopus_id/78650509983,"In this paper, a new control algorithm for operational space trajectory tracking control of robot arms is introduced. The new algorithm does not require velocity measurement and is based on (1) a primary controller which incorporates an algorithm to obtain synthesized velocity from joint position measurements and (2) a secondary controller which computes the desired joint acceleration and velocity required to achieve operational space motion control. The theory of singularly perturbed systems is crucial for the analysis of the closed-loop system trajectories. In addition, the practical viability of the proposed algorithm is explored through real-time experiments in a two degrees-of-freedom horizontal planar direct-drive arm.",robotics
10.1016/j.conengprac.2010.10.002,Journal,Control Engineering Practice,scopus,2011-01-01,sciencedirect,Real time implementation of CTRNN and BPTT algorithm to learn on-line biped robot balance: Experiments on the standing posture,https://api.elsevier.com/content/abstract/scopus_id/78649998184,This paper describes experimental results regarding the real time implementation of continuous time recurrent neural networks (CTRNN) and the dynamic back-propagation through time (BPTT) algorithm for the on-line learning control laws. Experiments are carried out to control the balance of a biped robot prototype in its standing posture. The neural controller is trained to compensate for external perturbations by controlling the torso’s joint motions. Algorithms are embedded in the real time electronic unit of the robot. On-line learning implementations are presented in detail. The results on learning behavior and control performance demonstrate the strength and the efficiency of the proposed approach.,robotics
10.1016/j.robot.2010.08.004,Journal,Robotics and Autonomous Systems,scopus,2010-12-31,sciencedirect,Open-ended evolution as a means to self-organize heterogeneous multi-robot systems in real time,https://api.elsevier.com/content/abstract/scopus_id/78649913375,"This work deals with the application of multi-robot systems to real tasks and, in particular, their coordination through interaction based control systems. Within this field, the practical solutions that have been implemented in real robots mainly use strongly coordinated architectures and assignment strategies because of reliability and fault tolerance issues when addressing problems in reality. Emergent approaches have also been proposed with limited success, basically due to the unpredictability of the behaviors obtained. Here, an emergent approach, called r-ASiCo, is presented containing a procedure to produce predictable solutions and thus avoiding the typical problems associated with these techniques. The r-ASico algorithm is the real time version of the Asynchronous Situated Co-evolution algorithm (ASiCo), which exploits natural open-ended evolution to generate emergent complex collective behaviors and deals with systems made up of a huge number of elements and nonlinear interactions. The goal of r-ASiCo is to design the global behavior desired for the robot team as a collective entity and allow the emergence of behaviors through the interaction of the team members using social rules they learn to implement. To this end, r-ASiCo manages a series of features that are inherent to natural evolution based methods such as energy exchange and mating selection procedures, together with a technique to guide the evolution towards a design objective, the principled evaluation function selection procedure. Hence, this paper presents the components and operation of r-ASiCo and illustrates its application through a collective cleaning task example. It was implemented using 8 e-puck robots in two different real scenarios and its results complemented with those of a 30 e-puck case. The results show the capabilities of r-ASiCo to create a self-organized and adaptive multi-robot system configuration that is tolerant to environmental changes and to failures within the robot team.",robotics
10.1016/j.cviu.2010.03.017,Journal,Computer Vision and Image Understanding,scopus,2010-11-01,sciencedirect,A modified model for the Lobula Giant Movement Detector and its FPGA implementation,https://api.elsevier.com/content/abstract/scopus_id/77957362557,"Bio-inspired vision sensors are particularly appropriate candidates for navigation of vehicles or mobile robots due to their computational simplicity, allowing compact hardware implementations with low power dissipation. The Lobula Giant Movement Detector (LGMD) is a wide-field visual neuron located in the Lobula layer of the Locust nervous system. The LGMD increases its firing rate in response to both the velocity of an approaching object and the proximity of this object. It has been found that it can respond to looming stimuli very quickly and trigger avoidance reactions. It has been successfully applied in visual collision avoidance systems for vehicles and robots. This paper introduces a modified neural model for LGMD that provides additional depth direction information for the movement. The proposed model retains the simplicity of the previous model by adding only a few new cells. It has been simplified and implemented on a Field Programmable Gate Array (FPGA), taking advantage of the inherent parallelism exhibited by the LGMD, and tested on real-time video streams. Experimental results demonstrate the effectiveness as a fast motion detector.",robotics
10.1016/j.cogsys.2009.12.003,Journal,Cognitive Systems Research,scopus,2010-09-01,sciencedirect,Cognitive concepts in autonomous soccer playing robots,https://api.elsevier.com/content/abstract/scopus_id/77952550318,"Computational concepts of cognition, their implementation in complex autonomous systems, and their empirical evaluation are key techniques to understand and validate concepts of cognition and intelligence. In this paper we want to describe computational concepts of cognition that were successfully implemented in the domain of soccer playing robots and show the interactions between cognitive concepts, software engineering and real-time application development. Beside a description of the general concepts we will focus on aspects of perception, behavior architecture, and reinforcement learning.",robotics
10.1016/j.robot.2010.03.008,Journal,Robotics and Autonomous Systems,scopus,2010-07-31,sciencedirect,Policy gradient learning for quadruped soccer robots,https://api.elsevier.com/content/abstract/scopus_id/78049435621,"In real-world robotic applications, many factors, both at low level (e.g., vision, motion control and behaviors) and at high level (e.g., plans and strategies) determine the quality of the robot performance. Consequently, fine tuning of the parameters, in the implementation of the basic functionalities, as well as in the strategic decisions, is a key issue in robot software development. In recent years, machine learning techniques have been successfully used to find optimal parameters for typical robotic functionalities. However, one major drawback of learning techniques is time consumption: in practical applications, methods designed for physical robots must be effective with small amounts of data. In this paper, we present a method for concurrent learning of best strategy and optimal parameters using policy gradient reinforcement learning algorithm. The results of our experimental work in a simulated environment and on a real robot show a very high convergence rate.",robotics
10.1016/S1672-6529(09)60199-2,Journal,Journal of Bionic Engineering,scopus,2010-06-01,sciencedirect,Robotic etiquette: Socially acceptable navigation of service robots with human motion pattern learning and prediction,https://api.elsevier.com/content/abstract/scopus_id/77954275590,"Nonverbal and noncontact behaviors play a significant role in allowing service robots to structure their interactions with humans. In this paper, a novel human-mimic mechanism of robot's navigational skills was proposed for developing socially acceptable robotic etiquette. Based on the sociological and physiological concerns of interpersonal interactions in movement, several criteria in navigation were represented by constraints and incorporated into a unified probabilistic cost grid for safe motion planning and control, followed by an emphasis on the prediction of the human's movement for adjusting the robot's pre-collision navigational strategy. The human motion prediction utilizes a clustering-based algorithm for modeling humans' indoor motion patterns as well as the combination of the long-term and short-term tendency prediction that takes into account the uncertainties of both velocity and heading direction. Both simulation and real-world experiments verified the effectiveness and reliability of the method to ensure human's safety and comfort in navigation. A statistical user trials study was also given to validate the users' favorable views of the human-friendly navigational behavior.",robotics
10.1016/j.eswa.2009.11.030,Journal,Expert Systems with Applications,scopus,2010-05-01,sciencedirect,Intelligent omni-directional vision-based mobile robot fuzzy systems design and implementation,https://api.elsevier.com/content/abstract/scopus_id/73249122254,"An evolutional particle swarm optimization (PSO)-learning algorithm is proposed to automatically generate fuzzy decision rules. Due to the development of the fuzzy rule-based system, it actually regulates the omni-directional vision-based mobile robot for obstacle avoidance and desired target approximation as soon as possible. In the proposed image processing algorithm, an image direct transformation method is applied to convert the omni-directional scene into panoramic normal-view. Thus, the objects positions of obstacle and target are detected by the proposed color image segmentation. Human knowledge-based fuzzy systems demonstrate their well adaptability for nonlinear and time-variant features of the mobile robot to actually approach the desired location whatever it is surrounded in a known or unknown environment. In software simulations, the omni-directional mobile robot can move toward desired targets from different initial positions and various block sizes. In hardware implementations, the fuzzy control system embedded in actual mobile robot platform is used to real-time manipulate the omni-directional wheels through the motor drivers by the captured image positions of the obstacle and target. The selected fuzzy rules are efficient to control the direction and speed of omni-directional wheels to achieve the desired targets.",robotics
10.1016/j.patcog.2009.09.021,Journal,Pattern Recognition,scopus,2010-04-01,sciencedirect,On-line independent support vector machines,https://api.elsevier.com/content/abstract/scopus_id/74449087432,"Support vector machines (SVMs) are one of the most successful algorithms for classification. However, due to their space and time requirements, they are not suitable for on-line learning, that is, when presented with an endless stream of training observations.
                  In this paper we propose a new on-line algorithm, called on-line independent support vector machines (OISVMs), which approximately converges to the standard SVM solution each time new observations are added; the approximation is controlled via a user-defined parameter. The method employs a set of linearly independent observations and tries to project every new observation onto the set obtained so far, dramatically reducing time and space requirements at the price of a negligible loss in accuracy. As opposed to similar algorithms, the size of the solution obtained by OISVMs is always bounded, implying a bounded testing time. These statements are supported by extensive experiments on standard benchmark databases as well as on two real-world applications, namely place recognition by a mobile robot in an indoor environment and human grasping posture classification.",robotics
10.1016/j.eswa.2009.06.010,Journal,Expert Systems with Applications,scopus,2010-03-01,sciencedirect,Effective page recommendation algorithms based on distributed learning automata and weighted association rules,https://api.elsevier.com/content/abstract/scopus_id/71749090581,"Different efforts have been done to address the problem of information overload on the Internet. Recommender systems aim at directing users through this information space, toward the resources that best meet their needs and interests by extracting knowledge from the previous users’ interactions. In this paper, we propose three algorithms to solve the web page recommendation problem. In our first algorithm, we use distributed learning automata to learn the behavior of previous users’ and recommend pages to the current user based on learned patterns. By introducing a novel weighted association rule mining algorithm, we present our second algorithm for recommendation purpose. Also, a novel method is proposed to pure the current session window. One of the challenging problems in recommendation systems is dealing with unvisited or newly added pages. By considering this problem and improving the efficiency of first two algorithms we present a hybrid algorithm based on distributed learning automata and proposed weighted association rule mining algorithm. In the hybrid algorithm we employ the HITS algorithm to extend the recommendation set. Our experiments on real data set show that the hybrid algorithm performs better than the other algorithms we compared to and, at the same time, it is less complex than other proposed algorithms with respect to memory usage and computational cost too.",robotics
10.1016/j.asoc.2009.08.003,Journal,Applied Soft Computing Journal,scopus,2010-03-01,sciencedirect,Credit rating by hybrid machine learning techniques,https://api.elsevier.com/content/abstract/scopus_id/70649085937,"It is very important for financial institutions to develop credit rating systems to help them to decide whether to grant credit to consumers before issuing loans. In literature, statistical and machine learning techniques for credit rating have been extensively studied. Recent studies focusing on hybrid models by combining different machine learning techniques have shown promising results. However, there are various types of combination methods to develop hybrid models. It is unknown that which hybrid machine learning model can perform the best in credit rating. In this paper, four different types of hybrid models are compared by ‘Classification+Classification’, ‘Classification+Clustering’, ‘Clustering+Classification’, and ‘Clustering+Clustering’ techniques, respectively. A real world dataset from a bank in Taiwan is considered for the experiment. The experimental results show that the ‘Classification+Classification’ hybrid model based on the combination of logistic regression and neural networks can provide the highest prediction accuracy and maximize the profit.",robotics
10.1016/j.robot.2009.09.009,Journal,Robotics and Autonomous Systems,scopus,2010-02-28,sciencedirect,Bridging the gap between feature- and grid-based SLAM,https://api.elsevier.com/content/abstract/scopus_id/75149183985,"One important design decision for the development of autonomously navigating mobile robots is the choice of the representation of the environment. This includes the question of which type of features should be used, or whether a dense representation such as occupancy grid maps is more appropriate. In this paper, we present an approach which performs SLAM using multiple representations of the environment simultaneously. It uses reinforcement to learn when to switch to an alternative representation method depending on the current observation. This allows the robot to update its pose and map estimate based on the representation that models the surrounding of the robot in the best way. The approach has been implemented on a real robot and evaluated in scenarios, in which a robot has to navigate in- and outdoors and therefore switches between a landmark-based representation and a dense grid map. In practical experiments, we demonstrate that our approach allows a robot to robustly map environments which cannot be adequately modeled by either of the individual representations.",robotics
10.1016/j.cose.2009.07.007,Journal,Computers and Security,scopus,2010-02-01,sciencedirect,On the detection and identification of botnets,https://api.elsevier.com/content/abstract/scopus_id/71649087916,"We develop and discuss automated and self-adaptive systems for detecting and classifying botnets based on machine learning techniques and integration of human expertise. The proposed concept is purely passive and is based on analyzing information collected at three levels: (i) the payload of single packets received, (ii) observed access patterns to a darknet at the level of network traffic, and (iii) observed contents of TCP/IP traffic at the protocol level.
                  We illustrate experiments based on real-life data collected with a darknet set up for this purpose to show the potential of the proposed concept for Levels (i) and (ii). As darknets cannot capture TCP/IP traffic data, we use a small spamtrap in our experiments at Level (iii). Strictly speaking, this approach for Level (iii) is not purely passive. However, traffic moving through a network could potentially be analyzed in a similar way to also obtain a purely passive system at this level.",robotics
10.3182/20100831-4-fr-2021.00081,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2010-01-01,sciencedirect,Simultaneous estimation of role and response strategy in human-robot role-reversal imitation learning,https://api.elsevier.com/content/abstract/scopus_id/84901925233,"In this paper, we describe a novel imitation learning method which enables an autonomous robot to acquire response strategy and to estimate roles through human-robot realtime interaction. The robot becomes able to respond to human user's social action, e.g. bye-bye and shake hand, correctly. We constructed the learning method based on role reversal imitation which is found in human infants in developmental psychological researches. A probabilistic model is proposed which assumes that delayed reactions are stochastically generated by initiative actions. In an experiment, we show a robot hand became able to exhibit correct reaction and estimate whether another's action is an initiative action or a reaction.",robotics
10.1016/j.eswa.2010.02.131,Journal,Expert Systems with Applications,scopus,2010-01-01,sciencedirect,Online tuning gain scheduling MIMO neural PID control of the 2-axes pneumatic artificial muscle (PAM) robot arm,https://api.elsevier.com/content/abstract/scopus_id/80053611092,This paper presents a detailed study to investigate the possibility of applying the online tuning gain scheduling MIMO neural dynamic DNN-PID control architecture to a nonlinear 2-axes pneumatic artificial muscle (PAM) robot arm so as to improve its joint angle position output performance. The proposed controller was implemented as a subsystem to control the real-time 2-axes PAM robot-arm system so as to control precisely the joint angle position of the 2-axes PAM robot arm when subjected to system internal interactions and load variations. The results of the experiment have demonstrated the feasibility and benefits of the novel proposed control approach in comparison with the traditional PID control strategy. The proposed gain scheduling neural MIMO DNN-PID control scheme forced both joint angle outputs of 2-axes PAM robot arm to track those of the reference simultaneously under changes of the load and system coupled internal interactions. The performance of this novel proposed controller was found to be outperforming in comparison with conventional PID. These results can be applied to control other highly nonlinear systems.,robotics
10.1016/j.robot.2010.04.001,Journal,Robotics and Autonomous Systems,scopus,2010-01-01,sciencedirect,Visual servoing of redundant manipulator with Jacobian matrix estimation using self-organizing map,https://api.elsevier.com/content/abstract/scopus_id/80052724345,"Vision based redundant manipulator control with a neural network based learning strategy is discussed in this paper. The manipulator is visually controlled with stereo vision in an eye-to-hand configuration. A novel Kohonen’s self-organizing map (KSOM) based visual servoing scheme has been proposed for a redundant manipulator with 7 degrees of freedom (DOF). The inverse kinematic relationship of the manipulator is learned using a Kohonen’s self-organizing map. This learned map is shown to be an approximate estimate of the inverse Jacobian, which can then be used in conjunction with the proportional controller to achieve closed loop servoing in real-time. It is shown through Lyapunov stability analysis that the proposed learning based servoing scheme ensures global stability. A generalized weight update law is proposed for KSOM based inverse kinematic control, to resolve the redundancy during the learning phase. Unlike the existing visual servoing schemes, the proposed KSOM based scheme eliminates the computation of the pseudo-inverse of the Jacobian matrix in real-time. This makes the proposed algorithm computationally more efficient. The proposed scheme has been implemented on a 7 DOF PowerCube™ robot manipulator with visual feedback from two cameras.",robotics
10.1016/j.jfranklin.2009.10.019,Journal,Journal of the Franklin Institute,scopus,2010-01-01,sciencedirect,Decentralized neural identification and control for uncertain nonlinear systems: Application to planar robot,https://api.elsevier.com/content/abstract/scopus_id/79251594751,"This paper presents a discrete-time decentralized neural identification and control for large-scale uncertain nonlinear systems, which is developed using recurrent high order neural networks (RHONN); the neural network learning algorithm uses an extended Kalman filter (EKF). The discrete-time control law proposed is based on block control and sliding mode techniques. The control algorithm is first simulated, and then implemented in real time for a two degree of freedom (DOF) planar robot.",robotics
10.1016/j.patcog.2010.05.030,Journal,Pattern Recognition,scopus,2010-01-01,sciencedirect,Viewpoint independent object recognition in cluttered scenes exploiting ray-triangle intersection and SIFT algorithms,https://api.elsevier.com/content/abstract/scopus_id/78049324913,"Viewpoint independent recognition of free-form objects and estimation of their exact position are a complex procedure with applications in robotics, artificial intelligence, computer vision and many other scientific fields. In this paper a novel approach is presented that addresses recognition of objects lying in highly cluttered and occluded scenes. The proposed procedure relies on distance maps, which are extracted and stored off-line for each of the 3D objects that might be contained in the scene. During the on-line recognition procedure distance maps are extracted from the scene. Greyscale images, derived from scene's distance maps, are matched with those of the object under recognition by applying similarity measures to the descriptors that are extracted from the images. The similarity is then estimated from image patches, which are defined using the SIFT descriptor in an appropriate way. After finding the best similarities the position of the object in the scene is estimated. This process is repeated until all objects are successfully recognized. Multiple experiments, which were performed on both 2.5D synthetic and real scenes, proved that the proposed method is robust and highly efficient to a satisfactory degree of occlusion and clutter.",robotics
