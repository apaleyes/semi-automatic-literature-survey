id,type,publication,publisher,publication_date,database,title,url,abstract,domain
10.1016/j.inffus.2021.09.004,Journal,Information Fusion,scopus,2022-02-01,sciencedirect,Multimodal Earth observation data fusion: Graph-based approach in shared latent space,https://api.elsevier.com/content/abstract/scopus_id/85115401406,"Multiple and heterogenous Earth observation (EO) platforms are broadly used for a wide array of applications, and the integration of these diverse modalities facilitates better extraction of information than using them individually. The detection capability of the multispectral unmanned aerial vehicle (UAV) and satellite imagery can be significantly improved by fusing with ground hyperspectral data. However, variability in spatial and spectral resolution can affect the efficiency of such dataset's fusion. In this study, to address the modality bias, the input data was projected to a shared latent space using cross-modal generative approaches or guided unsupervised transformation. The proposed adversarial networks and variational encoder-based strategies used bi-directional transformations to model the cross-domain correlation without using cross-domain correspondence. It may be noted that an interpolation-based convolution was adopted instead of the normal convolution for learning the features of the point spectral data (ground spectra). The proposed generative adversarial network-based approach employed dynamic time wrapping based layers along with a cyclic consistency constraint to use the minimal number of unlabeled samples, having cross-domain correlation, to compute a cross-modal generative latent space. The proposed variational encoder-based transformation also addressed the cross-modal resolution differences and limited availability of cross-domain samples by using a mixture of expert-based strategy, cross-domain constraints, and adversarial learning. In addition, the latent space was modelled to be composed of modality independent and modality dependent spaces, thereby further reducing the requirement of training samples and addressing the cross-modality biases. An unsupervised covariance guided transformation was also proposed to transform the labelled samples without using cross-domain correlation prior. The proposed latent space transformation approaches resolved the requirement of cross-domain samples which has been a critical issue with the fusion of multi-modal Earth observation data. This study also proposed a latent graph generation and graph convolutional approach to predict the labels resolving the domain discrepancy and cross-modality biases. Based on the experiments over different standard benchmark airborne datasets and real-world UAV datasets, the developed approaches outperformed the prominent hyperspectral panchromatic sharpening, image fusion, and domain adaptation approaches. By using specific constraints and regularizations, the network developed was less sensitive to network parameters, unlike in similar implementations. The proposed approach illustrated improved generalizability in comparison with the prominent existing approaches. In addition to the fusion-based classification of the multispectral and hyperspectral datasets, the proposed approach was extended to the classification of hyperspectral airborne datasets where the latent graph generation and convolution were employed to resolve the domain bias with a small number of training samples. Overall, the developed transformations and architectures will be useful for the semantic interpretation and analysis of multimodal data and are applicable to signal processing, manifold learning, video analysis, data mining, and time series analysis, to name a few.",autonomous vehicle
10.1016/j.dsp.2021.103290,Journal,Digital Signal Processing: A Review Journal,scopus,2022-01-01,sciencedirect,Deep residual learning-based cognitive model for detection and classification of transmitted signal patterns in 5G smart city networks,https://api.elsevier.com/content/abstract/scopus_id/85118634214,"Primary user (PU) signal detection or classification is a critical component of cognitive radio (CR) related wireless communication applications. In CR, the PU detection methods are mostly based on statistical models, and their detection performance heavily relies on the accuracy of assumed models. In this paper, we design a novel detector, dubbed as PU-Net, that dynamically learns the PU activity patterns in a cognitive 5G smart city, where a network of unmanned aerial vehicles (UAVs) is deployed as flying base stations to serve the Internet-of-Things (IoT) users. Unlike the traditional schemes, the PU-Net is free from signal-noise model assumptions and is leveraged through deep residual learning integrated with atrous spatial pyramid pooling (ASPP) to sense the PU's transmitted signal patterns in the network. The PU-Net detects and classifies the active and idle PU states by exploiting the multilevel spatial-temporal features in the signal and noise frames. The proposed model is trained using locally synthesized Rayleigh channel-impaired data with large variability of modulated signals and different noise floor regimes. Additionally, the PU-Net model is blind-tested and evaluated on real-world over-the-air signals and with variable-length frames and varying channel effects at secondary users (SUs). With extensive experiments, it is shown that PU-Net outperforms other benchmark detectors, obtaining an accuracy of 0.9974, with 0.9978 recall and 0.9970 precision in detecting and classifying the PU transmitted signal patterns. Correspondingly, the proposed PU-Net can be adopted for IoT/UAV-assisted communication systems in optimizing spectrum efficiency and resolving the coexistence issues in 5G and beyond networks.",autonomous vehicle
10.1016/j.inffus.2021.07.004,Journal,Information Fusion,scopus,2022-01-01,sciencedirect,SaccadeFork: A lightweight multi-sensor fusion-based target detector,https://api.elsevier.com/content/abstract/scopus_id/85112374720,"Commercialization of self-driving applications requires precision and reliability of the perception system due to the highly dynamic and complex road environment. Early perception systems either rely on the camera or on LiDAR for moving obstacle detection. With the development of vehicular sensors and deep learning technologies, the multi-view and sensor fusion based convolutional neural network (CNN) model for detection tasks has become a popular research area. In this paper, we present a novel multi-sensor fusion-based CNN model–SaccadeFork–that integrates the image and upsampled LiDAR point clouds as the input. SaccadeFork includes two modules: (1) a lightweight backbone that consists of hourglass convolution feature extraction module and a parallel dilation convolution module for adaptation of the system to different target sizes; (2) an anchor-based detection head. The model also considers deployment of resource-limited edge devices in the vehicle. Two refinement strategies, i.e., Mixup and Swish activation function are also adopted to improve the model. Comparison with a series of latest models on public dataset of KITTI shows that SaccadeFork can achieve the optimal detection accuracy on vehicles and pedestrians under different scenarios. The final model is also deployed and tested on a local dataset collected based on edge devices and low-cost sensor solutions, and the results show that the model can achieve real-time efficiency and high detection accuracy.",autonomous vehicle
10.1016/j.eswa.2021.115380,Journal,Expert Systems with Applications,scopus,2021-11-30,sciencedirect,Intelligent control of an UAV with a cable-suspended load using a neural network estimator,https://api.elsevier.com/content/abstract/scopus_id/85111010813,"Unmanned aerial vehicles (UAVs) have been proved very useful in civil and military sectors: defense, security, shipping, construction, agriculture, entertainment, etc. Some of these applications, especially those related to transport and logistic operations, require the use of suspended loads that may make the vehicle unstable. In order to deal with this non-linear complex system with a changing mass, further research on modelling and control must be developed. In this work, a new intelligent control strategy is proposed and applied to a quadrotor with a cable-suspended load. The UAV carrying a suspended load has two different dynamic behaviors, depending on the state of the cable. Thus, we proposed to model the complete system using the hybrid automata formalism. Using this novel UAV model approach, a hybrid control is designed based on feedback linearization controllers combined with an artificial neural network, which acts as an online estimator of the unknown mass. The suspended load is dealt with as an external disturbance. Simulation results show how the on-line learning control scheme increases the robustness of the control and it is able to stabilize the quadrotor without any information about neither the position of the load nor the tension of the cable. Additionally, the computational complexity of the proposal is studied to show the feasibility of the implementation of this intelligent control strategy on real hardware.",autonomous vehicle
10.1016/j.eswa.2021.115343,Journal,Expert Systems with Applications,scopus,2021-11-30,sciencedirect,A framework for 3D tracking of frontal dynamic objects in autonomous cars,https://api.elsevier.com/content/abstract/scopus_id/85108361303,"Both recognition and 3D tracking of frontal dynamic objects are crucial problems in an autonomous vehicle, while depth estimation as an essential issue becomes a challenging problem using a monocular camera. Since both camera and objects are moving, the issue can be formed as a structure from motion (SFM) problem. In this paper, to elicit features from an image, the YOLOv3 approach is utilized beside an OpenCV tracker. Subsequently, to obtain the lateral and longitudinal distances, a nonlinear SFM model is considered alongside a state-dependent Riccati equation (SDRE) filter and a newly developed observation model. Additionally, a switching method in the form of switching estimation error covariance is proposed to enhance the robust performance of the SDRE filter. The stability analysis of the presented filter is conducted on a class of discrete nonlinear systems. Furthermore, the ultimate bound of estimation error caused by model uncertainties is analytically obtained to investigate the switching significance. Simulations are reported to validate the performance of the switched SDRE filter. Finally, real-time experiments are performed through a multi-thread framework implemented on a Jetson TX2 board, while radar data is used for the evaluation.",autonomous vehicle
10.1016/j.neucom.2021.09.008,Journal,Neurocomputing,scopus,2021-11-20,sciencedirect,Fast intent prediction of multi-cyclists in 3D point cloud data using deep neural networks,https://api.elsevier.com/content/abstract/scopus_id/85115024624,"Inferring the intended actions of road-sharing users with autonomous ground vehicles in particularly vulnerable ones like cyclists is considered one of the tough tasks facing the wide-spread deployment of autonomous ground vehicles. One of the main reasons for that is the scarcity of the available datasets for that task due to the difficulty in obtaining those datasets in real environments. In this work, we first propose a pipeline that can synthetically produce 3D LiDAR data of cyclists hand-signalling a set of intended actions that are commonly done in real environments. Given the synthetically-produced labelled 3D LiDAR data sequences, we trained a framework that can simultaneously detect, track and give predictions about the intended actions of multi-cyclists in the scene on time. The proposed framework was evaluated using both synthetic and real data from a physical 3D LiDAR sensor. Our proposed framework has scored competitive and robust results in both synthetic and real environments with 88% in 
                        
                           
                              
                                 F
                              
                              
                                 1
                              
                           
                        
                      measure with higher frame per second rate (12.9 FPS) than the 3D LiDAR sensor frame rate (10 Hz).",autonomous vehicle
10.1016/j.oceaneng.2021.109531,Journal,Ocean Engineering,scopus,2021-09-15,sciencedirect,A remote anomaly detection system for Slocum underwater gliders,https://api.elsevier.com/content/abstract/scopus_id/85109609494,"Marine Autonomous Systems (MAS) operating at sea beyond visual line of sight need to be self-reliant, as any malfunction could lead to loss or pose a risk to other sea users. In the absence of fully automated on-board control and fault detection tools, MAS are piloted and monitored by experts, resulting in high operational costs and limiting the scale of observational fleets that can be deployed simultaneously. Hence, an effective anomaly detection system is fundamental to increase fleet capacity and reliability. In this study, an on-line, remote fault detection system is developed for underwater gliders. Two alternative methods are analysed using time series data: feedforward deep neural networks estimating the glider’s vertical velocity and an autoencoder. The systems are trained using field data from four baseline deployments of Slocum gliders and tested on six deployments of vehicles suffering from adverse behaviour. The methods are able to successfully detect a range of anomalies in the near real time data streams, whilst being able to generalise to different glider configurations. The autoencoder’s error in reconstructing the original signals is the clearest indicator of anomalies. Thus, the autoencoder is a prime candidate to be included into an all-encompassing condition monitoring system for MAS.",autonomous vehicle
10.1016/j.eswa.2021.114937,Journal,Expert Systems with Applications,scopus,2021-09-15,sciencedirect,Search and rescue operation using UAVs: A case study,https://api.elsevier.com/content/abstract/scopus_id/85104654242,"Many people go missing in the wild every year. In this paper, the Search and Rescue (SAR) mission is conducted using a novel system comprising an Unmanned Aerial Vehicle (UAV) coupled with real-time machine-learning-based object detection system embedded on a smartphone. Human detection from UAV in the wilderness is a challenging task, because of many constraints involved such as lack of computing and communication infrastructures. We proposed a novel combination of a robust architecture deployed on a smartphone and a novel Convolutional Neural Network (CNN) model to fulfil the goals of the project. Our approach achieved 94.73% of accuracy and 6.8 FPS on a smartphone. Our approach is highly portable, cost-effective, fast with high accuracy. This novel system is expected to contribute significantly to maximise chances of saving lives in the wild. This developed system has been recently launched by Police Scotland to facilitate the SAR teams to locate missing persons in Scotland wilderness.",autonomous vehicle
10.1016/j.trc.2021.103272,Journal,Transportation Research Part C: Emerging Technologies,scopus,2021-09-01,sciencedirect,Spatial-temporal pricing for ride-sourcing platform with reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85108979514,"Ever since the emergence of ride-sourcing services, the spatial–temporal pricing problem has been a hot research topic in both the transportation and management fields. The difficulty lies in simultaneously obtaining the optimal multivariable solution for spatial pricing and sequential solution for dynamic pricing, considering the heterogeneity, dynamics, and imbalance of on-demand ride supply/demand. Due to this problem's complexity, most studies have simplified the modeling setting and omitted the complicated matching and waiting process between drivers and passengers. To go beyond the existing models, this paper proposes a reinforcement learning enhanced agent-based modeling and simulation (RL-ABMS) system to reveal the complex mechanism in the ride-sourcing system and tackle the problem of spatial–temporal pricing for a ride-sourcing platform. The reinforcement learning approach proximal policy optimization (PPO) is implemented in the RL-ABMS system, where two feed-forward neural networks are built as critic and actor. The critic judges the goodness of the current state, and the actor generates the optimal pricing strategy.Compared with the fixed pricing strategy, the experimental results on a real-world urban network show that dynamic pricing raises the platform's profit to 1.25 times, and spatial–temporal pricing even raises it to 1.85 times. Besides, the number of idle drivers/vehicles has significantly dropped under the spatial–temporal pricing strategy, which indicates that our proposed strategy has a remarkable effect on coordinating supply and demand in the ride-sourcing market.",autonomous vehicle
10.1016/j.eswa.2021.114820,Journal,Expert Systems with Applications,scopus,2021-08-01,sciencedirect,Machine Learning for industrial applications: A comprehensive literature review,https://api.elsevier.com/content/abstract/scopus_id/85102967505,"Machine Learning (ML) is a branch of artificial intelligence that studies algorithms able to learn autonomously, directly from the input data. Over the last decade, ML techniques have made a huge leap forward, as demonstrated by Deep Learning (DL) algorithms implemented by autonomous driving cars, or by electronic strategy games. Hence, researchers have started to consider ML also for applications within the industrial field, and many works indicate ML as one the main enablers to evolve a traditional manufacturing system up to the Industry 4.0 level. Nonetheless, industrial applications are still few and limited to a small cluster of international companies. This paper deals with these topics, intending to clarify the real potentialities, as well as potential flaws, of ML algorithms applied to operation management. A comprehensive review is presented and organized in a way that should facilitate the orientation of practitioners in this field. To this aim, papers from 2000 to date are categorized in terms of the applied algorithm and application domain, and a keyword analysis is also performed, to details the most promising topics in the field. What emerges is a consistent upward trend in the number of publications, with a spike of interest for unsupervised and especially deep learning techniques, which recorded a very high number of publications in the last five years. Concerning trends, along with consolidated research areas, recent topics that are growing in popularity were also discovered. Among these, the main ones are production planning and control and defect analysis, thus suggesting that in the years to come ML will become pervasive in many fields of operation management.",autonomous vehicle
10.1016/j.jterra.2020.12.002,Journal,Journal of Terramechanics,scopus,2021-08-01,sciencedirect,Recurrent and convolutional neural networks for deep terrain classification by autonomous robots,https://api.elsevier.com/content/abstract/scopus_id/85099601850,"The future challenge for field robots is to increase the level of autonomy towards long distance (>1 km) and duration (>1h) applications. One of the key technologies is the ability to accurately estimate the properties of the traversed terrain to optimize onboard control strategies and energy efficient path-planning, ensuring safety and avoiding possible immobilization conditions that would lead to mission failure. Two main hypotheses are put forward in this research. The first hypothesis is that terrain can be effectively detected by relying exclusively on the measurement of quantities that pertain to the robot-ground interaction, i.e., on proprioceptive signals. Therefore, no visual or depth information is required. Then, artificial deep neural networks can provide an accurate and robust solution to the classification problem of different terrain types. Under these hypotheses, sensory signals are classified as time series directly by a Recurrent Neural Network or by a Convolutional Neural Network in the form of higher-level features or spectrograms resulting from additional processing. In both cases, results obtained from real experiments show comparable or better performance when contrasted with standard Support Vector Machine with the additional advantage of not requiring an a priori definition of the feature space.",autonomous vehicle
10.1016/j.bdr.2021.100241,Journal,Big Data Research,scopus,2021-07-15,sciencedirect,“Brains” for Robots: Application of the Mivar Expert Systems for Implementation of Autonomous Intelligent Robots,https://api.elsevier.com/content/abstract/scopus_id/85107957013,"Recently the contemporary robotic systems can manipulate different objects and make decisions in a range of situations due to significant advances in innovation technologies and artificial intelligence. The new expert technologies can handle millions of instructions on computers and smartphones, which allow them to be used as a tool to create “decision-making systems” for autonomous robots. The goal of this paper was to create a dynamic algorithm of robot actions that can be used in the decision module has been considered. It is proposed to use Mivar expert systems of a new generation for high-level control. The experiment results showed that Mivar decision-making systems can control groups of small robots and even an unmanned autonomous car in real time. The algorithms created in the Mivar environment can be very flexible, and their build-up depends only on engineering approaches. In addition to traditional low-level robot control systems, a Mivar decision-making system has been implemented, which can be considered as universal “Brains” for autonomous intelligent robots and now knowledge bases can be created and various robots can be trained for practical tasks.",autonomous vehicle
10.1016/j.rse.2021.112434,Journal,Remote Sensing of Environment,scopus,2021-07-01,sciencedirect,Estimation of root zone soil moisture from ground and remotely sensed soil information with multisensor data fusion and automated machine learning,https://api.elsevier.com/content/abstract/scopus_id/85104083550,"Root zone soil moisture (RZSM) estimation and monitoring based on high spatial resolution remote sensing information such as obtained with an Unmanned Aerial System (UAS) is of significant interest for field-scale precision irrigation management, particularly in water-limited regions of the world. To date, there is no accurate and widely accepted model that relies on UAS optical surface reflectance observations for RZSM estimation at high spatial resolution. This study is aimed at the development of a new approach for RZSM estimation based on the fusion of high spatial resolution optical reflectance UAS observations with physical and hydraulic soil information integrated into Automated Machine Learning (AutoML). The H2O AutoML platform includes a number of advanced machine learning algorithms that efficiently perform feature selection and automatically identify complex relationships between inputs and outputs. Twelve models combining UAS optical observations with various soil properties were developed in a hierarchical manner and fed into AutoML to estimate surface, near-surface, and root zone soil moisture. The addition of independently measured surface and near-surface soil moisture information to the hierarchical models to improve RZSM estimation was investigated. The accuracy of soil moisture estimates was evaluated based on a comparison with Time Domain Reflectometry (TDR) sensors that were deployed to monitor surface, near-surface and root zone soil moisture dynamics. The obtained results indicate that the consideration of physical and hydraulic soil properties together with UAS optical observations improves soil moisture estimation, especially for the root zone with a RMSE of about 0.04 cm3 cm−3. Accurate RZSM estimates were obtained when measured surface and near-surface soil moisture data was added to the hierarchical models, yielding RMSE values below 0.02 cm3 cm−3 and R and NSE values above 0.90. The generated high spatial resolution RZSM maps clearly capture the spatial variability of soil moisture at the field scale. The presented framework can aid farm scale precision irrigation management via improving the crop water use efficiency and reducing the risk of groundwater contamination.",autonomous vehicle
10.1016/j.comnet.2021.108057,Journal,Computer Networks,scopus,2021-06-19,sciencedirect,A deep reinforcement learning-based multi-optimality routing scheme for dynamic IoT networks,https://api.elsevier.com/content/abstract/scopus_id/85104075448,"With the development of Internet of Things (IoT) and 5G technologies, more and more applications, such as autonomous vehicles and tele-medicine, become more sensitive to network latency and accuracy, which require routing schemes to be more flexible and efficient. In order to meet such urgent need, learning-based routing strategies are emerging as strong candidate solutions, with the advantages of high flexibility and accuracy. These strategies can be divided into two categories, centralized and distributed, enjoying the advantages of high precision and high efficiency, respectively. However, routing becomes more complex in dynamic IoT network, where the link connections and access states are time-varying, hence these learning-based routing mechanisms are required to have the capability to adapt to network changes in real time. In this paper, we designed and implemented both centralized and distributed Reinforcement Learning-based Routing schemes combined with Multi-optimality routing criteria (RLR-M). By conducting a series of experiments, we performed a comprehensive analysis of the results and arrived at the conclusion that the centralized is better suited to cope with dynamic networks due to its faster reconvergence (2.2 
                        ×
                      over distributed), while the distributed is better positioned to handle with large-scale networks through its high scalability (1.6 
                        ×
                      over centralized). Moreover, the multi-optimality routing scheme is implemented through model fusion, which is more flexible than traditional strategies and as such is better placed to meet the needs of IoT.",autonomous vehicle
10.1016/j.treng.2021.100068,Journal,Transportation Engineering,scopus,2021-06-01,sciencedirect,Real-time traffic quantization using a mini edge artificial intelligence platform,https://api.elsevier.com/content/abstract/scopus_id/85111397458,"Traffic analysis is dependent on reliable and accurate datasets that quantify the vehicle composition, speed and traffic density over a long period of time. The utilisation of big data is required if equitable and efficient transportation networks are to be realised for smart, interconnected cities of the future. The rapid and widespread adoption of digital twins, IoT (Internet of Things), artificial intelligence and mini edge computing technologies serve as the catalyst to rapidly develop and deploy smart systems for real-time data acquisition of traffic in and around urban and metropolitan areas. This paper presents a proof of concept of a mini edge computing platform for real-time edge processing, which serves as a digital twin of a multi-lane freeway located in Pretoria, South Africa. Video data acquired from an Unmanned Aerial Vehicle (UAV) is processed using a neural network architecture designed for real-time object detection tracking of vehicles. The implementation successfully counted vehicles (cars and trucks) together with an estimation of the speed of each detected vehicle. These results compare favourably to the ground truth data with vehicle counting accuracies of 5% realised. Detection of sparse motorcycles and pedestrians were less than optimal. This proof of concept can be easily scaled and deployed over a wide geographic area. Integration of these cyber-physical assets can be incorporated into existing video monitoring systems or fused with optical sensors as a single data acquisition system.",autonomous vehicle
10.1016/j.bios.2021.113088,Journal,Biosensors and Bioelectronics,scopus,2021-05-15,sciencedirect,Machine learning-based cytokine microarray digital immunoassay analysis,https://api.elsevier.com/content/abstract/scopus_id/85101500120,"Serial measurement of a large panel of protein biomarkers near the bedside could provide a promising pathway to transform the critical care of acutely ill patients. However, attaining the combination of high sensitivity and multiplexity with a short assay turnaround poses a formidable technological challenge. Here, the authors develop a rapid, accurate, and highly multiplexed microfluidic digital immunoassay by incorporating machine learning-based autonomous image analysis. The assay has achieved 12-plexed biomarker detection in sample volume <15 μL at concentrations < 5 pg/mL while only requiring a 5-min assay incubation, allowing for all processes from sampling to result to be completed within 40 min. The assay procedure applies both a spatial-spectral microfluidic encoding scheme and an image data analysis algorithm based on machine learning with a convolutional neural network (CNN) for pre-equilibrated single-molecule protein digital counting. This unique approach remarkably reduces errors facing the high-capacity multiplexing of digital immunoassay at low protein concentrations. Longitudinal data obtained for a panel of 12 serum cytokines in human patients receiving chimeric antigen receptor-T (CAR-T) cell therapy reveals the powerful biomarker profiling capability. The assay could also be deployed for near-real-time immune status monitoring of critically ill COVID-19 patients developing cytokine storm syndrome.",autonomous vehicle
10.1016/j.eswa.2020.114549,Journal,Expert Systems with Applications,scopus,2021-05-15,sciencedirect,A novel pipeline framework for multi oriented scene text image detection and recognition,https://api.elsevier.com/content/abstract/scopus_id/85098992612,"Automatic text detection and recognition (end-to-end text recognition) in real-life images are the main elements of many applications including blind and low vision assistance systems and self-driving cars. However, it is challenging to detect curved and vertical texts due to their color bleeding, font size variation, and complicated background. In this paper, a convolutional neural network-based pipeline is introduced to obtain high-level visual features and improve text detection and recognition efficiency. A pre-trained ResNet-50 network on ImageNet and SynthText for extracting low-level visual features was used in this study. Moreover, new improved ReLU layer (new.i.ReLU) blocks are used with a varied receptive field with a strong ability to detect text components even on curved surfaces in the proposed structure. A new improved inception layer (new.i.inception layers) can obtain broadly varying-sized text more effectively than a linear chain of convolution layer. Also, we have proposed a pipeline framework for character recognition that is robust to irregular (curve and vertical) text. First, we introduced a novel algorithm for encoding pixel’s value to a new one called local word directional pattern (LWDP) that highlights the texture of the characters. Then, the output of LWDP was presented as an input image in the text recognition process. The experiments on standard benchmarks, including ICDAR 2013, ICDAR 2015, and ICDAR 2019 datasets, illustrated the superiority of the proposed architecture over prior works.",autonomous vehicle
10.1016/j.compag.2021.106091,Journal,Computers and Electronics in Agriculture,scopus,2021-05-01,sciencedirect,DeepWay: A Deep Learning waypoint estimator for global path generation,https://api.elsevier.com/content/abstract/scopus_id/85103275872,"Agriculture 3.0 and 4.0 have gradually introduced service robotics and automation into several agricultural processes, mostly improving crops quality and seasonal yield. Row-based crops are the perfect settings to test and deploy smart machines capable of monitoring and manage the harvest. In this context, global path generation is essential either for ground or aerial vehicles, and it is the starting point for every type of mission plan. Nevertheless, little attention has been currently given to this problem by the research community and global path generation automation is still far to be solved. In order to generate a viable path for an autonomous machine, the presented research proposes a feature learning fully convolutional model capable of estimating waypoints given an occupancy grid map. In particular, we apply the proposed data-driven methodology to the specific case of row-based crops with the general objective to generate a global path able to cover the extension of the crop completely. Extensive experimentation with a custom made synthetic dataset and real satellite-derived images of different scenarios have proved the effectiveness of our methodology and demonstrated the feasibility of an end-to-end and completely autonomous global path planner.",autonomous vehicle
10.1016/j.isprsjprs.2021.01.027,Journal,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2021-04-01,sciencedirect,VPC-Net: Completion of 3D vehicles from MLS point clouds,https://api.elsevier.com/content/abstract/scopus_id/85101520322,"As a dynamic and essential component in the road environment of urban scenarios, vehicles are the most popular investigation targets. To monitor their behavior and extract their geometric characteristics, an accurate and instant measurement of vehicles plays a vital role in traffic and transportation fields. Point clouds acquired from the mobile laser scanning (MLS) system deliver 3D information of road scenes with unprecedented detail. They have proven to be an adequate data source in the fields of intelligent transportation and autonomous driving, especially for extracting vehicles. However, acquired 3D point clouds of vehicles from MLS systems are inevitably incomplete due to object occlusion or self-occlusion. To tackle this problem, we proposed a neural network to synthesize complete, dense, and uniform point clouds for vehicles from MLS data, named Vehicle Points Completion-Net (VPC-Net). In this network, we introduce a new encoder module to extract global features from the input instance, consisting of a spatial transformer network and point feature enhancement layer. Moreover, a new refiner module is also presented to preserve the vehicle details from inputs and refine the complete outputs with fine-grained information. Given sparse and partial point clouds as inputs, the network can generate complete and realistic vehicle structures and keep the fine-grained details from the partial inputs. We evaluated the proposed VPC-Net in different experiments using synthetic and real-scan datasets and applied the results to 3D vehicle monitoring tasks. Quantitative and qualitative experiments demonstrate the promising performance of the proposed VPC-Net and show state-of-the-art results.",autonomous vehicle
10.1016/j.neucom.2020.03.121,Journal,Neurocomputing,scopus,2021-03-07,sciencedirect,Detecting urban hot regions by using massive geo-tagged image data,https://api.elsevier.com/content/abstract/scopus_id/85089358407,"Detecting hot regions plays an important role in urban traffic planning and analytics, which is also useful in self-driving car routing and navigation. In this light, we propose and study a novel urban hot-region detection method by using massive geo-tagged image data. Given a set Q of regions (each region q is made up by a set of geo-tagged images), and a matching threshold 
                        
                           θ
                        
                     , if a region q is matched with m other regions, its hot degree is defined by m. The hot-region detection (HRD) search finds the regions with the highest hot degrees. We believe that this type of search may benefit many applications in self-driving cars, including route planning and navigation, and traffic management and analytics in general. The HRD search is challenging due to two reasons. First, how to evaluate the similarity when matching different regions. Second, how to compute the HRD search efficiently, since its time complexity is 
                        
                           O
                           (
                           |
                           Q
                           
                              
                                 |
                              
                              
                                 2
                              
                           
                           )
                        
                     . To overcome the challenges, we define a novel spatial-density correlation measure to evaluate the similarity between two regions, and develop a parallel search framework to process the HRD efficiently. In addition, a series of optimization techniques, e.g., pruning techniques, are defined to further enhance the query efficiency. Finally, we conduct extensive experiments on real data sets to study the performance of the developed methods.",autonomous vehicle
10.1016/j.neucom.2020.02.128,Journal,Neurocomputing,scopus,2021-03-07,sciencedirect,Few-labeled visual recognition for self-driving using multi-view visual-semantic representation,https://api.elsevier.com/content/abstract/scopus_id/85089289924,"For a car self-driving system, it is vital to accurately recognize the objects on the road. This is often achieved by analyzing the images captured by various cameras. However, in real applications, we often have limited number of labeled images. Besides, it is very expensive to manually annotate objects in images. It is hard to learn reliable classifiers when only few-labeled images are available. To alleviate these problems, multi-view information is used. However, only using visual information is not enough for reliable classification. Besides, we often have limited images with labels. To cope with these problems, in this paper, we propose a novel multi-view visual-semantic representation method for few-labeled visual recognition (
                        
                           
                              
                                 MV
                              
                              
                                 2
                              
                           
                           S
                        
                     ). We make use of the state-of-the-art deep convolutional neural networks by viewing them as different views to extract semantic representations of images. This is achieved by using the learned deep convolutional neural networks to make predictions of the semantics of images. Both the visual and semantic representations of images are then used to predict the categories of images by combining the predictions of multi-views with visual and semantic consistency constraints. Experiments on four public available datasets prove the effectiveness of the proposed 
                        
                           
                              
                                 MV
                              
                              
                                 2
                              
                           
                           S
                        
                      method.",autonomous vehicle
10.1016/j.bdr.2020.100179,Journal,Big Data Research,scopus,2021-02-15,sciencedirect,A Data-Driven Method for Hybrid Data Assimilation with Multilayer Perceptron,https://api.elsevier.com/content/abstract/scopus_id/85098157745,"Accurate and timely weather prediction is of significance for autonomous vehicles, such as designing more appropriate sensors or other configurations and developing safer driving strategies. Generally, as the mainstream weather prediction method, numerical weather prediction (NWP) relies on high-quality spatio-temporal observations. However, the precise state of the real world is not measurable. Thus, how to obtain a proper initial condition estimation based on big geospatial-temporal data is a crucial procedure for NWP. Data assimilation (DA) has been a traditional solution to the problem, for the better performance of which various mathematical-physics models have been used. However, the computational effectiveness and efficiency are still largely compromised by the complicated and nonparallel integration process in existing DA methods. In this paper, we propose a novel data-driven method named HDA-MLP to address the DA problem. We first constructed a customized MLP by introducing the temporal peculiarities of the state variables to simulate and optimize pure 3DVar and EnKF. Then we blended the optimized analysis fields directly by implicitly updating the background error covariance matrix through another neural network model to alleviate the dependence on traditional DA methods. We conducted extensive experiments to investigate the effectiveness and efficiency of the proposal by utilizing two classical nonlinear dynamic models. Results reveal that our approach has better robustness and enhanced capability to capture the variation of state variables. Notably, the analysis quality and computational efficiency are significantly improved.",autonomous vehicle
10.1016/j.robot.2020.103701,Journal,Robotics and Autonomous Systems,scopus,2021-02-01,sciencedirect,On deep learning techniques to boost monocular depth estimation for autonomous navigation,https://api.elsevier.com/content/abstract/scopus_id/85098871371,"Inferring the depth of images is a fundamental inverse problem within the field of Computer Vision since depth information is obtained through 2D images, which can be generated from infinite possibilities of observed real scenes. Benefiting from the progress of Convolutional Neural Networks (CNNs) to explore structural features and spatial image information, Single Image Depth Estimation (SIDE) is often highlighted in scopes of scientific and technological innovation, as this concept provides advantages related to its low implementation cost and robustness to environmental conditions. In the context of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by producing high-quality depth maps, which are essential during the autonomous navigation process in different locations. However, such networks are usually supervised by sparse and noisy depth data, from Light Detection and Ranging (LiDAR) laser scans, and are carried out at high computational cost, requiring high-performance Graphic Processing Units (GPUs). Therefore, we propose a new lightweight and fast supervised CNN architecture combined with novel feature extraction models which are designed for real-world autonomous navigation. We also introduce an efficient surface normals module, jointly with a simple geometric 2.5D loss function, to solve SIDE problems. We also innovate by incorporating multiple Deep Learning techniques, such as the use of densification algorithms and additional semantic, surface normals and depth information to train our framework. The method introduced in this work focuses on robotic applications in indoor and outdoor environments and its results are evaluated on the competitive and publicly available NYU Depth V2 and KITTI Depth datasets.",autonomous vehicle
10.1016/j.engappai.2020.104116,Journal,Engineering Applications of Artificial Intelligence,scopus,2021-02-01,sciencedirect,Learning dynamic regression with automatic distractor repression for real-time UAV tracking,https://api.elsevier.com/content/abstract/scopus_id/85097581571,"With high efficiency and efficacy, the trackers based on the discriminative correlation filter have experienced rapid development in the field of unmanned aerial vehicle (UAV) over the past decade. In literature, these trackers aim at solving a regression problem in which the circulated samples are mapped into a Gaussian label for online filter training. However, the fixed target label for regression makes trackers lose adaptivity in uncertain tracking scenarios. One of the typical failure cases is that the distractors, e.g., background clutter, camouflage, and similar object, are prone to confuse these trackers. In this work, an efficient approach to instantly monitor the local maximums of the response map for discovering distractors automatically is proposed. In addition, the regression target is accordingly learned, i.e., the location possessing local maximum indicates latent distractor and thus should be repressed by reducing its target response value in filter training. Qualitative and quantitative experiments performed on three challenging well-known benchmarks demonstrate that the presented method not only outperforms the state-of-the-art handcrafted feature-based trackers but also exhibits comparable performance compared to deep learning-based approaches. Specifically, the presented tracker has phenomenal practicability in real-time UAV applications with an average speed of 
                        ∼
                     50 frames per second on an affordable CPU.",autonomous vehicle
10.1016/j.ocecoaman.2020.105478,Journal,Ocean and Coastal Management,scopus,2021-02-01,sciencedirect,Autonomous litter surveying and human activity monitoring for governance intelligence in coastal eco-cyber-physical systems,https://api.elsevier.com/content/abstract/scopus_id/85097580928,"The human impact on the coastal ecosystems is a global environmental concern. Due to the growing urbanization, industrialization, and transportation, this impact on the living and non-living components of the coastal area is expected to further increase in the coming years. Artificial intelligence based automation of the coastal monitoring, including data collection, analysis and decision making, provides real-time insights and opportunities for large-scale coastal management and governance. In this paper, a framework for autonomous litter surveying and human activity monitoring for governance intelligence in coastal eco-cyber-physical systems (ecoCystem) is presented. A large dataset of more than 20,000 images focused on smart coastal management is collected to model the real world scenarios. A combination of various artificial intelligence based methods are used for automatic detection and classification of various litter in the coastal environment. Furthermore, the proposed framework is capable of autonomous monitoring of humans activities and detection of illegal entry of vehicles and boats to the beach area. The accuracy of the proposed autonomous system is 87% for correct classification of fully visible litter and 95% for fully visible vehicles. The experimental results show that the application of computer vision and machine learning for autonomous litter classification shows promising results for increasing the speed and scale of litter surveying in the coastal area. Further training of the artificial intelligence models is necessary for increasing the accuracy of the proposed framework and real-world deployment in the coastal environment. The proposed human activity monitoring system can be used for autonomous coastal law enforcement and real-time and active protection of the coastal zones.",autonomous vehicle
10.1016/j.future.2020.09.001,Journal,Future Generation Computer Systems,scopus,2021-02-01,sciencedirect,Formal approach to thwart against drone discovery attacks: A taxonomy of novel 3D obfuscation mechanisms,https://api.elsevier.com/content/abstract/scopus_id/85091758369,"The pervasive capabilities and myriad of mission performance abilities of Unmanned (Combat) Aerial Vehicles (UAVs/UCAVs) have exponentially grown their deployment possibilities in the recent past. Advancements in artificial intelligence, sensing technologies and autonomous guidance, navigation and control capabilities have further fueled wide-scale deployments of UAVs, both for military and commercial applications, ranging from autonomous air taxis and cargo deliveries to intelligence surveillance, reconnaissance, and combat missions. Most of these applications consume Global Navigation Satellite System (GNSS) based location information for their services, which is also shared in real-time with ground control stations and centralized service operators, often using insecure communication channels. This limitation has significantly raised the location privacy concerns of aerial vehicles, deployed to conduct user-centric, safety-critical and localization-sensitive operations. A compromise of location privacy of a UAV can pose serious threats, including stalking, theft or damage of UAV/payload or even use of GNSS-guided munitions. These emerging threats call for robust and trust-worthy solutions for preserving the location privacy of aerial vehicles.
                  This paper proposes a novel obfuscation-based mechanism to safeguard location information against privacy attacks. Our proposed solution conceals actual information by transmitting modified location parameters, either after diluting their accuracy or by fabricating deceptive trajectories for a known eavesdropper. Based on these two broad categories, defined as Attenuation and Deception-based obfuscation techniques respectively, we also present a novel taxonomy of 3D obfuscation mechanisms, supported by formal descriptions of underlying operators. The operators can be used independently or in conjunction to satisfy diverse mission-specific obfuscation profiles. The proposed operators have been practically implemented and evaluated using a customizable obfuscator deployed over a Global Positioning System (GPS) guided UAV. The field experiments validate the efficacy, security and deployability of the proposed solution against location-privacy threats.",autonomous vehicle
10.1016/j.future.2020.08.046,Journal,Future Generation Computer Systems,scopus,2021-02-01,sciencedirect,A drone-based networked system and methods for combating coronavirus disease (COVID-19) pandemic,https://api.elsevier.com/content/abstract/scopus_id/85090189689,"Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus. It is similar to influenza viruses and raises concerns through alarming levels of spread and severity resulting in an ongoing pandemic worldwide. Within eight months (by August 2020), it infected 24.0 million persons worldwide and over 824 thousand have died. Drones or Unmanned Aerial Vehicles (UAVs) are very helpful in handling the COVID-19 pandemic. This work investigates the drone-based systems, COVID-19 pandemic situations, and proposes an architecture for handling pandemic situations in different scenarios using real-time and simulation-based scenarios. The proposed architecture uses wearable sensors to record the observations in Body Area Networks (BANs) in a push–pull data fetching mechanism. The proposed architecture is found to be useful in remote and highly congested pandemic areas where either the wireless or Internet connectivity is a major issue or chances of COVID-19 spreading are high. It collects and stores the substantial amount of data in a stipulated period and helps to take appropriate action as and when required. In real-time drone-based healthcare system implementation for COVID-19 operations, it is observed that a large area can be covered for sanitization, thermal image collection, and patient identification within a short period (2 KMs within 10 min approx.) through aerial route. In the simulation, the same statistics are observed with an addition of collision-resistant strategies working successfully for indoor and outdoor healthcare operations. Further, open challenges are identified and promising research directions are highlighted.",autonomous vehicle
10.1016/j.aap.2021.106473,Journal,Accident Analysis and Prevention,scopus,2021-01-01,sciencedirect,Mining patterns of autonomous vehicle crashes involving vulnerable road users to understand the associated factors,https://api.elsevier.com/content/abstract/scopus_id/85118989110,"Autonomous or automated vehicles (AVs) have the potential to improve traffic safety by eliminating majority of human errors. As the interest in AV deployment increases, there is an increasing need to assess and understand the expected implications of AVs on traffic safety. Until recently, most of the literature has been based on either survey questionnaires, simulation analysis, virtual reality, or simulation to assess the safety benefits of AVs. Although few studies have used AV crash data, vulnerable road users (VRUs) have not been a topic of interest. Therefore, this study uses crash narratives from four-year (2017–2020) of AV crash data collected from California to explore the direct and indirect involvement of VRUs. The study applied text network and compared the text classification performance of four classifiers - Support Vector Machine (SVM), Naïve Bayes (NB), Random Forest (RF), and Neural Network (NN) and associated performance metrics to attain the objective. It was found that out of 252 crashes, VRUs were, directly and indirectly, involved in 23 and 12 crashes, respectively. Among VRUs, bicyclists and scooterists are more likely to be involved in the AV crashes directly, and bicyclists are likely to be at fault, while pedestrians appear more in the indirectly involvements. Further, crashes that involve VRUs indirectly are likely to occur when the AVs are in autonomous mode and are slightly involved minor damages on the rear bumper than the ones that directly involve VRUs. Additionally, feature importance from the best performing classifiers (RF and NN) revealed that crosswalks, intersections, traffic signals, movements of AVs (turning, slowing down, stopping) are the key predictors of the VRUs-AV related crashes. These findings can be helpful to AV operators and city planners.",autonomous vehicle
10.1016/j.neucom.2021.04.133,Journal,Neurocomputing,scopus,2021-01-01,sciencedirect,Reinforcement learning-based finite-time tracking control of an unknown unmanned surface vehicle with input constraints,https://api.elsevier.com/content/abstract/scopus_id/85118987312,"In this paper, subject to completely unknown system dynamics and input constraints, a reinforcement learning-based finite-time trajectory tracking control (RLFTC) scheme is innovatively created for an unmanned surface vehicle (USV) by combining actor-critic reinforcement learning (RL) mechanism with finite-time control technique. Unlike previous RL-based tracking which requires infinite-time convergence thereby rather sensitive to complex unknowns, an actor-critic finite-time control structure is created by employing adaptive neural network identifiers to recursively update actor and critic, such that learning-based robustness can be sufficiently enhanced. Moreover, deduced from the Bellman error formulation, the proposed RLFTC is directly optimized in a finite-time manner. Theoretical analysis eventually shows that the proposed RLFTC scheme can ensure semi-global practical finite-time stability (SGPFS) for a closed-loop USV system and tracking errors converge to an arbitrarily small neighborhood of the origin in a finite time, subject to optimal cost. Both mathematical simulation and virtual-reality experiments demonstrate remarkable effectiveness and superiority of the proposed RLFTC scheme.",autonomous vehicle
10.1016/j.ejrs.2021.08.007,Journal,Egyptian Journal of Remote Sensing and Space Science,scopus,2021-01-01,sciencedirect,Smart farming for improving agricultural management,https://api.elsevier.com/content/abstract/scopus_id/85114414365,"The food shortage and the population growth are the most challenges facing sustainable development worldwide. Advanced technologies such as artificial intelligence (AI), the Internet of Things (IoT), and the mobile internet can provide realistic solutions to the challenges that are facing the world. Therefore, this work focuses on the new approaches regarding smart farming (SF) from 2019 to 2021, where the work illustrates the data gathering, transmission, storage, analysis, and also, suitable solutions. IoT is one of the essential pillars in smart systems, as it connects sensor devices to perform various basic tasks. The smart irrigation system included those sensors for monitoring water level, irrigation efficiency, climate, etc. Smart irrigation is based on smart controllers and sensors as well as some mathematical relations. In addition, this work illustrated the application of unmanned aerial vehicles (UAV) and robots, where they can be achieved several functions such as harvesting, seedling, weed detection, irrigation, spraying of agricultural pests, livestock applications, etc. real-time using IoT, artificial intelligence (AI), deep learning (DL), machine learning (ML) and wireless communications. Moreover, this work demonstrates the importance of using a 5G mobile network in developing smart systems, as it leads to high-speed data transfer, up to 20 Gbps, and can link a large number of devices per square kilometer. Although the applications of smart farming in developing countries are facing several challenges, this work highlighted some approaches the smart farming. In addition, the implementation of Smart Decision Support Systems (SDSS) in developing countries supports the real-time analysis, mapping of soil characteristics and also helps to make proper decision management. Finally, smart agriculture in developing countries needs more support from governments at the small farms and the private sector.",autonomous vehicle
10.1016/j.cja.2020.12.027,Journal,Chinese Journal of Aeronautics,scopus,2021-01-01,sciencedirect,Relevant experience learning: A deep reinforcement learning method for UAV autonomous motion planning in complex unknown environments,https://api.elsevier.com/content/abstract/scopus_id/85109444094,"Unmanned Aerial Vehicles (UAVs) play a vital role in military warfare. In a variety of battlefield mission scenarios, UAVs are required to safely fly to designated locations without human intervention. Therefore, finding a suitable method to solve the UAV Autonomous Motion Planning (AMP) problem can improve the success rate of UAV missions to a certain extent. In recent years, many studies have used Deep Reinforcement Learning (DRL) methods to address the AMP problem and have achieved good results. From the perspective of sampling, this paper designs a sampling method with double-screening, combines it with the Deep Deterministic Policy Gradient (DDPG) algorithm, and proposes the Relevant Experience Learning-DDPG (REL-DDPG) algorithm. The REL-DDPG algorithm uses a Prioritized Experience Replay (PER) mechanism to break the correlation of continuous experiences in the experience pool, finds the experiences most similar to the current state to learn according to the theory in human education, and expands the influence of the learning process on action selection at the current state. All experiments are applied in a complex unknown simulation environment constructed based on the parameters of a real UAV. The training experiments show that REL-DDPG improves the convergence speed and the convergence result compared to the state-of-the-art DDPG algorithm, while the testing experiments show the applicability of the algorithm and investigate the performance under different parameter conditions.",autonomous vehicle
10.1016/j.procs.2021.02.012,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,The impact of the soft errors in convolutional neural network on GPUS: Alexnet as case study,https://api.elsevier.com/content/abstract/scopus_id/85105461752,"Convolutional Neural Networks (CNNs) have been increasingly deployed in many applications, including safety critical system such as healthcare and autonomous vehicles. Meanwhile, the vulnerability of CNN model to soft errors (e.g., caused by radiation induced) rapidly increases, thus reliability is crucial especially in real-time system. There are many traditional techniques for improve the reliability of the system, e.g., Triple Modular Redundancy, but these techniques incur high overheads, which makes them hard to deploy. In this paper, we experimentally evaluate the vulnerable parts of Alexnet mode (e.g., fault injector). Results show that FADD and LD are the top vulnerable instructions against soft errors for Alexnet model, both instructions generate at least 84% of injected faults as SDC errors. Thus, these the only parts of the Alexnet model that need to be hardened instead of using fully duplication solutions.",autonomous vehicle
10.1016/j.compag.2020.105909,Journal,Computers and Electronics in Agriculture,scopus,2021-01-01,sciencedirect,State and parameter estimation of the AquaCrop model for winter wheat using sensitivity informed particle filter,https://api.elsevier.com/content/abstract/scopus_id/85098062759,"Crop models play a paramount role in providing quantitative information on crop growth and field management. However, its prediction performance degrades significantly in the presence of unknown, uncertain parameters and noisy measurements. Consequently, simultaneous state and parameter estimation (SSPE) for crop model is required to maximize its potentials. This work aims to develop an integrated dynamic SSPE framework for the AquaCrop model by leveraging constrained particle filter, crop sensitivity analysis and UAV remote sensing. Both Monte Carlo simulation and one winter wheat experimental case study are performed to validate the proposed framework. It is shown that: (i) the proposed framework with state/parameter bound and parameter sensitivity information outperforms conventional particle filter and constrained particle filter in both state and parameter estimation in Monte Carlo simulations; (ii) in real-world experiment, the proposed approach achieves the smallest root mean squared error for canopy cover estimation among the three algorithms by using day forward-chaining validation method.",autonomous vehicle
10.1016/j.cogsys.2020.09.006,Journal,Cognitive Systems Research,scopus,2021-01-01,sciencedirect,Learning data-driven decision-making policies in multi-agent environments for autonomous systems,https://api.elsevier.com/content/abstract/scopus_id/85092486365,"Autonomous systems such as Connected Autonomous Vehicles (CAVs), assistive robots are set improve the way we live. Autonomous systems need to be equipped with capabilities to Reinforcement Learning (RL) is a type of machine learning where an agent learns by interacting with its environment through trial and error, which has gained significant interest from research community for its promise to efficiently learn decision making through abstraction of experiences. However, most of the control algorithms used today in current autonomous systems such as driverless vehicle prototypes or mobile robots are controlled through supervised learning methods or manually designed rule-based policies. Additionally, many emerging autonomous systems such as driverless cars, are set in a multi-agent environment, often with partial observability. Learning decision making policies in multi-agent environments is a challenging problem, because the environment is not stationary from the perspective of a learning agent, and hence the Markov properties assumed in single agent RL does not hold. This paper focuses on learning decision-making policies in multi-agent environments, both in cooperative settings with full observability and dynamic environments with partial observability. We present experiments in simple, yet effective, new multi-agent environments to simulate policy learning in scenarios that could be encountered by an autonomous navigating agent such as a CAV. The results illustrate how agents learn to cooperate in order to achieve their objectives successfully. Also, it was shown that in a partially observable setting, an agent was capable of learning to roam around its environment without colliding in the presence of obstacles and other moving agents. Finally, the paper discusses how data-driven multi-agent policy learning can be extended to real-world environments by augmenting the intelligence of autonomous vehicles.",autonomous vehicle
10.1016/j.robot.2020.103652,Journal,Robotics and Autonomous Systems,scopus,2020-12-01,sciencedirect,Self-awareness in intelligent vehicles: Feature based dynamic Bayesian models for abnormality detection,https://api.elsevier.com/content/abstract/scopus_id/85092022930,"The evolution of Intelligent Transportation Systems in recent times necessitates the development of self-awareness in agents. Before the intensive use of Machine Learning, the detection of abnormalities was manually programmed by checking every variable and creating huge nested conditions that are very difficult to track. This paper aims to introduce a novel method to develop self-awareness in autonomous vehicles that mainly focuses on detecting abnormal situations around the considered agents. Multi-sensory time-series data from the vehicles are used to develop the data-driven Dynamic Bayesian Network (DBN) models used for future state prediction and the detection of dynamic abnormalities. Moreover, an initial level collective awareness model that can perform joint anomaly detection in co-operative tasks is proposed.
                  The GNG algorithm learns the DBN models’ discrete node variables; probabilistic transition links connect the node variables. A Markov Jump Particle Filter (MJPF) is applied to predict future states and detect when the vehicle is potentially misbehaving using learned DBNs as filter parameters.
                  In this paper, datasets from real experiments of autonomous vehicles performing various tasks used to learn and test a set of switching DBN models.",autonomous vehicle
10.1016/j.cogsys.2020.08.010,Journal,Cognitive Systems Research,scopus,2020-12-01,sciencedirect,Toward ethical cognitive architectures for the development of artificial moral agents,https://api.elsevier.com/content/abstract/scopus_id/85090423654,"New technologies based on artificial agents promise to change the next generation of autonomous systems and therefore our interaction with them. Systems based on artificial agents such as self-driving cars and social robots are examples of this technology that is seeking to improve the quality of people’s life. Cognitive architectures aim to create some of the most challenging artificial agents commonly known as bio-inspired cognitive agents. This type of artificial agent seeks to embody human-like intelligence in order to operate and solve problems in the real world as humans do. Moreover, some cognitive architectures such as Soar, LIDA, ACT-R, and iCub try to be fundamental architectures for the Artificial General Intelligence model of human cognition. Therefore, researchers in the machine ethics field face ethical questions related to what mechanisms an artificial agent must have for making moral decisions in order to ensure that their actions are always ethically right. This paper aims to identify some challenges that researchers need to solve in order to create ethical cognitive architectures. These cognitive architectures are characterized by the capacity to endow artificial agents with appropriate mechanisms to exhibit explicit ethical behavior. Additionally, we offer some reasons to develop ethical cognitive architectures. We hope that this study can be useful to guide future research on ethical cognitive architectures.",autonomous vehicle
10.1016/j.ndteint.2020.102341,Journal,NDT and E International,scopus,2020-12-01,sciencedirect,Automatic delamination segmentation for bridge deck based on encoder-decoder deep learning through UAV-based thermography,https://api.elsevier.com/content/abstract/scopus_id/85089808135,"Concrete deck delamination often demonstrates strong variations in size, shape, and temperature distribution under the influences of outdoor weather conditions. The strong variations create challenges for pure analytical solutions in infrared image segmentation of delaminated areas. The recently developed supervised deep learning approach demonstrated the potentials in achieving automatic segmentation of RGB images. However, its effectiveness in segmenting thermal images remains under-explored. The main challenge lies in the development of specific models and the generation of a large range of labeled infrared images for training. To address this challenge, a customized deep learning model based on encoder-decoder architecture is proposed to segment the delaminated areas in thermal images at the pixel level. Data augmentation strategies were implemented in creating the training data set to improve the performance of the proposed model. The deep learning generated model was deployed in a real-world project to further evaluate the model's applicability and robustness. The results of these experimental studies supported the effectiveness of the deep learning model in segmenting concrete delamination areas from infrared images. It also suggested that data augmentation is a helpful technique to address the small size issue of training samples. The field test with validation further demonstrated the generalizability of the proposed framework. Limitations of the proposed approach were also briefed at the end of the paper.",autonomous vehicle
10.1016/j.conengprac.2020.104630,Journal,Control Engineering Practice,scopus,2020-11-01,sciencedirect,Vision-based robust control framework based on deep reinforcement learning applied to autonomous ground vehicles,https://api.elsevier.com/content/abstract/scopus_id/85090751679,"Given the recent advances in computer vision, image processing and control systems, self-driving vehicles has been one of the most promising and challenging research topics nowadays. The design of vision-based robust controllers to keep an autonomous car in the center of the lane, despite uncertainties and disturbances, is still an ongoing challenge. This paper presents a hybrid control architecture that combines Deep Reinforcement Learning (DRL) and Robust Linear Quadratic Regulator (RLQR) for vision-based lateral control of an autonomous vehicle. Evolutionary estimation is used to model the vehicle uncertainties. For performance comparison, a DRL method and three other hybrid controllers are also evaluated. The inputs for each controller are real-time semantically segmented RGB camera images which serve as the basis to calculate continuous steering actions to keep the vehicle on the center of the lane with a constant velocity. Simulation results show that the proposed hybrid RLQR with evolutionary estimation of uncertainties architecture outperforms the other algorithms implemented. It presents lower tracking errors, smoother steering inputs, total collision avoidance and better generalization in new urban environments. Furthermore, it significantly decreases the required training time.",autonomous vehicle
10.1016/j.ssci.2020.104919,Journal,Safety Science,scopus,2020-11-01,sciencedirect,A systems-based application for autonomous vessels safety: Hazard identification as a function of increasing autonomy levels,https://api.elsevier.com/content/abstract/scopus_id/85088891787,"During the last decade, a series of autonomous vessels projects have been conducted strengthening the vision of the positive impacts on human safety, cost savings and environmental protection using advanced Artificial Intelligence (AI) based on data fused algorithms from various sensors. Although these projects have demonstrated the technological feasibility of this idea, the hypothesis that autonomous ships will be safer needs to be tested and safety constraints must be established. The main objective of this paper is to determine the hazards for the system as a function of the vessel’s autonomy level. This is accomplished by implementing the System Theoretic Process Analysis (STPA) on specific levels of increasing autonomy, as described by the International Maritime Organization (IMO), where the human element is limited to remote monitoring and the on-board control system makes most of the decisions and takes most of the actions. STPA assumes that accidents are caused by unsafe interactions among system components, none of which may have failed. STPA presents an advantage over other hazard analysis tools, by being more flexible in terms of required information and by having a top-down functional model approach. The STPA’s results are used to determine how hazard occurrence changes, considering the interactions among system components and hard to identify failure points. These results will be used to evaluate the hypothesis that autonomous shipping will be safer and potentially contribute to the ongoing discussion for the regulatory framework that must be implemented for autonomous shipping to become a large-scale reality.",autonomous vehicle
10.1016/j.comnet.2020.107388,Journal,Computer Networks,scopus,2020-10-24,sciencedirect,Towards cross-task universal perturbation against black-box object detectors in autonomous driving,https://api.elsevier.com/content/abstract/scopus_id/85088224849,"Deep neural network is the main research branch in artificial intelligence and suitable for many decision-making fields. Autonomous driving and unmanned vehicle often depend on deep neural networks for accurate and reliable detection, classification, and ranging of surrounding objects in real on-road environments, either locally or by swarm intelligence among distributed nodes via 5G channel. But, it has been demonstrated that deep neural networks are vulnerable to well-designed adversarial examples that are imperceptible to human eyes in computer vision tasks. It is valuable to study the vulnerability for enhancing the robustness of neural networks. However, existing adversarial examples against object detection models are image-dependent, so in this paper, we implement adversarial attacks against object detection models using universal perturbations. We find the cross-task, cross-model, and cross-dataset transferability of universal perturbations, we train universal perturbations generator firstly and then add the universal perturbations to the target images in two ways: resizing and pile-up, in order to solve the problem that universal perturbations cannot be directly applied to attack object detection models. We use the transferability of universal perturbations to attack black-box object detection models. In this way, the time cost of generating adversarial examples is reduced. A series of experiments are conducted on PASCAL VOC and MS COCO datasets demonstrating the feasibility of cross-task attacks and proving the effectiveness of our attack on two representative object detectors: regression-based models like YOLOv3 and proposal-based models like Faster R-CNN.",autonomous vehicle
10.1016/j.image.2020.115969,Journal,Signal Processing: Image Communication,scopus,2020-10-01,sciencedirect,Re-identification framework for long term visual object tracking based on object detection and classification,https://api.elsevier.com/content/abstract/scopus_id/85089267326,"In this paper, we address the problem of long-term visual object tracking and we present an efficient real-time single object tracking system suitable for integration in autonomous platforms that need to encompass intelligent capabilities. We propose a novel long-term tracking framework for classification based re-detection and tracking, that incorporates state estimation, object re-identification and automated management of tracking and detection results. Our method integrates a novel object re-identification technique which efficiently filters a number of detection candidates and systematically corrects the tracking results. Through extensive experimental validation on the UAV123, UAV20L and TLP datasets, we demonstrate the effectiveness of the proposed system and its advantage over several state-of-the art trackers. The results furthermore highlight the proposed tracker’s ability to handle challenges arising from real-world and long-term scenarios, such as variations in pose, scale, occlusions and out-of-view situations. Furthermore, we propose a variant that is suitable for deployment on autonomous robots, such as Unmanned Aerial Vehicles.",autonomous vehicle
10.1016/j.sysarc.2020.101835,Journal,Journal of Systems Architecture,scopus,2020-10-01,sciencedirect,MBBNet: An edge IoT computing-based traffic light detection solution for autonomous bus,https://api.elsevier.com/content/abstract/scopus_id/85087859478,"Traffic light detection is a key module in the autonomous driving system to enhance the interactions between drivers and unmanned vehicles. In recent studies, deep neural networks are widely used for traffic light detection and resource/power consumption is a major concern for model deployment in vehicular edge devices. This paper proposes a novel light-weight deep CNN model that integrates the multi-backbone of state-of-the-art architectures for the self-driving traffic light detection. The MBBNet (Multi-BackBone Network) consists of three common convolutional backbones, i.e., the normal, residual and highway (DenseNet) convolutional modules. Simple ensemble of those backbones may incur high computational load. Therefore, channel compression is adopted to control the model parameters, while guaranteeing the accuracy for mobile and embedded hardware. Evaluation of a dataset collected from real road conditions demonstrate the robustness of our detection system, and it achieves higher accuracy (accuracy > 0.94 and 
                        
                           A
                           v
                           e
                           r
                           a
                           g
                           e
                           _
                           I
                           O
                           U
                           >
                           74.05
                           %
                        
                     ) for self-driving buses. In terms of resource consumption, the trained model size is 1.35 MB, and can process high-resolution images (1280 × 960) at 14 FPS (frames per second) on low-power edge devices.",autonomous vehicle
10.1016/j.ast.2020.105965,Journal,Aerospace Science and Technology,scopus,2020-10-01,sciencedirect,Towards a PDE-based large-scale decentralized solution for path planning of UAVs in shared airspace,https://api.elsevier.com/content/abstract/scopus_id/85086828428,"Recently, there has been a tremendous increase of interest in utilizing Unmanned Aerial Vehicles (UAVs) for a number of civilian applications. With this increased interest, it is imperative that these UAVs are able to operate in shared airspace for enhanced efficiency. Multi-UAV systems are inherently safety-critical systems, which means that safety guarantees must be made to ensure no undesirable configurations, such as collisions, occur. This paper proposes a decentralized method based on a Partial Differential Equation (PDE) to generate collision-free 3D trajectories for multiple UAVs operating in a shared airspace. This method exploits the dynamical properties of multi-phase fluids flowing through a porous medium by modeling the porosity values as a function of the risk of collision. To highlight the feasibility for on-board implementation, we propose propose a machine learning technique for obtaining computationally efficient solutions of the PDE describing flow movements in porous medium. This method has been compared via a simulation study to two other path planning strategies, centralized and sequential planning, and the advantages of this method are presented. Furthermore, results from an experiment using three UAVs have been presented to demonstrate the applicability of the proposed method to real-world implementation.",autonomous vehicle
10.1016/j.iot.2020.100205,Journal,Internet of Things (Netherlands),scopus,2020-09-01,sciencedirect,Multi-UAV Allocation Framework for Predictive Crime Deterrence and Data Acquisition,https://api.elsevier.com/content/abstract/scopus_id/85105803842,"The recent decline in the number of police and security force personnel has raised a serious security issue that could lead to reduced public safety and delayed response to crimes in urban areas. This may be alleviated in part by utilizing micro or small unmanned aerial vehicles (UAVs) and their high-mobility on-board sensors in conjunction with machine-learning techniques such as neural networks to offer better performance in predicting times and places that are high-risk and deterring crimes. The key to the success of such operation lies in the suitable placement of UAVs. This paper proposes a multi-UAV allocation framework for predictive crime deterrence and data acquisition that consists of the overarching methodology, a problem formulation, and an allocation method that work with a prediction model using a machine learning approach. In contrast to previous studies, our framework provides the most effective arrangement of UAVs for maximizing the chance to apprehend offenders whilst also acquiring data that will help improve the performance of subsequent crime prediction. This paper presents the system architecture assumed in this study, followed by a detailed description of the methodology, the formulation of the problem, and the UAV allocation method of the proposed framework. Our framework is tested using a real-world crime dataset to evaluate its performance with respect to the expected number of crimes deterred by the UAV patrol. Furthermore, to address the engineering practice of the proposed framework, we discuss the feasibility of the simulated deployment scenario in terms of energy consumption and the relationship between data analysis and crime prediction.",autonomous vehicle
10.1016/j.engappai.2020.103799,Journal,Engineering Applications of Artificial Intelligence,scopus,2020-09-01,sciencedirect,Trajectory based lateral control: A Reinforcement Learning case study,https://api.elsevier.com/content/abstract/scopus_id/85087950678,"Reinforcement Learning (RL) has been employed in many applications of robotics and has steadily been gaining traction in the field of Autonomous Driving (AD). This paper proposes a Deep Reinforcement Learning based approach for lateral Vehicle Motion Control (VMC), and explores the generalization capabilities of the approach. The proposed methodology uses a sequence of waypoints generated from a planning module of an AD stack as the input. The network has been trained to predict accurate steering commands to follow the given trajectory. In this paper we detail our implementation and share our learning experience on real-vehicle deployment of the RL based controller. Our experiments yield promising results with an agent trained on less than 4 h of simulated driving experience without any real-world data. The trained agent is able to successfully complete unseen and more complex tracks using different unseen vehicle models. The agent safely reached up to 150km/h in simulation and up to 60km/h in a real-life Sport Utility Vehicle (SUV) weighing more than 2000kg.",autonomous vehicle
10.1016/j.trc.2020.102649,Journal,Transportation Research Part C: Emerging Technologies,scopus,2020-08-01,sciencedirect,Differential variable speed limits control for freeway recurrent bottlenecks via deep actor-critic algorithm,https://api.elsevier.com/content/abstract/scopus_id/85086802562,"Variable speed limit (VSL) control is a flexible way to improve traffic conditions, increase safety, and reduce emissions. There is an emerging trend of using reinforcement learning methods for VSL control. Currently, deep learning is enabling reinforcement learning to develop autonomous control agents for problems that were previously intractable. In this paper, a more effective deep reinforcement learning (DRL) model is developed for differential variable speed limit (DVSL) control, in which dynamic and distinct speed limits among lanes can be imposed. The proposed DRL model uses a novel actor-critic architecture to learn a large number of discrete speed limits in a continuous action space. Different reward signals, such as total travel time, bottleneck speed, emergency braking, and vehicular emissions are used to train the DVSL controller, and a comparison between these reward signals is conducted. The proposed DRL-based DVSL controllers are tested on a freeway with a simulated recurrent bottleneck. The simulation results show that the DRL based DVSL control strategy is able to improve the safety, efficiency and environment-friendliness of the freeway. In order to verify whether the controller generalizes to real world implementation, we also evaluate the generalization of the controllers on environments with different driving behavior attributes. and the robustness of the DRL agent is observed from the results.",autonomous vehicle
10.1016/j.neucom.2019.03.102,Journal,Neurocomputing,scopus,2020-07-20,sciencedirect,Coarse-to-fine object detection in unmanned aerial vehicle imagery using lightweight convolutional neural network and deep motion saliency,https://api.elsevier.com/content/abstract/scopus_id/85085863614,"Unmanned aerial vehicles (UAVs) have been widely applied to various fields, facing mass imagery data, object detection in UAV imagery is under extensive research for its significant status in both theoretical study and practical applications. In order to achieve the accurate object detection in UAV imagery on the premise of real-time processing, a coarse-to-fine object detection method for UAV imagery using lightweight convolutional neural network (CNN) and deep motion saliency is proposed in this paper. The proposed method includes three steps: (1) Key frame extraction using image similarity measurement is performed on the UAV imagery to accelerate the successive object detection procedure; (2) Deep features are extracted by PeleeNet, a lightweight CNN, to achieve the coarse object detection on the key frames; (3) LiteFlowNet and objects prior knowledge is utilized to analyze the deep motion saliency map, which further helps to refine the detection results. The detection results on key frames propagate to the temporally nearest non-key frames to achieve the fine detection. Five experiments are conducted to verify the effectiveness of the proposed method on Stanford drone dataset (SDD). The experimental results demonstrate that the proposed method can achieve comparable detection speed but superior accuracy to six state-of-the-art methods.",autonomous vehicle
10.1016/j.isatra.2020.02.012,Journal,ISA Transactions,scopus,2020-06-01,sciencedirect,Adaptive tracking control of an unmanned aerial system based on a dynamic neural-fuzzy disturbance estimator,https://api.elsevier.com/content/abstract/scopus_id/85080895180,"The main goal of this study is developing an adaptive controller which can solve the trajectory tracking for a class of quadcopter unmanned aerial system (UAS), namely a quadrotor. The control design introduces a new paradigm for adaptive controllers based on the implementation of a set of differential neural networks (DNNs) in the consequence section of a Takagi–Sugeno (T–S) fuzzy inference system. This dynamic fuzzy inference structure was used to approximate the UAS description. The particular form of interaction between neural networks and fuzzy inference systems proposed in the present work received the name of dynamic neural fuzzy system (DNFS). An adaptive controller based on this DNFS form was the main solution attained in this study. This DNFS controller was focused on the estimation and compensation of the uncertain section of the Quadrotor dynamics and then, forced the UAS to perform a hover flight while the tracking of desired angular positions succeeded, which results in tracking a desired trajectory in the X-Y plane. The control design methodology supported on the Lyapunov stability theory guaranteed ultimate boundedness of the estimation and tracking errors simultaneously. Several experimental tests in an outdoor environment by using a real Quadrotor platform was performed by using an RTK-GPS (Real Time Kinematic) system to determine the position of the vehicle in the X-Y plane. The experimental results confirmed the superior performance of the proposed algorithm based on the combination of DNNs and T–S techniques with respect to classical robust controllers.",autonomous vehicle
10.1016/j.neucom.2019.07.103,Journal,Neurocomputing,scopus,2020-05-21,sciencedirect,Visual Recognition of traffic police gestures with convolutional pose machine and handcrafted features,https://api.elsevier.com/content/abstract/scopus_id/85074513481,"Autonomous vehicles have become a hot spot of the automotive industry, many cities have claimed that autonomous vehicles should be capable of recognizing gestures used by traffic police. Traditional traffic police gesture recognition methods rely on depth-sensor or wearable-devices, which limits their availability in the domain of the intelligent vehicle. Vision-based methods have fewer requirements for distance, but the modeling process is challenging due to the complexity of the visual scenes. Inspired by the recent success in vision-based pose estimation networks such as Convolutional Pose Machine (CPM), in this paper, we propose a novel vision-based human-machine interface to recognize eight kinds of Chinese traffic police gestures and apply it in the real-time recognition tasks. This method integrates a modified CPM network and two kinds of handcrafted features: Relative Bone Length and Angle with Gravity as spatial domain features, and adopt a Long short-term memory (LSTM) network to extract temporal domain features. To train and validate our method, we create a gestures dataset with two hours of traffic police gesture videos, which has 3354 gesture instances. The experiment results show that the proposed method is capable of recognizing traffic police gestures, and is fast enough for online gesture prediction.",autonomous vehicle
10.1016/j.image.2020.115811,Journal,Signal Processing: Image Communication,scopus,2020-05-01,sciencedirect,Quality-guided lane detection by deeply modeling sophisticated traffic context,https://api.elsevier.com/content/abstract/scopus_id/85081131302,"Lane detection is a useful technique in modern autonomous vehicles systems, which assists vehicle to accurately localize itself according to detected road lines. Traditional methods leveraged edge detection and Hough transform based algorithms to plot lines along the detected lane. Noticeably, they did not take the informative feature road gradient into account. In addition, most previous deep learning-based algorithms consider lane detection as pixel-wise lane segmentation, where only fixed number of lanes can be detected. In order to solve these limitations, we propose a quality guided lane detection algorithm by modeling the sophisticated traffic context, where variable number of lanes can be satisfactorily handled. Specifically, we first leverage chessboard images for camera calibration to calculate correspondence between real world and image coordinate system. Subsequently, we capture image regions of interest that only contains lane information by leveraging the prior knowledge and image quality scores. Afterwards, we design an end-to-end two-stage CNN architecture for lane detection, where binary lane mask is utilized for lane matching. Comprehensive experiments have demonstrated that our proposed method can cope with variable number of lanes effectively.",autonomous vehicle
10.1016/j.rse.2020.111717,Journal,Remote Sensing of Environment,scopus,2020-05-01,sciencedirect,Assessing the relationship between macro-faunal burrowing activity and mudflat geomorphology from UAV-based Structure-from-Motion photogrammetry,https://api.elsevier.com/content/abstract/scopus_id/85079899077,"Characterisation of the ecosystem functioning of mudflats requires insight on the morphology and facies of these coastal features, but also on biological processes that influence mudflat geomorphology, such as crab bioturbation and the formation of benthic biofilms, as well as their heterogeneity at cm or less scales. Insight into this fine scale of ecosystem functioning is also important as far as minimizing errors in upscaling are concerned. The realisation of high-resolution ground surveys of these mudflats without perturbing their surface is a real challenge. Here, we address this challenge using UAV-supported photogrammetry based on the Structure-from-Motion (SfM) workflow. We produced a Digital Surface Model (DSM) and an orthophotograph at 1 cm and 0.5 cm pixel resolutions, respectively, of a mudflat in French Guiana, and mapped and classed into different size ranges intricate morphological features, including crab burrow apertures, tidal drainage creeks and depressions. We also determined subtle facies and elevation changes and slopes, and the footprint of different degrees of benthic biofilm development. The results generated at this scale of photogrammetric analysis also enabled us to relate macrofaunal crab burrowing activity to various parameters, including mudflat elevation, spatial distribution and sizes of creeks and depressions, benthic biofilm distribution, and flooding duration. SfM photogrammetry offers interesting new perspectives in fine-scale characterisation of the geomorphology, benthic activity and degree of biofilm development of dynamic muddy intertidal environments that are generally difficult of access. The main shortcomings highlighted in this study are a drift of accuracy of the DSM outside areas of ground control points and the deployment of which perturb the mudflat morphology and biology, the water-logged or very wet surfaces which generate reconstruction artefacts through the sun glint effect, and the time-consuming task of manual interpretation of extraction of features such as crab burrow apertures. On-going developments in UAV positioning integrating RTK/PPK GPS solutions for image-georeferencing and precise orientation with high-quality inertial measurement units will limit the difficulties inherent to ground control points, while conduction of surveys during homogeneous cloudy conditions could reduce the sun-glint effect. Manual extraction of image features could be automated in the future through the use of deep-learning algorithms.",autonomous vehicle
10.1016/j.patter.2020.100006,Journal,Patterns,scopus,2020-04-10,sciencedirect,Intelligent Electromagnetic Sensing with Learnable Data Acquisition and Processing,https://api.elsevier.com/content/abstract/scopus_id/85089142867,"Electromagnetic (EM) sensing is a widespread contactless examination technique with applications in areas such as health care and the internet of things. Most conventional sensing systems lack intelligence, which not only results in expensive hardware and complicated computational algorithms but also poses important challenges for real-time in situ sensing. To address this shortcoming, we propose the concept of intelligent sensing by designing a programmable metasurface for data-driven learnable data acquisition and integrating it into a data-driven learnable data-processing pipeline. Thereby, a measurement strategy can be learned jointly with a matching data post-processing scheme, optimally tailored to the specific sensing hardware, task, and scene, allowing us to perform high-quality imaging and high-accuracy recognition with a remarkably reduced number of measurements. We report the first experimental demonstration of “learned sensing” applied to microwave imaging and gesture recognition. Our results pave the way for learned EM sensing with low latency and computational burden.",autonomous vehicle
10.1016/j.robot.2020.103472,Journal,Robotics and Autonomous Systems,scopus,2020-04-01,sciencedirect,Deploying MAVs for autonomous navigation in dark underground mine environments,https://api.elsevier.com/content/abstract/scopus_id/85079573394,"Operating Micro Aerial Vehicles (MAVs) in subterranean environments is becoming more and more relevant in the field of aerial robotics. Despite the large spectrum of technological advances in the field, flying in such challenging environments is still an ongoing quest that requires the combination of multiple sensor modalities like visual/thermal cameras as well as 3D and 2D lidars. Nevertheless, there exist cases in subterranean environments where the aim is to deploy fast and lightweight aerial robots for area reckoning purposes after an event (e.g. blasting in production areas). This work proposes a novel baseline approach for the navigation of resource constrained robots, introducing the aerial underground scout, with the main goal to rapidly explore unknown areas and provide a feedback to the operator. The main proposed framework focuses on the navigation, control and vision capabilities of the aerial platforms with low-cost sensor suites, contributing significantly towards real-life applications. The merit of the proposed control architecture is that it considers the flying platform as a floating object, composing a velocity controller on the 
                        x
                     , 
                        y
                      axes and altitude control to navigate along the tunnel. Two novel approaches make up the cornerstone of the proposed contributions for the task of navigation: (1) a vector geometry method based on 2D lidar, and (2) a Deep Learning (DL) method through a classification process based on an on-board image stream, where both methods correct the heading towards the center of the mine tunnel. Finally, the framework has been evaluated in multiple field trials in an underground mine in Sweden.",autonomous vehicle
10.1016/j.imavis.2020.103889,Journal,Image and Vision Computing,scopus,2020-03-01,sciencedirect,Unsupervised domain adaptation for mobile semantic segmentation based on cycle consistency and feature alignment,https://api.elsevier.com/content/abstract/scopus_id/85079673578,"The supervised training of deep networks for semantic segmentation requires a huge amount of labeled real world data. To solve this issue, a commonly exploited workaround is to use synthetic data for training, but deep networks show a critical performance drop when analyzing data with slightly different statistical properties with respect to the training set. In this work, we propose a novel Unsupervised Domain Adaptation (UDA) strategy to address the domain shift issue between real world and synthetic representations. An adversarial model, based on the cycle consistency framework, performs the mapping between the synthetic and real domain. The data is then fed to a MobileNet-v2 architecture that performs the semantic segmentation task. An additional couple of discriminators, working at the feature level of the MobileNet-v2, allows to better align the features of the two domain distributions and to further improve the performance. Finally, the consistency of the semantic maps is exploited. After an initial supervised training on synthetic data, the whole UDA architecture is trained end-to-end considering all its components at once. Experimental results show how the proposed strategy is able to obtain impressive performance in adapting a segmentation network trained on synthetic data to real world scenarios. The usage of the lightweight MobileNet-v2 architecture allows its deployment on devices with limited computational resources as the ones employed in autonomous vehicles.",autonomous vehicle
10.1016/j.comcom.2020.02.009,Journal,Computer Communications,scopus,2020-03-01,sciencedirect,UAV monitoring and forecasting model in intelligent traffic oriented applications,https://api.elsevier.com/content/abstract/scopus_id/85079351564,"Intelligent transportation system is a traffic management system developed with the progress of society and traffic. Its idea is to integrate the real-time operation of people, vehicles, roads and traffic involved in the traffic. The purpose of this paper is to build a safe, reliable and efficient vehicle monitoring and forecasting model for IOT. Based on the Beidou satellite positioning technology and Lora communication technology, aiming at the problem that the deep learning detection method cannot meet the real-time requirements in processing the monitoring video, this paper proposes a method of using multiple single target trackers instead of some yolov3 detection tasks, and puts forward the design idea and specific implementation scheme of the vehicle monitoring and prediction model. The vehicle monitoring and prediction model is used to detect four kinds of targets, namely, small cars, buses, trucks and pedestrians. The multi-target trajectory tracking is used to carry out the traffic statistics of multi vehicle types, the detection of two kinds of abnormal behaviors of traffic targets is low speed and parking, and the capture of pedestrians. The experimental results show that the vehicle monitoring and prediction model has the highest accuracy of location and type recognition for four types of traffic objects, namely, small cars, trucks, buses and pedestrians, reaching 80%.",autonomous vehicle
10.1016/j.robot.2019.103410,Journal,Robotics and Autonomous Systems,scopus,2020-03-01,sciencedirect,Multi-agent sensitivity enhanced iterative best response: A real-time game theoretic planner for drone racing in 3D environments,https://api.elsevier.com/content/abstract/scopus_id/85078148856,"This paper presents a real-time game theoretic motion planning approach that enables an autonomous drone to race competitively against an arbitrary number of opponent drones along a 2D or 3D racecourse. Our method computes an approximate Nash equilibrium in the space of robot trajectories to maximally advance the ego robot while taking into account the opponents’ intentions and responses. The core of our solution is a “sensitivity enhanced” iterative best response algorithm that the ego robot uses to repeatedly plan its own trajectory and infer opponents’ trajectories, ultimately seeking a Nash equilibrium in the joint space of trajectories for all the drones. The algorithm includes a term that allows the ego vehicle to gain advantage by exploiting the influence of the ego drone’s trajectory on the adversaries’ objectives through the shared collision avoidance constraints among the vehicles. We also propose two methods for accelerating this computationally intensive iterative algorithm using (i) parallel computing with multiple CPU cores, and (ii) a neural network model that learns to predict trajectories close to the Nash equilibrium through offline training examples. Extensive simulation studies are conducted to benchmark the performance of our game theoretic planner and the statistical results show that our approach largely outperforms a baseline model predictive control algorithm that does not account for the opponents’ reactions. Hardware experiments with 4 quadrotor robots on a 3D racecourse are performed to show the applicability of our method in real-time robotic systems.",autonomous vehicle
10.1016/j.sysarc.2019.101694,Journal,Journal of Systems Architecture,scopus,2020-02-01,sciencedirect,Efficient drone hijacking detection using two-step GA-XGBoost,https://api.elsevier.com/content/abstract/scopus_id/85076687557,"With the fast growth of civilian drones, their security problems meet significant challenges. A commercial drone may be hijacked by Global Positioning System (GPS)-spoofing attacks for illegal activities, such as terrorist attacks. Ideally, comparing positions respectively estimated by GPS and Inertial Navigation System (INS) can detect such attacks, while the results may always get fault because of the accumulated errors over time in INS. Therefore, in this paper, we propose a two-step GA-XGBoost method to detect GPS-spoofing attacks that just uses GPS and Inertial Measurement Unit (IMU) data. However, tunning the proper values of XGBoost parameters directly on the drone to achieve high prediction results consumes lots of resources which would influence the real-time performance of the drone. The proposed method separates the training phase into offboard step and onboard step. In offboard step, model is first trained by flight logs, and the training parameter values are automatically tuned by Genetic Algorithm (GA). Once the offboard model is trained, it could be uploaded to drones. To adapt our method to drones with different types of sensors and improve the correctness of prediction results, in onboard step, the model is further trained when a drone starts a mission. After onboard training finishes, the proposed method switches to the prediction mode. Besides, our method does not require any extra onboard hardware. The experiments with a real quadrotor drone also show the detection correctness is 96.3% and 100% in hijacked and non-hijacked cases at each sampling time respectively. Moreover, our method can achieve 100% detection correctness just within 1 s just after the attacks start.",autonomous vehicle
10.1016/j.eng.2019.11.003,Journal,Engineering,scopus,2020-02-01,sciencedirect,Grasp Planning and Visual Servoing for an Outdoors Aerial Dual Manipulator,https://api.elsevier.com/content/abstract/scopus_id/85076562878,"This paper describes a system for grasping known objects with unmanned aerial vehicles (UAVs) provided with dual manipulators using an RGB-D camera. Aerial manipulation remains a very challenging task. This paper covers three principal aspects for this task: object detection and pose estimation, grasp planning, and in-flight grasp execution. First, an artificial neural network (ANN) is used to obtain clues regarding the object’s position. Next, an alignment algorithm is used to obtain the object’s six-dimensional (6D) pose, which is filtered with an extended Kalman filter. A three-dimensional (3D) model of the object is then used to estimate an arranged list of good grasps for the aerial manipulator. The results from the detection algorithm—that is, the object’s pose—are used to update the trajectories of the arms toward the object. If the target poses are not reachable due to the UAV’s oscillations, the algorithm switches to the next feasible grasp. This paper introduces the overall methodology, and provides the experimental results of both simulation and real experiments for each module, in addition to a video showing the results.",autonomous vehicle
10.1016/j.ifacol.2020.12.1387,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Modelling human driving behavior for constrained model predictive control in mixed traffic at intersections,https://api.elsevier.com/content/abstract/scopus_id/85105108834,"Safe autonomous passing of intersections with mixed traffic, including human drivers and autonomous vehicles, is challenging. We propose a tailored approach that provides guarantees despite uncertainties fusing learned models and model predictive control. A single autonomous vehicle is controlled by the predictive controller via acceleration and steering angle without assumption of a global controller. Each maneuver of the human behaviour is modeled with a neural network, which enters the predictive controller formulation as a constraint. As an example, we consider a single autonomous vehicle on an unsignalized intersection, which gives right-of-way to a human-driven vehicle. We show how human driving behavior can be modeled based on real recorded trajectory data and implemented in the proposed predictive control approach by dynamically changing the constraints of the optimization problem.",autonomous vehicle
10.1016/j.ifacol.2020.12.1459,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Deep learning based segmentation of fish in noisy forward looking MBES images,https://api.elsevier.com/content/abstract/scopus_id/85105082300,"In this work, we investigate a Deep Learning (DL) approach to fish segmentation in a small dataset of noisy low-resolution images generated by a forward-looking multibeam echosounder (MBES). We build on recent advances in DL and Convolutional Neural Networks (CNNs) for semantic segmentation and demonstrate an end-to-end approach for a fish/non-fish probability prediction for all range-azimuth positions projected by an imaging sonar. We use self-collected datasets from the Danish Sound and the Faroe Islands to train and test our model and present techniques to obtain satisfying performance and generalization even with a low-volume dataset. We show that our model proves the desired performance and has learned to harness the importance of semantic context and take this into account to separate noise and non-targets from real targets. Furthermore, we present techniques to deploy models on low-cost embedded platforms to obtain higher performance fit for edge environments – where compute and power are restricted by size/cost – for testing and prototyping.",autonomous vehicle
10.1016/j.ifacol.2020.12.2306,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Path-following control of fish-like robots: A deep reinforcement learning approach,https://api.elsevier.com/content/abstract/scopus_id/85102821107,"In this paper, we propose a deep reinforcement learning (DRL) approach for path-following control of a fish-like robot. The desired path may be a randomly generated Bézier curve. First, to implement the locomotion control of the fish-like robot, we design a modified Central Pattern Generated (CPG) model, using which the fish achieves varied swimming behaviors just by adjusting a single control input. To reduce the reality gap between simulation and the physical system, using the experimental data of the real fish-like robot, we build a surrogate simulation environment, which also well balances the accuracy and the speed of training. Second, for the path-following control, we select the advantage actor-critic (A2C) approach and train the control policy in the surrogate simulation environment with a straight line as the desired path. Then the trained control policy is directly deployed on a physical fish-like robot to follow a randomly generated Bézier curve. The experimental results show that our proposed approach has good practical applicability in view of its efficiency and feasibility in controlling the physical fishlike robot. This work shows a novel and promising way to control biomimetic underwater robots in the real world.",autonomous vehicle
10.1016/j.trpro.2020.03.108,Conference Proceeding,Transportation Research Procedia,scopus,2020-01-01,sciencedirect,Estimating time of arrival of trains at level crossings for the provision of multimodal cooperative services,https://api.elsevier.com/content/abstract/scopus_id/85084662425,"While cooperative services have been almost fully deployed in the road sector and are already being implemented in various cities in Europe as a pre-requisite for the introduction of autonomous vehicles, few attempts have been made in the same direction for the rail sector. This study proposes a system that aims to improve safety and minimize risk in the meeting point between road and rail, known as level crossings, by monitoring the location of floating road vehicles via a mobile device application. A neural network predictive model for estimating time of arrival of trains is also utilized. The safety system has been implemented and tested under real life conditions in the city of Thessaloniki, Greece.",autonomous vehicle
10.1016/j.eswa.2019.04.023,Journal,Expert Systems with Applications,scopus,2019-12-01,sciencedirect,Experimental analysis of heuristic solutions for the moving target traveling salesman problem applied to a moving targets monitoring system,https://api.elsevier.com/content/abstract/scopus_id/85068183652,"The Traveling Salesman Problem (TSP) is an important problem in computer science which consists in finding a path linking a set of cities so that each of then can be visited once, before the traveler comes back to the starting point. This is highly relevant because several real world problems can be mapped to it. A special case of TSP is the one in which the cities (the points to be visited) are not static as the cities, but mobile, changing their positions as the time passes. This variation is known as Moving Target TSP (MT-TSP). Emerging systems for crowd monitoring and control based on unmanned aerial vehicles (UAVs) can be mapped to this variation of the TSP problem, as a number of persons (targets) in the crowd can be assigned to be monitored by a given number of UAVs, which by their turn divide the targets among them. These target persons have to be visited from time to time, in a similar way to the cities in the traditional TSP. Aiming at finding a suitable solution for this type of crowd monitoring application, and considering the fact that exact solutions are too complex to perform in a reasonable time, this work explores and compares different heuristic methods for the intended solution. The performed experiments showed that the Genetic Algorithms present the best performance in finding acceptable solutions for the problem in restricted time and processing power situations, performing better compared to Ant Colony Optimization and Simulated Annealing Algorithms.",autonomous vehicle
10.1016/j.neucom.2019.08.022,Journal,Neurocomputing,scopus,2019-11-20,sciencedirect,Pixel and feature level based domain adaptation for object detection in autonomous driving,https://api.elsevier.com/content/abstract/scopus_id/85070501446,"Annotating large-scale datasets to train modern convolutional neural networks is prohibitively expensive and time-consuming for many real tasks. One alternative is to train the model on labeled synthetic datasets and apply it in the real scenes. However, this straightforward method often fails to generalize well mainly due to the domain bias between the synthetic and real datasets. Many unsupervised domain adaptation (UDA) methods were introduced to address this problem but most of them only focused on the simple classification task. This paper presents a novel UDA model which integrates both image and feature level based adaptations to solve the cross-domain object detection problem. We employ objectives of the generative adversarial network and the cycle consistency loss for image translation. Furthermore, region proposal based feature adversarial training and classification are proposed to further minimize the domain shifts and preserve the semantics of the target objects. Extensive experiments are conducted on several different adaptation scenarios, and the results demonstrate the robustness and superiority of the proposed method.",autonomous vehicle
10.1016/j.robot.2019.103270,Journal,Robotics and Autonomous Systems,scopus,2019-11-01,sciencedirect,Learning reciprocal actions for cooperative collision avoidance in quadrotor unmanned aerial vehicles,https://api.elsevier.com/content/abstract/scopus_id/85072155500,"The ability to avoid collisions with each other is one of the fundamental requirements for autonomous unmanned aerial vehicles (UAVs) to be safely integrated into the civilian airspace, and for the viability of multi-UAV operations. This paper introduces a new approach for online cooperative collision avoidance between quadcopters, involving reciprocal maneuvers, i.e., coherent maneuvers without requiring any real-time consensus. Two maneuver strategies are presented, where UAVs respectively change their speed or heading to avoid a collision. A learning-based framework that trains these reciprocal actions for collision evasion (called TRACE) is developed. The primary elements of this framework include: 1) designing simulated experiments that cover a variety of UAV–UAV approach scenarios; 2) performing optimization to identify speed/heading change actions that satisfy safety constraints while minimizing the energy cost of the maneuver; and 3) using the offline optimization outcomes to train classifier (via ensemble bagged tree) and function approximation (via neural networks and Kriging) models for respectively selecting and encoding the avoidance actions. Trajectory generation and dynamics/controls are incorporated in the simulation environment used for training and testing. Over 90% accuracy in action prediction and over 95% success in avoiding collisions is observed when the trained models are applied to simulated unseen test scenarios.",autonomous vehicle
10.1016/j.asoc.2019.105650,Journal,Applied Soft Computing Journal,scopus,2019-10-01,sciencedirect,Unsupervised anomaly detection in unmanned aerial vehicles,https://api.elsevier.com/content/abstract/scopus_id/85069952423,"A real-time anomaly detection solution indicates a continuous stream of operational and labelled data that must satisfy several resources and latency requirements. Traditional solutions to the problem rely heavily on well-defined features and prior supervised knowledge, where most techniques refer to hand-crafted rules derived from known conditions. While successful in controlled situations, these rules assume that good data is available for them to detect anomalies; indicating that these rules will fail to generalise beyond known scenarios.
                  To investigate these issues, current literature is examined for solutions that can be used to detect known and unknown anomalous instances whilst functioning as an out-of-the-box approach for efficient decision-making. The applicability of the isolation forest is discussed for engineering applications using the Aero-Propulsion System Simulation dataset as a benchmark where it is shown to outperform other unsupervised distance-based approaches. Also, the authors have carried out real-time experiments on an unmanned aerial vehicle to highlight further applications of the method. Finally, some conclusions are drawn concerning its simplicity and robustness in handling diagnostic problems.",autonomous vehicle
10.1016/j.isprsjprs.2019.07.009,Journal,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2019-09-01,sciencedirect,Development and evaluation of a deep learning model for real-time ground vehicle semantic segmentation from UAV-based thermal infrared imagery,https://api.elsevier.com/content/abstract/scopus_id/85069676654,"Real-time unmanned aerial vehicles (UAVs)-based thermal infrared images processing, due to high spatial resolution and knowledge of the various infrared radiant energy level distribution of solid bodies, has important applications such as monitoring and control of the various phenomena in different natural situations. One of these applications is monitoring the ground vehicles in cities by using detection or semantic segmentation of them in the thermal images. In this research, our purpose is to improve the performance of deep learning combined model by using Gaussian-Bernoulli Restricted Boltzmann Machine (GB-RBM) specifications for the segmentation of the ground vehicles from UAV-based thermal infrared imagery. The proposed model is studied in three steps. First, designing the proposed model by using an encoder-decoder structure and addition of extracted features from convolutional layers and restricted Boltzmann machine in the network. Second, the implementation of the research goals on four sets of UAV-based thermal infrared imagery named NPU_CS_UAV_IR_DATA that was collected from some streets of China by using FLIR TAU2 thermal infrared sensor in 2017. Finally, analyzing the performance of the proposed model by using five state-of-the-art models in semantic segmentation. The results evaluated the performance of the proposed model as a robust model with the average precision and average processing time of approximately 0.97, and 19.73 s for all datasets, respectively.",autonomous vehicle
10.1016/j.cie.2018.11.008,Journal,Computers and Industrial Engineering,scopus,2019-09-01,sciencedirect,A new approach to oil spill detection that combines deep learning with unmanned aerial vehicles,https://api.elsevier.com/content/abstract/scopus_id/85056668357,"This study presents a novel approach to automatic oil spill detection, using unmanned aerial vehicle (UAV) images to realize intelligent control in oil production. Despite considerable effort, oil spills still cannot be detected automatically and effectively due to the complexity of the real production environment, which forces oil enterprises to manually inspect facilities and detect oil spills. To solve the problem, we propose an approach consisting of UAVs, deep learning and traditional algorithms—an approach which divides the oil spill detection task into three independent sub-tasks. First, we constructed a model based on the deep convolutional neural network, which can quickly detect the suspected oil spill area in images to ensure there are no omissions. Second, to remove other obstacles in the images, we adjusted the Otsu algorithm to filter the detection results, which improves precision while not affecting the recall rate. Third, the Maximally Stable Extremal Regions algorithm was used to obtain the detail polygon region from the detection box, thus automatically evaluating the severity of the oil spill. Experiments showed that our method could solve problems effectively, reducing the cost of oil spill detection by 57.2% when compared with the traditional manual inspection process.",autonomous vehicle
10.1016/j.net.2018.12.020,Journal,Nuclear Engineering and Technology,scopus,2019-06-01,sciencedirect,Numerical evaluation of gamma radiation monitoring,https://api.elsevier.com/content/abstract/scopus_id/85059358720,"Airborne Gamma Ray Spectrometry (AGRS) with its important applications such as gathering radiation information of ground surface, geochemistry measuring of the abundance of Potassium, Thorium and Uranium in outer earth layer, environmental and nuclear site surveillance has a key role in the field of nuclear science and human life. The Broyden–Fletcher–Goldfarb–Shanno (BFGS), with its advanced numerical unconstrained nonlinear optimization in collaboration with Artificial Neural Networks (ANNs) provides a noteworthy opportunity for modern AGRS. In this study a new AGRS system empowered by ANN-BFGS has been proposed and evaluated on available empirical AGRS data. To that effect different architectures of adaptive ANN-BFGS were implemented for a sort of published experimental AGRS outputs. The selected approach among of various training methods, with its low iteration cost and non-diagonal scaling allocation is a new powerful algorithm for AGRS data due to its inherent stochastic properties. Experiments were performed by different architectures and trainings, the selected scheme achieved the smallest number of epochs, the minimum Mean Square Error (MSE) and the maximum performance in compare with different types of optimization strategies and algorithms. The proposed method is capable to be implemented on a cost effective and minimum electronic equipment to present its real-time process, which will let it to be used on board a light Unmanned Aerial Vehicle (UAV). The advanced adaptation properties and models of neural network, the training of stochastic process and its implementation on DSP outstands an affordable, reliable and low cost AGRS design. The main outcome of the study shows this method increases the quality of curvature information of AGRS data while cost of the algorithm is reduced in each iteration so the proposed ANN-BFGS is a trustworthy appropriate model for Gamma-ray data reconstruction and analysis based on advanced novel artificial intelligence systems.",autonomous vehicle
10.1016/j.robot.2018.11.017,Journal,Robotics and Autonomous Systems,scopus,2019-05-01,sciencedirect,A real-time framework for kinodynamic planning in dynamic environments with application to quadrotor obstacle avoidance,https://api.elsevier.com/content/abstract/scopus_id/85062619374,"The objective of this paper is to present a full-stack, real-time motion planning framework for kinodynamic robots and then show how it is applied and demonstrated on a physical quadrotor system operating in a laboratory environment. The proposed framework utilizes an offline–online computation paradigm, neighborhood classification through machine learning, sampling-based motion planning with an optimal cost distance metric, and trajectory smoothing to achieve real-time planning for aerial vehicles. This framework accounts for dynamic obstacles with an event-based replanning structure and a locally reactive control layer that minimizes replanning events. The approach is demonstrated on a quadrotor navigating moving obstacles in an indoor space and stands as, arguably, one of the first demonstrations of full-online kinodynamic motion planning, with execution cycles of 3 Hz to 5 Hz. For the quadrotor, a simplified dynamics model is used during the planning phase to accelerate online computation. A trajectory smoothing phase, which leverages the differentially flat nature of quadrotor dynamics, is then implemented to guarantee a dynamically feasible trajectory.",autonomous vehicle
10.1016/j.asoc.2018.12.013,Journal,Applied Soft Computing Journal,scopus,2019-03-01,sciencedirect,Online identification of a rotary wing Unmanned Aerial Vehicle from data streams,https://api.elsevier.com/content/abstract/scopus_id/85059117819,"Until now the majority of the neuro and fuzzy modeling and control approaches for rotary wing Unmanned Aerial Vehicles (UAVs), such as the quadrotor, have been based on batch learning techniques, therefore static in structure, and cannot adapt to rapidly changing environments. Implication of Evolving Intelligent System (EIS) based model-free data-driven techniques in fuzzy system are good alternatives, since they are able to evolve both their structure and parameters to cope with sudden changes in behavior, and performs perfectly in a single pass learning mode which is suitable for online real-time deployment. The Metacognitive Scaffolding Learning Machine (McSLM) is seen as a generalized version of EIS since the metacognitive concept enables the what-to-learn, how-to-learn, and when-to-learn scheme, and the scaffolding theory realizes a plug-and-play property which strengthens the online working principle of EISs. This paper proposes a novel online identification scheme, applied to a quadrotor using real-time experimental flight data streams based on McSLM, namely Metacognitive Scaffolding Interval Type 2 Recurrent Fuzzy Neural Network (McSIT2RFNN). Our proposed approach demonstrated significant improvements in both accuracy and complexity against some renowned existing variants of the McSLMs and EISs.",autonomous vehicle
10.1016/j.ifacol.2019.12.517,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,The development of autonomous navigation and obstacle avoidance for a robotic mower using machine vision technique,https://api.elsevier.com/content/abstract/scopus_id/85081065786,"The autonomous driving of agricultural machinery using information from global navigation satellite system (GNSS) information has developed rapidly because it is considered as a labor-saving measure in agriculture. The agricultural machinery is able to locate its position using a GNSS signal allowing it to move in an area autonomously. However, if machinery uses the GNSS signal only to self-locate it may run the risk of colliding with obstacles as it may not accurately sense the surrounding environment. Furthermore, sensors such as radars or lasers cannot distinguish between grass and obstacles; hence they cannot be used for sensing an agricultural environment including the detection of obstacles that are likely to be encountered by the machinery. Autonomous driving cannot be performed in environments such as orchards where the satellite positioning accuracy is low. This paper presents an autonomous driving system that we developed that is able to avoid obstacles and drive without the aid of a GNSS signal. The system uses an object detection system that is based on a stereo camera and deep learning technique i.e. convolutional neural networks as they can be used to recognize an environment and avoid obstacles. The autonomous driving ability of the vehicle was evaluated using real-time kinematic-GNSS to measure the true values through experiments that were conducted in the Tanashi Forest of the University of Tokyo.",autonomous vehicle
10.1016/j.ifacol.2019.12.529,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,Autonomous Canal Following by a Micro-Aerial Vehicle Using Deep CNN,https://api.elsevier.com/content/abstract/scopus_id/85081052924,"Globally, large-scale irrigation canal networks serve as the backbone of agriculture in many important river basins. However, these water channels are in a constant threat of erosion, silt accumulation and structural damages over time which significantly reduces the water carrying capacity. Therefore, periodic inspections of the canals are required for critical operations and maintenance tasks. Due to the vast lengths of the channels and time-critical operations, automation has become a necessity. In this paper, we have proposed an aerial autonomous canal traversal system using ResNet50 inspired deep convolutional neural network. Given the uniqueness of our problem, we have generated our dataset for supervised learning and validation and later evaluated the proposed approach on a real canal. We have implemented our approach on a COTS micro-aerial vehicle. We have designed our system in such a way that it takes 200ms from perception to action thereby making the system real-time. We compare the superior performance of our Res Net 50 inspired network with other state-of-the-art CNNs trained on canal datasets.",autonomous vehicle
10.1016/j.procs.2019.09.442,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,Unmanned aerial vehicle in the machine learning environment,https://api.elsevier.com/content/abstract/scopus_id/85079097933,"Unmanned Aerial Vehicles and machine learning have started gaining attentions of academic and industrial research. The Unmanned Aerial Vehicles have extended the freedom to operate and monitor the activities from remote locations. This study retrieved and synthesized research on the use of Unmanned Aerial Vehicles along with machine learning and its algorithms in different areas and regions. The objective was to synthesize the scope and importance of machine learning models in enhancing Unmanned Aerial Vehicles capabilities, solutions to problems, and numerous application areas.
                  The machine learning implementation has reduced numbers of challenges to Unmanned Aerial Vehicles besides enhancing the capabilities and opening the door to the different sectors. The Unmanned Aerial Vehicles and machine learning association has resulted in fast and reliable outputs. The combination of Unmanned Aerial Vehicles and machine learning has helped in real time monitoring, data collection and processing, and prediction in the computer/wireless networks, smart cities, military, agriculture, and mining.",autonomous vehicle
10.1016/j.asoc.2018.10.010,Journal,Applied Soft Computing Journal,scopus,2019-01-01,sciencedirect,A hybridization of extended Kalman filter and Ant Colony Optimization for state estimation of nonlinear systems,https://api.elsevier.com/content/abstract/scopus_id/85056208326,"In this paper, a new nonlinear heuristic filter based on the hybridization of an extended Kalman filter and an ant colony estimator is proposed to estimate the states of a nonlinear system. In this filter, a group of virtual ants searches the state space stochastically and dynamically to find and track the best state estimation while the position of each ant is updated at the measurement time using the extended Kalman filter. The performance of the proposed filter is compared with well-known heuristic filters using a nonlinear benchmark problem. The statistical results show that this algorithm is able to provide promising and competitive results. Then, the new filter is tested on a nonlinear engineering problem with more than one state. The problem is to estimate simultaneously the states of an unmanned aerial vehicle as well as the wind disturbances, applied to the system. In this case, a processor-in-the-loop experiment is also performed to verify the implementation capability of the proposed approach. This paper also investigates the real-time implementation capability of the proposed filter in the attitude estimation of a three degrees of freedom experimental setup of a quadrotor to further investigate its effectiveness in practice.",autonomous vehicle
10.1016/j.patrec.2017.09.038,Journal,Pattern Recognition Letters,scopus,2018-11-01,sciencedirect,Multimodal vehicle detection: fusing 3D-LIDAR and color camera data,https://api.elsevier.com/content/abstract/scopus_id/85030769477,"Most of the current successful object detection approaches are based on a class of deep learning models called Convolutional Neural Networks (ConvNets). While most existing object detection researches are focused on using ConvNets with color image data, emerging fields of application such as Autonomous Vehicles (AVs) which integrates a diverse set of sensors, require the processing for multisensor and multimodal information to provide a more comprehensive understanding of real-world environment. This paper proposes a multimodal vehicle detection system integrating data from a 3D-LIDAR and a color camera. Data from LIDAR and camera, in the form of three modalities, are the inputs of ConvNet-based detectors which are later combined to improve vehicle detection. The modalities are: (i) up-sampled representation of the sparse LIDAR’s range data called dense-Depth Map (DM), (ii) high-resolution map from LIDAR’s reflectance data hereinafter called Reflectance Map (RM), and (iii) RGB image from a monocular color camera calibrated wrt the LIDAR. Bounding Box (BB) detections in each one of these modalities are jointly learned and fused by an Artificial Neural Network (ANN) late-fusion strategy to improve the detection performance of each modality. The contribution of this paper is two-fold: (1) probing and evaluating 3D-LIDAR modalities for vehicle detection (specifically the depth and reflectance map modalities), and (2) joint learning and fusion of the independent ConvNet-based vehicle detectors (in each modality) using an ANN to obtain more accurate vehicle detection. The obtained results demonstrate that (1) DM and RM are very promising modalities for vehicle detection, and (2) experiments show that the proposed fusion strategy achieves higher accuracy compared to each modality alone in all the levels of increasing difficulty (easy, moderate, hard) in KITTI object detection dataset.",autonomous vehicle
10.1016/j.robot.2018.05.016,Journal,Robotics and Autonomous Systems,scopus,2018-09-01,sciencedirect,Adaptive low-level control of autonomous underwater vehicles using deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85048259874,"Low-level control of autonomous underwater vehicles (AUVs) has been extensively addressed by classical control techniques. However, the variable operating conditions and hostile environments faced by AUVs have driven researchers towards the formulation of adaptive control approaches. The reinforcement learning (RL) paradigm is a powerful framework which has been applied in different formulations of adaptive control strategies for AUVs. However, the limitations of RL approaches have lead towards the emergence of deep reinforcement learning which has become an attractive and promising framework for developing real adaptive control strategies to solve complex control problems for autonomous systems. However, most of the existing applications of deep RL use video images to train the decision making artificial agent but obtaining camera images only for an AUV control purpose could be costly in terms of energy consumption. Moreover, the rewards are not easily obtained directly from the video frames. In this work we develop a deep RL framework for adaptive control applications of AUVs based on an actor-critic goal-oriented deep RL architecture, which takes the available raw sensory information as input and as output the continuous control actions which are the low-level commands for the AUV’s thrusters. Experiments on a real AUV demonstrate the applicability of the stated deep RL approach for an autonomous robot control problem.",autonomous vehicle
10.1016/j.knosys.2018.04.015,Journal,Knowledge-Based Systems,scopus,2018-08-01,sciencedirect,Teaching a vehicle to autonomously drift: A data-based approach using Neural Networks,https://api.elsevier.com/content/abstract/scopus_id/85045849458,"This paper presents a novel approach to teach a vehicle how to drift, in a similar manner that professional drivers do. Specifically, a hybrid structure formed by a Model Predictive Controller and feedforward Neural Networks is employed for this purpose. The novelty of this work lies in a) the adoption of a data-based approach to achieve autonomous drifting along a wide range of road radii and body slip angles, and b) in the implementation of a road terrain classifier to adjust the system actuation depending on the current friction characteristics. The presented drift control system is implemented in a multi-actuated ground vehicle equipped with active front steering and in-wheel electric motors and trained to drift by a real test driver using a driver-in-the-loop setup. Its performance is verified in the simulation environment IPG-CarMaker through different open loop and path following drifting manoeuvres.",autonomous vehicle
10.1016/j.compind.2018.03.014,Journal,Computers in Industry,scopus,2018-06-01,sciencedirect,Real-time object detection in agricultural/remote environments using the multiple-expert colour feature extreme learning machine (MEC-ELM),https://api.elsevier.com/content/abstract/scopus_id/85044151304,"It is necessary for autonomous robotics in agriculture to provide real time feedback, but due to a diverse array of objects and lack of landscape uniformity this objective is inherently complex. The current study presents two implementations of the multiple-expert colour feature extreme learning machine (MEC-ELM). The MEC-ELM is a cascading algorithm that has been implemented along side a summed area table (SAT) for fast feature extraction and object classification, for a fully functioning object detection algorithm. The MEC-ELM is an implementation of the colour feature extreme learning machine (CF-ELM), which is an extreme learning machine (ELM) with a partially connected hidden layer; taking three colour bands as inputs. The colour implementation used with the SAT enable the MEC-ELM to find and classify objects quickly, with 84% precision and 91% recall in weed detection in the Y’UV colour space and in 0.5 s per frame. The colour implementation is however limited to low resolution images and for this reason a colour level co-occurrence matrix (CLCM) variant of the MEC-ELM is proposed. This variant uses the SAT to produce a CLCM and texture analyses, with texture values processed as an input to the MEC-ELM. This enabled the MEC-ELM to achieve 78–85% precision and 81–93% recall in cattle, weed and quad bike detection and in times between 1 and 2 s per frame. Both implementations were benchmarked on a standard i7 mobile processor. Thus the results presented in this paper demonstrated that the MEC-ELM with SAT grid and CLCM makes an ideal candidate for fast object detection in complex and/or agricultural landscapes.",autonomous vehicle
10.1016/j.compind.2018.02.015,Journal,Computers in Industry,scopus,2018-06-01,sciencedirect,Comprehensive evaluation method for performance of unmanned robot applied to automotive test using fuzzy logic and evidence theory and FNN,https://api.elsevier.com/content/abstract/scopus_id/85043482449,"In order to obtain reliable and exact evaluation, a new comprehensive evaluation method for performance of an unmanned robot applied to automotive test (URAT) using fuzzy logic, evidence theory and fuzzy neural network (FNN) is presented in this paper. Throttle repeatability, speed tracking accuracy, speed repeatability, driving shock degree are used as the system evaluation index. The subjective evaluation results with various expressions are quantified using fuzzy logic. The group decision making with quantified subjective evaluation results from various drivers is combined through evidence theory. The objective evaluation indexes measured by instrumentation and the corresponding combined subjective evaluation are self-learned and trained with FNN. The comprehensive performance evaluation system of the URAT is established. Finally, real vehicle experiments are conducted. The effectiveness of the presented method for the URAT is experimentally verified.",autonomous vehicle
10.1016/j.neucom.2017.12.016,Journal,Neurocomputing,scopus,2018-03-22,sciencedirect,Use of human gestures for controlling a mobile robot via adaptive CMAC network and fuzzy logic controller,https://api.elsevier.com/content/abstract/scopus_id/85038844344,"Mobile robots with manipulators have been more and more commonly applied in extreme and hostile environments to assist or even replace human operators for complex tasks. In addition to autonomous abilities, mobile robots need to facilitate the human–robot interaction control mode that enables human users to easily control or collaborate with robots. This paper proposes a system which uses human gestures to control an autonomous mobile robot integrating a manipulator and a video surveillance platform. A human user can control the mobile robot just as one drives an actual vehicle in the vehicle’s driving cab. The proposed system obtains human’s skeleton joints information using a motion sensing input device, which is then recognized and interpreted into a set of control commands. This is implemented, based on the availability of training data set and requirement of in-time performance, by an adaptive cerebellar model articulation controller neural network, a finite state machine, a fuzzy controller and purposely designed gesture recognition and control command generation systems. These algorithms work together implement the steering and velocity control of the mobile robot in real-time. The experimental results demonstrate that the proposed approach is able to conveniently control a mobile robot using virtual driving method, with smooth manoeuvring trajectories in various speeds.",autonomous vehicle
10.1016/j.artint.2017.12.001,Journal,Artificial Intelligence,scopus,2018-03-01,sciencedirect,Decentralized Reinforcement Learning of robot behaviors,https://api.elsevier.com/content/abstract/scopus_id/85038868982,"A multi-agent methodology is proposed for Decentralized Reinforcement Learning (DRL) of individual behaviors in problems where multi-dimensional action spaces are involved. When using this methodology, sub-tasks are learned in parallel by individual agents working toward a common goal. In addition to proposing this methodology, three specific multi agent DRL approaches are considered: DRL-Independent, DRL Cooperative-Adaptive (CA), and DRL-Lenient. These approaches are validated and analyzed with an extensive empirical study using four different problems: 3D Mountain Car, SCARA Real-Time Trajectory Generation, Ball-Dribbling in humanoid soccer robotics, and Ball-Pushing using differential drive robots. The experimental validation provides evidence that DRL implementations show better performances and faster learning times than their centralized counterparts, while using less computational resources. DRL-Lenient and DRL-CA algorithms achieve the best final performances for the four tested problems, outperforming their DRL-Independent counterparts. Furthermore, the benefits of the DRL-Lenient and DRL-CA are more noticeable when the problem complexity increases and the centralized scheme becomes intractable given the available computational resources and training time.",autonomous vehicle
10.1016/j.bdr.2017.06.002,Journal,Big Data Research,scopus,2018-03-01,sciencedirect,Fast Deep Convolutional Face Detection in the Wild Exploiting Hard Sample Mining,https://api.elsevier.com/content/abstract/scopus_id/85025460499,"Face detection constitutes a key visual information analysis task in Machine Learning. The rise of Big Data has resulted in the accumulation of a massive volume of visual data which requires proper and fast analysis. Deep Learning methods are powerful approaches towards this task as training with large amounts of data exhibiting high variability has been shown to significantly enhance their effectiveness, but often requires expensive computations and leads to models of high complexity. When the objective is to analyze visual content in massive datasets, the complexity of the model becomes crucial to the success of the model. In this paper, a lightweight deep Convolutional Neural Network (CNN) is introduced for the purpose of face detection, designed with a view to minimize training and testing time, and outperforms previously published deep convolutional networks in this task, in terms of both effectiveness and efficiency. To train this lightweight deep network without compromising its efficiency, a new training method of progressive positive and hard negative sample mining is introduced and shown to drastically improve training speed and accuracy. Additionally, a separate deep network was trained to detect individual facial features and a model that combines the outputs of the two networks was created and evaluated. Both methods are capable of detecting faces under severe occlusion and unconstrained pose variation and meet the difficulties of large scale real-world, real-time face detection, and are suitable for deployment even in mobile environments such as Unmanned Aerial Vehicles (UAVs).",autonomous vehicle
10.1016/j.neucom.2017.11.008,Journal,Neurocomputing,scopus,2018-01-31,sciencedirect,A dynamic colour perception system for autonomous robot navigation on unmarked roads,https://api.elsevier.com/content/abstract/scopus_id/85033771926,"Navigation on unmarked and possible poorly delineated roads where the boundaries between the road and the non-road surfaces are not clearly indicated is a particularly challenging task for autonomous vehicles. The results of this study show that fairly robust navigation strategies can be generated by a robot equipped with a dynamic active-vision based control system represented by an artificial neural network synthesized using evolutionary computation techniques. In the experiments described in this paper, a simulated Pioneer robot is required to visually navigate multiple poorly delineated roads that differ in terms of variations in luminance and/or chrominance between the road and the adjacent non-road areas. Low resolution camera images are processed by a mechanism that continuously adjusts the contribution of each component of a three dimensional colour model (e.g., R, G and B) to the generation of the robot perceptual experience. We show that the best controller can successfully drive a simulated Pioneer robot in environments with colour characteristics never encountered during the design phase, and operate with colour models never used during training. We show that the dynamic differential weighting of the colour components is underpinned by a complex pattern of neural activity that allows the robot to successfully adapt its perceptual system to the colour characteristics of different visual scenes. We also show that the controller can be easily ported onto real hardware, by showing the results of a series of tests with a physical Pioneer robot required to navigate various poorly delineated pedestrian roads.",autonomous vehicle
10.1016/j.neucom.2017.09.044,Journal,Neurocomputing,scopus,2018-01-31,sciencedirect,Transferring deep knowledge for object recognition in Low-quality underwater videos,https://api.elsevier.com/content/abstract/scopus_id/85030701582,"In recent years, underwater video technologies allow us to explore the ocean in scientific and noninvasive ways, such as environmental monitoring, marine ecology studies, and fisheries management. However the low-light and high-noise scenarios pose great challenges for the underwater image and video analysis. We here propose a CNN knowledge transfer framework for underwater object recognition and tackle the problem of extracting discriminative features from relatively low contrast images. Even with the insufficient training set, the transfer framework can well learn a recognition model for the special underwater object recognition task together with the help of data augmentation. For better identifying objects from an underwater video, a weighted probabilities decision mechanism is introduced to identify the object from a series of frames. The proposed framework can be implemented for real-time underwater object recognition on autonomous underwater vehicles and video monitoring systems. To verify the effectiveness of our method, experiments on a public dataset are carried out. The results show that the proposed method achieves promising results for underwater object recognition on both test image datasets and underwater videos.",autonomous vehicle
10.1016/j.procs.2018.10.294,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,DSRC based sensor-pooling protocol for connected vehicles in future smart cities,https://api.elsevier.com/content/abstract/scopus_id/85061968937,"Smart cities are racing to create a more connected Intelligent Transportation Systems (ITS) that rely on collecting data from every possible sensor such as a smart utility meter or a smart parking meter. The use of more sensors resulted in generating a lot of information that maps the smart city environment conditions to more real time data points that needed to be shared and analyzed among smart city nodes. One possibility, to carry and share the collected data, is in autonomous vehicles systems, which use the Dedicated Short Range Communications (DSRC) technology. For example, in a Car-to-Parking-Meter or a Vehicle-to-Vehicle (V2V) communications, short-range embedded sensors such as Bluetooth, Cameras, Lidar send the collected data to the vehicle’s Electronic Control Unit (ECU) or to a road side gateway for making collaborative decisions and react to the environment’s surrounding conditions.
                  The goal of this research is to develop and test a DSRC based sensor-pooling protocol for vehicles to cooperatively communicate inclement weather or environment conditions. Five simulation experiments are setup using PreScan and Simulink to validate and study the scalability of the proposed solution. PreScan is an automotive simulation platform that is used for developing and testing Advanced Driver Assistance System (ADAS). The research findings proved that the DSRC can be used to effectively stream the short range sensors’ collected data over a long distance communications link.",autonomous vehicle
10.1016/j.ifacol.2018.07.044,Conference Proceeding,,scopus,2018-01-01,sciencedirect,A Novel Driver Performance Model Based on Machine Learning,https://api.elsevier.com/content/abstract/scopus_id/85050093718,"Models of road vehicle driver behaviour are widely used in several disciplines, like driver distraction and autonomous driving. In this paper, a novel driver performance model, which is unique for every driver, is introduced. The driver is modelled with machine learning algorithms, namely artificial neural network and adaptive neuro-fuzzy inference system. Every model is trained and validated with the data collected during the real-time driver-in-the-loop experiment on a vehicle simulator for each driver separately. In total, 18 participants contributed to the experiment. Although the prediction accuracy of the models depends on the algorithm specifications, the artificial neural network was slightly more accurate in driver performance prediction comparing to the adaptive neuro-fuzzy inference system. The driver models may be used in detection of driver distraction induced by in-vehicle information system.",autonomous vehicle
10.1016/j.ifacol.2018.05.081,Conference Proceeding,,scopus,2018-01-01,sciencedirect,Vision-based Control for Aerial Obstacle Avoidance in Forest Environments,https://api.elsevier.com/content/abstract/scopus_id/85048938144,"The increasing application of unmanned aerial vehicles (UAVs) in unstructured, natural environments raises the demand for robust obstacle avoidance systems that work in realtime. In view of this problem, we present a control algorithm for quadrotors based on monocular vision that is specifically designed for obstacle avoidance in forest environments. The algorithm presented is an enhancement of an existing algorithm, originally proposed and tested for usage with rovers, that uses a weighted combination of texture features to compute distances to nearest obstacle in various longitudinal strips of the image frame. The weights are pre-computed by means of supervised learning of correspondences of the features to ground-truth distances processed on frames derived from a simulated forest environment. The modifications proposed on the original algorithm show significant improvement in its obstacle-distance estimation accuracy and computational efficiency as observed from actual autonomous flying experiments carried out on an off-the-shelf quadrotor. The modified algorithm works at 15 frames/sec for a frame size of 160 x 120 pixels as profiled on an Odroid XU4 mini-computer. Results from both simulated images and real videos have been presented.",autonomous vehicle
10.1016/j.ins.2017.07.020,Journal,Information Sciences,scopus,2017-11-01,sciencedirect,Novel Levenberg–Marquardt based learning algorithm for unmanned aerial vehicles,https://api.elsevier.com/content/abstract/scopus_id/85025694069,"In this paper, Levenberg–Marquardt inspired sliding mode control theory based adaptation laws are proposed to train an intelligent fuzzy neural network controller for a quadrotor aircraft. The proposed controller is used to control and stabilize a quadrotor unmanned aerial vehicle in the presence of periodic wind gust. A proportional-derivative controller is firstly introduced based on which fuzzy neural network is able to learn the quadrotor’s control model on-line. The proposed design allows handling uncertainties and lack of modelling at a computationally inexpensive cost. The parameter update rules of the learning algorithms are derived based on a Levenberg–Marquardt inspired approach, and the proof of the stability of two proposed control laws are verified by using the Lyapunov stability theory. In order to evaluate the performance of the proposed controllers extensive simulations and real-time experiments are conducted. The 3D trajectory tracking problem for a quadrotor is considered in the presence of time-varying wind conditions.",autonomous vehicle
10.1016/j.patrec.2017.09.007,Journal,Pattern Recognition Letters,scopus,2017-10-15,sciencedirect,Let the robot tell: Describe car image with natural language via LSTM,https://api.elsevier.com/content/abstract/scopus_id/85029030417,"Image-based car detection and classification has remained as a research hub in self-driving for decades. However, natural language description of car images is still a virgin territory even though it is a simple task for human to describe it by sentences within a glimpse at the image. In this paper, we present an end-to-end trainable and spatial-temporal deep recurrent neural network: LSTM (Long-Short Term Memory) to automatically convert car images to human understandable natural language descriptions. Our model builds on state of the art progress in computer vision and machine translation: we extract car region proposals with Region Convolutional Neural Networks(R-CNN) and embed them into fixed-sized vectors. Each word in a sentence is also embedded into real-valued vectors of the same size of images through a local global context aware neural network. The LSTM, feeding by image-sentence pairs sequentially in the training stage, is trained to maximize the joint probability of target word in each time step. In the test stage, the pre-trained LSTM receives a car image and predicts natural language description word by word. Finally, we evaluate our model regarding car’s static/dynamic attribute description on both 30,000 CompCar dataset [21] and 1000 video dataset collected on street scenario by our self-driving car, with quantitative BLEU score and subjective human-rating system evaluation metrics. We test our model’s generalization ability, its transfer ability to address car property classification issue and various image feature extractors’ impact on our model. Experiment results show the superiority and robustness of our model (refer to www.carlib.net/carimg2text.html for more experiment results).",autonomous vehicle
10.1016/j.ifacol.2017.08.573,Conference Proceeding,IFAC-PapersOnLine,scopus,2017-07-01,sciencedirect,Emotions Embodied in the SVC of an Autonomous Driver System,https://api.elsevier.com/content/abstract/scopus_id/85031823496,"A concept of embodied intelligence (EI) is considered. None of such implementations can be fully identified with artificial intelligence. Projects that dare to approach AI and EI should be based on both the AI concepts (symbolic and sub-symbolic), in solving real problems of perception and decision-making. Therefore, the EI, in this paper, is understood as a methodology that uses all available resources and algorithms from the analytic and synthetic approaches, in order to implement an intelligent and autonomous agent.",autonomous vehicle
10.1016/j.ifacol.2017.08.1219,Conference Proceeding,IFAC-PapersOnLine,scopus,2017-07-01,sciencedirect,Implementation of Brain Emotional Learning-Based Intelligent Controller for Flocking of Multi-Agent Systems,https://api.elsevier.com/content/abstract/scopus_id/85031823232,"The Brain Emotional Learning Based Intelligent Controller (BELBIC) is a neurobiologically-motivated intelligent controller based on a computational model of emotional learning in mammalian limbic system. The learning capabilities, multi-objective properties, and low computational complexity of BELBIC make it a very promising tool for implementation in real-time applications.
                  Our research combines, in an original way, the BELBIC methodology with a flocking control strategy, in order to perform real-time coordination of multiple Unmanned Aircraft Systems (UAS). The characteristics of BELBIC fit well in this scenario, since almost always the dynamics of the autonomous agents are not fully known, and furthermore, since they operate in close proximity, they are subjected to aggressive external disturbances. Numerical and experimental results based on the coordination of multiple quad rotorcraft UAS platforms demonstrate the applicability and satisfactory performance of the proposed method.",autonomous vehicle
10.1016/j.firesaf.2017.03.085,Journal,Fire Safety Journal,scopus,2017-07-01,sciencedirect,An integrated approach for tactical monitoring and data-driven spread forecasting of wildfires,https://api.elsevier.com/content/abstract/scopus_id/85019046888,"In recent times there have been increasing efforts to integrate technology into wildfire management, especially in the fields of tactical monitoring and simulation. On the one hand, thermal infrared imaging (TIR) systems have been installed aboard surveillance aircraft including unmanned systems (UAS). On the other, there exists a variety of models and simulators able to forecast the fire spread. However, both fields currently present significant limitations. While relevant information is still extracted manually from aerial thermal imagery and is most times merely qualitative, simulators’ accuracy on fire spread prediction has proved insufficient. To solve these issues, this article presents a twofold methodology to couple meaningful automated wildfire monitoring with accurate fire spread forecasting. The main goals are to, firstly, automatically process aerial TIR imagery so that valuable information can be produced in real time during the event and, secondly, use this information to adjust a Rothermel-based simulator in order to improve its accuracy on-line. The fire perimeter location is tracked automatically through an unsupervised edge detector. Afterwards, an assimilation module uses the remotely sensed data to optimise the simulator's fuel and wind parameters, which are assumed to remain constant for a certain period of time. Subsequently, the optimum parameters’ values are used to issue a fire evolution forecast. All outputs are projected onto the corresponding Digital Terrain Model (DTM) and integrated into a Geographic Information System (GIS) for visualization. The global system was validated using two large-scale experiments. If these algorithms can be applied to a sufficiently rich and varied set of experimental data and further developed to cope with more complex scenarios, they could eventually be incorporated into a fire management decision support system.",autonomous vehicle
10.1016/j.engappai.2016.08.019,Journal,Engineering Applications of Artificial Intelligence,scopus,2017-06-01,sciencedirect,GPU-based parallel optimization of immune convolutional neural network and embedded system,https://api.elsevier.com/content/abstract/scopus_id/84995489085,"Up to now, the image recognition system has been utilized more and more widely in the security monitoring, the industrial intelligent monitoring, the unmanned vehicle, and even the space exploration. In designing the image recognition system, the traditional convolutional neural network has some defects such as long training time, easy over-fitting and high misclassification rate. In order to overcome these defects, we firstly used the immune mechanism to improve the convolutional neural network and put forward a novel immune convolutional neural network algorithm, after we analyzed the network structure and parameters of the convolutional neural network. Our algorithm not only integrated the location data of the network nodes and the adjustable parameters, but also dynamically adjusted the smoothing factor of the basis function. In addition, we utilized the NVIDIA GPU (Graphics Processing Unit) to accelerate the new immune convolutional neural network (ICNN) in parallel computing and built a real-time embedded image recognition system for this ICNN. The immune convolutional neural network algorithm was improved with CUDA programming and was tested with the sample data in the GPU-based environment. The GPU-based implementation of the novel immune convolutional neural network algorithm was made with the cuDNN, which was designed by NVIDIA for GPU-based accelerating of DNNs in machine learning. Experimental results show that our new immune convolutional neural network has higher recognition rate, more stable performance and faster computing speed than the traditional convolutional neural network.",autonomous vehicle
10.1016/j.jpowsour.2017.01.105,Journal,Journal of Power Sources,scopus,2017-01-01,sciencedirect,Adaptive prognosis of lithium-ion batteries based on the combination of particle filters and radial basis function neural networks,https://api.elsevier.com/content/abstract/scopus_id/85011392342,"Lithium-Ion rechargeable batteries are widespread power sources with applications to consumer electronics, electrical vehicles, unmanned aerial and spatial vehicles, etc. The failure to supply the required power levels may lead to severe safety and economical consequences. Thus, in view of the implementation of adequate maintenance strategies, the development of diagnostic and prognostic tools for monitoring the state of health of the batteries and predicting their remaining useful life is becoming a crucial task. Here, we propose a method for predicting the end of discharge of Li-Ion batteries, which stems from the combination of particle filters with radial basis function neural networks. The major innovation lies in the fact that the radial basis function model is adaptively trained on-line, i.e., its parameters are identified in real time by the particle filter as new observations of the battery terminal voltage become available. By doing so, the prognostic algorithm achieves the flexibility needed to provide sound end-of-discharge time predictions as the charge-discharge cycles progress, even in presence of anomalous behaviors due to failures or unforeseen operating conditions. The method is demonstrated with reference to actual Li-Ion battery discharge data contained in the prognostics data repository of the NASA Ames Research Center database.",autonomous vehicle
10.1016/j.rse.2016.10.005,Journal,Remote Sensing of Environment,scopus,2016-12-15,sciencedirect,Development of methods to improve soybean yield estimation and predict plant maturity with an unmanned aerial vehicle based platform,https://api.elsevier.com/content/abstract/scopus_id/84991461437,"Advances in phenotyping technology are critical to ensure the genetic improvement of crops meet future global demands for food and fuel. Field-based phenotyping platforms are being evaluated for their ability to deliver the necessary throughput for large scale experiments and to provide an accurate depiction of trait performance in real-world environments. We developed a dual-camera high throughput phenotyping (HTP) platform on an unmanned aerial vehicle (UAV) and collected time course multispectral images for large scale soybean [Glycine max (L.) Merr.] breeding trials. We used a supervised machine learning model (Random Forest) to measure crop geometric features and obtained high correlations with final yield in breeding populations (r
                     =0.82). The traditional yield estimation model was significantly improved by incorporating plot row length as covariate (p
                     <0.01). We developed a binary prediction model from time-course multispectral HTP image data and achieved over 93% accuracy in classifying soybean maturity. This prediction model was validated in an independent breeding trial with a different plot type. These results show that multispectral data collected from the UAV-based HTP platform could improve yield estimation accuracy and maturity recording efficiency in a modern soybean breeding program.",autonomous vehicle
10.1016/j.oceaneng.2016.09.034,Journal,Ocean Engineering,scopus,2016-11-15,sciencedirect,Model-based adaptive control system for autonomous underwater vehicles,https://api.elsevier.com/content/abstract/scopus_id/84989182103,"The paper deals with the development of indirect adaptive controllers based on Hybrid Neuro-Fuzzy Network (HNFN) approach for Autonomous Underwater Vehicles (AUVs). The non-linear, coupled and time-varying dynamics of AUVs necessitates the development of adaptive controllers. The on-line identification and adaptation of the controller is carried out using the HNFN approach. The methodology uses the input-output data to come up with a structure for the controller and optimal adaptation of the parameters to achieve the required accuracy. The Semi-Serial-Parallel-Model is employed both for identification and control. Initial validation of the identification results are carried out numerically using a mathematical model. Hardware-in-loop (HIL) simulations are presented to validate the controller before carrying out the experiments. Experimental results show that the proposed controller is capable of suitably controlling the AUV in real environment and demonstrate its robust characteristics.",autonomous vehicle
10.1016/j.robot.2016.07.003,Journal,Robotics and Autonomous Systems,scopus,2016-10-01,sciencedirect,A practical approach for detection and classification of traffic signs using Convolutional Neural Networks,https://api.elsevier.com/content/abstract/scopus_id/84991736217,"Automatic detection and classification of traffic signs is an important task in smart and autonomous cars. Convolutional Neural Networks has shown a great success in classification of traffic signs and they have surpassed human performance on a challenging dataset called the German Traffic Sign Benchmark. However, these ConvNets suffer from two important issues. They are not computationally suitable for real-time applications in practice. Moreover, they cannot be used for detecting traffic signs for the same reason. In this paper, we propose a lightweight and accurate ConvNet for detecting traffic signs and explain how to implement the sliding window technique within the ConvNet using dilated convolutions. Then, we further optimize our previously proposed real-time ConvNet for the task of traffic sign classification and make it faster and more accurate. Our experiments on the German Traffic Sign Benchmark datasets show that the detection ConvNet locates the traffic signs with average precision equal to 
                        99.89
                        %
                     . Using our sliding window implementation, it is possible to process 37.72 high-resolution images per second in a multi-scale fashion and locate traffic signs. Moreover, single ConvNet proposed for the task of classification is able to classify 
                        99.55
                        %
                      of the test samples, correctly. Finally, our stability analysis reveals that the ConvNet is tolerant against Gaussian noise when 
                        σ
                        <
                        10
                     .",autonomous vehicle
10.1016/j.eswa.2016.03.024,Journal,Expert Systems with Applications,scopus,2016-10-01,sciencedirect,Building detection from orthophotos using a machine learning approach: An empirical study on image segmentation and descriptors,https://api.elsevier.com/content/abstract/scopus_id/84963795376,"Building detection from aerial images has many applications in fields like urban planning, real-estate management, and disaster relief. In the last two decades, a large variety of methods on automatic building detection have been proposed in the remote sensing literature. Many of these approaches make use of local features to classify each pixel or segment to an object label, therefore involving an extra step to fuse pixelwise decisions. This paper presents a generic framework that exploits recent advances in image segmentation and region descriptors extraction for the automatic and accurate detection of buildings on aerial orthophotos. The proposed solution is supervised in the sense that appearances of buildings are learnt from examples. For the first time in the context of building detection, we use the matrix covariance descriptor, which proves to be very informative and compact. Moreover, we introduce a principled evaluation that allows selecting the best pair segmentation algorithm-region descriptor for the task of building detection. Finally, we provide a performance evaluation at pixel level using different classifiers. This evaluation is conducted over 200 buildings using different segmentation algorithms and descriptors. The performance analysis quantifies the quality of both the image segmentation and the descriptor used. The proposed approach presents several advantages in terms of scalability, suitability and simplicity with respect to the existing methods. Furthermore, the proposed scheme (detection chain and evaluation) can be deployed for detecting multiple object categories that are present in images and can be used by intelligent systems requiring scene perception and parsing such as intelligent unmanned aerial vehicle navigation and automatic 3D city modeling.",autonomous vehicle
10.1016/j.ijpe.2016.01.003,Journal,International Journal of Production Economics,scopus,2016-04-01,sciencedirect,Using an agent-based neural-network computational model to improve product routing in a logistics facility,https://api.elsevier.com/content/abstract/scopus_id/84960121547,"This study tests whether a simplified neural-network computational model can make routing decisions in a logistics facility more efficiently than five ׳intelligent׳ routing heuristics from the logistics literature. The experiment uses a real-world simulation scenario based on the Hamburg Harbor Car Terminal, a logistic site faced with managing approximately 46,500 car-routing decisions on a yearly basis. The simulation environment has been built based on a data set provided by the Terminal operator to reflect a real-world case. The simulation results show that the percent-improvement of the neural-net model׳s performance is 48% better than that of the best routing heuristic tested in previous studies. To test the applicability of the method with more complex logistic scenarios, we relaxed the sequence constraints for routing in a subsequent simulations study. If logistic complexity in terms of more freedom in decision-making is increased, the neural net model׳s percent-improvement performance of routing decisions is around three times better than the best-performing heuristic.",autonomous vehicle
10.1016/j.ifacol.2016.10.485,Conference Proceeding,IFAC-PapersOnLine,scopus,2016-01-01,sciencedirect,"Towards Visual Detection, Mapping and Quantification of Posidonia Oceanica using a Lightweight AUV",https://api.elsevier.com/content/abstract/scopus_id/84994128037,"Posidonia Oceanica (P.O.) is a Mediterranean endemic seagrass strongly related to the health of the coastal ecosystems. Monitoring the presence and state of P.O. is essential not only for safeguarding the shallow-water life diversity, but also as an indicator of the water quality. Nowadays, the control of P.O. is done by divers in successive missions of a duration limited by the capacity of the scuba tanks. This paper proposes the application of robotic and computer vision technologies to upgrade these current methods, namely: 1) employing a lightweight Autonomous Underwater Vehicle (AUV) equipped with cameras to survey and image marine areas, 2) the automatic discrimination of P.O. from the rest of the seafloor, using several techniques based on image texture analysis and machine learning, and, 3) the fast computation of 2D maps (photo-mosaics) of the surveyed areas from all the images included in the grabbed video sequences; these mosaics are extremely useful to measure the real extension of the meadows and some of the descriptors needed for a biological analysis. Experiments conducted with an AUV in several marine areas of Mallorca reveal promising results in the discrimination of different patterns of P.O. and in the construction of highly realistic photo-mosaics of the surveyed areas.",autonomous vehicle
10.1016/j.trpro.2016.05.240,Conference Proceeding,Transportation Research Procedia,scopus,2016-01-01,sciencedirect,Advanced Driver Assistance System for Road Environments to Improve Safety and Efficiency,https://api.elsevier.com/content/abstract/scopus_id/84991383337,"The advances in Information Technologies have led to more complex road safety applications. These systems provide multiple possibilities for improving road transport. The integrated system that this paper presents deals with two aspects that have been identified as key topics: safety and efficiency. To this end, the development and implementation of an integrated advanced driver assistance system (ADAS) for rural and intercity environments is proposed. The system focuses mainly on single-carriageways roads, given the complexity of these environments compared to motorways and the high number of severe and fatal accidents on them. The proposed system is based on advanced perception techniques, vehicle automation and communications between vehicles (V2V) and with the infrastructure (V2I). Sensor fusion architecture based on computer vision and laser scanner technologies are developed. It allows real time detection and classification of obstacles, and the identification of potential risks. The driver receives this information and some warnings generated by the system. In case, he does not react in a proper way, the vehicle could perform autonomous actions (both on speed control or steering maneuvers) to improve safety and/or efficiency. Furthermore, a multimodal V2V and V2I communication system, based on GeoNetworking, facilitates the flow of information between vehicles and assists in the detection and information broadcasting processes. All this, combined with vehicle positioning, detailed digital maps and advanced map-matching algorithms, establish the decision algorithms of different ADAS systems.
                  The applications developed include: adaptive cruise control with consumption optimization, overtaking assistance system in single-carriageways roads that takes into account appropriate speed evolution and identifies most suitable road stretches for the maneuver; assistance system in intersections with speed control during approximation maneuvers, and collision avoidance system with the possibility of evasive maneuvers. To this end, mathematical vehicle dynamics models have been used to ensure the stability, and propulsion system models are used to establish efficient patterns, Artificial Intelligence and simulation are used for experimentation and evaluation of algorithms to be implemented in the control unit. Finally, the system is designed to warn the driver if a risk is detected and, if necessary, to take control of the vehicle. The system has been implemented on a passenger car and has been tested in specific scenarios on a test track with satisfactory results.",autonomous vehicle
10.1016/j.apor.2015.09.010,Journal,Applied Ocean Research,scopus,2015-10-01,sciencedirect,Neural adaptive robust control of underactuated marine surface vehicles with input saturation,https://api.elsevier.com/content/abstract/scopus_id/84945290024,"This paper proposes a saturated tracking controller for underactuated autonomous marine surface vehicles with limited torque. First, a second-order open-loop error dynamic model is developed in the actuated degrees of freedom to simplify the design procedure. Then, a saturated tracking controller is designed by utilizing generalized saturation functions to reduce the risk of actuator saturation. This, in turn, improves the transient performance of the control system. A multi-layer neural network and adaptive robust control techniques are also employed to preserve the controller robustness against unmodeled dynamics and environmental disturbances induced by waves and ocean currents. A Lyapunov stability analysis shows that all signals of the closed-loop system are bounded and tracking errors are semi-globally uniformly ultimately bounded. Finally, simulation results are provided for a hovercraft vehicle to illustrate the effectiveness of the proposed controller as a qualified candidate for real implementations in offshore applications.",autonomous vehicle
10.1016/j.oceaneng.2015.06.026,Journal,Ocean Engineering,scopus,2015-07-14,sciencedirect,Leader-follower formation control of underactuated autonomous marine surface vehicles with limited torque,https://api.elsevier.com/content/abstract/scopus_id/84937210986,"This paper proposes a leader–follower formation tracking controller for underactuated autonomous marine surface vehicles with limited torque under environmental disturbances. A second-order formation dynamic model is developed in the actuated degrees of freedom of the followers to simplify the design procedure. Then, a formation tracking controller is designed by utilizing generalized saturation functions to reduce the risk of actuator saturation. Radial basis function neural network and adaptive robust control techniques are also adopted to preserve the controller robustness against unmodeled dynamics and environmental disturbances induced by waves and ocean currents. A Lyapunov-based stability analysis shows that all signals of the closed-loop system are bounded and tracking errors are semi-globally uniformly ultimately bounded. Finally, simulation results are provided for a group of surface vessels to illustrate the effectiveness of the proposed controller as a qualified candidate for real implementations in offshore applications.",autonomous vehicle
10.1016/j.bica.2015.04.008,Journal,Biologically Inspired Cognitive Architectures,scopus,2015-01-01,sciencedirect,Automatic navigation of wall following mobile robot using Adaptive Resonance Theory of Type-1,https://api.elsevier.com/content/abstract/scopus_id/84960798237,"The automatic navigation of wall following robot is playing important role in various real world tasks such as underwater exploration, unmanned flight, and in automotive industries based on its computational complexity. In this work, a novel navigation approach based on biologically inspired neural network, known as “Adaptive Resonance Theory-1” which was proposed by Carpenter and Grossberg, has been implemented and investigated for navigation of wall following mobile robots. The proposed navigation algorithm is successfully tested with three sensor reading datasets obtained from clockwise navigation of SCITOS G5 mobile robot. Test decision accuracy (%), and simulation time were used as performance analysis parameters for the proposed algorithm and it has been found that the present work can achieve 99.59% of maximum decision accuracy.",autonomous vehicle
10.1016/j.procs.2015.07.309,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,WWN-8: Incremental online stereo with shape-from-X using life-long big data from multiple modalities,https://api.elsevier.com/content/abstract/scopus_id/84939200801,"When a child lives in the real world, from infancy to adulthood, his retinae receive a flood of stereo sensory stream. His muscles produce another action stream. How does the child's brain deal with such big data from multiple sensory modalities (left- and right-eye modalities) and multiple effector modalities (location, disparity map, and shape type)? This capability incrementally learns to produce simple-to-complex sensorimotor behaviors — autonomous development. We present a model that incrementally fuses such an open-ended life-long stream and updates the “brain” online so the perceived world is 3D. Traditional methods for shape- from-X use a particular type of cue X (e.g., stereo disparity, shading, etc.) to compute depths or local shapes based on a handcrafted physical model. Such a model likely results in a brit- tle system because of the fluctuation of the availability of the cue. An embodiment of the Developmental Network (DN), called Stereo Where-What Network (WWN-8), learns to per- form simultaneous attention and recognition, while developing invariances in location, disparity, shape, and surface type, so that multiple cues can automatically fill in if a particular type of cue (e.g., texture) is missing locally from the real world. We report some experiments: 1) dynamic synapse retraction and growth as a method of developing receptive fields. 2) training for recognizing 3D objects directly in cluttered natural backgrounds. 3) integration of depth perception with location and type information. The experiments used stereo images and motor actions on the order of 105 frames. Potential applications include driver assistance for road safety, mobile robots, autonomous navigation, and autonomous vision-guided manipulators.",autonomous vehicle
10.1016/j.neucom.2013.03.060,Journal,Neurocomputing,scopus,2014-05-20,sciencedirect,Autonomous UAV based search operations using constrained sampling evolutionary algorithms,https://api.elsevier.com/content/abstract/scopus_id/84896707806,"This paper introduces and studies the application of Constrained Sampling Evolutionary Algorithms in the framework of an UAV based search and rescue scenario. These algorithms have been developed as a way to harness the power of Evolutionary Algorithms (EA) when operating in complex, noisy, multimodal optimization problems and transfer the advantages of their approach to real time real world problems that can be transformed into search and optimization challenges. These types of problems are denoted as Constrained Sampling problems and are characterized by the fact that the physical limitations of reality do not allow for an instantaneous determination of the fitness of the points present in the population that must be evolved. A general approach to address these problems is presented and a particular implementation using Differential Evolution as an example of CS-EA is created and evaluated using teams of UAVs in search and rescue missions. The results are compared to those of a Swarm Intelligence based strategy in the same type of problem as this approach has been widely used within the UAV path planning field in different variants by many authors.",autonomous vehicle
10.1016/j.engappai.2014.01.007,Journal,Engineering Applications of Artificial Intelligence,scopus,2014-01-01,sciencedirect,Adaptive multi-objective reinforcement learning with hybrid exploration for traffic signal control based on cooperative multi-agent framework,https://api.elsevier.com/content/abstract/scopus_id/84894106219,"In this paper, we focus on computing a consistent traffic signal configuration at each junction that optimizes multiple performance indices, i.e., multi-objective traffic signal control. The multi-objective function includes minimizing trip waiting time, total trip time, and junction waiting time. Moreover, the multi-objective function includes maximizing flow rate, satisfying green waves for platoons traveling in main roads, avoiding accidents especially in residential areas, and forcing vehicles to move within moderate speed range of minimum fuel consumption. In particular, we formulate our multi-objective traffic signal control as a multi-agent system (MAS). Traffic signal controllers have a distributed nature in which each traffic signal agent acts individually and possibly cooperatively in a MAS. In addition, agents act autonomously according to the current traffic situation without any human intervention. Thus, we develop a multi-agent multi-objective reinforcement learning (RL) traffic signal control framework that simulates the driver's behavior (acceleration/deceleration) continuously in space and time dimensions. The proposed framework is based on a multi-objective sequential decision making process whose parameters are estimated based on the Bayesian interpretation of probability. Using this interpretation together with a novel adaptive cooperative exploration technique, the proposed traffic signal controller can make real-time adaptation in the sense that it responds effectively to the changing road dynamics. These road dynamics are simulated by the Green Light District (GLD) vehicle traffic simulator that is the testbed of our traffic signal control. We have implemented the Intelligent Driver Model (IDM) acceleration model in the GLD traffic simulator. The change in road conditions is modeled by varying the traffic demand probability distribution and adapting the IDM parameters to the adverse weather conditions. Under the congested and free traffic situations, the proposed multi-objective controller significantly outperforms the underlying single objective controller which only minimizes the trip waiting time (i.e., the total waiting time in the whole vehicle trip rather than at a specific junction). For instance, the average trip and waiting times are ≃8 and 6 times lower respectively when using the multi-objective controller.",autonomous vehicle
10.1016/j.jmsy.2011.09.002,Journal,Journal of Manufacturing Systems,scopus,2012-04-01,sciencedirect,Intelligent evaluation of supplier bids using a hybrid technique in distributed supply chains,https://api.elsevier.com/content/abstract/scopus_id/84858340427,"The main idea of this research is to devise the smart module to pick the best supplier bid(s) automatically. The hybrid model is composed of three useful tools: fuzzy logic, AHP, and QFD. The approach has been carefully implemented and verified via a real-world case study in a medium-to-large industry manufacturing vehicle tires and other rubber products. A collection of 12 assessment criteria classified into two categories have been considered. Eight factors are derived from customer suggestions and the other four are design specifications required to manufacture the product. The main outcomes are: a hybrid autonomous model to evaluate supplier bids without direct human intervention; devising a hybrid three-module method and overcoming complexity of computations in resulting algorithm by means of agents; outlining the best criteria to assess suppliers; evaluating the suppliers based on voice of customer during all stages of the process; and discussing analysis, design, and implementation issues of the evaluation agent. The paper includes implications for development of an integrated total system for supply chain coordination. The most important advantages of this work over earlier researches on supplier selection are: implementation of an autonomous assessment mechanism using intelligent agents for the first time, making the best out of three widely applied methodologies all at once, evaluation process mainly based on features of customer order, coordination of supply job based on a bidding system, and portal-mediated operation and control.",autonomous vehicle
10.3182/20120403-3-DE-3010.00041,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,Embedded system for controlling a mini underwater vehicle in autonomous hover mode,https://api.elsevier.com/content/abstract/scopus_id/84866098847,"This work presents the development of a mini underwater vehicle (Triton-PR), the embedded system, and the experiments in real-time for autonomous hover operation. Artificial vision allows the vehicle to obtain the translational position and velocity. The main characteristic of the embedded system is the implementation of low cost devices and materials, besides the number and location of the thrusters was chosen in order to have enough power and generate the rotation and translation movements. The dynamical model of the (Triton-PR) is described by the classic Euler-Lagrange equations, and a PD controller based on saturation functions is proposed for providing autonomous attitude and position of the robot. Finally, the performance of the vehicle is shown in simulation and real-time experimental results.",autonomous vehicle
10.1016/j.apergo.2011.09.004,Journal,Applied Ergonomics,scopus,2012-01-01,sciencedirect,Cognitive conflict in human-automation interactions: A psychophysiological study,https://api.elsevier.com/content/abstract/scopus_id/84855890488,"The review of literature in sociology and distributed artificial intelligence reveals that the occurrence of conflict is a remarkable precursor to the disruption of multi-agent systems. The study of this concept could be applied to human factors concerns, as man-system conflict appears to provoke perseveration behavior and to degrade attentional abilities with a trend to excessive focus. Once entangled in such conflicts, the human operator will do anything to succeed in his current goal even if it jeopardizes the mission. In order to confirm these findings, an experimental setup, composed of a real unmanned ground vehicle, a ground station is developed. A scenario involving an authority conflict between the participants and the robot is proposed. Analysis of the effects of the conflict on the participants’ cognition and arousal is assessed through heart-rate measurement (reflecting stress level) and eye-tracking techniques (index of attentional focus). Our results clearly show that the occurrence of the conflict leads to perseveration behavior and can induce higher heart rate as well as excessive attentional focus. These results are discussed in terms of task commitment issues and increased arousal. Moreover, our results suggest that individual differences may predict susceptibility to perseveration behavior.",autonomous vehicle
10.1016/j.neunet.2009.06.032,Journal,Neural Networks,scopus,2009-07-01,sciencedirect,Extending the Evolutionary Robotics approach to flying machines: An application to MAV teams,https://api.elsevier.com/content/abstract/scopus_id/68149170032,"The work presented in this article focuses on the use of embodied neural networks–developed through Evolutionary Robotics and Multi-Agent Systems methodologies–as autonomous distributed controllers for Micro-unmanned Aerial Vehicle (MAV) teams. The main aim of the research is to extend the range of domains that could be successfully tackled by the Evolutionary Robotics approach. The flying robots realm is an area that has not been yet thoroughly investigated by this discipline. This is due to the lack of an affordable and reliable robotic platform to use for carrying out experiments, and to the difficulty and the high computational load involved in experiments based upon a realistic software simulator for aircraft. We believe that the most recent improvements to the state of the art now permit the investigation of this domain. For demonstrating this point, two different evolutionary computer simulation models are presented in this article. The first model, which uses a simplified 2D test environment, has resulted in controllers evolved with the following capabilities: (1) navigation through unknown environments, (2) obstacle-avoidance, (3) tracking of a movable target, and (4) execution of cooperative and coordinated behaviors based on implicit communication strategies. In order to improve the robustness of these results and their potential use in real MAV teams, a more sophisticated 3D model is presented herein. The results obtained so far using the two models demonstrate the feasibility of the chosen approach for further research on the design of autonomous controllers for MAVs.",autonomous vehicle
10.1016/j.asoc.2007.07.009,Journal,Applied Soft Computing Journal,scopus,2008-03-01,sciencedirect,Direct adaptive neural flight control system for an unstable unmanned aircraft,https://api.elsevier.com/content/abstract/scopus_id/37249000905,"A direct adaptive controller design using neural network is proposed for an unstable unmanned research aircraft similar in configuration to combat aircraft. The control law to track the pitch rate command is developed based on system theory. Neural network with linear filters and back propagation through time learning algorithm is used to approximate the control law. The bounded signal requirement to develop the neural controller is circumvented using an off-line finite time training scheme, which provides the necessary stability and tracking performances. On-line learning scheme is implemented to compensate for uncertainties due to variation in aerodynamic coefficients, control surface failures and also variations in center of gravity position. The performance of the proposed control scheme is validated at different flight conditions. The disturbance rejection capability of the neural controller is analyzed in the presence of the realistic gust and sensor noises. Hardware-in-loop simulation is also carried out to study the behavior of control surface deflections in real-time.",autonomous vehicle
10.1016/j.robot.2007.03.003,Journal,Robotics and Autonomous Systems,scopus,2007-08-31,sciencedirect,Application of SONQL for real-time learning of robot behaviors,https://api.elsevier.com/content/abstract/scopus_id/34447537708,"This paper describes the Semi-Online Neural-Q-learning (SONQL) algorithm designed for real-time learning of reactive robot behaviors. The Q-function is generalized by a multilayer neural network allowing the use of continuous states. The algorithm uses a database of the most recent learning samples to accelerate and improve the convergence. Each SONQL algorithm represents an independent, reactive and adaptive state-action mapping, which implements the function of a robot behavior for one degree of freedom (DOF). The generalization capability of the SONQL algorithm was demonstrated by computer simulation with the “mountain–car” benchmark. The SONQL was also investigated by experiment on a mobile robot for a target-following task. Experimental results show that the SONQL is promising for online robot learning.",autonomous vehicle
10.1016/j.jnca.2004.10.005,Journal,Journal of Network and Computer Applications,scopus,2005-08-01,sciencedirect,A future framework for interfacing BDI agents in a real-time teaming environment,https://api.elsevier.com/content/abstract/scopus_id/29144487764,"Research and applications in human–machine teaming continue to evolve the role of the human from immediate (manual) operator into supervisory and televisory controller. In the supervisory control role, the human operator will be functionally removed from the system under control and in the televisory role, the human operator will be physically removed. Although unmanned systems and vehicles have become a technical reality that drives this change, they will not eliminate the importance of the human operator as the commanding and controlling element in-the-loop. This paper will argue that existing automation concepts remain equally valid with an even greater emphasis on the need for a human-centered automation approach. Intelligent agent technology has become mature and attractive enough to implement the automated components of the human–machine team. Agents that implement the Beliefs-Desire-Intention syntax will be discussed as being of particular interest for human–machine teaming applications. This paper proposes a theoretical framework for teaming human and intelligent agents. The teaming framework will be demonstrated in a real-time simulation environment using the commercial game called Unreal Tournament and its existing GameBot extension. The intelligent agents will be implemented based on the Belief-Desire-Intention (BDI) syntax and using JACK, a commercial BDI Agent development language. The requirements for follow-on research, such as human–agent teaming, human–agent coordination and agent learning will be highlighted.",autonomous vehicle
10.3182/20020721-6-es-1901.01303,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2002-01-01,sciencedirect,High-level control of autonomous robots using a behavior-based scheme and reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/84945581469,"This paper proposes a behavior-based scheme for high-level control of autonomous robots. Two main characteristics can be highlighted in the control scheme. Behavior coordination is done through a hybrid methodology, which takes in advantages of the robustness and modularity in competitive approaches, as well as optimized trajectories in cooperative ones. As a second feature, behavior state/action mapping is learnt by means of Reinforcement Learning (RL). A new continuous approach of the Q_learning algorithm, implemented with a multi-layer neural network, is used. The behavior-based scheme attempts to fulfill simple missions in which several behaviors/tasks compete for the vehicle's control. This paper is centered in the RL-based behaviors. In order to test the feasibility of the proposed Neural-Q_learning scheme, real experiments with the underwater robot ODIN in a target following behavior were done. Results showed the convergence of the behavior into an optimal state/action mapping. Discussion about the proposed approach is given, as well as an overall description of the high level control scheme.",autonomous vehicle
10.1016/S0952-1976(01)00004-5,Journal,Engineering Applications of Artificial Intelligence,scopus,2001-01-01,sciencedirect,Parallel and diagonal parking in nonholonomic autonomous vehicles,https://api.elsevier.com/content/abstract/scopus_id/0035435172,"This paper considers the problem of parallel and diagonal parking in wheeled vehicles. A method to plan in real-time a set of collision-free manoeuvres is presented. Artificial intelligent techniques, namely fuzzy logic, play an important role in the practical application of the method. Thus, a fuzzy system is used to select the most suitable manoeuvre from the solution set according with the environment, dealing with optimality, path tracking performance and collision avoidance trade-off. This technique has been implemented in a fuzzy behaviour-based control architecture combining planning and reactivity. The efficiency of the proposed method is demonstrated using the nonholonomic mobile robot ROMEO-3R designed and built at the University of Seville.",autonomous vehicle
10.1016/S0967-0661(99)00153-7,Journal,Control Engineering Practice,scopus,2000-02-01,sciencedirect,An adaptive neural-net controller system for an underwater vehicle,https://api.elsevier.com/content/abstract/scopus_id/0034112414,"This paper describes the concept and an example of an adaptive neural-net controller system for an underwater vehicle. The system consists of two parts, a real-world part and an imaginary-world part. The real-world part is a feedback control system for the actual robot. In the imaginary-world part, the model of robot and the controller are adjusted continuously in order to deal with changes of dynamic properties caused by disturbances and so on. The system is designed to be suitable for a computer system with parallel processing ability. In this paper, the adaptability of the controller system is investigated by heading-keeping and path-following experiments in conditions where unpredictable disturbances are applied to the robot.",autonomous vehicle
10.1016/S0736-5845(99)00033-2,Journal,Robotics and Computer-Integrated Manufacturing,scopus,1999-01-01,sciencedirect,A new approach to vision-based unsupervised learning of unexplored indoor environment for autonomous land vehicle navigation,https://api.elsevier.com/content/abstract/scopus_id/0042538803,"A vision-based approach to unsupervised learning of the indoor environment for autonomous land vehicle (ALV) navigation is proposed. The ALV may, without human's involvement, self-navigate systematically in an unexplored closed environment, collect the information of the environment features, and then build a top-view map of the environment for later planned navigation or other applications. The learning system consists of three subsystems: a feature location subsystem, a model management subsystem, and an environment exploration subsystem. The feature location subsystem processes input images, and calculates the locations of the local features and the ALV by model matching techniques. To facilitate feature collection, two laser markers are mounted on the vehicle which project laser light on the corridor walls to form easily detectable line and corner features. The model management subsystem attaches the local model into a global one by merging matched corner pairs as well as line segment pairs. The environment exploration subsystem guides the ALV to explore the entire navigation environment by using the information of the learned model and the current ALV location. The guidance scheme is based on the use of a pushdown transducer derived from automata theory. A prototype learning system was implemented on a real vehicle, and simulations and experimental results in real environments show the feasibility of the proposed approach.",autonomous vehicle
10.1016/S0921-8890(98)00005-0,Journal,Robotics and Autonomous Systems,scopus,1998-10-31,sciencedirect,A robust landmark-based system for vehicle location using low-bandwidth vision,https://api.elsevier.com/content/abstract/scopus_id/0032180335,"This paper presents novel computer algorithms, a system architecture, and the prototype implementation of a vision-based automatic vehicle location system. The objective of the vehicle location system is to keep track of the vehicle location for a human driver, and perhaps to provide the driver with real-time audio directions to his destination. The techniques developed here are equally applicable to autonomous robot navigation. The prototype system uses odometer readings and a skeleton map to perform dead reckoning, and uses low-bandwidth visual information and neural networks to recognize places for correcting cumulative dead reckoning errors. The visual information is also used to detect turns, for dead reckoning at intersections. The system is self-contained in the sense that it requires no infrastructure outside the vehicle, such as external beacons installed on roadways or satellites used by Global Positioning Systems (GPS). The system maintains a large number of location hypotheses and searches for a large number of landmarks stored in a database in real time. Hence the system is robustly able to recover the vehicle location after being lost for various reasons. The system has been tested, with success, in both day and night time, in all four seasons, and on roads in New York City, a regional highway, and on suburban streets.",autonomous vehicle
10.1016/S0921-8890(97)00039-0,Journal,Robotics and Autonomous Systems,scopus,1997-01-01,sciencedirect,The evolution of different neuronal control structures for autonomous agents,https://api.elsevier.com/content/abstract/scopus_id/0031380539,"The use of evolutionary methods to generate controllers for real-world autonomous agents has attracted recent attention. Most of the pertinent research has employed genetic algorithms or variations thereof. Recent research has indicated that the presence of epistasis drastically slows down genetic algorithms. For this reason, this paper uses a different evolutionary method, evolution strategies, for the evolution of various (complex) neuronal control architectures for mobile robots inspired by Braitenberg vehicles. In these experiments, the evolution strategy accelerates the development process by more than an order of magnitude (a few hours compared to more than two days). Furthermore, the evolution strategy yields the same efficacy when applied to receptive-field controllers that require many more parameters than Braitenberg controllers. This dramatic speedup is very important, since the development process is to be done in real robots.",autonomous vehicle
10.1016/0925-2312(94)00018-N,Journal,Neurocomputing,scopus,1995-01-01,sciencedirect,Fast computation of optimal paths using a parallel Dijkstra algorithm with embedded constraints,https://api.elsevier.com/content/abstract/scopus_id/0029329099,"We have developed a new optimal path algorithm in which the paths are subjected to turning constraints. The restriction which we have incorporated is the next link in the path must not make an angle exceeding 45 ° in magnitude with the preceeding link. This algorithm has a natural implementation as an artificial neural system with either synchronous or asynchronous weight updating, and as an automata executing on a massively parallel array processor. At a given step in the path solution process our path planning artificial neural system keeps track of all constrained optimal paths flowing into the nodes of the network. This new algorithm has applications to any path planning problem where the vehicle traveling the path is subject to a limited turning capability. The ability of the network to solve for constrained paths is illustrated with both a graph theoretic example and a scenario involving an unmanned vehicle that must travel a constrained path through a real terrain area containing artificially generated keep out zones.",autonomous vehicle
10.1016/0921-8890(94)00028-Z,Journal,Robotics and Autonomous Systems,scopus,1995-01-01,sciencedirect,ALEF: An autonomous vehicle which learns basic skills and constructs maps for navigation,https://api.elsevier.com/content/abstract/scopus_id/0029305017,"This paper introduces a control system for a mobile robot which provides learning for two essential kinds of knowledge representation: internal world models and internal abstract concepts. Concretely, learning methods are applied to evolve basic skills (goal oriented driving and collision avoidance) and to generate maps of the environment, even if the mobile robot acts in an a-priori unknown environment and no external navigation aids (like beacons) are used. Results of experiments are presented performed with a real mobile robot equipped with simple and inaccurate sensors (a ring of ultrasonic sensors and goniometers on the wheel axles).",autonomous vehicle
10.1016/0952-1976(94)90020-5,Journal,Engineering Applications of Artificial Intelligence,scopus,1994-01-01,sciencedirect,Curvature-optimal path planning and servoing for autonomous vehicles: A neural net implementation,https://api.elsevier.com/content/abstract/scopus_id/0028406446,"This paper shows how to implement curvature-optimal solutions of path-planning and trajectoryservoing problems for autonomous vehicles by using neural nets. The neural net is used to implement the nonlinear optimal state-feedback laws. The trained net can be used in real-time control. The net is trained by simulated data generated at random. No two-point boundary-value problems need to be solved. The method presented can, in general, be applied to a wide range of time-invariant terminal control or servo control problems.",autonomous vehicle
10.1016/0165-1684(93)90040-H,Journal,Signal Processing,scopus,1993-01-01,sciencedirect,Determination of road directions using feedback neural nets,https://api.elsevier.com/content/abstract/scopus_id/0027588835,"Autonomous vehicles may be driven by image data of real-world scenes collected through a TV camera. Detecting the clues for safe navigation requires, among other things, the estimation of the path to be followed by the vehicle, which has proven to be a formidable task in outdoor scenes. In this paper, an innovative system for road direction detection is proposed which is composed of three specialized blocks performing edge extraction, image-segments detection and road estimation. The road direction estimation block is implemented as a feedback neural network and is not fed directly with image data but with higher-level image features which are extracted through the preprocessing stages. The use of feedback, while reducing the complexity of the network, improves the estimation robustness and the noise immunity. A novel algorithm is defined and employed for the training step and experimental results in outdoor scenes are reported.",autonomous vehicle
